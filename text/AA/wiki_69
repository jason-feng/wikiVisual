<doc id="9580" url="http://en.wikipedia.org/wiki?curid=9580" title="European Free Trade Association">
European Free Trade Association

The European Free Trade Association (EFTA) is a free trade organisation between four European countries that operates in parallel with – and is linked to – the European Union (EU). The EFTA was established on 3 May 1960 as a trade bloc-alternative for European states who were either unable or unwilling to join the then-European Economic Community (EEC) which has now become the EU. The Stockholm Convention, establishing the EFTA, was signed on 4 January 1960 in the Swedish capital by seven countries (known as the "outer seven").
Today's EFTA members are Iceland, Liechtenstein, Norway, and Switzerland, of which the latter two were founding members. The initial Stockholm Convention was superseded by the Vaduz Convention, which enabled greater liberalisation of trade among the member states.
EFTA states have jointly concluded free trade agreements with a number of other countries. All four current members of EFTA are part of the European Union's internal market; Iceland, Liechtenstein and Norway through the Agreement on a European Economic Area (EEA) and Switzerland through a set of bilateral agreements. This development prompted the EFTA states to modernise their Convention to ensure that it will continue to provide a successful framework for the expansion and liberalization of trade among themselves and with the rest of the world.
History.
British reaction to the creation of the EEC was mixed and complex. Britain was also preoccupied with the Commonwealth, which, at the time of EFTA's formation, was in transition. Britain therefore brought together several countries, including some bordering the EEC, to form the European Free Trade Association soon after the establishment of the six-nation EEC (France, West Germany, Italy, Belgium, Luxembourg, and the Netherlands).
On 12 January 1960, the Treaty on European Free Trade Association was initialled in the Golden Hall of the Prince's Palace of Stockholm. This established the progressive elimination of customs duties on industrial products, but did not affect agricultural products or maritime trade.
The main difference between the early EEC and the EFTA was the absence of a common external customs tariff, and therefore each EFTA member was free to establish individual customs duties against, or individual free trade agreements with, non EFTA countries.
Despite this modest initiative, the financial results were excellent, as it stimulated an increase of foreign trade volume among its members from 3.5 to 8.2 billion US dollars between 1959 and 1967. This was rather less than the increase enjoyed by countries inside the EEC.
After the accession of Denmark and the UK to the EEC in January 1973, EFTA began to falter. For this reason most countries eased or eliminated their trade tariffs in preparation to join the EEC, but experienced declining revenue which reduced the importance of EFTA. Four members remain: Switzerland, Norway, Liechtenstein and Iceland. Iceland applied for EU membership in 2009 due to the 2008–2011 Icelandic financial crisis.
Membership.
History.
The founding members of EFTA were Austria, Denmark, Norway, Portugal, Sweden, Switzerland and the United Kingdom. During the 1960s these countries were often referred to as the Outer Seven, as opposed to the Inner Six of the then-European Economic Community (EEC).
Finland became an associate member in 1961 and a full member in 1986, and Iceland joined in 1970. The United Kingdom and Denmark joined the EEC in 1973, and hence ceased to be EFTA members. Portugal also left EFTA for the European Community in 1986. Liechtenstein joined EFTA in 1991 (previously its interests had been represented by Switzerland). Austria, Sweden and Finland joined the EU in 1995 and thus ceased to be EFTA members.
Twice, in 1973 and 1995, Norway has tried to join the EU (still the EEC in 1973) and by doing so, leave the EFTA. Both times, membership of the EU was rejected in national referendums, keeping Norway in the EFTA. Iceland currently is in negotiations as a candidate for EU membership. If they were to complete negotiations and all countries ratified the accession treaty, they would leave the EFTA and join the EU. As of June 2013 negotiations between Iceland and EU stopped by request of the Icelandic minister of foreign affairs.
Future.
The Norwegian electorate has rejected treaties of accession to the EU in two referendums. At the time of the first referendum (1972) their neighbour Denmark joined. The second time (1994) two other Nordic neighbours, Sweden and Finland, joined the EU. The last two governments of Norway have been unable and unwilling to advance the question, as they have both been coalition governments consisting of proponents and opponents.
Since Switzerland rejected the EEA in 1992, referendums on EU membership have been initiated, the last time in 2001. These were rejected.
Iceland may join the EU in the future, following the global financial crisis of 2008, which has particularly affected the local economy. On 16 July 2009, the government formally applied for EU membership. As noted, the negotiation process has been halted since mid-2013.
Between 1994 and 2011, EFTA membership has been discussed regarding Andorra, San Marino, Monaco, Isle of Man, Morocco, Turkey, Israel and other ENP partners.
In mid-2005, representatives of the Faroe Islands hinted at the possibility of their territory joining EFTA. However, the chances of the Faroes' bid for membership are uncertain because, according to Article 56 of the EFTA Convention, only states may become members of the Association. The Faroes already have an extensive bilateral free trade agreement with Iceland, known as the Hoyvík Agreement.
In November 2012, after the Council of the European Union had called for an evaluation of the EU's relations with the sovereign European microstates of Andorra, Monaco and San Marino, which they described as "fragmented", the European Commission published a report outlining options for their further integration into the EU. Unlike Liechtenstein, which is a member of the EEA via the EFTA and the Schengen Agreement, relations with these three states are based on a collection of agreements covering specific issues. The report examined four alternatives to the current situation: 1) a Sectoral Approach with separate agreements with each state covering an entire policy area, 2) a comprehensive, multilateral Framework Association Agreement (FAA) with the three states, 3) EEA membership, and 4) EU membership. The Commission argued that the sectoral approach did not address the major issues and was still needlessly complicated, while EU membership was dismissed in the near future because "the EU institutions are currently not adapted to the accession of such small-sized countries." The remaining options, EEA membership and a FAA with the states, were found to be viable and were recommended by the Commission. In response, the Council requested that negotiations with the three microstates on further integration continue, and that a report be prepared by the end of 2013 detailing the implications of the two viable alternatives and recommendations on how to proceed.
As EEA membership is currently only open to EFTA or EU members, the consent of existing EFTA member states is required for the microstates to join the EEA without becoming members of the EU. In 2011, Jonas Gahr Støre, the then Foreign Minister of Norway which is an EFTA member state, said that EFTA/EEA membership for the microstates was not the appropriate mechanism for their integration into the internal market due to their different requirements than large countries such as Norway, and suggested that a simplified association would be better suited for them. Espen Barth Eide, Støre's successor, responded to the Commission's report in late 2012 by questioning whether the microstates have sufficient administrative capabilities to meet the obligations of EEA membership. However, he stated that Norway was open to the possibility of EFTA membership for the microstates if they decide to submit an application, and that the country had not made a final decision on the matter. Pascal Schafhauser, the Counsellor of the Liechtenstein Mission to the EU, said that Liechtenstein, another EFTA member state, was willing to discuss EEA membership for the microstates provided their joining did not impede the functioning of the organization. However, he suggested that the option direct membership in the EEA for the microstates, outside of both the EFTA and the EU, should be given consideration.
On 18 November 2013 the EU Commission concluded that "the participation of the small-sized countries in the EEA is not judged to be a viable option at present due to the political and institutional reasons", and that Association Agreements were a more feasible mechanism to integrate the microstates into the internal market.
Institutions.
EFTA is governed by the EFTA Council and serviced by the EFTA Secretariat. In addition, in connection with the EEA Agreement of 1992, two other EFTA organisations were established, the EFTA Surveillance Authority and the EFTA Court.
Council.
The EFTA Council is the highest governing body of EFTA. The Council usually meets eight times a year at the ambassadorial level (heads of permanent delegations to EFTA) and twice a year at Ministerial level. In the Council meetings, the delegations consult with one another, negotiate and decide on policy issues regarding EFTA. Each Member State is represented and has one vote, though decisions are usually reached through consensus.
The Council discusses substantive matters, especially relating to the development of EFTA relations with third countries and the management of free trade agreements, and keeps under general review relations with the EU third-country policy and administration. It has a broad mandate to consider possible policies to promote the overall objectives of the Association and to facilitate the development of links with other states, unions of states or international organisations. The Council also manages relations between the EFTA States under the EFTA Convention. Questions relating to the EEA are dealt with by the Standing Committee in Brussels.
Secretariat.
The day-to-day running of the Secretariat is headed by the Secretary-General, Kristinn F. Árnason, who is assisted by two Deputy Secretaries-General, one based in Geneva and the other in Brussels. The three posts are shared between the Member State. The division of the Secretariat reflects the division of EFTA’s activities. The Secretariat employs approximately 100 staff members, of whom a third are based in Geneva and two thirds in Brussels and Luxembourg.
The Headquarters in Geneva deals with the management and negotiation of free trade agreements with non-EU countries, and provide support to the EFTA Council.
In Brussels, the Secretariat provides support for the management of the EEA Agreement and assists the Member States in the preparation of new legislation for integration into the EEA Agreement. The Secretariat also assists the Member States in the elaboration of input to EU decision making.
The two duty stations work together closely to implement the Vaduz Convention’s stipulations on the intra-EFTA Free Trade Area.
The EFTA Statistical Office in Luxembourg contributes to the development of a broad and integrated European Statistical System.
The EFTA Statistical Office in Luxembourg contributes to the development of a broad and integrated European Statistical System. The EFTA Statistical Office (ESO) is located in the premises of Eurostat, the Statistical Office of the European Union, in Luxembourg, and functions as a liaison office between Eurostat and the EFTA National Statistical Institutes. ESO's main objective is to promote the full inclusion of the EFTA States in the European Statistical System, thus providing harmonised and comparable statistics to support the general cooperation process between EFTA and the EU within and outside the EEA Agreement. The cooperation also entails technical cooperation programmes with third countries and training of European statisticians.
European Economic Area.
A Joint Committee consisting of the EEA States plus the European Commission (representing the EU) has the function of extending relevant EU law to the non EU members. An EEA Council meets twice yearly to govern the overall relationship between the EEA members.
Rather than setting up pan-EEA institutions, the activities of the EEA are regulated by the EFTA Surveillance Authority and the EFTA Court. The EFTA Surveillance Authority and the EFTA Court regulate the activities of the EFTA members in respect of their obligations in the European Economic Area (EEA). Since Switzerland is not an EEA member, it does not participate in these institutions.
The EFTA Surveillance Authority performs the European Commission's role as "guardian of the treaties" for the EFTA countries, while the EFTA Court performs the European Court of Justice's role for those countries.
The original plan for the EEA lacked the EFTA Court or the EFTA Surveillance Authority, the European Court of Justice and the European Commission were to exercise those roles. However, during the negotiations for the EEA agreement, the European Court of Justice informed the Council of the European Union by way of letter that they considered that giving the EU institutions powers with respect to non-EU member states would be a violation of the treaties, and therefore the current arrangement was developed instead.
Norway Grants.
The EEA and Norway Grants are the financial contributions of Iceland, Liechtenstein and Norway to reduce social and economic disparities in Europe. They were established in conjunction with the 2004 enlargement of the European Economic Area (EEA), which brought together the EU, Iceland, Liechtenstein and Norway in the Internal Market. In the period from 2004 to 2009, €1.3 billion of project funding was made available for project funding in the 15 beneficiary states in Central and Southern Europe. The EEA and Norway Grants are administered by the Financial Mechanism Office, which is affiliated to the EFTA Secretariat in Brussels.
Locations.
The EFTA Secretariat is headquartered in Geneva, Switzerland, but also has duty stations in Brussels, Belgium and Luxembourg. The EFTA Surveillance Authority has its headquarters in Brussels, Belgium (the same location as the headquarters of the European Commission), while the EFTA Court has its headquarters in Luxembourg (the same location as the headquarters of the European Court of Justice).
Portugal Fund.
The Portugal Fund was established in 1975 when Portugal was still a member of EFTA, to provide funding for the development and reconstruction of Portugal after the Carnation Revolution. When Portugal left EFTA in 1985 to join the EEC, the remaining EFTA members decided to nonetheless continue the Portugal Fund, so Portugal would continue to benefit from it. The Fund originally took the form of a low-interest loan from the EFTA member states to Portugal, to the value of 100 million US dollars. Repayment was originally to commence in 1988, but EFTA then decided to postpone the start of repayments until 1998. The Portugal Fund has now been dissolved by the Member States.
European Union.
Except for Switzerland, the EFTA members are also members of the European Economic Area (EEA). The EEA comprises three member states of the European Free Trade Association (EFTA) and 28 member states of the European Union (EU), including Croatia which is provisionally applying the agreement pending its ratification by all EEA countries. It was established on 1 January 1994 following an agreement with the European Community (which had become the EU two months earlier). It allows the EFTA-EEA states to participate in the EU's Internal Market without being members of the EU. They adopt almost all EU legislation related to the single market, except laws on agriculture and fisheries. However, they also contribute to and influence the formation of new EEA relevant policies and legislation at an early stage as part of a formal decision-shaping process. One EFTA member, Switzerland, has not joined the EEA but has a series of bilateral agreements, including a free trade agreement, with the EU.
The following table summarises the various components of EU laws applied in the EFTA countries and their sovereign territories. Some territories of EU member states also have a special status in regard to EU laws applied as is the case with some European microstates.
International conventions.
EFTA also originated the Hallmarking Convention and the Pharmaceutical Inspection Convention, both of which are open to non-EFTA states.
International relationships.
EFTA has several free trade agreements with non-EU countries as well as declarations on cooperation and joint workgroups to improve trade. Currently, the EFTA States have established preferential trade relations with 24 states and territories, in addition to the 28 member states of the European Union.

</doc>
<doc id="9581" url="http://en.wikipedia.org/wiki?curid=9581" title="European Parliament">
European Parliament

The European Parliament (abbreviated as EU Parliament or the EP) is the directly elected parliamentary institution of the European Union (EU). Together with the Council of the European Union (the Council) and the European Commission, it exercises the legislative function of the EU. The Parliament is composed of 751 (previously 766) members, who represent the second largest democratic electorate in the world (after the Parliament of India) and the largest trans-national democratic electorate in the world (375 million eligible voters in 2009).
It has been directly elected every five years by universal suffrage since 1979. However, turnout at European Parliament elections has fallen consecutively at each election since that date, and has been under 50% since 1999. Turnout in 2014 stood at 42.54% of all European voters.
Although the European Parliament has legislative power that the Council and Commission do not possess, it does not formally possess legislative initiative, as most national parliaments of European Union member states do. The Parliament is the "first institution" of the EU (mentioned first in the treaties, having ceremonial precedence over all authority at European level), and shares equal legislative and budgetary powers with the Council (except in a few areas where the special legislative procedures apply). It likewise has equal control over the EU budget. Finally, the European Commission, the executive body of the EU, is accountable to Parliament. In particular, Parliament elects the President of the Commission, and approves (or rejects) the appointment of the Commission as a whole. It can subsequently force the Commission as a body to resign by adopting a motion of censure.
The President of the European Parliament (Parliament's speaker) is Martin Schulz (S&D), elected in January 2012. He presides over a multi-party chamber, the two largest groups being the Group of the European People's Party (EPP) and the Progressive Alliance of Socialists and Democrats (S&D). The last union-wide elections were the 2014 elections. The European Parliament has three places of work – Brussels (Belgium), the city of Luxembourg (Luxembourg) and Strasbourg (France).
Luxembourg is home to the administrative offices (the 'General Secretariat'). Meetings of the whole Parliament ('plenary sessions') take place in Strasbourg and in Brussels. Committee meetings are held in Brussels.
History.
The Parliament, like the other institutions, was not designed in its current form when it first met on 10 September 1952. One of the oldest common institutions, it began as the "Common Assembly" of the European Coal and Steel Community (ECSC). It was a consultative assembly of 78 appointed parliamentarians drawn from the national parliaments of member states (see dual mandate), having no legislative powers. The change since its foundation was highlighted by Professor David Farrell of the University of Manchester;
For much of its life, the European Parliament could have been justly labeled a 'multi-lingual talking shop'. But this is no longer the case: the EP is now one of the most powerful legislatures in the world both in terms of its legislative and executive oversight powers.
Its development since its foundation is a testament to the evolution of the Union's structures without one clear "master plan". Some such as Tom Reid of the "Washington Post" said of the union, "nobody would have deliberately designed a government as complex and as redundant as the EU". Even the Parliament's two seats, which have switched several times, are a result of various agreements or lack of agreements. Although most MEPs would prefer to be based just in Brussels, at John Major's 1992 Edinburgh summit, France engineered a treaty amendment to maintain Parliament's plenary seat permanently at Strasbourg.
Consultative assembly.
The body was not mentioned in the original Schuman Declaration. It was assumed or hoped that difficulties with the British would be resolved to allow the Council of Europe's Assembly to perform the task. A separate Assembly was introduced during negotiations on the Treaty as an institution which would counterbalance and monitor the executive while providing democratic legitimacy. The wording of the ECSC Treaty demonstrated the leaders' desire for more than a normal consultative assembly by using the term "representatives of the people" and allowed for direct election. Its early importance was highlighted when the Assembly was given the task of drawing up the draft treaty to establish a European Political Community. In this, the Ad Hoc Assembly was established on 13 September 1952 with extra members but after the failure of the proposed European Defence Community the project was dropped. 
Despite this the European Economic Community and Euratom were established in 1958 by the Treaties of Rome. The Common Assembly was shared by all three communities (which had separate executives) and it renamed itself the "European Parliamentary Assembly". The first meeting was held on 19 March 1958 having been set up in Luxembourg, it elected Schuman as its president and on 13 May it rearranged itself to sit according to political ideology rather than nationality. This is seen as the birth of the modern European Parliament, with Parliament's 50 years celebrations being held in March 2008 rather than 2002.
The three communities merged their remaining organs as the European Communities in 1967 and the body was renamed to the current "European Parliament" in 1962. In 1970 the Parliament was granted power over areas of the Community's budget, which were expanded to the whole budget in 1975. Under the Rome Treaties, the Parliament should have become elected. However the Council was required to agree a uniform voting system beforehand, which it failed to do. The Parliament threatened to take the Council to the European Court of Justice leading to a compromise whereby the Council would agree to elections, but the issue of voting systems would be put off till a later date.
Elected Parliament.
In 1979, its members were directly elected for the first time. This sets it apart from similar institutions such as those of the Parliamentary Assembly of the Council of Europe or Pan-African Parliament which are appointed. After that first election, the parliament held its first session on 11 July 1979, electing Simone Veil MEP as its President. Veil was also the first female President of the Parliament since it was formed as the Common Assembly.
As an elected body, the Parliament began to draft proposals addressing the functioning of the EU. For example in 1984, inspired by its previous work on the Political Community, it drafted the "draft Treaty establishing the European Union" (also known as the 'Spinelli Plan' after its rapporteur Altiero Spinelli MEP). Although it was not adopted, many ideas were later implemented by other treaties. Furthermore the Parliament began holding votes on proposed Commission Presidents from the 1980s, before it was given any formal right to veto.
Since the election the membership of the European Parliament has simply expanded whenever new nations have joined (the membership was also adjusted upwards in 1994 after German reunification). Following this the Treaty of Nice imposed a cap on the number of members to be elected, 732. 
Like the other institutions, the Parliament's seat was not yet fixed. The provisional arrangements placed Parliament in Strasbourg, while the Commission and Council had their seats in Brussels. In 1985 the Parliament, wishing to be closer to these institutions, built a second chamber in Brussels and moved some of its work there despite protests from some states. A final agreement was eventually reached by the European Council in 1992. It stated the Parliament would retain its formal seat in Strasbourg, where twelve sessions a year would be held, but with all other parliamentary activity in Brussels. This two seat arrangement was contested by Parliament but was later enshrined in the Treaty of Amsterdam. To this day the institution's locations are a source of contention.
The Parliament gained more powers from successive treaties, namely through the extension of the ordinary legislative procedure (then called the codecision procedure), and in 1999, the Parliament forced the resignation of the Santer Commission. The Parliament had refused to approve the Community budget over allegations of fraud and mis-management in the Commission. The two main parties took on a government-opposition dynamic for the first time during the crisis which ended in the Commission resigning en masse, the first of any forced resignation, in the face of an impending censure from the Parliament.
Barroso I.
In 2004, following the largest trans-national election in history, despite the European Council choosing a President from the largest political group (the EPP), the Parliament again exerted pressure on the Commission. During the Parliament's hearings of the proposed Commissioners MEPs raised doubts about some nominees with the Civil liberties committee rejecting Rocco Buttiglione from the post of Commissioner for Justice, Freedom and Security over his views on homosexuality. That was the first time the Parliament had ever voted against an incoming Commissioner and despite Barroso's insistence upon Buttiglione the Parliament forced Buttiglione to be withdrawn. A number of other Commissioners also had to be withdrawn or reassigned before Parliament allowed the Barroso Commission to take office.
Along with the extension of the ordinary legislative procedure, the Parliament's democratic mandate has given it greater control over legislation against the other institutions. In voting on the Bolkestein directive in 2006, the Parliament voted by a large majority for over 400 amendments that changed the fundamental principle of the law. The "Financial Times" described it in the following terms:
 That is where the European parliament has suddenly come into its own. It marks another shift in power between the three central EU institutions. Last week's vote suggests that the directly elected MEPs, in spite of their multitude of ideological, national and historical allegiances, have started to coalesce as a serious and effective EU institution, just as enlargement has greatly complicated negotiations inside both the Council and Commission.
In 2007, for the first time, Justice Commissioner Franco Frattini included Parliament in talks on the second Schengen Information System even though MEPs only needed to be consulted on parts of the package. After that experiment, Frattini indicated he would like to include Parliament in all justice and criminal matters, informally pre-empting the new powers they could gain as part of the Treaty of Lisbon. Between 2007 and 2009, a special working group on parliamentary reform implemented a series of changes to modernise the institution such as more speaking time for rapporteurs, increase committee co-operation and other efficiency reforms.
Recent history.
The Lisbon Treaty finally came into force on 1 December 2009, granting Parliament powers over the entire of the EU budget, making Parliament's legislative powers equal to the Council's in nearly all areas and linking the appointment of the Commission President to Parliament's own elections. Despite some calls for the parties to put forward candidates beforehand, only the EPP (which had re-secured their position as largest party) had one in re-endorsing Barroso.
Barroso gained the support of the European Council for a second term and secured majority support from the Parliament in September 2009. Parliament voted 382 votes in favour and 219 votes against (117 abstentions ) with support of the European People's Party, European Conservatives and Reformists and the Alliance of Liberals and Democrats for Europe. The liberals gave support after Barroso gave them a number of concessions; the liberals previously joined the socialists' call for a delayed vote (the EPP had wanted to approve Barroso in July of that year).
Once Barroso put forward the candidates for his next Commission, another opportunity to gain concessions arose. Bulgarian nominee Rumiana Jeleva was forced to step down by Parliament due to concerns over her experience and financial interests. She only had the support of the EPP which began to retaliate on left wing candidates before Jeleva gave in and was replaced (setting back the final vote further).
Before the final vote, Parliament demanded a number of concessions as part of a future working agreement under the new Lisbon Treaty. The deal includes that Parliament's President will attend high level Commission meetings. Parliament will have a seat in the EU's Commission-lead international negotiations and have a right to information on agreements. However Parliament secured only an observer seat. Parliament also did not secure a say over the appointment of delegation heads and special representatives for foreign policy. Although they will appear before parliament after they have been appointed by the High Representative. One major internal power was that Parliament wanted a pledge from the Commission that it would put forward legislation when parliament requests. Barroso considered this an infringement on the Commission's powers but did agree to respond within three months. Most requests are already responded to positively.
During the setting up of the European External Action Service (EEAS), Parliament used its control over the EU budget to influence the shape of the EEAS. MEPs had aimed at getting greater oversight over the EEAS by linking it to the Commission and having political deputies to the High Representative. MEPs didn't manage to get everything they demanded, however they got broader financial control over the new body.
Powers and functions.
The Parliament and Council can be regarded as two chambers in a bicameral legislative branch of the European Union, with law-making power being officially distributed equally between both parliamentary chambers. However there are some differences from national legislatures; for example, neither the Parliament nor the Council have the power of legislative initiative (except for the fact that the Council has the power in some intergovernmental matters). In Community matters, this is a power uniquely reserved for the European Commission (the executive). Therefore, while Parliament can amend and reject legislation, to make a proposal for legislation, it needs the Commission to draft a bill before anything can become law. The value of such a power has been questioned by noting that in the national legislatures of the member states 85% of initiatives introduced without executive support fail to become law. Yet it has been argued by former Parliament president Hans-Gert Pöttering that as the Parliament does have the right to ask the Commission to draft such legislation, and as the Commission is following Parliament's proposals more and more Parliament does have a "de facto" right of legislative initiative.
The Parliament also has a great deal of indirect influence, through non-binding resolutions and committee hearings, as a "pan-European soapbox" with the ear of thousands of Brussels-based journalists. There is also an indirect effect on foreign policy; the Parliament must approve all development grants, including those overseas. For example, the support for post-war Iraq reconstruction, or incentives for the cessation of Iranian nuclear development, must be supported by the Parliament. Parliamentary support was also required for the transatlantic passenger data-sharing deal with the United States. Finally, Parliament holds a non-binding vote on new EU treaties but cannot veto it. However when Parliament threatened to vote down the Nice Treaty, the Belgian and Italian Parliaments said they would veto the treaty on the European Parliament's behalf.
Legislative procedure.
With each new treaty, the powers of the Parliament, in terms of its role in the Union's legislative procedures, have expanded. The procedure which has slowly become dominant is the "ordinary legislative procedure" (previously named "codecision procedure"), which provides an equal footing between Parliament and Council. In particular, under the procedure, the Commission presents a proposal to Parliament and the Council which can only become law if both agree on a text, which they do (or not) through successive readings up to a maximum of three. In its first reading, Parliament may send amendments to the Council which can either adopt the text with those amendments or send back a "common position". That position may either be approved by Parliament, or it may reject the text by an absolute majority, causing it to fail, or it may adopt further amendments, also by an absolute majority. If the Council does not approve these, then a "Conciliation Committee" is formed. The Committee is composed of the Council members plus an equal number of MEPs who seek to agree a compromise. Once a position is agreed, it has to be approved by Parliament, by a simple majority. This is also aided by Parliament's mandate as the only directly democratic institution, which has given it leeway to have greater control over legislation than other institutions, for example over its changes to the Bolkestein directive in 2006.
The few other areas that operate the "special legislative procedures" are justice & home affairs, budget and taxation and certain aspects of other policy areas: such as the fiscal aspects of environmental policy. In these areas, the Council or Parliament decide law alone. The procedure also depends upon which type of institutional act is being used. The strongest act is a regulation, an act or law which is directly applicable in its entirety. Then there are directives which bind member states to certain goals which they must achieve. They do this through their own laws and hence have room to manoeuvre in deciding upon them. A decision is an instrument which is focused at a particular person or group and is directly applicable. Institutions may also issue recommendations and opinions which are merely non-binding, declarations. There is a further document which does not follow normal procedures, this is a "written declaration" which is similar to an early day motion used in the Westminster system. It is a document proposed by up to five MEPs on a matter within the EU's activities used to launch a debate on that subject. Having been posted outside the entrance to the hemicycle, members can sign the declaration and if a majority do so it is forwarded to the President and announced to the plenary before being forwarded to the other institutions and formally noted in the minutes.
Budget.
The legislative branch officially holds the Union's budgetary authority with powers gained through the Budgetary Treaties of the 1970s and the Lisbon Treaty. The EU budget is subject to a form of the ordinary legislative procedure with a single reading giving Parliament power over the entire budget (before 2009, its influence was limited to certain areas) on an equal footing to the Council. If there is a disagreement between them, it is taken to a conciliation committee as it is for legislative proposals. If the joint conciliation text is not approved, the Parliament may adopt the budget definitively.
The Parliament is also responsible for discharging the implementation of previous budgets based on the annual report of the European Court of Auditors. It has refused to approve the budget only twice, in 1984 and in 1998. On the latter occasion it led to the resignation of the Santer Commission; highlighting how the budgetary power gives Parliament a great deal of power over the Commission. Parliament also makes extensive use of its budgetary, and other powers, elsewhere; for example in the setting up of the European External Action Service, Parliament has a de facto veto over its design as it has to approve the budgetary and staff changes.
Control of the executive.
Unlike most EU states, which usually operate parliamentary systems, there is a separation of powers between the executive and legislative which makes the European Parliament more akin to the United States Congress than an EU state legislature. The President of the European Commission is proposed by the European Council on the basis of the European elections to Parliament. That proposal has to be approved by the Parliament (by a simple majority) who "elect" the President according to the treaties. Following the approval of the Commission President, the members of the Commission are proposed by the President in accord with the member-states. Each Commissioner comes before a relevant parliamentary committee hearing covering the proposed portfolio. They are then, as a body, approved or rejected by the Parliament. In practice, the Parliament has never voted against a President or his Commission, but it did seem likely when the Barroso Commission was put forward. The resulting pressure forced the proposal to be withdrawn and changed to be more acceptable to parliament. That pressure was seen as an important sign by some of the evolving nature of the Parliament and its ability to make the Commission accountable, rather than being a rubber stamp for candidates. Furthermore, in voting on the Commission, MEPs also voted along party lines, rather than national lines, despite frequent pressure from national governments on their MEPs. This cohesion and willingness to use the Parliament's power ensured greater attention from national leaders, other institutions and the public—who previously gave the lowest ever turnout for the Parliament's elections.
The Parliament also has the power to censure the Commission if they have a two-thirds majority which will force the resignation of the entire Commission from office. As with approval, this power has never been used but it was threatened to the Santer Commission, who subsequently resigned of their own accord. There are a few other controls, such as: the requirement of Commission to submit reports to the Parliament and answer questions from MEPs; the requirement of the President-in-office of the Council to present its programme at the start of their presidency; the obligation on the President of the European Council to report to Parliament after each of its meetings; the right of MEPs to make requests for legislation and policy to the Commission; and the right to question members of those institutions (e.g. "Commission Question Time" every Tuesday). At present, MEPs may ask a question on any topic whatsoever, but in July 2008 MEPs voted to limit questions to those within the EU's mandate and ban offensive or personal questions.
Supervisory powers.
The Parliament also has other powers of general supervision, mainly granted by the Maastricht Treaty. The Parliament has the power to set up a Committee of Inquiry, for example over mad cow disease or CIA detention flights—the former led to the creation of the European veterinary agency. The Parliament can call other institutions to answer questions and if necessary to take them to court if they break EU law or treaties. Furthermore it has powers over the appointment of the members of the Court of Auditors and the president and executive board of the European Central Bank. The ECB president is also obliged to present an annual report to the parliament.
The European Ombudsman is elected by the Parliament, who deals with public complaints against all institutions. Petitions can also be brought forward by any EU citizen on a matter within the EU's sphere of activities. The Committee on Petitions hears cases, some 1500 each year, sometimes presented by the citizen themselves at the Parliament. While the Parliament attempts to resolve the issue as a mediator they do resort to legal proceedings if it is necessary to resolve the citizens dispute.
Members.
The parliamentarians are known in English as Members of the European Parliament (MEPs). They are elected every five years by universal adult suffrage and sit according to political allegiance; about a third are women. Before 1979 they were appointed by their national parliaments.
Under the Lisbon Treaty, seats are allocated to each state according to population and the maximum number of members is set at 751 (however, as the President cannot vote while in the chair there will only be 750 voting members at any one time).
The seats are distributed according to "degressive proportionality", i.e., the larger the state, the more citizens are represented per MEP. As a result, Maltese and Luxembourgish voters have roughly 10x more influence per voter than citizens of the six large countries.
s of 2014[ [update]], Germany (80.9 million inhabitants) has 96 seats (previously 99 seats), i.e. one seat for 843,000 inhabitants. Malta (0.4 million inhabitants) has 6 seats, i.e. one seat for 70,000 inhabitants.
The new system implemented under the Lisbon Treaty, including revising the seating well before elections, was intended to avoid political horse trading when the allocations have to be revised to reflect demographic changes.
Pursuant to this apportionment, the constituencies are formed. In six EU member states (Belgium, France, Ireland, Italy, Poland, and the United Kingdom), the national territory is divided into a number of constituencies. In the remaining member states, the whole country forms a single constituency. All member states hold elections to the European Parliament using various forms of proportional representation.
Transitional arrangements.
Due to the delay in ratifying the Lisbon Treaty, the seventh parliament was elected under the lower Nice Treaty cap. A small scale treaty amendment was ratified on 29 November 2011. This amendment brought in transitional provisions to allow the 18 additional MEPs created under the Lisbon Treaty to be elected or appointed before the 2014 election. Under the Lisbon Treaty reforms, Germany was the only state to lose members from 99 to 96. However, these seats were not removed until the 2014 election.
Salaries and expenses.
Before 2009, members received the same salary as members of their national parliament. However from 2009 a new members statute came into force, after years of attempts, which gave all members an equal monthly pay, of 8,020.53 euro each in 2014, subject to a European Union tax and which can also be taxed nationally. MEPs are entitled to a pension, paid by Parliament, from the age of 63. Members are also entitled to allowances for office costs and subsistence, and travelling expenses, based on actual cost. Besides their pay, members are granted a number of privileges and immunities. To ensure their free movement to and from the Parliament, they are accorded by their own states the facilities accorded to senior officials travelling abroad and, by other state governments, the status of visiting foreign representatives. When in their own state, they have all the immunities accorded to national parliamentarians, and, in other states, they have immunity from detention and legal proceedings. However, immunity cannot be claimed when a member is found committing a criminal offence and the Parliament also has the right to strip a member of their immunity.
Political groups.
MEPs in Parliament are organised into seven different parliamentary groups, including thirty non-attached members known as "non-inscrits". The two largest groups are the European People's Party (EPP) and the Socialists & Democrats (S&D). These two groups have dominated the Parliament for much of its life, continuously holding between 50 and 70 percent of the seats between them. No single group has ever held a majority in Parliament. As a result of being broad alliances of national parties, European group parties are very decentralised and hence have more in common with parties in federal states like Germany or the United States than unitary states like the majority of the EU states. Nevertheless, the European groups were actually more cohesive than their US counterparts between 2004 and 2009.
Groups are often based on a single European political party such as the socialist group (before 2009). However, they can, like the liberal group, include more than one European party as well as national parties and independents. For a group to be recognised, it needs 25 MEPs from seven different countries. Once recognised, groups receive financial subsidies from the parliament and guaranteed seats on committees, creating an incentive for the formation of groups. However, some controversy occurred with the establishment of the short-lived Identity, Tradition, Sovereignty (ITS) due to its ideology; the members of the group were far-right, so there were concerns about public funds going towards such a group. There were attempts to change the rules to block the formation of ITS, but they never came to fruition. The group was, however, blocked from gaining leading positions on committees — traditionally (by agreement, not a rule) shared among all parties. When this group engaged in infighting, leading to the withdrawal of some members, its size fell below the threshold for recognition causing its collapse.
Grand coalition.
Given that the Parliament does not form the government in the traditional sense of a Parliamentary system, its politics have developed along more consensual lines rather than majority rule of competing parties and coalitions. Indeed for much of its life it has been dominated by a grand coalition of the European People's Party and the Party of European Socialists. The two major parties tend to co-operate to find a compromise between their two groups leading to proposals endorsed by huge majorities. However, this does not always produce agreement, and each may instead try to build other alliances, the EPP normally with other centre-right or right wing Groups and the PES with centre-left or left wing Groups. Sometimes, the Liberal Group is then in the pivotal position. There are also occasions where very sharp party political divisions have emerged, for example over the resignation of the Santer Commission.
When the initial allegations against the Commission emerged, they were directed primarily against Édith Cresson and Manuel Marín, both socialist members. When the parliament was considering refusing to discharge the Community budget, President Jacques Santer stated that a no vote would be tantamount to a vote of no confidence. The Socialist group supported the Commission and saw the issue as an attempt by the EPP to discredit their party ahead of the 1999 elections. Socialist leader, Pauline Green MEP, attempted a vote of confidence and the EPP put forward counter motions. During this period the two parties took on similar roles to a government-opposition dynamic, with the Socialists supporting the executive and EPP renouncing its previous coalition support and voting it down. Politicisation such as this has been increasing, in 2007 Simon Hix of the London School of Economics noted that:
Our work also shows that politics in the European Parliament is becoming increasingly based around party and ideology. Voting is increasingly split along left-right lines, and the cohesion of the party groups has risen dramatically, particularly in the fourth and fifth parliaments. So there are likely to be policy implications here too.
During the fifth term, 1999 to 2004, there was a break in the grand coalition resulting in a centre-right coalition between the Liberal and People's parties. This was reflected in the Presidency of the Parliament with the terms being shared between the EPP and the ELDR, rather than the EPP and Socialists. In the following term the liberal group grew to hold 88 seats, the largest number of seats held by any third party in Parliament.
Elections.
Elections have taken place, directly in every member-state, every five years since 1979. s of 2014[ [update]] there have been eight elections. When a nation joins mid-term, a by-election will be held to elect their representatives. This has happened six times, most recently when Croatia joined in 2013 (see below). Elections take place across four days according to local custom and, apart from having to be proportional, the electoral system is chosen by the member-state. This includes allocation of sub-national constituencies; while most members have a national list, some, like the UK and France, divide their allocation between regions. Seats are allocated to member-states according to their population, with no state having more than 99, but no fewer than 5, to maintain proportionality.
The most recent Union-wide elections to the European Parliament were the European elections of 2014, held from 22 to 25 May 2014. They were the largest simultaneous transnational elections ever held anywhere in the world.
The eighth term of Parliament started on 1 July 2014.
The proportion of MEPs elected in 2009 who were female was 35%; in 1979 it was just 16.5%.
There have been a number of proposals designed to attract greater public attention to the elections. One such innovation in the 2014 elections was that the pan-European political parties fielded "candidates" for president of the Commission. However, European Union governance is based on a mixture of intergovernmental and supranational features: the President of the European Commission is nominated by the European Council, representing the governments of the member states, and there is no obligation for them to nominate the successful "candidate". The Lisbon Treaty merely states that they should take account of the results of the elections when choosing whom to nominate. The so-called "candidates" were Jean-Claude Juncker for the European People's Party, Martin Schulz for the Party of European Socialists, Guy Verhofstadt for the Alliance of Liberals and Democrats for Europe Party, Ska Keller and José Bové jointly for the European Green Party and Alexis Tsipras for the Party of the European Left.
Turnout has dropped consistently every year since the first election, and from 1999 it has been below 50%. In 2007 both Bulgaria and Romania elected their MEPs in by-elections, having joined at the beginning of 2007. The Bulgarian and Romanian elections saw two of the lowest turnouts for European elections, just 28.6% and 28.3% respectively.
In England, Scotland and Wales, EP elections were originally held for a constituency MEP on a first-past-the-post basis. In 1999 the system was changed to a form of PR where a large group of candidates would stand for a post within a very large regional constituency. One could vote for a party, but not a candidate (unless that party had a single candidate).
Proceedings.
Each year the activities of the Parliament cycle between committee weeks where reports are discussed in committees and interparliamentary delegations meet, political group weeks for members to discuss work within their political groups and session weeks where members spend 3½ days in Strasbourg for part-sessions. In addition six 2-day part-sessions are organised in Brussels throughout the year. Four weeks are allocated as constituency week to allow members to do exclusively constituency work. Finally there are no meetings planned during the summer weeks. The Parliament has the power to meet without being convened by another authority. Its meetings are partly controlled by the treaties but are otherwise up to Parliament according to its own "Rules of Procedure" (the regulations governing the parliament).
During sessions, members may speak after being called on by the President. Members of the Council or Commission may also attend and speak in debates. Partly due to the need for translation, and the politics of consensus in the chamber, debates tend to be calmer and more polite than, say, the Westminster system. Voting is conducted primarily by a show of hands, that may be checked on request by electronic voting. Votes of MEPs are not recorded in either case however, that only occurs when there is a roll-call ballot. This is required for the final votes on legislation and also whenever a political group or 30 MEPs request it. The number of roll-call votes has increased with time. Votes can also be a completely secret ballot (for example when the President is elected). All recorded votes, along with minutes and legislation, are recorded in the Official Journal of the European Union and can be accessed online. Votes usually do not follow a debate, but rather they are grouped with other due votes on specific occasions, usually at noon on Tuesdays, Wednesdays or Thursdays. This is because the length of the vote is unpredictable and if it continues for longer than allocated it can disrupt other debates and meetings later in the day.
Members are arranged in a hemicycle according to their political groups (in the Common Assembly, prior to 1958, members sat alphabetically) who are ordered mainly by left to right, but some smaller groups are placed towards the outer ring of the Parliament. All desks are equipped with microphones, headphones for translation and electronic voting equipment. The leaders of the groups sit on the front benches at the centre, and in the very centre is a podium for guest speakers. The remaining half of the circular chamber is primarily composed of the raised area where the President and staff sit. Further benches are provided between the sides of this area and the MEPs, these are taken up by the Council on the far left and the Commission on the far right. Both the Brussels and Strasbourg hemicycle roughly follow this layout with only minor differences. The hemicycle design is a compromise between the different Parliamentary systems. The British-based system has the different groups directly facing each other while the French-based system is a semicircle (and the traditional German system had all members in rows facing a rostrum for speeches). Although the design is mainly based on a semicircle, the opposite ends of the spectrum do still face each other. With access to the chamber limited, entrance is controlled by ushers who aid MEPs in the chamber (for example in delivering documents). The ushers can also occasionally act as a form of police in enforcing the President, for example in ejecting an MEP who is disrupting the session (although this is rare). The first head of protocol in the Parliament was French, so many of the duties in the Parliament are based on the French model first developed following the French Revolution. The 180 ushers are highly visible in the Parliament, dressed in black tails and wearing a silver chain, and are recruited in the same manner as the European civil service. The President is allocated a personal usher.
President and organisation.
The President is essentially the speaker of the Parliament and presides over the plenary when it is in session. The President's signature is required for all acts adopted by co-decision, including the EU budget. The President is also responsible for representing the Parliament externally, including in legal matters, and for the application of the rules of procedure. He or she is elected for two-and-a-half-year terms, meaning two elections per parliamentary term. The President is currently Martin Schulz MEP of the S&D.
In most countries, the protocol of the head of state comes before all others, however in the EU the Parliament is listed as the first institution, and hence the protocol of its President comes before any other European, or national, protocol. The gifts given to numerous visiting dignitaries depend upon the President. President Josep Borrell MEP of Spain gave his counterparts a crystal cup created by an artist from Barcelona who had engraved upon it parts of the Charter of Fundamental Rights among other things.
A number of notable figures have been President of the Parliament and its predecessors. The first President was Paul-Henri Spaak MEP, one of the founding fathers of the Union. Other founding fathers include Alcide de Gasperi MEP and Robert Schuman MEP. The two female Presidents were Simone Veil MEP in 1979 (first President of the elected Parliament) and Nicole Fontaine MEP in 1999, both Frenchwomen. The previous president, Jerzy Buzek was the first East-Central European to lead an EU institution, a former Prime Minister of Poland who rose out of the Solidarity movement in Poland that helped overthrow communism in the Eastern Bloc.
During the election of a President, the previous President (or, if unable to, one of the previous Vice-Presidents) presides over the chamber. Prior to 2009, the oldest member fulfilled this role but the rule was changed to prevent far-right French MEP Jean-Marie Le Pen taking the chair.
Below the President, there are 14 Vice-Presidents who chair debates when the President is not in the chamber. There are a number of other bodies and posts responsible for the running of parliament besides these speakers. The two main bodies are the Bureau, which is responsible for budgetary and administration issues, and the Conference of Presidents which is a governing body composed of the presidents of each of the parliament's political groups. Looking after the financial and administrative interests of members are five Quaestors. In August 2002, Nichole Robichaux [nee Braucksieker] became the first American citizen to intern for a member of the European Parliament—Monica Frassoni [Green Party].
s of 2014[ [update]], the European Parliament budget was EUR 1.756 billion. A 2008 report on the Parliament's finances highlighted certain overspending and miss-payments. Despite some MEPs calling for the report to be published, Parliamentary authorities had refused until an MEP broke confidentiality and leaked it.
Committees and delegations.
The Parliament has 20 Standing Committees consisting of 28 to 86 MEPs each (reflecting the political makeup of the whole Parliament) including a Chairman, a bureau and secretariat. They meet twice a month in public to draw up, amend to adopt legislative proposals and reports to be presented to the plenary. The rapporteurs for a committee are supposed to present the view of the committee, although notably this has not always been the case. In the events leading to the resignation of the Santer Commission, the rapporteur went against the Budgetary Control Committee's narrow vote to discharge the budget, and urged the Parliament to reject it.
Committees can also set up sub-committees (e.g. the Subcommittee on Human Rights) and temporary committees to deal with a specific topic (e.g. on extraordinary rendition). The chairs of the Committees co-ordinate their work through the "Conference of Committee Chairmen". When co-decision was introduced it increased the Parliaments powers in a number of areas, but most notably those covered by the Committee on the Environment, Public Health and Food Safety. Previously this committee was considered by MEPs as a "Cinderella committee", however as it gained a new importance, it became more professional and rigorous attracting increasing attention to its work.
The nature of the committees differ from their national counterparts as, although smaller in comparison to those of the United States Congress, the European Parliament's committees are unusually large by European standards with between eight and twelve dedicated members of staff and three to four support staff. Considerable administration, archives and research resources are also at the disposal of the whole Parliament when needed.
Delegations of the Parliament are formed in a similar manner and are responsible for relations with Parliaments outside the EU. There are 34 delegations made up of around 15 MEPs, chairpersons of the delegations also cooperate in a conference like the committee chairs do. They include "Interparliamentary delegations" (maintain relations with Parliament outside the EU), "joint parliamentary committees" (maintaining relations with parliaments of states which are candidates or associates of the EU), the delegation to the ACP EU Joint Parliamentary Assembly and the delegation to the Euro-Mediterranean Parliamentary Assembly. MEPs also participate in other international activities such as the Euro-Latin American Parliamentary Assembly, the Transatlantic Legislators' Dialogue and through election observation in third countries.
Translation and interpretation.
Speakers in the European Parliament are entitled to speak in any of the EU's 24 official languages, ranging from English and German to Maltese and Irish. Simultaneous interpreting is offered in all plenary sessions, and all final texts of legislation are translated. With twenty-four languages, the European Parliament is the most multilingual parliament in the world and the biggest employer of interpreters in the world (employing 350 full-time and 400 free-lancers when there is higher demand). Citizens may also address the Parliament in Basque, Catalan/Valencian and Galician.
Usually a language is translated from a foreign tongue into a translator's native tongue. Due to the large number of languages, some being minor ones, since 1995 interpreting is sometimes done the opposite way, out of an interpreter's native tongue (the "retour" system). In addition, a speech in a minor language may be interpreted through a third language for lack of interpreters ("relay" interpreting) —for example, when interpreting out of Estonian into Maltese. Interpreters need to be proficient in two other Union languages besides their native language. Due to the complexity of the issues, interpretation is not word for word. Instead, interpreters have to convey the political meaning of a speech, regardless of their own views. This requires detailed understanding of the politics and terms of the Parliament, involving a great deal of preparation beforehand (e.g. reading the documents in question). Difficulty can often arise when MEPs use profanities, jokes and word play or speak too fast.
While some see speaking their native language as an important part of their identity, and can speak more fluently in debates, interpretation and its cost has been criticised by some. A 2006 report by Alexander Stubb MEP highlighted that by only using English, French and German costs could be reduced from €118,000 per day (for 21 languages then—Romanian, Bulgarian and Croatian having not yet been included) to €8,900 per day. Many see the ideal single language as being English due to its widespread usage, although there has been a small-scale campaign to make French the reference language for all legal texts, due to the claim that it is more clear and precise for legal purposes.
Because the proceedings are translated into all of the official EU languages, they have been used to make a multilingual corpus known as Europarl. It is widely used to train statistical machine translation systems.
Seat.
The Parliament is based in three different cities with numerous buildings. A protocol attached to the Treaty of Amsterdam requires that 12 plenary sessions be held in Strasbourg (none in August but two in September), which is the Parliament's official seat, while extra part sessions as well as committee meetings are held in Brussels. Luxembourg hosts the Secretariat of the European Parliament. The European Parliament is the only assembly in the world with more than one meeting place and one of the few that does not have the power to decide its own location.
The Strasbourg seat is seen as a symbol of reconciliation between France and Germany, the Strasbourg region having been fought over by the two countries in the past. However, it is questioned over the cost and inconvenience of having two seats for the parliament. While Strasbourg is the official seat, and sits alongside the Council of Europe (with which the "mutual cooperation" is being continuously "fostered"), Brussels is home to nearly all other major EU institutions, with the majority of Parliament's work already being carried out there. Therefore despite Strasbourg being the main seat, it is the one most questioned, although some do believe Strasbourg should be the single capital.
Critics have described the two-seat arrangement as a "travelling circus", and there is a strong movement to establish Brussels as the sole seat. This is because the other political institutions (the Commission, Council and European Council) are located there, and hence Brussels is treated as the 'capital' of the EU. This movement has received strong backing through numerous figures, including the Commission First-Vice President who stated that "something that was once a very positive symbol of the EU reuniting France and Germany has now become a negative symbol—of wasting money, bureaucracy and the insanity of the Brussels institutions". The Green party has also noted the environmental cost in a study led by Jean Lambert MEP and Caroline Lucas MEP; in addition to the extra 200 million euro spent on the extra seat, there are over 20,268 tonnes of additional carbon dioxide, undermining any environmental stance of the institution and the Union. The campaign is further backed by a million-strong online petition started by Cecilia Malmström MEP. In August 2014, an assessment by the European Court of Auditors calculated that relocating the Strasbourg seat of the European Parliament to Brussels would save €113.8 million per year. In 2006, there were allegations of irregularity in the charges made by the city of Strasbourg on buildings the Parliament rented, thus further harming the case for the Strasbourg seat.
A poll of MEPs also found 89% of the respondents wanting a single seat, and 81% preferring Brussels. Another, more academic, survey found 68% support. In July 2011, an absolute majority of MEPs voted in favour of a single seat. In early 2011, the Parliament voted to scrap one of the Strasbourg sessions by holding two within a single week. The mayor of Strasbourg officially reacted by stating "we will counter-attack by upturning the adversary's strength to our own profit, as a judoka would do." However, as Parliament's seat is now fixed by the treaties, it can only be changed by the Council acting unanimously, meaning that France could veto any move. The former French President Nicolas Sarkozy has stated that the Strasbourg seat is "non-negotiable", and that France has no intention of surrendering the only EU Institution on French soil. Given France's declared intention to veto any relocation to Brussels, some MEPs have advocated civil disobedience by refusing to take part in the monthly exodus to Strasbourg.
Further reading.
</dl>

</doc>
<doc id="9582" url="http://en.wikipedia.org/wiki?curid=9582" title="European Council">
European Council

The European Council is the Institution of the European Union (EU) that comprises the heads of state or government of the member states, along with the council's own president and the president of the Commission. The High Representative of the Union for Foreign Affairs and Security Policy also takes part in its meetings. Established as an informal summit in 1975, the council was formalised as an Institution in 2009 upon the entry into force of the Treaty of Lisbon. The current president of the European Council is Donald Tusk.
Scope.
While the European Council has no formal legislative power, it is a strategic (and crisis-solving) body that provides the union with general political directions and priorities, and acts as a collective presidency. The European Commission remains the sole initiator of legislation, but the European Council is able to provide an impetus to guide legislative policy.
The meetings of the European Council, still commonly referred to as EU summits, are chaired by its president and take place at least twice every six months; usually in the Justus Lipsius building, the headquarters of the Council of the European Union in Brussels. Decisions of the European Council are taken by a simple majority consensus, except where the Treaties provide otherwise.
History.
The first summits of EU heads of state or government were held in February and July 1961 (in Paris and Bonn respectively). They were informal summits of the leaders of the European Community and were started due to then-French President Charles de Gaulle's resentment at the domination of supranational institutions (e.g. the European Commission) over the integration process, but petered out. The first influential summit held, after the departure of De Gaulle, was The Hague summit of 1969, which reached an agreement on the admittance of the United Kingdom into the Community and initiated foreign policy cooperation (the European Political Cooperation) taking integration beyond economics.
The summits were only formalised in the period between 1974 and 1988. At the December summit in Paris in 1974, following a proposal from then-French president Valéry Giscard d'Estaing, it was agreed that more high level, political input was needed following the "empty chair crisis" and economic problems. The inaugural "European Council", as it became known, was held in Dublin on 10 and 11 March 1975 during Ireland's first Presidency of the Council of Ministers. In 1987, it was included in the treaties for the first time (the Single European Act) and had a defined role for the first time in the Maastricht Treaty. At first only a minimum of two meetings per year were required, which resulted in an average of three meetings per year being held for the 1975-1995 period. Since 1996, the number of meetings were required to be minimum four per year. For the latest 2008-2014 period, this minimum was well exceeded, by an average of seven meetings being held per year. The seat of the Council was formalised in 2002, basing it in Brussels. Three types of European Councils exist: Informal, Scheduled and Extraordinary. While the informal meetings are also scheduled 1½ year in advance, they differ from the scheduled ordinary meetings by not ending with official "Council conclusions", as they instead end by more broad political "Statements" on some cherry picked policy matters. The extraordinary meetings always end with official "Council conclusions" - but differs from the scheduled meetings by not being scheduled more than a year in advance, as for example in 2001 when the European Council gathered to lead the EU's response to the 11 September attacks.
Some meetings of the European Council are seen by some as turning points in the history of the European Union. For example:
As such, the European Council had already existed before it gained the status as an institution of the European Union with the entering into force of the Treaty of Lisbon. Indeed, Article 214(2) of the Treaty establishing the European Community provided (before it was amended by the Treaty of Lisbon) that ‘the Council, meeting "in the composition of Heads of State or Government" and acting by a qualified majority, shall nominate the person it intends to appoint as President of the Commission’ (emphasis added); this may be seen as an early codification of the European Council in the Treaties. In the event, (amended by the Treaty of Lisbon) officially introduces the term "European Council" as a substitute for the phrase "Council [of the European Union] meeting in the composition of the Heads of State or Government", which was previously sometimes used in the treaties to refer to this body.
The Treaty of Lisbon made the European Council a formal institution distinct from the (ordinary) Council of the EU, and created the present longer term and full-time presidency. As an outgrowth of the Council of the EU, the European Council had previously followed the same Presidency, rotating between each member state. While the Council of the EU retains that system, the European Council established, with no change in powers, a system of appointing an individual (without them being a national leader) for a two-and-a-half-year term - which can be renewed for the same person only once. Following the ratification of the treaty in December 2009, the European Council elected the then-Prime Minister of Belgium Herman Van Rompuy as its first permanent president (resigning from Belgian Prime Minister).
Powers and functions.
The European Council is an official institution of the EU, mentioned by the Lisbon Treaty as a body which "shall provide the Union with the necessary impetus for its development". Essentially it defines the EU's policy agenda and has thus been considered to be the motor of European integration. It does this without any formal powers, only the influence it has being composed of national leaders. Beyond the need to provide "impetus", the Council has developed further roles; to "settle issues outstanding from discussions at a lower level", to lead in foreign policy — acting externally as a "collective Head of State", "formal ratification of important documents" and "involvement in the negotiation of the treaty changes".
Since the institution is composed of national leaders, it gathers the executive power of the member states and has thus a great influence in high profile policy areas as for example foreign policy. It also exercises powers of appointment, such as appointment of its own President, the High Representative of the Union for Foreign Affairs and Security Policy, and the President of the European Central Bank. It proposes, to the European Parliament, a candidate for President of the European Commission. Moreover, the European Council influences police and justice planning, the composition of the Commission, matters relating to the organisation of the rotating Council presidency, the suspension of membership rights, and changing the voting systems through the Passerelle Clause. Although the European Council has no direct legislative power, under the "emergency brake" procedure, a state outvoted in the Council of Ministers may refer contentious legislation to the European Council. However, the state may still be outvoted in the European Council. Hence with powers over the supranational executive of the EU, in addition to its other powers, the European Council has been described by some as the Union's "supreme political authority".
Composition.
The European Council consists of the heads of state or government of the member states, alongside its own President and the Commission President (non-voting). The meetings used to be regularly attended by the national foreign minister as well, and the Commission President likewise accompanied by another member of the Commission. However, since the Treaty of Lisbon, this has been discontinued, as the size of the body had become somewhat large following successive accessions of new Member States to the Union.
Meetings can also include other invitees, such as the President of the European Central Bank, as required. The Secretary-General of the Council attends, and is responsible for organisational matters, including minutes. The President of the European Parliament also attends to give an opening speech outlining the European Parliament's position before talks begin.
Additionally, the negotiations involve a large number of other people working behind the scenes. Most of those people, however, are not allowed to the conference room, except for two delegates per state to relay messages. At the push of a button members can also call for advice from a Permanent Representative via the "Antici Group" in an adjacent room. The group is composed of diplomats and assistants who convey information and requests. Interpreters are also required for meetings as members are permitted to speak in their own languages.
As the composition is not precisely defined, some states which have a considerable division of executive power can find it difficult to decide who should attend the meetings. While an MEP, Alexander Stubb argued that there was no need for the President of Finland to attend Council meetings with or instead of the Prime Minister of Finland (who was head of European foreign policy). In 2008, having become Finnish Foreign Minister, Stubb was forced out of the Finnish delegation to the emergency council meeting on the Georgian crisis because the President wanted to attend the high profile summit as well as the Prime Minister (only two people from each country could attend the meetings). This was despite Stubb being head of the Organisation for Security and Co-operation in Europe at the time which was heavily involved in the crisis. Problems also occurred in Poland where the President of Poland and the Prime Minister of Poland were of different parties and had a different foreign policy response to the crisis. A similar situation arose in Romania between President Traian Băsescu and Prime Minister Călin Popescu-Tăriceanu in 2007–2008 and again in 2012 with Prime Minister Victor Ponta, who both opposed the president.
Eurozone summits.
A number of ad hoc meetings of Heads of State or Government of the Euro area countries were held in 2010 and 2011 to discuss the Sovereign Debt crisis. It was agreed in October 2011 that they should meet regularly twice a year (with extra meetings if needed). This will normally be at the end of a European Council meeting and according to the same format (chaired by the President of the European Council and including the President of the Commission), but usually restricted to the (currently 17) Heads of State or Government of countries whose currency is the euro.
President.
The President of the European Council is elected by the European Council by a qualified majority for a once-renewable term of two and a half years. The President must report to the European Parliament after each European Council meeting.
The post was created by the Treaty of Lisbon and was subject to a debate over its exact role. Prior to Lisbon, the Presidency rotated in accordance with the Presidency of the Council of the European Union. The role of that President-in-Office was in no sense (other than protocol) equivalent to an office of a head of state, merely a "primus inter pares" (first among equals) role among other European heads of government. The President-in-Office was primarily responsible for preparing and chairing the Council meetings, and had no executive powers other than the task of representing the Union externally. Now the leader of the Council Presidency country can still act as president when the permanent president is absent.
Political parties.
Almost all members of the European Council are members of a political party at national level, and most of these are members of a European-level political party. These frequently hold pre-meetings of their European Council members, prior to its meetings. However, the European Council is composed to represent the EU's states rather than political parties and decisions are generally made on these lines, though ideological alignment can colour their political agreements and their choice of appointments (such as their president).
The table below outlines the number of leaders affiliated to each party and their total voting weight. The map to the right indicates the alignment of each individual country.
Seat and meetings.
Meetings of the European Council usually take place four times a year in Brussels. Meetings traditionally last for two days, sometimes even longer when contentious issues were on the agenda. However, former President Van Rompuy preferred to keep the summit to a single day. Until 2002, the venue of the council meeting rotated between member states, as its location was decided by the country holding the rotating presidency. However, the 22nd declaration attached to the Treaty of Nice stated that; "As from 2002, one European Council meeting per Presidency will be held in Brussels. When the Union comprises 18 members, all European Council meetings will be held in Brussels."
Between 2002 and 2004, half the councils were held in Brussels and, after the 2004 enlargement, all were. The European Council uses the same building as the Council of the European Union, i.e., the Justus Lipsius building. However, some extraordinary councils have taken place in the member state holding the Presidency, e.g., 2003 in Rome or 2005 in Hampton Court Palace. A new building (the "Europa building") is currently being built at the northern end of the adjacent historical Résidence Palace complex for use as a purpose built summit building by the European Council and the Council. It is due to be completed in 2013.
The choice of a single seat was due to a number of factors, mostly logistical (organising the meetings became ever more onerous with the enlargement of the EU, especially for smaller countries) and security (the experience of the Belgian police in dealing with protesters (a protester in Gothenburg was shot by police)) as well as Brussels having fixed facilities for the Council and journalists at every meeting. Having a permanent seat in Brussels also emphasised that the European Council is an EU institution rather than a summit of sovereign States in the manner of the G20. Some have argued it is the "de facto" EU government, while others underline that it is the Commission that is the EU's day-to-day government and the European Council can best be compared to a collective head of state.
In 2007, the new situation for locating meetings became a source of contention with the Portuguese government wanting to sign the Lisbon Treaty in Lisbon, Portugal. The Belgian government, however, was keen not to set a precedent and insisted that the regular end of year summit took place in Brussels as usual. This meant that after the signing, photo suit, and formal dinner, the attendees of the summit were transferred from Lisbon to Brussels. Mirrored with the "travelling circus" of the European Parliament, this garnered protests from environmental groups describing the hypocrisy of demanding lower carbon emissions while flying across Europe for the same summit for political reasons.
There are no current plans to hold meetings outside of Brussels, except for "force majeure" (for instance a strike by air traffic controllers nearly caused the January 2012 informal meeting to be switched to Luxembourg).
President's cabinet.
Although the European Council is, under the terms of the Lisbon treaty, a separate institution of the EU, it does not have its own administration. The administrative support for both the European Council and its president is provided by the General Secretariat of the Council of the European Union. The president does have, however, his own private office ("cabinet") of close advisers. Van Rompuy chose as his chief of staff ("chef de cabinet") Baron Frans van Daele, formerly Belgian ambassador to, variously, the USA, the UN, the EU and NATO and chief of staff of several Belgian foreign ministers. Also in his team are the former UK Labour MEP Richard Corbett, former Hungarian Ambassador to NATO Zoltán Martinusz, former head of the EU's economic & financial committee Odile Renaud-Basso, and Van Rompuy's long standing press officer Dirk De Backer.
Reflection Group "Horizon 2020–2030".
The European Council of December 2007 established the Reflection Group "Horizon 2020–2030" to assist the European Union in effectively anticipating and meeting challenges in the longer term horizon of 2020 to 2030. The group of 12 is chaired by Felipe González. It started the work in December 2008 and presented its report to the European Council in May 2010.
Its Members were:

</doc>
<doc id="9587" url="http://en.wikipedia.org/wiki?curid=9587" title="Euthanasia">
Euthanasia

Euthanasia (from Greek: εὐθανασία; "good death": εὖ, "eu"; "well" or "good" – θάνατος, "thanatos"; "death") is the practice of intentionally ending a life in order to relieve pain and suffering.
There are different euthanasia laws in each country. The British House of Lords Select Committee on Medical Ethics defines euthanasia as "a deliberate intervention undertaken with the express intention of ending a life, to relieve intractable suffering". In the Netherlands and Flanders, euthanasia is understood as "termination of life by a doctor at the request of a patient".
Euthanasia is categorized in different ways, which include voluntary, non-voluntary, or involuntary. Voluntary euthanasia is legal in some countries, U.S. states, and Canadian Provinces. Non-voluntary euthanasia is illegal in all countries. Involuntary euthanasia is usually considered murder. As of 2006, euthanasia is the most active area of research in contemporary bioethics.
In some countries there is a divisive public controversy over the moral, ethical, and legal issues of euthanasia. Those who are against euthanasia may argue for the sanctity of life, while proponents of euthanasia rights emphasize alleviating suffering, and preserving bodily integrity, self-determination, and personal autonomy. Jurisdictions where euthanasia or assisted suicide is legal include the Netherlands, Colombia, Switzerland, Japan, Germany, Belgium, Luxembourg, Estonia, Albania, the US states of Washington, Oregon, Montana, and Vermont and, starting in 2015, the Canadian Province of Quebec.
Definition.
Like other terms borrowed from history, "euthanasia" has had different meanings depending on usage. The first apparent usage of the term "euthanasia" belongs to the historian Suetonius, who described how the Emperor Augustus, "dying quickly and without suffering in the arms of his wife, Livia, experienced the 'euthanasia' he had wished for." The word "euthanasia" was first used in a medical context by Francis Bacon in the 17th century, to refer to an easy, painless, happy death, during which it was a "physician's responsibility to alleviate the 'physical sufferings' of the body." Bacon referred to an "outward euthanasia"—the term "outward" he used to distinguish from a spiritual concept—the euthanasia "which regards the preparation of the soul."
In current usage, euthanasia has been defined as the "painless inducement of a quick death". However, it is argued that this approach fails to properly define euthanasia, as it leaves open a number of possible actions which would meet the requirements of the definition, but would not be seen as euthanasia. In particular, these include situations where a person kills another, painlessly, but for no reason beyond that of personal gain; or accidental deaths that are quick and painless, but not intentional.
Another approach incorporates the notion of suffering into the definition. The definition offered by the Oxford English Dictionary incorporates suffering as a necessary condition, with "the painless killing of a patient suffering from an incurable and painful disease or in an irreversible coma", This approach is included in Marvin Khol and Paul Kurtz's definition of it as "a mode or act of inducing or permitting death painlessly as a relief from suffering". Counterexamples can be given: such definitions may encompass killing a person suffering from an incurable disease for personal gain (such as to claim an inheritance), and commentators such as Tom Beauchamp and Arnold Davidson have argued that doing so would constitute "murder simpliciter" rather than euthanasia.
The third element incorporated into many definitions is that of intentionality – the death must be intended, rather than being accidental, and the intent of the action must be a "merciful death". Michael Wreen argued that "the principal thing that distinguishes euthanasia from intentional killing simpliciter is the agent's motive: it must be a good motive insofar as the good of the person killed is concerned." Similarly, Heather Draper speaks to the importance of motive, arguing that "the motive forms a crucial part of arguments for euthanasia, because it must be in the best interests of the person on the receiving end." Definitions such as that offered by the House of Lords Select Committee on Medical Ethics take this path, where euthanasia is defined as "a deliberate intervention undertaken with the express intention of ending a life, to relieve intractable suffering." Beauchamp and Davidson also highlight Baruch Brody's "an act of euthanasia is one in which one person ... (A) kills another person (B) for the benefit of the second person, who actually does benefit from being killed".
Draper argued that any definition of euthanasia must incorporate four elements: an agent and a subject; an intention; a causal proximity, such that the actions of the agent lead to the outcome; and an outcome. Based on this, she offered a definition incorporating those elements, stating that euthanasia "must be defined as death that results from the intention of one person to kill another person, using the most gentle and painless means possible, that is motivated solely by the best interests of the person who dies." Prior to Draper, Beauchamp and Davidson had also offered a definition that includes these elements. Their definition specifically discounts fetuses in order to distinguish between abortions and euthanasia:
"In summary, we have argued ... that the death of a human being, A, is an instance of euthanasia if and only if (1) A's death is intended by at least one other human being, B, where B is either the cause of death or a causally relevant feature of the event resulting in death (whether by action or by omission); (2) there is either sufficient current evidence for B to believe that A is acutely suffering or irreversibly comatose, or there is sufficient current evidence related to A's present condition such that one or more known causal laws supports B's belief that A will be in a condition of acute suffering or irreversible comatoseness; (3) (a) B's primary reason for intending A's death is cessation of A's (actual or predicted future) suffering or irreversible comatoseness, where B does not intend A's death for a different primary reason, though there may be other relevant reasons, and (b) there is sufficient current evidence for either A or B that causal means to A's death will not produce any more suffering than would be produced for A if B were not to intervene; (4) the causal means to the event of A's death are chosen by A or B to be as painless as possible, unless either A or B has an overriding reason for a more painful causal means, where the reason for choosing the latter causal means does not conflict with the evidence in 3b; (5) A is a nonfetal organism."
Wreen, in part responding to Beauchamp and Davidson, offered a six-part definition:
"Person A committed an act of euthanasia if and only if (1) A killed B or let her die; (2) A intended to kill B; (3) the intention specified in (2) was at least partial cause of the action specified in (1); (4) the causal journey from the intention specified in (2) to the action specified in (1) is more or less in accordance with A's plan of action; (5) A's killing of B is a voluntary action; (6) the motive for the action specified in (1), the motive standing behind the intention specified in (2), is the good of the person killed."
Wreen also considered a seventh requirement: "(7) The good specified in (6) is, or at least includes, the avoidance of evil", although as Wreen noted in the paper, he was not convinced that the restriction was required.
In discussing his definition, Wreen noted the difficulty of justifying euthanasia when faced with the notion of the subject's "right to life". In response, Wreen argued that euthanasia has to be voluntary, and that "involuntary euthanasia is, as such, a great wrong". Other commentators incorporate consent more directly into their definitions. For example, in a discussion of euthanasia presented in 2003 by the European Association of Palliative Care (EPAC) Ethics Task Force, the authors offered: "Medicalized killing of a person without the person's consent, whether nonvoluntary (where the person in unable to consent) or involuntary (against the person's will) is not euthanasia: it is murder. Hence, euthanasia can be voluntary only." Although the EPAC Ethics Task Force argued that both non-voluntary and involuntary euthanasia could not be included in the definition of euthanasia, there is discussion in the literature about excluding one but not the other.
Classification of euthanasia.
Euthanasia may be classified according to whether a person gives informed consent into three types: voluntary, non-voluntary and involuntary.
There is a debate within the medical and bioethics literature about whether or not the non-voluntary (and by extension, involuntary) killing of patients can be regarded as euthanasia, irrespective of intent or the patient's circumstances. In the definitions offered by Beauchamp and Davidson and, later, by Wreen, consent on the part of the patient was not considered as one of their criteria, although it may have been required to justify euthanasia. However, others see consent as essential.
Voluntary euthanasia.
Euthanasia conducted with the consent of the patient is termed voluntary euthanasia. Active voluntary euthanasia is legal in Belgium, Luxembourg and the Netherlands. Passive voluntary euthanasia is legal throughout the U.S. per "Cruzan v. Director, Missouri Department of Health". When the patient brings about his or her own death with the assistance of a physician, the term assisted suicide is often used instead. Assisted suicide is legal in Switzerland and the U.S. states of Oregon, Washington, Montana and Vermont.
Non-voluntary euthanasia.
Euthanasia conducted where the consent of the patient is unavailable is termed non-voluntary euthanasia. Examples include child euthanasia, which is illegal worldwide but decriminalised under certain specific circumstances in the Netherlands under the Groningen Protocol.
Involuntary euthanasia.
Euthanasia conducted against the will of the patient is termed involuntary euthanasia.
Passive and active euthanasia.
Voluntary, non-voluntary and involuntary euthanasia can all be further divided into passive or active variants. Passive euthanasia entails the withholding of common treatments, such as antibiotics, necessary for the continuance of life. Active euthanasia entails the use of lethal substances or forces, such as administering a lethal injection, to kill and is the most controversial means. A number of authors consider these terms to be misleading and unhelpful.
History.
According to the historian N. D. A. Kemp, the origin of the contemporary debate on euthanasia started in 1870. Euthanasia is known to have been debated and practiced long before that date. Euthanasia was practiced in Ancient Greece and Rome: for example, hemlock was employed as a means of hastening death on the island of Kea, a technique also employed in Marseilles and by Socrates in Athens. Euthanasia, in the sense of the deliberate hastening of a person's death, was supported by Socrates, Plato and Seneca the Elder in the ancient world, although Hippocrates appears to have spoken against the practice, writing "I will not prescribe a deadly drug to please someone, nor give advice that may cause his death" (noting there is some debate in the literature about whether or not this was intended to encompass euthanasia).
Early modern period.
The term "euthanasia" in the earlier sense of supporting someone as they died was used for the first time by Francis Bacon (1561-1626). In his work, "Euthanasia medica", he chose this ancient Greek word and, in doing so, distinguished between "euthanasia interior", the preparation of the soul for death, and "euthanasia exterior", which was intended to make the end of life easier and painless, in exceptional circumstances by shortening life. That the ancient meaning of an easy death came to the fore again in the early modern period can be seen from its definition in the 18th century "Zedlers Universallexikon":
The concept of euthanasia in the sense of alleviating the process of death goes back to the medical historian, Karl Friedrich Heinrich Marx, who drew on Bacon's philosophical ideas. According to Marx, a doctor had a moral duty, to ease the suffering of death through encouragement, support and mitigation using medication. Such an "alleviation of death" reflected the contemporary "Zeitgeist", but was brought into the medical canon of responsibility for the first time by Marx. Marx also stressed the distinction between the theological care of the soul of sick people from the physical care and medical treatment by doctors.
Euthanasia in its modern sense has always been strongly opposed in the Judeo-Christian tradition. Thomas Aquinas opposed both and argued that the practice of euthanasia contradicted our natural human instincts of survival, as did Francois Ranchin (1565–1641), a French physician and professor of medicine, and Michael Boudewijns (1601–1681), a physician and teacher.:208 Other voices argued for euthanasia, such as John Donne in 1624, and euthanasia continued to be practised. In 1678, the publication of Caspar Questel's "De pulvinari morientibus non subtrahend", ("On the pillow of which the dying should not be deprived"), initiated debate on the topic. Questel described various customs which were employed at the time to hasten the death of the dying, (including the sudden removal of a pillow, which was believed to accelerate death), and argued against their use, as doing so was "against the laws of God and Nature".:209–211 This view was shared by many who followed, including Philipp Jakob Spener, Veit Riedlin and Johann Georg Krünitz.:211 Despite opposition, euthanasia continued to be practised, involving techniques such as bleeding, suffocation, and removing people from their beds to be placed on the cold ground.:211–214
Suicide and euthanasia became more accepted during the Age of Enlightenment. Thomas More wrote of euthanasia in "Utopia", although it is not clear if More was intending to endorse the practice.:208–209 Other cultures have taken different approaches: for example, in Japan suicide has not traditionally been viewed as a sin, as it is used in cases of honor, and accordingly, the perceptions of euthanasia are different from those in other parts of the world.
Beginnings of the contemporary euthanasia debate.
In the mid-1800s, the use of morphine to treat "the pains of death" emerged, with John Warren recommending its use in 1848. A similar use of chloroform was revealed by Joseph Bullar in 1866. However, in neither case was it recommended that the use should be to hasten death. In 1870 Samuel Williams, a schoolteacher, initiated the contemporary euthanasia debate through a speech given at the Birmingham Speculative Club in England, which was subsequently published in a one-off publication entitled "Essays of the Birmingham Speculative Club", the collected works of a number of members of an amateur philosophical society.:794 Williams' proposal was to use chloroform to deliberately hasten the death of terminally ill patients:
That in all cases of hopeless and painful illness, it should be the recognized duty of the medical attendant, whenever so desired by the patient, to administer choloroform or such other anaesthetic as may by-and-bye supersede chloroform – so as to destroy consciousness at once, and put the sufferer to a quick and painless death; all needful precautions being adopted to prevent any possible abuse of such duty; and means being taken to establish, beyond the possibility of doubt or question, that the remedy was applied at the express wish of the patient.—Samuel Williams (1872), "Euthanasia" Williams and Northgate: London.:794
The essay was favourably reviewed in "The Saturday Review", but an editorial against the essay appeared in "The Spectator". From there it proved to be influential, and other writers came out in support of such views: Lionel Tollemache wrote in favour of euthanasia, as did Annie Besant, the essayist and reformer who later became involved with the National Secular Society, considering it a duty to society to "die voluntarily and painlessly" when one reaches the point of becoming a 'burden'. "Popular Science" analyzed the issue in May 1873, assessing both sides of the argument. Kemp notes that at the time, medical doctors did not participate in the discussion; it was "essentially a philosophical enterprise ... tied inextricably to a number of objections to the Christian doctrine of the sanctity of human life".
Early euthanasia movement in the United States.
The rise of the euthanasia movement in the United States coincided with the so-called Gilded Age, a time of social and technological change that encompassed an "individualistic conservatism that praised laissez-faire economics, scientific method, and rationalism", along with major depressions, industrialisation and conflict between corporations and labor unions.:794 It was also the period in which the modern hospital system was developed, which has been seen as a factor in the emergence of the euthanasia debate.
Robert Ingersoll argued for euthanasia, stating in 1894 that where someone is suffering from a terminal illness, such as terminal cancer, they should have a right to end their pain through suicide. Felix Adler offered a similar approach, although, unlike Ingersoll, Adler did not reject religion. In fact, he argued from an Ethical Culture framework. In 1891, Alder argued that those suffering from overwhelming pain should have the right to commit suicide, and, furthermore, that it should be permissible for a doctor to assist – thus making Adler the first "prominent American" to argue for suicide in cases where people were suffering from chronic illness. Both Ingersoll and Adler argued for voluntary euthanasia of adults suffering from terminal ailments. Dowbiggin argues that by breaking down prior moral objections to euthanasia and suicide, Ingersoll and Adler enabled others to stretch the definition of euthanasia.
The first attempt to legalise euthanasia took place in the United States, when Henry Hunt introduced legislation into the General Assembly of Ohio in 1906.:614 Hunt did so at the behest of Anna Hall, a wealthy heiress who was a major figure in the euthanasia movement during the early 20th century in the United States. Hall had watched her mother die after an extended battle with liver cancer, and had dedicated herself to ensuring that others would not have to endure the same suffering. Towards this end she engaged in an extensive letter writing campaign, recruited Lurana Sheldon and Maud Ballington Booth, and organised a debate on euthanasia at the annual meeting of the American Humane Association in 1905 – described by Jacob Appel as the first significant public debate on the topic in the 20th century.:614–616 
Hunt's bill called for the administration of an anesthetic to bring about a patient's death, so long as the person is of lawful age and sound mind, and was suffering from a fatal injury, an irrevocable illness, or great physical pain. It also required that the case be heard by a physician, required informed consent in front of three witnesses, and required the attendance of three physicians who had to agree that the patient's recovery was impossible. A motion to reject the bill outright was voted down, but the bill failed to pass, 79 to 23.:796:618–619
Along with the Ohio euthanasia proposal, in 1906 Assemblyman Ross Gregory introduced a proposal to permit euthanasia to the Iowa legislature. However, the Iowa legislation was far broader in scope than that offered in Ohio. It allowed for the death of any person of at least ten years of age who suffered from an ailment that would prove fatal and cause extreme pain, should they be of sound mind and express a desire to artificially hasten their death. In addition, it allowed for infants to be euthanised if they were sufficiently deformed, and permitted guardians to request euthanasia on behalf of their wards. The proposed legislation also imposed penalties on physicians who refused to perform euthanasia when requested: a 6–12 month prison term and a fine of between $200 and $1000. The proposal proved to be controversial.:619–621 It engendered considerable debate and failed to pass, having been withdrawn from consideration after being passed to the Committee on Public Health.:623
After 1906 the euthanasia debate reduced in intensity, resurfacing periodically but not returning to the same level of debate until the 1930s in the United Kingdom.:796
1930s in Britain.
The Voluntary Euthanasia Legalisation Society was founded in 1935 by Charles Killick Millard (now called Dignity in Dying). The movement campaigned for the legalisation of euthanasia in Great Britain.
In January 1936, King George V was given a fatal dose of morphine and cocaine in order to hasten his death. At the time he was suffering from cardio-respiratory failure, and the decision to end his life was made by his physician, Lord Dawson. Although this event was kept a secret for over 50 years, the death of George V coincided with proposed legislation in the House of Lords to legalise euthanasia. The legislation came through the British Volunteer Euthanasia Legalisation Society.
Euthanasia opponent Ian Dowbiggin argues that the early membership of the Euthanasia Society of America (ESA) reflected how many perceived euthanasia at the time, often seeing it as a eugenics matter rather than an issue concerning individual rights. Dowbiggin argues that not every eugenist joined the ESA "solely for eugenic reasons", but he postulates that there were clear ideological connections between the eugenics and euthanasia movements.
Nazi Euthanasia Program (Action T4).
A 24 July 1939 killing of a severely disabled infant in Nazi Germany was described in a BBC "Genocide Under the Nazis Timeline" as the first "state-sponsored euthanasia". Parties that consented to the killing included Hitler's office, the parents, and the Reich Committee for the Scientific Registration of Serious and Congenitally Based Illnesses. "The Telegraph" noted that the killing of the disabled infant—whose name was Gerhard Kretschmar, born blind, with missing limbs, subject to convulsions, and reportedly "an idiot"— provided "the rationale for a secret Nazi decree that led to 'mercy killings' of almost 300,000 mentally and physically handicapped people". While Kretchmar's killing received parental consent, most of the 5,000 to 8,000 children killed afterwards were forcibly taken from their parents.
The "euthanasia campaign" of mass murder gathered momentum on 14 January 1940 when the "handicapped" were killed with gas vans and killing centres, eventually leading to the deaths of 70,000 adult Germans. Professor Robert Jay Lifton, author of "The Nazi Doctors" and a leading authority on the T4 program, contrasts this program with what he considers to be a genuine euthanasia. He explains that the Nazi version of "euthanasia" was based on the work of Adolf Jost, who published "The Right to Death" (Das Recht auf den Tod) in 1895. Lifton writes: "Jost argued that control over the death of the individual must ultimately belong to the social organism, the state. This concept is in direct opposition to the Anglo-American concept of euthanasia, which emphasizes the "individual's" 'right to die' or 'right to death' or 'right to his or her own death,' as the ultimate human claim. In contrast, Jost was pointing to the state's right to kill. ... Ultimately the argument was biological: 'The rights to death [are] the key to the fitness of life.' The state must own death—must kill—in order to keep the social organism alive and healthy."
In modern terms, the use of "euthanasia" in the context of Action T4 is seen to be a euphemism to disguise a program of genocide, in which people were killed on the grounds of "disabilities, religious beliefs, and discordant individual values". Compared to the discussions of euthanasia that emerged post-war, the Nazi program may have been worded in terms that appear similar to the modern use of "euthanasia", but there was no "mercy" and the patients were not necessarily terminally ill. Despite these differences, historian and euthanasia opponent Ian Dowbiggin writes that "the origins of Nazi euthanasia, like those of the American euthanasia movement, predate the Third Reich and were intertwined with the history of eugenics and Social Darwinism, and with efforts to discredit traditional morality and ethics.":65
Euthanasia debate.
Historically, the euthanasia debate has tended to focus on a number of key concerns. According to euthanasia opponent Ezekiel Emanuel, proponents of euthanasia have presented four main arguments: a) that people have a right to self-determination, and thus should be allowed to choose their own fate; b) assisting a subject to die might be a better choice than requiring that they continue to suffer; c) the distinction between passive euthanasia, which is often permitted, and active euthanasia, which is not substantive (or that the underlying principle–the doctrine of double effect–is unreasonable or unsound); and d) permitting euthanasia will not necessarily lead to unacceptable consequences. Pro-euthanasia activists often point to countries like the Netherlands and Belgium, and states like Oregon, where euthanasia has been legalized, to argue that it is mostly unproblematic.
Similarly, Emanuel argues that there are four major arguments presented by opponents of euthanasia: a) not all deaths are painful; b) alternatives, such as cessation of active treatment, combined with the use of effective pain relief, are available; c) the distinction between active and passive euthanasia is morally significant; and d) legalising euthanasia will place society on a slippery slope, which will lead to unacceptable consequences.:797–8
Elisabeth Kübler-Ross, an eminent Swiss American psychiatrist (a pioneer in near-death studies and the author of the groundbreaking book "On Death and Dying" (1969), where she first discussed her theory of the five stages of grief), encouraged the hospice care movement, believing that euthanasia prevents people from completing their 'unfinished business'.
Legal status.
West's "Encyclopedia of American Law" states that "a 'mercy killing' or euthanasia is generally considered to be a criminal homicide" and is normally used as a synonym of homicide committed at a request made by the patient.
The judicial sense of the term "homicide" includes any intervention undertaken with the express intention of ending a life, even to relieve intractable suffering. Not all homicide is unlawful. Two designations of homicide that carry no criminal punishment are justifiable and excusable homicide. In most countries this is not the status of euthanasia. The term "euthanasia" is usually confined to the active variety; the University of Washington website states that "euthanasia generally means that the physician would act directly, for instance by giving a lethal injection, to end the patient's life". Physician-assisted suicide is thus not classified as euthanasia by the US State of Oregon, where it is legal under the Oregon Death with Dignity Act, and despite its name, it is not legally classified as suicide either. Unlike physician-assisted suicide, withholding or withdrawing life-sustaining treatments with patient consent (voluntary) is almost unanimously considered, at least in the United States, to be legal. The use of pain medication in order to relieve suffering, even if it hastens death, has been held as legal in several court decisions.
Some governments around the world have legalized voluntary euthanasia but most commonly it is still considered to be criminal homicide. In the Netherlands and Belgium, where euthanasia has been legalized, it still remains homicide although it is not prosecuted and not punishable if the perpetrator (the doctor) meets certain legal conditions.
Physician sentiment.
A survey in the United States of more than 10,000 physicians came to the result that approximately 16% of physicians would ever consider halting life-sustaining therapy because the family demands it, even if they believed that it was premature. Approximately 55% would not, and for the remaining 29%, it would depend on circumstances.
This study also stated that approximately 46% of physicians agree that physician-assisted suicide should be allowed in some cases; 41% do not, and the remaining 14% think it depends.
In the United Kingdom, the pro-assisted dying group Dignity in Dying cite conflicting research on attitudes by doctors to assisted dying: with a 2009 "Palliative Medicine"-published survey showing 64% support (to 34% oppose) for assisted dying in cases where a patient has an incurable and painful disease, while 49% of doctors in a study published in "BMC Medical Ethics" oppose changing the law on assisted dying to 39% in favour.

</doc>
<doc id="9588" url="http://en.wikipedia.org/wiki?curid=9588" title="Extraterrestrial life">
Extraterrestrial life

Extraterrestrial life is life that does not originate from Earth. It is also called alien life, or, if it is a sentient and/or relatively complex individual, an "extraterrestrial" or "alien" (or, to avoid confusion with the legal sense of "alien", a "space alien"). These as yet hypothetical forms of life range from simple bacteria-like organisms to beings far more complex than humans. The possibility that viruses might exist extraterrestrially has also been proposed.
The development and testing of hypotheses on extraterrestrial life is known as "exobiology" or "astrobiology", although astrobiology also considers Earth-based life in its astronomical context. Many scientists consider extraterrestrial life plausible, but there is no direct evidence of its existence. Since the mid-20th century, there has been an ongoing search for signs of extraterrestrial life, from radios used to detect possible extraterrestrial signals, to telescopes used to search for potentially habitable extrasolar planets. It has also played a major role in works of science fiction. Over the years, science fiction works, especially Hollywood's involvement, has increased the public's interest in the possibility of extraterrestrial life. Some encourage aggressive methods to try and get in contact with life in outer space, while others argue that it might be dangerous to actively call attention to Earth. In the past, the clash between civilized culture and indigenous people has not gone well.
On 13 February 2015, scientists (including Geoffrey Marcy, Seth Shostak, Frank Drake and David Brin) at a convention of the American Association for the Advancement of Science, discussed Active SETI and whether transmitting a message to possible intelligent extraterrestrials in the Cosmos was a good idea; one result was a statement, signed by many, that a "worldwide scientific, political and humanitarian discussion must occur before any message is sent".
Background.
Alien life, such as microorganisms, has been hypothesized to exist in the Solar System and throughout the universe. This hypothesis relies on the vast size and consistent physical laws of the observable universe. According to this argument, made by scientists such as Carl Sagan and Stephen Hawking, it would be improbable for life "not" to exist somewhere other than Earth. This argument is embodied in the Copernican principle, which states that Earth does not occupy a unique position in the Universe, and the mediocrity principle, which suggests that there is nothing special about life on Earth. The chemistry of life may have begun shortly after the Big Bang, 13.8 billion years ago, during a habitable epoch when the Universe was only 10–17 million years old. Life may have emerged independently at many places throughout the Universe. Alternatively, life may have formed less frequently, then spread—by meteoroids, asteroids and other small Solar System bodies—between habitable planets in a process called panspermia. In any case, complex organic molecules necessary for life may have formed in the protoplanetary disk of dust grains surrounding the Sun before the formation of Earth based on computer model studies. According to these studies, this same process may also occur around other stars that acquire planets. (Also see Extraterrestrial organic molecules.)
Suggested locations where life might have developed include the planets Venus and Mars, Jupiter's moon Europa, and Saturn's moons Titan and Enceladus. In May 2011, NASA scientists reported that Enceladus "is emerging as the most habitable spot beyond Earth in the Solar System for life as we know it".
Since the 1950s, scientists have promoted the idea that "habitable zones" are the most likely places to find life. Numerous discoveries in this zone since 2007 have stimulated estimations of frequencies of Earth-like habitats numbering in the many billions though as of 2013, only a small number of planets have been discovered in these zones. Nonetheless, on November 4, 2013, astronomers reported, based on "Kepler" space mission data, that there could be as many as 40 billion Earth-sized planets orbiting in the habitable zones of Sun-like stars and red dwarfs in the Milky Way, 11 billion of which may be orbiting Sun-like stars. The nearest such planet may be 12 light-years away, according to the scientists. Astrobiologists have also considered a "follow the energy" view of potential habitats.
No widely accepted evidence of extraterrestrial life has been found; however, various controversial claims have been made. Beliefs that some unidentified flying objects are of extraterrestrial origin, along with claims of alien abduction, are dismissed by most scientists. Most UFO sightings are explained either as sightings of Earth-based aircraft or known astronomical objects, or as hoaxes.
In November 2011, the White House released an official response to two petitions asking the U.S. government to acknowledge formally that aliens have visited Earth and to disclose any intentional withholding of government interactions with extraterrestrial beings. According to the response, "The U.S. government has no evidence that any life exists outside our planet, or that an extraterrestrial presence has contacted or engaged any member of the human race." Also, according to the response, there is "no credible information to suggest that any evidence is being hidden from the public's eye." The response further noted that efforts, like SETI, the "Kepler" space telescope and the NASA Mars rover, continue looking for signs of life. The response noted "odds are pretty high" that there may be life on other planets but "the odds of us making contact with any of them—especially any intelligent ones—are extremely small, given the distances involved."
Possible basis.
Several hypotheses have been proposed about the possible basis of alien life from a biochemical, evolutionary or morphological viewpoint.
Biochemistry.
All life on Earth is based upon 26 chemical elements. However, about 95% of this life is built upon only six of these elements: carbon, hydrogen, nitrogen, oxygen, phosphorus and sulfur, abbreviated CHNOPS. These six elements form the basic building blocks of virtually all life on Earth, whereas most of the remaining elements are found only in trace amounts.
Life on Earth requires water as its solvent in which biochemical reactions take place. Sufficient quantities of carbon and the other elements along with water, may enable the formation of living organisms on other planets with a chemical make-up and temperature range similar to that of Earth. Terrestrial planets such as Earth are formed in a process that allows for the possibility of having compositions similar to Earth's. The combination of carbon, hydrogen and oxygen in the chemical form of carbohydrates (e.g. sugar) can be a source of chemical energy on which life depends, and can provide structural elements for life (such as ribose, in the molecules DNA and RNA, and cellulose in plants). Plants derive energy through the conversion of light energy into chemical energy via photosynthesis. Life, as currently recognized, requires carbon in both reduced (methane derivatives) and partially oxidized (carbon oxides) states. Nitrogen is needed as a reduced ammonia derivative in all proteins, sulfur as a derivative of hydrogen sulfide in some necessary proteins, and phosphorus oxidized to phosphates in genetic material and in energy transfer.
Water is amphoteric, meaning it can donate and accept an H+ ion, allowing it to act as an acid or a base. This property is crucial in many organic and biochemical reactions, where water serves as a solvent, a reactant, or a product. Furthermore, the fact that organic molecules can be either hydrophobic (repelled by water) or hydrophilic (soluble in water) creates the ability of organic compounds to orient themselves to form water-enclosing membranes. Additionally, the hydrogen bonds between water molecules give it an ability to store energy with evaporation, which upon condensation is released. This helps to moderate the climate, cooling the tropics and warming the poles, helping to maintain the thermodynamic stability needed for life.
Carbon is fundamental to terrestrial life for its immense flexibility in creating covalent chemical bonds with a variety of non-metallic elements, principally nitrogen, oxygen and hydrogen. Carbon dioxide and water together enable the storage of solar energy in sugars and starches, such as glucose. The oxidation of glucose releases biochemical energy needed to fuel all other biochemical reactions.
Norman Horowitz was the head of the Jet Propulsion Laboratory bioscience section for the Mariner and Viking missions to Mars (1965–1976). In his 1986 book "Utopia and Back; the Search for life in the Solar System", Horowitz noted that life on Earth is based on the chemistry of carbon. He considered that the unique characteristics of carbon made it unlikely that any other element could replace carbon, even on another planet, to generate the biochemistry necessary for life. The carbon atom has the unique ability to make four strong chemical bonds with other atoms, including other carbon atoms. These covalent bonds have a direction in space, so that carbon atoms can form the skeletons of complex 3-dimensional structures with definite architectures such as nucleic acids and proteins. Horowitz noted that carbon forms more compounds than all other elements combined. He considered that the great versatility of the carbon atom makes it the element most likely to provide solutions—even exotic ones—to the survival of life on other planets.
The ability to form organic acids (–COOH) and amine bases (–NH2) gives rise to the possibility of neutralization dehydrating reactions to build long polymer peptides and catalytic proteins from monomer amino acids. When combined with phosphates, these acids can build the information-storing molecule of inheritance, DNA, and the principal energy transfer molecule of cellular life, ATP.
Due to their relative abundance and usefulness in sustaining life, many have hypothesized that life forms elsewhere in the universe would utilize these basic materials. However, other elements and solvents could provide a basis for life.
Life forms based in ammonia (rather than water) have been suggested, though this solution appears less optimal than water. It is also conceivable that there are forms of life whose solvent is a liquid hydrocarbon, such as methane, ethane or propane.
From a chemical perspective, life is fundamentally a self-replicating reaction that could arise under many conditions and with various possible ingredients—though carbon–oxygen within the liquid temperature range of water seems most conducive. Suggestions have even been made that self-replicating reactions of some sort could occur within the plasma of a star, though it would be highly unconventional. Life on the surface of a neutron star, based on nuclear reactions, has also been suggested. However, communicating with such creatures would be difficult because the time scales involved are much faster.
It has been argued that the evolution of order in living systems and certain non-living physical systems obeys a common fundamental principle termed the "The Darwinian Dynamic" that can be represented by set of related equations. Thus evolution of life on Earth can be viewed as a special case of systems that evolve macroscopically ordered structures. That all such ordered systems appear to share a common dynamic indicates that "life", wherever it might exist in the universe, may evolve according to a common dynamic principle.
Several pre-conceived ideas about the characteristics of life outside Earth have been questioned. For example, a NASA scientist suggested that the color of photosynthesizing pigments of hypothetical life on extrasolar planets might not be green.
Evolution and morphology.
In addition to the biochemical basis of extraterrestrial life, many have considered evolution and morphology. Science fiction has often depicted extraterrestrial life with humanoid or reptilian forms. Aliens have often been depicted as having light green or grey skin, with a large head, as well as four limbs—i.e. fundamentally humanoid. Other subjects, such as felines, insects, blobs, etc., have occurred in fictional representations of aliens.
A division has been suggested between universal and parochial (narrowly restricted) characteristics. Universals are features thought to have evolved independently more than once on Earth (and thus, presumably, are not too difficult to develop) and are so intrinsically useful that species will inevitably tend towards them. The most fundamental of these is probably bilateral symmetry, but more complex (though still basic) characteristics include flight, sight, photosynthesis and limbs, all of which are thought to have evolved several times here on Earth. There is a huge variety of eyes, for example, and many of these have radically different working schematics and different visual foci: the visual spectrum, infrared, polarity and echolocation. Parochials, however, are essentially arbitrary evolutionary forms. These often have little inherent utility (or at least have a function that can be equally served by dissimilar morphology) and probably will not be replicated. Intelligent aliens could communicate through gestures, as deaf humans do, by sounds created from structures unrelated to breathing, which happens on Earth when, for instance, cicadas vibrate their wings or crickets stridulate their wings, or visually through bioluminescence or chromatophore-like structures.
Attempting to define parochial features challenges many taken-for-granted notions about morphological necessity. Skeletons essential to large terrestrial organisms (according to gravitational biology experts) could likely exist elsewhere in some form. The assumption of radical diversity amongst putative extraterrestrials is by no means settled. Although many exobiologists do stress that the enormously heterogeneous nature of life on Earth foreshadows an even greater variety in outer space, others point out that convergent evolution may dictate substantial similarities between extraterrestrial life and that on Earth. These two schools of thought are called "divergionism" and "convergionism" respectively.
Planetary habitability in the Solar System.
Some bodies in the Solar System have been suggested as having the potential for an environment that could host extraterrestrial life, particularly those with possible subsurface oceans. Though due to the lack of habitable environments beyond Earth, should life be discovered elsewhere in the Solar System, astrobiologists suggest that it will more likely be in the form of extremophile microorganisms.
Some speculate that the planets Venus and Mars, several natural satellites that orbit Jupiter and Saturn, and even comets may possess niche environments where life might exist. A subsurface marine environment on Jupiter's moon Europa might be the most suitable habitat in the Solar System, outside Earth, for multicellular organisms.
Panspermia suggests that life elsewhere in the Solar System may have a common origin. If extraterrestrial life was found on another body in the Solar System, it could have originated from Earth just as life on Earth may have been seeded from elsewhere (exogenesis). The Living Interplanetary Flight Experiment, developed by the Planetary Society launched in 2011 was designed to test some aspect of these hypotheses, but it was destroyed along with the carrier Fobos-Grunt mission.
The first known mention of the term Panspermia was in the writings of the 5th century BC Greek philosopher Anaxagoras. In the nineteenth century it was again revived in modern form by several scientists, including Jöns Jacob Berzelius (1834), Kelvin (1871), Hermann von Helmholtz (1879) and, somewhat later, by Svante Arrhenius (1903).
Sir Fred Hoyle (1915–2001) and Chandra Wickramasinghe (born 1939) were important proponents of the hypothesis who further contended that lifeforms continue to enter Earth's atmosphere, and may be responsible for epidemic outbreaks, new diseases, and the genetic novelty necessary for macroevolution.
Directed panspermia concerns the deliberate transport of microorganisms in space, sent to Earth to start life here, or sent from Earth to seed new stellar systems with life.
The Nobel prize winner Francis Crick, along with Leslie Orgel proposed that seeds of life may have been purposely spread by an advanced extraterrestrial civilization, but considering an early "RNA world". Crick noted later that life may have originated on Earth.
In a virtual presentation on Tuesday, April 7, 2009, Stephen Hawking discussed the possibility of building a human base on another planet and gave reasons why alien life might not be contacting the human race, during his conclusion of the Origins Symposium at Arizona State University. Hawking also talked about what humans may find when venturing into space, such as the possibility of alien life through the theory of panspermia.
On August 20, 2014, Russian cosmonauts claimed to have found sea plankton living on the "outside" window surfaces of the International Space Station and have been unable to explain how it got there.
Venus.
Carl Sagan, David Grinspoon, Geoffrey A. Landis and Dirk Schulze-Makuch have put forward a hypothesis that microbes could exist in the stable cloud layers 50 km above the surface of Venus; the hypothesis is based on the premises of hospitable climates and chemical disequilibrium.
Mars.
Life on Mars has been long speculated. Liquid water is widely thought to have existed on Mars in the past, and there may still be liquid water beneath the surface. It may also be present as thin films of salty brine in the first centimeter or so of the soil for part of the year in some locations. The origin of the potential biosignature of methane in Mars atmosphere is unexplained, although abiotic hypotheses have also been proposed. By July 2008, laboratory tests aboard NASA's Phoenix Mars Lander had identified water in a soil sample. The lander's robotic arm delivered the sample to an instrument that identifies vapours produced by heating samples. Photographs from the Mars Global Surveyor from 2006 showed evidence of recent (i.e. within 10 years) flows of a liquid on Mars's frigid surface.
There is evidence that Mars had a warmer and wetter past: dried-up river beds, polar ice caps, volcanos, and minerals that form in the presence of water have all been found. Nevertheless, present conditions on Mars may support life, because lichens were found to successfully survive Martian conditions in the Mars Simulation Laboratory (MSL) maintained by the German Aerospace Center (DLR). In June 2012, scientists reported that measuring the ratio of hydrogen and methane levels on Mars may help determine the likelihood of life on Mars. According to the scientists, "...low H2/CH4 ratios (less than approximately 40) indicate that life is likely present and active." Other scientists have recently reported methods of detecting hydrogen and methane in extraterrestrial atmospheres. On December 9, 2013, NASA reported that, based on evidence from "Curiosity" studying Aeolis Palus, Gale Crater contained an ancient freshwater lake that could have been a hospitable environment for microbial life.
On January 24, 2014, NASA reported that current studies on the planet Mars by the "Curiosity" and "Opportunity" rovers will now search for evidence of ancient life, including a biosphere based on autotrophic, chemotrophic and/or chemolithoautotrophic microorganisms, as well as ancient water, including fluvio-lacustrine environments (plains related to ancient rivers or lakes) that may have been habitable. The search for evidence of habitability, taphonomy (related to fossils), and organic carbon on the planet Mars is now a primary NASA objective.
Ceres.
Ceres, the only dwarf planet in the asteroid belt, has recently been confirmed by the Herschel Space Observatory to have water vapor in its atmosphere. Frost on the surface may also have been detected. The presence of water, and the temperatures on Ceres has led to speculation that life may be possible there. The Dawn space probe is in orbit around Ceres since March 6, 2015.
Jupiter system.
Jupiter.
Based on observations, Carl Sagan and others in the 1960s and 1970s computed conditions for hypothetical amino-acid-based macroscopic life in the atmosphere of Jupiter. However, the conditions do not appear to permit the type of encapsulation thought necessary for molecular biochemistry, so life is thought unlikely.
However, some of Jupiter's moons may have habitats capable of sustaining life. Scientists have suggested that heated subsurface oceans of water may exist deep under the crusts of the three outer Galilean moons—Europa, Ganymede, and Callisto. The EJSM/Laplace mission is planned to determine the habitability of these environments. However, Europa is seen as the main target for the discovery of life.
Europa.
Jupiter's moon Europa has been subject to speculation about the existence of life due to the strong possibility of a liquid water ocean beneath its ice surface. Hydrothermal vents on the bottom of the ocean, if they exist, may warm the ice and could be capable of supporting multicellular microorganisms. It is also possible that Europa could support aerobic macrofauna using oxygen created by cosmic rays impacting its surface ice.
The case for life on Europa was greatly enhanced in 2011 when it was discovered that vast lakes exist within Europa's thick, icy shell. Scientists found that ice shelves surrounding the lakes appear to be collapsing into them, thereby providing a mechanism through which life-forming chemicals created in sunlit areas on Europa's surface could be transferred to its interior.
On December 11, 2013, NASA reported the detection of "clay-like minerals" (specifically, phyllosilicates), often associated with organic materials, on the icy crust of Europa. The presence of the minerals may have been the result of a collision with an asteroid or comet according to the scientists.
Saturn system.
Although astronomers consider Saturn inhospitable to life, its natural satellites Titan and Enceladus have been speculated to possess possible habitats for life.
Enceladus.
Enceladus, a moon of Saturn, has some of the conditions for life including geothermal activity and water vapor as well as possible under-ice oceans heated by tidal effects. The Cassini probe detected carbon, hydrogen, nitrogen and oxygen—all key elements for supporting living organisms—during a fly-by through one of Enceladus's geysers spewing ice and gas in 2005. The temperature and density of the plumes indicate a warmer, watery source beneath the surface. However, no life has been detected.
Titan.
Titan, the largest moon of Saturn, is the only known moon with a significant atmosphere. Data from the Cassini–Huygens mission refuted the hypothesis of a global hydrocarbon ocean, but later demonstrated the existence of liquid hydrocarbon lakes in the polar regions—the first stable bodies of surface liquid discovered outside Earth. Analysis of data from the mission has uncovered aspects of atmospheric chemistry near the surface that are consistent with—but do not prove—the hypothesis that organisms there are consuming hydrogen, acetylene and ethane, and producing methane.
An alternate explanation for the hypothetical existence of microbial life on Titan has already been formally proposed—hypothesizing that microorganisms could have left Earth when it suffered a massive asteroid or comet impact (such as the impact that created Chicxulub crater only 66 mya), and survived a journey through space to land on Titan.
Small Solar System bodies.
Small Solar System bodies have also been suggested as habitats for extremophiles. Fred Hoyle and Chandra Wickramasinghe have proposed that microbial life might exist on comets and asteroids.
Scientific search.
The scientific search for extraterrestrial life is being carried out both directly and indirectly.
Direct search.
Scientists are directly searching for biosignatures within the Solar System, carrying out studies on the surface of Mars and examining meteors that have fallen to Earth. At the moment, no concrete plan exists for exploration of Europa for life. In 2008, a joint mission by NASA and the European Space Agency was announced that would have included studies of Europa. However, in 2011 NASA was forced to deprioritize the mission due to a lack of funding, and it is possible that the ESA will take on the mission by itself.
There is some limited evidence that microbial life might possibly exist (or have existed) on Mars. An experiment on the Viking Mars lander reported gas emissions from heated Martian soil that some argue are consistent with the presence of microbes. However, the lack of corroborating evidence from other experiments on the Viking lander indicates that a non-biological reaction is a more likely hypothesis. Independently, in 1996, structures resembling nanobacteria were reportedly discovered in a meteorite, ALH84001, thought to be formed of rock ejected from Mars. This report is controversial.
In February 2005, NASA scientists reported that they may have found some evidence of present life on Mars. The two scientists, Carol Stoker and Larry Lemke of NASA's Ames Research Center, based their claim on methane signatures found in Mars's atmosphere resembling the methane production of some forms of primitive life on Earth, as well as on their own study of primitive life near the Rio Tinto river in Spain. NASA officials soon distanced NASA from the scientists' claims, and Stoker herself backed off from her initial assertions.
Though such methane findings are still debated, support among some scientists for the existence of life on Mars seems to be growing. An informal survey at the conference where the European Space Agency presented its findings on methane in Mars's atmosphere indicated that 75% of the people present agreed that bacteria once lived on Mars. Roughly 25% agreed that bacteria inhabit the planet today.
In November 2011, NASA launched the Mars Science Laboratory (MSL) rover, which is designed to search for past or present habitability on Mars using a variety of scientific instruments. The MSL landed on Mars at Gale Crater in August 2012.
The Gaia hypothesis stipulates that any planet with a robust population of life will have an atmosphere in chemical disequilibrium, which is relatively easy to determine from a distance by spectroscopy. However, significant advances in the ability to find and resolve light from smaller rocky worlds near their star are necessary before such spectroscopic methods can be used to analyze extrasolar planets.
In March 2011, Richard B. Hoover, an astrobiologist with the Marshall Space Flight Center, speculated on the finding of alleged microfossils similar to cyanobacteria in CI1 carbonaceous meteorites. However, NASA formally distanced itself from Hoover's claim. See Hoover paper controversy for more details.
In August 2011, findings by NASA, based on studies of meteorites found on Earth, suggests DNA and RNA components (adenine, guanine and related organic molecules), building blocks for life as we know it, may be formed extraterrestrially in outer space. In October 2011, scientists reported that cosmic dust contains complex organic matter ("amorphous organic solids with a mixed aromatic-aliphatic structure") that could be created naturally, and rapidly, by stars. One of the scientists suggested that these compounds may have been related to the development of life on Earth and said that, "If this is the case, life on Earth may have had an easier time getting started as these organics can serve as basic ingredients for life."
In August 2012, and in a world first, astronomers at Copenhagen University reported the detection of a specific sugar molecule, glycolaldehyde, in a distant star system. The molecule was found around the protostellar binary "IRAS 16293-2422", which is located 400 light years from Earth. Glycolaldehyde is needed to form ribonucleic acid, or RNA, which is similar in function to DNA. This finding suggests that complex organic molecules may form in stellar systems prior to the formation of planets, eventually arriving on young planets early in their formation.
In September 2012, NASA scientists reported that polycyclic aromatic hydrocarbons (PAHs), subjected to interstellar medium (ISM) conditions, are transformed, through hydrogenation, oxygenation and hydroxylation, to more complex organics - "a step along the path toward amino acids and nucleotides, the raw materials of proteins and DNA, respectively". Further, as a result of these transformations, the PAHs lose their spectroscopic signature—which could be one reason "for the lack of PAH detection in interstellar ice grains, particularly the outer regions of cold, dense clouds or the upper molecular layers of protoplanetary disks."
On February 21, 2014, NASA announced a for tracking polycyclic aromatic hydrocarbons (PAHs) in the universe. According to scientists, more than 20% of the carbon in the universe may be associated with PAHs, possible starting materials for the formation of life. PAHs seem to have been formed shortly after the Big Bang, are widespread throughout the universe, and are associated with new stars and exoplanets.
In March 2015, NASA scientists reported that, for the first time, complex DNA and RNA organic compounds of life, including uracil, cytosine and thymine, have been formed in the laboratory under outer space conditions, using starting chemicals, such as pyrimidine, that are found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), the most carbon-rich chemical found in the Universe, may have been formed in red giants or in interstellar dust and gas clouds, according to the scientists.
Indirect search.
If there is an advanced extraterrestrial society, there is no guarantee that it is transmitting information in the direction of Earth or that this information could be interpreted as such by humans. The length of time required for a signal to travel across the vastness of space means that any signal detected, or not detected, would come from the distant past.
Projects such as SETI are conducting an astronomical search for radio activity that would confirm the presence of intelligent life. A related suggestion is that aliens might broadcast pulsed and continuous laser signals in the optical, as well as infrared, spectrum; laser signals have the advantage of not "smearing" in the interstellar medium, and may prove more conducive to communication between the stars. Although other communication techniques, including laser transmission and interstellar spaceflight, have been discussed seriously and may well be feasible, the measure of effectiveness is the amount of information communicated per unit cost. This results in radio transmission as the method of choice.
Some have hypothesized that very advanced civilizations may create artificial black holes as an energy source or method of waste disposal. Thus, they suggest that the observation of a black hole with a mass of less than 3.5 solar masses, the theoretical lower mass limit for a naturally occurring black hole, would be evidence of an alien civilization. Another possible evidence is the abundance anomalies of heavy elements in the spectrum of a star, which would take place if the star was used as a repository for nuclear waste material.
Scientists have proposed radiated heat from a planet as a way to find advanced civilizations. Large-scale power generation releases large amounts of heat, which can be detected using an advanced telescope, such as the proposed Colossus interferometric telescope. The fact that the heat signatures would be coming from power generation, especially in urban areas, where heat islands are created, would also confirm that the observed civilization is advanced. Larger power consumption correlates with more-advanced societies. Detecting thermal signatures allows the ability to detect advanced civilizations indirectly, as opposed to trying to make direct contact through messages like those created by Active SETI.
Extrasolar planets.
Astronomers search for extrasolar planets that may be conducive to life, narrowing the search to terrestrial planets within the habitable zone. Current radiodetection methods have been inadequate for such a search, because the resolution afforded by recent technology is inadequate for a detailed study of extrasolar planetary objects. Future telescopes should be able to image planets around nearby stars, which may reveal the presence of life—either directly or through spectrography—and would reveal key information, such as the presence of free oxygen in a planet's atmosphere:
It has been argued that Alpha Centauri, the closest star system to Earth, may contain planets that could sustain life.
On April 24, 2007, scientists at the European Southern Observatory in La Silla, Chile said they had found the first Earth-like planet. The planet, known as Gliese 581 c, orbits within the habitable zone of its star Gliese 581, a red dwarf 20.5 light years (194 trillion km) from Earth. It was initially thought that this planet could contain liquid water, but recent computer simulations of the climate on Gliese 581 c by Werner von Bloh and his team at Germany's Institute for Climate Impact Research suggest that carbon dioxide and methane in the atmosphere would create a runaway greenhouse effect. This would warm the planet well above the boiling point of water (100 degrees Celsius/212 degrees Fahrenheit), thus dimming the hopes of finding life. As a result of greenhouse models, scientists are now turning their attention to Gliese 581 d, which lies just outside the star's traditional habitable zone.
On May 29, 2007, the Associated Press released a report stating that scientists identified twenty-eight new extra-solar planetary bodies. One of these newly discovered planets is said to have many similarities to Neptune.
In May 2011, researchers predicted that Gliese 581 d, not only exists in the "Goldilocks zone" where water can be present in liquid form, but is big enough to have a stable carbon dioxide atmosphere and "warm enough to have oceans, clouds, and rainfall," according to France's National Centre for Scientific Research.
In December 2011, NASA confirmed that 600-light-year distant Kepler-22b, at 2.4 times the radius of Earth, is potentially the closest match to Earth in terms of both size and temperature.
Since 1992, hundreds of planets around other stars ("extrasolar planets" or "exoplanets") in the Milky Way Galaxy have been discovered. As of none }}, the Extrasolar Planets Encyclopaedia identified 1919 extrasolar planets (in 1212 planetary systems and 482 multiple planetary systems); the extrasolar planets range in size from that of terrestrial planets similar to Earth to that of gas giants larger than Jupiter. The number of observed exoplanets is expected to increase greatly in the coming years. Because the "Kepler" spacecraft must view three stellar transits by exoplanets before it identifies them as candidate planets, it has so far only been able to identify planets that orbit their star at a relatively quick rate. The mission is expected to continue until at least 2016, in which time astronomers expect to find many more exoplanet candidates.
Despite these successes, the transit method employed by the "Kepler" spacecraft requires that planetary orbits be at a small inclination to the line of sight of the observer. Due to this constraint, the probability of detecting a planet of Earth's size and orbital radius around a distant star is just 0.47%. Thus, the number of planets that may be detected by means of the transit method is only a small fraction of the total number of planets present within the galaxy.
The Drake equation.
In 1961, University of California, Santa Cruz, astronomer and astrophysicist Dr. Frank Drake devised the Drake equation. This controversial equation multiplied estimates of the following terms together:
Criticism of the Drake equation follows mostly from the observation that the terms in the equation are entirely based on conjecture. Thus the equation cannot be used to draw firm conclusions of any kind. Although the Drake equation currently involves speculation about unmeasured parameters, it was not meant to be science, but intended as a way to stimulate dialogue on these topics. Then the focus becomes how to proceed experimentally. Indeed, Drake originally formulated the equation merely as an agenda for discussion at the Green Bank conference.
Drake used the equation to estimate that there are approximately 10,000 planets in the Milky Way containing intelligent life with the possible capability of communicating with Earth.
Based on observations from the Hubble Space Telescope, there are at least 125 billion galaxies in the observable Universe. It is estimated that at least ten percent of all Sun-like stars have a system of planets, i.e. there are 6.25×1018 stars with planets orbiting them in the observable Universe. Even if we assume that only one out of a billion of these stars have planets supporting life, there would be some 6.25×109 (billion) life-supporting planetary systems in the observable Universe.
The apparent contradiction between high estimates of the probability of the existence of extraterrestrial civilizations and the lack of evidence for, or contact with, such civilizations is known as the Fermi paradox.
It has been suggested that the frequency of gamma-ray bursts as a function of the metallicity of a galaxy means that only in the past five billion years the conditions have been right for complex life to arise.
Cultural impact.
Ancient and medieval ideas.
In antiquity, it was common to assume a cosmos consisting of "many worlds" inhabited by intelligent, non-human life-forms, but these "worlds" were mythological and not informed by the heliocentric understanding of the Solar System, or the understanding of the Sun as one among countless stars. An example would be the fourteen Loka of Hindu cosmology, or the Nine Worlds of Old Norse mythology, etc.
The Sun and the Moon often appear as inhabited worlds in such contexts, or as vehicles (chariots or boats, etc.) driven by gods. The Japanese folk tale of "The Tale of the Bamboo Cutter" (10th century) is an example of a princess of the Moon people visiting Earth.
Buddhist and Hindu beliefs of endlessly repeated cycles of life called Samsara have led to descriptions of multiple worlds in existence and their mutual contacts (Sanskrit word "sampark" (सम्पर्क) means "contact" as in Mahasamparka (महासम्पर्क) = "the great contact"). According to Buddhist and Hindu scriptures, there are numerous universes.
The Jewish Talmud states that there are at least 18,000 other worlds, but provides little elaboration on the nature of those worlds, or on whether they are physical or spiritual. Based on this, however, the 18th-century exposition "Sefer HaB'rit" posits that extraterrestrial creatures exist, and that some may well possess intelligence. It adds that humans should not expect creatures from another world to resemble earthly life any more than sea creatures resemble land animals.
According to Ahmadiyya a more direct reference from the Quran is presented by Mirza Tahir Ahmad as a proof that life on other planets may exist according to the Quran. In his book, "Revelation, Rationality, Knowledge & Truth", he quotes verse 42:29 "And among His Signs is the creation of the heavens and the earth, and of whatever living creatures ("da'bbah") He has spread forth in both..."; according to this verse there is life in heavens. According to the same verse "And He has the power to gather them together ("jam-'i-him") when He will so please"; indicates the bringing together the life on Earth and the life elsewhere in the Universe. The verse does not specify the time or the place of this meeting but rather states that this event will most certainly come to pass whenever God so desires. It should be pointed out that the Arabic term Jam-i-him used to express the gathering event can imply either a physical encounter or a contact through communication.
When Christianity spread throughout the West, the Ptolemaic system became very widely accepted, and although the Church never issued any formal pronouncement on the question of alien life at least tacitly, the idea was aberrant. In 1277, the Bishop of Paris, Étienne Tempier, did overturn Aristotle on one point: God "could" have created more than one world (given His omnipotence). Taking a further step, and arguing that aliens actually existed, remained rare. Notably, Cardinal Nicholas of Kues speculated about aliens on the Moon and Sun. William Vorilong also speculated about the existence of humans on alien worlds, but he came to the conclusion that God, although empowered to create them, would choose to not do so.
Early modern period.
There was a dramatic shift in thinking initiated by the invention of the telescope and the Copernican assault on geocentric cosmology. Once it became clear that Earth was merely one planet amongst countless bodies in the universe, the theory of extraterrestrial life started to become a topic in the scientific community. The best known early-modern proponent of such ideas was the Italian philosopher Giordano Bruno, who argued in the 16th century for an infinite Universe in which every star is surrounded by its own planetary system. Bruno wrote that other worlds "have no less virtue nor a nature different to that of our earth" and, like Earth, "contain animals and inhabitants".
In the early 17th century, the Czech astronomer Anton Maria Schyrleus of Rheita mused that "if Jupiter has (...) inhabitants (...) they must be larger and more beautiful than the inhabitants of Earth, in proportion to the [characteristics] of the two spheres".
In Baroque literature such as "The Other World: The Societies and Governments of the Moon" by Cyrano de Bergerac, extraterrestrial societies are presented as humoristic or ironic parodies of earthly society.
The didactic poet Henry More took up the classical theme of the Greek Democritus in "Democritus Platonissans, or an Essay Upon the Infinity of Worlds" (1647).
In "The Creation: a Philosophical Poem in Seven Books" (1712), Sir Richard Blackmore observed: "We may pronounce each orb sustains a race / Of living things adapted to the place". With the new relative viewpoint that the Copernican revolution had wrought, he suggested "our world's sunne / Becomes a starre elsewhere". Fontanelle's "Conversations on the Plurality of Worlds" (translated into English in 1686) offered similar excursions on the possibility of extraterrestrial life, expanding, rather than denying, the creative sphere of a Maker.
The possibility of extraterrestrials remained a widespread speculation as scientific discovery accelerated. William Herschel, the discoverer of Uranus, was one of many 18th–19th-century astronomers convinced that the Solar System, and perhaps others, would be well populated by alien life. Other luminaries of the period who championed "cosmic pluralism" included Immanuel Kant and Benjamin Franklin. At the height of the Enlightenment, even the Sun and Moon were considered candidates for extraterrestrial inhabitants.
19th century.
Since the 1830s, Mormons have believed that God has created and will create many Earth-like planets on which humans live. In Mormon cosmology, all of these people are children of God. Joseph Smith, the founder of the Latter Day Saints movement, taught that God revealed this information to Moses and that the Creation account written by Moses corresponded only to "our" earth. There is no official doctrine relating to the location or commonality of these inhabited planets.
Speculation about life on Mars increased in the late 19th century, following telescopic observation of apparent Martian canal—which soon, however, turned out to be optical illusions. Despite this, in 1895, American astronomer Percival Lowell published his book "Mars," followed by "Mars and its Canals" in 1906, proposing that the canals were the work of a long-gone civilization. This idea led British writer H. G. Wells to write "The War of the Worlds" in 1897, telling of an invasion by aliens from Mars who were fleeing the planet's desiccation.
Spectroscopic analysis of Mars's atmosphere began in earnest in 1894, when U.S. astronomer William Wallace Campbell showed that neither water nor oxygen was present in the Martian atmosphere.
By 1909 better telescopes and the best perihelic opposition of Mars since 1877 conclusively put an end to the canal hypothesis.
The science fiction genre, although not so named during the time, develops during the late 19th century. Jules Verne's "Around the Moon" (1870) features a discussion of the possibility of life on the Moon, but with the conclusion that it is barren.
Stories involving extraterrestrials are found in e.g. Garrett P. Serviss's "Edison's Conquest of Mars" (1898), an unauthorized sequel to 
"The War of the Worlds" by H. G. Wells was published in 1897 which stands at the beginning of the popular idea of the "Martian invasion" of Earth prominent in 20th-century pop culture.
20th century.
 A radio drama version of Wells' novel broadcast in 1938 over the CBS Radio Network led to outrage because it supposedly suggested to many listeners that an actual alien invasion by Martians was in progress.
In the wake of the Roswell UFO incident in 1947, conspiracy theories on the presence of extraterrestrials became a widespread phenomenon in the United States during the 1940s and the beginning Space Age during the 1950s, accompanied by a surge of UFO reports. The term UFO itself was coined in 1952 in the context of the enormous popularity of the concept of "flying saucers" in the wake of the Kenneth Arnold UFO sighting in 1947.
The trend to assume that celestial bodies were populated almost by default was tempered as actual probes visited potential alien abodes in the Solar System beginning in the second half of the 20th century, and by the 1970s belief in UFOs had become part of the fringe beliefs associated with the paranormal, New Age, Earth mysteries, Forteana etc. A number of UFO religions developed during the surge in UFO belief during the 1950s to 1970s period, and some, such as Scientology (founded 1953) and Raëlism (founded 1974) remain active into the present. The idea of "paleocontact", supposing that extraterrestrials ("ancient astronauts") have visited Earth in the remote past and left traces in ancient cultures, appears in early-20th-century fiction such as "The Call of Cthulhu" (1926) and the idea came to be established as a notable aspect of the Ufology subculture in the wake of Erich von Däniken's "Chariots of the Gods?" (1968). Alien abduction claims were widespread during the 1960s and 1970s in the United States.
On the scientific side, the possibility of extraterrestrial life on the Moon was decisively ruled out by the 1960s, and during the 1970s it became clear that most of the other bodies of the Solar System do not harbour highly developed life, although the question of primitive life on bodies in the Solar System remains an open question. Carl Sagan, Bruce Murray, and Louis Friedman founded the U.S. Planetary Society, partly as a vehicle for SETI studies in 1980, and since the 1990s, systematic search for radio signals attributable to intelligent extraterrestrial life has been ongoing.
In the early 1990s, NASA was set to join in on SETI research with a planned targeted search and all-sky survey. However, senator Richard Bryan of Nevada cut funding for the project, and no comparable search has taken place since.
Recent history.
The failure so far of the SETI program to detect an intelligent radio signal after decades of effort, has at least partially dimmed the prevailing optimism of the beginning of the space age. Notwithstanding, belief in extraterrestrial beings continues to be voiced in pseudoscience, conspiracy theories, and in popular folklore, notably "Area 51" and legends. It has become a pop culture trope given less-than-serious treatment in popular entertainment with e.g. the ALF TV series (1986–1990), "The X-Files" (1993–2002), etc.
The SETI program is not the result of a continuous, dedicated search, but instead utilizes what resources and manpower it can, when it can. Furthermore, the SETI program only searches a limited range of frequencies at any one time.
In the words of SETI's Frank Drake, "All we know for sure is that the sky is not littered with powerful microwave transmitters". Drake noted that it is entirely possible that advanced technology results in communication being carried out in some way other than conventional radio transmission. At the same time, the data returned by space probes, and giant strides in detection methods, have allowed science to begin delineating habitability criteria on other worlds, and to confirm that at least other planets are plentiful, though aliens remain a question mark. The Wow! signal, detected in 1977 by a SETI project, remains a subject of speculative debate.
In 2000, geologist and paleontologist Peter Ward and astrobiologist Donald Brownlee published a book entitled "Rare Earth: Why Complex Life is Uncommon in the Universe". In it, they discussed the Rare Earth hypothesis, in which they claim that Earth-like life is rare in the Universe, whereas microbial life is common. Ward and Brownlee are open to the idea of evolution on other planets that is not based on essential Earth-like characteristics (such as DNA and carbon).
The possible existence of primitive (microbial) life outside Earth is much less controversial to mainstream scientists, although, at present, no direct evidence of such life has been found. Indirect evidence has been offered for the current existence of primitive life on Mars. However, the conclusions that should be drawn from such evidence remain in debate.
In September 2010, it was reported that the U.N. General Assembly had appointed Mazlan Othman as their official extraterrestrial liaison by the UK paper "The Sunday Times". This claim was later refuted.
Theoretical physicist Stephen Hawking in 2010 warned that humans should not try to contact alien life forms. He warned that aliens might pillage Earth for resources. "If aliens visit us, the outcome would be much as when Columbus landed in America, which didn't turn out well for the Native Americans", he said. Jared Diamond has expressed similar concerns. Scientists at NASA and Penn State University published a paper in April 2011 addressing the question "Would contact with extraterrestrials benefit or harm humanity?" The paper describes positive, negative and neutral scenarios.
In 2011, Richard Hoover, an astrobiologist at the U.S. Space Flight Center in Alabama, claimed that filaments and other structures in rare meteorites appear to be microscopic fossils of extraterrestrial beings that resemble cyanobacteria—a phylum of photosynthetic bacteria.
On May 9, 2013, a by two U. S. House of Representatives subcommittees discussed "", prompted by the discovery of exoplanet Kepler-62f, along with Kepler-62e and Kepler-62c. A related of the journal Science, published earlier, described the discovery of the exoplanets.
On 17 April 2014, the discovery of the Earth-size exoplanet Kepler-186f, far 500 light years from Earth, was publicly announced; it is the first Earth-size planet to be discovered in the habitable zone and it has been hypothesized that there may be presence of water in its surface.
Further reading.
</dl>

</doc>
<doc id="9589" url="http://en.wikipedia.org/wiki?curid=9589" title="European Strategic Program on Research in Information Technology">
European Strategic Program on Research in Information Technology

European Strategic Program on Research in Information Technology (ESPRIT) was a series of integrated programmes of information technology research and development projects and industrial technology transfer measures. It was a European Union initiative managed by the Directorate General for Industry (DG III) of the European Commission. Five ESPRIT programmes (ESPRIT 0 to ESPRIT 4) ran consecutively from 1983 to 1998. ESPRIT 4 was succeeded by the Information Society Technologies (IST) programme in 1999.
Some of the projects and products supported by ESPRIT are: 

</doc>
<doc id="9591" url="http://en.wikipedia.org/wiki?curid=9591" title="E. E. Cummings">
E. E. Cummings

Edward Estlin Cummings (October 14, 1894 – September 3, 1962), known as E. E. Cummings, with the abbreviated form of his name often written by others in lowercase letters as e e cummings (in the style of some of his poems—see name and capitalization, below), was an American poet, painter, essayist, author, and playwright. His body of work encompasses approximately 2,900 poems, two autobiographical novels, four plays and several essays, as well as numerous drawings and paintings. He is remembered as an eminent voice of 20th century English literature.
Life.
<poem>
i thank You God for most this amazing
day:for the leaping greenly spirits of trees
and a blue true dream of sky; and for everything
which is natural which is infinite which is yes 
</poem>
From "i thank You God for most this amazing" (1950)
Early years.
Edward Estlin Cummings was born to Robert Cummings and Whelma Haswell Clarke, who were Unitarian. He exhibited transcendental leanings his entire life. As he grew in maturity and age, Cummings moved more toward an "I, Thou" relationship with God. His journals are replete with references to "le bon Dieu," as well as prayers for inspiration in his poetry and artwork (such as “Bon Dieu! may I some day do something truly great. amen.”). Cummings "also prayed for strength to be his essential self ('may I be I is the only prayer—not may I be great or good or beautiful or wise or strong'), and for relief of spirit in times of depression ('almighty God! I thank thee for my soul; & may I never die spiritually into a mere mind through disease of loneliness')."
Cummings wanted to be a poet from childhood and wrote poetry daily aged 8 to 22, exploring assorted forms. He went to Harvard and developed an interest in modern poetry which ignored conventional grammar and syntax, aiming for a dynamic use of language. Upon graduating, he worked for a book dealer.
The war years.
In 1917, with the First World War ongoing in Europe, Cummings enlisted in the Norton-Harjes Ambulance Corps, along with his college friend John Dos Passos. Due to an administrative mix-up, Cummings was not assigned to an ambulance unit for five weeks, during which time he stayed in Paris. He fell in love with the city, to which he would return throughout his life.
During their service in the ambulance corps, they sent letters home that drew the attention of the military censors, and were known to prefer the company of French soldiers over fellow ambulance drivers. The two openly expressed anti-war views; Cummings spoke of his lack of hatred for the Germans. On September 21, 1917, just five months after his belated assignment, he and a friend, William Slater Brown, were arrested by the French military on suspicion of espionage and undesirable activities. They were held for 3½ months in a military detention camp at the "Dépôt de Triage", in La Ferté-Macé, Orne, Normandy.
They were imprisoned with other detainees in a large room. Cummings' father failed to obtain his son's release through diplomatic channels and in December 1917 wrote a letter to President Wilson. Cummings was released on December 19, 1917, and Brown was released two months later. Cummings used his prison experience as the basis for his novel, "The Enormous Room" (1922), about which F. Scott Fitzgerald said, "Of all the work by young men who have sprung up since 1920 one book survives—"The Enormous Room" by e e cummings... Those few who cause books to live have not been able to endure the thought of its mortality." 
Cummings returned to the United States on New Year's Day 1918. Later in 1918 he was drafted into the army. He served in the 12th Division at Camp Devens, Massachusetts, until November 1918.
<poem>
Buffalo Bill's
defunct
 who used to
 ride a watersmooth-silver
 stallion
and break onetwothreefourfive pigeonsjustlikethat
 Jesus
he was a handsome man
 and what i want to know is
how do you like your blueeyed boy
Mister Death
</poem>
From "Buffalo Bill's" (1920)
Post-war years.
Cummings returned to Paris in 1921 and remained there for two years before returning to New York. His collection "Tulips and Chimneys" came in 1923 and his inventive use of grammar and syntax is evident. The book was heavily cut by his editor. "XLI Poems", was then published in 1925. With these collections Cummings made his reputation as an avant garde poet.
During the rest of the 1920s and 1930s Cummings returned to Paris a number of times, and traveled throughout Europe, meeting, among others, Pablo Picasso. In 1931 Cummings traveled to the Soviet Union, recounting his experiences in "Eimi", published two years later. During these years Cummings also traveled to Northern Africa and Mexico and worked as an essayist and portrait artist for "Vanity Fair" magazine (1924–1927).
In 1926, Cummings's parents were in a car accident; his father was killed and his mother survived, although severely injured. Cummings later described the accident in the following passage from his "i: six nonlectures" series given at Harvard (as part of the Charles Eliot Norton Lectures) in 1952 and 1953:
A locomotive cut the car in half, killing my father instantly. When two brakemen jumped from the halted train, they saw a woman standing – dazed but erect – beside a mangled machine; with blood spouting (as the older said to me) out of her head. One of her hands (the younger added) kept feeling her dress, as if trying to discover why it was wet. These men took my sixty-six year old mother by the arms and tried to lead her toward a nearby farmhouse; but she threw them off, strode straight to my father's body, and directed a group of scared spectators to cover him. When this had been done (and only then) she let them lead her away.
His father's death had a profound effect on Cummings, who entered a new period in his artistic life. He began to focus on more important aspects of life in his poetry. He started this new period by paying homage to his father in the poem "my father moved through dooms of love"
In the 1930s Samuel Aiwaz Jacobs was Cummings' publisher; he had started the Golden Eagle Press after working as a typographer and publisher.
Final years.
In 1952, his alma mater, Harvard University awarded Cummings an honorary seat as a guest professor. The Charles Eliot Norton Lectures he gave in 1952 and 1955 were later collected as "i: six nonlectures".
Cummings spent the last decade of his life traveling, fulfilling speaking engagements, and spending time at his summer home, Joy Farm, in Silver Lake, New Hampshire. He died of a stroke on September 3, 1962, at the age of 67 in North Conway, New Hampshire at the Memorial Hospital. His cremated remains were buried in Lot 748 Althaeas Path, in Section 6, Forest Hills Cemetery and Crematory in Boston. In 1969, his third wife, model and photographer Marion Morehouse Cummings, died and was buried in an adjoining plot.
Cummings's papers are held at the Houghton Library at Harvard University and the Harry Ransom Center at the University of Texas at Austin.
Personal life.
Marriages.
Cummings was married briefly twice; his longest relationship was outside marriage. Cummings's first marriage, to Elaine Orr, began as a love affair in 1918 while she was married to Scofield Thayer, one of Cummings's friends from Harvard. During this time he wrote a good deal of his erotic poetry. The couple had a daughter together, Nancy, born on December 20, 1919. Nancy was Cummings's only child.
After divorcing Thayer, Elaine married Cummings on March 19, 1924. However, they separated after two months and divorced less than nine months later. Elaine left Cummings for a wealthy Irish banker, moved to Ireland, and took Nancy with her. Under the terms of the divorce Cummings was granted custody of Nancy for three months each year, but Elaine refused to abide by the agreement. Cummings did not see his daughter again until 1946. Nancy later married Joseph Willard Roosevelt, second son of Kermit Roosevelt and Belle Wyatt Willard.
Cummings married his second wife Anne Minnerly Barton on May 1, 1929, and they separated three years later in 1932. That same year, Anne obtained a Mexican divorce; it was not officially recognized in the United States until August 1934.
The year Cummings and Anne separated, he met Marion Morehouse, a fashion model and photographer. Although it is not clear whether the two were ever formally married, Morehouse lived with Cummings in a common-law marriage until his death in 1962. She died on May 18, 1969, while living at 4 Patchin Place, Greenwich Village, New York City, where Cummings had resided since September 8, 1924.
Political views.
According to his testimony in "EIMI", Cummings had little interest in politics until his trip to the Soviet Union in 1931, after which he shifted rightward on many political and social issues. Despite his radical and bohemian public image, he was a Republican and, later, an ardent supporter of Joseph McCarthy.
Work.
Poetry.
Despite Cummings's familiarity with avant-garde styles (undoubtedly affected by the "Calligrammes" of Apollinaire, according to a contemporary observation), much of his work is quite traditional. Many of his poems are sonnets, albeit often with a modern twist, and he occasionally made use of the blues form and acrostics. Cummings' poetry often deals with themes of love and nature, as well as the relationship of the individual to the masses and to the world. His poems are also often rife with satire.
While his poetic forms and themes share an affinity with the romantic tradition, Cummings' work universally shows a particular idiosyncrasy of syntax, or way of arranging individual words into larger phrases and sentences. Many of his most striking poems do not involve any typographical or punctuation innovations at all, but purely syntactic ones.
<poem>
i carry your heart with me(i carry it in
my heart)i am never without it(anywhere
i go you go,my dear;and whatever is done
by only me is your doing,my darling)
 i fear
no fate(for you are my fate,my sweet)i want
no world(for beautiful you are my world,my true)
and it's you are whatever a moon has always meant
and whatever a sun will always sing is you 
</poem>
From "i carry your heart with me(i carry it in" (1952)
As well as being influenced by notable modernists, including Gertrude Stein and Ezra Pound, Cummings in his early work drew upon the imagist experiments of Amy Lowell. Later, his visits to Paris exposed him to Dada and surrealism, which he reflected in his work. He began to rely on symbolism and allegory where he once used simile and metaphor. In his later work, he rarely used comparisons that required objects that were not previously mentioned in the poem, choosing to use a symbol instead. Due to this, his later poetry is "frequently more lucid, more moving, and more profound than his earlier." Cummings also liked to incorporate imagery of nature and death into much of his poetry.
While some of his poetry is free verse (with no concern for rhyme or meter), many have a recognizable sonnet structure of 14 lines, with an intricate rhyme scheme. A number of his poems feature a typographically exuberant style, with words, parts of words, or punctuation symbols scattered across the page, often making little sense until read aloud, at which point the meaning and emotion become clear. Cummings, who was also a painter, understood the importance of presentation, and used typography to "paint a picture" with some of his poems.
The seeds of Cummings' unconventional style appear well established even in his earliest work. At age six, he wrote to his father:
Following his autobiographical novel, "The Enormous Room", Cummings' first published work was a collection of poems entitled "Tulips and Chimneys" (1923). This work was the public's first encounter with his characteristic eccentric use of grammar and punctuation.
Some of Cummings' most famous poems do not involve much, if any, odd typography or punctuation, but still carry his unmistakable style, particularly in unusual and impressionistic word order.
<poem>
anyone lived in a pretty how town
spring summer autumn winter
he sang his didn't he danced his did
Women and men (both little and small)
cared for anyone not at all
they sowed their isn't they reaped their same
sun moon stars rain
</poem>
From "anyone lived in a pretty how town" (1940)
Cummings' work often does not proceed in accordance with the conventional combinatorial rules that generate typical English sentences (for example, "they sowed their isn't"). His readings of Stein in the early part of the century probably served as a springboard to this aspect of his artistic development. In some respects, Cummings' work is more stylistically continuous with Stein's than with any other poet or writer.
In addition, a number of Cummings' poems feature, in part or in whole, intentional misspellings, and several incorporate phonetic spellings intended to represent particular dialects. Cummings also made use of inventive formations of compound words, as in "in Just" which features words such as "mud-luscious", "puddle-wonderful", and "eddieandbill." This poem is part of a sequence of poems entitled "Chansons Innocentes"; it has many references comparing the "balloonman" to Pan, the mythical creature that is half-goat and half-man. Literary critic R.P. Blackmur has commented that this usage of language is “frequently unintelligible because he disregards the historical accumulation of meaning in words in favour of merely private and personal associations.”
Many of Cummings' poems are satirical and address social issues but have an equal or even stronger bias toward romanticism: time and again his poems celebrate love, sex, and the season of rebirth.
Cummings also wrote children's books and novels. A notable example of his versatility is an introduction he wrote for a collection of the comic strip "Krazy Kat".
Controversy.
Cummings is also known for controversial subject matter, as he has a large collection of erotic poetry. In his 1950 collection "Xaipe: Seventy-One Poems", Cummings published two poems containing words that caused an outrage in some quarters.
</poem>
and
<poem>
</poem>
Cummings biographer Catherine Reef notes of the incident:
Friends begged Cummings to reconsider publishing these poems, and the book's editor pleaded with him to withdraw them, but he insisted that they stay. All the fuss perplexed him. The poems were commenting on prejudice, he pointed out, and not condoning it. He intended to show how derogatory words cause people to see others in terms of stereotypes rather than as individuals. "America (which turns Hungarian into 'hunky' & Irishman into 'mick' and Norwegian into 'square- head') is to blame for 'kike,'" he said.
But readers were still hurt, despite his commentary. Jews, living in the painful aftermath of the Holocaust, felt his very words were antisemitic, in spite of their purpose. William Carlos Williams spoke out in his defence.
Plays.
During his lifetime, Cummings published four plays. "HIM", a three-act play, was first produced in 1928 by the Provincetown Players in New York City. The production was directed by James Light. The play's main characters are "Him", a playwright, and "Me", his girlfriend. Cummings said of the unorthodox play:
Relax and give the play a chance to strut its stuff—relax, stop wondering what it is all 'about'—like many strange and familiar things, Life included, this play isn't 'about,' it simply is. . . . Don't try to enjoy it, let it try to enjoy you. "
"Anthropos, or the Future of Art" is a short, one-act play that Cummings contributed to the anthology "Whither, Whither or After Sex, What? A Symposium to End Symposium". The play consists of dialogue between Man, the main character, and three "infrahumans", or inferior beings. The word "anthropos" is the Greek word for "man", in the sense of "mankind".
"Tom, A Ballet" is a ballet based on "Uncle Tom's Cabin". The ballet is detailed in a "synopsis" as well as descriptions of four "episodes", which were published by Cummings in 1935. It has never been performed.
"" was probably Cummings's most successful play. It is an allegorical Christmas fantasy presented in one act of five scenes. The play was inspired by his daughter Nancy, with whom he was reunited in 1946. It was first published in the Harvard College magazine the "Wake". The play's main characters are Santa Claus, his family (Woman and Child), Death, and Mob. At the outset of the play, Santa Claus's family has disintegrated due to their lust for knowledge (Science). After a series of events, however, Santa Claus's faith in love and his rejection of the materialism and disappointment he associates with Science are reaffirmed, and he is reunited with Woman and Child.
Name and capitalization.
Cummings's publishers and others have sometimes echoed the unconventional orthography in his poetry by writing his name in lowercase and without periods (full stops), but normal orthography (uppercase and full stops) is supported by scholarship and preferred by publishers today. Cummings himself used both the lowercase and capitalized versions, though he most often signed his name with capitals.
The use of lowercase for his initials was popularized in part by the title of some books, particularly in the 1960s, printing his name in lower case on the cover and spine. In the preface to "E. E. Cummings: The Growth of a Writer" by Norman Friedman, critic Harry T. Moore notes, "He [Cummings] had his name put legally into lower case, and in his later books the titles and his name were always in lower case." According to Cummings's widow, however, this is incorrect. She wrote to Friedman: "you should not have allowed H. Moore to make such a stupid & childish statement about Cummings & his signature." On February 27, 1951, Cummings wrote to his French translator D. Jon Grossman that he preferred the use of upper case for the particular edition they were working on. One Cummings scholar believes that on the rare occasions Cummings signed his name in all lowercase, he may have intended it as a gesture of humility, not as an indication that it was the preferred orthography for others to use.
Adaptations.
In 1943, modern dancer and choreographer Jean Erdman presented "The Transformations of Medusa, Forever and Sunsmell" with a commissioned score by John Cage and a spoken text from the titular poem by E. E. Cummings, sponsored by the Arts Club of Chicago. Erdman also choreographed "Twenty Poems" (1960), a cycle of E. E. Cummings's poems for eight dancers and one actor with a commissioned score by Teiji Ito, performed in the round at the Circle in the Square Theatre in Greenwich Village.
In 1961, Pierre Boulez composed "Cummings ist der dichter" from poems by E.E. Cummings.
Aribert Reimann set Cummings to music in "Impression IV" (1961) for soprano and piano.
The Icelandic singer Björk used lines from Cummings's poem "I Will Wade Out" for the lyrics of "Sun In My Mouth" on her 2001 album "Vespertine." On her next album, "Medúlla" (2004), Björk used his poem "It May Not Always Be So" as the lyrics for the song "Sonnets/Unrealities XI."
The 2007 Ra Ra Riot song, "Dying Is Fine," is based on the Cummings' poem "dying is fine)but death".
The Bloc Party song, "Ion Square", on their album "Intimacy", contains lines from Cummings' poem "I carry your heart with me".
The American composer Eric Whitacre wrote a cycle of works for choir entitled "The City and the Sea", which consists of five poems by Cummings set to music.
Numerous composers have set Cummings' poems to music. Among them are Dominic Argento, Gary Backlund, William Bergsma, Leonard Bernstein, Allen Blank, Marc Blitzstein, John Cage, Romeo Cascarino, Aaron Copland, Serge de Gastyne, David Diamond, John Duke, Brian Fennelly, Margaret Garwood, John Gruen, Daron Hagen, Richard Hundley, Barbara Kolb, Robert Manno, Salvatore Martirano, John Musto, Paul Nordoff, Tobias Picker, Vincent Persichetti, Ned Rorem, Peter Schickele, Elie Siegmeister, Hugo Weisgall, Dan Welcher, Eric Whitacre, and James Yannatos, among many others.
Awards.
During his lifetime, Cummings received numerous awards in recognition of his work, including:

</doc>
<doc id="9592" url="http://en.wikipedia.org/wiki?curid=9592" title="East River">
East River

The East River is a salt water tidal strait in New York City. The waterway, which is actually not a river despite its name, connects Upper New York Bay on its south end to Long Island Sound on its north end. It separates Long Island – including the boroughs of Queens and Brooklyn – from the Bronx on the North American mainland, and the island of Manhattan. Because of its connection to Long Island Sound, it was once also known as the "Sound River". The tidal strait changes its flow direction frequently.
Formation.
The strait was formed approximately 11,000 years ago at the end of the Wisconsin glaciation. The distinct change in the shape of the strait between the lower and upper portions is evidence of this glacial activity. The upper portion (from Long Island Sound to Hell Gate), running largely perpendicular to the glacial motion, is wide, meandering, and has deep narrow bays on both banks, scoured out by the glacier's movement. The lower portion (from Hell Gate to New York Bay) runs north-south, parallel to the glacial motion. It is much narrower, with straight banks. The bays that exist (or existed before being filled in by human activity), are largely wide and shallow.
The channel.
Historically, the lower portion of the strait (separating Manhattan from Brooklyn) was one of the busiest and most important channels in the world, particularly during the first three centuries of New York City's history. The Brooklyn Bridge, opened in 1883, was the first bridge to span the strait, replacing frequent ferry service. Some passenger ferry service remains between Manhattan, and Queens and Brooklyn.
Due to heavy pollution, the East River is dangerous to people who fall in or attempt to swim in it, although as of mid-2007 the water was cleaner than it had been in decades. As of 2010, the New York City Department of Environmental Protection categorizes the East River as Use Classification I, meaning it is safe for secondary contact activities such as boating and fishing. According to the marine sciences section of the city Department of Environmental Protection, the channel is swift, with water moving as fast as four knots (just as it does in the Hudson River on the other side of Manhattan). That speed can push casual swimmers out to sea. A few people drown in the waters around New York City each year. The strength of the current foiled an effort in 2007 to tap it for tidal power. However, in February 2012 the federal government announced an agreement with Verdant Power to install 30 tidal turbines in the channel, projected to begin operations in 2015 and produce 1.05 MW of power.
Tributaries.
The Bronx River drains into the East River in the northern section of the strait.
North of Randalls Island, it is joined by the Bronx Kill. Along the east of Wards Island, at approximately the strait's midpoint, it narrows into a channel called Hell Gate, which is spanned by both the Robert F. Kennedy Bridge (formerly the Triborough), and the Hell Gate Bridge. On the south side of Wards Island, it is joined by the Harlem River.
Newtown Creek on Long Island drains into the East River, forming part of the boundary between Queens and Brooklyn.
The East River contains a number of islands, including:
In popular culture.
Music
Television
Games
References.
Notes

</doc>
<doc id="9593" url="http://en.wikipedia.org/wiki?curid=9593" title="Existentialism">
Existentialism

Existentialism () is a term applied to the work of certain late 19th- and 20th-century philosophers who, despite profound doctrinal differences, shared the belief that philosophical thinking begins with the human subject—not merely the thinking subject, but the acting, feeling, living human individual. In existentialism, the individual's starting point is characterized by what has been called "the existential attitude", or a sense of disorientation and confusion in the face of an apparently meaningless or absurd world. Many existentialists have also regarded traditional systematic or academic philosophies, in both style and content, as too abstract and remote from concrete human experience.
Søren Kierkegaard is generally considered to have been the first existentialist philosopher, though he did not use the term existentialism. He proposed that each individual—not society or religion—is solely responsible for giving meaning to life and living it passionately and sincerely ("authentically"). Existentialism became popular in the years following World War II, and strongly influenced many disciplines besides philosophy, including theology, drama, art, literature, and psychology.
Definitional issues and background.
There has never been general agreement on the definition of existentialism. The term is often seen as a historical convenience as it was first applied to many philosophers in hindsight, long after they had died. In fact, while existentialism is generally considered to have originated with Kierkegaard, the first prominent existentialist philosopher to adopt the term as a self-description was Jean-Paul Sartre. Sartre posits the idea that "what all existentialists have in common is the fundamental doctrine that existence precedes essence", as scholar Frederick Copleston explains. According to philosopher Steven Crowell, defining existentialism has been relatively difficult, and he argues that it is better understood as a general approach used to reject certain systematic philosophies rather than as a systematic philosophy itself. Sartre himself, in a lecture delivered in 1945, described existentialism as "the attempt to draw all the consequences from a position of consistent atheism".
Although many outside Scandinavia consider the term existentialism to have originated from Kierkegaard himself, it is more likely that Kierkegaard adopted this term (or at least the term "existential" as a description of his philosophy) from the Norwegian poet and literary critic Johan Sebastian Cammermeyer Welhaven. This assertion comes from two sources. The Norwegian philosopher Erik Lundestad refers to the Danish philosopher Fredrik Christian Sibbern. Sibbern is supposed to have had two conversations in 1841, the first with Welhaven and the second with Kierkegaard. It is in the first conversation that it is believed that Welhaven came up with "a word that he said covered a certain thinking, which had a close and positive attitude to life, a relationship he described as existential". This was then brought to Kierkegaard by Sibbern.
The second claim comes from the Norwegian historian Rune Slagstad, who claims to prove that Kierkegaard himself said the term "existential" was borrowed from the poet. He strongly believes that it was Kierkegaard himself who said that "Hegelians do not study philosophy 'existentially'; to use a phrase by Welhaven from one time when I spoke with him about philosophy". On the other hand, the Norwegian historian Anne-Lise Seip is critical of Slagstad, and believes the statement in fact stems from the Norwegian literary historian Cathrinus Bang.
There also exists the belief that meaninglessness and absurdity create a behavior pattern that is not consistent with that which is considered "normal". In other words, existentialism "jars you out of your habits." Like war, sexual disease, and the like, the individual consciousness is paramount to the societal impact one may have and it is your reality that dictates your actions, not anybody else's.
Concepts.
Existence precedes essence.
A central proposition of Existentialism is that "existence precedes essence", which means that the most important consideration for individuals is that they are individuals—independently acting and responsible, conscious beings ("existence")—rather than what labels, roles, stereotypes, definitions, or other preconceived categories the individuals fit ("essence"). The actual life of the individuals is what constitutes what could be called their "true essence" instead of there being an arbitrarily attributed essence others use to define them. Thus, human beings, through their own consciousness, create their own values and determine a meaning to their life. Although it was Sartre who explicitly coined the phrase, similar notions can be found in the thought of existentialist philosophers such as Heidegger, and Kierkegaard:
 "The subjective "thinker’s form", the form of his communication, is his "style". His form must be just as manifold as are the opposites that he holds together. The systematic "eins, zwei, drei" is an abstract form that also must inevitably run into trouble whenever it is to be applied to the concrete. To the same degree as the subjective thinker is concrete, to the same degree his form must also be concretely dialectical. But just as he himself is not a poet, not an ethicist, not a dialectician, so also his form is none of these directly. His form must first and last be related to existence, and in this regard he must have at his disposal the poetic, the ethical, the dialectical, the religious. Subordinate character, setting, etc., which belong to the well balanced character of the esthetic production, are in themselves breadth; the subjective thinker has only one setting—existence—and has nothing to do with localities and such things. The setting is not the fairyland of the imagination, where poetry produces consummation, nor is the setting laid in England, and historical accuracy is not a concern. The setting is inwardness in existing as a human being; the concretion is the relation of the existence-categories to one another. Historical accuracy and historical actuality are breadth." Søren Kierkegaard (Concluding Postscript, Hong p. 357–358)
It is often claimed in this context that people define themselves, which is often perceived as stating that they can wish to be something—anything, a bird, for instance—and then be it. According to most existentialist philosophers, however, this would constitute an inauthentic existence. Instead, the phrase should be taken to say that people are (1) defined only insofar as they act and (2) that they are responsible for their actions. For example, someone who acts cruelly towards other people is, by that act, defined as a cruel person. Furthermore, by this action of cruelty, such persons are themselves responsible for their new identity (cruel persons). This is as opposed to their genes, or "human nature", bearing the blame.
As Sartre writes in his work "Existentialism is a Humanism": "... man first of all exists, encounters himself, surges up in the world—and defines himself afterwards." Of course, the more positive, therapeutic aspect of this is also implied: A person can choose to act in a different way, and to be a good person instead of a cruel person. Here it is also clear that since humans can choose to be either cruel or good, they are, in fact, neither of these things essentially.
The Absurd.
The notion of the Absurd contains the idea that there is no meaning in the world beyond what meaning we give it. This meaninglessness also encompasses the amorality or "unfairness" of the world. This contrasts with the notion that "bad things don't happen to good people"; to the world, metaphorically speaking, there is no such thing as a good person or a bad person; what happens happens, and it may just as well happen to a "good" person as to a "bad" person.
Because of the world's absurdity, at any point in time, anything can happen to anyone, and a tragic event could plummet someone into direct confrontation with the Absurd. The notion of the absurd has been prominent in literature throughout history. Many of the literary works of Søren Kierkegaard, Samuel Beckett, Franz Kafka, Fyodor Dostoyevsky, Eugène Ionesco, Luigi Pirandello, Jean-Paul Sartre, Joseph Heller and Albert Camus contain descriptions of people who encounter the absurdity of the world.
It is in relation to the concept of the devastating awareness of meaninglessness that Albert Camus claimed that "there is only one truly serious philosophical problem, and that is suicide" in his "The Myth of Sisyphus". Although "prescriptions" against the possibly deleterious consequences of these kinds of encounters vary, from Kierkegaard's religious "stage" to Camus' insistence on persevering in spite of absurdity, the concern with helping people avoid living their lives in ways that put them in the perpetual danger of having everything meaningful break down is common to most existentialist philosophers. The possibility of having everything meaningful break down poses a threat of quietism, which is inherently against the existentialist philosophy. It has been said that the possibility of suicide makes all humans existentialists.
Facticity.
Facticity is a concept defined by Sartre in "Being and Nothingness" as the "in-itself", of which humans are in the mode of not being. This can be more easily understood when considering it in relation to the temporal dimension of past: one's past is what one is, in the sense that it co-constitutes oneself. However, to say that one is only one's past would be to ignore a significant part of reality (the present and the future), while saying that one's past is only what one was, would entirely detach it from oneself now. A denial of one's own concrete past constitutes an inauthentic lifestyle, and the same goes for all other kinds of facticity (having a body—e.g. one that doesn't allow a person to run faster than the speed of sound—identity, values, etc.).
Facticity is both a limitation and a condition of freedom. It is a limitation in that a large part of one's facticity consists of things one couldn't have chosen (birthplace, etc.), but a condition in the sense that one's values most likely depend on it. However, even though one's facticity is "set in stone" (as being past, for instance), it cannot determine a person: The value ascribed to one's facticity is still ascribed to it freely by that person. As an example, consider two men, one of whom has no memory of his past and the other remembers everything. They have both committed many crimes, but the first man, knowing nothing about this, leads a rather normal life while the second man, feeling trapped by his own past, continues a life of crime, blaming his own past for "trapping" him in this life. There is nothing essential about his committing crimes, but he ascribes this meaning to his past.
However, to disregard one's facticity when, in the continual process of self-making, one projects oneself into the future,that would be to put oneself in denial of oneself, and would thus be inauthentic. In other words, the origin of one's projection must still be one's facticity, though in the mode of not being it (essentially). Another aspect of facticity is that it entails angst, both in the sense that freedom "produces" angst when limited by facticity, and in the sense that the lack of the possibility of having facticity to "step in" for one to take responsibility for something one has done also produces angst.
Another aspect of existential freedom is that one can change one's values. Thus, one is responsible for one's values, regardless of society's values. The focus on freedom in existentialism is related to the limits of the responsibility one bears as a result of one's freedom: the relationship between freedom and responsibility is one of interdependency, and a clarification of freedom also clarifies that for which one is responsible.
Authenticity.
Many noted existentialist writers consider the theme of authentic existence important. Authentic existence involves the idea that one has to "create oneself" and then live in accordance with this self. What is meant by authenticity is that in acting, one should act as oneself, not as "one" acts or as "one's genes" or any other essence requires. The authentic act is one that is in accordance with one's freedom. Of course, as a condition of freedom is facticity, this includes one's facticity, but not to the degree that this facticity can in any way determine one's choices (in the sense that one could then blame one's background for making the choice one made). The role of facticity in relation to authenticity involves letting one's actual values come into play when one makes a choice (instead of, like Kierkegaard's Aesthete, "choosing" randomly), so that one also takes responsibility for the act instead of choosing either-or without allowing the options to have different values.
In contrast to this, the inauthentic is the denial to live in accordance with one's freedom. This can take many forms, from pretending choices are meaningless or random, through convincing oneself that some form of determinism is true, to a sort of "mimicry" where one acts as "one should." How "one" should act is often determined by an image one has of how one such as oneself (say, a bank manager, lion tamer, prostitute, etc.) acts. This image usually corresponds to some sort of social norm, but this does not mean that all acting in accordance with social norms is inauthentic: The main point is the attitude one takes to one's own freedom and responsibility, and the extent to which one acts in accordance with this freedom.
The Other and the Look.
The Other (when written with a capital "O") is a concept more properly belonging to phenomenology and its account of intersubjectivity. However, the concept has seen widespread use in existentialist writings, and the conclusions drawn from it differ slightly from the phenomenological accounts. The experience of the Other is the experience of another free subject who inhabits the same world as a person does. In its most basic form, it is this experience of the Other that constitutes intersubjectivity and objectivity. To clarify, when one experiences someone else, and this Other person experiences the world (the same world that a person experiences)--only from "over there"--the world itself is constituted as objective in that it is something that is "there" as identical for both of the subjects; a person experiences the other person as experiencing the same things. This experience of the Other's look is what is termed the Look (sometimes the Gaze).
While this experience, in its basic phenomenological sense, constitutes the world as objective, and oneself as objectively existing subjectivity (one experiences oneself as seen in the Other's Look in precisely the same way that one experiences the Other as seen by him, as subjectivity), in existentialism, it also acts as a kind of limitation of freedom. This is because the Look tends to objectify what it sees. As such, when one experiences oneself in the Look, one doesn't experience oneself as nothing (no thing), but as something. Sartre's own example of a man peeping at someone through a keyhole can help clarify this: at first, this man is entirely caught up in the situation he is in; he is in a pre-reflexive state where his entire consciousness is directed at what goes on in the room. Suddenly, he hears a creaking floorboard behind him, and he becomes aware of himself as seen by the Other. He is thus filled with shame for he perceives himself as he would perceive someone else doing what he was doing, as a Peeping Tom. The Look is then co-constitutive of one's facticity.
Another characteristic feature of the Look is that no Other really needs to have been there: It is quite possible that the creaking floorboard was nothing but the movement of an old house; the Look isn't some kind of mystical telepathic experience of the actual way the other sees one (there may also have been someone there, but he could have not noticed that the person was there). It is only one's perception of the way another might perceive him.
Angst and Dread.
"Existential angst", sometimes called dread, anxiety, or anguish, is a term that is common to many existentialist thinkers. It is generally held to be a negative feeling arising from the experience of human freedom and responsibility. The archetypal example is the experience one has when standing on a cliff where one not only fears falling off it, but also dreads the possibility of throwing oneself off. In this experience that "nothing is holding me back", one senses the lack of anything that predetermines one to either throw oneself off or to stand still, and one experiences one's own freedom.
It can also be seen in relation to the previous point how angst is before nothing, and this is what sets it apart from fear that has an object. While in the case of fear, one can take definitive measures to remove the object of fear, in the case of angst, no such "constructive" measures are possible. The use of the word "nothing" in this context relates both to the inherent insecurity about the consequences of one's actions, and to the fact that, in experiencing freedom as angst, one also realizes that one is fully responsible for these consequences. There is nothing in people (genetically, for instance) that acts in their stead—that they can blame if something goes wrong. Therefore, not every choice is perceived as having dreadful possible consequences (and, it can be claimed, human lives would be unbearable if every choice facilitated dread). However, this doesn't change the fact that freedom remains a condition of every action.
Despair.
Despair, in existentialism, is generally defined as a loss of hope. More specifically, it is a loss of hope in reaction to a breakdown in one or more of the defining qualities of one's self or identity. If a person is invested in being a particular thing, such as a bus driver or an upstanding citizen, and then finds his being-thing compromised, he would normally be found in state of despair — a hopeless state. For example, a singer who loses the ability to sing may despair if she has nothing else to fall back on—nothing to rely on for her identity. She finds herself unable to be what defined her being.
What sets the existentialist notion of despair apart from the conventional definition is that existentialist despair is a state one is in even when he isn't overtly in despair. So long as a person's identity depends on qualities that can crumble, he is in perpetual despair—and as there is, in Sartrean terms, no human essence found in conventional reality on which to constitute the individual's sense of identity, despair is a universal human condition. As Kierkegaard defines it in "Either/Or": "Let each one learn what he can; both of us can learn that a person’s unhappiness never lies in his lack of control over external conditions, since this would only make him completely unhappy." In "Works of Love", he said: When the God-forsaken worldliness of earthly life shuts itself in complacency, the confined air develops poison, the moment gets stuck and stands still, the prospect is lost, a need is felt for a refreshing, enlivening breeze to cleanse the air and dispel the poisonous vapors lest we suffocate in worldliness. ... Lovingly to hope all things is the opposite of despairingly to hope nothing at all. Love hopes all things – yet is never put to shame. To relate oneself expectantly to the possibility of the good is to hope. To relate oneself expectantly to the possibility of evil is to fear. By the decision to choose hope one decides infinitely more than it seems, because it is an eternal decision. p. 246-250
Opposition to positivism and rationalism.
Existentialists oppose definitions of human beings as primarily rational, and, therefore, oppose positivism and rationalism. Existentialism asserts that people actually make decisions based on subjective meaning rather than pure rationality. The rejection of reason as the source of meaning is a common theme of existentialist thought, as is the focus on the feelings of anxiety and dread that we feel in the face of our own radical freedom and our awareness of death. Kierkegaard advocated rationality as means to interact with the objective world (e.g. in the natural sciences), but when it comes to existential problems, reason is insufficient: "Human reason has boundaries".
Like Kierkegaard, Sartre saw problems with rationality, calling it a form of "bad faith", an attempt by the self to impose structure on a world of phenomena — "the Other" — that is fundamentally irrational and random. According to Sartre, rationality and other forms of bad faith hinder people from finding meaning in freedom. To try to suppress their feelings of anxiety and dread, people confine themselves within everyday experience, Sartre asserts, thereby relinquishing their freedom and acquiescing to being possessed in one form or another by "the Look" of "the Other" (i.e. possessed by another person — or at least one's idea of that other person).
Existentialism and religion.
An existentialist reading of the Bible would demand that the reader recognize that he is an existing subject studying the words more as a recollection of events. This is in contrast to looking at a collection of "truths" that are outside and unrelated to the reader, but may develop a sense of reality/God. Such a reader is not obligated to follow the commandments as if an external agent is forcing them upon him, but as though they are inside him and guiding him from inside. This is the task Kierkegaard takes up when he asks: "Who has the more difficult task: the teacher who lectures on earnest things a meteor's distance from everyday life-or the learner who should put it to use?"
Existentialism and nihilism.
Although nihilism and existentialism are distinct philosophies, they are often confused with one another. A primary cause of confusion is that Friedrich Nietzsche is an important philosopher in both fields, but also the existentialist insistence on the inherent meaninglessness of the world. Existentialist philosophers often stress the importance of Angst as signifying the absolute lack of any objective ground for action, a move that is often reduced to a moral or an existential nihilism. A pervasive theme in the works of existentialist philosophy, however, is to persist through encounters with the absurd, as seen in Camus' "The Myth of Sisyphus" ("One must imagine Sisyphus happy"), and it is only very rarely that existentialist philosophers dismiss morality or one's self-created meaning: Kierkegaard regained a sort of morality in the religious (although he wouldn't himself agree that it was ethical; the religious suspends the ethical), and Sartre's final words in "Being and Nothingness" are "All these questions, which refer us to a pure and not an accessory (or impure) reflection, can find their reply only on the ethical plane. We shall devote to them a future work."
Etymology.
The term "existentialism" was coined by the French Catholic philosopher Gabriel Marcel in the mid-1940s. At first, when Marcel applied the term to him at a colloquium in 1945, Jean-Paul Sartre rejected it. Sartre subsequently changed his mind and, on October 29, 1945, publicly adopted the existentialist label in a lecture to the "Club Maintenant" in Paris. The lecture was published as "L'existentialisme est un humanisme" (Existentialism is a Humanism), a short book that did much to popularize existentialist thought.
Some scholars argue that the term should be used only to refer to the cultural movement in Europe in the 1940s and 1950s associated with the works of the philosophers Jean-Paul Sartre, Simone de Beauvoir, Maurice Merleau-Ponty, and Albert Camus. Other scholars extend the term to Kierkegaard, and yet others extend it as far back as Socrates. However, the term is often identified with the philosophical views of Jean-Paul Sartre.
History.
19th century.
Kierkegaard and Nietzsche.
Søren Kierkegaard and Friedrich Nietzsche were two of the first philosophers considered fundamental to the existentialist movement, though neither used the term "existentialism" and it is unclear whether they would have supported the existentialism of the 20th century. They focused on subjective human experience rather than the objective truths of mathematics and science, which they believed were too detached or observational to truly get at the human experience. Like Pascal, they were interested in people's quiet struggle with the apparent meaninglessness of life and the use of diversion to escape from boredom. Unlike Pascal, Kierkegaard and Nietzsche also considered the role of making free choices, particularly regarding fundamental values and beliefs, and how such choices change the nature and identity of the chooser. Kierkegaard's knight of faith and Nietzsche's Übermensch are representative of people who exhibit Freedom, in that they define the nature of their own existence. Nietzsche's idealized individual invents his own values and creates the very terms they excel under. By contrast, Kierkegaard, opposed to the level of abstraction in Hegel, and not nearly as hostile (actually welcoming) to Christianity as Nietzsche, argues through a pseudonym that the objective certainty of religious truths (specifically Christian) is not only impossible, but even founded on logical paradoxes. Yet he continues to imply that a leap of faith is a possible means for an individual to reach a higher stage of existence that transcends and contains both an aesthetic and ethical value of life. Kierkegaard and Nietzsche were also precursors to other intellectual movements, including postmodernism, and various strands of psychology. However, Kierkegaard believed that individuals should live in accordance with their thinking.
Dostoyevsky.
The first important literary author also important to existentialism was the Russian Fyodor Dostoyevsky. Dostoyevsky's "Notes from Underground" portrays a man unable to fit into society and unhappy with the identities he creates for himself. Jean-Paul Sartre, in his book on existentialism "Existentialism is a Humanism", quoted Dostoyevsky's "The Brothers Karamazov" as an example of existential crisis. Sartre attributes Ivan Karamazov's claim, "If God did not exist, everything would be permitted" to Dostoyevsky himself, though this quote does not appear in the novel. Other Dostoyevsky novels covered issues raised in existentialist philosophy while presenting story lines divergent from secular existentialism: for example, in "Crime and Punishment", the protagonist Raskolnikov experiences an existential crisis and then moves toward a Christian Orthodox worldview similar to that advocated by Dostoyevsky himself.
Early 20th century.
In the first decades of the 20th century, a number of philosophers and writers explored existentialist ideas. The Spanish philosopher Miguel de Unamuno y Jugo, in his 1913 book "The Tragic Sense of Life in Men and Nations", emphasized the life of "flesh and bone" as opposed to that of abstract rationalism. Unamuno rejected systematic philosophy in favor of the individual's quest for faith. He retained a sense of the tragic, even absurd nature of the quest, symbolized by his enduring interest in Cervantes' fictional character Don Quixote. A novelist, poet and dramatist as well as philosophy professor at the University of Salamanca, Unamuno wrote a short story about a priest's crisis of faith, "Saint Manuel the Good, Martyr", which has been collected in anthologies of existentialist fiction. Another Spanish thinker, Ortega y Gasset, writing in 1914, held that human existence must always be defined as the individual person combined with the concrete circumstances of his life: "Yo soy yo y mi circunstancia" ("I am myself and my circumstances"). Sartre likewise believed that human existence is not an abstract matter, but is always situated, also many thought his plays were absurd ("en situación").
Although Martin Buber wrote his major philosophical works in German, and studied and taught at the Universities of Berlin and Frankfurt, he stands apart from the mainstream of German philosophy. Born into a Jewish family in Vienna in 1878, he was also a scholar of Jewish culture and involved at various times in Zionism and Hasidism. In 1938, he moved permanently to Jerusalem. His best-known philosophical work was the short book "I and Thou", published in 1922. For Buber, the fundamental fact of human existence, too readily overlooked by scientific rationalism and abstract philosophical thought, is "man with man", a dialogue that takes place in the so-called "sphere of between" ("das Zwischenmenschliche").
Two Ukrainian/Russian thinkers, Lev Shestov and Nikolai Berdyaev, became well known as existentialist thinkers during their post-Revolutionary exiles in Paris. Shestov, born into a Ukrainian-Jewish family in Kiev, had launched an attack on rationalism and systematization in philosophy as early as 1905 in his book of aphorisms "All Things Are Possible".
Berdyaev, also from Kiev but with a background in the Eastern Orthodox Church, drew a radical distinction between the world of spirit and the everyday world of objects. Human freedom, for Berdyaev, is rooted in the realm of spirit, a realm independent of scientific notions of causation. To the extent the individual human being lives in the objective world, he is estranged from authentic spiritual freedom. "Man" is not to be interpreted naturalistically, but as a being created in God's image, an originator of free, creative acts. He published a major work on these themes, "The Destiny of Man", in 1931.
Gabriel Marcel, long before coining the term "existentialism", introduced important existentialist themes to a French audience in his early essay "Existence and Objectivity" (1925) and in his "Metaphysical Journal" (1927). A dramatist as well as a philosopher, Marcel found his philosophical starting point in a condition of metaphysical alienation: the human individual searching for harmony in a transient life. Harmony, for Marcel, was to be sought through "secondary reflection", a "dialogical" rather than "dialectical" approach to the world, characterized by "wonder and astonishment" and open to the "presence" of other people and of God rather than merely to "information" about them. For Marcel, such presence implied more than simply being there (as one thing might be in the presence of another thing); it connoted "extravagant" availability, and the willingness to put oneself at the disposal of the other.
Marcel contrasted "secondary reflection" with abstract, scientific-technical "primary reflection", which he associated with the activity of the abstract Cartesian ego. For Marcel, philosophy was a concrete activity undertaken by a sensing, feeling human being incarnate — embodied — in a concrete world. Although Jean-Paul Sartre adopted the term "existentialism" for his own philosophy in the 1940s, Marcel's thought has been described as "almost diametrically opposed" to that of Sartre. Unlike Sartre, Marcel was a Christian, and became a Catholic convert in 1929.
In Germany, the psychologist and philosopher Karl Jaspers — who later described existentialism as a "phantom" created by the public — called his own thought, heavily influenced by Kierkegaard and Nietzsche, "Existenzphilosophie". For Jaspers, ""Existenz"-philosophy is the way of thought by means of which man seeks to become himself...This way of thought does not cognize objects, but elucidates and makes actual the being of the thinker."
Jaspers, a professor at the University of Heidelberg, was acquainted with Martin Heidegger, who held a professorship at Marburg before acceding to Husserl's chair at Freiburg in 1928. They held many philosophical discussions, but later became estranged over Heidegger's support of National Socialism. They shared an admiration for Kierkegaard, and in the 1930s, Heidegger lectured extensively on Nietzsche. Nevertheless, the extent to which Heidegger should be considered an existentialist is debatable. In "Being and Time" he presented a method of rooting philosophical explanations in human existence ("Dasein") to be analysed in terms of existential categories ("existentiale"); and this has led many commentators to treat him as an important figure in the existentialist movement.
After the Second World War.
Following the Second World War, existentialism became a well-known and significant philosophical and cultural movement, mainly through the public prominence of two French writers, Jean-Paul Sartre and Albert Camus, who wrote best-selling novels, plays and widely read journalism as well as theoretical texts. These years also saw the growing reputation of Heidegger's book "Being and Time" outside Germany.
Sartre dealt with existentialist themes in his 1938 novel "Nausea" and the short stories in his 1939 collection "The Wall", and had published his treatise on existentialism, "Being and Nothingness", in 1943, but it was in the two years following the liberation of Paris from the German occupying forces that he and his close associates — Camus, Simone de Beauvoir, Maurice Merleau-Ponty, and others — became internationally famous as the leading figures of a movement known as existentialism. In a very short space of time, Camus and Sartre in particular became the leading public intellectuals of post-war France, achieving by the end of 1945 "a fame that reached across all audiences." Camus was an editor of the most popular leftist (former French Resistance) newspaper "Combat"; Sartre launched his journal of leftist thought, "Les Temps Modernes", and two weeks later gave the widely reported lecture on existentialism and secular humanism to a packed meeting of the Club Maintenant. Beauvoir wrote that "not a week passed without the newspapers discussing us"; existentialism became "the first media craze of the postwar era."
By the end of 1947, Camus' earlier fiction and plays had been reprinted, his new play "Caligula" had been performed and his novel "The Plague" published; the first two novels of Sartre's "The Roads to Freedom" trilogy had appeared, as had Beauvoir's novel "The Blood of Others". Works by Camus and Sartre were already appearing in foreign editions. The Paris-based existentialists had become famous.
Sartre had traveled to Germany in 1930 to study the phenomenology of Edmund Husserl and Martin Heidegger, and he included critical comments on their work in his major treatise "Being and Nothingness". Heidegger's thought had also become known in French philosophical circles through its use by Alexandre Kojève in explicating Hegel in a series of lectures given in Paris in the 1930s. The lectures were highly influential; members of the audience included not only Sartre and Merleau-Ponty, but Raymond Queneau, Georges Bataille, Louis Althusser, André Breton, and Jacques Lacan. A selection from Heidegger's "Being and Time" was published in French in 1938, and his essays began to appear in French philosophy journals.
Heidegger read Sartre's work and was initially impressed, commenting: "Here for the first time I encountered an independent thinker who, from the foundations up, has experienced the area out of which I think. Your work shows such an immediate comprehension of my philosophy as I have never before encountered." Later, however, in response to a question posed by his French follower Jean Beaufret, Heidegger distanced himself from Sartre's position and existentialism in general in his "Letter on Humanism". Heidegger's reputation continued to grow in France during the 1950s and 1960s. In the 1960s, Sartre attempted to reconcile existentialism and Marxism in his work "Critique of Dialectical Reason". A major theme throughout his writings was freedom and responsibility.
Camus was a friend of Sartre, until their falling-out, and wrote several works with existential themes including "The Rebel", "Summer in Algiers", "The Myth of Sisyphus", and "The Stranger", the latter being "considered—to what would have been Camus's irritation—the exemplary existentialist novel." Camus, like many others, rejected the existentialist label, and considered his works concerned with facing the absurd. In the titular book, Camus uses the analogy of the Greek myth of Sisyphus to demonstrate the futility of existence. In the myth, Sisyphus is condemned for eternity to roll a rock up a hill, but when he reaches the summit, the rock will roll to the bottom again. Camus believes that this existence is pointless but that Sisyphus ultimately finds meaning and purpose in his task, simply by continually applying himself to it. The first half of the book contains an extended rebuttal of what Camus took to be existentialist philosophy in the works of Kierkegaard, Shestov, Heidegger, and Jaspers.
Simone de Beauvoir, an important existentialist who spent much of her life as Sartre's partner, wrote about feminist and existentialist ethics in her works, including "The Second Sex" and "The Ethics of Ambiguity". Although often overlooked due to her relationship with Sartre, de Beauvoir integrated existentialism with other forms of thinking such as feminism, unheard of at the time, resulting in alienation from fellow writers such as Camus.
Paul Tillich, an important existentialist theologian following Kierkegaard and Karl Barth, applied existentialist concepts to Christian theology, and helped introduce existential theology to the general public. His seminal work "The Courage to Be" follows Kierkegaard's analysis of anxiety and life's absurdity, but puts forward the thesis that modern humans must, via God, achieve selfhood in spite of life's absurdity. Rudolf Bultmann used Kierkegaard's and Heidegger's philosophy of existence to demythologize Christianity by interpreting Christian mythical concepts into existentialist concepts.
Maurice Merleau-Ponty, an existential phenomenologist, was for a time a companion of Sartre. His understanding of Husserl's phenomenology was far greater than that of Merleau-Ponty's fellow existentialists. It has been said that his work "Humanism and Terror" greatly influenced Sartre. However, in later years they were to disagree irreparably, dividing many existentialists such as de Beauvoir, who sided with Sartre.
Colin Wilson, an English writer, published his study "The Outsider" in 1956, initially to critical acclaim. In this book and others (e.g. "Introduction to the New Existentialism"), he attempted to reinvigorate what he perceived as a pessimistic philosophy and bring it to a wider audience. He was not, however, academically trained, and his work was attacked by professional philosophers for lack of rigor and critical standards.
Influence outside philosophy.
Art.
Film and television.
The French director Jean Genet's 1950 fantasy-erotic film "Un chant d'amour" shows two inmates in solitary cells whose only contact is through a hole in their cell wall, who are spied on by the prison warden. Reviewer James Travers calls the film a, "...visual poem evoking homosexual desire and existentialist suffering," which "... conveys the bleakness of an existence in a godless universe with painful believability"; he calls it "... probably the most effective fusion of existentialist philosophy and cinema."
Stanley Kubrick's 1957 anti-war film "Paths of Glory" "illustrates, and even illuminates...existentialism" by examining the "necessary absurdity of the human condition" and the "horror of war". The film tells the story of a fictional World War I French army regiment ordered to attack an impregnable German stronghold; when the attack fails, three soldiers are chosen at random, court-martialed by a "kangaroo court", and executed by firing squad. The film examines existentialist ethics, such as the issue of whether objectivity is possible and the "problem of authenticity".
Neon Genesis Evangelion, commonly referred to as Evangelion or Eva, is a Japanese science-fiction animation series created by the anime studio Gainax and was both directed and written by Hideaki Anno. Existential themes of individuality, consciousness, freedom, choice, and responsibility are heavily relied upon throughout the entire series, particularly through the philosophies of Jean-Paul Sartre and Søren Kierkegaard. Episode 16's title, "The Sickness Unto Death, And…" (死に至る病、そして, Shi ni itaru yamai, soshite) is a reference to Kierkegaard's book, "The Sickness Unto Death".
On the lighter side, the British comedy troupe Monty Python have explored existentialist themes throughout their works, from many of the sketches in their original television show, "Monty Python's Flying Circus", to their 1983 film "Monty Python's The Meaning of Life".
Some contemporary films dealing with existentialist issues include "Fight Club", "I ♥ Huckabees", "Waking Life", "The Matrix", "Ordinary People", and "Life in a Day". Likewise, films throughout the 20th century such as "The Seventh Seal", "Ikiru", "Taxi Driver"," Toy Story", "Ghost in the Shell", "Harold and Maude", "High Noon", "Easy Rider", "One Flew Over the Cuckoo's Nest", "A Clockwork Orange", "Groundhog Day", "Apocalypse Now", "Badlands", and "Blade Runner" also have existentialist qualities.
"The Matrix" has been compared with another movie, "Dark City" where the issues of identity and reality are raised. In "Dark City", the inhabitants of the city are situated in a world controlled by demiurges, much like the prisoners in Plato's cave, in which prisoners see a world of shadows reflected onto a cave wall, rather than the world as it actually is.
Musician-Popular Film Artist John Lennon's "God" models existentialist ideals. Lennon says, "God is a concept by which we measure our pain...I just believe in me."
Notable directors known for their existentialist films include Ingmar Bergman, François Truffaut, Jean-Luc Godard, Michelangelo Antonioni, Akira Kurosawa, Terrence Malick, Stanley Kubrick, Andrei Tarkovsky, Hideaki Anno, Wes Anderson, Woody Allen, and Christopher Nolan. Charlie Kaufman's "Synecdoche, New York" focuses on the protagonist's desire to find existential meaning. Similarly, in Kurosawa's "Red Beard", the protagonist's experiences as an intern in a rural health clinic in Japan lead him to an existential crisis whereby he questions his reason for being. This, in turn, leads him to a better understanding of humanity.
Recently released French film, "Mood Indigo" (directed by Michel Gondry) embraced various elements of existentialism.
The film "The Shawshank Redemption", released in 1994, depicts life in a prison in Maine, USA to explore several existentialist concepts.
Literature.
Existential perspectives are also found in modern literature to varying degrees, especially since the 1920s. Louis-Ferdinand Céline's "Journey to the End of the Night" (Voyage au bout de la nuit, 1932) celebrated by both Sartre and Beauvoir, contained many of the themes that would be found in later existential literature, and is in some ways, the proto-existential novel. Jean-Paul Sartre's 1938 novel "Nausea" was "steeped in Existential ideas", and is considered an accessible way of grasping his philosophical stance. Between 1900 and 1960, other authors such as Albert Camus, Franz Kafka, Rainer Maria Rilke, T.S. Eliot, Herman Hesse, Luigi Pirandello, Ralph Ellison, and Jack Kerouac, composed literature or poetry that contained, to varying degrees, elements of existential or proto-existential thought. The philosophy's influence even reached pulp literature shortly after the turn of the 20th century, as seen in the existential disparity witnessed in Man's lack of control of his fate in the works of H.P. Lovecraft. Since the late 1960s, a great deal of cultural activity in literature contains postmodernist as well as existential elements. Books such as "Do Androids Dream of Electric Sheep?" (1968) (now republished as "Blade Runner") by Philip K. Dick, "Slaughterhouse-Five" by Kurt Vonnegut, and "Fight Club" by Chuck Palahniuk all distort the line between reality and appearance while simultaneously espousing existential themes. Ideas from such writers as Fyodor Dostoyevsky, Michel Foucault, Franz Kafka, Friedrich Nietzsche, Søren Kierkegaard, Herbert Marcuse, Gilles Deleuze, Arthur Schopenhauer, and Eduard von Hartmann permeate the works of modern novelists such as Chuck Palahniuk, Crispin Glover, Andrew Hussie, David Foster Wallace, and Charles Bukowski, and one often finds in their works a delicate balance between distastefulness and beauty.
Theatre.
Jean-Paul Sartre wrote "No Exit" in 1944, an existentialist play originally published in French as "Huis Clos" (meaning "In Camera" or "behind closed doors"), which is the source of the popular quote, "Hell is other people." (In French, "L'enfer, c'est les autres"). The play begins with a Valet leading a man into a room that the audience soon realizes is in hell. Eventually he is joined by two women. After their entry, the Valet leaves and the door is shut and locked. All three expect to be tortured, but no torturer arrives. Instead, they realize they are there to torture each other, which they do effectively by probing each other's sins, desires, and unpleasant memories.
Existentialist themes are displayed in the Theatre of the Absurd, notably in Samuel Beckett's "Waiting for Godot", in which two men divert themselves while they wait expectantly for someone (or something) named Godot who never arrives. They claim Godot is an acquaintance, but in fact, hardly know him, admitting they would not recognize him if they saw him. Samuel Beckett, once asked who or what Godot is, replied, "If I knew, I would have said so in the play." To occupy themselves, the men eat, sleep, talk, argue, sing, play games, exercise, swap hats, and contemplate suicide—anything "to hold the terrible silence at bay". The play "exploits several archetypal forms and situations, all of which lend themselves to both comedy and pathos." The play also illustrates an attitude toward human experience on earth: the poignancy, oppression, camaraderie, hope, corruption, and bewilderment of human experience that can be reconciled only in the mind and art of the absurdist. The play examines questions such as death, the meaning of human existence and the place of God in human existence.
Tom Stoppard's "Rosencrantz & Guildenstern Are Dead" is an absurdist tragicomedy first staged at the Edinburgh Festival Fringe in 1966. The play expands upon the exploits of two minor characters from Shakespeare's "Hamlet". Comparisons have also been drawn to Samuel Beckett's "Waiting For Godot", for the presence of two central characters who appear almost as two halves of a single character. Many plot features are similar as well: the characters pass time by playing Questions, impersonating other characters, and interrupting each other or remaining silent for long periods of time. The two characters are portrayed as two clowns or fools in a world beyond their understanding. They stumble through philosophical arguments while not realizing the implications, and muse on the irrationality and randomness of the world.
Jean Anouilh's "Antigone" also presents arguments founded on existentialist ideas. It is a tragedy inspired by Greek mythology and the play of the same name (Antigone, by Sophocles) from the 5th century BC. In English, it is often distinguished from its antecedent by being pronounced in its original French form, approximately "Ante-GŌN." The play was first performed in Paris on 6 February 1944, during the Nazi occupation of France. Produced under Nazi censorship, the play is purposefully ambiguous with regards to the rejection of authority (represented by Antigone) and the acceptance of it (represented by Creon). The parallels to the French Resistance and the Nazi occupation have been drawn. Antigone rejects life as desperately meaningless but without affirmatively choosing a noble death. The crux of the play is the lengthy dialogue concerning the nature of power, fate, and choice, during which Antigone says that she is, "... disgusted with [the]...promise of a humdrum happiness." She states that she would rather die than live a mediocre existence.
Critic Martin Esslin in his book "Theatre of the Absurd" pointed out how many contemporary playwrights such as Samuel Beckett, Eugène Ionesco, Jean Genet, and Arthur Adamov wove into their plays the existentialist belief that we are absurd beings loose in a universe empty of real meaning. Esslin noted that many of these playwrights demonstrated the philosophy better than did the plays by Sartre and Camus. Though most of such playwrights, subsequently labeled "Absurdist" (based on Esslin's book), denied affiliations with existentialism and were often staunchly anti-philosophical (for example Ionesco often claimed he identified more with 'Pataphysics or with Surrealism than with existentialism), the playwrights are often linked to existentialism based on Esslin's observation.
Psychoanalysis and psychotherapy.
A major offshoot of existentialism as a philosophy is existentialist psychology and psychoanalysis, which first crystallized in the work of Otto Rank, Freud's closest associate for 20 years. Without awareness of the writings of Rank, Ludwig Binswanger was influenced by Freud, Edmund Husserl, Heidegger, and Sartre. A later figure was Viktor Frankl, who briefly met Freud and studied with Jung as a young man. His logotherapy can be regarded as a form of existentialist therapy. The existentialists would also influence social psychology, antipositivist micro-sociology, symbolic interactionism, and post-structuralism, with the work of thinkers such as Georg Simmel and Michel Foucault. Foucault was a great reader of Kierkegaard even though he almost never refers this author, who nonetheless had for him an importance as secret as it was decisive.
An early contributor to existentialist psychology in the United States was Rollo May, who was strongly influenced by Kierkegaard and Otto Rank. One of the most prolific writers on techniques and theory of existentialist psychology in the USA is Irvin D. Yalom. Yalom states that
Aside from their reaction against Freud's mechanistic, deterministic model of the mind and their assumption of a phenomenological approach in therapy, the existentialist analysts have little in common and have never been regarded as a cohesive ideological school. These thinkers - who include Ludwig Binswanger, Medard Boss, Eugène Minkowski, V.E. Gebsattel, Roland Kuhn, G. Caruso, F.T. Buytendijk, G. Bally and Victor Frankl - were almost entirely unknown to the American psychotherapeutic community until Rollo May's highly influential 1985 book "Existence" - and especially his introductory essay - introduced their work into this country.
A more recent contributor to the development of a European version of existentialist psychotherapy is the British-based Emmy van Deurzen.
Anxiety's importance in existentialism makes it a popular topic in psychotherapy. Therapists often offer existentialist philosophy as an explanation for anxiety. The assertion is that anxiety is manifested of an individual's complete freedom to decide, and complete responsibility for the outcome of such decisions. Psychotherapists using an existentialist approach believe that a patient can harness his anxiety and use it constructively. Instead of suppressing anxiety, patients are advised to use it as grounds for change. By embracing anxiety as inevitable, a person can use it to achieve his full potential in life. Humanistic psychology also had major impetus from existentialist psychology and shares many of the fundamental tenets. Terror management theory, based on the writings of Ernest Becker and Otto Rank, is a developing area of study within the academic study of psychology. It looks at what researchers claim are implicit emotional reactions of people confronted with the knowledge that they will eventually die.
Also, Gerd B. Achenbach has refreshed the socratic tradition with his own blend of philosophical counseling. So did Michel Weber with his Chromatiques Center in Belgium.
Criticisms.
General criticisms.
Walter Kaufmann criticized 'the profoundly unsound methods and the dangerous contempt for reason that have been so prominent in existentialism.' 
Logical positivist philosophers, such as Rudolf Carnap and Alfred Ayer, assert that existentialists are often confused about the verb "to be" in their analyses of "being". Specifically, they argue that the verb is transitive and pre-fixed to a predicate (e.g., an apple "is red") (without a predicate, the word is meaningless), and that existentialists frequently misuse the term in this manner.
Sartre's philosophy.
Many critics argue Sartre's philosophy is contradictory. Specifically, they argue that Sartre makes metaphysical arguments despite his claiming that his philosophical views ignore metaphysics. Herbert Marcuse criticized "Being and Nothingness" (1943) by Jean-Paul Sartre for projecting anxiety and meaninglessness onto the nature of existence itself: "Insofar as Existentialism is a philosophical doctrine, it remains an idealistic doctrine: it hypostatizes specific historical conditions of human existence into ontological and metaphysical characteristics. Existentialism thus becomes part of the very ideology which it attacks, and its radicalism is illusory".
In "Letter on Humanism", Heidegger criticized Sartre's existentialism:
Existentialism says existence precedes essence. In this statement he is taking "existentia" and "essentia" according to their metaphysical meaning, which, from Plato's time on, has said that "essentia" precedes "existentia". Sartre reverses this statement. But the reversal of a metaphysical statement remains a metaphysical statement. With it, he stays with metaphysics, in oblivion of the truth of Being.
References.
</dl>
Further reading.
</dl>

</doc>
<doc id="9596" url="http://en.wikipedia.org/wiki?curid=9596" title="Ellipsis">
Ellipsis

Ellipsis (plural ellipses; from the Ancient Greek: ἔλλειψις, "élleipsis", "omission" or "falling short") is a series of dots that usually indicates an intentional omission of a word, sentence, or whole section from a text without altering its original meaning. Depending on their context and placement in a sentence, ellipses can also indicate an unfinished thought, a leading statement, a slight pause, a mysterious, echoing voice, or a nervous or awkward silence. Aposiopesis is the use of an ellipsis to trail off into silence—for example: "But I thought he was . . ." When placed at the beginning or end of a sentence, the ellipsis can also inspire a feeling of melancholy or longing.
The most common form of an ellipsis is a row of three periods or full stops (. . .) or a precomposed triple-dot glyph (…). The usage of the em dash (—) can overlap the usage of the ellipsis, especially in dialogue. Style guides often have their own rules governing the use of ellipses. For example, "the Chicago Manual of Style" recommends that an ellipsis be formed by typing three periods, each with a space on both sides.
The triple-dot punctuation mark is also called a suspension point, points of ellipsis, periods of ellipsis, or colloquially, "dot-dot-dot".
In writing.
It is used to build tension or show that the sentence has been left unfinished or unstarted.
In the 19th and early 20th centuries, an ellipsis was often used when a writer intentionally omitted a specific proper noun, such as a location: "Jan was born on . . . Street in Warsaw."
As commonly used, this juxtaposition of characters is referred to as "dots of ellipsis" in the English language.
Occasionally, it would be used in pulp fiction and other works of early 20th-century fiction to denote expletives that would otherwise have been censored.
An ellipsis may also imply an unstated alternative indicated by context. For example, when Sue says "I never drink wine . . . ", the implication is that she does drink something else—such as vodka.
In reported speech, the ellipsis can be used to represent an intentional silence
In poetry, this is used to highlight sarcasm or make the reader think about the last points in the poem.
In news reporting, often associated with brackets, it is used to indicate that a quotation has been condensed for space, brevity or relevance.
Herb Caen, Pulitzer-prize-winning columnist for the San Francisco Chronicle, became famous for his "Three-dot journalism".
In different languages.
In English.
"The Chicago Manual of Style" suggests the use of an ellipsis for any omitted word, phrase, line, or paragraph from within but not at the end of a quoted passage. There are two commonly used methods of using ellipses: one uses three dots for any omission, while the second one makes a distinction between omissions within a sentence (using three dots: . . .) and omissions between sentences (using a period and a space followed by three dots: . ...). An ellipsis at the end of a sentence with a sentence following should be preceded by a period (for a total of four dots).
The Modern Language Association (MLA), however, used to indicate that an ellipsis must include spaces before and after each dot in all uses. If an ellipsis is meant to represent an omission, square brackets must surround the ellipsis to make it clear that there was no pause in the original quote: [ . . . ]. Currently, the MLA has removed the requirement of brackets in its style handbooks. However, some maintain that the use of brackets is still correct because it clears confusion.
The MLA now indicates that a three-dot, spaced ellipsis ( … ) should be used for removing material from within one sentence within a quote. When crossing sentences (when the omitted text contains a period, so that omitting the end of a sentence counts), a four-dot, spaced (except for before the first dot) ellipsis (. . . . ) should be used. When ellipsis points are used in the original text, ellipsis points that are not in the original text should be distinguished by enclosing them in square brackets (e.g. "text […] text").
According to the Associated Press, the ellipsis should be used to condense quotations. It is less commonly used to indicate a pause in speech or an unfinished thought or to separate items in material such as show business gossip. The stylebook indicates that if the shortened sentence before the mark can stand as a sentence, it should do so, with an ellipsis placed after the period or other ending punctuation. When material is omitted at the end of a paragraph and also immediately following it, an ellipsis goes both at the end of that paragraph and in front of the beginning of the next, according to this style.
According to Robert Bringhurst's "Elements of Typographic Style", the details of typesetting ellipses depend on the character and size of the font being set and the typographer's preference. Bringhurst writes that a full space between each dot is "another Victorian eccentricity. In most contexts, the Chicago ellipsis is much too wide"—he recommends using flush dots, or "thin"-spaced dots (up to one-fifth of an em), or the prefabricated ellipsis character (Unicode U+2026, Latin entity codice_1). Bringhurst suggests that normally an ellipsis should be spaced fore-and-aft to separate it from the text, but when it combines with other punctuation, the leading space disappears and the other punctuation follows. This is the usual practice in typesetting. He provides the following examples:
In legal writing in the United States, Rule 5.3 in the "Bluebook" citation guide governs the use of ellipses and requires a space before the first dot and between the two subsequent dots. If an ellipsis ends the sentence, then there are three dots, each separated by a space, followed by the final punctuation. In some legal writing, an ellipsis is written as three asterisks (*** or * * *) to make it obvious that text has been omitted.
(...) is also used for awkward silence.
In Polish.
When applied in Polish language syntax, the ellipsis is called "wielokropek", which means "multidot". The word "wielokropek" distinguishes the ellipsis of Polish syntax from that of mathematical notation, in which it is known as an "elipsa".
When an ellipsis replaces a fragment omitted from a quotation, the ellipsis is enclosed in parentheses or square brackets. An unbracketed ellipsis indicates an interruption or pause in speech.
The syntactical rules for ellipses are standardized by the 1983 Polska Norma document PN-83/P-55366, "Zasady składania tekstów w języku polskim" ("Rules for setting texts in the Polish Language").
In Russian.
The combination "ellipsis+period" is replaced by the ellipsis. The combinations "ellipsis+exclamation mark" and "ellipsis+question mark" are written in this way: !.. ?..
In Japanese.
The most common character corresponding to an ellipsis is called "3"-ten rīdā (""3"-dot leaders", …). 2-ten rīdā exists as a character, but it is used less commonly. In writing, the ellipsis consists usually of six dots (two "3"-ten rīdā characters, ……). Three dots (one "3"-ten rīdā character) may be used where space is limited, such as in a header. However, variations in the number of dots exist. In horizontally written text the dots are commonly vertically centered within the text height (between the baseline and the ascent line), as in the standard Japanese Windows fonts; in vertically written text the dots are always centered horizontally. 
In manga, the ellipsis by itself represents speechlessness, or a "pregnant pause". Given the context, this could be anything from an admission of guilt to an expression of being dumbfounded at another person's words or actions. As a device, the "ten-ten-ten" is intended to focus the reader on a character while allowing the character to not speak any dialogue. This conveys to the reader a focus of the narrative "camera" on the silent subject, implying an expectation of some motion or action. It is not unheard of to see inanimate objects "speaking" the ellipsis.
In Chinese.
In Chinese, the ellipsis is six dots (in two groups of three dots, occupying the same horizontal space as two characters) (i.e. ……). The dots are always centered within the baseline and the ascender when horizontal (on the baseline has become acceptable) and centered horizontally when vertical.
In Spanish.
In Spanish, ellipsis is commonly used as a substitute of "et cetera" at the end of unfinished lists. So it means "and so forth" or "and other things".
Other use is the suspension of a part of a text, or a paragraph, or a phrase or a part of a word because it is obvious, or unnecessary, or implied. For instance, sometimes the ellipsis is used to avoid the complete use of expletives.
When the ellipse is emplaced alone into a parenthesis (...) or -less often- between brackets […], what happen usually within a text transcription, it means the original text had more contents on the same position but are unuseful to our target in the transcription. When the suppressed text is at the beginning or at the end of a text, the ellipse do not need to be placed in a parenthesis.
In French.
In French, the ellipsis is commonly used at the end of lists to represent "et cetera".
However, any omitted word, phrase or line at the end of a quoted passage would be indicated like this : [...] (space before and after the square brackets but not inside).
In German.
In German, the ellipsis in general is surrounded by spaces, if it stands for one or more omitted words. On the other side there is no space between a letter or (part of) a word and an ellipsis, if it stands for one ore more omitted letters, that should stick to the written letter or letters.
Example for both cases, using German style: "The first el...is stands for omitted letters, the second ... for an omitted word."
In mathematical notation.
An ellipsis is also often used in mathematics to mean "and so forth". In a list, between commas, or following a comma, a normal ellipsis is used, as in:
To indicate the omission of values in a repeated operation, an ellipsis raised to the center of the line is used between two operation symbols or following the last operation symbol, as in:
(though sometimes, for example, in Russian mathematical texts, normal, non-raised, ellipses are used even in repeated summations).
The latter formula means the sum of all natural numbers from 1 to 100. However, it is not a formally defined mathematical symbol. Repeated summations or products may similarly be denoted using capital sigma and capital pi notation, respectively:
Normally dots should be used only where the pattern to be followed is clear, the exception being to show the indefinite continuation of an irrational number such as:
Sometimes, it is useful to display a formula compactly, for example:
Another example is the set of zeros of the cosine function.
There are many related uses of the ellipsis in set notation.
The diagonal and vertical forms of the ellipsis are particularly useful for showing missing terms in matrices, such as the size-"n" identity matrix
The use of ellipses in mathematical proofs is often discouraged because of the potential for ambiguity. For this reason, and because the ellipsis supports no systematic rules for symbolic calculation, in recent years some authors have recommended avoiding its use in mathematics altogether.
Computer interfaces.
Ellipses are often used in an operating system's taskbars or web browser tabs to indicate that a user interface string is longer than what can fit in the screen. Hovering the cursor over the tab often displays a tooltip of the full title. When many programs are open, or during a "tab explosion" in web browsing, the tabs may be reduced in size so much that no characters from the actual titles show, and ellipses take up all the space besides the program icon or favicon.
In many user interface guidelines, a "…" after the name of a command implies that the user will need to provide further information, for example in a subsequent dialog box, before the action can be completed. A typical example is the "Save As…" command, which after being clicked will usually require the user to enter a filename, as opposed to "Save" where the file will usually be saved under its existing name.
An ellipsis character after a status message signifies that an operation is in progress and may take some time, as in "Downloading updates…".
Programming languages.
The ellipsis is used as an operator in some programming languages. The precise meaning varies by language, but it generally involves something dealing with multiple items. One of its most common uses is in defining variadic functions which can take an unknown number of arguments in the C, C++ and Java languages. "See Ellipsis (programming operator)".
On the Internet and in text messaging.
The ellipsis is a non-verbal cue that is often used in computer-mediated interactions, in particular in synchronous genres, such as chat. The reason behind its popularity is the fact that it allows people to indicate in writing several functions:
Although an ellipsis is technically complete with three periods (...), its rise in popularity as a "trailing-off" or "silence" indicator, particularly in mid-20th-century comic strip and comic book prose writing, has led to expanded uses online. Today, extended ellipsis anywhere from two to dozens of periods have become common constructions in Internet chat rooms and text messages. The extent of repetition in itself might serve as an additional contextualization or paralinguistic cue, to "extend the lexical meaning of the words, add character to the sentences, and allow fine-tuning and personalisation of the message"
Computer representations.
In computing, several ellipsis characters have been codified, depending on the system used.
In the Unicode standard, there are the following characters:
In Windows, it can be inserted with Alt+0133.
In OS X, it can be inserted with ⌥ Opt+; (on an English language keyboard).
In Linux, it can be inserted with AltGr+.
In Chinese and sometimes in Japanese, ellipsis characters are made by entering two consecutive "horizontal ellipsis" (U+2026). In vertical texts, the application should rotate the symbol accordingly.
Unicode recognizes a series of three period characters (U+002E) as compatibility equivalent (though not canonical) to the horizontal ellipsis character.
In HTML, the horizontal ellipsis character may be represented by the entity reference codice_1 (since HTML 4.0), and the vertical ellipsis character by the entity reference codice_3 (since HTML 5.0). Alternatively, in HTML, XML, and SGML, a numeric character reference such as codice_4 or codice_5 can be used.
In the TeX typesetting system, the following types of ellipsis are available:
The horizontal ellipsis character also appears in the following older character maps:
Note that ISO/IEC 8859 encoding series provides no code point for ellipsis.
As with all characters, especially those outside the ASCII range, the author, sender and receiver of an encoded ellipsis must be in agreement upon what bytes are being used to represent the character. Naive text processing software may improperly assume that a particular encoding is being used, resulting in mojibake.
The Chicago Style Q&A recommends to avoid the use of … (U+2026) character in manuscripts and to place three periods plus two nonbreaking spaces (. . .) instead, so that an editor, publisher, or designer can replace them later.
In Abstract Syntax Notation One (ASN.1), the ellipsis is used as an extension marker to indicate the possibility of type extensions in future revisions of a protocol specification. In a type constraint expression like A ::= INTEGER (0..127, ..., 256..511) an ellipsis is used to separate the extension root from extension additions. The definition of type A in version 1 system of the form A ::= INTEGER (0..127, ...) and the definition of type A in version 2 system of the form A ::= INTEGER (0..127, ..., 256..511) constitute an extension series of the same type A in different versions of the same specification. The ellipsis can also be used in compound type definitions to separate the set of fields belonging to the extension root from the set of fields constituting extension additions. Here is an example: B ::= SEQUENCE { a INTEGER, b INTEGER, ..., c INTEGER } 
Further reading.
</dl>

</doc>
<doc id="9597" url="http://en.wikipedia.org/wiki?curid=9597" title="Enola Gay">
Enola Gay

The Enola Gay is a Boeing B-29 Superfortress bomber, named for Enola Gay Tibbets, the mother of the pilot, Colonel Paul Tibbets, who selected the aircraft while it was still on the assembly line. On 6 August 1945, during the final stages of World War II, it became the first aircraft to drop an atomic bomb. The bomb, code-named "Little Boy", was targeted at the city of Hiroshima, Japan, and caused unprecedented destruction. "Enola Gay" participated in the second atomic attack as the weather reconnaissance aircraft for the primary target of Kokura. Clouds and drifting smoke resulted in Nagasaki being bombed instead.
After the war, the "Enola Gay" returned to the United States, where it was operated from Roswell Army Air Field, New Mexico. It was flown to Kwajalein for the Operation Crossroads nuclear tests in the Pacific, but was not chosen to make the test drop at Bikini Atoll. Later that year it was transferred to the Smithsonian Institution, and spent many years parked at air bases exposed to the weather and souvenir hunters, before being disassembled and transported to the Smithsonian's storage facility at Suitland, Maryland, in 1961.
In the 1980s, veterans groups engaged in a call for the Smithsonian to put the aircraft on display, leading to an acrimonious debate with historians and activists about exhibiting the aircraft without a proper historical context. The cockpit and nose section of the aircraft were exhibited at the National Air and Space Museum (NASM) in downtown Washington, D.C., for the bombing's 50th anniversary in 1995, amid a storm of controversy. Since 2003, the entire restored B-29 has been on display at NASM's Steven F. Udvar-Hazy Center. The last survivor of its crew, Theodore Van Kirk, died on July 28, 2014, at the age of 93.
World War II.
Early history.
The "Enola Gay" (Model number B-29-45-MO, Serial number 44-86292, Victor number 82) was built by the Glenn L. Martin Company (now Lockheed Martin) at its Bellevue, Nebraska plant, located at what is now known as Offutt Air Force Base. The bomber was one of 15 B-29s with the "Silverplate" modifications necessary to deliver atomic weapons. These modifications included an extensively modified bomb bay with pneumatic doors and British bomb attachment and release systems, reversible pitch propellers that gave more braking power on landing, improved engines with fuel injection and better cooling, and the removal of protective armor and gun turrets.
"Enola Gay" was personally selected by Colonel Paul W. Tibbets, Jr., the commander of the 509th Composite Group, on 9 May 1945, while still on the assembly line. The aircraft was accepted by the United States Army Air Forces (USAAF) on 18 May 1945 and assigned to the 393d Bombardment Squadron, Heavy, 509th Composite Group. Crew B-9, commanded by Captain Robert A. Lewis, took delivery of the bomber and flew it from Omaha to the 509th's base at Wendover Army Air Field, Utah, on 14 June 1945.
Thirteen days later, the aircraft left Wendover for Guam, where it received a bomb-bay modification, and flew to North Field, Tinian, on 6 July. It was initially given the Victor (squadron-assigned identification) number 12, but on 1 August, was given the circle R tail markings of the 6th Bombardment Group as a security measure and had its Victor number changed to 82 to avoid misidentification with actual 6th Bombardment Group aircraft. During July, the bomber made eight practice or training flights, and flew two missions, on 24 and 26 July, to drop pumpkin bombs on industrial targets at Kobe and Nagoya. "Enola Gay" was used on 31 July on a rehearsal flight for the actual mission.
The partially assembled Little Boy gun-type nuclear weapon L-11 was contained inside a 41 in x 47 in x 138 in wooden crate weighing 10000 lb that was secured to the deck of the USS "Indianapolis". Unlike the six Uranium-235 target discs, which were later flown to Tinian on three separate aircraft arriving 28 and 29 July, the assembled projectile with the nine Uranium-235 rings installed was shipped in a single lead-lined steel container weighing 300 lb that was securely locked to brackets welded to the deck of Captain Charles B. McVay III's quarters. Both the L-11 and projectile were dropped off at Tinian on 26 July 1945.
Hiroshima mission.
On 5 August 1945, during preparation for the first atomic mission, Tibbets assumed command of the aircraft and named it after his mother, Enola Gay Tibbets, who had herself been named for the heroine of a novel. When it came to selecting a name for the plane, Tibbets later recalled that: my thoughts turned at this point to my courageous red-haired mother, whose quiet confidence had been a source of strength to me since boyhood, and particularly during the soul-searching period when I decided to give up a medical career to become a military pilot. At a time when Dad had thought I had lost my marbles, she had taken my side and said, "I know you will be all right son."
The name was painted on the aircraft on 5 August by Allan L. Karl, an enlisted man in the 509th. Regularly assigned aircraft commander Robert Lewis was unhappy to be displaced by Tibbets for this important mission, and became furious when he arrived at the aircraft on the morning of 6 August to see it painted with the now-famous nose art.
Hiroshima was the primary target of the first nuclear bombing mission on 6 August, with Kokura and Nagasaki as alternative targets. "Enola Gay", piloted by Tibbets, took off from North Field, about six hours' flight time from Japan, accompanied by two other B-29s, "The Great Artiste", carrying instrumentation, and a then-nameless aircraft later called "Necessary Evil", commanded by Captain George Marquardt, to take photographs. The director of the Manhattan Project, Major General Leslie R. Groves, Jr., wanted the event recorded for posterity, so the takeoff was illuminated by floodlights. When he wanted to taxi, Tibbets leaned out the window to direct the bystanders out of the way. On request, he gave a friendly wave for the cameras.
After leaving Tinian the aircraft made their way separately to Iwo Jima, where they rendezvoused at 2440 m and set course for Japan. The aircraft arrived over the target in clear visibility at 9855 m. Captain William S. "Deak" Parsons of Project Alberta, who was in command of the mission, armed the bomb during the flight to minimize the risks during takeoff. His assistant, Second Lieutenant Morris R. Jeppson, removed the safety devices 30 minutes before reaching the target area.
The release at 08:15 (Hiroshima time) went as planned, and the Little Boy took 43 seconds to fall from the aircraft flying at 31060 ft to the predetermined detonation height about 1968 ft above the city. "Enola Gay" traveled 11.5 mi before it felt the shock waves from the blast. Although buffeted by the shock, neither "Enola Gay" nor "The Great Artiste" was damaged.
The detonation created a blast equivalent to 16 ktonTNT. The U-235 weapon was considered very inefficient, with only 1.7% of its fissile material fissioning. The radius of total destruction was about one mile (1.6 km), with resulting fires across 4.4 sqmi. Americans estimated that 4.7 sqmi of the city were destroyed. Japanese officials determined that 69% of Hiroshima's buildings were destroyed and another 6–7% damaged. Some 70,000–80,000 people, or some 30% of the city's population, were killed by the blast and resultant firestorm, and another 70,000 injured. Out of those killed, 20,000 were soldiers.
"Enola Gay" returned safely to its base on Tinian to great fanfare, touching down at 2:58 pm, after 12 hours 13 minutes. "The Great Artiste" and "Necessary Evil" followed at short intervals. Several hundred people, including journalists and photographers, had gathered to watch the planes return. Tibbets was the first to disembark, and was presented with the Distinguished Service Cross on the spot.
Nagasaki mission.
The Hiroshima mission was followed by another atomic strike. Originally scheduled for 11 August, it was brought forward by two days to 9 August owing to bad weather. This time, a Fat Man nuclear weapon was carried by B-29 "Bockscar", piloted by Major Charles W. Sweeney. "Enola Gay", flown by Captain George Marquardt's Crew B-10, was the weather reconnaissance aircraft for Kokura, the primary target. "Enola Gay" reported clear skies over Kokura, but by the time "Bockscar" arrived, the city was obscured by smoke from fires from the conventional bombing of Yawata by 224 B-29s the day before. After three unsuccessful passes, "Bockscar" diverted to its secondary target, Nagasaki, where it dropped its bomb. In contrast to the Hiroshima mission, the Nagasaki mission has been described as tactically botched, although the mission did meet its objectives. The crew encountered a number of problems in execution, and had very little fuel by the time it landed at Yontan Airfield on Okinawa.
Crews.
Hiroshima mission.
"Enola Gay"'s crew on 6 August 1945, consisted of 12 men. The crew was:
Nagasaki mission.
For the Nagasaki mission, "Enola Gay" was flown by Crew B-10, normally assigned to "Up An' Atom":
Subsequent history.
On 6 November 1945, Lewis flew the "Enola Gay" back to the United States, arriving at the 509th's new base at Roswell Army Air Field, New Mexico, on 8 November. On 29 April 1946, "Enola Gay" left Roswell as part of Operation Crossroads nuclear tests in the Pacific. It flew to Kwajalein on 1 May. It was not chosen to make the test drop at Bikini Atoll and left Kwajalein on 1 July, the date of the test, reaching Fairfield-Suisun Army Air Field, California, the next day.
The decision was made to preserve the "Enola Gay", and on 24 July 1946, the aircraft was flown to Davis-Monthan Air Force Base, Arizona, in preparation for storage. On 30 August 1946, the title to the aircraft was transferred to the Smithsonian Institution and the "Enola Gay" was removed from the USAAF inventory. From 1946 to 1961, the "Enola Gay" was put into temporary storage at a number of locations. It was at Davis-Monthan from 1 September 1946 until 3 July 1949, when it was flown to Orchard Place Air Field, Park Ridge, Illinois, by Tibbets for acceptance by the Smithsonian. It was moved to Pyote Air Force Base, Texas, on 12 January 1952, and then to Andrews Air Force Base, Maryland, on 2 December 1953, because the Smithsonian had no storage space for the aircraft.
It was hoped that the Air Force would guard the plane but, lacking hangar space, it was left outdoors on a remote part of the air base, exposed to the elements. Souvenir hunters broke in and removed parts. Insects and birds then gained access to the aircraft. Paul E. Garber, the first head of the National Air Museum of the Smithsonian Institution, became concerned about the "Enola Gay"‍ '​s condition, and on 10 August 1960, Smithsonian staff began dismantling the aircraft. The components were transported to the Smithsonian storage facility at Suitland, Maryland, on 21 July 1961.
"Enola Gay" remained at Suitland for many years. By the early 1980s, two veterans of the 509th, Don Rehl and his former navigator in the 509th, Frank B. Stewart, began lobbying for the aircraft to be restored and put on display. They enlisted Tibbets and Senator Barry Goldwater in their campaign. In 1983, Walter Boyne, a former B-52 pilot with the Strategic Air Command, became director of the National Air and Space Museum, and he made the "Enola Gay"‍ '​s restoration a priority. Looking at the aircraft, Tibbets recalled, was a "sad meeting. [My] fond memories, and I don't mean the dropping of the bomb, were the numerous occasions I flew the airplane... I pushed it very, very hard and it never failed me... It was probably the most beautiful piece of machinery that any pilot ever flew."
Restoration of the bomber began on 5 December 1984, at the Paul E. Garber Preservation, Restoration, and Storage Facility in Suitland-Silver Hill, Maryland. The propellers that were used on the bombing mission were later shipped to Texas A&M University. One of these propellers was trimmed to 12.5 ft for use in the university's Oran W. Nicks Low Speed Wind Tunnel. The lightweight aluminium variable-pitch propeller is powered by a 1,250 kVA electric motor providing a wind speed up to 200 mph. Two engines were rebuilt at Garber and two at San Diego Air & Space Museum. The work was slow and meticulous. Every component was carefully cleaned. Some parts and instruments had been removed and could not be located. Replacements were found or fabricated, and marked so that future curators could distinguish them from the original components.
Restoration.
Exhibition controversy.
"Enola Gay" became the center of a controversy at the Smithsonian Institution when the museum planned to put its fuselage on public display in 1995 as part of an exhibit commemorating the 50th anniversary of the atomic bombing of Hiroshima. The exhibit, "The Crossroads: The End of World War II, the Atomic Bomb and the Cold War", was drafted by the Smithsonian's National Air and Space Museum staff, and arranged around the restored "Enola Gay".
Critics of the planned exhibit, especially those of the American Legion and the Air Force Association, charged that the exhibit focused too much attention on the Japanese casualties inflicted by the nuclear bomb, rather than on the motivations for the bombing or the discussion of the bomb's role in ending the conflict with Japan. The exhibit brought to national attention many long-standing academic and political issues related to retrospective views of the bombings. As a result, after various failed attempts to revise the exhibit in order to meet the satisfaction of competing interest groups, the exhibit was canceled on 30 January 1995. Martin O. Harwit, Director of the National Air and Space Museum, was compelled to resign over the controversy.
The forward fuselage did go on display on 28 June 1995. On 2 July 1995, three people were arrested for throwing ash and human blood on the aircraft's fuselage, following an earlier incident in which a protester had thrown red paint over the gallery's carpeting. The exhibition closed on 18 May 1998, and the fuselage was returned to the Garber Facility for final restoration.
Complete restoration and display.
Restoration work began in 1984, and would eventually require 300,000 staff hours. While the fuselage was on display, from 1995 to 1998, work continued on the remaining unrestored components. The aircraft was shipped in pieces to the National Air and Space Museum's Steven F. Udvar-Hazy Center in Chantilly, Virginia from March–June 2003, with the fuselage and wings reunited for the first time since 1960 on 10 April 2003 and assembly completed on 8 August 2003. The aircraft is currently at Washington Dulles International Airport in the Steven F. Udvar-Hazy Center, since the museum annex opened on 15 December 2003.
The "Enola Gay" on display at the National Air & Space Museum, Steven F. Udvar-Hazy Center
References.
Bibliography.
</dl>
Further reading.
</dl>

</doc>
<doc id="9598" url="http://en.wikipedia.org/wiki?curid=9598" title="Electronvolt">
Electronvolt

In physics, the electronvolt (symbol eV; also written electron volt) is a unit of energy equal to approximately 160 zeptojoules (symbol zJ) or joules (symbol J). By definition, it is the amount of energy gained (or lost) by the charge of a single electron moved across an electric potential difference of one volt. Thus it is 1 volt (1 joule per coulomb, ) multiplied by the elementary charge ("e", or ). Therefore, one electron volt is equal to . Historically, the electron volt was devised as a standard unit of measure through its usefulness in electrostatic particle accelerator sciences because a particle with charge "q" has an energy "E" = "qV" after passing through the potential "V"; if "q" is quoted in integer units of the elementary charge and the terminal bias in volts, one gets an energy in eV.
The electron volt is not an SI unit, and its definition is empirical (unlike the litre, the light year and other such non-SI units), thus its value in SI units must be obtained experimentally. Like the elementary charge on which it is based, it is not an independent quantity but is equal to 1 J/C √2"h""α" / "μ"0"c"0. It is a common unit of energy within physics, widely used in solid state, atomic, nuclear, and particle physics. It is commonly used with the metric prefixes milli-, kilo-, mega-, giga-, tera-, peta- or exa- (meV, keV, MeV, GeV, TeV, PeV and EeV respectively). Thus meV stands for milli-electron volt.
In some older documents, and in the name Bevatron, the symbol BeV is used, which stands for billion electron volts; it is equivalent to the GeV.
Mass.
By mass–energy equivalence, the electronvolt is also a unit of mass. It is common in particle physics, where units of mass and energy are often interchanged, to express mass in units of eV/"c"2, where "c" is the speed of light in vacuum (from "E" = "mc"2). It is common to simply express mass in terms of "eV" as a unit of mass, effectively using a system of natural units with "c" set to 1.
The mass equivalent of is .
For example, an electron and a positron, each with a mass of , can annihilate to yield of energy. The proton has a mass of . In general, the masses of all hadrons are of the order of , which makes the GeV (gigaelectronvolt) a convenient unit of mass for particle physics:
The atomic mass unit, 1 gram divided by Avogadro's number, is almost the mass of a hydrogen atom, which is mostly the mass of the proton. To convert to megaelectronvolts, use the formula:
Momentum.
In high-energy physics, the electron volt is often used as a unit of momentum. A potential difference of 1 volt causes an electron to gain an amount of energy (i.e., ). This gives rise to usage of eV (and keV, MeV, GeV or TeV) as units of momentum, for the energy supplied results in acceleration of the particle.
The dimensions of momentum units are . The dimensions of energy units are . Then, dividing the units of energy (such as eV) by a fundamental constant that has units of velocity (), facilitates the required conversion of using energy units to describe momentum. In the field of high-energy particle physics, the fundamental velocity unit is the speed of light in vacuum "c". Thus, dividing energy in eV by the speed of light, one can describe the momentum of an electron in units of eV/"c".
The fundamental velocity constant "c" is often "dropped" from the units of momentum by way of defining units of length such that the value of "c" is unity. For example, if the momentum "p" of an electron is said to be , then the conversion to MKS can be achieved by:
Distance.
In particle physics, a system of "natural units" in which the speed of light in vacuum "c" and the reduced Planck constant "ħ" are dimensionless and equal to unity is widely used: "c" = "ħ" = 1. In these units, both distances and times are expressed in inverse energy units (while energy and mass are expressed in the same units, see mass–energy equivalence). In particular, particle scattering lengths are often presented in units of inverse particle masses.
Outside this system of units, the conversion factors between electronvolt, second, and nanometer are the following:
The above relations also allow expressing the mean lifetime "τ" of an unstable particle (in seconds) in terms of its decay width "Γ" (in eV) via "Γ" = "ħ"/"τ". For example, the B0 meson has a lifetime of 1.530(9) picoseconds, mean decay length is "cτ" = , or a decay width of .
Conversely, the tiny meson mass differences responsible for meson oscillations are often expressed in the more convenient inverse picoseconds.
Temperature.
In certain fields, such as plasma physics, it is convenient to use the electronvolt as a unit of temperature. The conversion to the Kelvin scale is defined by using "k"B, the Boltzmann constant:
For example, a typical magnetic confinement fusion plasma is , or 170 megakelvin.
As an approximation: "k"B"T" is about (≈ 290 K/11604 K/eV) at a temperature of .
Properties.
The energy "E", frequency "v", and wavelength λ of a photon are related by
where "h" is the Planck constant, "c" is the speed of light. This reduces to
A photon with a wavelength of (green light) would have an energy of approximately . Similarly, would correspond to an infrared photon of wavelength or frequency .
Scattering experiments.
In a low-energy nuclear scattering experiment, it is conventional to refer to the nuclear recoil energy in units of eVr, keVr, etc. This distinguishes the nuclear recoil energy from the "electron equivalent" recoil energy (eVee, keVee, etc.) measured by scintillation light. For example, the yield of a phototube is measured in phe/keVee (photoelectrons per keV electron-equivalent energy). The relationship between eV, eVr, and eVee depends on the medium the scattering takes place in, and must be established empirically for each material.

</doc>
<doc id="9601" url="http://en.wikipedia.org/wiki?curid=9601" title="Electrochemistry">
Electrochemistry

Electrochemistry is the branch of physical chemistry that studies chemical reactions which take place at the interface of an electrode, usually a solid metal or a semiconductor, and an ionic conductor, the electrolyte. These reactions involve electric charges moving between the electrodes and the electrolyte (or ionic species in a solution). Thus electrochemistry deals with the interaction between electrical energy and chemical change.
When a chemical reaction is caused by an externally supplied current, as in electrolysis, or if an electrical current is produced by a spontaneous chemical reaction as in a battery, it is called an "electrochemical" reaction. Chemical reactions where electrons are transferred directly between molecules and/or atoms are called oxidation-reduction or (redox) reactions. In general, electrochemistry describes the overall reactions when individual redox reactions are separate but connected by an external electric circuit and an intervening electrolyte.
History.
16th to 18th century developments.
Understanding of electrical matters began in the sixteenth century. During this century, the English scientist William Gilbert spent 17 years experimenting with magnetism and, to a lesser extent, electricity. For his work on magnets, Gilbert became known as the "Father of Magnetism." He discovered various methods for producing and strengthening magnets.
In 1663, the German physicist Otto von Guericke created the first electric generator, which produced static electricity by applying friction in the machine. The generator was made of a large sulfur ball cast inside a glass globe, mounted on a shaft. The ball was rotated by means of a crank and an electric spark was produced when a pad was rubbed against the ball as it rotated. The globe could be removed and used as source for experiments with electricity.
By the mid—18th century the French chemist Charles François de Cisternay du Fay had discovered two types of static electricity, and that like charges repel each other whilst unlike charges attract. Du Fay announced that electricity consisted of two fluids: "vitreous" (from the Latin for "glass"), or positive, electricity; and "resinous," or negative, electricity. This was the "two-fluid theory" of electricity, which was to be opposed by Benjamin Franklin's "one-fluid theory" later in the century.
In 1785, Charles-Augustin de Coulomb developed the law of electrostatic attraction as an outgrowth of his attempt to investigate the law of electrical repulsions as stated by Joseph Priestley in England.
In the late 18th century the Italian physician and anatomist Luigi Galvani marked the birth of electrochemistry by establishing a bridge between chemical reactions and electricity on his essay "De Viribus Electricitatis in Motu Musculari Commentarius" (Latin for Commentary on the Effect of Electricity on Muscular Motion) in 1791 where he proposed a "nerveo-electrical substance" on biological life forms.
In his essay Galvani concluded that animal tissue contained a here-to-fore neglected innate, vital force, which he termed "animal electricity," which activated nerves and muscles spanned by metal probes. He believed that this new force was a form of electricity in addition to the "natural" form produced by lightning or by the electric eel and torpedo ray as well as the "artificial" form produced by friction (i.e., static electricity).
Galvani's scientific colleagues generally accepted his views, but Alessandro Volta rejected the idea of an "animal electric fluid," replying that the frog's legs responded to differences in metal temper, composition, and bulk. Galvani refuted this by obtaining muscular action with two pieces of the same material.
19th century.
In 1800, William Nicholson and Johann Wilhelm Ritter succeeded in decomposing water into hydrogen and oxygen by electrolysis. Soon thereafter Ritter discovered the process of electroplating. He also observed that the amount of metal deposited and the amount of oxygen produced during an electrolytic process depended on the distance between the electrodes. By 1801, Ritter observed thermoelectric currents and anticipated the discovery of thermoelectricity by Thomas Johann Seebeck.
By the 1810s, William Hyde Wollaston made improvements to the galvanic cell.
Sir Humphry Davy's work with electrolysis led to the conclusion that the production of electricity in simple electrolytic cells resulted from chemical action and that chemical combination occurred between substances of opposite charge. This work led directly to the isolation of sodium and potassium from their compounds and of the alkaline earth metals from theirs in 1808.
Hans Christian Ørsted's discovery of the magnetic effect of electrical currents in 1820 was immediately recognized as an epoch-making advance, although he left further work on electromagnetism to others. André-Marie Ampère quickly repeated Ørsted's experiment, and formulated them mathematically.
In 1821, Estonian-German physicist Thomas Johann Seebeck demonstrated the electrical potential in the juncture points of two dissimilar metals when there is a heat difference between the joints.
In 1827, the German scientist Georg Ohm expressed his law in this famous book "Die galvanische Kette, mathematisch bearbeitet" (The Galvanic Circuit Investigated Mathematically) in which he gave his complete theory of electricity.
In 1832, Michael Faraday's experiments led him to state his two laws of electrochemistry. In 1836, John Daniell invented a primary cell which solved the problem of polarization by eliminating hydrogen gas generation at the positive electrode. Later results revealed that alloying the amalgamated zinc with mercury would produce a higher voltage.
William Grove produced the first fuel cell in 1839. In 1846, Wilhelm Weber developed the electrodynamometer. In 1868, Georges Leclanché patented a new cell which eventually became the forerunner to the world's first widely used battery, the zinc carbon cell.
Svante Arrhenius published his thesis in 1884 on "Recherches sur la conductibilité galvanique des électrolytes" (Investigations on the galvanic conductivity of electrolytes). From his results the author concluded that electrolytes, when dissolved in water, become to varying degrees split or dissociated into electrically opposite positive and negative ions.
In 1886, Paul Héroult and Charles M. Hall developed an efficient method (the Hall–Héroult process) to obtain aluminium using electrolysis of molten alumina.
In 1894, Friedrich Ostwald concluded important studies of the conductivity and electrolytic dissociation of organic acids.
Walther Hermann Nernst developed the theory of the electromotive force of the voltaic cell in 1888. In 1889, he showed how the characteristics of the current produced could be used to calculate the free energy change in the chemical reaction producing the current. He constructed an equation, known as Nernst equation, which related the voltage of a cell to its properties.
In 1898, Fritz Haber showed that definite reduction products can result from electrolytic processes if the potential at the cathode is kept constant. In 1898, he explained the reduction of nitrobenzene in stages at the cathode and this became the model for other similar reduction processes.
20th century and recent developments.
In 1902, The Electrochemical Society (ECS) was founded.
In 1909, Robert Andrews Millikan began a series of experiments (see oil drop experiment) to determine the electric charge carried by a single electron.
In 1923, Johannes Nicolaus Brønsted and Martin Lowry published essentially the same theory about how acids and bases behave, using an electrochemical basis.
In 1937, Arne Tiselius developed the first sophisticated electrophoretic apparatus. Some years later, he was awarded the 1948 Nobel Prize for his work in protein electrophoresis.
A year later, in 1949, the International Society of Electrochemistry (ISE) was founded.
By the 1960s–1970s quantum electrochemistry was developed by Revaz Dogonadze and his pupils.
Principles.
Oxidation and reduction.
The term "redox" stands for reduction-oxidation. It refers to electrochemical processes involving electron transfer to or from a molecule or ion changing its oxidation state. This reaction can occur through the application of an external voltage or through the release of chemical energy. Oxidation and reduction describe the change of oxidation state that takes place in the atoms, ions or molecules involved in an electrochemical reaction. Formally, oxidation state is the hypothetical charge that an atom would have if all bonds to atoms of different elements were 100% ionic. An atom or ion that gives up an electron to another atom or ion has its oxidation state increase, and the recipient of the negatively charged electron has its oxidation state decrease.
For example, when atomic sodium reacts with atomic chlorine, sodium donates one electron and attains an oxidation state of +1. Chlorine accepts the electron and its oxidation state is reduced to −1. The sign of the oxidation state (positive/negative) actually corresponds to the value of each ion's electronic charge. The attraction of the differently charged sodium and chlorine ions is the reason they then form an ionic bond.
The loss of electrons from an atom or molecule is called oxidation, and the gain of electrons is reduction. This can be easily remembered through the use of mnemonic devices. Two of the most popular are "OIL RIG" (Oxidation Is Loss, Reduction Is Gain) and "LEO" the lion says "GER" (Lose Electrons: Oxidation, Gain Electrons: Reduction). Oxidation and reduction always occur in a paired fashion such that one species is oxidized when another is reduced. For cases where electrons are shared (covalent bonds) between atoms with large differences in electronegativity, the electron is assigned to the atom with the largest electronegativity in determining the oxidation state.
The atom or molecule which loses electrons is known as the "reducing agent", or "reductant", and the substance which accepts the electrons is called the "oxidizing agent", or "oxidant". Thus, the oxidizing agent is always being reduced in a reaction; the reducing agent is always being oxidized. Oxygen is a common oxidizing agent, but not the only one. Despite the name, an oxidation reaction does not necessarily need to involve oxygen. In fact, a fire can be fed by an oxidant other than oxygen; fluorine fires are often unquenchable, as fluorine is an even stronger oxidant (it has a higher electronegativity and thus accepts electrons even better) than oxygen.
For reactions involving oxygen, the gain of oxygen implies the oxidation of the atom or molecule to which the oxygen is added (and the oxygen is reduced). In organic compounds, such as butane or ethanol, the loss of hydrogen implies oxidation of the molecule from which it is lost (and the hydrogen is reduced). This follows because the hydrogen donates its electron in covalent bonds with non-metals but it takes the electron along when it is lost. Conversely, loss of oxygen or gain of hydrogen implies reduction.
Balancing redox reactions.
Electrochemical reactions in water are better understood by balancing redox reactions using the ion-electron method where H+, OH− ion, H2O and electrons (to compensate the oxidation changes) are added to cell's half-reactions for oxidation and reduction.
Acidic medium.
In acid medium H+ ions and water are added to half-reactions to balance the overall reaction.
For example, when manganese reacts with sodium bismuthate.
Finally, the reaction is balanced by multiplying the number of electrons from the reduction half reaction to oxidation half reaction and vice versa and adding both half reactions, thus solving the equation.
Reaction balanced:
Basic medium.
In basic medium OH− ions and water are added to half reactions to balance the overall reaction. For example, on reaction between potassium permanganate and sodium sulfite.
The same procedure as followed on acid medium by multiplying electrons to opposite half reactions solve the equation thus balancing the overall reaction.
Equation balanced:
Neutral medium.
The same procedure as used on acid medium is applied, for example on balancing using electron ion method to complete combustion of propane.
As in acid and basic medium, electrons which were used to compensate oxidation changes are multiplied to opposite half reactions, thus solving the equation.
Equation balanced:
Electrochemical cells.
An electrochemical cell is a device that produces an electric current from energy released by a spontaneous redox reaction. This kind of cell includes the Galvanic cell or Voltaic cell, named after Luigi Galvani and Alessandro Volta, both scientists who conducted several experiments on chemical reactions and electric current during the late 18th century.
Electrochemical cells have two conductive electrodes (the anode and the cathode). The anode is defined as the electrode where oxidation occurs and the cathode is the electrode where the reduction takes place. Electrodes can be made from any sufficiently conductive materials, such as metals, semiconductors, graphite, and even conductive polymers. In between these electrodes is the electrolyte, which contains ions that can freely move.
The galvanic cell uses two different metal electrodes, each in an electrolyte where the positively charged ions are the oxidized form of the electrode metal. One electrode will undergo oxidation (the anode) and the other will undergo reduction (the cathode). The metal of the anode will oxidize, going from an oxidation state of 0 (in the solid form) to a positive oxidation state and become an ion. At the cathode, the metal ion in solution will accept one or more electrons from the cathode and the ion's oxidation state is reduced to 0. This forms a solid metal that electrodeposits on the cathode. The two electrodes must be electrically connected to each other, allowing for a flow of electrons that leave the metal of the anode and flow through this connection to the ions at the surface of the cathode. This flow of electrons is an electrical current that can be used to do work, such as turn a motor or power a light.
A galvanic cell whose electrodes are zinc and copper submerged in zinc sulfate and copper sulfate, respectively, is known as a Daniell cell.
Half reactions for a Daniell cell are these:
In this example, the anode is zinc metal which oxidizes (loses electrons) to form zinc ions in solution, and copper ions accept electrons from the copper metal electrode and the ions deposit at the copper cathode as an electrodeposit. This cell forms a simple battery as it will spontaneously generate a flow of electrical current from the anode to the cathode through the external connection. This reaction can be driven in reverse by applying a voltage, resulting in the deposition of zinc metal at the anode and formation of copper ions at the cathode.
To provide a complete electric circuit, there must also be an ionic conduction path between the anode and cathode electrolytes in addition to the electron conduction path. The simplest ionic conduction path is to provide a liquid junction. To avoid mixing between the two electrolytes, the liquid junction can be provided through a porous plug that allows ion flow while reducing electrolyte mixing. To further minimize mixing of the electrolytes, a salt bridge can be used which consists of an electrolyte saturated gel in an inverted U-tube. As the negatively charged electrons flow in one direction around this circuit, the positively charged metal ions flow in the opposite direction in the electrolyte.
A voltmeter is capable of measuring the change of electrical potential between the anode and the cathode.
Electrochemical cell voltage is also referred to as electromotive force or emf.
A cell diagram can be used to trace the path of the electrons in the electrochemical cell. For example, here is a cell diagram of a Daniell cell:
First, the reduced form of the metal to be oxidized at the anode (Zn) is written. This is separated from its oxidized form by a vertical line, which represents the limit between the phases (oxidation changes). The double vertical lines represent the saline bridge on the cell. Finally, the oxidized form of the metal to be reduced at the cathode, is written, separated from its reduced form by the vertical line. The electrolyte concentration is given as it is an important variable in determining the cell potential.
Standard electrode potential.
To allow prediction of the cell potential, tabulations of standard electrode potential are available. Such tabulations are referenced to the standard hydrogen electrode (SHE). The standard hydrogen electrode undergoes the reaction
which is shown as reduction but, in fact, the SHE can act as either the anode or the cathode, depending on the relative oxidation/reduction potential of the other electrode/electrolyte combination. The term standard in SHE requires a supply of hydrogen gas bubbled through the electrolyte at a pressure of 1 atm and an acidic electrolyte with H+ activity equal to 1 (usually assumed to be [H+] = 1 mol/liter).
The SHE electrode can be connected to any other electrode by a salt bridge to form a cell. If the second electrode is also at standard conditions, then the measured cell potential is called the standard electrode potential for the electrode. The standard electrode potential for the SHE is zero, by definition. The polarity of the standard electrode potential provides information about the relative reduction potential of the electrode compared to the SHE. If the electrode has a positive potential with respect to the SHE, then that means it is a strongly reducing electrode which forces the SHE to be the anode (an example is Cu in aqueous CuSO4 with a standard electrode potential of 0.337 V). Conversely, if the measured potential is negative, the electrode is more oxidizing than the SHE (such as Zn in ZnSO4 where the standard electrode potential is −0.76 V).
Standard electrode potentials are usually tabulated as reduction potentials. However, the reactions are reversible and the role of a particular electrode in a cell depends on the relative oxidation/reduction potential of both electrodes. The oxidation potential for a particular electrode is just the negative of the reduction potential. A standard cell potential can be determined by looking up the standard electrode potentials for both electrodes (sometimes called half cell potentials). The one that is smaller will be the anode and will undergo oxidation. The cell potential is then calculated as the sum of the reduction potential for the cathode and the oxidation potential for the anode.
For example, the standard electrode potential for a copper electrode is:
At standard temperature, pressure and concentration conditions, the cell's emf (measured by a multimeter) is 0.34 V. By definition, the electrode potential for the SHE is zero. Thus, the Cu is the cathode and the SHE is the anode giving
Or,
Changes in the stoichiometric coefficients of a balanced cell equation will not change E°red value because the standard electrode potential is an intensive property.
Spontaneity of redox reaction.
During operation of electrochemical cells, chemical energy is transformed into electrical energy and is expressed mathematically as the product of the cell's emf and the electric charge transferred through the external circuit.
where Ecell is the cell potential measured in volts (V) and Ctrans is the cell current integrated over time and measured in coulombs (C); Ctrans can also be determined by multiplying the total number of electrons transferred (measured in moles) times Faraday's constant (F).
The emf of the cell at zero current is the maximum possible emf. It is used to calculate the maximum possible electrical energy that could be obtained from a chemical reaction. This energy is referred to as electrical work and is expressed by the following equation:
where work is defined as positive into the system.
Since the free energy is the maximum amount of work that can be extracted from a system, one can write:
A positive cell potential gives a negative change in Gibbs free energy. This is consistent with the cell production of an electric current from the cathode to the anode through the external circuit. If the current is driven in the opposite direction by imposing an external potential, then work is done on the cell to drive electrolysis.
A spontaneous electrochemical reaction (change in Gibbs free energy less than zero) can be used to generate an electric current in electrochemical cells. This is the basis of all batteries and fuel cells. For example, gaseous oxygen (O2) and
hydrogen (H2) can be combined in a fuel cell to form water and energy, typically a combination of heat and electrical energy.
Conversely, non-spontaneous electrochemical reactions can be driven forward by the application of a current at sufficient voltage. The electrolysis of water into gaseous oxygen and hydrogen is a typical example.
The relation between the equilibrium constant, "K", and the Gibbs free energy for an electrochemical cell is expressed as follows:
Rearranging to express the relation between standard potential and equilibrium constant yields
The previous equation can use Briggsian logarithm as shown below:
Cell emf dependency on changes in concentration.
Nernst equation.
The standard potential of an electrochemical cell requires standard conditions (ΔG°) for all of the reactants. When reactant concentrations differ from standard conditions, the cell potential will deviate from the standard potential. In the 20th century German chemist Walther Nernst proposed a mathematical model to determine the effect of reactant concentration on electrochemical cell potential.
In the late 19th century, Josiah Willard Gibbs had formulated a theory to predict whether a chemical reaction is spontaneous based on the free energy
Here "ΔG" is change in Gibbs free energy, "ΔG°" is the cell potential when "Q" is equal to 1, "T" is absolute temperature(Kelvin), "R" is the gas constant and "Q" is reaction quotient which can be found by dividing products by reactants using only those products and reactants that are aqueous or gaseous.
Gibbs' key contribution was to formalize the understanding of the effect of reactant concentration on spontaneity.
Based on Gibbs' work, Nernst extended the theory to include the contribution from electric potential on charged species. As shown in the previous section, the change in Gibbs free energy for an electrochemical cell can be related to the cell potential. Thus, Gibbs' theory becomes
Here "n" is the number of electrons/mole product, "F" is the Faraday constant (coulombs/mole), and "ΔE" is cell potential.
Finally, Nernst divided through by the amount of charge transferred to arrive at a new equation which now bears his name:
Assuming standard conditions (T = 25 °C) and R = 8.3145 J/(K·mol), the equation above can be expressed on base—10 logarithm as shown below:
Concentration cells.
A concentration cell is an electrochemical cell where the two electrodes are the same material, the electrolytes on the two half-cells involve the same ions, but the electrolyte concentration differs between the two half-cells.
An example is an electrochemical cell, where two copper electrodes are submerged in two copper(II) sulfate solutions, whose concentrations are 0.05 M and 2.0 M, connected through a salt bridge. This type of cell will generate a potential that can be predicted by the Nernst equation. Both can undergo the same chemistry (although the reaction proceeds in reverse at the anode)
Le Chatelier's principle indicates that the reaction is more favorable to reduction as the concentration of Cu2+ ions increases. Reduction will take place in the cell's compartment where concentration is higher and oxidation will occur on the more dilute side.
The following cell diagram describes the cell mentioned above:
Where the half cell reactions for oxidation and reduction are:
The cell's emf is calculated through Nernst equation as follows:
The value of "E"° in this kind of cell is zero, as electrodes and ions are the same in both half-cells.
After replacing values from the case mentioned, it is possible to calculate cell's potential:
or by:
However, this value is only approximate, as reaction quotient is defined in terms of ion activities which can be approximated with the concentrations as calculated here.
The Nernst equation plays an important role in understanding electrical effects in cells and organelles. Such effects include nerve synapses and cardiac beat as well as the resting potential of a somatic cell.
Battery.
Many types of battery have been commercialized and represent an important practical application of electrochemistry. Early wet cells powered the first telegraph and telephone systems, and were the source of current for electroplating. The zinc-manganese dioxide dry cell was the first portable, non-spillable battery type that made flashlights and other portable devices practical. The mercury battery using zinc and mercuric oxide provided higher levels of power and capacity than the original dry cell for early electronic devices, but has been phased out of common use due to the danger of mercury pollution from discarded cells.
The lead–acid battery was the first practical secondary (rechargeable) battery that could have its capacity replenished from an external source. The electrochemical reaction that produced current was (to a useful degree) reversible, allowing electrical energy and chemical energy to be interchanged as needed. Common lead acid batteries contain a mixture of acid and water, as well as lead plates. The most common mixture used today is 30% acid. One problem however is if left uncharged acid will crystallize within the lead plates of the battery rendering it useless. These batteries last an average of 3 years with daily use however it is not unheard of for a lead acid battery to still be functional after 7–10 years. Lead-acid cells continue to be widely used in automobiles.
All the preceding types have water-based electrolytes, which limits the maximum voltage per cell. The freezing of water limits low temperature performance. The lithium battery, which does not (and cannot) use water in the electrolyte, provides improved performance over other types; a rechargeable lithium ion battery is an essential part of many mobile devices.
The flow battery, an experimental type, offers the option of vastly larger energy capacity because its reactants can be replenished from external reservoirs. The fuel cell can turn the chemical energy bound in hydrocarbon gases or hydrogen directly into electrical energy with much higher efficiency than any combustion process; such devices have powered many spacecraft and are being applied to grid energy storage for the public power system.
Corrosion.
Corrosion is the term applied to steel rust caused by an electrochemical process. Most people are likely familiar with the corrosion of iron, in the form of reddish rust. Other examples include the black tarnish on silver, and red or green corrosion that may appear on copper and its alloys, such as brass. The cost of replacing metals lost to corrosion is in the multi-billions of dollars per year.
Iron corrosion.
For iron rust to occur the metal has to be in contact with oxygen and water, although chemical reactions for this process are relatively complex and not all of them are completely understood, it is believed the causes are the following:
Electron transferring (reduction-oxidation)
Iron corrosion takes place on acid medium; H+ ions come from reaction between carbon dioxide in the atmosphere and water, forming carbonic acid. Fe2+ ions oxides, following this equation:
Iron(III) oxide hydrated is known as rust. The concentration of water associated with iron oxide varies, thus chemical representation is presented as Fe2O3·x H2O.
The electric circuit works as passage of electrons and ions occurs, thus if an electrolyte is present it will facilitate oxidation, this explains why rusting is quicker on salt water.
Corrosion of common metals.
Coinage metals, such as copper and silver, slowly corrode through use.
A patina of green-blue copper carbonate forms on the surface of copper with exposure to the water and carbon dioxide in the air. Silver coins or cutlery that are exposed to high sulfur foods such as eggs or the low levels of sulfur species in the air develop a layer of black Silver sulfide.
Gold and platinum are extremely difficult to oxidize under normal circumstances, and require exposure to a powerful chemical oxidizing agent such as aqua regia.
Some common metals oxidize extremely rapidly in air. Titanium and aluminium oxidize instantaneously in contact with the oxygen in the air. These metals form an extremely thin layer of oxidized metal on the surface. This thin layer of oxide protects the underlying layers of the metal from the air preventing the entire metal from oxidizing. These metals are used in applications where corrosion resistance is important. Iron, in contrast, has an oxide that forms in air and water, called rust, that does not stop the further oxidation of the iron. Thus iron left exposed to air and water will continue to rust until all of the iron is oxided.
Prevention of corrosion.
Attempts to save a metal from becoming anodic are of two general types. Anodic regions dissolve and destroy the structural integrity of the metal.
While it is almost impossible to prevent anode/cathode formation, if a non-conducting material covers the metal, contact with the electrolyte is not possible and corrosion will not occur.
Coating.
Metals can be coated with paint or other less conductive metals ("passivation"). This prevents the metal surface from being exposed to electrolytes. Scratches exposing the metal substrate will result in corrosion. The region under the coating adjacent to the scratch acts as the anode of the reaction.
See Anodizing
Sacrificial anodes.
A method commonly used to protect a structural metal is to attach a metal which is more anodic than the metal to be protected. This forces the structural metal to be cathodic, thus spared corrosion. It is called "sacrificial" because the anode dissolves and has to be replaced periodically.
Zinc bars are attached to various locations on steel ship hulls to render the ship hull cathodic. The zinc bars are replaced periodically. Other metals, such as magnesium, would work very well but zinc is the least expensive useful metal.
To protect pipelines, an ingot of buried or exposed magnesium (or zinc) is buried beside the pipeline and is connected electrically to the pipe above ground. The pipeline is forced to be a cathode and is protected from being oxidized and rusting. The magnesium anode is sacrificed. At intervals new ingots are buried to replace those lost.
Electrolysis.
The spontaneous redox reactions of a conventional battery produce electricity through the different chemical potentials of the cathode and anode in the electrolyte. However, electrolysis requires an external source of electrical energy to induce a chemical reaction, and this process takes place in a compartment called an electrolytic cell.
Electrolysis of molten sodium chloride.
When molten, the salt sodium chloride can be electrolyzed to yield metallic sodium and gaseous chlorine. Industrially this process takes place in a special cell named Down's cell. The cell is connected to an electrical power supply, allowing electrons to migrate from the power supply to the electrolytic cell.
Reactions that take place at Down's cell are the following:
This process can yield large amounts of metallic sodium and gaseous chlorine, and is widely used on mineral dressing and metallurgy industries.
The emf for this process is approximately −4 V indicating a (very) non-spontaneous process. In order for this reaction to occur the power supply should provide at least a potential of 4 V. However, larger voltages must be used for this reaction to occur at a high rate.
Electrolysis of water.
Water can be converted to its component elemental gasses, H2 and O2 through the application of an external voltage. Water doesn't decompose into hydrogen and oxygen spontaneously as the Gibbs free energy for the process at standard conditions is about 474.4 kJ. The decomposition of water into hydrogen and oxygen can be performed in an electrolytic cell. In it, a pair of inert electrodes usually made of platinum immersed in water act as anode and cathode in the electrolytic process. The electrolysis starts with the application of an external voltage between the electrodes. This process will not occur except at extremely high voltages without an electrolyte such as sodium chloride or sulfuric acid (most used 0.1 M).
Bubbles from the gases will be seen near both electrodes. The following half reactions describe the process mentioned above:
Although strong acids may be used in the apparatus, the reaction will not net consume the acid. While this reaction will work at any conductive electrode at a sufficiently large potential, platinum catalyzes both hydrogen and oxygen formation, allowing for relatively mild voltages (~2 V depending on the pH).
Electrolysis of aqueous solutions.
Electrolysis in an aqueous is a similar process as mentioned in electrolysis of water. However, it is considered to be a complex process because the contents in solution have to be analyzed in half reactions, whether reduced or oxidized.
Electrolysis of a solution of sodium chloride.
The presence of water in a solution of sodium chloride must be examined in respect to its reduction and oxidation in both electrodes. Usually, water is electrolysed as mentioned in electrolysis of water yielding "gaseous oxygen in the anode" and gaseous hydrogen in the cathode. On the other hand, sodium chloride in water dissociates in Na+ and Cl− ions, cation, which is the positive ion, will be attracted to the cathode (-), thus reducing the sodium ion. The anion will then be attracted to the anode (+) oxidizing chloride ion.
The following half reactions describes the process mentioned:
Reaction 1 is discarded as it has the most negative value on standard reduction potential thus making it less thermodynamically favorable in the process.
When comparing the reduction potentials in reactions 2 and 4, the reduction of chloride ion is favored. Thus, if the Cl− ion is favored for reduction, then the water reaction is favored for oxidation producing gaseous oxygen, however experiments show gaseous chlorine is produced and not oxygen.
Although the initial analysis is correct, there is another effect that can happen, known as the overvoltage effect. Additional voltage is sometimes required, beyond the voltage predicted by the E°cell. This may be due to kinetic rather than thermodynamic considerations. In fact, it has been proven that the activation energy for the chloride ion is very low, hence favorable in kinetic terms. In other words, although the voltage applied is thermodynamically sufficient to drive electrolysis, the rate is so slow that to make the process proceed in a reasonable time frame, the voltage of the external source has to be increased (hence, overvoltage).
Finally, reaction 3 is favorable because it describes the proliferation of OH− ions thus letting a probable reduction of H+ ions less favorable an option.
The overall reaction for the process according to the analysis would be the following:
As the overall reaction indicates, the concentration of chloride ions is reduced in comparison to OH− ions (whose concentration increases). The reaction also shows the production of gaseous hydrogen, chlorine and aqueous sodium hydroxide.
Quantitative electrolysis and Faraday's laws.
Quantitative aspects of electrolysis were originally developed by Michael Faraday in 1834. Faraday is also credited to have coined the terms "electrolyte", electrolysis, among many others while he studied quantitative analysis of electrochemical reactions. Also he was an advocate of the law of conservation of energy.
First law.
Faraday concluded after several experiments on electrical current in non-spontaneous process, the mass of the products yielded on the electrodes was proportional to the value of current supplied to the cell, the length of time the current existed, and the molar mass of the substance analyzed. In other words, the amount of a substance deposited on each electrode of an electrolytic cell is directly proportional to the quantity of electricity passed through the cell.
Below is a simplified equation of Faraday's first law:
Where
Second law.
Faraday devised the laws of chemical electrodeposition of metals from solutions in 1857. He formulated the second law of electrolysis stating "the amounts of bodies which are equivalent to each other in their ordinary chemical action have equal quantities of electricity naturally associated with them." In other words, the quantities of different elements deposited by a given amount of electricity are in the ratio of their chemical equivalent weights.
An important aspect of the second law of electrolysis is electroplating which together with the first law of electrolysis, has a significant number of applications in the industry, as when used to protect metals to avoid corrosion.
Applications.
There are various extremely important electrochemical processes in both nature and industry, like the coating of objects with metals or metal oxides through electrodeposition and the detection of alcohol in drunken drivers through the redox reaction of ethanol. The generation of chemical energy through photosynthesis is inherently an electrochemical process, as is production of metals like aluminum and titanium from their ores. Certain diabetes blood sugar meters measure the amount of glucose in the blood through its redox potential. As well as the established electrochemical technologies (like deep cycle lead acid batteries) there is also a wide range of new emerging technologies such as fuel cells, large format lithium ion batteries, electrochemical reactors and super-capacitors that are becoming increasingly commercial
The action potentials that travel down neurons are based on electric current generated by the movement of sodium and potassium ions into and out of cells. Specialized cells in certain animals like the electric eel can generate electric currents powerful enough to disable much larger animals.

</doc>
<doc id="9602" url="http://en.wikipedia.org/wiki?curid=9602" title="Edinburgh">
Edinburgh

Edinburgh (; Scottish Gaelic: "Dùn Èideann") is the capital city of Scotland, located in Lothian on the southern shore of the Firth of Forth. It is the second most populous city in Scotland and the seventh most populous in the United Kingdom. The population in 2013 was 487,500. Edinburgh lies at the heart of a larger urban zone with a population of 778,000. Recognised as the capital of Scotland since at least the 15th century, Edinburgh is home to the Scottish Parliament and the seat of the monarchy in Scotland. The city is also the annual venue of the General Assembly of the Church of Scotland and home to national institutions such as the National Museum of Scotland, the National Library of Scotland and the Scottish National Gallery. It is the largest financial centre in the UK after London.
The city has long been known as a centre of education, particularly in the fields of medicine, Scots law, the sciences and engineering. The University of Edinburgh, founded in 1583 and now one of four in the city, was placed 17th in the QS World University Rankings in 2014. The city is also famous for the Edinburgh International Festival and the Fringe, the latter being the largest annual international arts festival in the world. The city's historical and cultural attractions have made it the second most popular tourist destination in the United Kingdom after London, attracting over one million overseas visitors each year . Historic sites in Edinburgh include Edinburgh Castle, Holyrood Palace, the churches of St. Giles, Greyfriars and the Canongate, and the extensive Georgian New Town, built in the 18th century. Edinburgh's Old Town and New Town together are listed as a UNESCO World Heritage Site, which have been managed by Edinburgh World Heritage Trust since 1999.
Etymology.
"Edin", the root of the city's name, is most likely of Brittonic Celtic origin, from the Cumbric language or a variation of it that would have been spoken by the earliest known people of the area, an Iron Age tribe known to the Romans as the "Votadini", and latterly in sub-Roman history as the Gododdin. It appears to derive from the place name "Eidyn" mentioned in the Old Welsh epic poem "Y Gododdin".
The poem names "Din Eidyn" as a hill fort ("Din" meaning "dun") in the territory of the Gododdin. The change in nomenclature, from "Din Eidyn" to "Edinburgh", reflects changes in the local language from Cumbric to Old English, the Germanic language of the Anglian kingdom of Bernicia that permeated the area from the mid-7th century and is regarded as the ancestor of modern Scots. The Celtic element "din" was dropped and replaced by the Old English "burh". The first documentary evidence of the medieval burgh is a royal charter, c.1124–1127, by King David I granting a toft in "burgo meo de Edenesburg" to the Priory of Dunfermline.
History.
Early history.
The earliest known human habitation in the Edinburgh area is from Cramond where evidence was found of a Mesolithic camp-site dated to c. 8500 BC. Traces of later Bronze Age and Iron Age settlements have been found on Castle Rock, Arthur's Seat, Craiglockhart Hill and the Pentland Hills.
When the Romans arrived in Lothian at the end of the 1st century AD, they discovered a Celtic Britonnic tribe whose name they recorded as the Votadini. At some point before the 7th century AD, the Gododdin, who were presumably descendants of the Votadini, built the hill fort of "Din Eidyn" or "Etin". Although its exact location has not been identified, it seems more than likely they would have chosen a commanding position like the Castle Rock or Arthur's Seat or Calton Hill.
In 638 AD the Gododdin stronghold was besieged by forces loyal to King Oswald of Northumbria, and around this time control of Lothian passed to the Angles. Their influence continued for the next three centuries until around 950 AD, when, during the reign of Indulf, son of Constantine II, the "burh" (fortress), named in the 10th-century "Pictish Chronicle" as "oppidum Eden", fell to the Scots and thenceforth remained under their jurisdiction.
The royal burgh was founded by King David I in the early 12th century on land belonging to the Crown, though the precise date is unknown. By the middle of the 14th century, the French chronicler Jean Froissart was describing it as the capital of Scotland (c.1365), and James III (1451–88) referred to it in the 15th century as "the principal burgh of our kingdom". Despite the destruction caused by an English assault in 1544, the town slowly recovered, and was at the centre of events in the 16th-century Scottish Reformation and 17th-century Wars of the Covenant.
17th century.
In 1603, King James VI of Scotland succeeded to the English throne, uniting the crowns of Scotland and England in a personal union known as the Union of the Crowns, though Scotland remained, in all other respects, a separate kingdom. In 1638, King Charles I's attempt to introduce Anglican church forms in Scotland encountered stiff Presbyterian opposition culminating in the conflicts of the Wars of the Three Kingdoms. Subsequent Scottish support for Charles Stuart's restoration to the throne of England resulted in Edinburgh's occupation by Oliver Cromwell's Commonwealth of England forces – the New Model Army – in 1650.
In the 17th century, the boundaries of Edinburgh were still defined by the city's defensive town walls. As a result, expansion took the form of the houses increasing in height to accommodate a growing population. Buildings of 11 storeys or more were common, and have been described as forerunners of the modern-day skyscraper. Most of these old structures were later replaced by the predominantly Victorian buildings seen in today's Old Town.
18th century.
In 1706 and 1707, the Acts of Union were passed by the Parliaments of England and Scotland uniting the two kingdoms into the Kingdom of Great Britain. As a consequence, the Parliament of Scotland merged with the Parliament of England to form the Parliament of Great Britain, which sat at Westminster in London. The Union was opposed by many Scots at the time, resulting in riots in the city.
By the first half of the 18th century, despite rising prosperity evidenced by its growing importance as a banking centre, Edinburgh was being described as one of the most densely populated, overcrowded and unsanitary towns in Europe. Visitors were struck by the fact that the various social classes shared the same urban space, even inhabiting the same tenement buildings; although here a form of social segregation did prevail, whereby shopkeepers and tradesmen tended to occupy the cheaper-to-rent cellars and garrets, while the more well-to-do professional classes occupied the more expensive middle storeys.
During the Jacobite rising of 1745, Edinburgh was briefly occupied by the Jacobite "Highland Army" before its march into England. After its eventual defeat at Culloden, there followed a period of reprisals and pacification, largely directed at the rebellious clans. In Edinburgh, the Town Council, keen to emulate London by initiating city improvements and expansion to the north of the castle, re-affirmed its belief in the Union and loyalty to the Hanoverian monarch George III by its choice of names for the streets of the New Town, for example, Rose Street and Thistle Street, and for the royal family: George Street, Queen Street, Hanover Street, Frederick Street and Princes Street (in honour of George's two sons).
In the second half of the century, the city was at the heart of the Scottish Enlightenment, when thinkers like David Hume, Adam Smith, James Hutton and Joseph Black were familiar figures in its streets. Edinburgh became a major intellectual centre, earning it the nickname "Athens of the North" because of its many neo-classical buildings and reputation for learning, similar to Ancient Athens. In the 18th century novel "The Expedition of Humphry Clinker" by Tobias Smollett one character describes Edinburgh as a "hotbed of genius".
From the 1770s onwards, the professional and business classes gradually deserted the Old Town in favour of the more elegant "one-family" residences of the New Town, a migration that changed the social character of the city. According to the foremost historian of this development, "Unity of social feeling was one of the most valuable heritages of old Edinburgh, and its disappearance was widely and properly lamented."
19th and 20th centuries.
Although Edinburgh's traditional industries of printing, brewing and distilling continued to grow in the 19th century and were joined by new rubber works and engineering works there was little industrialisation compared with other cities in Britain. By 1821, Edinburgh had been overtaken by Glasgow as Scotland's largest city. The city centre between Princes Street and George Street became a major commercial and shopping district, a development partly stimulated by the arrival of railways in the 1840s. The Old Town became an increasingly dilapidated, overcrowded slum with high mortality rates. Improvements carried out under Lord Provost William Chambers in the 1860s began the transformation of the area into the predominantly Victorian Old Town seen today. More improvements followed in the early 20th century as a result of the work of Patrick Geddes, but relative economic stagnation during the two world wars and beyond saw the Old Town deteriorate further before major slum clearance in the 1960s and 1970s began to reverse the process. University building developments which transformed the George Square and Potterrow areas proved highly controversial.
Since the 1990s a new "financial district", including a new Edinburgh International Conference Centre, has grown mainly on demolished railway property to the west of the castle, stretching into Fountainbridge, a run-down 19th-century industrial suburb which has undergone radical change since the 1980s with the demise of industrial and brewery premises. This ongoing development has enabled Edinburgh to maintain its place as the second largest financial and administrative centre in the United Kingdom after London. Financial services now account for a third of all commercial office space in the city. The development of Edinburgh Park, a new business and technology park covering 38 acres, 4 mi west of the city centre, has also contributed to the District Council's strategy for the city's major economic regeneration.
In 1998, the Scotland Act, which came into force the following year, established a devolved Scottish Parliament and Scottish Executive (renamed the Scottish Government since September 2007 ). Both based in Edinburgh, they are responsible for governing Scotland while reserved matters such as defence, taxation and foreign affairs remain the responsibility of the Parliament of the United Kingdom in London.
Geography.
Cityscape.
Situated in Scotland's Central Belt, Edinburgh lies on the southern shore of the Firth of Forth. The city centre is 2+1/2 mi southwest of the shoreline of Leith and 26 mi inland, as the crow flies, from the east coast of Scotland and the North Sea at Dunbar. While the early burgh grew up in close proximity to the prominent Castle Rock, the modern city is often said to be built on seven hills, namely Calton Hill, Corstorphine Hill, Craiglockhart Hill, Braid Hill, Blackford Hill, Arthur's Seat and the Castle Rock, giving rise to allusions to the seven hills of Rome.
Occupying a narrow gap between the Firth of Forth to the north and the Pentland Hills and their outrunners to the south, the city sprawls over a landscape which is the product of early volcanic activity and later periods of intensive glaciation.
 Igneous activity between 350 and 400 million years ago, coupled with faulting, led to the creation of tough basalt volcanic plugs, which predominate over much of the area. One such example is the Castle Rock which forced the advancing icesheet to divide, sheltering the softer rock and forming a 1 mi tail of material to the east, thus creating a distinctive crag and tail formation. Glacial erosion on the north side of the crag gouged a deep valley later filled by the now drained Nor Loch. These features, along with another hollow on the south side of the rock, formed an ideal natural strongpoint upon which Edinburgh Castle was built. Similarly, Arthur's Seat is the remains of a volcano dating from the Carboniferous period, which was eroded by a glacier moving west to east during the ice age. Erosive action such as plucking and abrasion exposed the rocky crags to the west before leaving a tail of deposited glacial material swept to the east. This process formed the distinctive Salisbury Crags, a series of teschenite cliffs between Arthur's Seat and the location of the early burgh. The residential areas of Marchmont and Bruntsfield are built along a series of drumlin ridges south of the city centre, which were deposited as the glacier receded.
Other prominent landforms such as Calton Hill and Corstorphine Hill are similarly products of glacial erosion. The Braid Hills and Blackford Hill are a series of small summits to the south west of the city commanding expansive views looking northwards over the urban area to the Forth.
Edinburgh is drained by the river named the Water of Leith, which rises at the Colzium Springs in the Pentland Hills and runs for 29 km through the south and west of the city, emptying into the Firth of Forth at Leith. The nearest the river gets to the city centre is at Dean Village on the north-western edge of the New Town, where a deep gorge is spanned by Thomas Telford's Dean Bridge, built in 1832 for the road to Queensferry. The Water of Leith Walkway is a mixed use trail that follows the course of the river for 19.6 km from Balerno to Leith.
Excepting the shoreline of the Firth of Forth, Edinburgh is encircled by a green belt, designated in 1957, which stretches from Dalmeny in the west to Prestongrange in the east. With an average width of 3.2 km the principal objectives of the green belt were to contain the outward expansion of the city and to prevent the agglomeration of urban areas. Expansion affecting the green belt is strictly controlled but developments such as Edinburgh Airport and the Royal Highland Showground at Ingliston lie within the zone. Similarly, outlying suburbs such as Juniper Green and Balerno are situated on green belt land. One feature of the Edinburgh green belt is the inclusion of parcels of land within the city which are designated green belt, even though they do not connect with the peripheral ring. Examples of these independent wedges of green belt include Holyrood Park and Corstorphine Hill.
Areas.
Edinburgh is divided into distinct areas that retain much of their original character as settlements in existence before they were absorbed into the sprawling city of the nineteenth century. Many residences are multi-occupancy buildings known as tenements, although the more southern and western parts of the city have traditionally been more affluent with a greater number of detached and semi-detached villas.
The historic centre of Edinburgh is divided in two by the broad green swath of Princes Street Gardens. To the south the view is dominated by Edinburgh Castle, built high on the castle rock, and the long sweep of the Old Town descending towards Holyrood Palace. To the north lie Princes Street and the New Town.
The West End includes the financial district, with insurance and banking offices as well as the Edinburgh International Conference Centre.
The Old and New Towns of Edinburgh were listed as a UNESCO World Heritage Site in 1995 in recognition of the unique character of the Old Town with its medieval street layout and the planned Georgian New Town, including the adjoining Dean Village and Calton Hill areas. There are over 4,500 listed buildings within the city, a higher proportion relative to area than any other city in the United Kingdom.
The Old Town runs downhill and terminates at Holyrood Palace. Minor streets (called closes or wynds) lie on either side of the main spine forming a herringbone pattern. The street has several fine public buildings such as the church of St Giles, the City Chambers and the Law Courts. Other places of historical interest nearby are Greyfriars Kirkyard and the Grassmarket. The street layout is typical of the old quarters of many northern European cities.
The castle perches on top of a rocky crag (the remnant of an extinct volcano) and the Royal Mile runs down the crest of a ridge from it. Due to space restrictions imposed by the narrowness of this landform, the Old Town became home to some of the earliest "high rise" residential buildings. Multi-storey dwellings known as "lands" were the norm from the 16th century onwards with ten and eleven storeys being typical and one even reaching fourteen or fifteen storeys.
 Numerous vaults below street level were inhabited to accommodate the influx of incomers, particularly Irish immigrants, during the Industrial Revolution.
The New Town was an 18th-century solution to the problem of an increasingly crowded city which had been confined to the ridge sloping down from the castle. In 1766 a competition to design a "New Town" was won by James Craig, a 27-year-old architect. The plan was a rigid, ordered grid, which fitted in well with Enlightenment ideas of rationality. The principal street is George Street, running along the natural ridge to the north of what became known as the "Old Town". To either side of it are two other main streets: Princes Street and Queen Street. Princes Street has become the main shopping street in Edinburgh and now has few of its original Georgian buildings. The three main streets are connected by a series of streets running perpendicular to them. The east and west ends of George Street are terminated by St Andrew Square and Charlotte Square respectively. The latter, designed by Robert Adam, influenced the architectural style of the New Town into the early 19th century. Bute House, the official residence of the First Minister of Scotland, is on the north side of Charlotte Square.
The hollow between the Old and New Towns was formerly the Nor Loch, which was originally created for the town's defence but came to be used by the inhabitants for dumping their sewage. It was drained by the 1820s as part of the city's northward expansion. Craig's original plan included an ornamental canal on the site of the loch, but this idea was abandoned. Soil excavated while laying the foundations of buildings in the New Town was dumped on the site of the loch to create the slope connecting the Old and New Towns known as The Mound.
In the middle of the 19th century the National Gallery of Scotland and Royal Scottish Academy Building were built on The Mound, and tunnels for the railway line between Haymarket and Waverley stations were driven through it.
The Southside is a popular residential part of the city, which includes the districts of St Leonards, Marchmont, Newington, Sciennes, the Grange and Blackford. The Southside is broadly analogous to the area covered formerly by the Burgh Muir, and grew in popularity as a residential area after the opening of the South Bridge in the 1780s. The Southside is particularly popular with families (many state and private schools are here), young professionals and students (the central University of Edinburgh campus is based around George Square just north of Marchmont and the Meadows), and Napier University (with major campuses around Merchiston and Morningside). The area is also well provided with hotel and "bed and breakfast" accommodation for visiting festival-goers. These districts often feature in works of fiction. For example, Church Hill in Morningside, was the home of Muriel Spark's Miss Jean Brodie,
and Ian Rankin's Inspector Rebus lives in Marchmont and works in St Leonards.
Leith was historically the port of Edinburgh, an arrangement of unknown date that was reconfirmed by the royal charter Robert the Bruce granted to the city in 1329. The port developed a separate identity from Edinburgh, which to some extent it still retains, and it was a matter of great resentment when the two burghs merged in 1920 into the City of Edinburgh. Even today the parliamentary seat is known as "Edinburgh North and Leith". The loss of traditional industries and commerce (the last shipyard closed in 1983) resulted in economic decline. In 2012, the City of Edinburgh Council announced a programme of environmental improvements which will see £5.5 million spent on the area over the next two years
The Granton Waterfront Edinburgh development, which has transformed the shore at Granton's residential area with leisure amenities, has helped rejuvenate the area.
The urban area of Edinburgh is almost entirely contained within the City of Edinburgh Council boundary, merging with Musselburgh in East Lothian. Towns within easy reach of the city boundary include Dalkeith, Bonnyrigg, Loanhead, Newtongrange, Prestonpans, Tranent, Penicuik, Haddington, Livingston, Broxburn and Dunfermline. According to the European Statistical agency, Eurostat, Edinburgh lies at the heart of a Larger urban zone covering 1724 km2 with a population of 778,000.
Climate.
Like most of Scotland, Edinburgh has a temperate, maritime climate which is relatively mild despite its northerly latitude. Winter daytime temperatures rarely fall below freezing and are milder than places such as Moscow and Newfoundland which lie at similar latitudes. Summer temperatures are normally moderate, rarely exceeding 22 C. The highest temperature ever recorded in the city was 31.4 C on 4 August 1975 at Turnhouse Airport. The lowest temperature recorded in recent years was -14.6 C during December 2010 at Gogarbank.
The proximity of the city to the sea mitigates any large variations in temperature or extremes of climate. Given Edinburgh's position between the coast and hills, it is renowned as "the windy city", with the prevailing wind direction coming from the south west, which is frequently associated with warm, unstable air from the North Atlantic Current that can give rise to rainfall – although considerably less than cities to the west, such as Glasgow. Rainfall is distributed fairly evenly throughout the year. Winds from an easterly direction are usually drier but considerably colder, and may be accompanied by haar, a persistent coastal fog. Vigorous Atlantic depressions, known as European windstorms, can affect the city between October and May.
Demography.
Current.
At the United Kingdom Census 2011, Edinburgh had a population of 476,626, showing a rise of 6.2% since 2001. This makes Edinburgh the second largest city in Scotland after Glasgow and the seventh largest in Britain. Edinburgh lies at the heart of a Larger Urban Zone with a population of 778,000.
Edinburgh has a high proportion of young adults, with 19.5% of the population in their 20s (exceeded only by Aberdeen) and 15.2% in their 30s which is the highest in Scotland. The proportion of Edinburgh's population who were born in the UK fell from 92% to 84% between 2001 and 2011, while the proportion born in Scotland fell from 78% to 70%. Of those Edinburgh residents born in the UK, 335,000 or 83% were born in Scotland, with 58,000 or 14% being born in England.
The proportion of people born outside the UK was 15.9% comparing with 8% in 2001. Countries accounting for the largest number of Edinburgh citizens born overseas are: Poland (13,000), Republic of Ireland (8,603), China (8,076), India (6,470), Pakistan (5,858), United States (3,700), Germany (3,500), Australia (2,100), France (2,000) Spain (2,000), South Africa (1,800) and Canada (1,800). 47% of the non-UK born population in Edinburgh is of European origin, which is amongst the highest for any city in the UK.
Some 13,000 people or 2.7% of the city's total population are Polish. 39,500 people or 8.2% of Edinburgh's population class themselves as Non-White which is an increase from 4% in 2001. Of the Non-White population, the largest group by far are Asian, totalling 26,264 people. Within the Asian population, the Chinese are now the largest sub-group, with 8,076 people, amounting to about 1.7% of the city's total population. The city's Indian population amounts to 6,470 (1.4% of the total population), while there are some 5,858 Pakistanis (1.2% of the total population). Although they account for only 1,277 people or 0.3% of the city's population, Edinburgh has the highest number and proportion of Bangladeshis in Scotland. Over 7,000 people were born in African countries (1.6% of the total population) and nearly 7,000 in the Americas. With the notable exception of Inner London, Edinburgh has a higher number of people born in the United States (over 3,700) than any other city in the UK.
Historical.
A census conducted by the Edinburgh presbytery in 1592 recorded a population of 8,003 adults spread equally north and south of the High Street which runs along the spine of the ridge sloping down from the Castle. In the 18th and 19th centuries, the population expanded rapidly, rising from 49,000 in 1751 to 136,000 in 1831, primarily due to migration from rural areas. As the population grew, problems of overcrowding in the Old Town, particularly in the cramped tenements that lined the present day Royal Mile and the Cowgate, were exacerbated. Poor sanitary arrangements resulted in a high incidence of disease, with outbreaks of cholera occurring in 1832, 1848 and 1866.
The construction of the New Town from 1767 onwards witnessed the migration of the professional and business classes from the difficult living conditions in the Old Town to the lower density, higher quality surroundings taking shape on land to the north.
 Expansion southwards from the Old Town saw more tenements being built in the 19th century, giving rise to Victorian suburbs such as Newington, Marchmont and Bruntsfield.
Early 20th century population growth coincided with lower-density suburban development. As the city expanded to the south and west, detached and semi-detached villas with large gardens replaced tenements as the predominant building style. Nonetheless, the 2001 census revealed that over 55% of Edinburgh's population were still living in tenements or blocks of flats, a figure in line with other Scottish cities, but much higher than other British cities, and even central London.
From the early to mid 20th century the growth in population, together with slum clearance in the Old Town and other areas, such as Dumbiedykes, Leith, and Fountainbridge, led to the creation of new estates such as Stenhouse and Saughton, Craigmillar and Niddrie, Pilton and Muirhouse, Piershill, and Sighthill.
Religion.
The Church of Scotland claims the largest membership of any single religious denomination in Edinburgh. In 2010 there were 83 congregations in the Presbytery of Edinburgh. Its most prominent church is St Giles on the Royal Mile, first dedicated in 1243 but believed to date from before the 12th century. Saint Giles is historically the patron saint of Edinburgh. St Cuthbert's, situated at the west end of Princes Street Gardens in the shadow of Edinburgh Castle and St Giles' can lay claim to being the oldest Christian sites in the city, though the present St Cuthbert's, designed by Hippolyte Blanc, was dedicated in 1894.
Other Church of Scotland churches include Greyfriars Kirk, the Canongate Kirk, St Andrew's and St George's West Church and the Barclay Church. The Church of Scotland Offices are in Edinburgh, as is the Assembly Hall where the annual General Assembly is held.
The Roman Catholic Archdiocese of St Andrews and Edinburgh has 27 parishes across the city. The Archbishop of the Archdiocese of St Andrew's and Edinburgh has his official residence in Greenhill, and the diocesan offices are in nearby Marchmont. The Diocese of Edinburgh of the Scottish Episcopal Church has over 50 churches, half of them in the city. Its centre is the late 19th century Gothic style St Mary's Cathedral in the West End's Palmerston Place. There are several independent churches in the city, both Catholic and Protestant, including Charlotte Chapel, Carrubbers Christian Centre, Morningside Baptist Church, Bellevue Chapel and Sacred Heart. There are also churches belonging to Quakers, Christadelphians, Seventh-day Adventists, Church of Christ, Scientist and The Church of Jesus Christ of Latter-day Saints (LDS Church).
Edinburgh Central Mosque – Edinburgh's main mosque and Islamic Centre – is in Potterrow, on the city's Southside, near Bristo Square. Construction was largely financed by a gift from King Fahd of Saudi Arabia and was completed in 1998. There are other mosques in Annandale Street Lane, off Leith Walk, and in Queensferry Road, Blackhall as well as other Islamic centres across the city. There is also an active presence of the Ahmadiyya Muslim community.
The first recorded presence of a Jewish community in Edinburgh dates back to the late 18th century. Edinburgh's Orthodox synagogue, opened in 1932, is in Salisbury Road and can accommodate a congregation of 2000. A Liberal Jewish congregation also meets in the city. There are a Sikh gurdwara and a Hindu mandir, both in Leith, and a Brahma Kumaris centre in the Polwarth area. The Edinburgh Buddhist Centre, run by the Triratna Buddhist Community, formerly situated in Melville Terrace, now runs sessions at the Healthy Life Centre, Bread Street. Other Buddhist traditions are represented by groups which meet in the capital: the Community of Interbeing (followers of Thich Nhat Hanh), Rigpa, Samye Dzong, Theravadin, Pure Land and Shambala. There is a Sōtō Zen Priory in Portobello and a Theravadin Thai Buddhist Monastery in Slateford Road. Edinburgh is home to an active Bahá'í Community, and a Theosophical Society meets in Great King Street. Edinburgh has an active Inter-Faith Association.
Economy.
Edinburgh has the strongest economy of any city in the United Kingdom outside of London and the highest percentage of professionals in the UK with 43% of the population holding a degree-level or professional qualification. According to the Centre for International Competitiveness, it is the most competitive large city in the United Kingdom. It also has the highest gross value added per employee of any city in the UK outside London, measuring £57,594 in 2010. It was named European "Best Large City of the Future for Foreign Direct Investment" and "Best Large City for Foreign Direct Investment Strategy" in the" Financial Times "fDi magazine awards 2012/13.
Known primarily for brewing and distilling, banking and insurance and printing and publishing in the 19th century, Edinburgh's economy is now based mainly on financial services, scientific research, higher education, and tourism. In March 2010 unemployment in Edinburgh was comparatively low at 3.6%, and it remains consistently below the Scottish average of 4.5%. Edinburgh is the 2nd most visited city by foreign visitors in the UK after London.
Banking has been a mainstay of the Edinburgh economy for over 300 years, since the Bank of Scotland (now part of the Lloyds Banking Group) was established by an act of the Scottish Parliament in 1695. Today, the financial services industry, with its particularly strong insurance and investment sectors, and underpinned by Edinburgh-based firms such as Scottish Widows and Standard Life, accounts for the city being the UK's second financial centre after London and Europe's fourth in terms of equity assets. The Royal Bank of Scotland opened new global headquarters at Gogarburn in the west of the city in October 2005, and Edinburgh is home to the headquarters of Bank of Scotland, Tesco Bank and Virgin Money.
Tourism is also an important element in the city's economy. As a World Heritage Site, tourists come to visit historical sites such as Edinburgh Castle, the Palace of Holyroodhouse and view the Old and New Towns. Their numbers are augmented in August each year during the Edinburgh Festivals, which attracts 4.4 million visitors, and generates in excess of £100m for the local economy.
As the centre of Scotland's government and legal system, the public sector plays a central role in the economy of Edinburgh. Many departments of the Scottish Government are located in the city. Other major employers include NHS Scotland and local government administration.
Culture.
Festivals and celebrations.
The city hosts the annual Edinburgh International Festival, which is one of many events that run between the end of July and early September each year. The best known of these events are the Edinburgh Festival Fringe, the Edinburgh International Festival, the Edinburgh Military Tattoo and the Edinburgh International Book Festival.
The longest established of these festivals is the Edinburgh International Festival, which was first held in 1947 and consists mainly of a programme of high-profile theatre productions and classical music performances, featuring international directors, conductors, theatre companies and orchestras.
This has since been overtaken both in size and popularity by the Edinburgh Fringe which began as a programme of marginal acts alongside the "official" Festival and has become the largest performing arts festival in the world. In 2006, 1867 different shows were staged in 261 venues across the city. Comedy has become one of the mainstays of the Fringe, with numerous well-known comedians getting their first 'break' here, often by being chosen to receive the Edinburgh Comedy Award. In 2008, the largest comedy venues "on the Fringe" launched the Edinburgh Comedy Festival as a festival within a festival.
Other festivals include the Edinburgh Mountain Film Festival which takes place in February, Edinburgh Art Festival, Edinburgh International Film Festival, which takes place in June, the Edinburgh Jazz and Blues Festival, and the Edinburgh International Book Festival. The Edge Festival (formerly known as T on the Fringe), a popular music offshoot of the Fringe, began in 2000, replacing the smaller Flux and Planet Pop series of shows.
The Edinburgh Military Tattoo, one of the centrepieces of the "official" Festival, occupies the Castle Esplanade every night, with massed pipers and military bands drawn from around the world. Performances end with a short fireworks display. As well as the various summer festivals the Edinburgh International Science Festival is held annually in April and is one of the largest of its kind in Europe.
The annual Hogmanay celebration was originally an informal street party focused on the Tron Kirk in the High Street of the Old Town. Since 1993 it has been officially organised with the focus moved to Princes Street. In 1996, over 300,000 people attended, leading to ticketing of the main street party in later years up to a limit of 100,000 tickets. Hogmanay now covers four days of processions, concerts and fireworks, with the street party beginning on Hogmanay. Alternative tickets are available for entrance into the Princes Street Gardens concert and Céilidh, where well-known artists perform and ticket holders can participate in traditional Scottish céilidh dancing. The event attracts thousands of people from all over the world.
On the night of 30 April the Beltane Fire Festival takes place on Calton Hill, involving a procession followed by scenes inspired by pagan old spring fertility celebrations. At the beginning of October each year the Dussehra Hindu Festival is also held on Calton Hill.
Music, theatre and film.
Outside the Festival season, Edinburgh supports several theatres and production companies. The Royal Lyceum Theatre has its own company, while the King's Theatre, Edinburgh Festival Theatre and Edinburgh Playhouse stage large touring shows. The Traverse Theatre presents a more contemporary repertoire. Amateur theatre companies productions are staged at the Bedlam Theatre, Church Hill Theatre and King's Theatre among others.
The Usher Hall is Edinburgh's premier venue for classical music, as well as occasional popular music concerts. It was the venue for the Eurovision Song Contest 1972. Other halls staging music and theatre include The Hub, the Assembly Rooms and the Queen's Hall. The Scottish Chamber Orchestra is based in Edinburgh.
Edinburgh has two repertory cinemas, the Edinburgh Filmhouse and The Cameo, as well as the independent Dominion Cinema and a range of multiplexes.
Edinburgh has a healthy popular music scene. Occasionally large concerts are staged at Murrayfield and Meadowbank, while mid-sized events take place at smaller venues such as the Corn Exchange, the Liquid Rooms and the Bongo Club. In 2010, PRS for Music listed Edinburgh among the UK's top ten 'most musical' cities. Several city pubs are well known for their live performances of folk music. They include 'Sandy Bell's' in Forrest Road, 'The Captain's Bar' in South College Street, and 'Whistlebinkies' in Niddry Street.
Edinburgh is home to a flourishing group of contemporary composers such as Nigel Osborne, Peter Nelson, Lyell Cresswell, Hafliði Hallgrímsson, Edward Harper, Robert Crawford, Robert Dow and John McLeod. McLeod's music is heard regularly on BBC Radio 3 and throughout the UK.
Rockstar North, formerly DMA Design, known for creating the Grand Theft Auto series, is based in Edinburgh.
Media.
The "Edinburgh Evening News" is based in Edinburgh and published every day except Sunday. Johnston Press owns the title and "The Scotsman"; their corporate headquarters are in Edinburgh and their national newspaper is the only one published in the city. "The Herald" newspaper, published in Glasgow, also covers Edinburgh.
The city has two commercial radio stations: Forth 1, a station which broadcasts mainstream chart music, and Forth 2 on medium wave which plays classic hits. Capital Radio Scotland and Eklipse Sports Radio also have transmitters covering Edinburgh. Along with the UK national radio stations, Radio Scotland and the Gaelic language service BBC Radio nan Gàidheal are also broadcast. DAB digital radio is broadcast over two local multiplexes. BFBS Radio broadcasts from studios on the base at Dreghorn Barracks across the city on 98.5FM as part of its UK Bases network
STV Edinburgh, a local TV channel for the city, launched on 12 January 2015. Television, along with most radio services, is broadcast to the city from the Craigkelly transmitting station situated in Fife on the opposite side of the Firth of Forth.
Museums, libraries and galleries.
Edinburgh has many museums and libraries. These include the National Museum of Scotland, the National Library of Scotland, National War Museum, the Museum of Edinburgh, Surgeons' Hall Museum, the Grand Lodge of Scotland Museum and Library, and the Museum of Childhood.
Edinburgh Zoo, covering 82 acres on Corstorphine Hill, is the second most popular paid tourist attraction in Scotland, and currently home to two giant pandas, Tian Tian and Yang Guang, on loan from the People's Republic of China.
Edinburgh is also home to The Royal Yacht Britannia, decommissioned in 1997 and now a five-star visitor attraction and evening events venue permanently berthed at Ocean Terminal.
Edinburgh contains Scotland's five National Galleries of Art as well as numerous smaller art galleries. The national collection is housed in the National Gallery of Scotland, located on the Mound, now linked to the Royal Scottish Academy which holds regular major exhibitions of paintings. Contemporary collections are shown in the Scottish National Gallery of Modern Art which occupies a split site at Belford. The Scottish National Portrait Gallery in Queen Street focuses on portraits and photography.
The council-owned City Art Centre in Market Street mounts regular art exhibitions. Across the road, The Fruitmarket Gallery offers world class exhibitions of contemporary art, featuring work by British and international artists with both emerging and established international reputations.
There are many small private galleries, including the Ingleby Gallery. This provides a varied programme including shows by Callum Innes, Peter Liversidge, Ellsworth Kelly, Richard Forster, and Sean Scully.
The city hosts several of Scotland's galleries and organisations dedicated to contemporary visual art. Significant strands of this infrastructure include: The Scottish Arts Council, Edinburgh College of Art, Talbot Rice Gallery (University of Edinburgh) and the Edinburgh Annuale.
Shopping.
The locale around Princes Street is the main shopping area in the city centre, with souvenir shops, chain stores such as Boots the Chemist, H&M and Jenners. George Street, north of Princes Street, is the preferred location for some upmarket shops and independent stores. The St. James Centre at the east end of Princes Street hosts national chains including a large John Lewis store. Multrees Walk, adjacent to the St. James Centre, is a recent addition to the central shopping district, dominated by the presence of Harvey Nichols. Shops here include Louis Vuitton, Emporio Armani, Mulberry and Calvin Klein.
Edinburgh also has substantial retail parks outside the city centre. These include The Gyle Shopping Centre and Hermiston Gait in the west of the city, Cameron Toll Shopping Centre, Straiton Retail Park and Fort Kinnaird in the south and east, and Ocean Terminal in the north on the Leith waterfront.
Governance.
Following local government reorganisation in 1996, Edinburgh constitutes one of the 32 council areas of Scotland. Like all other local authorities of Scotland, the council has powers over most matters of local administration such as housing, planning, local transport, parks, economic development and regeneration. The council comprises 58 elected councillors, returned from 17 multi-member electoral wards in the city. Following the 2007 Scottish Local Elections the incumbent Labour Party lost majority control of the council after 23 years to a Liberal Democrat/SNP coalition.
The city's coat of arms was registered by the Lord Lyon King of Arms in 1732.
Edinburgh is represented in the Scottish Parliament. For electoral purposes, the city is divided into six of the nine constituencies in the Lothians electoral region. Each constituency elects one Member of the Scottish Parliament (MSP) by the first past the post system of election, and the region elects seven additional MSPs to produce a result based on a form of proportional representation.
Edinburgh is also represented in the British House of Commons by five Members of Parliament. The city is divided into Edinburgh North and Leith, Edinburgh East, Edinburgh South, Edinburgh South West, and Edinburgh West, each constituency electing one member by the first past the post system.
In the 2014 Scottish independence referendum, voters in Edinburgh rejected independence by a margin of 61.1% No to 38.9% Yes. Turnout was at 84.4%. Numerically, Edinburgh had the largest number of No votes out of all 32 council areas in Scotland with 194,638 No votes to 123,927 Yes votes. Further to this, the difference between the number of Yes and No votes was largest in Edinburgh by comparison to any other council area at 70,711.
Transport.
Edinburgh Airport is Scotland's busiest and biggest airport and the principal international gateway to the capital, handling around 9 million passengers in 2012. In anticipation of rising passenger numbers, the airport operator BAA outlined a draft masterplan in 2011 to provide for the expansion of the airfield and the terminal building. The airport has since been sold, in June 2012, to Global Infrastructure Partners (GIP). The possibility of building a second runway to cope with an increased number of aircraft movements has also been mooted.
Travel in Edinburgh is undertaken predominantly by bus. Lothian Buses operate the majority of city bus services within the city and to surrounding suburbs, with the most routes running via Princes Street. Services further afield operate from the Edinburgh Bus Station off St Andrew Square and Waterloo Place and are operated mainly by Stagecoach, Scottish Citylink, National Express Coaches, First Scotland East & Perryman's Buses.
Lothian Buses, as the successor company to the Edinburgh Corporation Transport Department, also operates all of the city's branded public tour buses, night bus service and airport bus link.
 In 2010, Lothian Buses recorded 109 million passenger journeys – a 1.9% rise on the previous year.
Edinburgh Waverley Station is the second-busiest railway station in Scotland, with only Glasgow Central handling more passengers. On the evidence of passenger entries and exits between April 2010 and March 2011, Edinburgh Waverley is the fifth-busiest station outside London; it is also the UK's second biggest station in terms of the number of platforms. Waverley is the terminus for most trains arriving from London King's Cross and the departure point for many rail services within Scotland operated by Abellio ScotRail.
To the west of the city centre lies Haymarket Station which is an important commuter stop. Opened in 2003, Edinburgh Park station serves the Gyle business park in the west of the city and the nearby Gogarburn headquarters of the Royal Bank of Scotland. The Edinburgh Crossrail route connects Edinburgh Park with Haymarket, Edinburgh Waverley and the suburban stations of Brunstane and Newcraighall in the east of the city. There are also commuter lines to South Gyle and Dalmeny, the latter serving South Queensferry by the Forth Bridges, and to Wester Hailes and Curriehill in the south west of the city.
To tackle traffic congestion, Edinburgh is now served by six park and ride sites on the periphery of the city at Sheriffhall (in Midlothian), Ingliston, Riccarton, Inverkeithing (in Fife), Newcraighall and Straiton (in Midlothian). A referendum of Edinburgh residents in February 2005 rejected a proposal to introduce congestion charging in the city.
Edinburgh Trams became operational on 31 May 2014. The city had been without a tram system since Edinburgh Corporation Tramways ceased on 16 November 1956. Following parliamentary approval in 2007, construction began in early 2008. The first stage of the project was expected to be completed by July 2011 but, following delays caused by extra utility work and a long-running contractual dispute between the Council and the main contractor, Bilfinger, the project was rescheduled. The cost of the project rose from the original projection of £545 million to £750 million in mid-2011 and some suggest that it could eventually exceed £1 billion. The completed line is 8.7 mi in length, running from Edinburgh Airport, west of the city, to its current terminus at York Place in the city centre's East End. It was originally planned to continue down Leith Walk to Ocean Terminal and where it would terminate at Newhaven
Should the original plan be resumed and taken to completion, trams will also run from Haymarket through Ravelston and Craigleith to Granton Square on the Waterfront Edinburgh. Long-term proposals envisage a line running west from the airport to Ratho and Newbridge and another connecting Granton Square to Newhaven via Lower Granton Road, thus completing the Line 1 (North Edinburgh) loop. A further line serving the south of the city has also been suggested.
Transport for Edinburgh released a that average weekly tram passengers are currently in excess of 90,000 after the first week of service saw 130,000 journeys taken.
Education.
There are four universities in Edinburgh (including Queen Margaret University which lies just outwith the city boundary) with students making up around one-fifth of the population. Established by Royal Charter in 1583, the University of Edinburgh is one of Scotland's ancient universities and is the fourth oldest in the country after St Andrews, Glasgow and Aberdeen. Originally centred on Old College the university expanded to premises on The Mound, the Royal Mile and George Square. Today, the King's Buildings in the south of the city contain most of the schools within the College of Science and Engineering. In 2002, the medical school moved to purpose built accommodation adjacent to the new Edinburgh Royal Infirmary at Little France. The University was placed 17th in the QS World University Rankings for 2013.
Heriot-Watt University and Napier Technical College were established in the 1960s. Heriot-Watt began as the world's first Mechanics' Institute, tracing its origins to 1821 when it opened as a school for the technical education of the working classes. The former Napier College was renamed Napier Polytechnic in 1986 and gained university status in 1992. Edinburgh Napier University has campuses in the south and west of the city, including the former Merchiston Tower and Craiglockhart Hydropathic. It is home to the Screen Academy Scotland.
Queen Margaret University was located in Edinburgh before it moved to a new campus near Musselburgh in 2008.
Until 2012 further education colleges in the city included Jewel and Esk College (incorporating Leith Nautical College founded in 1903), Telford College, opened in 1968, and Stevenson College, opened in 1970. These have now been amalgamated to form Edinburgh College. The Scottish Agricultural College also has a campus in south Edinburgh. Other institutions include the Royal College of Surgeons of Edinburgh and the Royal College of Physicians of Edinburgh which were established by Royal Charter in 1506 and 1681 respectively. The Trustees Drawing Academy of Edinburgh, founded in 1760, became the Edinburgh College of Art in 1907.
There are 18 nursery, 94 primary and 23 secondary schools administered by the City of Edinburgh Council.
Edinburgh is home to The Royal High School, one of the oldest schools in the country and the world. The city also has several independent, fee-paying schools including Edinburgh Academy, Fettes College, George Heriot's School, George Watson's College, Merchiston Castle School, Stewart's Melville College and The Mary Erskine School. In 2009, the proportion of pupils attending independent schools was 24.2%, far above the Scottish national average of just over 7% and higher than in any other region of Scotland. In August 2013, Edinburgh City Council opened the city's first stand-alone Gaelic primary school, Bun-sgoil Taobh na Pàirce.
Healthcare.
The main NHS Lothian hospitals serving the Edinburgh area are the Royal Infirmary of Edinburgh, which includes the University of Edinburgh Medical School, and the Western General Hospital, which has a large cancer treatment centre and nurse-led Minor Injuries Clinic. The Royal Edinburgh Hospital in Morningside specialises in mental health. The Royal Hospital for Sick Children, popularly referred to as 'the Sick Kids', is a specialist paediatrics hospital.
There are two private hospitals: Murrayfield Hospital in the west of the city and Shawfield Hospital in the south. Both are owned by Spire Healthcare.
Sport.
Football.
Edinburgh has two professional football clubs: Heart of Midlothian, founded in 1874, and Hibernian, founded in 1875. Known locally as "Hearts" and "Hibs", both teams play in the Scottish Championship. They are the oldest city rivals in Scotland and the Edinburgh derby is one of the oldest derby matches in world football. Both clubs have won the Scottish league championship four times. Hearts have won the Scottish Cup eight times and the Scottish League Cup four times. Hibs have won the Scottish Cup twice and the Scottish League Cup three times.
Edinburgh was also home to four other former Scottish Football League clubs: Edinburgh City, Leith Athletic, Meadowbank Thistle and St Bernard's. Meadowbank Thistle played at Meadowbank Stadium until 1995, when the club moved to Livingston and became Livingston FC. The Scottish national team has very occasionally played at Easter Road and Tynecastle, although its normal home stadium is Hampden Park in Glasgow.
The city also plays host to Lowland Football League clubs Spartans, Edinburgh City & Edinburgh University.
Rugby.
The Scotland national rugby union team and the professional Edinburgh Rugby team play at Murrayfield Stadium, which is owned by the Scottish Rugby Union and also used for other events, including music concerts. It is the largest capacity stadium in Scotland, seating 67130 spectators. Edinburgh is also home to RBS Premier One rugby teams Heriot's Rugby Club, Boroughmuir RFC, the Edinburgh Academicals and Currie RFC.
Rugby league is represented by the Edinburgh Eagles who play in the Rugby League Conference Scotland Division. Murrayfield Stadium has hosted the Magic Weekend where all Super League matches are played in the stadium over one weekend.
Other sports.
The Scottish cricket team, which represents Scotland internationally and in the Friends Provident Trophy, play their home matches at the Grange cricket club.
The Edinburgh Capitals are the latest of a succession of ice hockey clubs in the Scottish capital. Previously Edinburgh was represented by the Murrayfield Racers and the Edinburgh Racers. The club play their home games at the Murrayfield Ice Rink and have competed in the ten-team professional Elite Ice Hockey League since the 2005–06 season.
Right next door to Murrayfield Ice Rink is a 7-sheeter dedicated curling facility where curling is played from October to March each season.
The Edinburgh Diamond Devils is a baseball club which won its first Scottish Championship in 1991 as the "Reivers." 1992 saw the team repeat the achievement, becoming the first team to do so in league history. The same year saw the start of their first youth team, the Blue Jays. The club adopted its present name in 1999.
Edinburgh has also hosted national and international sports events including the World Student Games, the 1970 British Commonwealth Games, the 1986 Commonwealth Games and the inaugural 2000 Commonwealth Youth Games. For the 1970 Games the city built Olympic standard venues and facilities including Meadowbank Stadium and the Royal Commonwealth Pool. The Pool underwent refurbishment in 2012 and is due to host the Diving competition in the 2014 Commonwealth Games which will be held in Glasgow.
In American football, the Scottish Claymores played WLAF/NFL Europe games at Murrayfield, including their World Bowl 96 victory. From 1995 to 1997 they played all their games there, from 1998 to 2000 they split their home matches between Murrayfield and Glasgow's Hampden Park, then moved to Glasgow full-time, with one final Murrayfield appearance in 2002. The city's most successful non-professional team are the Edinburgh Wolves who play at Meadowbank Stadium.
The Edinburgh Marathon has been held annually in the city since 2003 with more than 16,000 runners taking part on each occasion. Its organisers have called it "the fastest marathon in the UK" due to the elevation drop of 40 m. The city also organises a half-marathon, as well as 10 km (5 km) and 5 km (5 km) races, including a 5 km race on 1 January each year.
Edinburgh has a speedway team, the Edinburgh Monarchs, which, since the loss of its stadium in the city, has raced at the Lothian Arena in Armadale, West Lothian. The Monarchs have won the Premier League championship three times in their history, in 2003 and again in 2008 and 2010.
Notable residents.
Edinburgh has a long literary tradition, which became especially evident during the Scottish Enlightenment. This heritage and the city's lively literary life in the present led to it being declared the first UNESCO City of Literature in 2004. Famous authors who have lived in Edinburgh include the economist Adam Smith, born in Kirkcaldy and author of "The Wealth of Nations",
 James Boswell biographer of Samuel Johnson; Sir Walter Scott, creator of the historical novel and author of famous titles such as "Rob Roy", "Ivanhoe", and "Heart of Midlothian"; James Hogg, author of "The Private Memoirs and Confessions of a Justified Sinner"; Robert Louis Stevenson, creator of "Treasure Island", "Kidnapped", and "The Strange Case of Dr Jekyll and Mr Hyde"; Sir Arthur Conan Doyle, the creator of Sherlock Holmes; Muriel Spark, author of "The Prime of Miss Jean Brodie" and Irvine Welsh, author of "Trainspotting", whose novels are mostly set in the city and often written in colloquial Scots;
 Ian Rankin, author of the Inspector Rebus series of crime thrillers, Alexander McCall Smith, author of the No. 1 Ladies' Detective Agency series, and J. K. Rowling, creator of Harry Potter, who began her first book in an Edinburgh coffee shop.
Scotland has a rich history of science and engineering, with Edinburgh producing a number of famous names. John Napier, inventor of logarithms, was born in Merchiston Tower and lived and died in the city. His house now forms part the original campus of Napier University which was named in his honour. He lies buried under St. Cuthbert's Church. James Clerk Maxwell, founder of the modern theory of electromagnetism, was born in the city and educated at the Edinburgh Academy and the University of Edinburgh, as was the engineer and telephone pioneer Alexander Graham Bell. James Braidwood, who organised Britain's first municipal fire brigade was also born in the city and began his career there.
Other names connected with the city include Max Born, physicist and Nobel laureate; Charles Darwin, the biologist who propounded the theory of natural selection; David Hume, philosopher, economist and historian; James Hutton, regarded as the "Father of Geology"; Joseph Black, the chemist and one of the founders of thermodynamics; pioneering medical researchers Joseph Lister and James Young Simpson; chemist and discoverer of the element nitrogen Daniel Rutherford, Colin Maclaurin, mathematician and developer of the Maclaurin series, and Ian Wilmut, the geneticist involved in the cloning of Dolly the sheep just outside Edinburgh. The stuffed carcass of Dolly the sheep is now on display in the National Museum of Scotland. The latest in a long line of science celebrities associated with the city is theoretical physicist and Nobel Prizewinner Professor Emeritus Peter Higgs, born in Newcastle but resident in Edinburgh for most of his academic career, after whom the Higgs boson particle has been named.
Edinburgh has been the birthplace of actors like Alastair Sim and Sir Sean Connery, famed as the first cinematic James Bond, the comedian and actor Ronnie Corbett, best known as one of The Two Ronnies, and the impressionist Rory Bremner. Famous artists from the city include the portrait painters Sir Henry Raeburn, Sir David Wilkie and Allan Ramsay.
The city has produced or been home to some very successful musicians in recent decades, particularly Ian Anderson, front man of the band Jethro Tull, The Incredible String Band, the folk duo The Corries, Wattie Buchan, lead singer and founding member of punk band The Exploited, Shirley Manson, lead singer of the band Garbage, the Bay City Rollers, The Proclaimers, Boards of Canada and Idlewild.
Edinburgh is the birthplace of former British Prime Minister Tony Blair who attended the city's Fettes College.
Notorious criminals from Edinburgh's past include Deacon Brodie, head of a trades guild and Edinburgh city councillor by day but a burglar by night, who is said to have been the inspiration for Robert Louis Stevenson's story, the "Strange Case of Dr Jekyll and Mr Hyde", and murderers Burke and Hare who delivered fresh corpses for dissection to the famous anatomist Robert Knox.
Another well-known Edinburgh resident was Greyfriars Bobby. The small Skye Terrier reputedly kept vigil over his dead master's grave in Greyfriars Kirkyard for 14 years in the 1860s and 1870s, giving rise to a story of canine devotion which plays a part in attracting visitors to the city.
International relations.
Twin towns and sister cities.
The City of Edinburgh has entered into 14 international twinning arrangements since 1954. Most of the arrangements are styled as 'Twin Cities' but the agreement with Kraków is designated as a 'Partner City', and the agreement with Kyoto Prefecture is officially styled as a 'Friendship Link', reflecting its status as the only region to be twinned with Edinburgh.
For a list of consulates in Edinburgh see List of diplomatic missions in Scotland.

</doc>
<doc id="9603" url="http://en.wikipedia.org/wiki?curid=9603" title="Ernest Rutherford">
Ernest Rutherford

Ernest Rutherford, 1st Baron Rutherford of Nelson, OM
 FRS
 (30 August 1871 – 19 October 1937) was a New Zealand-born British physicist who became known as the father of nuclear physics. "Encyclopædia Britannica" considers him to be the greatest experimentalist since Michael Faraday (1791–1867).
In early work he discovered the concept of radioactive half-life, proved that radioactivity involved the transmutation of one chemical element to another, and also differentiated and named alpha and beta radiation. This work was done at McGill University in Canada. It is the basis for the Nobel Prize in Chemistry he was awarded in 1908 "for his investigations into the disintegration of the elements, and the chemistry of radioactive substances", for which he remains the first Canadian and Oceanian Nobel laureate, and the only laureate born in the South Island.
Rutherford moved in 1907 to the Victoria University of Manchester (today University of Manchester) in the UK, where he and Thomas Royds proved that alpha radiation is helium nuclei. Rutherford performed his most famous work after he became a Nobel laureate. In 1911, although he could not prove that it was positive or negative,
he theorized that atoms have their charge concentrated in a very small nucleus,
and thereby pioneered the Rutherford model of the atom, through his discovery and interpretation of Rutherford scattering in his gold foil experiment. He is widely credited with first "splitting the atom" in 1917 in a nuclear reaction between nitrogen and alpha particles, in which he also discovered (and named) the proton.
Rutherford became Director of the Cavendish Laboratory at Cambridge University in 1919. Under his leadership the neutron was discovered by James Chadwick in 1932 and in the same year the first experiment to split the nucleus in a fully controlled manner, performed by students working under his direction, John Cockcroft and Ernest Walton. After his death in 1937, he was honoured by being interred with the greatest scientists of the United Kingdom, near Sir Isaac Newton's tomb in Westminster Abbey. The chemical element rutherfordium (element 104) was named after him in 1997.
Biography.
Early life and education.
Ernest Rutherford was the son of James Rutherford, a farmer, and his wife Martha Thompson, originally from Hornchurch, Essex, England. James had emigrated to New Zealand from Perth, Scotland, "to raise a little flax and a lot of children". Ernest was born at Brightwater, near Nelson, New Zealand. His first name was mistakenly spelled 'Earnest' when his birth was registered.
He studied at Havelock School and then Nelson College and won a scholarship to study at Canterbury College, University of New Zealand where he participated in the debating society and played rugby. After gaining his BA, MA and BSc, and doing two years of research during which he invented a new form of radio receiver, in 1895 Rutherford was awarded an 1851 Research Fellowship from the Royal Commission for the Exhibition of 1851, to travel to England for postgraduate study at the Cavendish Laboratory, University of Cambridge. He was among the first of the 'aliens' (those without a Cambridge degree) allowed to do research at the university, under the inspiring leadership of J. J. Thomson, and the newcomers aroused jealousies from the more conservative members of the Cavendish fraternity. With Thomson's encouragement, he managed to detect radio waves at half a mile and briefly held the world record for the distance over which electromagnetic waves could be detected, though when he presented his results at the British Association meeting in 1896, he discovered he had been outdone by another lecturer, by the name of Marconi.
In 1898 Thomson recommended Rutherford for a position at McGill University in Montreal, Canada. He was to replace Hugh Longbourne Callendar who held the chair of Macdonald Professor of physics and was coming to Cambridge. Rutherford was accepted, which meant that in 1900 he could marry Mary Georgina Newton (1876–1945) to whom he had become engaged before leaving New Zealand; they had one daughter, Eileen Mary (1901–1930), who married Ralph Fowler. In 1900 he gained a DSc from the University of New Zealand. In 1907 Rutherford returned to Britain to take the chair of physics at the University of Manchester.
Later years and honours.
He was knighted in 1914. During World War I, he worked on a top secret project to solve the practical problems of submarine detection by sonar. In 1916 he was awarded the Hector Memorial Medal. In 1919 he returned to the Cavendish succeeding J. J. Thomson as the Cavendish professor and Director. Under him, Nobel Prizes were awarded to James Chadwick for discovering the neutron (in 1932), John Cockcroft and Ernest Walton for an experiment which was to be known as "splitting the atom" using a particle accelerator, and Edward Appleton for demonstrating the existence of the ionosphere. In 1925, Rutherford pushed calls to the Government of New Zealand to support education and research, which led to the formation of the Department of Scientific and Industrial Research (DSIR) in the following year. Between 1925 and 1930 he served as President of the Royal Society, and later as president of the Academic Assistance Council which helped almost 1,000 university refugees from Germany. He was admitted to the Order of Merit in 1925 and raised to the peerage as Baron Rutherford of Nelson, in 1931, a title that became extinct upon his unexpected death in 1937.
For some time beforehand, Rutherford had a small hernia, which he had neglected to have fixed, and it became strangulated, causing him to be violently ill. Despite an emergency operation in London, he died four days afterwards of what physicians termed "intestinal paralysis", at Cambridge. After cremation at Golders Green Crematorium, he was given the high honour of burial in Westminster Abbey, near Isaac Newton and other illustrious British scientists.
Scientific research.
At Cambridge, Rutherford started to work with J. J. Thomson on the conductive effects of X-rays on gases, work which led to the discovery of the electron which Thomson presented to the world in 1897. Hearing of Becquerel's experience with uranium, Rutherford started to explore its radioactivity, discovering two types that differed from X-rays in their penetrating power. Continuing his research in Canada, he coined the terms alpha ray and beta ray in 1899 to describe the two distinct types of radiation. He then discovered that thorium gave off a gas which produced an emanation which was itself radioactive and would coat other substances. He found that a sample of this radioactive material of any size invariably took the same amount of time for half the sample to decay – its "half-life" (11½ minutes in this case).
From 1900 to 1903, he was joined at McGill by the young chemist Frederick Soddy (Nobel Prize in Chemistry, 1921) for whom he set the problem of identifying the thorium emanations. Once he had eliminated all the normal chemical reactions, Soddy suggested that it must be one of the inert gases, which they named thoron (later found to be an isotope of radon). They also found another type of thorium they called Thorium X, and kept on finding traces of helium. They also worked with samples of "Uranium X" from William Crookes and radium from Marie Curie.
In 1902, they produced a "Theory of Atomic Disintegration" to account for all their experiments. Up till then atoms were assumed to be the indestructable basis of all matter and although Curie had suggested that radioactivity was an atomic phenomenon, the idea of the atoms of radioactive substances breaking up was a radically new idea. Rutherford and Soddy demonstrated that radioactivity involved the spontaneous disintegration of atoms into other types of atoms (one element spontaneously being changed to another).
In 1903, Rutherford considered a type of radiation discovered (but not named) by French chemist Paul Villard in 1900, as an emission from radium, and realised that this observation must represent something different from his own alpha and beta rays, due to its very much greater penetrating power. Rutherford therefore gave this third type of radiation the name of gamma ray. All three of Rutherford's terms are in standard use today – other types of radioactive decay have since been discovered, but Rutherford's three types are among the most common.
In Manchester, he continued to work with alpha radiation. In conjunction with Hans Geiger, he developed zinc sulfide scintillation screens and ionisation chambers to count alphas. By dividing the total charge they produced by the number counted, Rutherford decided that the charge on the alpha was two. In late 1907, Ernest Rutherford and Thomas Royds allowed alphas to penetrate a very thin window into an evacuated tube. As they sparked the tube into discharge, the spectrum obtained from it changed, as the alphas accumulated in the tube. Eventually, the clear spectrum of helium gas appeared, proving that alphas were at least ionised helium atoms, and probably helium nuclei.
Gold foil experiment.
Rutherford performed his most famous work "after" receiving the Nobel prize in 1908. Along with Hans Geiger and Ernest Marsden in 1909, he carried out the Geiger–Marsden experiment, which demonstrated the nuclear nature of atoms by deflecting alpha particles passing through a thin gold foil. Rutherford was inspired to ask Geiger and Marsden in this experiment to look for alpha particles with very high deflection angles, of a type not expected from any theory of matter at that time. Such deflections, though rare, were found, and proved to be a smooth but high-order function of the deflection angle. It was Rutherford's interpretation of this data that led him to formulate the Rutherford model of the atom in 1911 – that a very small charged nucleus, containing much of the atom's mass, was orbited by low-mass electrons.
Before leaving Manchester in 1919 to take over the Cavendish laboratory in Cambridge, Rutherford became, in 1919, the first person to deliberately transmute one element into another. In this experiment, he had discovered peculiar radiations when alphas were projected into air, and narrowed the effect down to the nitrogen, not the oxygen in the air. Using pure nitrogen, Rutherford used alpha radiation to convert nitrogen into oxygen through the nuclear reaction 14N + α → 17O + proton. The proton was not then known. In the products of this reaction Rutherford simply identified hydrogen nuclei, by their similarity to the particle radiation from earlier experiments in which he had bombarded hydrogen gas with alpha particles to knock hydrogen nuclei out of hydrogen atoms. This result showed Rutherford that hydrogen nuclei were a part of nitrogen nuclei (and by inference, probably other nuclei as well). Such a construction had been suspected for many years on the basis of atomic weights which were whole numbers of that of hydrogen; see Prout's hypothesis. Hydrogen was known to be the lightest element, and its nuclei presumably the lightest nuclei. Now, because of all these considerations, Rutherford decided that a hydrogen nucleus was possibly a fundamental building block of all nuclei, and also possibly a new fundamental particle as well, since nothing was known from the nucleus that was lighter. Thus, Rutherford postulated hydrogen nuclei to be a new particle in 1920, which he dubbed the "proton".
In 1921, while working with Niels Bohr (who postulated that electrons moved in specific orbits), Rutherford theorized about the existence of neutrons, (which he had christened in his 1920 Bakerian Lecture), which could somehow compensate for the repelling effect of the positive charges of protons by causing an attractive nuclear force and thus keep the nuclei from flying apart from the repulsion between protons. The only alternative to neutrons was the existence of "nuclear electrons" which would counteract some of the proton charges in the nucleus, since by then it was known that nuclei had about twice the mass that could be accounted for if they were simply assembled from hydrogen nuclei (protons). But how these nuclear electrons could be trapped in the nucleus, was a mystery.
Rutherford's theory of neutrons was proved in 1932 by his associate James Chadwick, who recognized neutrons immediately when they were produced by other scientists and later himself, in bombarding beryllium with alpha particles. In 1935, Chadwick was awarded the Nobel Prize in Physics for this discovery.
Legacy.
Nuclear physics.
Rutherford's research, and work done under him as laboratory director, established the nuclear structure of the atom and the essential nature of radioactive decay as a nuclear process. Rutherford's team, using natural alpha particles, demonstrated "induced" nuclear transmutation, and later, using protons from an accelerator, demonstrated "artificially-induced" nuclear reactions and transmutation. He is known as the father of nuclear physics. Rutherford died too early to see Leó Szilárd's idea of controlled nuclear chain reactions come into being. However, a speech of Rutherford's about his artificially-induced transmutation in lithium, printed in 12 September 1933 London paper "The Times", was reported by Szilárd to have been his inspiration for thinking of the possibility of a controlled energy-producing nuclear chain reaction. Szilard had this idea while walking in London, on the same day.
Rutherford's speech touched on the 1932 work of his students John Cockcroft and Ernest Walton in "splitting" lithium into alpha particles by bombardment with protons from a particle accelerator they had constructed. Rutherford realized that the energy released from the split lithium atoms was enormous, but he also realized that the energy needed for the accelerator, and its essential inefficiency in splitting atoms in this fashion, made the project an impossibility as a practical source of energy (accelerator-induced fission of light elements remains too inefficient to be used in this way, even today). Rutherford's speech in part, read:
We might in these processes obtain very much more energy than the proton supplied, but on the average we could not expect to obtain energy in this way. It was a very poor and inefficient way of producing energy, and anyone who looked for a source of power in the transformation of the atoms was talking moonshine. But the subject was scientifically interesting because it gave insight into the atoms.
Incidences of cancer at Rutherford's former laboratory.
The Coupland Building at Manchester University, at which Rutherford conducted many of his experiments, has been the subject of a cancer cluster investigation. There has been a statistically high incidence of pancreatic cancer, brain cancer, and motor neuron disease occurring in and around Rutherford's former laboratories and, since 1984, a total of six workers have been stricken with these ailments. In 2009, an independent commission concluded that the very slightly elevated levels of various radiation related to Rutherford's experiments decades earlier are not the likely cause of such cancers and ruled the illnesses a coincidence.

</doc>
<doc id="9604" url="http://en.wikipedia.org/wiki?curid=9604" title="Many-worlds interpretation">
Many-worlds interpretation

The many-worlds interpretation is an interpretation of quantum mechanics that asserts the objective reality of the universal wavefunction and denies the actuality of wavefunction collapse. Many-worlds implies that all possible alternate histories and futures are real, each representing an actual "world" (or "universe"). In lay terms, the hypothesis states there is a very large—perhaps infinite—number of universes, and everything that could possibly have happened in our past, but did not, has occurred in the past of some other universe or universes. The theory is also referred to as MWI, the relative state formulation, the Everett interpretation, the theory of the universal wavefunction, many-universes interpretation, or just many-worlds.
The original relative state formulation is due to Hugh Everett in 1957. Later, this formulation was popularized and renamed "many-worlds" by Bryce Seligman DeWitt in the 1960s and 1970s. The decoherence approaches to interpreting quantum theory have been further explored and developed, becoming quite popular. MWI is one of many multiverse hypotheses in physics and philosophy. It is currently considered a mainstream interpretation along with the other decoherence interpretations, collapse theories (including the historical Copenhagen interpretation), and hidden variable theories such as the Bohmian mechanics.
Before many-worlds, reality had always been viewed as a single unfolding history. Many-worlds, however, views reality as a many-branched tree, wherein every possible quantum outcome is realised. Many-worlds reconciles the observation of non-deterministic events, such as the random radioactive decay, with the fully deterministic equations of quantum physics.
In many-worlds, the subjective appearance of wavefunction collapse is explained by the mechanism of quantum decoherence, and this is supposed to resolve all of the correlation paradoxes of quantum theory, such as the EPR paradox and Schrödinger's cat, since every possible outcome of every event defines or exists in its own "history" or "world".
Outline.
Although several versions of many-worlds have been proposed since Hugh Everett's original work, they all contain one key idea: the equations of physics that model the time evolution of systems "without" embedded observers are sufficient for modelling systems which "do" contain observers; in particular there is no observation-triggered wave function collapse which the Copenhagen interpretation proposes. Provided the theory is linear with respect to the wavefunction, the exact form of the quantum dynamics modelled, be it the non-relativistic Schrödinger equation, relativistic quantum field theory or some form of quantum gravity or string theory, does not alter the validity of MWI since MWI is a metatheory applicable to all linear quantum theories, and there is no experimental evidence for any non-linearity of the wavefunction in physics. MWI's main conclusion is that the universe (or multiverse in this context) is composed of a quantum superposition of very many, possibly even non-denumerably infinitely many, increasingly divergent, non-communicating parallel universes or quantum worlds.
The idea of MWI originated in Everett's Princeton Ph.D. thesis "The Theory of the Universal Wavefunction", developed under his thesis advisor John Archibald Wheeler, a shorter summary of which was published in 1957 entitled "Relative State Formulation of Quantum Mechanics" (Wheeler contributed the title "relative state"; Everett originally called his approach the "Correlation Interpretation", where "correlation" refers to quantum entanglement). The phrase "many-worlds" is due to Bryce DeWitt, who was responsible for the wider popularisation of Everett's theory, which had been largely ignored for the first decade after publication. DeWitt's phrase "many-worlds" has become so much more popular than Everett's "Universal Wavefunction" or Everett–Wheeler's "Relative State Formulation" that many forget that this is only a difference of terminology; the content of both of Everett's papers and DeWitt's popular article is the same.
The many-worlds interpretation shares many similarities with later, other "post-Everett" interpretations of quantum mechanics which also use decoherence to explain the process of measurement or wavefunction collapse. MWI treats the other histories or worlds as real since it regards the universal wavefunction as the "basic physical entity" or "the fundamental entity, obeying at all times a deterministic wave equation". The other decoherent interpretations, such as consistent histories, the Existential Interpretation etc., either regard the extra quantum worlds as metaphorical in some sense, or are agnostic about their reality; it is sometimes hard to distinguish between the different varieties. MWI is distinguished by two qualities: it assumes realism, which it assigns to the wavefunction, and it has the minimal formal structure possible, rejecting any hidden variables, quantum potential, any form of a collapse postulate (i.e., Copenhagenism) or mental postulates (such as the many-minds interpretation makes).
Decoherent interpretations of many-worlds using einselection to explain how a small number of classical pointer states can emerge from the enormous Hilbert space of superpositions have been proposed by Wojciech H. Zurek. "Under scrutiny of the environment, only pointer states remain unchanged. Other states decohere into mixtures of stable pointer states that can persist, and, in this sense, exist: They are einselected." These ideas complement MWI and bring the interpretation in line with our perception of reality.
Many-worlds is often referred to as a theory, rather than just an interpretation, by those who propose that many-worlds can make testable predictions (such as David Deutsch) or is falsifiable (such as Everett) or by those who propose that all the other, non-MW interpretations, are inconsistent, illogical or unscientific in their handling of measurements; Hugh Everett argued that his formulation was a metatheory, since it made statements about other interpretations of quantum theory; that it was the "only completely coherent approach to explaining both the contents of quantum mechanics and the appearance of the world." Deutsch is dismissive that many-worlds is an "interpretation", saying that calling it an interpretation "is like talking about dinosaurs as an 'interpretation' of fossil records."
Interpreting wavefunction collapse.
As with the other interpretations of quantum mechanics, the many-worlds interpretation is motivated by behavior that can be illustrated by the double-slit experiment. When particles of light (or anything else) are passed through the double slit, a calculation assuming wave-like behavior of light can be used to identify where the particles are likely to be observed. Yet when the particles are observed in this experiment, they appear as particles (i.e., at definite places) and not as non-localized waves.
Some versions of the Copenhagen interpretation of quantum mechanics proposed a process of "collapse" in which an indeterminate quantum system would probabilistically collapse down onto, or select, just one determinate outcome to "explain" this phenomenon of observation. Wavefunction collapse was widely regarded as artificial and "ad hoc", so an alternative interpretation in which the behavior of measurement could be understood from more fundamental physical principles was considered desirable.
Everett's Ph.D. work provided such an alternative interpretation. Everett stated that for a composite system – for example a subject (the "observer" or measuring apparatus) observing an object (the "observed" system, such as a particle) – the statement that either the observer or the observed has a well-defined state is meaningless; in modern parlance, the observer and the observed have become entangled; we can only specify the state of one "relative" to the other, i.e., the state of the observer and the observed are correlated "after" the observation is made. This led Everett to derive from the unitary, deterministic dynamics alone (i.e., without assuming wavefunction collapse) the notion of a "relativity of states".
Everett noticed that the unitary, deterministic dynamics alone decreed that after an observation is made each element of the quantum superposition of the combined subject–object wavefunction contains two "relative states": a "collapsed" object state and an associated observer who has observed the same collapsed outcome; what the observer sees and the state of the object have become correlated by the act of measurement or observation. The subsequent evolution of each pair of relative subject–object states proceeds with complete indifference as to the presence or absence of the other elements, "as if" wavefunction collapse has occurred, which has the consequence that later observations are always consistent with the earlier observations. Thus the "appearance" of the object's wavefunction's collapse has emerged from the unitary, deterministic theory itself. (This answered Einstein's early criticism of quantum theory, that the theory should define what is observed, not for the observables to define the theory). Since the wavefunction appears to have collapsed then, Everett reasoned, there was no need to actually assume that it had collapsed. And so, invoking Occam's razor, he removed the postulate of wavefunction collapse from the theory.
Probability.
A consequence of removing wavefunction collapse from the quantum formalism is that the Born rule requires derivation, since many-worlds derives its interpretation from the formalism. Attempts have been made, by many-world advocates and others, over the years to "derive" the Born rule, rather than just conventionally "assume" it, so as to reproduce all the required statistical behaviour associated with quantum mechanics. There is no consensus on whether this has been successful.
Everett, Gleason and Hartle.
Everett (1957) briefly derived the Born rule by showing that the Born rule was the only possible rule, and that its derivation was as justified as the procedure for defining probability in classical mechanics. Everett stopped doing research in theoretical physics shortly after obtaining his Ph.D., but his work on probability has been extended by a number of people. Andrew Gleason (1957) and James Hartle (1965) independently reproduced Everett's work, known as Gleason's theorem which was later extended.
De Witt and Graham.
Bryce De Witt and his doctoral student R. Neill Graham later provided alternative (and longer) derivations to Everett's derivation of the Born rule. They demonstrated that the norm of the worlds where the usual statistical rules of quantum theory broke down vanished, in the limit where the number of measurements went to infinity.
Deutsch "et al.".
An information-theoretic derivation of the Born rule from Everettarian assumptions, was produced by David Deutsch (1999) and refined by Wallace (2002–2009) and Saunders (2004). Deutsch's derivation is a two-stage proof: first he shows that the number of orthonormal Everett-worlds after a branching is proportional to the conventional probability density. Then he uses game theory to show that these are all equally likely to be observed. The last step in particular has been criticised for circularity. Some other reviews have been positive, although the status of these arguments remains highly controversial; some theoretical physicists have taken them as supporting the case for parallel universes. In the "New Scientist" article, reviewing their presentation at a September 2007 conference, Andy Albrecht, a physicist at the University of California at Davis, is quoted as saying "This work will go down as one of the most important developments in the history of science."
Wojciech H. Zurek (2005) has produced a derivation of the Born rule, where decoherence has replaced Deutsch's informatic assumptions. Lutz Polley (2000) has produced Born rule derivations where the informatic assumptions are replaced by symmetry arguments.
The Born rule and the collapse of the wave function have been obtained in the framework of the relative-state formulation of quantum mechanics by Armando V.D.B. Assis. He has proved that the Born rule and the collapse of the wave function follow from a game-theoretical strategy, namely the Nash equilibrium within a von Neumann zero-sum game between nature and observer.
Brief overview.
In Everett's formulation, a measuring apparatus M and an object system S form a composite system, each of which prior to measurement exists in well-defined (but time-dependent) states. Measurement is regarded as causing M and S to interact. After S interacts with M, it is no longer possible to describe either system by an independent state. According to Everett, the only meaningful descriptions of each system are relative states: for example the relative state of S given the state of M or the relative state of M given the state of S. In DeWitt's formulation, the state of S after a sequence of measurements is given by a quantum superposition of states, each one corresponding to an alternative measurement history of S.
For example, consider the smallest possible truly quantum system S, as shown in the illustration. This describes for instance, the spin-state of an electron. Considering a specific axis (say the "z"-axis) the north pole represents spin "up" and the south pole, spin "down". The superposition states of the system are described by (the surface of) a sphere called the Bloch sphere. To perform a measurement on S, it is made to interact with another similar system M. After the interaction, the combined system is described by a state that ranges over a six-dimensional space (the reason for the number six is explained in the article on the Bloch sphere). This six-dimensional object can also be regarded as a quantum superposition of two "alternative histories" of the original system S, one in which "up" was observed and the other in which "down" was observed. Each subsequent binary measurement (that is interaction with a system M) causes a similar split in the history tree. Thus after three measurements, the system can be regarded as a quantum superposition of 8 = 2 × 2 × 2 copies of the original system S.
The accepted terminology is somewhat misleading because it is incorrect to regard the universe as splitting at certain times; at any given instant there is one state in one universe.
Relative state.
In his 1957 doctoral dissertation, Everett proposed that rather than modeling an isolated quantum system subject to external observation, one could mathematically model an object as well as its observers as purely physical systems within the mathematical framework developed by Paul Dirac, von Neumann and others, discarding altogether the "ad hoc" mechanism of wave function collapse. Since Everett's original work, there have appeared a number of similar formalisms in the literature. One such idea is discussed in the next section.
The relative state formulation makes two assumptions. The first is that the wavefunction is not simply a description of the object's state, but that it actually is entirely equivalent to the object, a claim it has in common with some other interpretations. The second is that observation or measurement has no special laws or mechanics, unlike in the Copenhagen interpretation which considers the wavefunction collapse as a special kind of event which occurs as a result of observation. Instead, measurement in the relative state formulation is the consequence of a configuration change in the memory of an observer described by the same basic wave physics as the object being modeled.
The many-worlds interpretation is DeWitt's popularisation of Everett's work, who had referred to the combined observer–object system as being split by an observation, each split corresponding to the different or multiple possible outcomes of an observation. These splits generate a possible tree as shown in the graphic below. Subsequently DeWitt introduced the term "world" to describe a complete measurement history of an observer, which corresponds roughly to a single branch of that tree. Note that "splitting" in this sense, is hardly new or even quantum mechanical. The idea of a space of complete alternative histories had already been used in the theory of probability since the mid-1930s for instance to model Brownian motion. 
Under the many-worlds interpretation, the Schrödinger equation, or relativistic analog, holds all the time everywhere. An observation or measurement of an object by an observer is modeled by applying the wave equation to the entire system comprising the observer "and" the object. One consequence is that every observation can be thought of as causing the combined observer–object's wavefunction to change into a quantum superposition of two or more non-interacting branches, or split into many "worlds". Since many observation-like events have happened, and are constantly happening, there are an enormous and growing number of simultaneously existing states.
If a system is composed of two or more subsystems, the system's state will be a superposition of products of the subsystems' states. Once the subsystems interact, their states are no longer independent. Each product of subsystem states in the overall superposition evolves over time independently of other products. The subsystems states have become correlated or entangled and it is no longer possible to consider them independent of one another. In Everett's terminology each subsystem state was now "correlated" with its "relative state", since each subsystem must now be considered relative to the other subsystems with which it has interacted.
Comparative properties and possible experimental tests.
One of the salient properties of the many-worlds interpretation is that it does not require an exceptional method of wave function collapse to explain it. "It seems that there is no experiment distinguishing the MWI from other no-collapse theories such as Bohmian mechanics or other variants of MWI... In most no-collapse interpretations, the evolution of the quantum state of the Universe is the same. Still, one might imagine that there is an experiment distinguishing the MWI from another no-collapse interpretation based on the difference in the correspondence between the formalism and the experience (the results of experiments)."
However, in 1985, David Deutsch published three related thought experiments which could test the theory vs the Copenhagen interpretation. The experiments require macroscopic quantum state preparation and quantum erasure by a hypothetical quantum computer which is currently outside experimental possibility. Since then Lockwood (1989), Vaidman and others have made similar proposals. These proposals also require an advanced technology which is able to place a macroscopic object in a coherent superposition, another task for which it is uncertain whether it will ever be possible. Many other controversial ideas have been put forward though, such as a recent claim that cosmological observations could test the theory, and another claim by Rainer Plaga (1997), published in "Foundations of Physics", that communication might be possible between worlds. As of 2010, there are no feasible experiments to test the differences between MWI and other theories.
Copenhagen interpretation.
In the Copenhagen interpretation, the mathematics of quantum mechanics allows one to predict probabilities for the occurrence of various events. When an event occurs, it becomes part of the definite reality, and alternative possibilities do not. There is no necessity to say anything definite about what is not observed.
The universe decaying to a new vacuum state.
Any event that changes the number of observers in the universe may have experimental consequences. Quantum tunnelling to a new vacuum state would reduce the number of observers to zero (i.e., kill all life). Some cosmologists argue that the universe is in a false vacuum state and that consequently the universe should have already experienced quantum tunnelling to a true vacuum state. This has not happened and is cited as evidence in favor of many-worlds. In some worlds, quantum tunnelling to a true vacuum state has happened but most other worlds escape this tunneling and remain viable. This can be thought of as a variation on quantum suicide.
Many-minds.
The "many-minds" interpretation is a multi-world interpretation that defines the splitting of reality on the level of the observers' minds. In this, it differs from Everett's many-worlds interpretation, in which there is no special role for the observer's mind.
Reception.
There is a wide range of claims that are considered "many-worlds" interpretations. It was often claimed by those who do not believe in MWI that Everett himself was not entirely clear as to what he believed; however, MWI adherents (such as DeWitt, Tegmark, Deutsch and others) believe they fully understand Everett's meaning as implying the literal existence of the other worlds. Additionally, recent biographical sources make it clear that Everett believed in the literal reality of the other quantum worlds. Everett's son reported that Hugh Everett "never wavered in his belief over his many-worlds theory". Also Everett was reported to believe "his many-worlds theory guaranteed him immortality".
One of MWI's strongest advocates is David Deutsch. According to Deutsch, the single photon interference pattern observed in the double slit experiment can be explained by interference of photons in multiple universes. Viewed in this way, the single photon interference experiment is indistinguishable from the multiple photon interference experiment. In a more practical vein, in one of the earliest papers on quantum computing, he suggested that parallelism that results from the validity of MWI could lead to "a method by which certain probabilistic tasks can be performed faster by a universal quantum computer than by any classical restriction of it". Deutsch has also proposed that when reversible computers become conscious that MWI will be testable (at least against "naive" Copenhagenism) via the reversible observation of spin.
Asher Peres was an outspoken critic of MWI; for example, a section in his 1993 textbook had the title "Everett's interpretation and other bizarre theories". In fact, Peres not only questioned whether MWI is really an "interpretation", but rather, if "any" interpretations of quantum mechanics are needed at all. Indeed, an interpretation can be regarded as a purely formal transformation, which adds nothing to the rules of the quantum mechanics. Peres seems to suggest that positing the existence of an infinite number of non-communicating parallel universes is highly suspect per those who interpret it as a violation of Occam's razor, i.e., that it does not minimize the number of hypothesized entities. However, it is understood that the number of elementary particles are not a gross violation of Occam's Razor, one counts the types, not the tokens. Max Tegmark remarks that the alternative to many-worlds is "many words", an allusion to the complexity of von Neumann's collapse postulate. On the other hand, the same derogatory qualification "many words" is often applied to MWI by its critics who see it as a word game which obfuscates rather than clarifies by confounding the von Neumann branching of possible worlds with the Schrödinger parallelism of many worlds in superposition.
MWI is considered by some to be unfalsifiable and hence unscientific because the multiple parallel universes are non-communicating, in the sense that no information can be passed between them. Others claim MWI is directly testable. Everett regarded MWI as falsifiable since any test that falsifies conventional quantum theory would also falsify MWI.
According to Martin Gardner, the "other" worlds of MWI have two different interpretations: real or unreal; he claims that Stephen Hawking and Steve Weinberg both favour the unreal interpretation. Gardner also claims that the nonreal interpretation is favoured by the majority of physicists, whereas the "realist" view is only supported by MWI experts such as Deutsch and Bryce DeWitt. Hawking has said that "according to Feynman's idea", all the other histories are as "equally real" as our own, and Martin Gardner reports Hawking saying that MWI is "trivially true". In a 1983 interview, Hawking also said he regarded the MWI as "self-evidently correct" but was dismissive towards questions about the interpretation of quantum mechanics, saying, "When I hear of Schrödinger's cat, I reach for my gun." In the same interview, he also said, "But, look: All that one does, really, is to calculate conditional probabilities—in other words, the probability of A happening, given B. I think that that's all the many worlds interpretation is. Some people overlay it with a lot of mysticism about the wave function splitting into different parts. But all that you're calculating is conditional probabilities." Elsewhere Hawking contrasted his attitude towards the "reality" of physical theories with that of his colleague Roger Penrose, saying, "He's a Platonist and I'm a positivist. He's worried that Schrödinger's cat is in a quantum state, where it is half alive and half dead. He feels that can't correspond to reality. But that doesn't bother me. I don't demand that a theory correspond to reality because I don't know what it is. Reality is not a quality you can test with litmus paper. All I'm concerned with is that the theory should predict the results of measurements. Quantum theory does this very successfully." For his own part, Penrose agrees with Hawking that QM applied to the universe implies MW, although he considers the current lack of a successful theory of quantum gravity negates the claimed universality of conventional QM.
Polls.
Advocates of MWI often cite a poll of 72 "leading cosmologists and other quantum field theorists" conducted by the American political scientist David Raub in 1995 showing 58% agreement with "Yes, I think MWI is true".
The poll is controversial: for example, Victor J. Stenger remarks that Murray Gell-Mann's published work explicitly rejects the existence of simultaneous parallel universes. Collaborating with James Hartle, Gell-Mann is working toward the development a more "palatable" "post-Everett quantum mechanics". Stenger thinks it's fair to say that most physicists dismiss the many-world interpretation as too extreme, while noting it "has merit in finding a place for the observer inside the system being analyzed and doing away with the troublesome notion of wave function collapse".
Max Tegmark also reports the result of a "highly unscientific" poll taken at a 1997 quantum mechanics workshop. According to Tegmark, "The many worlds interpretation (MWI) scored second, comfortably ahead of the consistent histories and Bohm interpretations." Such polls have been taken at other conferences, for example, in response to Sean Carroll's observation, "As crazy as it sounds, most working physicists buy into the many-worlds theory" Michael Nielsen counters: "at a quantum computing conference at Cambridge in 1998, a many-worlder surveyed the audience of approximately 200 people... Many-worlds did just fine, garnering support on a level comparable to, but somewhat below, Copenhagen and decoherence." However, Nielsen notes that it seemed most attendees found it to be a waste of time: Asher Peres "got a huge and sustained round of applause… when he got up at the end of the polling and asked 'And who here believes the laws of physics are decided by a democratic vote?'"
A 2005 poll of fewer than 40 students and researchers taken after a course on the Interpretation of Quantum Mechanics at the Institute for Quantum Computing University of Waterloo found "Many Worlds (and decoherence)" to be the least favored.
A 2011 poll of 33 participants at an Austrian conference found 6 endorsed MWI, 8 "Information-based/information-theoretical", and 14 Copenhagen; the authors remark that the results are similar to Tegmark's 1998 poll.
Speculative implications.
Speculative physics deals with questions which are also discussed in science fiction.
Quantum suicide thought experiment.
Quantum suicide, as a thought experiment, was published independently by Hans Moravec in 1987 and Bruno Marchal in 1988 and was independently developed further by Max Tegmark in 1998. It attempts to distinguish between the Copenhagen interpretation of quantum mechanics and the Everett many-worlds interpretation by means of a variation of the Schrödinger's cat thought experiment, from the cat's point of view. Quantum immortality refers to the subjective experience of surviving quantum suicide regardless of the odds.
Weak coupling.
Another speculation is that the separate worlds remain weakly coupled (e.g., by gravity) permitting "communication between parallel universes". A possible test of this using quantum-optical equipment is described in a 1997 "Foundations of Physics" article by Rainer Plaga. It involves an isolated ion in an ion trap, a quantum measurement that would yield two parallel worlds (their difference just being in the detection of a single photon), and the excitation of the ion from only one of these worlds. If the excited ion can be detected from the other parallel universe, then this would constitute direct evidence in support of the many-worlds interpretation and would automatically exclude the orthodox, "logical", and "many-histories" interpretations. The reason the ion is isolated is to make it not participate immediately in the decoherence which insulates the parallel world branches, therefore allowing it to act as a gateway between the two worlds, and if the measure apparatus could perform the measurements quickly enough before the gateway ion is decoupled then the test would succeed (with electronic computers the necessary time window between the two worlds would be in a time scale of milliseconds or nanoseconds, and if the measurements are taken by humans then a few seconds would still be enough). R. Plaga shows that macroscopic decoherence timescales are a possibility. The proposed test is based on technical equipment described in a 1993 "Physical Review" article by Itano et al. and R. Plaga says that this level of technology is enough to realize the proposed inter-world communication experiment. The necessary technology for precision measurements of single ions already exists since the 1970s, and the ion recommended for excitation is 199Hg+. The excitation methodology is described by Itano et al. and the time needed for it is given by the Rabi flopping formula
Such a test as described by R. Plaga would mean that energy transfer is possible between parallel worlds. This does not violate the fundamental principles of physics because these require energy conservation only for the whole universe and not for the single parallel branches. Neither the excitation of the single ion (which is a degree of freedom of the proposed system) leads to decoherence, something which is proven by Welcher Weg detectors which can excite atoms without momentum transfer (which causes the loss of coherence).
The proposed test would allow for low-bandwidth inter-world communication, the limiting factors of bandwidth and time being dependent on the technology of the equipment. Because of the time needed to determine the state of the partially decohered isolated excited ion based on Itano et al.'s methodology, the ion would decohere by the time its state is determined during the experiment, so Plaga's proposal would pass just enough information between the two worlds to confirm their parallel existence and nothing more. The author contemplates that with increased bandwidth, one could even transfer television imagery across the parallel worlds. For example, Itano et al.'s methodology could be improved (by lowering the time needed for state determination of the excited ion) if a more efficient process were found for the detection of fluorescence radiation using 194 nm photons.
A 1991 article by J.Polchinski also supports the view that inter-world communication is a theoretical possibility. Other authors in a 1994 preprint article also contemplated similar ideas.
The reason inter-world communication seems like a possibility is because decoherence which separates the parallel worlds is never fully complete, therefore weak influences from one parallel world to another can still pass between them, and these should be measurable with advanced technology. Deutsch proposed such an experiment in a 1985 "International Journal of Theoretical Physics" article, but the technology it requires involves human-level artificial intelligence.
Similarity to modal realism.
The many-worlds interpretation has some similarity to modal realism in philosophy, which is the view that the possible worlds used to interpret modal claims exist and are of a kind with the actual world. Unlike the possible worlds of philosophy, however, in quantum mechanics counterfactual alternatives can influence the results of experiments, as in the Elitzur–Vaidman bomb-testing problem or the Quantum Zeno effect. Also, while the worlds of the many-worlds interpretation all share the same physical laws, modal realism postulates a world for every way things could conceivably have been.
Time travel.
The many-worlds interpretation could be one possible way to resolve the paradoxes that one would expect to arise "if" time travel turns out to be permitted by physics (permitting closed timelike curves and thus violating causality). Entering the past would itself be a quantum event causing branching, and therefore the timeline accessed by the time traveller simply would be another timeline of many. In that sense, it would make the Novikov self-consistency principle unnecessary.
Many-worlds in literature and science fiction.
The many-worlds interpretation (and the somewhat related concept of possible worlds) has been associated to numerous themes in literature, art and science fiction.
Some of these stories or films violate fundamental principles of causality and relativity, and are extremely misleading since the information-theoretic structure of the path space of multiple universes (that is information flow between different paths) is very likely extraordinarily complex. Also see referenced in the external links section below where these issues (and other similar ones) are dealt with more decisively.
Another kind of popular illustration of many-worlds splittings, which does not involve information flow between paths, or information flow backwards in time considers alternate outcomes of historical events. According to the many-worlds interpretation, all of the historical speculations entertained within the alternate history genre are realized in parallel universes.
The many-worlds interpretation of reality was anticipated with remarkable fidelity in Olaf Stapledon's 1937 science fiction novel Star Maker, in a paragraph describing one of the many universes created by the Star Maker god of the title. "In one inconceivably complex cosmos, whenever a creature was faced with several possible courses of action, it took them all, thereby creating many distinct temporal dimensions and distinct histories of the cosmos. Since in every evolutionary sequence of the cosmos there were very many creatures, and each was constantly faced with many possible courses, and the combinations of all their courses were innumerable, an infinity of distinct universes exfoliated from every moment of every temporal sequence in this cosmos."

</doc>
<doc id="9611" url="http://en.wikipedia.org/wiki?curid=9611" title="E-commerce">
E-commerce

Electronic commerce, commonly known as e-commerce or eCommerce, is trading in products or services using computer networks, such as the Internet. Electronic commerce draws on technologies such as mobile commerce, electronic funds transfer, supply chain management, Internet marketing, online transaction processing, electronic data interchange (EDI), inventory management systems, and automated data collection systems. Modern electronic commerce typically uses the World Wide Web for at least one part of the transaction's life cycle, although it may also use other technologies such as e-mail.
eCommerce businesses may employ some or all of the following:
Timeline.
A timeline for the development of e-commerce:
emy :)
Business applications.
Some common applications related to electronic commerce are:
Governmental regulation.
In the United States, some electronic commerce activities are regulated by the Federal Trade Commission (FTC). These activities include the use of commercial e-mails, online advertising and consumer privacy. The CAN-SPAM Act of 2003 establishes national standards for direct marketing over e-mail. The Federal Trade Commission Act regulates all forms of advertising, including online advertising, and states that advertising must be truthful and non-deceptive. Using its authority under Section 5 of the FTC Act, which prohibits unfair or deceptive practices, the FTC has brought a number of cases to enforce the promises in corporate privacy statements, including promises about the security of consumers' personal information. As result, any corporate privacy policy related to e-commerce activity may be subject to enforcement by the FTC.
The Ryan Haight Online Pharmacy Consumer Protection Act of 2008, which came into law in 2008, amends the Controlled Substances Act to address online pharmacies.
There is also collaboration between Google and US federal authorities to block illegal online pharmacies from appearing in Google search results.<ref name=" http://ptlb.in/ccici/?p=416"></ref> Recently FedEx Corporation pleaded not guilty to charges made against it regarding dealing with illegal online pharmacies.<ref name=" http://ptlb.in/ecommerce/?p=352"></ref>
Conflict of laws in cyberspace <ref name=" http://perry4law.org/clic/"></ref> is a major hurdle for harmonisation of legal framework for e-commerce around the world. In order to give a uniformity to e-commerce law around the world, many countries adopted the UNCITRAL Model Law on Electronic Commerce (1996) <ref name=" http://www.uncitral.org/uncitral/en/uncitral_texts/electronic_commerce/1996Model.html"></ref>
Internationally there is the International Consumer Protection and Enforcement Network (ICPEN), which was formed in 1991 from an informal network of government customer fair trade organisations. The purpose was stated as being to find ways of co-operating on tackling consumer problems connected with cross-border transactions in both goods and services, and to help ensure exchanges of information among the participants for mutual benefit and understanding. From this came Econsumer.gov, an ICPEN initiative since April 2001. It is a portal to report complaints about online and related transactions with foreign companies.
There is also Asia Pacific Economic Cooperation (APEC) was established in 1989 with the vision of achieving stability, security and prosperity for the region through free and open trade and investment. APEC has an Electronic Commerce Steering Group as well as working on common privacy regulations throughout the APEC region.
In Australia, Trade is covered under Australian Treasury Guidelines for electronic commerce, and the Australian Competition and Consumer Commission regulates and offers advice on how to deal with businesses online, and offers specific advice on what happens if things go wrong.
In the United Kingdom, The Financial Services Authority (FSA) was formerly the regulating authority for most aspects of the EU's Payment Services Directive (PSD), until its replacement in 2013 by the Prudential Regulation Authority and the Financial Conduct Authority. The UK implemented the PSD through the Payment Services Regulations 2009 (PSRs), which came into effect on 1 November 2009. The PSR affects firms providing payment services and their customers. These firms include banks, non-bank credit card issuers and non-bank merchant acquirers, e-money issuers, etc. The PSRs created a new class of regulated firms known as payment institutions (PIs), who are subject to prudential requirements. Article 87 of the PSD requires the European Commission to report on the implementation and impact of the PSD by 1 November 2012.
In India, the Information Technology Act 2000 governs the basic applicability of e-commerce. It is based upon UNCITRAL Model but is not a comprehensive legislation to deal with e-commerce related activities in India. Further, e-commerce laws and regulations in India <ref name=" http://ptlb.in/ecommerce/"></ref> are also supplemented by different laws of India as applicable to the field of e-commerce. For instance, e-commerce relating to pharmaceuticals, healthcare, traveling, etc. are governed by different laws though the information technology act, 2000 prescribes some common requirements for all these fields. The competition commission of India (CCI) regulates anti competition and anti trade practices in e-commerce fields in India.<ref name=" http://ptlb.in/ecommerce/?p=373"></ref> Some stakeholders have decided to approach courts and CCI against e-commerce websites to file complaint about unfair trade practices and predatory pricing by such e-commerce websites.<ref name=" http://ptlb.in/ecommerce/?p=378"></ref><ref name=" http://www.business-standard.com/article/pti-stories/cait-to-launch-nationwide-protest-against-online-retail-cos-114101301124_1.html "></ref>
In China, the Telecommunications Regulations of the People's Republic of China (promulgated on September 25, 2000), stipulated the Ministry of Industry and Information Technology (MIIT) as the government department regulating all telecommunications related activities, including electronic commerce. On the same day, The Administrative Measures on Internet Information Services released, is the first administrative regulation to address profit-generating activities conducted through the Internet, and lay the foundation for future regulations governing e-commerce in China. In August 28, 2004, the eleventh session of the tenth NPC Standing Committee adopted The Electronic Signature Law, which regulates data message, electronic signature authentication and legal liability issues. It is considered the first law in China’s e-commerce legislation. It was a milestone in the course of improving China’s electronic commerce legislation, and also marks the entering of China’s rapid development stage for electronic commerce legislation.
Forms.
Contemporary electronic commerce involves everything from ordering "digital" content for immediate online consumption, to ordering conventional goods and services, to "meta" services to facilitate other types of electronic commerce.
On the institutional level, big corporations and financial institutions use the internet to exchange financial data to facilitate domestic and international business. Data integrity and security are very hot and pressing issues for electronic commerce.
Aside from traditional e-Commerce, m-Commerce as well as the nascent t-Commerce channels are often seen as the current 2013 poster children of electronic I-Commerce.
Global trends.
In 2010, the United Kingdom had the biggest e-commerce market in the world when measured by the amount spent per capita. The Czech Republic is the European country where ecommerce delivers the biggest contribution to the enterprises´ total revenue. Almost a quarter (24%) of the country’s total turnover is generated via the online channel.
Among emerging economies, China's e-commerce presence continues to expand every year. With 384 million internet users, China's online shopping sales rose to $36.6 billion in 2009 and one of the reasons behind the huge growth has been the improved trust level for shoppers. The Chinese retailers have been able to help consumers feel more comfortable shopping online. China's cross-border e-commerce is also growing rapidly. E-commerce transactions between China and other countries increased 32% to 2.3 trillion yuan ($375.8 billion) in 2012 and accounted for 9.6% of China's total international trade In 2013, Alibaba had an e-commerce market share of 80% in China.
Other BRIC countries are witnessing the accelerated growth of eCommerce as well. Brazil's eCommerce is growing quickly with retail eCommerce sales expected to grow at a healthy double-digit pace through 2014. By 2016, eMarketer expects retail ecommerce sales in Brazil to reach $17.3 billion. India has an internet user base of about 243.2 million as of January 2014. Despite being third largest userbase in world, the penetration of Internet is low compared to markets like the United States, United Kingdom or France but is growing at a much faster rate, adding around 6 million new entrants every month. The industry consensus is that growth is at an inflection point. In India, cash on delivery is the most preferred payment method, accumulating 75% of the e-retail activities.
E-Commerce has become an important tool for small and large businesses worldwide, not only to sell to customers, but also to engage them.
In 2012, ecommerce sales topped $1 trillion for the first time in history.
Mobile devices are playing an increasing role in the mix of eCommerce. Some estimates show that purchases made on mobile devices will make up 25% of the market by 2017. According to Cisco Visual Networking Index, in 2014 the amount of mobile devices will outnumber the number of world population.
In the past 10 years, e-commerce is in a period of rapid development. Cross-border e-commerce is called the Internet thinking along with traditional import and export trade. Cross-border e-commerce enables international trade towards more convenient and free open to cooperate between different countries in the world, incorporating developed and developing countries. In the short term, developing countries may be limited to IT, but in the long term, they would change the barrier to develop their IT facilities, and continuing to close to developed countries. The moment, developing countries like China and India are developing e-commerce very rapidly, such as China 's Alibaba, the financing capital (£15 billions) is the highest ever in e-commerce company. In addition, China is becoming the biggest e-commerce provider in the world.The number of Internet users in China which amounts to 600 millions, and which is doubled than USA users in total.
For traditional businesses, one research stated that information technology and cross-border e-commerce is a good opportunity for the rapid development and growth of enterprises. Many companies have invested enormous volume of investment in mobile applications.The DeLone and McLean Model stated that 3 perspectives are contributed to a successful e-business, including information system quality, service quality and users satisfaction.There is no limit of time and space, there are more opportunities to reach out to customers around the world, and to cut down unnecessary intermediate links, thereby reducing the cost price, and can benefit from one on one large customer data analysis, to achieve a high degree of personal customization strategic plan, in order to fully enhance the core competitiveness of the products in company
Impact on markets and retailers.
Economists have theorized that e-commerce ought to lead to intensified price competition, as it increases consumers' ability to gather information about products and prices. Research by four economists at the University of Chicago has found that the growth of online shopping has also affected industry structure in two areas that have seen significant growth in e-commerce, bookshops and travel agencies. Generally, larger firms are able to use economies of scale and offer lower prices. The lone exception to this pattern has been the very smallest category of bookseller, shops with between one and four employees, which appear to have withstood the trend.
Individual or business involved in e-commerce whether buyers or sellers rely on Internet-based technology in order to accomplish their transactions. E-commerce is recognized for its ability to allow business to communicate and to form transaction anytime and anyplace. Whether an individual is in the US or overseas, business can be conducted through the internet. The power of e-commerce allows geophysical barriers to disappear, making all consumers and businesses on earth potential customers and suppliers. eBay is a good example of e-commerce business individuals and businesses are able to post their items and sell them around the Globe.
In e-commerce activities, supply chain and logistics are two most crucial factors need to be considered. Typically, cross-border logistics need about few weeks time round. Based on this low efficiency of the supply chain service, customer satisfaction will be greatly reduced. Some researcher stated that combining e-commerce competence and IT setup could well enhance company’s overall business worth. Other researcher stated that e-commerce need to consider the establishment of warehouse centers in foreign countries, to create high efficiency of the logistics system, not only improve customers’ satisfaction, but also can improve customers’ loyalty.
Some researcher investigated that if a company want to enhance international customers’ satisfaction, where cultural website need to be adapted in particular country, rather than solely depending own its local country. However, according to this research findings, the researcher found that German company had treated its international website as the same local model, such as in UK and US online marketing. A company could save money and make decision quickly via the identical strategy in different country. However, opportunity cost could be occurred, if the local strategy does not match to a new market, the company could lose its potential customer.
Impact on supply chain management.
For a long time, companies had been troubled by the gap between the benefits which supply chain technology has and the solutions to deliver those benefits. However, the emergence of e-commerce has provided a more practical and effective way of delivering the benefits of the new supply chain technologies.
E-commerce has the capability to integrate all inter-company and intra-company functions, meaning that the three flows (physical flow, financial flow and information flow) of the supply chain could be also affected by e-commerce. The affections on physical flows improved the way of product and inventory movement level for companies. For the information flows, e-commerce optimised the capacity of information processing than companies used to have, and for the financial flows, e-comers allows companies to have more efficient payment and settlement solutions.
In addition, e-commerce has a more sophisticated level of impact on supply chains: Firstly, the performance gap will be eliminated since companies can identify gaps between different levels of supply chains by electronic means of solutions; Secondly, as a result of e-commerce emergence, new capabilities such implementing ERP systems have helped companies to manage operations with customers and suppliers. Yet these new capabilities are still not fully exploited. Thirdly, technology companies would keep investing on new e-commerce software solutions as they are expecting investment return. Fourthly, e-commerce would help to solve many aspects of issues that companies may feel difficult to cope with, such as political barriers or cross-country changes. Finally, e-commerce provides companies a more efficient and effective way to collaborate with each other within the supply chain.
The social impact of e-commerce.
Along with the e-commerce and its unique charm that has appeared gradually, virtual enterprise, virtual bank, network marketing, online shopping, payment and advertising, such this new vocabulary which is unheard-of and now has become as familiar to people. This reflects that the e-commerce has huge impact on the economy and society from the other side. For instance, B2B is a rapidly growing business in the world that leads to lower cost and then improves the economic efficiency and also bring along the growth of employment. 
To understand how the e-commerce has affected the society and economy, this article will mention three issues below: 
1. The e-commerce has changed the relative importance of time, but as the pillars of indicator of the country’s economic state that the importance of time should not be ignored.
2. The e-commerce offers the consumer or enterprise various information they need, making information into total transparency, will force enterprise no longer is able to use the mode of space or advertisement to raise their competitive edge. Moreover, in theory, perfect competition between the consumer sovereignty and industry will maximize social welfare.
3. In fact, during the economic activity in the past, large enterprise frequently has advantage of information resource, and thus at the expense of consumers. Nowadays, the transparent and real-time information protects the rights of consumers, because the consumers can use internet to pick out the portfolio to the benefit of themselves. The competitiveness of enterprises will be much more obvious than before, consequently, social welfare would be improved by the development of the e-commerce.
4. The new economy led by the e-commerce change humanistic spirit as well, but above all, is the employee loyalty. Due to the market with competition, the employee’s level of professionalism becomes the crucial for enterprise in the niche market. The enterprises must pay attention to how to build up the enterprises inner culture and a set of interactive mechanisms and it is the prime problem for them. Furthermore, though the mode of e-commerce decrease the information cost and transaction cost, however, its development also makes human being are overly computer literate. In hence, emphasized more humanistic attitude to work is another project for enterprise to development. Life is the root of all and high technology are merely an assistive tool to support our quality of life.
The e-commerce is not a kind of new industry, but it is creating a new economic model. Most of people agree that the e-commerce indeed to be important and significant for economic society in the future, but actually that is a bit of clueless feeling at the beginning, this problem is exactly prove the e-commerce is a sort of incorporeal revolution. Generally speaking, as a type of business active procedure, the e-commerce is going to leading an unprecedented revolution in the world, the influence of this model far exceeded the commercial affair itself. Except the mentioned above, in the area of law, education, culture and also policy, the e-commerce will continue that rise in impact. The e-commerce is truly to take human beings into the information society.
Distribution channels.
E-commerce has grown in importance as companies have adopted pure-click and brick-and-click channel systems. We can distinguish pure-click and brick-and-click channel system adopted by companies.
Examples of new e-commerce systems.
According to eMarketer research company, "by 2017, 65.8 per cent of Britons will use smartphones" (cited by Williams, 2014).
Bringing online experience into the real world, also allows the development of the economy and the interaction between stores and customers. A great example of this new e-commerce system is what the Burberry store in London did in 2012. They refurbished the entire store with numerous big screens, photo-studios, and also provided a stage for live acts. Moreover, on the digital screens which are across the store, some fashion shows´ images and advertising campaigns are displayed (William, 2014). In this way, the experience of purchasing becomes more vivid and entertaining while the online and offline components are working together.
Another example is the Kiddicare smartphone app, in which consumers can compare prices. The app allows people to identify the location of sale products and to check whether the item they are looking for is in stock, or if it can be ordered online without going to the `real´ store (William, 2014).
In the United States, the Walmart app allows consumers to check product availability and prices both online and offline. Moreover, you can also add to your shopping list items by scanning them, see their details and information, and check purchasers´ ratings and reviews.
Further reading.
</dl>

</doc>
<doc id="9613" url="http://en.wikipedia.org/wiki?curid=9613" title="Euler's formula">
Euler's formula

Euler's formula, named after Leonhard Euler, is a mathematical formula in complex analysis that establishes the fundamental relationship between the trigonometric functions and the complex exponential function. Euler's formula states that, for any real number "x".
where "e" is the base of the natural logarithm, "i" is the imaginary unit, and cos and sin are the trigonometric functions cosine and sine respectively, with the argument "x" given in radians. This complex exponential function is sometimes denoted cis("x") ("cosine plus i sine"). The formula is still valid if "x" is a complex number, and so some authors refer to the more general complex version as Euler's formula.
Euler's formula is ubiquitous in mathematics, physics, and engineering. The physicist Richard Feynman called the equation "our jewel" and "the most remarkable formula in mathematics."
History.
It was Johann Bernoulli who noted that
formula_2
And since
formula_3
the above equation tells us something about complex logarithms. Bernoulli, however, did not evaluate the integral. 
Bernoulli's correspondence with Euler (who also knew the above equation) shows that Bernoulli did not fully understand complex logarithms. Euler also suggested that the complex logarithms can have infinitely many values.
Meanwhile, Roger Cotes, in 1714, discovered that
formula_4
(formula_5 is the natural logarithm). 
Cotes missed the fact that a complex logarithm can have infinitely many values, differing by multiples of 2π, due to the periodicity of the trigonometric functions.
Around 1740 Euler turned his attention to the exponential function instead of logarithms, and obtained the formula used today that is named after him. It was published in 1748, obtained by comparing the series expansions of the exponential and trigonometric expressions.
None of these mathematicians saw the geometrical interpretation of the formula; the view of complex numbers as points in the complex plane was described some 50 years later by Caspar Wessel.
Applications in complex number theory.
This formula can be interpreted as saying that the function "eix" is a unit complex number, i.e., traces out the unit circle in the complex plane as "x" ranges through the real numbers. Here, "x" is the angle that a line connecting the origin with a point on the unit circle makes with the positive real axis, measured counter clockwise and in radians.
The original proof is based on the Taylor series expansions of the exponential function "ez" (where "z" is a complex number) and of sin "x" and cos "x" for real numbers "x" (see below). In fact, the same proof shows that Euler's formula is even valid for all "complex" numbers "x".
A point in the complex plane can be represented by a complex number written in
cartesian coordinates. Euler's formula provides a means of conversion between cartesian coordinates and polar coordinates. The polar form simplifies the mathematics when used in multiplication or powers of complex numbers. Any complex number "z" = "x" + "iy", and its complex conjugate, "z" = "x" − "iy", can be written as
where
"ϕ" is the "argument" of "z"—i.e., the angle between the "x" axis and the vector "z" measured counterclockwise and in radians—which is defined up to addition of 2π. Many texts write θ = tan−1("y"/"x") instead of θ = atan2("y","x"), but the first equation needs adjustment when "x" ≤ 0. This is because, for any real x, y not both zero, the angles of the vectors (x,y) and (-x,-y) differ by π radians, but have the identical value of tan(θ) = y/x.
Now, taking this derived formula, we can use Euler's formula to define the logarithm of a complex number. To do this, we also use the definition of the logarithm (as the inverse operator of exponentiation) that
and that
both valid for any complex numbers "a" and "b".
Therefore, one can write:
for any "z" ≠ 0. Taking the logarithm of both sides shows that:
and in fact this can be used as the definition for the complex logarithm. The logarithm of a complex number is thus a multi-valued function, because "ϕ" is multi-valued.
Finally, the other exponential law
which can be seen to hold for all integers "k", together with Euler's formula, implies several trigonometric identities as well as de Moivre's formula.
Relationship to trigonometry.
Euler's formula provides a powerful connection between analysis and trigonometry, and provides an interpretation of the sine and cosine functions as weighted sums of the exponential function:
The two equations above can be derived by adding or subtracting Euler's formulas:
and solving for either cosine or sine.
These formulas can even serve as the definition of the trigonometric functions for complex arguments "x". For example, letting "x" = "iy", we have:
Complex exponentials can simplify trigonometry, because they are easier to manipulate than their sinusoidal components. One technique is simply to convert sinusoids into equivalent expressions in terms of exponentials. After the manipulations, the simplified result is still real-valued. For example:
Another technique is to represent the sinusoids in terms of the real part of a complex expression, and perform the manipulations on the complex expression. For example:
This formula is used for recursive generation of cos("nx") for integer values of "n" and arbitrary "x" (in radians).
See also Phasor arithmetic.
Topological interpretation.
In the language of topology, Euler's formula states that the imaginary exponential function formula_21 is a (surjective) morphism of topological groups from the real line ℝ to the unit circle formula_22. In fact, this exhibits ℝ as a covering space of formula_22. Similarly, Euler's identity says that the kernel of this map is formula_24, where formula_25. These observations may be combined and summarized in the commutative diagram below:
Other applications.
In differential equations, the function "eix" is often used to simplify derivations, even if the final answer is a real function involving sine and cosine. The reason for this is that the complex exponential is the eigenfunction of differentiation. Euler's identity is an easy consequence of Euler's formula.
In electronic engineering and other fields, signals that vary periodically over time are often described as a combination of sine and cosine functions (see Fourier analysis), and these are more conveniently expressed as the real part of exponential functions with imaginary exponents, using Euler's formula. Also, phasor analysis of circuits can include Euler's formula to represent the impedance of a capacitor or an inductor.
Definitions of complex exponentiation.
The exponential function "ex" for real values of "x" may be defined in a few different equivalent ways (see Characterizations of the exponential function). Several of these methods may be directly extended to give definitions of "ez" for complex values of "z" simply by substituting "z" in place of "x" and using the complex algebraic operations. In particular we may use either of the two following definitions which are equivalent. From a more advanced perspective, each of these definitions may be interpreted as giving the unique analytic continuation of "ex" to the complex plane.
Power series definition.
For complex "z"
Using the ratio test it is possible to show that this power series has an infinite radius of convergence, and so defines "ez" for all complex "z".
Limit definition.
For complex "z"
Proofs.
Various proofs of the formula are possible.
Using power series.
Here is a proof of Euler's formula using power series expansions
as well as basic facts about the powers of "i":
and so on. Using now the power series definition from above we see that for real values of "x"
In the last step we have simply recognized the Maclaurin series for "cos(x)" and "sin(x)". The rearrangement of terms is justified because each series is absolutely convergent.
Using the limit definition.
An alternative proof is based on the limit definition of "ez":
Substitute "z" = "ix", and let "n" be a very large integer, say 1000. Then, based on the limit definition, the complex number (1+"ix"/1000)1000 is supposed to be a good approximation to "e""ix". So, what is the value of (1+"ix"/1000)1000?
Consider the sequence of 1000 complex numbers:
(We started with 1, and successively multiplied it by (1+"ix"/1000), 1000 times.) If the points of this sequence are plotted in the complex plane (see animation at right), they approximately trace out the unit circle, with each point being "x"/1000 radians counterclockwise of the previous point. (The proof of this is based on the rules of trigonometry and complex-number algebra.) Therefore, the last point in the sequence, (1 + "ix"/1000)1000, is approximately the point on the unit circle of the complex plane located "x" radians counterclockwise from +1, that is the point cos "x" + "i" sin "x". If we replaced the number 1000 by larger and larger numbers, all of the approximations in this paragraph become more and more accurate. Therefore, "e""ix" = cos "x" + "i" sin "x".
Using calculus.
Another proof is based on the fact that all complex numbers can be expressed in polar coordinates. Therefore for some "r" and "θ" depending on "x",
Now from any of the definitions of the exponential function it can be shown that the derivative of "e""ix" is "ie""ix". Therefore differentiating both sides gives
Substituting formula_34 for formula_35 and equating real and imaginary parts in this formula gives formula_36 and formula_37. Together with the initial values formula_38 and formula_39 which come from formula_40 this gives formula_41 and formula_42. This proves the formula formula_43.

</doc>
<doc id="9615" url="http://en.wikipedia.org/wiki?curid=9615" title="Édouard Manet">
Édouard Manet

Édouard Manet ( or ; ]; 23 January 1832 – 30 April 1883) was a French painter. He was one of the first 19th-century artists to paint modern life, and a pivotal figure in the transition from Realism to Impressionism.
His early masterworks, "The Luncheon on the Grass (Le déjeuner sur l'herbe)" and "Olympia", both 1863, caused great controversy and served as rallying points for the young painters who would create Impressionism. Today, these are considered watershed paintings that mark the genesis of modern art.
Biography.
Born into an upper-class household with strong political connections, Manet rejected the future originally envisioned for him, and became engrossed in the world of painting. He married Suzanne Leenhoff in 1863. The last 20 years of Manet's life saw him form bonds with other great artists of the time, and develop his own style that would be heralded as innovative and serve as a major influence for future painters.
Early life.
Édouard Manet was born in Paris on 23 January 1832, in the ancestral hôtel particulier (mansion) on the rue Bonaparte to an affluent and well-connected family. His mother, Eugénie-Desirée Fournier, was the daughter of a diplomat and goddaughter of the Swedish crown prince Charles Bernadotte, from whom the Swedish monarchs are descended. His father, Auguste Manet, was a French judge who expected Édouard to pursue a career in law. His uncle, Edmond Fournier, encouraged him to pursue painting and took young Manet to the Louvre. In 1841 he enrolled at secondary school, the Collège Rollin. In 1845, at the advice of his uncle, Manet enrolled in a special course of drawing where he met Antonin Proust, future Minister of Fine Arts and subsequent lifelong friend.
At his father's suggestion, in 1848 he sailed on a training vessel to Rio de Janeiro. After he twice failed the examination to join the Navy, his father relented to his wishes to pursue an art education. From 1850 to 1856, Manet studied under the academic painter Thomas Couture. In his spare time, Manet copied the old masters in the Louvre.
From 1853 to 1856, Manet visited Germany, Italy, and the Netherlands, during which time he was influenced by the Dutch painter Frans Hals, and the Spanish artists Diego Velázquez and Francisco José de Goya.
In 1856, Manet opened a studio. His style in this period was characterized by loose brush strokes, simplification of details and the suppression of transitional tones. Adopting the current style of realism initiated by Gustave Courbet, he painted "The Absinthe Drinker" (1858–59) and other contemporary subjects such as beggars, singers, Gypsies, people in cafés, and bullfights. After his early career, he rarely painted religious, mythological, or historical subjects; examples include his "Christ Mocked", now in the Art Institute of Chicago, and "Christ with Angels", in the Metropolitan Museum of Art, New York. Manet had two canvases accepted at the Salon in 1861. A portrait of his mother and father, who at the time was paralysed and robbed of speech by a stroke, was ill received by critics. The other, "The Spanish Singer", was admired by Theophile Gautier, and placed in a more conspicuous location as a result of its popularity with Salon-goers. Manet's work, which appeared "slightly slapdash" when compared with the meticulous style of so many other Salon paintings, intrigued some young artists. "The Spanish Singer", painted in a "strange new fashion [-] caused many painters' eyes to open and their jaws to drop."
"Music in the Tuileries".
"Music in the Tuileries" is an early example of Manet's painterly style. Inspired by Hals and Velázquez, it is a harbinger of his lifelong interest in the subject of leisure.
While the picture was regarded as unfinished by some, the suggested atmosphere imparts a sense of what the Tuileries gardens were like at the time; one may imagine the music and conversation.
Here, Manet has depicted his friends, artists, authors, and musicians who take part, and he has included a self-portrait among the subjects.
"Luncheon on the Grass "("Le déjeuner sur l'herbe").
A major early work is "The Luncheon on the Grass (Le déjeuner sur l'herbe)". The Paris Salon rejected it for exhibition in 1863 but Manet exhibited it at the Salon des Refusés (Salon of the Rejected) later in the year. Emperor Napoleon III had initiated The Salon des Refusés after the Paris Salon rejected more than 4,000 paintings in 1863. Manet employed model Victorine Meurent, his wife Suzanne, future brother-in-law Ferdinand Leenhoff, and one of his brothers to pose. Meurent also posed for several more of Manet's important paintings including "Olympia"; and by the mid-1870s she became an accomplished painter in her own right.
The painting's juxtaposition of fully dressed men and a nude woman was controversial, as was its abbreviated, sketch-like handling, an innovation that distinguished Manet from Courbet. At the same time, Manet's composition reveals his study of the old masters, as the disposition of the main figures is derived from Marcantonio Raimondi's engraving of the "Judgement of Paris" (c. 1515) based on a drawing by Raphael.
Two additional works cited by scholars as important precedents for "Le déjeuner sur l'herbe" are "Pastoral Concert" (c. 1510, The Louvre) and "The Tempest" (Gallerie dell'Accademia, Venice), both of which are attributed variously to Italian Renaissance masters Giorgione or Titian. "The Tempest" is an enigmatic painting featuring a fully dressed man and a nude woman in a rural setting. The man is standing to the left and gazing to the side, apparently at the woman, who is seated and breastfeeding a baby; the relationship between the two figures is unclear. In "Pastoral Concert", two clothed men and a nude woman are seated on the grass, engaged in music making, while a second nude woman stands beside them.
"Olympia".
As he had in "Luncheon on the Grass", Manet again paraphrased a respected work by a Renaissance artist in the painting "Olympia" (1863), a nude portrayed in a style reminiscent of early studio photographs, but whose pose was based on Titian's "Venus of Urbino" (1538). The painting is also reminiscent of Francisco Goya's painting "The Nude Maja" (1800).
Manet embarked on the canvas after being challenged to give the Salon a nude painting to display. His uniquely frank depiction of a self-assured prostitute was accepted by the Paris Salon in 1865, where it created a scandal. According to Antonin Proust, "only the precautions taken by the administration prevented the painting being punctured and torn" by offended viewers. The painting was controversial partly because the nude is wearing some small items of clothing such as an orchid in her hair, a bracelet, a ribbon around her neck, and mule slippers, all of which accentuated her nakedness, sexuality, and comfortable courtesan lifestyle. The orchid, upswept hair, black cat, and bouquet of flowers were all recognized symbols of sexuality at the time. This modern Venus' body is thin, counter to prevailing standards; the painting's lack of idealism rankled viewers. The painting's flatness, inspired by Japanese wood block art, serves to make the nude more human and less voluptuous. A fully dressed black servant is featured, exploiting the then-current theory that black people were hyper-sexed. That she is wearing the clothing of a servant to a courtesan here furthers the sexual tension of the piece.
Olympia's body as well as her gaze is unabashedly confrontational. She defiantly looks out as her servant offers flowers from one of her male suitors. Although her hand rests on her leg, hiding her pubic area, the reference to traditional female virtue is ironic; a notion of modesty is notoriously absent in this work. A contemporary critic denounced Olympia's "shamelessly flexed" left hand, which seemed to him a mockery of the relaxed, shielding hand of Titian's Venus. Likewise, the alert black cat at the foot of the bed strikes a sexually rebellious note in contrast to that of the sleeping dog in Titian's portrayal of the goddess in his Venus of Urbino.
"Olympia" was the subject of caricatures in the popular press, but was championed by the French avant-garde community, and the painting's significance was appreciated by artists such as Gustave Courbet, Paul Cézanne, Claude Monet, and later Paul Gauguin.
As with "Luncheon on the Grass", the painting raised the issue of prostitution within contemporary France and the roles of women within society.
Life and times.
The roughly painted style and photographic lighting in these works was seen as specifically modern, and as a challenge to the Renaissance works Manet copied or used as source material. His work is considered 'early modern', partially because of the black outlining of figures, which draws attention to the surface of the picture plane and the material quality of paint.
He became friends with the Impressionists Edgar Degas, Claude Monet, Pierre-Auguste Renoir, Alfred Sisley, Paul Cézanne and Camille Pissarro through another painter, Berthe Morisot, who was a member of the group and drew him into their activities. The grand niece of the painter Jean-Honoré Fragonard, Morisot had her first painting accepted in the Salon de Paris in 1864, and she continued to show in the salon for the next ten years.
Manet became the friend and colleague of Berthe Morisot in 1868. She is credited with convincing Manet to attempt plein air painting, which she had been practicing since she was introduced to it by another friend of hers, Camille Corot. They had a reciprocating relationship and Manet incorporated some of her techniques into his paintings. In 1874, she became his sister-in-law when she married his brother, Eugene.
Unlike the core Impressionist group, Manet maintained that modern artists should seek to exhibit at the Paris Salon rather than abandon it in favor of independent exhibitions. Nevertheless, when Manet was excluded from the International Exhibition of 1867, he set up his own exhibition. His mother worried that he would waste all his inheritance on this project, which was enormously expensive. While the exhibition earned poor reviews from the major critics, it also provided his first contacts with several future Impressionist painters, including Degas.
Although his own work influenced and anticipated the Impressionist style, he resisted involvement in Impressionist exhibitions, partly because he did not wish to be seen as the representative of a group identity, and partly because he preferred to exhibit at the Salon. Eva Gonzalès was his only formal student.
He was influenced by the Impressionists, especially Monet and Morisot. Their influence is seen in Manet's use of lighter colors, but he retained his distinctive use of black, uncharacteristic of Impressionist painting. He painted many outdoor (plein air) pieces, but always returned to what he considered the serious work of the studio.
Manet enjoyed a close friendship with composer Emmanuel Chabrier, painting two portraits of him; the musician owned 14 of Manet's paintings and dedicated his "Impromptu" to Manet's wife.
Throughout his life, although resisted by art critics, Manet could number as his champions Émile Zola, who supported him publicly in the press, Stéphane Mallarmé, and Charles Baudelaire, who challenged him to depict life as it was. Manet, in turn, drew or painted each of them.
Cafe scenes.
Manet's paintings of cafe scenes are observations of social life in 19th-century Paris. People are depicted drinking beer, listening to music, flirting, reading, or waiting. Many of these paintings were based on sketches executed on the spot. He often visited the Brasserie Reichshoffen on boulevard de Rochechourt, upon which he based "At the Cafe" in 1878. Several people are at the bar, and one woman confronts the viewer while others wait to be served. Such depictions represent the painted journal of a flâneur. These are painted in a style which is loose, referencing Hals and Velázquez, yet they capture the mood and feeling of Parisian night life. They are painted snapshots of bohemianism, urban working people, as well as some of the bourgeoisie.
In "Corner of a Cafe Concert", a man smokes while behind him a waitress serves drinks. In "The Beer Drinkers" a woman enjoys her beer in the company of a friend. In "The Cafe Concert", shown at right, a sophisticated gentleman sits at a bar while a waitress stands resolutely in the background, sipping her drink. In "The Waitress", a serving woman pauses for a moment behind a seated customer smoking a pipe, while a ballet dancer, with arms extended as she is about to turn, is on stage in the background.
Manet also sat at the restaurant on the Avenue de Clichy called Pere Lathuille's, which had a garden in addition to the dining area. One of the paintings he produced here was "Chez le père Lathuille" (At Pere Lathuille's), in which a man displays an unrequited interest in a woman dining near him.
In "Le Bon Bock" (1873), a large, cheerful, bearded man sits with a pipe in one hand and a glass of beer in the other, looking straight at the viewer.
Paintings of social activities.
Manet painted the upper class enjoying more formal social activities. In "Masked Ball at the Opera", Manet shows a lively crowd of people enjoying a party. Men stand with top hats and long black suits while talking to women with masks and costumes. He included portraits of his friends in this picture.
His 1868 painting "The Luncheon" was posed in the dining room of the Manet house.
Manet depicted other popular activities in his work. In "The Races at Longchamp", an unusual perspective is employed to underscore the furious energy of racehorses as they rush toward the viewer. In "Skating", Manet shows a well dressed woman in the foreground, while others skate behind her. Always there is the sense of active urban life continuing behind the subject, extending outside the frame of the canvas.
In "View of the International Exhibition", soldiers relax, seated and standing, prosperous couples are talking. There is a gardener, a boy with a dog, a woman on horseback—in short, a sample of the classes and ages of the people of Paris.
War.
Manet's response to modern life included works devoted to war, in subjects that may be seen as updated interpretations of the genre of "history painting". The first such work was the "Battle of the Kearsarge and Alabama" (1864), a sea skirmish known as the "Battle of Cherbourg (1864)" from the American Civil War which took place off the French coast, and may have been witnessed by the artist.
Of interest next was the French intervention in Mexico; from 1867 to 1869 Manet painted three versions of the "Execution of Emperor Maximilian", an event which raised concerns regarding French foreign and domestic policy. The several versions of the "Execution" are among Manet's largest paintings, which suggests that the theme was one which the painter regarded as most important. Its subject is the execution by Mexican firing squad of a Habsburg emperor who had been installed by Napoleon III. Neither the paintings nor a lithograph of the subject were permitted to be shown in France. As an indictment of formalized slaughter the paintings look back to Goya, and anticipate Picasso's "Guernica".
In January 1871, Manet traveled to Oloron-Sainte-Marie in the Pyrenees. In his absence his friends added his name to the "Fédération des artistes" (see: Courbet) of the Paris Commune. Manet stayed away from Paris, perhaps, until after the "semaine sanglante": in a letter to Berthe Morisot at Cherbourg (10 June 1871) he writes, "We came back to Paris a few days ago..." (the semaine sanglante ended on 28 May).
The Prints and Drawings Collection of the Museum of Fine Arts (Budapest) has a watercolour/gouache ("The Barricade") by Manet, depicting a summary execution of Communards by Versailles troops based on a lithograph of the execution of Maximilian. A similar piece ("The Barricade"), oil on plywood, is held by a private collector.
On 18 March 1871, he wrote to his (confederate) friend Félix Bracquemond in Paris about his visit to Bordeaux, the provisory seat of the French National Assembly of the Third French Republic where Émile Zola introduced him to the sites: "I never imagined that France could be represented by such doddering old fools, not excepting that little twit Thiers..." If this could be interpreted as support of the Commune, a following letter to Bracquemond (21 March 1871) expressed his idea more clearly: "Only party hacks and the ambitious, the Henrys of this world following on the heels of the Milliéres, the grotesque imitators of the Commune of 1793..." He knew the communard Lucien Henry to have been a former painter's model and Millière, an insurance agent. "What an encouragement all these bloodthirsty caperings are for the arts! But there is at least one consolation in our misfortunes: that we're not politicians and have no desire to be elected as deputies".
Paris.
Manet depicted many scenes of the streets of Paris in his works. The "Rue Mosnier Decked with Flags" depicts red, white, and blue pennants covering buildings on either side of the street; another painting of the same title features a one-legged man walking with crutches. Again depicting the same street, but this time in a different context, is "Rue Mosnier with Pavers", in which men repair the roadway while people and horses move past.
"The Railway", widely known as "The Gare Saint-Lazare", was painted in 1873. The setting is the urban landscape of Paris in the late 19th century. Using his favorite model in his last painting of her, a fellow painter, Victorine Meurent, also the model for "Olympia" and the "Luncheon on the Grass", sits before an iron fence holding a sleeping puppy and an open book in her lap. Next to her is a little girl with her back to the painter, watching a train pass beneath them.
Instead of choosing the traditional natural view as background for an outdoor scene, Manet opts for the iron grating which "boldly stretches across the canvas" The only evidence of the train is its white cloud of steam. In the distance, modern apartment buildings are seen. This arrangement compresses the foreground into a narrow focus. The traditional convention of deep space is ignored.
Historian Isabelle Dervaux has described the reception this painting received when it was first exhibited at the official Paris Salon of 1874: "Visitors and critics found its subject baffling, its composition incoherent, and its execution sketchy. Caricaturists ridiculed Manet's picture, in which only a few recognized the symbol of modernity that it has become today". The painting is currently in the National Gallery of Art in Washington, D.C.
Manet painted several boating subjects in 1874. "Boating", now in the Metropolitan Museum of Art, exemplifies in its conciseness the lessons Manet learned from Japanese prints, and the abrupt cropping by the frame of the boat and sail adds to the immediacy of the image. X-rays and pentimenti indicate that the man originally held the rope in his right hand.
Late works.
He completed painting his last major work, "A Bar at the Folies-Bergère (Un Bar aux Folies-Bergère)", in 1882 and it hung in the Salon that year.
In 1875, a book-length French edition of Edgar Allan Poe's "The Raven" included lithographs by Manet and translation by Mallarmé.
In 1881, with pressure from his friend Antonin Proust, the French government awarded Manet the Légion d'honneur.
Personal life.
After the death of his father in 1862, Manet married Suzanne Leenhoff in 1863. Leenhoff was a Dutch-born piano teacher of Manet's age with whom he had been romantically involved for approximately ten years. Leenhoff initially had been employed by Manet's father, Auguste, to teach Manet and his younger brother piano. She also may have been Auguste's mistress. In 1852, Leenhoff gave birth, out of wedlock, to a son, Leon Koella Leenhoff.
Eleven-year-old Leon Leenhoff, whose father may have been either of the Manets, posed often for Manet. Most famously, he is the subject of the "Boy Carrying a Sword" of 1861 (Metropolitan Museum of Art, New York). He also appears as the boy carrying a tray in the background of "The Balcony".
Manet painted his wife in "The Reading", among other paintings.
Death.
In his forties Manet contracted syphilis, for which he received no treatment. He also suffered from rheumatism. In the years before his death, he developed locomotor ataxia, a known side-effect of syphilis, which caused him considerable pain.
In April 1883, his left foot was amputated because of gangrene, and he died eleven days later in Paris. He is buried in the Passy Cemetery in the city.
Further reading.
Short introductory works:
Longer works:

</doc>
<doc id="9616" url="http://en.wikipedia.org/wiki?curid=9616" title="Evolutionarily stable strategy">
Evolutionarily stable strategy

An evolutionarily stable strategy (ESS) is a strategy which, if adopted by a population in a given environment, cannot be invaded by any alternative strategy that is initially rare. It is relevant in game theory, behavioural ecology, and evolutionary psychology. An ESS is an equilibrium refinement of the Nash equilibrium. It is a Nash equilibrium that is "evolutionarily" stable: once it is fixed in a population, natural selection alone is sufficient to prevent alternative (mutant) strategies from invading successfully. The theory is not intended to deal with the possibility of gross external changes to the environment that bring new selective forces to bear. 
First published as a specific term in the 1972 book by John Maynard Smith, the ESS is widely used in behavioural ecology and economics, and has been used in anthropology, evolutionary psychology, philosophy, and political science.
History.
Evolutionarily stable strategies were defined and introduced by John Maynard Smith and George R. Price in a 1973 "Nature" paper. Such was the time taken in peer-reviewing the paper for "Nature" that this was preceded by a 1972 essay by Maynard Smith in a book of essays titled "On Evolution". The 1972 essay is sometimes cited instead of the 1973 paper, but university libraries are much more likely to have copies of "Nature". Papers in "Nature" are usually short; in 1974, Maynard Smith published a longer paper in the "Journal of Theoretical Biology". Maynard Smith explains further in his 1982 book "Evolution and the Theory of Games". Sometimes these are cited instead. In fact, the ESS has become so central to game theory that often no citation is given, as the reader is assumed to be familiar with it. 
Maynard Smith mathematically formalised a verbal argument made by Price, which he read while peer-reviewing Price's paper. When Maynard Smith realized that the somewhat disorganised Price was not ready to revise his article for publication, he offered to add Price as co-author.
The concept was derived from R. H. MacArthur and W. D. Hamilton's work on sex ratios, derived from Fisher's principle, especially Hamilton's (1967) concept of an unbeatable strategy. Maynard Smith was jointly awarded the 1999 Crafoord Prize for his development of the concept of evolutionarily stable strategies and the application of game theory to the evolution of behaviour.
Uses of ESS:
Motivation.
The Nash equilibrium is the traditional solution concept in game theory. It depends on the cognitive abilities of the players. It is assumed that players are aware of the structure of the game and consciously try to predict the moves of their opponents and to maximize their own payoffs. In addition, it is presumed that all the players know this (see common knowledge). These assumptions are then used to explain why players choose Nash equilibrium strategies.
Evolutionarily stable strategies are motivated entirely differently. Here, it is presumed that the players' strategies are biologically encoded and heritable. Individuals have no control over their strategy and need not be aware of the game. They reproduce and are subject to the forces of natural selection (with the payoffs of the game representing reproductive success (biological fitness)). It is imagined that alternative strategies of the game occasionally occur, via a process like mutation. To be an ESS, a strategy must be resistant to these alternatives.
Given the radically different motivating assumptions, it may come as a surprise that ESSes and Nash equilibria often coincide. In fact, every ESS corresponds to a Nash equilibrium, but some Nash equilibria are not ESSes.
Nash equilibria and ESS.
An ESS is a refined or modified form of a Nash equilibrium. (See the next section for examples which contrast the two.) In a Nash equilibrium, if all players adopt their respective parts, no player can "benefit" by switching to any alternative strategy. In a two player game, it is a strategy pair. Let E("S","T") represent the payoff for playing strategy "S" against strategy "T". The strategy pair ("S", "S") is a Nash equilibrium in a two player game if and only if this is true for both players and for all "T"≠"S":
In this definition, strategy "T" can be a neutral alternative to "S" (scoring equally well, but not better). 
A Nash equilibrium is presumed to be stable even if "T" scores equally, on the assumption that there is no long-term incentive for players to adopt "T" instead of "S". This fact represents the point of departure of the ESS.
Maynard Smith and Price specify two conditions for a strategy "S" to be an ESS. Either
for all "T"≠"S".
The first condition is sometimes called a "strict" Nash equilibrium. The second is sometimes called "Maynard Smith's second condition". The second condition means that although strategy "T" is neutral with respect to the payoff against strategy "S", the population of players who continue to play strategy "S" has an advantage when playing against "T".
There is also an alternative definition of ESS, which places a different emphasis on the role of the Nash equilibrium concept in the ESS concept. Following the terminology given in the first definition above, we have (adapted from Thomas, 1985):
for all "T"≠"S".
In this formulation, the first condition specifies that the strategy is a Nash equilibrium, and the second specifies that Maynard Smith's second condition is met. Note that the two definitions are not precisely equivalent: for example, each pure strategy in the coordination game below is an ESS by the first definition but not the second.
In words, this definition looks like this: The payoff of the first player when both players play strategy S is higher than (or equal to) the payoff of the first player when he changes to another strategy T and the second players keeps his strategy S. *AND* The payoff of the first player when only his opponent changes his strategy to T is higher than his payoff in case that both of players change their strategies to T.
This formulation more clearly highlights the role of the Nash equilibrium condition in the ESS. It also allows for a natural definition of related concepts such as a weak ESS or an evolutionarily stable set.
Examples of differences between Nash Equilibria and ESSes.
In most simple games, the ESSes and Nash equilibria coincide perfectly. For instance, in the Prisoner's Dilemma there is only one Nash equilibrium, and its strategy ("Defect") is also an ESS.
Some games may have Nash equilibria that are not ESSes. For example, in Harm thy neighbor both ("A", "A") and ("B", "B") are Nash equilibria, since players cannot do better by switching away from either. However, only "B" is an ESS (and a strong Nash). "A" is not an ESS, so "B" can neutrally invade a population of "A" strategists and predominate, because "B" scores higher against "B" than "A" does against "B". This dynamic is captured by Maynard Smith's second condition, since E("A", "A") = E("B", "A"), but it is not the case that E("A","B") > E("B","B").
Nash equilibria with equally scoring alternatives can be ESSes. For example, in the game "Harm everyone", "C" is an ESS because it satisfies Maynard Smith's second condition. "D" strategists may temporarily invade a population of "C" strategists by scoring equally well against "C", but they pay a price when they begin to play against each other; "C" scores better against "D" than does "D". So here although E("C", "C") = E("D", "C"), it is also the case that E("C","D") > E("D","D"). As a result "C" is an ESS.
Even if a game has pure strategy Nash equilibria, it might be that none of those pure strategies are ESS. Consider the Game of chicken. There are two pure strategy Nash equilibria in this game ("Swerve", "Stay") and ("Stay", "Swerve"). However, in the absence of an uncorrelated asymmetry, neither "Swerve" nor "Stay" are ESSes. There is a third Nash equilibrium, a mixed strategy which is an ESS for this game (see Hawk-dove game and Best response for explanation).
This last example points to an important difference between Nash equilibria and ESS. Nash equilibria are defined on "strategy sets" (a specification of a strategy for each player), while ESS are defined in terms of strategies themselves. The equilibria defined by ESS must always be symmetric, and thus have fewer equilibrium points.
ESS vs. Evolutionarily Stable State.
In population biology, the two concepts of an "evolutionarily stable strategy" (ESS) and an "evolutionarily stable state" are closely linked but describe different situations. 
Thomas (1984) applies the term ESS to an individual strategy which may be mixed, and evolutionarily stable population state to a population mixture of pure strategies which may be formally equivalent to the mixed ESS.
Whether a population is evolutionarily stable does not relate to its genetic diversity: it can be genetically monomorphic or polymorphic.
Stochastic ESS.
In the classic definition of an ESS, no mutant strategy can invade. In finite populations, any mutant could in principle invade, albeit at low probability, implying that no ESS can exist. In a finite population, an ESS can instead be defined as a strategy which, should it become invaded by a new mutant strategy with probability p, would be able to counterinvade from a single starting individual with probability >p.
Prisoner's dilemma and ESS.
A common model of altruism and social cooperation is the Prisoner's dilemma. Here a group of players would collectively be better off if they could play "Cooperate", but since "Defect" fares better each individual player has an incentive to play "Defect". One solution to this problem is to introduce the possibility of retaliation by having individuals play the game repeatedly against the same player. In the so-called "iterated" Prisoner's dilemma, the same two individuals play the prisoner's dilemma over and over. While the Prisoner's dilemma has only two strategies ("Cooperate" and "Defect"), the iterated Prisoner's dilemma has a huge number of possible strategies. Since an individual can have different contingency plan for each history and the game may be repeated an indefinite number of times, there may in fact be an infinite number of such contingency plans.
Three simple contingency plans which have received substantial attention are "Always Defect", "Always Cooperate", and "Tit for Tat". The first two strategies do the same thing regardless of the other player's actions, while the later responds on the next round by doing what was done to it on the previous round—it responds to "Cooperate" with "Cooperate" and "Defect" with "Defect".
If the entire population plays "Tit-for-Tat" and a mutant arises who plays "Always Defect", "Tit-for-Tat" will outperform "Always Defect". If the population of the mutant becomes too large — the percentage of the mutant will be kept small. "Tit for Tat" is therefore an ESS, "with respect to only these two strategies". On the other hand, an island of "Always Defect" players will be stable against the invasion of a few "Tit-for-Tat" players, but not against a large number of them. If we introduce "Always Cooperate", a population of "Tit-for-Tat" is no longer an ESS. Since a population of "Tit-for-Tat" players always cooperates, the strategy "Always Cooperate" behaves identically in this population. As a result, a mutant who plays "Always Cooperate" will not be eliminated. However, even though a population of "Always Cooperate" and "Tit-for-Tat" can coexist, if there is a small percentage of the population that is "Always Defect", the selective pressure is against "Always Cooperate", and in favour of "Tit-for-Tat". This is due to the lower payoffs of cooperating than those of defecting in case the opponent defects.
This demonstrates the difficulties in applying the formal definition of an ESS to games with large strategy spaces, and has motivated some to consider alternatives.
ESS and human behavior.
The fields of sociobiology and evolutionary psychology attempt to explain animal and human behavior and social structures, largely in terms of evolutionarily stable strategies. Sociopathy (chronic antisocial or criminal behavior) may be a result of a combination of two such strategies.
Evolutionarily stable strategies were originally considered for biological evolution, but they can apply to other contexts. In fact, there are stable states for a large class of adaptive dynamics. As a result, they can be used to explain human behaviours that lack any genetic influences.

</doc>
<doc id="9617" url="http://en.wikipedia.org/wiki?curid=9617" title="Element">
Element

Element or elements may refer to:

</doc>
<doc id="9619" url="http://en.wikipedia.org/wiki?curid=9619" title="Extremophile">
Extremophile

An extremophile (from Latin "extremus" meaning "extreme" and Greek "philiā" (φιλία) meaning "love") is an organism that thrives in physically or geochemically extreme conditions that are detrimental to most life on Earth. In contrast, organisms that live in more moderate environments may be termed mesophiles or neutrophiles.
Characteristics.
In the 1980s and 1990s, biologists found that microbial life has an amazing flexibility for surviving in extreme environments — niches that are extraordinarily hot, or acidic, for example — that would be completely inhospitable to complex organisms. Some scientists even concluded that life may have begun on Earth in hydrothermal vents far under the ocean's surface. According to astrophysicist Dr. Steinn Sigurdsson, "There are viable bacterial spores that have been found that are 40 million years old on Earth — and we know they're very hardened to radiation." On 6 February 2013, scientists reported that bacteria were found living in the cold and dark in a lake buried a half-mile deep under the ice in Antarctica. On 17 March 2013, researchers reported data that suggested microbial life forms thrive in the Mariana Trench, the deepest spot on the Earth. Other researchers reported related studies that microbes thrive inside rocks up to 1900 feet below the sea floor under 8500 feet of ocean off the coast of the northwestern United States. According to one of the researchers,"You can find microbes everywhere — they're extremely adaptable to conditions, and survive wherever they are."
Morphology.
Most known extremophiles are microbes. The domain Archaea contains renowned examples, but extremophiles are present in numerous and diverse genetic lineages of bacteria and archaeans. Furthermore, it is erroneous to use the term extremophile to encompass all archaeans, as some are mesophilic. Neither are all extremophiles unicellular; protostome animals found in similar environments include the Pompeii worm, the psychrophilic Grylloblattidae (insects) and Antarctic krill (a crustacean). Many would also classify tardigrades (water bears) as extremophiles but while tardigrades can survive in extreme environments, they are not considered extremophiles because they are not adapted to live in these conditions. Their chances of dying increase the longer they are exposed to the extreme environment.
Classifications.
There are many classes of extremophiles that range all around the globe, each corresponding to the way its environmental niche differs from mesophilic conditions. These classifications are not exclusive. Many extremophiles fall under multiple categories and termed as polyextremophiles. For example, organisms living inside hot rocks deep under Earth's surface are thermophilic and barophilic such as "Thermococcus barophilus". A polyextremophile living at the summit of a mountain in the Atacama Desert might be a radioresistant xerophile, a psychrophile, and an oligotroph. Polyextremophiles are well known for their ability to tolerate both high and low pH levels.
In astrobiology.
Astrobiology is the field concerned with forming theories, such as panspermia, about the distribution, nature, and future of life in the universe. In it, microbial ecologists, astronomers, planetary scientists, geochemists, philosophers, and explorers cooperate constructively to guide the search for life on other planets. Astrobiologists are particularly interested in studying extremophiles, as many organisms of this type are capable of surviving in environments similar to those known to exist on other planets. For example, Mars may have regions in its deep subsurface permafrost that could harbor endolith communities. The subsurface water ocean of Jupiter's moon Europa may harbor life, especially at hypothesized hydrothermal vents at the ocean floor.
Recent research carried out on extremophiles in Japan involved a variety of bacteria including "Escherichia coli" and "Paracoccus denitrificans" being subject to conditions of extreme gravity. The bacteria were cultivated while being rotated in an ultracentrifuge at high speeds corresponding to 403,627 g (i.e. 403,627 times the gravity experienced on Earth). "Paracoccus denitrificans" was one of the bacteria which displayed not only survival but also robust cellular growth under these conditions of hyperacceleration which are usually found only in cosmic environments, such as on very massive stars or in the shock waves of supernovas. Analysis showed that the small size of prokaryotic cells is essential for successful growth under hypergravity. The research has implications on the feasibility of panspermia.
On 26 April 2012, scientists reported that lichen survived and showed remarkable results on the adaptation capacity of photosynthetic activity within the simulation time of 34 days under Martian conditions in the Mars Simulation Laboratory (MSL) maintained by the German Aerospace Center (DLR).
On 29 April 2013, scientists in Rensselaer Polytechnic Institute, funded by NASA, reported that, during spaceflight on the International Space Station, microbes seem to adapt to the space environment in ways "not observed on Earth" and in ways that "can lead to increases in growth and virulence".
On 19 May 2014, scientists announced that numerous microbes, like "Tersicoccus phoenicis", may be resistant to methods usually used in spacecraft assembly clean rooms. It's not currently known if such resistant microbes could have withstood space travel and are present on the "Curiosity" rover now on the planet Mars.
On 20 August 2014, scientists confirmed the existence of microorganisms living half a mile below the ice of Antarctica.
On 20 August 2014, Russian cosmonauts reported finding sea plankton on "outer" window surfaces of the International Space Station and have been unable to explain how it got there.
Examples.
New sub-types of -philes are identified frequently and the sub-category list for extremophiles is always growing. For example, microbial life lives in the liquid asphalt lake, Pitch Lake. Research indicates that extremophiles inhabit the asphalt lake in populations ranging between 106 to 107 cells/gram. Likewise, until recently boron tolerance was unknown but a strong borophile was discovered in bacteria. With the recent isolation of "Bacillus boroniphilus", borophiles came into discussion. Studying these borophiles may help illuminate the mechanisms of both boron toxicity and boron deficiency.
Industrial uses.
The thermoalkaliphilic catalase, which initiates the breakdown of hydrogen peroxide into oxygen and water, was isolated from an organism, "Thermus brockianus", found in Yellowstone National Park by Idaho National Laboratory researchers. The catalase operates over a temperature range from 30°C to over 94°C and a pH range from 6-10. This catalase is extremely stable compared to other catalases at high temperatures and pH. In a comparative study, the "T. brockianus" catalase exhibited a half life of 15 days at 80°C and pH 10 while a catalase derived from "Aspergillus niger" had a half life of 15 seconds under the same conditions. The catalase will have applications for removal of hydrogen peroxide in industrial processes such as pulp and paper bleaching, textile bleaching, food pasteurization, and surface decontamination of food packaging.
DNA modifying enzymes such as "Taq" DNA polymerase and some "Bacillus" enzymes used in clinical diagnostics and starch liquefaction are produced commercially by several biotechnology companies.
DNA transfer.
Over 65 prokaryotic species are known to be naturally competent for genetic transformation, the ability to transfer DNA from one cell to another cell followed by integration of the donor DNA into the recipient cell’s chromosome. Several extremophiles are able to carry out species-specific DNA transfer, as described below. However, it is not yet clear how common such a capability is among extremophiles.
The bacterium "Deinococcus radiodurans" is one of the most radioresistant organisms known. This bacterium can also survive cold, dehydration, vacuum and acid and is thus known as a polyextremophile. "D. radiodurans" is competent to perform genetic transformation. Recipient cells are able to repair DNA damage in donor transforming DNA that had been UV irradiated as efficiently as they repair cellular DNA when the cells themselves are irradiated. The extreme thermophilic bacterium "Thermus thermophilus" and other related "Thermus" species are also capable of genetic transformation.
"Halobacterium volcanii", an extreme halophilic (saline tolerant) archaeon, is capable of natural genetic transformation. Cytoplasmic bridges are formed between cells that appear to be used for DNA transfer from one cell to another in either direction.
"Sulfolobus solfataricus" and "Sulfolobus acidocaldarius" are hyperthermophilic archaea. Exposure of these organisms to the DNA damaging agents UV irradiation, bleomycin or mitomycin C induces species-specific cellular aggregation. UV-induced cellular aggregation of "S. acidocaldarius" mediates chromosomal marker exchange with high frequency. Recombination rates exceed those of uninduced cultures by up to three orders of magnitude. Frols et al. and Ajon et al. hypothesized that cellular aggregation enhances species-specific DNA transfer between "Sulfolobus" cells in order to repair damaged DNA by means of homologous recombination. Van Wolferen et al. noted that this DNA exchange process may be crucial under DNA damaging conditions such as high temperatures. It has also been suggested that DNA transfer in "Sulfolobus" may be an early form of sexual interaction similar to the more well-studied bacterial transformation systems that involve species-specific DNA transfer leading to homologous recombinational repair of DNA damage (and see Transformation (genetics)).
Extracellular membrane vesicles (MVs) might be involved in DNA transfer between different hyperthermophilic archaeal species. It has been shown that both plasmids and viral genomes can be transferred via MVs. Notably, a horizontal plasmid transfer has been documented between hyperthermophilic "Thermococcus" and "Methanocaldococcus" species, respectively belonging to the orders "Thermococcales" and "Methanococcales".

</doc>
<doc id="9620" url="http://en.wikipedia.org/wiki?curid=9620" title="Education reform">
Education reform

Education reform is the name given to the goal of changing public education. Historically, reforms have taken different forms because the motivations of reformers have differed. In modern years, education reform desires to reform an existing system, as opposed to revolutionizing, supplanting, or providing competition to it. In the United States, therefore, education reform acknowledges and encourages public education as the primary source of K-12 education for American youth. These reformers desire to make public education more effective, with higher standards, higher achievement, and higher focus on the needs of students. Additionally, great focus has been given to closing the gap between racial and economic divides, and is perhaps one of the most important issues facing the country . While education reform has many similar goals for student achievement and parental empowerment as school choice, they differ in that the latter encourages free market competition between all the forms of education that are available and where parents are given the means to choose which is best for their children, whether it be public or private, charter or homeschooling.
The one constant for all forms of education reform includes the idea that small changes in education will have large social returns in citizen health, wealth and well-being. For example, a stated motivation has been to reduce cost to students and society. From the ancient times until the 1800s, one goal was to reduce the expense of a classical education. Ideally, classical education is undertaken with a highly educated full-time (extremely expensive) personal tutor. Historically, this was available only to the most wealthy. Encyclopedias, public libraries and grammar schools are examples of innovations intended to lower the cost of a classical education.
Related reforms attempted to develop similar classical results by concentrating on "why", and "which" questions neglected by classical education. Abstract, introspective answers to these questions can theoretically compress large amounts of facts into relatively few principles. This path was taken by some Transcendentalist educators, such as Amos Bronson Alcott.
In the early modern age, Victorian schools were reformed to teach commercially useful topics, such as modern languages and mathematics, rather than classical subjects, such as Latin and Greek.
Many reformers focused on reforming society by reforming education on more scientific, humanistic, pragmatic or democratic principles. John Dewey, and Anton Makarenko are prominent examples of such reformers. Some reformers incorporated several motivations, e.g. Maria Montessori, who both "educated for peace" (a social goal), and to "meet the needs of the child" (A humanistic goal). In historic Prussia, an important motivation for the invention of Kindergarten was to foster national unity by teaching a national language while children were young enough that learning a language was easy.
The reform has taken many forms and directions. Throughout history and the present day, the meaning and methods of education have changed through debates over what content or experiences result in an educated individual or an educated society. Changes may be implemented by individual educators and/or by broad-based school organization and/or by curriculum changes with performance evaluations.
History.
Early history.
Classical times.
Plato believed that children would never learn unless they wanted to learn. In "The Republic", he said, " . . compulsory learning never sticks in the mind."
An educational debate in the time of the Roman Empire arose after Christianity had achieved broad acceptance. The question concerned the educational value of pre-Christian classical thought: "Given that the body of knowledge of the pre-Christian Romans was heathen in origin, was it safe to teach it to Christian children?" 
Modern reforms.
Though educational reform occurred on a local level at various points throughout history, the modern notion of education reform is tied with the spread of Compulsory education - education reforms did not become widespread until after organized schooling was sufficiently systematized to be 'reformed.'
In the modern world, economic growth and the spread of democracy have raised the value of education and increased the importance of ensuring that all children and adults have access to high quality and effective education. Modern education reforms are increasingly driven by a growing understanding of what works in education and how to go about successfully improving teaching and learning in schools.
Reforms of classical education.
Western classical education as taught from the 18th to the 19th century has missing features that inspired reformers. Classical education is most concerned with answering the who, what, where, and when? questions that concern a majority of students. Unless carefully taught, group instruction naturally neglects the theoretical "why" and "which" questions that strongly concern fewer students.
Classical education in this period also did not teach local (vernacular) languages and cultures. Instead it taught high-status ancient languages (Greek and Latin) and their cultures. This produced odd social effects in which an intellectual class might be more loyal to ancient cultures and institutions than to their native vernacular languages and their actual governing authorities.
England in the 19th century.
Before there were government-funded public schools, education of the lower classes was by the charity school, pioneered in the 19th century by Protestant organizations and adapted by the Roman Catholic Church and governments. Because these schools operated on very small budgets and attempted to serve as many needy children as possible, they were designed to be inexpensive.
The basic program was to develop "grammar" schools. These taught only grammar and bookkeeping. This program permitted people to start businesses to make money, and gave them the skills to continue their education inexpensively from books. "Grammar" was the first third of the then-prevalent system of Classical Education.
The ultimate development of the grammar school was by Joseph Lancaster and Andrew Bell who developed the monitorial system. Lancaster started as a poor Quaker in early 19th century London. Bell started the Madras School of India. The monitorial system uses slightly more-advanced students to teach less-advanced students, achieving student-teacher ratios as small as 2, while educating more than a thousand students per adult. Lancaster promoted his system in a piece called that spread widely throughout the English-speaking world.
Discipline and labor in a Lancaster school were provided by an economic system. Scrip, a form of money meaningless outside the school, was created at a fixed exchange rate from a student's tuition. Every job of the school was bid-for by students in scrip, with the largest bid winning. However, "any" student tutor could auction positions in his or her classes. Besides tutoring, students could use scrip to buy food, school supplies, books, and childish luxuries in a school store. The adult supervisors were paid from the bids on jobs.
With fully developed internal economies, Lancaster schools provided a grammar-school education for a cost per student near $40 per year in 1999 U.S. dollars. The students were very clever at reducing their costs, and once invented, improvements were widely adopted in a school. For example, Lancaster students, motivated to save scrip, ultimately rented individual pages of textbooks from the school library, and read them in groups around music stands to reduce textbook costs. Students commonly exchanged tutoring, and paid for items and services with receipts from "down tutoring."
Lancaster schools usually lacked sufficient adult supervision. As a result, the older children acting as disciplinary monitors tended to become brutal task masters. Also, the schools did not teach submission to orthodox Christian beliefs or government authorities. As a result, most English-speaking countries developed mandatory publicly paid education explicitly to keep public education in "responsible" hands. These elites said that Lancaster schools might become dishonest, provide poor education and were not accountable to established authorities.
Lancaster's supporters responded that any schoolchild could avoid cheats, given the opportunity, and that the government was not paying for the education, and thus deserved no say in their composition.
Lancaster, though motivated by charity, claimed in his pamphlets to be surprised to find that he lived well on the income of his school, even while the low costs made it available to the poorest street-children.
Ironically, Lancaster lived on the charity of friends in his later life.
Progressive reforms in Europe and the United States.
The term "progressive" in education has been used somewhat indiscriminately; there are a number of kinds of educational progressivism, most of the historically significant kinds peaking in the period between the late 19th and the middle of the 20th centuries.
Child-study.
Jean-Jacques Rousseau has been called the father of the child-study movement. It has been said that Rousseau "discovered" the child (as an object of study).
Rousseau's principal work on education is "", in which he lays out an educational program for a hypothetical newborn's education to adulthood. Rousseau provided a dual critique of both the vision of education set forth in Plato's Republic and also of the society of his contemporary Europe and the educational methods he regarded as contributing to it; he held that a person can either be a man or a citizen, and that while Plato's plan could have brought the latter at the expense of the former, contemporary education failed at both tasks. He advocated a radical withdrawal of the child from society and an educational process that utilized the natural potential of the child and its curiosity, teaching it by confronting it with simulated real-life obstacles and conditioning it by experience rather than teaching it intellectually. His ideas were rarely implemented directly, but were influential on later thinkers, particularly Johann Heinrich Pestalozzi and Friedrich Wilhelm August Fröbel, the inventor of the kindergarten.
Horace Mann.
In the United States, Horace Mann (1796 – 1859) of Massachusetts used his political base and role as Secretary of the Massachusetts State Board of Education to promote public education in his home state and nationwide. His crusading style attracted wide middle class support. Historian Ellwood P. Cubberley asserts:
National identity.
Education is often seen in Europe and Asia as an important system to maintain national, cultural and linguistic unity. Prussia instituted primary school reforms expressly to teach a unified version of the national language, "Hochdeutsch". One significant reform was kindergarten, whose purpose was to have the children spend time in supervised activities in the national language, when the children were young enough that they could easily learn new language skills.
Since most modern schools copy the Prussian models, children start school at an age when their language skills remain plastic, and they find it easy to learn the national language. This was an intentional design on the part of the Prussians.
In the U.S. over the last twenty years, more than 70% of non-English-speaking school-age immigrants have arrived in the U.S. before they were 6 years old. At this age, they could have been taught English in school, and achieved a proficiency indistinguishable from a native speaker. In other countries, such as the Soviet Union, France, Spain, and Germany this approach has dramatically improved reading and math test scores for linguistic minorities.
Dewey.
John Dewey, a philosopher and educator based in Chicago and New York, helped conceptualize the role of American and international education during the first four decades of the 20th century. An important member of the American Pragmatist movement, he carried the subordination of knowledge to action into the educational world by arguing for experiential education that would enable children to learn theory and practice simultaneously; a well-known example is the practice of teaching elementary physics and biology to students while preparing a meal. He was a harsh critic of "dead" knowledge disconnected from practical human life.
Dewey criticized the rigidity and volume of humanistic education, and the emotional idealizations of education based on the child-study movement that had been inspired by Bill Joel and those who followed him. He presented his educational theories as a synthesis of the two views. His slogan was that schools should encourage children to "Learn by doing." He wanted people to realize that children are naturally active and curious. Dewey's understanding of logic is best presented in his "Logic, the Theory of Inquiry" (1938). His educational theories were presented in "My Pedagogic Creed", "The School and Society", "The Child and Curriculum", and "Democracy and Education" (1916).
The question of the history of Deweyan educational practice is a difficult one. He was a widely known and influential thinker, but his views and suggestions were often misunderstood by those who sought to apply them, leading some historians to suggest that there was never an actual implementation on any considerable scale of Deweyan progressive education. The schools with which Dewey himself was most closely associated (though the most famous, the "Laboratory School", was really run by his wife) had considerable ups and downs, and Dewey left the University of Chicago in 1904 over issues relating to the Dewey School.
Dewey's influence began to decline in the time after the Second World War and particularly in the Cold War era, as more conservative educational policies came to the fore.
The administrative progressives.
The form of educational progressivism which was most successful in having its policies implemented has been dubbed "administrative progressivism" by historians. This began to be implemented in the early 20th century. While influenced particularly in its rhetoric by Dewey and even more by his popularizers, administrative progressivism was in its practice much more influenced by the industrial revolution and the concept economies of scale.
The administrative progressives are responsible for many features of modern American education, especially American high schools: counseling programs, the move from many small local high schools to large centralized high schools, curricular differentiation in the form of electives and tracking, curricular, professional, and other forms of standardization, and an increase in state and federal regulation and bureaucracy, with a corresponding reduction of local control at the school board level. (Cf. "State, federal, and local control of education in the United States", below) (Tyack and Cuban, pp. 17–26)
These reforms have since become heavily entrenched, and many today who identify themselves as progressives are opposed to many of them, while conservative education reform during the Cold War embraced them as a framework for strengthening traditional curriculum and standards.
In more recent times, groups such as the think tank Reform's education division, and S.E.R. have attempted to pressure the government of the U.K. into more modernist educational reform, though this has met with limited success.
Critiques of progressive and classical reforms.
Many progressive reforms failed to transfer learned skills. Evidence suggests that higher-order thinking skills are unused by many people (cf. Jean Piaget, Isabel Myers, and Katharine Cook Briggs). Some authorities say that this refutes key assumptions of progressive thinkers such as Dewey.
Jean Piaget was a Swiss psychologist who studied people's developmental stages. He showed by widely reproduced experiments that most young children do not analyze or synthesize as Dewey expected. Some authorities therefore say that Dewey's reforms do not apply to the primary education of young children.
Katherine Briggs and her daughter Isabel Myers developed a psychological test that reproducibly identifies sixteen distinct human temperaments, building on work by Jung. A wide class of temperaments ("Sensors", half by category, 60% of the general population) prefer to use concrete information such as facts and procedures. They prefer not to use abstract theories or logic. In terms of education, some authorities interpret this to mean that 60% of the general population only use, and therefore would prefer to learn answers to concrete "Who, what, when, where", and "how" questions, rather than answers to the theoretical "which" and "why" questions advocated by progressives. This information was confirmed (on another research track) by Jean Piaget, who discovered that nearly 60% of adults never habitually use what he called "formal operational reasoning", a term for the development and use of theories and explicit logic. If this criticism is true, then schools that teach only "principles" would fail to educate 60% of the general population.
The data from Piaget, Myers and Briggs can also be used to criticize classical teaching styles that "never" teach theory or principle. In particular, a wide class of temperaments ("Intuitives", half by category, 40% of the general population) prefer to reason from trusted first principles, and then apply that theory to predict concrete facts. In terms of education, some authorities interpret this to mean that 40% of the general population prefer to use, and therefore want to learn, answers to theoretical "Which and "Why" questions, rather than answers to the concrete "Who, what, when, where" and "How" questions.
The synthesis resulting from this two-part critique is a "neoclassical" learning theory similar to that practiced by Marva Collins, in which both learning styles are accommodated. The classroom is filled with facts, that are organized with theories, providing a rich environment to feed children's natural preferences. To reduce the limitations of depending only on natural preferences, all children are required to learn both important facts, and important forms of reasoning.
Diane Ravitch argues that "progressive" reformers have replaced a challenging liberal arts curriculum with ever-lower standards and indoctrination, particularly in inner-city schools, thereby preventing vast numbers of students from achieving their full potential.
Late-20th century (United states).
Reforms arising from the civil rights era.
From the 1950s to the 1970s, many of the proposed and implemented reforms in U.S. education stemmed from the Civil Rights Movement and related trends; examples include ending racial segregation, and busing for the purpose of desegregation, affirmative action, and banning of school prayer.
1980s.
In the 1980s, some of the momentum of education reform moved from the left to the right, with the release of "A Nation at Risk", Ronald Reagan's efforts to reduce or eliminate the United States Department of Education. "[T]he federal government and virtually all state governments, teacher training institutions, teachers' unions, major foundations, and the mass media have all pushed strenuously for higher standards, greater accountability, more "time on task," and more impressive academic results".
This shift to the right caused many families to seek alternatives, including "charter schools, progressive schools, Montessori schools, Waldorf schools, Afrocentric schools, religious schools - or teaching them at home and in their communities."
In the latter half of the decade, E. D. Hirsch put forth an influential attack on one or more versions of progressive education, advocating an emphasis on "cultural literacy"—the facts, phrases, and texts that Hirsch asserted every American had once known and that now only some knew, but was still essential for decoding basic texts and maintaining communication. Hirsch's ideas remain significant through the 1990s and into the 21st century, and are incorporated into classroom practice through textbooks and curricula published under his own imprint.
1990s and 2000s.
Most states and districts in the 1990s adopted Outcome-Based Education (OBE) in some form or another. A state would create a committee to adopt standards, and choose a quantitative instrument to assess whether the students knew the required content or could perform the required tasks. The standards-based National Education Goals (Goals 2000) were set by the U.S. Congress in the 1990s. Many of these goals were based on the principles of outcomes-based education, and not all of the goals were attained by the year 2000 as was intended. The standards-based reform movement culminated in the No Child Left Behind Act of 2001, which as of 2009 is still an active nation-wide mandate in the United States.
OBE reforms usually had other disputed methods, such as constructivist mathematics and whole language, added onto them. Some proponents advocated replacing the traditional high school diploma with a Certificate of Initial Mastery. Other reform movements were school-to-work, which would require all students except those in a university track to spend substantial class time on a job site. See also Uncommon Schools.
Contemporary issues (United States).
Overview.
In the first decade of the 21st century, several issues are salient in debates over further education reform:
Funding levels.
According to a 2005 report from the OECD, the United States is tied for first place with Switzerland when it comes to annual spending per student on its public schools, with each of those two countries spending more than $11,000 (in U.S. currency).
Despite this high level of funding, U.S. public schools lag behind the schools of other rich countries in the areas of reading, math, and science. A further analysis of developed countries shows no correlation between per student spending and student performance, suggesting that there are other factors influencing education. Top performers include Singapore, Finland and Korea, all with relatively low spending on education, while high spenders including Norway and Luxembourg have relatively low performance. One possible factor is the distribution of the funding. In the US, schools in wealthy areas tend to be over-funded while schools in poorer areas tend to be underfunded. These differences in spending between schools or districts may accentuate inequalities, if they result in the best teachers moving to teach in the most wealthy areas. It has also been shown that the socioeconomic situation of the students family has the most influence in determining success; suggesting that even if increased funds in a low income area increase performance, they may still perform worse than their peers from wealthier districts.
Starting in the early 1980s, a series of analyses by Eric Hanushek indicated that the amount spent on schools bore little relationship to student learning. This controversial argument, which focused attention on how money was spent instead of how much was spent, led to lengthy scholarly exchanges. In part the arguments fed into the class size debates and other discussions of "input policies." It also moved reform efforts towards issues of school accountability (including No Child Left Behind) and the use of merit pay and other incentives.
There have been studies that show smaller class sizes and newer buildings (both of which require higher funding to implement) lead to academic improvements. It should also be noted that many of the reform ideas that stray from the traditional format require greater funding.
It has been shown that some school districts do not use their funds in the most productive way. For example, according to a 2007 article in the "Washington Post", the Washington, D.C. public school district spends $12,979 per student per year. This is the third highest level of funding per student out of the 100 biggest school districts in the United States. Despite this high level of funding, the school district provides outcomes that are lower than the national average. In reading and math, the district's students score the lowest among 11 major school districts—even when poor children are compared only with other poor children. 33% of poor fourth graders in the United States lack basic skills in math, but in Washington, D.C., it's 62%. According to a 2006 study by the Goldwater Institute, Arizona's public schools spend 50% more per student than Arizona's private schools. The study also says that while teachers constitute 72% of the employees at private schools, they make up less than half of the staff at public schools. According to the study, if Arizona's public schools wanted to be like private schools, they would have to hire approximately 25,000 more teachers, and eliminate 21,210 administration employees. The study also said that public school teachers are paid about 50% more than private school teachers.
In 1985 in Kansas City, Missouri, a judge ordered the school district to raise taxes and spend more money on public education. Spending was increased so much, that the school district was spending more money per student than any of the country's other 280 largest school districts. Although this very high level of spending continued for more than a decade, there was no improvement in the school district's academic performance.
According to a 1999 article, William J. Bennett, former U.S. Secretary of Education, argued that increased levels of spending on public education have not made the schools better, citing the following statistics:
Alternatives to public education.
In the United States, Private schools (independent schools) have long been an alternative to public education for those with the ability to pay tuition. These include religious schools, preparatory and boarding schools, and schools based on alternative philosophies such as Montessori education. Over 4 million students, about one in twelve children attend religious schools in the United States, most of them Christian.
Montessori pre- and primary school programs employ alternative theories of guided exploration which seek to embrace children's natural curiosity rather than, for instance, scolding them for falling out of rank.
Home education is favored by a growing number of parents who take direct responsibility for their children's education rather than enrolling them in local public schools seen as not meeting expectations.
School choice.
Economists such as Nobel laureate Milton Friedman advocate school choice to promote excellence in education through competition and choice. A competitive "market" for schools eliminates the need to otherwise attempt a workable method of accountability for results. Public education vouchers permit guardians to select and pay any school, public or private, with public funds currently allocated to local public schools. The theory is that children's guardians will naturally shop for the best schools, much as is already done at college level.
Though appealing in theory, many reforms based on school choice have led to slight to moderate improvements—which some teachers' union members see as insufficient to offset the decreased teacher pay and job security. For instance, New Zealand's landmark reform in 1989, during which schools were granted substantial autonomy, funding was devolved to schools, and parents were given a free choice of which school their children would attend, led to moderate improvements in most schools. It was argued that the associated increases in inequity and greater racial stratification in schools nullified the educational gains. Others, however, argued that the original system created more inequity (due to lower income students being required to attend poorer performing inner city schools and not being allowed school choice or better educations that are available to higher income inhabitants of suburbs). Instead, it was argued that the school choice promoted social mobility and increased test scores especially in the cases of low income students. Similar results have been found in other jurisdictions. Though discouraging, the merely slight improvements of some school choice policies often seems to reflect weaknesses in the way that choice is implemented rather than a failure of the basic principle itself.
Teacher tenure.
In October 2010 Apple Inc. CEO Steve Jobs had a consequential meeting with U.S. President Barack Obama to discuss U.S. competitiveness and the nation's education system. During the meeting Jobs recommended pursuing policies that would make it easier for school principals to hire and fire teachers based on merit.
In 2012 tenure for school teachers was challenged in a California lawsuit called "Vergara v. California". The primary issue in the case was the impact of tenure on student outcomes and on equity in education. On June 10, 2014, the trial judge ruled that California's teacher tenure statute produced disparities that " shock the conscience" and violate the equal protection clause of the California Constitution. On July 7, 2014, U.S. Secretary of Education Arne Duncan commented on the "Vergara" decision during a meeting with President Barack Obama and representatives of teacher's unions. Duncan said that tenure for school teachers "should be earned through demonstrated effectiveness" and should not be granted too quickly. Specifically, he criticized the 18-month tenure period at the heart of the "Vergara" case as being too short to be a "meaningful bar."
Barriers to reform.
A study by the Fordham Institute found that some labor agreements with teachers' unions may restrict the ability of school systems to implement merit pay and other reforms. Contracts were more restrictive in districts with high concentrations of poor and minority students. The methodology and conclusions of the study have been criticized by teachers' unions.
Another barrier to reform is assuming that schools are like businesses—when in fact they are very different.
Legal barriers to reform are low in the United States compared to other countries: State and local governance of education creates "wiggle room for educational innovators" who can change local laws or move somewhere more favourable. Cultural barriers to reform are also relatively low, because the question of who should control education is still open.
Internationally.
Taiwan.
In other parts of the world, educational reform has had a number of different meanings. In Taiwan in the 1990s and first decade of the 21st century a movement tried to prioritize reasoning over mere facts, reduce the emphasis on central control and standardized testing. There was consensus on the problems. Efforts were limited because there was little consensus on the goals of educational reforms, and therefore on how to fix the problems. By 2003, the push for education reform had declined.
Motivations.
Education reform has been pursued for a variety of specific reasons, but generally most reforms aim at redressing some societal ills, such as poverty-, gender-, or class-based inequities, or perceived ineffectiveness. Current education trends in the United States represent multiple achievement gaps across ethnicities, income levels, and geographies. As McKinsey and Company reported in a 2009 analysis, “These educational gaps impose on the United States the economic equivalent of a permanent national recession.” Reforms are usually proposed by thinkers who aim to redress societal ills or institute societal changes, most often through a change in the education of the members of a class of people—the preparation of a ruling class to rule or a working class to work, the social hygiene of a lower or immigrant class, the preparation of citizens in a democracy or republic, etc. The idea that all children should be provided with a high level of education is a relatively recent idea, and has arisen largely in the context of Western democracy in the 20th century.
The "beliefs" of school districts are optimistic that quite literally "all students will succeed", which in the context of high school graduation examination in the United States, all students in all groups, regardless of heritage or income will pass tests that in the introduction typically fall beyond the ability of all but the top 20 to 30 percent of students. The claims clearly renounce historical research that shows that all ethnic and income groups score differently on all standardized tests and standards based assessments and that students will achieve on a bell curve. Instead, education officials across the world believe that by setting clear, achievable, higher standards, aligning the curriculum, and assessing outcomes, learning can be increased for all students, and more students can succeed than the 50 percent who are defined to be above or below grade level by norm referenced standards.
States have tried to use state schools to increase state power, especially to make better soldiers and workers. This strategy was first adopted to unify related linguistic groups in Europe, including France, Germany and Italy. Exact mechanisms are unclear, but it often fails in areas where populations are culturally segregated, as when the U.S. Indian school service failed to suppress Lakota and Navaho, or when a culture has widely respected autonomous cultural institutions, as when the Spanish failed to suppress Catalan.
Many students of democracy have desired to improve education in order to improve the quality of governance in democratic societies; the necessity of good public education follows logically if one believes that the quality of democratic governance depends on the ability of citizens to make informed, intelligent choices, and that education can improve these abilities.
Politically motivated educational reforms of the democratic type are recorded as far back as Plato in "The Republic". In the United States of America, this lineage of democratic education reform was continued by Thomas Jefferson, who advocated ambitious reforms partly along Platonic lines for public schooling in Virginia.
Another motivation for reform is the desire to address socio-economic problems, which many people see as having significant roots in lack of education. Starting in the 20th century, people have attempted to argue that small improvements in education can have large returns in such areas as health, wealth and well-being. For example, in Kerala, India in the 1950s, increases in women's health were correlated with increases in female literacy rates. In Iran, increased primary education was correlated with increased farming efficiencies and income. In both cases some researchers have concluded these correlations as representing an underlying causal relationship: education causes socio-economic benefits. In the case of Iran, researchers concluded that the improvements were due to farmers gaining reliable access to national crop prices and scientific farming information.
Strategies.
Reforms can be based on bringing education into alignment with a society's core values. Reforms that attempt to change a society's core values can connect alternative education initiatives with a network of other alternative institutions.
Digital Education.
The movement to use computers more in education naturally includes many unrelated ideas, methods, and pedagogies since there are many uses for digital computers. For example, the fact that computers are naturally good at math leads to the question of the use of calculators in math education. The Internet's communication capabilities make it potentially useful for collaboration, and foreign language learning. The computer's ability to simulate physical systems makes it potentially useful in teaching science. More often, however, debate of digital education reform centers around more general applications of computers to education, such as electronic test-taking and online classes.
The idea of creating artificial intelligence led some computer scientists to believe that teachers could be replaced by computers, through something like an expert system; however, attempts to accomplish this have predictably proved inflexible. The computer is now more understood to be a tool or assistant for the teacher and students.
Harnessing the richness of the Internet is another goal. In some cases classrooms have been moved entirely online, while in other instances the goal is more to learn how the Internet can be more than a classroom.
Web-based international educational software is under development by students at New York University, based on the belief that current educational institutions are too rigid: effective teaching is not routine, students are not passive, and questions of practice are not predictable or standardized. The software allows for courses tailored to an individual's abilities through frequent and automatic multiple intelligences assessments. Ultimate goals include assisting students to be intrinsically motivated to educate themselves, and aiding the student in self-actualization. Courses typically taught only in college are being reformatted so that they can be taught to any level of student, whereby elementary school students may learn the foundations of any topic they desire. Such a program has the potential to remove the bureaucratic inefficiencies of education in modern countries, and with the decreasing digital divide, help developing nations rapidly achieve a similar quality of education. With an open format similar to Wikipedia, any teacher may upload their courses online and a feedback system will help students choose relevant courses of the highest quality. Teachers can provide links in their digital courses to webcast videos of their lectures. Students will have personal academic profiles and a forum will allow students to pose complex questions, while simpler questions will be automatically answered by the software, which will bring you to a solution by searching through the knowledge database, which includes all available courses and topics.
The 21st century ushered in the acceptance and encouragement of internet research conducted on college and university campuses, in homes, and even in gathering areas of shopping centers. Addition of cyber cafes on campuses and coffee shops, loaning of communication devices from libraries, and availability of more portable technology devices, opened up a world of educational resources. Availability of knowledge to the elite had always been obvious, yet provision of networking devices, even wireless gadget sign-outs from libraries, made availability of information an expectation of most persons. Cassandra B. Whyte researched the future of computer use on higher education campuses focusing on student affairs. Though at first seen as a data collection and outcome reporting tool, the use of computer technology in the classrooms, meeting areas, and homes continued to unfold. The sole dependence on paper resources for subject information diminished and e-books and articles, as well as on-line courses, were anticipated to become increasingly staple and affordable choices provided by higher education institutions according to Whyte in a 2002 presentation.
Digitally "flipping" classrooms is a trend in digital education that has gained significant momentum. Will Richardson, author and visionary for the digital education realm, points to the not-so-distant future and the seemingly infinite possibilities for digital communication linked to improved education. Education on the whole, as a stand-alone entity, has been slow to embrace these changes. The use of web tools such as wikis, blogs, and social networking sites is tied to increasing overall effectiveness of digital education in schools. Examples exist of teacher and student success stories where learning has transcended the classroom and has reached far out into society.
Creativity is of the utmost importance when improving education. The "creative teachers" must have the confidence through training and availability of support and resources. These creative teachers are strongly encouraged to embrace a person-centered approach that develops the psychology of the educator ahead or in conjunction with the deployment of machines. Creative teachers have been also been inspired through Crowd-Accelerated Innovation. Crowd-Accelerated Innovation has pushed people to transition between media types and their understanding thereof at record-breaking paces. This process serves as a catalyst for creative direction and new methods of innovation. Innovation without desire and drive inevitably flat lines.
Mainstream media continues to be both very influential and the medium where Crowd-Accelerated Innovation gains its leverage. Media is in direct competition with formal educational institutions in shaping the minds of today and those of tomorrow. [Buchanan, Rachel footnote] The media has been instrumental in pushing formal educational institutions to become savvier in their methods. Additionally, advertising has been (and continues to be) a vital force in shaping students and parents thought patterns.
Technology is a dynamic entity that is constantly in flux. As time presses on, new technologies will continue to break paradigms that will reshape human thinking regarding technological innovation. This concept stresses a certain disconnect between teachers and learners and the growing chasm that started some time ago. Richardson asserts that traditional classroom’s will essentially enter entropy unless teachers increase their comfort and proficiency with technology.
Administrators are not exempt from the technological disconnect. They must recognize the existence of a younger generation of teachers who were born during the Digital Age and are very comfortable with technology. However, when old meets new, especially in a mentoring situation, conflict seems inevitable. Ironically, the answer to the outdated mentor may be digital collaboration with worldwide mentor webs; composed of individuals with creative ideas for the classroom.
Another viable addition to digital education has been blended learning. In 2009, over 3 million K-12 students took an online course, compared to 2000 when 45,000 took an online course. Blended learning examples include pure online, blended, and traditional education. Research results show that the most effective learning takes place in a blended format. This allows children to view the lecture ahead of time and then spend class time practicing, refining, and applying what they have previously learned.
Notable reforms.
Some of the methods and reforms have gained permanent advocates, and are widely utilized.
Many educators now believe that anything that more precisely meets the needs of the child will work better. This was initiated by M. Montessori and is still utilized in Montessori schools.
The teaching method must be teachable! This is a lesson from both Montessori and Dewey. This view now has very wide currency, and is used to select much of the curricula of teachers' colleges.
Conservative programs are often based on classical education, which is seen by conservatives to reliably teach valuable skills in a developmentally appropriate order to the majority of Myers-Briggs temperaments, by teaching facts.
New programs based on modern learning theories that test individual learning, and teach to mastery of a subject have been proved by the Kentucky Education Reform Act (KERA) to be far more effective than group instruction with compromise schedules, or even class-size reduction.
Schools with limited resources, such as most public schools and most third-world and missionary schools, use a grammar-school approach. The evidence of Lancaster schools suggests using students as teachers. If the culture supports it, perhaps the economic discipline of the Lancaster school can reduce costs even further. However, much of the success of Lancaster's "school economy" was that the children were natives of an intensely mercantile culture.
In order to be effective, classroom instruction needs to change subjects at times near a typical student's attention span, which can be as frequently as every two minutes for young children. This is an important part of Marva Collins' method.
The Myers-Briggs temperaments fall into four broad categories, each sufficiently different to justify completely different educational theories. Many developmental psychologists say that it might be socially profitable to test for and target temperaments with special curricula.
Some of the Myers-Briggs temperaments are known to despise educational material that lacks theory. Therefore, effective curricula need to raise and answer "which" and "why" questions, to teach students with "intuitive" (Myers-Briggs) modalities.
Philosophers identify independent, logical reasoning as a precondition to most western science, engineering, economic and political theory. Therefore, every educational program that desires to improve students' outcomes in political, health and economic behavior should include a Socratically taught set of classes to teach logic and critical thinking.
Substantial resources and time can be saved by permitting students to test out of classes. This also increases motivation, directs individual study, and reduces boredom and disciplinary problems.
To support inexpensive continuing adult education a community needs a free public library. It can start modestly as shelves in an attended shop or government building, with donated books. Attendants are essential to protect the books from vandalism. Adult education repays itself many times over by providing direct opportunity to adults. Free libraries are also powerful resources for schools and businesses.
A notable reform of the education system of occurred in 1993.
The current student voice effort echoes past school reform initiatives focusing on parent involvement, community involvement, and other forms of participation in schools. However, it is finding a significant amount of success in schools because of the inherent differences: student voice is central to the daily schooling experience because students spend all day there. Many educators today strive for meaningful student involvement in their classrooms, while school administrators, school board members, and elected officials each lurch to hear what students have to say.

</doc>
<doc id="9621" url="http://en.wikipedia.org/wiki?curid=9621" title="Ellensburg, Washington">
Ellensburg, Washington

Ellensburg is a city in, and the county seat of, Kittitas County, Washington, United States. The population was 18,174 at the 2010 census. Ellensburg is located just east of the Cascade Range on Interstate 90 and is known as the most centrally located city in the state. Ellensburg is the home of Central Washington University (CWU).
The surrounding Kittitas Valley is internationally known for the timothy-hay that it produces. There are several local hay brokering and processing operations that ship to Pacific Rim countries. Downtown Ellensburg has many historic buildings, many of which were constructed in the late 19th century. This is a legacy of its bid to be the state capital, which it lost to Olympia. CWU being placed there is another product of that legacy. The state legislature selected Ellensburg as the location for the then Normal School as a consolation prize. Eastern Washington has a much drier climate than Western Washington, and some Seattle-area residents have moved to the city and commute over Snoqualmie Pass on Interstate 90 to jobs located in the Puget Sound region.
History.
Ellensburg was officially incorporated on November 26, 1883. John Alden Shoudy came to the Kittitas Valley in 1871, and purchased a small trading post from Andrew Jackson "A.J." Splawn, called "Robber's Roost." Robber's Roost was the first business in the valley, other than the early trading that occurred among American Indians, cattle drivers, trappers, and miners. Robber's Roost was located on the present-day 3rd Avenue, just west of Main Street near the alley. There is a placard on the wall commemorating the location, as well as a small stone monument against the wall on the sidewalk. Shoudy named the town after his wife, Mary Ellen Shoudy, and officially began the city of Ellensburgh around 1872. Shoudy was not the first settler in the Kittitas Valley, nor was he the first businessperson, but he was responsible for platting the city of Ellensburgh in the 1870s, and he was the person who named the streets in the downtown district.
The city was originally named Ellensburgh, until the final -h was dropped under standardization pressure from the United States Postal Service and Board of Geography Names in 1894.
There were several early newspapers in Ellensburg. The Daily Record, however, began in 1909 and is the name of the local newspaper today.
Concerns over the state of Ellensburg's historic downtown led to the formation of the Ellensburg Downtown Association to work on revitalizing the area.
Geography.
According to the United States Census Bureau, the city has a total area of 6.97 sqmi, of which 6.92 sqmi is land and 0.05 sqmi is water.
Climate.
Ellensburg experiences a semi-arid climate (Köppen "BSk").
Demographics.
2010 census.
As of the census of 2010, there were 18,174 people, 7,301 households, and 2,889 families residing in the city. The population density was 2626.3 PD/sqmi. There were 7,867 housing units at an average density of 1136.8 /sqmi. The racial makeup of the city was 85.7% White, 1.5% African American, 1.0% Native American, 3.2% Asian, 0.2% Pacific Islander, 4.6% from other races, and 3.7% from two or more races. Hispanic or Latino of any race were 9.7% of the population.
There were 7,301 households, of which 19.3% had children under the age of 18 living with them, 28.2% were married couples living together, 8.2% had a female householder with no husband present, 3.1% had a male householder with no wife present, and 60.4% were non-families. 35.1% of all households were made up of individuals and 9.6% had someone living alone who was 65 years of age or older. The average household size was 2.16 and the average family size was 2.86.
The median age in the city was 23.5 years. 14.2% of residents were under the age of 18; 41.2% were between the ages of 18 and 24; 21.8% were from 25 to 44; 13.9% were from 45 to 64; and 8.9% were 65 years of age or older. The gender makeup of the city was 50.1% male and 49.9% female.
2000 census.
As of the census of 2000, there were 15,414 people, 6,249 households, and 2,649 families residing in the city. The population density was 2,338.9 people per square mile (903.1/km²). There were 6,732 housing units at an average density of 1,021.5 per square mile (394.4/km²). The racial makeup of the city was 88.07% White, 1.17% Black or African American, 0.95% Native American, 4.09% Asian, 0.16% Pacific Islander, 2.86% from other races, and 2.69% from two or more races. 6.33% of the population were Hispanic or Latino of any race.
There were 6,249 households, of which 20.8% had children under the age of 18 living with them, 31.4% were married couples living together, 8.1% had a female householder with no husband present, and 57.6% were non-families. 35.5% of all households were made up of individuals and 9.1% had someone living alone who was 65 years of age or older. The average household size was 2.12 and the average family size was 2.84.
In the city, the population was spread out with 15.8% under the age of 18, 39.3% from 18 to 24, 22.7% from 25 to 44, 12.8% from 45 to 64, and 9.4% who were 65 years of age or older. The median age was 24 years. For every 100 females there were 95.0 males. For every 100 females age 18 and over, there were 93.1 males.
The median income for a household in the city was $20,034, and the median income for a family was $37,625. Males had a median income of $31,022 versus $22,829 for females. The per capita income for the city was $13,662. About 18.8% of families and 34.3% of the population were below the poverty line, including 29.0% of those under age 18 and 11.2% of those age 65 or over.
Politics and government.
The City of Ellensburg uses the Manager/Council form of government with a City Manager hired by the City Council. The seven-member City Council is elected at large and serve 4-year terms. The City Council elects a Mayor and Deputy Mayor from the Council to serve 2-year terms. The Council meets the first and third Monday of each month, at 7:00 pm, in the City Council Chambers at City Hall.
On the state legislative level, Ellensburg is in the 13th district. As of 2013, its state senator is Republican Janéa Holmquist Newbry, and its two state representatives are Republicans Judy Warnick and Matt Manweller. On the congressional level, Ellensburg is located in Washington's 8th congressional district and is represented by Republican Dave Reichert.
Kittitas County supported Senator John McCain of Arizona over then-Senator Barack Obama of Illinois in the 2008 presidential election, with both receiving 53 and 45 percent of the vote, respectively. Then, in the 2010 Senate race, Republican Dino Rossi carried the city over Democratic Senator Patty Murray with 50.7 percent of the vote to Murray's 49.3 percent.
Education.
Public schools.
Public schools are operated by Ellensburg School District 401. The district includes one high school (Ellensburg High School), one middle school, and three elementary schools.
Notable people.
Ellensburg produced the grunge / alternative rock band Screaming Trees, which included members:

</doc>
<doc id="9623" url="http://en.wikipedia.org/wiki?curid=9623" title="Eugene, Oregon">
Eugene, Oregon

Eugene ( ) is a city of the Pacific Northwest located in the State of Oregon. It is the second largest city in the state (after Portland) and the county seat of Lane County. It is located at the south end of the Willamette Valley, near the confluence of the McKenzie and Willamette rivers, about 50 mi east of the Oregon Coast.
As of the 2010 census, Eugene had a population of 156,185, and Lane County (co-located with the Eugene-Springfield Metropolitan Statistical Area) (MSA) had a population of 351,715. While Eugene has long been the second-largest city in Oregon, it was briefly surpassed by Salem between 2005 and 2007. The Eugene-Springfield, Oregon MSA is the 146th largest metropolitan statistical area of the U.S., and the third-largest in the state, behind the Portland Metropolitan Area and the Salem Metropolitan Area. The city's population was estimated by the Portland Research Center to be 159,580 in 2013.
Eugene is home to the University of Oregon. The city is also noted for its natural beauty, recreational opportunities (especially bicycling, running/jogging, rafting, kayaking), and focus on the arts. Eugene's slogan is "A Great City for the Arts and Outdoors". It is also referred to as the "Emerald City", and as "Track Town, USA". The Nike corporation had its beginnings in Eugene. In 2021, the city will host the 18th Track and Field World Championships.
History.
Eugene is named after its founder, Eugene Franklin Skinner. Until 1889, it was named Eugene City. In 1846, Skinner erected the first cabin in the area. It was used as a trading post and was registered as an official post office on January 8, 1850. At this time the settlement was known as Skinner's Mudhole. It was relocated in 1853 and named Eugene City, but was not formally incorporated as a city until 1862. Skinner later ran a ferry service across the Willamette River where the Ferry Street Bridge now stands.
The first major educational institution in the area was Columbia College, founded a few years earlier than the University of Oregon. It fell victim to two major fires in four years, and after the second fire, the college decided not to rebuild again. The part of south Eugene known as College Hill was the former location of Columbia College. There is no college there today.
The town raised the initial funding to start a public university, which later became the University of Oregon, with the hope of turning the small town into a center of learning. In 1872, the Legislative Assembly passed a bill creating the University of Oregon as a state institution. Eugene bested the nearby town of Albany in the competition for the state university. In 1873, community member J.H.D. Henderson donated the hilltop land for the campus, overlooking the city.
The university first opened in 1876 with the regents electing the first faculty and naming John Wesley Johnson as president. The first students registered on October 16, 1876. The first building was completed in 1877; it was named Deady Hall in honor of the first Board of Regents President and community leader Judge Matthew P. Deady. The city's name was shortened from Eugene City to Eugene in 1889.
Eugene grew rapidly throughout most of the twentieth century, with the exception being the early 1980s when a downturn in the timber industry caused high unemployment. By 1985, the industry had recovered and Eugene began to attract more high-tech industries.
Geography and climate.
Geography.
According to the United States Census Bureau, the city has a total area of 43.74 sqmi, of which, 43.72 sqmi is land and 0.02 sqmi is water. Eugene is located at an elevation of 426 ft.
To the north of downtown is Skinner Butte. Northeast of the city are the Coburg Hills. Spencer Butte is a prominent landmark south of the city. Mount Pisgah is southeast of Eugene and includes Mount Pisgah Arboretum and Howard Buford Recreation Area, a Lane County Park. Eugene is surrounded by foothills and forests to the south, east and west, while to the north the land levels out into the Willamette Valley and consists of mostly farmland.
The Willamette and McKenzie rivers run through Eugene and neighboring city, Springfield. Another important stream is Amazon Creek, whose headwaters are near Spencer Butte. The creek discharges west of the city into Fern Ridge Reservoir, maintained for winter flood control by the Army Corps of Engineers. Eugene Yacht Club hosts a sail school and sailing regattas at Fern Ridge during summer months.
Neighborhoods.
Eugene has 23 neighborhood associations:
Climate.
Like the rest of the Willamette Valley, Eugene lies in the Marine West Coast climate zone, with Mediterranean characteristics. Under the Köppen climate classification scheme, Eugene has a subtropical dry summer climate (Köppen "Csb"). Temperatures can vary from cool to warm, with warm, dry summers and cool, wet winters. Spring and fall are also moist seasons, with light rain falling for long periods. Winter snowfall does occur, but it is sporadic and rarely accumulates in large amounts: the average seasonal amount is 5 in, and the median is 0. The record snowfall was 3 ft deep due to a pineapple express in late January 1969. The record snowfall for March was 8 in deep in 2012. The hottest months are July and August, with average highs of around 82 °F, with an average of 15 days per year above 90 °F. The coolest month is December, with the average daytime high in the mid-40s°F (7–8 °C), and nights averaging just above freezing. There are 54 nights per year with a low below freezing, and about three days with highs not exceeding freezing. The record high low was 73 °F in 2006.
Eugene's average annual temperature is 52.1 °F, and annual precipitation at 50.9 in. Eugene is slightly cooler on average than Portland. Despite being located about 100 mi south and having only a slightly higher elevation, Eugene has a more continental climate, less subject to the maritime air that blows inland from the Pacific Ocean via the Columbia River. Eugene's average August low is 50.8 °F, while Portland's average August low is 56.5 °F. Average winter temperatures (and summer high temperatures) are similar for the two cities. This disparity may be additionally caused by Portland's urban heat island, where the combination of black pavement and urban energy use raises nighttime temperatures. A lesser heat island may also exist in the immediate downtown of Eugene.
Extreme temperatures range from -12 °F, recorded on December 8, 1972, to 108 °F on August 9, 1981.
Air quality and allergies.
Eugene is downwind of Willamette Valley grass seed farms, a $500 million industry. The combination of summer grass pollen and the confining shape of the hills around Eugene make it "the area of the highest grass pollen counts in the USA (>1,500 pollen grains/m3 of air)." These high pollen counts have led to difficulties for some track athletes who compete in Eugene. In the Olympic trials in 1972, "Jim Ryun won the 1,500 after being flown in by helicopter because he was allergic to Eugene's grass seed pollen." Further, six-time Olympian Maria Mutola abandoned Eugene as a training area "in part to avoid allergies".
Demographics.
2010 census.
According to the 2010 census, Eugene's population was 156,185. The population density was 3,572.2 people per square mile. There were 69,951 housing units at an average density of 1,600 per square mile. Those age 18 and over accounted for 81.8% of the total population.
The racial makeup of the city was 85.8% White, 4.0% Asian, 1.4% Black or African American, 1.0% Native American, 0.2% Pacific Islander, and 4.7% from other races.
Hispanics and Latinos of any race accounted for 7.8% of the total population. Of the non-Hispanics, 82% were White, 1.3% Black or African American, 0.8% Native American, 4% Asian, 0.2% Pacific Islander, 0.2% some other race alone, and 3.4% were of two or more races.
Females represented 51.1% of the total population, and males represented 48.9%. The median age in the city was 33.8 years.
2000 census.
The census of 2000 showed that there were 137,893 people, 58,110 households, and 31,321 families residing in the city of Eugene. The population density was 3,404.8 people per square mile (1,314.5/km²). There were 61,444 housing units at an average density of 1,516.4 per square mile (585.5/km²). The racial makeup of the city was 88.15% White, down from 99.5% in 1950, 3.57% Asian, 1.25% Black or African American, 0.93% Native American, 0.21% Pacific Islander, 2.18% from other races, and 3.72% from two or more races. 4.96% of the population were Hispanic or Latino of any race.
There were 58,110 households, of which 25.8% had children under the age of 18 living with them, 40.6% were married couples living together, 9.7% had a female householder with no husband present, and 46.1% were non-families. 31.7% of all households were made up of individuals and 9.4% had someone living alone who was 65 years of age or older. The average household size was 2.27 and the average family size was 2.87. In the city, the population was 20.3% under the age of 18, 17.3% from 18 to 24, 28.5% from 25 to 44, 21.8% from 45 to 64, and 12.1% who were 65 years of age or older. The median age was 33 years. For every 100 females, there were 96.0 males. For every 100 females age 18 and over, there were 94.0 males. The median income for a household in the city was $35,850, and the median income for a family was $48,527. Males had a median income of $35,549 versus $26,721 for females. The per capita income for the city was $21,315. About 8.7% of families and 17.1% of the population were below the poverty line, including 14.8% of those under age 18 and 7.1% of those age 65 or over.
Religion.
Religious institutions of higher learning in Eugene include Northwest Christian University and New Hope Christian College. Northwest Christian University (formerly Northwest Christian College), founded in 1895, has ties with the Christian Church (Disciples of Christ). New Hope Christian College (formerly Eugene Bible College) originated with the Bible Standard Conference in 1919, which joined with Open Bible Evangelistic Association to create Open Bible Standard Churches in 1932. Eugene Bible College was started from this movement by Fred Hornshuh in 1925.
There are two Eastern Orthodox Church parishes in Eugene: St John the Wonderworker Orthodox Christian Church in the Historic Whiteaker Neighborhood and Saint George Greek Orthodox Church.
There is a mainline Protestant contingency in the city as well—such as the largest of the Lutheran Churches, Central Lutheran near the U of O Campus and the Episcopal Church of the Resurrection.
The Eugene area has a sizeable LDS Church presence, with three stakes, consisting of 23 congregations (wards and branches). The Portland Oregon Temple is the nearest temple.
The Reconstructionist Temple Beth Israel is Eugene's largest Jewish congregation. It was also, for many decades, Eugene's only synagogue, until Orthodox members broke away in 1992 and formed "Congregation Ahavas Torah".
Eugene has a community of some 140 Sikhs, who have established a Sikh temple.
The 340-member congregation of the Unitarian Universalist Church in Eugene (UUCE) purchased the former Eugene Scottish Rite Temple in May 2010, renovated it, and began services there in September 2012.
Saraha Nyingma Buddhist Temple in Eugene opened in 2012 in the former site of the Unitarian Universalist Church.
Community.
Eugene is noted for its "community inventiveness." Many U.S. trends in community development originated here. The University of Oregon's participatory planning process, known as The Oregon Experiment, was the result of student protests in the early 1970s. The book of the same name is a major document in modern enlightenment thinking in planning and architectural circles. The process, still used by the university in modified form, was created by Christopher Alexander, whose works also directly inspired the creation of the Wiki. Some research for the book "A Pattern Language", which inspired the Design Patterns movement and Extreme Programming, was done by Alexander in Eugene. Not coincidentally, those engineering movements also had origins here. Decades after its publication, "A Pattern Language" is still one of the best-selling books on urban design.
In the 1970s, Eugene was packed with cooperative and community projects. It still has small natural food stores in many neighborhoods, some of the oldest student cooperatives in the country, and alternative schools have been part of the school district since 1971. The old Grower's Market, downtown near the Amtrak depot, is the only food cooperative in the U.S. with no employees. It is possible to see Eugene's trend-setting non-profit tendencies in much newer projects, such as the Tango Center and the Center for Appropriate Transport. In 2006, an initiative began to create a tenant-run development process for downtown Eugene.
In the fall of 2003, neighbors noticed that "an unassuming two-acre remnant orchard tucked into the Friendly Area Neighborhood" had been put up for sale by its owner, a resident of New York City. Learning that a prospective buyer had plans to build several houses on the property, they formed a nonprofit organization called Madison Meadow in June 2004 in order to buy the property and "preserve it as undeveloped space in perpetuity." In 2007 their effort was named Third Best Community Effort by the "Eugene Weekly", and by the end of 2008 they had raised enough money to purchase the property.
The City of Eugene has an active Neighborhood Program. Several neighborhoods are known for their green activism. Friendly Neighborhood has a highly popular neighborhood garden established on the right of way of a street never built. There are a number of community gardens on public property. Amazon Neighborhood has a former church turned into a community center. Whiteaker hosts a housing co-op that dates from the early 1970s that has re-purposed both their parking lots into food production and play space. An unusual eco-village with natural building techniques and large shared garden can be found in Jefferson Westside neighborhood. A several block area in the River Road Neighborhood is known as a permaculture hotspot with an increasing number of suburban homes trading grass for garden, installing rain water catchment systems, food producing landscapes and solar retrofits. Several sites have planted gardens by removing driveways. A 65-tree filbert grove on public property is being restored by citizen volunteers in cooperation with the city of Eugene. There are deepening social and economic networks in the neighborhood.
Anarchism.
Some Eugene anarchists gained international notoriety in 1999 for their perceived role in the violent protests at the WTO Conference in Seattle. Eugene resident John Zerzan, an editor of the "Green Anarchy" magazine, has been associated with the growth of the green anarchist movement and with the philosophy behind black bloc tactics of the Seattle riots. During a Reclaim the Streets event in 1999, some protesters blocked downtown streets and smashed the windows of three stores and threw stones and bottles at police. Following those protests, then-mayor Jim Torrey described the city as "the anarchist capital of the United States."
In January 2006, the FBI conducted Operation Backfire, leading to federal indictment of eleven people, including Chelsea Dawn Gerlach and Stanislas Gregory Meyerhoff. Gerlach and Meyerhoff pleaded guilty to $20 million damages in arsons committed by a Eugene-based cell of the Earth Liberation Front (ELF). Ongoing trials of accused eco-terrorists kept Eugene in the spotlight for a few years.
Economy.
Eugene's largest employers are PeaceHealth Medical Group, the University of Oregon and the Eugene School District. Eugene's largest industries are wood products manufacturing and recreational vehicle manufacturing.
Luckey's Club Cigar Store is one of the oldest bars in Oregon. Tad Luckey, Sr., purchased it in 1911, making it one of the oldest businesses in Eugene. The “Club Cigar,” as it was called in the late 19th century, was for many years a men-only salon. It survived both the Great Depression and Prohibition, partly because Eugene was a dry town before the end of Prohibition.
Corporate headquarters for the employee-owned Bi-Mart corporation and family-owned Market of Choice remain located in Eugene. Emporium Department Stores, which was founded in North Bend, Oregon, had its headquarters in Eugene, but closed all stores in 2002.
Organically Grown Company, the largest distributor of organic fruits and vegetables in the northwest, started in Eugene in 1978 as a non-profit co-op for organic farmers. Notable local food processors, many of whom manufacture certified organic products, include Golden Temple (Yogi Tea), Merry Hempsters and Springfield Creamery (Nancy's Yogurt & owned by the Kesey Family), and Mountain Rose Herbs.
Until July 2008, Hynix Semiconductor America had operated a large semiconductor plant in west Eugene. In late September 2009, Uni-Chem of South Korea announced its intention to purchase the Hynix site for solar cell manufacturing. However, this deal fell through and as of late 2012 is no longer planned.
The footwear repair product Shoe Goo is manufactured by Eclectic Products, based in Eugene.
Burley Design LLC produces bicycle trailers, and was founded in Eugene by Alan Scholz out of a Saturday Market business in 1978. Eugene is also the birthplace and home of Bike Friday bicycle manufacturer, Green Gear Cycling.
Many multinational businesses were launched in Eugene. Some of the most famous include Nike, Taco Time, and Brøderbund Software.
Top employers.
According to Eugene's 2014 Comprehensive Annual Financial Report, the city's top employers are:
Arts and culture.
Eugene has a significant population of people in pursuit of alternative ideas, and a large original hippie population. Beginning in the 1960s, the countercultural ideas and viewpoints espoused by Ken Kesey became established as the seminal elements of the vibrant social tapestry that continue to define Eugene. The Merry Prankster, as Kesey was known, has arguably left the most indelible imprint of any cultural icon in his hometown. He is best known as the author of "One Flew Over the Cuckoo's Nest" and as the male protagonist in Tom Wolfe's "The Electric Kool-Aid Acid Test".
In 2005, the city council unanimously approved a new slogan for the city, "World's Greatest City for the Arts & Outdoors." While Eugene has a vibrant arts community for a city its size, and is well situated near many outdoor opportunities, this slogan was frequently criticized by locals as embarrassing and ludicrous. In early 2010, the slogan was changed to "A Great City for the Arts & Outdoors."
Eugene's Saturday Market, open every Saturday from April through November, was founded in 1970 as the first "Saturday Market" in the United States. It is adjacent to the Lane County Farmer's Market in downtown Eugene. All vendors must create or grow all their own products. The market reappears as the "Holiday Market" between Thanksgiving and New Years in the Lane County Events Center at the fairgrounds.
Museums.
Eugene museums include the University of Oregon's Jordan Schnitzer Museum of Art and Museum of Natural and Cultural History, the Oregon Air and Space Museum, Conger Street Clock Museum, Lane County Historical Museum, Maude Kerns Art Center, Shelton McMurphey Johnson House, and the Science Factory Children's Museum & Planetarium.
Libraries.
The largest library in Oregon is the University of Oregon's Knight Library, with collections totaling more than 3 million volumes and over 100,000 audio and video items. The Eugene Public Library moved into a new, larger building downtown in 2002. The four-story library is an increase from 38000 to. There are also two branches of the Eugene Public Library, the Sheldon Branch Library in the neighborhood of Cal Young/Sheldon, and the Bethel Branch Library, in the neighborhood of Bethel. Eugene also has the Lane County Law Library.
Performing arts.
Eugene is home to numerous cultural organizations, including the Eugene Symphony, the Eugene Ballet, the Eugene Opera, the Eugene Concert Choir, the Northwest Christian University Community Choir, the Oregon Mozart Players, the Oregon Bach Festival, the Oregon Children's Choir, the Eugene Youth Symphony, Ballet Fantastique and Oregon Festival of American Music. Principal performing arts venues include the Hult Center for the Performing Arts, The John G. Shedd Institute for the Arts ("The Shedd"), Matthew Knight Arena, Beall Concert Hall and the Erb Memorial Union ballroom on the University of Oregon campus, the McDonald Theatre, and W.O.W. Hall.
A number of live theater groups are based in Eugene, including Free Shakespeare in the Park, Oregon Contemporary Theatre, The Very Little Theatre, Actors Cabaret, LCC Theatre, and University Theatre. Each has its own performance venue.
Music.
Because of its status as a college town, Eugene has been home to many music genres, musicians and bands, ranging from electronic dance music such as dubstep and drum and bass to garage rock, hip hop, folk and heavy metal. Eugene also has a growing reggae and street-performing bluegrass and jug band scene. Multi-genre act the Cherry Poppin' Daddies became a prominent figure in Eugene's music scene and became the house band at Eugene's W.O.W. Hall. In the late 1990s, their contributions to the swing revival movement propelled them to national stardom. Rock band Floater originated in Eugene. Doom metal band YOB is among the leaders of the Eugene heavy music scene.
Eugene is home to "Classical Gas" Composer and two-time Grammy award winner Mason Williams who spent his years as a youth living between his parents in Oakridge, Oregon and Oklahoma. Mason Williams puts on a yearly Christmas show at the Hult center for performing arts with a full orchestra produced by author, audio engineer and University of Oregon professor Don Latarski.
Dick Hyman, noted jazz pianist and musical director for many of Woody Allen's films, designs and hosts the annual Now Hear This! jazz festival at the Oregon Festival of American Music (OFAM). OFAM and the Hult Center routinely draw major jazz talent for concerts.
Eugene is also home to a large Zimbabwean music community. Kutsinhira Cultural Arts Center, which is "dedicated to the music and people of Zimbabwe," is based in Eugene.
Visual arts.
Eugene's visual arts community is supported by over 20 private art galleries and several organizations, including Maude Kerns Art Center, Lane Arts Council, DIVA (the Downtown Initiative for the Visual Arts), the Hult Center's Jacobs Gallery, and the Eugene Glass School.
Annual visual arts events include the Mayor's Art Show and Art and the Vineyard.
Film.
The Eugene area has been used as a filming location for several Hollywood films, most famously for 1978's "National Lampoon's Animal House", which was also filmed in nearby Cottage Grove. John Belushi had the idea for the film "The Blues Brothers" during filming of "Animal House" when he happened to meet Curtis Salgado at what was then the Eugene Hotel.
"Getting Straight", starring Elliott Gould and Candice Bergen, was filmed at Lane Community College in 1969. As the campus was still under construction at the time, the "occupation scenes" were easier to shoot.
The "Chicken Salad on Toast" scene in the 1970 Jack Nicholson movie "Five Easy Pieces" was filmed at the Denny's restaurant at the southern I-5 freeway interchange near Glenwood. Nicholson directed the 1971 film "Drive, He Said" in Eugene.
"How to Beat the High Co$t of Living", starring Jane Curtin, Jessica Lange and Susan St. James, was filmed in Eugene in the fall of 1979. Locations visible in the film include Valley River Center (which is a driving force in the plot), Skinner Butte and Ya-Po-Ah Terrace, the Willamette River and River Road Hardware.
Several track and field movies have used Eugene as a setting and/or a filming location. "Personal Best", starring Mariel Hemingway, was filmed in Eugene in 1982. The film centered on a group of women who are trying to qualify for the Olympic track and field team. Two track and field movies about the life of Steve Prefontaine, "Prefontaine" and "Without Limits" were released within a year of each other in 1997–1998. Kenny Moore, Eugene-trained Olympic runner and co-star in "Prefontaine", co-wrote the screenplay for "Without Limits". "Prefontaine" was filmed in Washington because the "Without Limits" production bought out Hayward Field for the summer to prevent its competition from shooting there. Kenny Moore also wrote a biography of Bill Bowerman, played in "Without Limits" by Donald Sutherland back in Eugene 20 years after he had appeared in "Animal House". Moore had also had a role in "Personal Best".
"Stealing Time", a 2003 independent film, was partially filmed in Eugene. When the film premiered in June 2001 at the Seattle International Film Festival, it was titled "Rennie's Landing" after a popular bar near the University of Oregon campus. The title was changed for its DVD release. "Zerophilia" was filmed in Eugene in 2006.
Sports.
Eugene's Oregon Ducks are part of the Pacific-12 Conference (Pac-12). American football is especially popular, with intense rivalries between the Ducks and both the Oregon State University Beavers and the University of Washington Huskies. Autzen Stadium is home to Duck football, with a seating capacity of 54,000.
The basketball arena, McArthur Court, was built in 1926. The arena was replaced by the Matthew Knight Arena in late 2010.
For nearly 40 years, Eugene has been the "Track and Field Capital of the World." Oregon's most famous track icon is the late world-class distance runner Steve Prefontaine, who was killed in a car crash in 1975.
Eugene's jogging trails include Pre's Trail in Alton Baker Park, Rexius Trail, the Adidas Oregon Trail, and the Ridgeline Trail. Jogging was introduced to the U.S. through Eugene, brought from New Zealand by Bill Bowerman, who wrote the best-selling book "Jogging", and coached the champion University of Oregon track and cross country teams. During Bowerman's tenure, his "Men of Oregon" won 24 individual NCAA titles, including titles in 15 out of the 19 events contested. During Bowerman's 24 years at Oregon, his track teams finished in the top ten at the NCAA championships 16 times, including four team titles (1962, '64, '65, '70), and two second-place trophies. His teams also posted a dual meet record of 114–20.
Bowerman also invented the waffle sole for running shoes in Eugene, and with Oregon alumnus Phil Knight founded shoe giant Nike. Eugene's miles of running trails, through its unusually large park system, are the most extensive in the U.S. The city has dozens of running clubs. The climate is cool and temperate, good both for jogging and record-setting. Eugene is home to the University of Oregon's Hayward Field track, which hosts numerous collegiate and amateur track and field meets throughout the year, most notably the Prefontaine Classic. Hayward Field was host to the 2004 AAU Junior Olympic Games, the 1989 World Masters Athletics Championships, the track and field events of the 1998 World Masters Games, the 2006 Pacific-10 track and field championships, the 1971, 1975, 1986, 1993, 1999, 2001, 2009, and 2011 USA Outdoor Track and Field Championships and the 1972, 1976, 1980, 2008, and 2012 U.S. Olympic trials, and is designated to host them again in 2016.
On April 16, 2015, it was announced by the IAAF that Eugene had been awarded the right to host the 2021 World Championships in Athletics. The city had previously bid for the 2019 event but lost narrowly to Doha, Qatar.
Eugene is also home to the Eugene Emeralds, a short-season Class A minor-league baseball team. The "Ems" play their home games in PK Park, also the home of the University of Oregon baseball team.
The Nationwide Tour's golfing event Oregon Classic takes place at Shadow Hills Country Club, just north of Eugene. The event has been played every year since 1998, except in 2001 when it was slated to begin the day after the 9/11 terrorist attacks. The top 20 players from the Nationwide Tour are promoted to the PGA Tour for the following year.
The Eugene Jr. Generals, a Tier III Junior "A" hockey team belonging to the Northern Pacific Hockey League (NPHL) consisting of 8 teams throughout Oregon and Washington, plays at the Lane County Ice Center.
The following table lists some sports clubs in Eugene and their usual home venue:
Parks and recreation.
Spencer Butte Park at the southern edge of town provides access to Spencer Butte, a dominant feature of Eugene's skyline. Hendricks Park, situated on a knoll to the east of downtown, is known for its rhododendron garden and nearby memorial to Steve Prefontaine, known as Pre's Rock, where the legendary University of Oregon runner was killed in an auto accident. Alton Baker Park, next to the Willamette River, contains Pre's Trail. Also located next to the Willamette are Skinner Butte Park and the Owen Memorial Rose Garden, which is home to more than 4,500 roses of over 400 varieties, as well as the 150-year-old Black Tartarian Cherry tree, an Oregon Heritage Tree.
The city of Eugene maintains an urban forest. The University of Oregon campus is an arboretum, with over 500 species of trees. The city operates and maintains scenic hiking trails that pass through and across the ridges of a cluster of hills in the southern portion of the city, on the fringe of residential neighborhoods. Some trails allow biking and others are for hikers and runners only.
The nearest ski resort, Willamette Pass, is one hour from Eugene by car. On the way, along Oregon Route 58, are several reservoirs and lakes, the Oakridge mountain bike trails, hot springs, and waterfalls within Willamette National Forest. Eugene residents also frequent Hoodoo and Mount Bachelor ski resorts. The Three Sisters Wilderness, the Oregon Dunes National Recreation Area and Smith Rock are just a short drive away.
Government.
In 1944, Eugene adopted a council-manager form of government, replacing the day-to-day management of city affairs by the part-time mayor and volunteer city council with a full-time professional city manager. The subsequent history of Eugene city government has largely been one of the dynamics—often contentious—between the city manager, the mayor and city council.
The current mayor of Eugene is Kitty Piercy, who has been in office since 2005. Recent mayors include Edwin Cone (1958–69), Les Anderson (1969–77) Gus Keller (1977–84), Brian Obie (1985–88), Jeff Miller (1989–92), Ruth Bascom (1993–96), and Jim Torrey (1997–2004).
Jon Ruiz has been the city manager since April 2008. Ten other people have held the city manager position. They were: Deane Seeger (1945–49), Oren King (1949–53), Robert Finlayson (1953–59), Hugh McKinley (1959–75), Charles Henry (1975–80), Mike Gleason (1981–96), Vicki Elmer (1996–98), Jim Johnson (1998–2002), Dennis Taylor (2002–2007), Angel Jones (2007–2008).
Eugene City Council.
Mayor: Kitty Piercy
Public safety.
The Eugene Police Department (EPD) is the city's law enforcement and public safety agency. The Lane County Sheriff's Office also has its headquarters in Eugene.
The University of Oregon is served by the University of Oregon Police Department (UOPD), and EPD has a police station in the West University District near campus. Lane Community College is served by the Lane Community College Public Safety Department. The Oregon State Police have a presence in the rural areas and highways around the Eugene metro area. The LTD downtown station, and the EmX lines are patrolled by G4S Transit Officers.
Eugene City Hall was abandoned in 2012 for reasons of structural integrity, energy efficiency, and obsolete size. Various offices of city government became tenants in eight other buildings.
Education.
Eugene is home to the University of Oregon. Other institutions of higher learning include Northwest Christian University, Lane Community College, New Hope Christian College, Gutenberg College, and Pacific University's Eugene campus.
Public schools.
The Eugene School District includes four full-service high schools (Churchill, North Eugene, Sheldon, and South Eugene) and many alternative education programs, such as international schools and charter schools. Foreign language immersion programs in the district are available in Spanish, French, and Japanese.
The Bethel School District serves children in the Bethel neighborhood on the northwest edge of Eugene. The district is home to the traditional Willamette High School and the alternative Kalapuya High School. There are 11 schools in this district.
Eugene also has several private schools, including the Eugene Waldorf School, two private Montessori schools: Eugene Montessori and Far Horizon Montessori, Eugene Sudbury School, Wellsprings Friends School, Oak Hill School, and The Little French School.
Parochial schools in Eugene include Marist Catholic High School, O'Hara Catholic Elementary School, and St. Paul Parish School.
Media.
Print.
The largest newspaper serving the area is "The Register-Guard", a daily newspaper with a circulation of about 70,000, published independently by the Baker family of Eugene. Other newspapers serving the area include the "Eugene Weekly", the "Emerald", the student-run independent newspaper at the University of Oregon, now published on Mondays and Thursdays;"The Torch", the student-run newspaper at Lane Community College, the "Ignite", the newspaper at New Hope Christian College and "The Mishpat," the student-run newspaper at Northwest Christian University. "Eugene Magazine", Lane County's Lifestyle Quarterly and "Eugene Living", Sustainable Home and Garden magazine also serves the area. "Adelante Latino" is a Spanish Newspaper in Eugene, it serves all of Lane County.
Television.
Local television stations include KMTR (NBC), KVAL (CBS), KLSR-TV (Fox), KEVU-CD, KEZI (ABC), KEPB (PBS), and KTVC (independent).
Radio.
The local NPR affiliates are KOPB, and KLCC. Radio station KRVM is an affiliate of Jefferson Public Radio, based at Southern Oregon University. The Pacifica Radio affiliate is the University of Oregon student-run radio station, KWVA. Additionally, the community supports two other radio stations: KWAX (classical) and KRVM-FM (alternative).
AM Stations
FM Stations
Infrastructure.
Transportation.
Mass transit.
Lane Transit District (LTD), a public transportation agency formed in 1970, covers 240 sqmi of Lane County, including Creswell, Cottage Grove, Junction City, Veneta, and Blue River. Operating more than 90 buses during peak hours, LTD carries riders on 3.7 million trips every year. LTD also operates a bus rapid transit line that runs between Eugene and Springfield—Emerald Express (EmX)—much of which runs in its own lane. LTD's main terminus in Eugene is at the Eugene Station. LTD also offers paratransit.
Cycling.
Cycling is popular in Eugene and many people commute via bicycle. Summertime events and festivals frequently have bike parking "corrals" that many times are filled to capacity by three hundred or more bikes. Many people commute to work by bicycle every month of the year. Numerous bike shops provide the finest rain gear products, running lights and everything a biker needs to ride and stay comfortable in heavy rain. Bike trails take commuting and recreational bikers along the Willamette River past a scenic rose garden, along Amazon Creek, through the downtown, and through the University of Oregon campus.
In 2009, the League of American Bicyclists cited Eugene as 1 of 10 "Gold-level" cities in the U.S. because of its "remarkable commitments to bicycling." In 2010, "Bicycling" magazine named Eugene the 5th most bike-friendly city in America. The U.S. Census Bureau's annual American Community Survey reported that Eugene had a bicycle commuting mode share of 7.3% in 2011, the fifth highest percentage nationwide among U.S. cities with 65,000 people or more, and 13 times higher than the national average of 0.56%.
Rail.
The 1908 Amtrak depot downtown was restored in 2004; it is the southern terminus for two daily runs of the Amtrak "Cascades", and a stop along the route in each direction for the daily "Coast Starlight".
Air travel.
Air travel is served by the Eugene Airport, also known as Mahlon Sweet Field, which is the fifth largest airport in the Northwest and second largest airport in Oregon. The Eugene Metro area also has numerous private airports. The Eugene Metro area also has several heliports, such as the Sacred Heart Medical Center Heliport and Mahlon Sweet Field Heliport, and many single helipads.
Highways.
Highways traveling within and through Eugene include:
Utilities.
Eugene is the home of Oregon's largest publicly owned water and power utility, the Eugene Water & Electric Board (EWEB). EWEB got its start in the first decade of the 20th century, after an epidemic of typhoid found in the groundwater supply. The City of Eugene condemned Eugene's private water utility and began treating river water (first the Willamette; later the McKenzie) for domestic use. EWEB got into the electric business when power was needed for the water pumps. Excess electricity generated by the EWEB's hydropower plants was used for street lighting.
Natural gas service is provided by NW Natural.
Wastewater treatment services are provided by the Metropolitan Wastewater Management Commission, a partnership between the Cities of Eugene and Springfield and Lane County.
Healthcare.
Three hospitals serve the Eugene-Springfield area. Sacred Heart Medical Center University District is the only one within Eugene city limits. McKenzie-Willamette Medical Center and Sacred Heart Medical Center at RiverBend are in Springfield. Oregon Medical Group, a primary care based multi-specialty group, operates several clinics in Eugene, as does PeaceHealth Medical Group. White Bird Clinic provides a broad range of health and human services, including low-cost clinics. The Volunteers in Medicine Clinic provides free medical and mental care to low-income adults without health insurance.
Notable people.
Notable athletes from Eugene include football players such as Todd Christensen, Quintin Mikell, Kailee Wong, Alex Brink,and Chris Miller. Basketball players have included Danny Ainge and Luke Jackson, while runners include Mary Decker, Marla Runyan, Alberto Salazar, and Steve Prefontaine. Decathlete Ashton Eaton competes for the Oregon Track Club Elite team based in Eugene.
Politicians from Eugene include U.S. Senators Wayne Morse and Paul Martin Simon, as well as Congressmen Jim Weaver and Peter DeFazio. Actors of note include Jenny Wade, Howard Hesseman and David Ogden Stiers, while Bryce Zabel chaired the Academy of Television Arts & Sciences. Author Ken Kesey also called the city home, as did Nike co-founder Phil Knight. Other past residents include Eugene Lazowski who saved 8,000 during World War II, New Orleans Saints general manager Mickey Loomis, and astronaut Stanley G. Love, among others. Later in his life L. Ron Hubbard, author and founder of The Church of Scientology, lived in Eugene. Singer-songwriter Mat Kearney is also from Eugene.
Sister cities.
Eugene has four sister cities:

</doc>
<doc id="9627" url="http://en.wikipedia.org/wiki?curid=9627" title="Elizabeth Barrett Browning">
Elizabeth Barrett Browning

Elizabeth Barrett Browning (; 6 March 1806 – 29 June 1861) was one of the most prominent English poets of the Victorian era. Her poetry was widely popular in both Britain and the United States during her lifetime.
Born in County Durham, the eldest of 12 children, Elizabeth Barrett was educated at home. She wrote poetry from around the age of six and this was compiled by her mother, comprising what is now one of the largest collections extant of juvenilia by any English writer. At 15 she became ill, suffering from intense head and spinal pain for the rest of her life, rendering her frail. She took laudanum for the pain, which may have led to a lifelong addiction and contributed to her weak health.
In the 1830s Elizabeth's cousin John Kenyon introduced her to prominent literary figures of the day such as William Wordsworth, Mary Russell Mitford, Samuel Taylor Coleridge, Alfred Tennyson and Thomas Carlyle. Her first adult collection, "The Seraphim and Other Poems", was published in 1838. During this time she contracted a disease, possibly tuberculosis, which weakened her further. Living at Wimpole Street, in London, she wrote prolifically between 1841 and 1844, producing poetry, translation and prose. She campaigned for the abolition of slavery and her work helped influence reform in the child labour legislation. Her prolific output made her a rival to Tennyson as a candidate for poet laureate on the death of Wordsworth.
Elizabeth's volume "Poems" (1844) brought her great success. During this time she met and corresponded with the writer Robert Browning, who admired her work. The courtship and marriage between the two were carried out in secret, for fear of her father's disapproval. Following the wedding she was disinherited by her father and rejected by her brothers. The couple moved to Italy in 1846, where she would live for the rest of her life. They had one son, Robert Barrett Browning, whom they called Pen. Towards the end of her life, her lung function worsened, and she died in Florence in 1861. A collection of her last poems was published by her husband shortly after her death.
Elizabeth was brought up in a strongly religious household, and much of her work carries a Christian theme. Her work had a major influence on prominent writers of the day, including the American poets Edgar Allan Poe and Emily Dickinson. She is remembered for such poems as "How Do I Love Thee?" (Sonnet 43, 1845) and "Aurora Leigh "(1856).
Life and career.
Early life.
Some of Elizabeth Barrett's family had lived in Jamaica since 1655. The main wealth of the household derived from Edward Barrett (1734–1798), landowner of 10,000 acre in Cinnamon Hill, Cornwall, Cambridge, and Oxford estates in northern Jamaica. Elizabeth's maternal grandfather owned sugar plantations, mills, glassworks and ships that traded between Jamaica and Newcastle. Biographer Julia Markus states that the poet 'believed that she had African blood through her grandfather Charles Moulton'. There is no evidence to suggest that her line of the Barrett family had any African ancestry, although other branches did, through the children of plantation owners and slaves. What the family believed to be their genealogy in relation to Jamaica is unclear.
The family wished to hand down their name as well as their wealth, stipulating that Barrett should be held as a surname. In some cases inheritance was given on condition that the name Barrett had to be used by the beneficiary. Given the strong tradition, Elizabeth used 'Elizabeth Barrett Moulton Barrett' on legal documents and before she was married often signed herself as 'Elizabeth Barrett Barrett', or 'EBB' (initials which she was able to keep after her wedding). Elizabeth's father chose to raise his family in England while his fortune grew in Jamaica. The fortune of Elizabeth's mother's line, the Graham Clarke family, also derived in part from slave labour, and was considerable.
Elizabeth Barrett Moulton-Barrett was born on 6 March 1806, in Coxhoe Hall, between the villages of Coxhoe and Kelloe in County Durham, England. Her parents were Edward Barrett Moulton Barrett and Mary Graham Clarke; Elizabeth was the eldest of their 12 children (eight boys and four girls). All the children lived to adulthood except for one girl, who died at the age of three when Elizabeth was eight. The children in her family all had nicknames: Elizabeth was "Ba" to her family. She rode her pony in the lanes around the Barrett estate, went with her brothers and sisters for walks and picnics in the countryside, visited other county families to drink tea, accepted visits in return, and participated with her brothers and sisters in homemade theatrical productions. But, unlike her two sisters and eight brothers, she immersed herself in the world of books as often as she could get away from the social rituals of her family. She was baptized in 1809 at Kelloe Parish Church, though she had already been baptized by a family friend in her first week of life.
In 1809, after the fifth child, Henrietta, was born, their father bought Hope End, a 500 acre estate near the Malvern Hills in Ledbury, Herefordshire, where Elizabeth spent her childhood. Her wealthy father converted the stately Georgian home into stables and built a new mansion of opulent Turkish design, including minarets, which his wife described as something from Arabian Nights Entertainments. The interior's brass balustrades, mahogany doors inlaid with mother-of-pearl, and finely carved fireplaces were eventually complemented by lavish landscaping: ponds, grottos, kiosks, an icehouse, a hothouse, and a subterranean passage from house to gardens. Her time at Hope End would inspire her in later life to write "Aurora Leigh".Her most ambitious work, Aurora Leigh (1857), went through more than twenty editions by 1900 but none between 1905 and 1978.
She was educated at home and attended lessons with her oldest brother, tutored by Daniel McSwiney. She began writing poetry at the age of four, a calling to which her father encouraged her. During the Hope End period, she was an intensely studious, precocious child. She writes that at age six she was reading novels, at eight she was entranced by Pope's translations of Homer, studying Greek at ten and writing her own Homeric epic "".In 1820 Mr. Barrett privately published The Battle of Marathon, an epic-style poem Browning had written around the age of twelve, though the fifty copies he printed remained within the family. Her mother compiled early efforts of the child's poetry into collections of "Poems by Elizabeth B. Barrett". Her father called her the 'Poet Laureate of Hope End' and encouraged her work. The result is one of the largest collections of juvenilia of any English writer. Also about this time, Browning injured her spine in a riding accident, and seven years later she suffered a burst blood vessel in her chest, leaving her permanently weakened. Despite frequent assertions beginning in early biographical accounts, no evidence exists to link her invalidism to any mishap in saddling or riding a horse. Sent to recover at the Gloucester spa, she was treated—in the absence of symptoms supporting another diagnosis—for a spinal problem. She went on to delight in reading Virgil in the original Latin, Shakespeare and Milton. By 1821 she had read Mary Wollstonecraft's "Vindication of the Rights of Woman" (1792), and she became a passionate supporter of Wollstonecraft's ideas. She watched her brothers go off to school knowing that there was no chance of that education for herself. The child's intellectual fascination with the classics and metaphysics was reflected in a religious intensity which she later described as "not the deep persuasion of the mild Christian but the wild visions of an enthusiast". The Barretts attended services at the nearest Dissenting chapel, and Edward was active in Bible and Missionary societies. Elizabeth was close to her siblings and had great respect for her father: she claimed that life was no fun without him, and her mother agreed. Her family's fortunes also began to suffer. Mrs. Barrett died in 1828, and in 1832 the mismanagement of Mr. Barrett's sugar plantations forced him to sell Hope End at a public auction. The family rented houses in Sidmouth, Devonshire, before settling in London in 1835. By the time Browning arrived in London, she had already developed a reputation as an emerging poetic talent.
Publication.
Barrett Browning's first known poem was written at the age of six or eight, "On the Cruelty of Forcement to Man". The manuscript, which protests against impressment, is currently in the Berg Collection of the New York Public Library; the exact date is controversial because the "2" in the date 1812 is written over something else that is scratched out. Her first independent publication was "Stanzas Excited by Reflections on the Present State of Greece" in "The New Monthly Magazine" of May 1821; this was followed in the same publication two months later by "Thoughts Awakened by Contemplating a Piece of the Palm which Grows on the Summit of the Acropolis at Athens". Her first collection of poems, "An Essay on Mind, with Other Poems," was published in 1826 and reflected her passion for Byron and Greek politics. Its publication drew the attention of a blind scholar of the Greek language, Hugh Stuart Boyd, and that of another Greek scholar, Uvedale Price, with whom she maintained a sustained scholarly correspondence. Among other neighbours was Mrs. James Martin from Colwall, with whom she also corresponded throughout her life. Later, at Boyd's suggestion, she translated Aeschylus' "Prometheus Bound" (published in 1833; retranslated in 1850). During their friendship Barrett studied Greek literature, including Homer, Pindar and Aristophanes.
At about age 15 Elizabeth began to battle with a lifelong illness, which the medical science of the time was unable to diagnose. All three sisters came down with the syndrome although it lasted only with Elizabeth. She had intense head and spinal pain with loss of mobility. Apocryphally it was told that she fell while trying to dismount a horse, or that she was creating the illness; However, there is strong evidence that she was seriously sick. The illness(es) of this time were, however, unrelated to the lung disease which she contracted in 1837. This illness caused her to be frail and weak. Mary Russell Mitford described the young Elizabeth at this time, as having "a slight, delicate figure, with a shower of dark curls falling on each side of a most expressive face; large, tender eyes, richly fringed by dark eyelashes, and a smile like a sunbeam". She began to take opiates for the pain, laudanum (an opium concoction) then morphine, commonly prescribed at the time. She would become dependent on them for much of her adulthood; the use from an early age would have contributed to her frail health. Biographers such as Alethea Hayter have suggested that this may have contributed to the wild vividness of her imagination and the poetry that it produced.
Elizabeth's mother died in 1828; Elizabeth later wrote "scarcely I was a woman when I lost my mother". The mother is buried at the Parish Church of St Michael and All Angels in Ledbury, next to her daughter Mary. Sarah Graham-Clarke, Elizabeth's aunt, helped to care for the children, and she had clashes with Elizabeth's strong will. In 1831 Elizabeth's grandmother, Elizabeth Moulton, died. The family moved three times between 1832 and 1837, first to a white Georgian building in Sidmouth, Devonshire, where they remained for three years. Later they moved to Gloucester Place in London.
Elizabeth opposed slavery and published two poems highlighting the barbarity of slavers and her support for the abolitionist cause: "The Runaway Slave at Pilgrim's Point"; and "A Curse for a Nation". In "Runaway" she describes a slave woman who is whipped, raped, and made pregnant as she curses the slavers. Elizabeth declared herself glad that the slaves were "virtually free" when the Emancipation Act abolishing slavery in British colonies was passed in 1833, despite the fact that her father believed that Abolitionism would ruin his business. The date of publication of these poems is in dispute but her position on slavery in the poems is clear and may have led to a rift between Elizabeth and her father. She wrote to John Ruskin in 1855 "I belong to a family of West Indian slaveholders, and if I believed in curses, I should be afraid". After the Jamaican slave uprising of 1831–2 her father and uncle continued to treat the slaves humanely but the family became mired in thirty-eight years of chancery litigation over the division of land and other property. Following lawsuits and the abolition of slavery Mr. Barrett incurred great financial and investment losses that forced him to sell Hope End. Although the family were never poor, the place was seized and put up for sale to satisfy creditors. Always secret in his financial dealings, he would not discuss his situation and the family was haunted by the idea that they might have to move to Jamaica. In 1838, some years after the sale of Hope End, the family settled at 50 Wimpole Street.
In London, John Kenyon, a distant cousin, introduced Elizabeth to literary figures including William Wordsworth, Mary Russell Mitford, Samuel Taylor Coleridge, Alfred Tennyson and Thomas Carlyle. Elizabeth continued to write, contributing "The Romaunt of Margaret", "The Romaunt of the Page", "The Poet's Vow" and other pieces to various periodicals. She corresponded with other writers, including Mary Russell Mitford, who would become a close friend and who would support Elizabeth's literary ambitions. In 1838 "The Seraphim and Other Poems" appeared, the first volume of Elizabeth's mature poetry to appear under her own name. During 1837–8 the poet was struck with illness again, with symptoms today suggesting tuberculous ulceration of the lungs. In 1838, at her physician's insistence, she moved from London to Torquay, on the Devonshire coast. Two tragedies then struck: in February 1840 her brother Samuel died of a fever in Jamaica and her brother Edward ("Bro"), with whom she was very close, went with her to Torquay and was drowned in a sailing accident in July. This had a serious effect on her already fragile health; when they found his body after a couple of days, she had no strength for tears or words. She felt guilty as her father had disapproved of Edward's trip to Torquay but had not forbidden the visit. She wrote to Mitford "That was a very near escape from madness, absolute hopeless madness". The family returned to Wimpole Street in 1841.
A Royal Society of Arts blue plaque now commemorates Elizabeth at 50 Wimpole Street.
Success.
At Wimpole Street Browning spent most of her time in her upstairs room. Her health began to improve, though she saw few people other than her immediate family. One of those she did see was Kenyon, a wealthy friend of the family and patron of the arts. She received comfort from her spaniel named Flush, a gift from Mary Mitford. (Virginia Woolf later fictionalised the life of the dog, making him the protagonist of her 1933 novel "").
Between 1841 and 1844 Browning was prolific in poetry, translation and prose. The poem "The Cry of the Children", published in 1842 in "Blackwoods", condemned child labour and helped bring about child labour reforms by raising support for Lord Shaftesbury's Ten Hours Bill (1844). At about the same time, she contributed critical prose pieces to Richard Henry Horne's "A New Spirit of the Age". In 1844 she published two volumes of "Poems", which included "A Drama of Exile", "A Vision of Poets", and "Lady Geraldine's Courtship" and two substantial critical essays for 1842 issues of "The Athenaeum". "Since she was not burdened with any domestic duties expected of her sisters, Browning could now devote herself entirely to the life of the mind, cultivating an enormous correspondence, reading widely". Her prolific output made her a rival to Tennyson as a candidate for poet laureate in 1850 on the death of Wordsworth.
Robert Browning and Italy.
Her 1844 volume "Poems" made her one of the most popular writers in the country at the time, and it inspired Robert Browning to write to her, telling her how much he loved her work. He had admired her poetry for a long time and wrote "I love your verses with all my heart, dear Miss Barrett" praising their "fresh strange music, the affluent language, the exquisite pathos and true new brave thought." Kenyon arranged for Robert Browning to meet Elizabeth on 20 May 1845, in her rooms, and so began one of the most famous courtships in literature. Elizabeth had produced a large amount of work and had been writing long before Robert Browning had. However, he had a great influence on her writing, as did she on his: two of Barrett’s most famous pieces were produced after she met Browning, "Sonnets from the Portuguese" and "Aurora Leigh." Robert's "Men and Women" is a product of that time. Some critics, however, point to him as an undermining influence: "Until her relationship with Robert Browning began in 1845, Barrett's willingness to engage in public discourse about social issues and about aesthetic issues in poetry, which had been so strong in her youth, gradually diminished, as did her physical health. As an intellectual presence and a physical being, she was becoming a shadow of herself." Her doctors strongly encouraged her to go to the warmer climates of Italy to avoid another English winter, but her father would not hear of it.
"My Little Portuguese" was a pet name that Browning had adopted for Elizabeth. The title of "Sonnets from the Portuguese" also refers to the series of sonnets of the 16th-century Portuguese poet Luís de Camões; in all these poems she used rhyme schemes typical of the Portuguese sonnets. The verse-novel "Aurora Leigh," her most ambitious and perhaps the most popular of her longer poems, appeared in 1856. It is the story of a female writer making her way in life, balancing work and love. The writings depicted in this novel are based on Elizabeth's own experiences. The "North American Review" praised Elizabeth's poem in these words: "Mrs. Browning's poems are, in all respects, the utterance of a woman — of a woman of great learning, rich experience, and powerful genius, uniting to her woman's nature the strength which is sometimes thought peculiar to a man."
The courtship and marriage between Robert Browning and Elizabeth were carried out secretly as she and her siblings were convinced that their father would disapprove. Six years his elder and an invalid, she could not believe that the vigorous and worldly Robert Browning really loved her as much as he professed. After a private marriage at St. Marylebone Parish Church, they honeymooned in Paris. Browning then imitated his hero Shelley by spiriting his wife off to Italy, in September 1846, which became their home almost continuously until her death. Elizabeth's loyal nurse, Wilson, who witnessed the marriage, accompanied the couple to Italy.
Mr. Barrett disinherited Elizabeth, as he did each of his children who married. Elizabeth had foreseen her father's anger but had not anticipated her brothers' rejection; they saw Browning as a lower-class gold-digger and refused to receive him socially. As Elizabeth had some money of her own, the couple were reasonably comfortable in Italy, and their relationship together was harmonious. The Brownings were well respected in Italy, and even famous. Elizabeth grew stronger and in 1849, at the age of 43, between four miscarriages, she gave birth to a son, Robert Wiedeman Barrett Browning, whom they called Pen. Their son later married but had no legitimate children.
At her husband's insistence, Elizabeth's second edition of "Poems" included her love sonnets; as a result, her popularity increased (as well as critical regard), and her artistic position was confirmed.
The couple came to know a wide circle of artists and writers including, in Italy, William Makepeace Thackeray, sculptor Harriet Hosmer (who, she wrote, seemed to be the "perfectly emancipated female") and Harriet Beecher Stowe. In 1849 she met Margaret Fuller, and the female French novelist George Sand in 1852, whom she had long admired. Among her intimate friends in Florence was the writer Isa Blagden, whom she encouraged to turn to writing novels. They met Alfred Tennyson in Paris, and John Forster, Samuel Rogers and the Carlyles in London, later befriending Charles Kingsley and John Ruskin.
Decline and Death.
After the death of an old friend, G. B. Hunter, and then of her father, Barrett Browning's health started to deteriorate once again. Her lungs failed to adequately function, and she was moved from Florence to Siena, residing at the "Villa Alberti". Deeply engrossed in Italian politics, she issued a small volume of political poems titled "Poems before Congress" (1860) "most of which were written to express her sympathy with the Italian cause after the outbreak of fighting in 1859". They caused a furore in England, and the conservative magazines "Blackwood's" and the "Saturday Review" labeled her a fanatic. She dedicated this book to her husband. Her last work was "A Musical Instrument", published posthumously.
Browning's sister Henrietta died in November 1860. The couple spent the winter of 1860–61 in Rome where Browning's health further deteriorated and they returned to Florence in early June 1861. She became gradually weaker, using morphine to ease her pain. She died on 29 June 1861 in her husband's arms. Browning said that she died "smilingly, happily, and with a face like a girl's. … Her last word was … 'Beautiful'". She was buried in the Protestant English Cemetery of Florence. "On Monday July 1 the shops in the section of the city around Casa Guidi were closed, while Elizabeth was mourned with unusual demonstrations." The nature of her illness is still unclear, although medical and literary scholars have speculated that longstanding pulmonary problems, combined with palliative opiates, contributed to her decline. Some modern scientists speculate her illness may have been hypokalemic periodic paralysis, a genetic disorder that causes weakness and many of the other symptoms she described.
Spiritual influence.
Much of Barrett Browning's work carries a religious theme. She had read and studied such famous literary works as Milton's "Paradise Lost" and Dante's "Inferno". She says in her writing, "We want the sense of the saturation of Christ's blood upon the souls of our poets, that it may cry through them in answer to the ceaseless wail of the Sphinx of our humanity, expounding agony into renovation. Something of this has been perceived in art when its glory was at the fullest. Something of a yearning after this may be seen among the Greek Christian poets, something which would have been much with a stronger faculty". She believed that "Christ's religion is essentially poetry—poetry glorified". She explored the religious aspect in many of her poems, especially in her early work, such as the sonnets. She was interested in theological debate, had learned Hebrew and read the Hebrew Bible. The poem "Aurora Leigh", for example, features religious imagery and allusion to the apocalypse.
Barrett Browning Institute.
In 1892, Ledbury, Herefordshire held a design competition to build an Institute in honour of the poet Elizabeth. Brightwen Binyon beat 44 other designs for the Institute in Ledbury. The design was based on the timber-framed Market House, which was opposite the site. It was completed in 1896, although Nikolaus Pevsner was not impressed by its style. In 1938, it became a Public Library, now Grade II-listed since 2007.
Critical reception.
<poem>
How Do I Love Thee?
How do I love thee? Let me count the ways.
I love thee to the depth and breadth and height
My soul can reach, when feeling out of sight
For the ends of being and ideal grace.
I love thee to the level of every day's
Most quiet need, by sun and candle-light.
I love thee freely, as men strive for right.
I love thee purely, as they turn from praise.
I love thee with the passion put to use
In my old griefs, and with my childhood's faith.
I love thee with a love I seemed to lose
With my lost saints. I love thee with the breath,
</poem>
”
 Sonnet XLIII from "Sonnets from the Portuguese", 1845 (published 1850)
Browning was widely popular in the U.K. and America during her lifetime. American poet Edgar Allan Poe was inspired by her poem "Lady Geraldine's Courtship" and specifically borrowed the poem's meter for his poem "The Raven". Poe had reviewed Browning's work in the January 1845 issue of the "Broadway Journal" and said that "her poetic inspiration is the highest — we can conceive of nothing more august. Her sense of Art is pure in itself." In return, she praised "The Raven" and Poe dedicated his 1845 collection "The Raven and Other Poems" to her, referring to her as "the noblest of her sex".
Her poetry greatly influenced Emily Dickinson, who admired her as a woman of achievement. Her popularity in the United States and Britain was further advanced by her stands against social injustice, including slavery in the United States, injustice toward Italian citizens by foreign rulers, and child labour.
Lilian Whiting published a biography of Browning (1899) which describes her as "the most philosophical poet" and depicts her life as "a Gospel of applied Christianity". To Whiting, the term "art for art's sake" did not apply to Barrett Browning's work for the reason that each poem, distinctively purposeful, was borne of a more "honest vision". In this critical analysis, Whiting portrays Barrett Browning as a poet who uses knowledge of Classical literature with an "intuitive gift of spiritual divination". In "Elizabeth Barrett Browning", Angela Leighton suggests that the portrayal of Barrett Browning as the "pious iconography of womanhood" has distracted us from her poetic achievements. Leighton cites the 1931 play by Rudolf Besier, "The Barretts of Wimpole Street", as evidence that 20th-century literary criticism of Barrett Browning's work has suffered more as a result of her popularity than poetic ineptitude. The play was popularized by actress Katharine Cornell, for whom it became a signature role. It was an enormous success, both artistically and commercially, and was revived several times and adapted twice into movies.
Throughout the 20th century, literary criticism of Barrett Browning's poetry remained sparse until her poems were discovered by the women's movement. She once described herself as being inclined to reject several women's rights principles, suggesting in letters to Mary Russell Mitford and her husband that she believed that there was an inferiority of intellect in women. In "Aurora Leigh", however, she created a strong and independent woman who embraces both work and love. Leighton writes that because Elizabeth participates in the literary world, where voice and diction are dominated by perceived masculine superiority, she "is defined only in mysterious opposition to everything that distinguishes the male subject who writes..." A five-volume scholarly edition of her works was published in 2010, the first in over a century.

</doc>
<doc id="9628" url="http://en.wikipedia.org/wiki?curid=9628" title="Enlil">
Enlil

Enlil (nlin), 𒂗𒇸 (EN = Lord + LÍL = Wind, "Lord (of the) Storm") is the God of breath, wind, loft and breadth (height and distance). It was the name of a chief deity listed and written about in Sumerian religion, and later in Akkadian (Assyrian and Babylonian), Hittite, Canaanite and other Mesopotamian clay and stone tablets. The name is perhaps pronounced and sometimes rendered in translations as "Ellil" in later Akkadian, Hittite, and Canaanite literature. In later Akkadian, Enlil is the son of Anshar and Kishar.
Origins.
The myth of Enlil and Ninlil discusses when Enlil was a young god, he was banished from Ekur in Nippur, home of the gods, to Kur, the underworld for seducing a goddess named Ninlil. Ninlil followed him to the underworld where she bore his first child, the moon god Sin (Sumerian Nanna/Suen). After fathering three more underworld-deities (substitutes for Sin), Enlil was allowed to return to the Ekur.
Enlil was known as the inventor of the mattock (a key agricultural pick, hoe, ax or digging tool of the Sumerians) and helped plants to grow.
Cosmological role.
Enlil, along with Anu/An, Enki and Ninhursag were gods of the Sumerians.
By his wife Ninlil or Sud, Enlil was father of the moon god Nanna/Suen (in Akkadian, Sin) and of Ninurta (also called Ningirsu). Enlil is the father of Nisaba the goddess of grain, of Pabilsag who is sometimes equated with Ninurta, and sometimes of Enbilulu. By Ereshkigal Enlil was father of Namtar.
In one myth, Enlil gives advice to his son, the god Ninurta, advising him on a strategy to slay the demon Asag. This advice is relayed to Ninurta by way of Sharur, his enchanted talking mace, which had been sent by Ninurta to the realm of the gods to seek counsel from Enlil directly.
Cultural histories.
Enlil is associated with the ancient city of Nippur, sometimes referred to as the cult city of Enlil. His temple was named Ekur, "House of the Mountain." Such was the sanctity acquired by this edifice that Babylonian and Assyrian rulers, down to the latest days, vied with one another to embellish and restore Enlil's seat of worship. Eventually, the name Ekur became the designation of a temple in general.
Grouped around the main sanctuary, there arose temples and chapels to the gods and goddesses who formed his court, so that Ekur became the name for an entire sacred precinct in the city of Nippur. The name "mountain house" suggests a lofty structure and was perhaps the designation originally of the staged tower at Nippur, built in imitation of a mountain, with the sacred shrine of the god on the top.
Enlil was also known as the god of weather. According to the Sumerians, Enlil requested the creation of a slave race, but then got tired of their noise and tried to kill them by sending a flood. A mortal known as Utnapishtim survived the flood through the help of another god, Ea, and he was made immortal by Enlil after Enlil's initial fury had subsided.
As Enlil was the only god who could reach An, the god of heaven, he held sway over the other gods who were assigned tasks by his agent and would travel to Nippur to draw in his power. He is thus seen as the model for kingship. Enlil was assimilated to the north "Pole of the Ecliptic". His sacred number name was 50.
At a very early period prior to 3000 BC, Nippur had become the centre of a political district of considerable extent. Inscriptions found at Nippur, where extensive excavations were carried on during 1888–1900 by John P. Peters and John Henry Haynes, under the auspices of the University of Pennsylvania, show that Enlil was the head of an extensive pantheon. Among the titles accorded to him are "king of lands", "king of heaven and earth", and "father of the gods".

</doc>
<doc id="9630" url="http://en.wikipedia.org/wiki?curid=9630" title="Ecology">
Ecology

Ecology (from Greek: οἶκος, "house"; -λογία, "study of"[A]) is the scientific analysis and study of interactions among organisms and their environment, such as the interactions organisms have with each other and with their abiotic environment. Topics of interest to ecologists include the diversity, distribution, amount (biomass), number (population) of organisms, as well as competition between them within and among ecosystems. Ecosystems are composed of dynamically interacting parts including organisms, the communities they make up, and the non-living components of their environment. Ecosystem processes, such as primary production, pedogenesis, nutrient cycling, and various niche construction activities, regulate the flux of energy and matter through an environment. These processes are sustained by organisms with specific life history traits, and the variety of organisms is called biodiversity. Biodiversity, which refers to the varieties of species, genes, and ecosystems, enhances certain ecosystem services.
Ecology is an interdisciplinary field that includes biology and Earth science. The word "ecology" ("Ökologie") was coined in 1866 by the German scientist Ernst Haeckel (1834–1919). Ecological thought is derivative of established currents in philosophy, particularly from ethics and politics. Ancient Greek philosophers such as Hippocrates and Aristotle laid the foundations of ecology in their studies on natural history. Modern ecology transformed into a more rigorous science in the late 19th century. Evolutionary concepts on adaptation and natural selection became cornerstones of modern ecological theory. Ecology is not synonymous with environment, environmentalism, natural history, or environmental science. It is closely related to evolutionary biology, genetics, and ethology. An understanding of how biodiversity affects ecological function is an important focus area in ecological studies. Ecologists seek to explain:
Ecology is a human science as well. There are many practical applications of ecology in conservation biology, wetland management, natural resource management (agroecology, agriculture, forestry, agroforestry, fisheries), city planning (urban ecology), community health, economics, basic and applied science, and human social interaction (human ecology). For example, the "Circles of Sustainability" approach treats ecology as more than the environment 'out there'. It is not treated as separate from humans. Organisms (including humans) and resources compose ecosystems which, in turn, maintain biophysical feedback mechanisms that moderate processes acting on living (biotic) and non-living (abiotic) components of the planet. Ecosystems sustain life-supporting functions and produce natural capital like biomass production (food, fuel, fiber and medicine), the regulation of climate, global biogeochemical cycles, water filtration, soil formation, erosion control, flood protection and many other natural features of scientific, historical, economic, or intrinsic value.
Integrative levels, scope, and scale of organization.
The scope of ecology contains a wide array of interacting levels of organization spanning micro-level (e.g., cells) to planetary scale (e.g., biosphere) phenomena. Ecosystems, for example, contain abiotic resources and interacting life forms (i.e., individual organisms that aggregate into populations which aggregate into distinct ecological communities). Ecosystems are dynamic, they do not always follow a linear successional path, but they are always changing, sometimes rapidly and sometimes so slowly that it can take thousands of years for ecological processes to bring about certain successional stages of a forest. An ecosystem's area can vary greatly, from tiny to vast. A single tree is of little consequence to the classification of a forest ecosystem, but critically relevant to organisms living in and on it. Several generations of an aphid population can exist over the lifespan of a single leaf. Each of those aphids, in turn, support diverse bacterial communities. The nature of connections in ecological communities cannot be explained by knowing the details of each species in isolation, because the emergent pattern is neither revealed nor predicted until the ecosystem is studied as an integrated whole. Some ecological principles, however, do exhibit collective properties where the sum of the components explain the properties of the whole, such as birth rates of a population being equal to the sum of individual births over a designated time frame.
Hierarchical ecology.
System behaviors must first be arrayed into different levels of organization. Behaviors corresponding to higher levels occur at slow rates. Conversely, lower organizational levels exhibit rapid rates. For example, individual tree leaves respond rapidly to momentary changes in light intensity, CO2 concentration, and the like. The growth of the tree responds more slowly and integrates these short-term changes.
O'Neill et al. (1986):76
The scale of ecological dynamics can operate like a closed system, such as aphids migrating on a single tree, while at the same time remain open with regard to broader scale influences, such as atmosphere or climate. Hence, ecologists classify ecosystems hierarchically by analyzing data collected from finer scale units, such as vegetation associations, climate, and soil types, and integrate this information to identify emergent patterns of uniform organization and processes that operate on local to regional, landscape, and chronological scales.
To structure the study of ecology into a conceptually manageable framework, the biological world is organized into a nested hierarchy, ranging in scale from genes, to cells, to tissues, to organs, to organisms, to species, to populations, to communities, to ecosystems, to biomes, and up to the level of the biosphere. This framework forms a panarchy and exhibits non-linear behaviors; this means that "effect and cause are disproportionate, so that small changes to critical variables, such as the number of nitrogen fixers, can lead to disproportionate, perhaps irreversible, changes in the system properties.":14
Biodiversity.
 Biodiversity refers to the variety of life and its processes. It includes the variety of living organisms, the genetic differences among them, the communities and ecosystems in which they occur, and the ecological and evolutionary processes that keep them functioning, yet ever changing and adapting.
Noss & Carpenter (1994):5
Biodiversity (an abbreviation of "biological diversity") describes the diversity of life from genes to ecosystems and spans every level of biological organization. The term has several interpretations, and there are many ways to index, measure, characterize, and represent its complex organization. Biodiversity includes species diversity, ecosystem diversity, and genetic diversity and scientists are interested in the way that this diversity affects the complex ecological processes operating at and among these respective levels. Biodiversity plays an important role in ecosystem services which by definition maintain and improve human quality of life. Preventing species extinctions is one way to preserve biodiversity and that goal rests on techniques that preserve genetic diversity, habitat and the ability for species to migrate. Conservation priorities and management techniques require different approaches and considerations to address the full ecological scope of biodiversity. Natural capital that supports populations is critical for maintaining ecosystem services and species migration (e.g., riverine fish runs and avian insect control) has been implicated as one mechanism by which those service losses are experienced. An understanding of biodiversity has practical applications for species and ecosystem-level conservation planners as they make management recommendations to consulting firms, governments, and industry.
Habitat.
The habitat of a species describes the environment over which a species is known to occur and the type of community that is formed as a result. More specifically, "habitats can be defined as regions in environmental space that are composed of multiple dimensions, each representing a biotic or abiotic environmental variable; that is, any component or characteristic of the environment related directly (e.g. forage biomass and quality) or indirectly (e.g. elevation) to the use of a location by the animal.":745 For example, a habitat might be an aquatic or terrestrial environment that can be further categorized as a montane or alpine ecosystem. Habitat shifts provide important evidence of competition in nature where one population changes relative to the habitats that most other individuals of the species occupy. For example, one population of a species of tropical lizards ("Tropidurus hispidus") has a flattened body relative to the main populations that live in open savanna. The population that lives in an isolated rock outcrop hides in crevasses where its flattened body offers a selective advantage. Habitat shifts also occur in the developmental life history of amphibians and in insects that transition from aquatic to terrestrial habitats. Biotope and habitat are sometimes used interchangeably, but the former applies to a community's environment, whereas the latter applies to a species' environment.
Additionally, some species are ecosystem engineers, altering the environment within a localized region. For instance, beavers manage water levels by building dams which improves their habitat in a landscape.
Niche.
Definitions of the niche date back to 1917, but G. Evelyn Hutchinson made conceptual advances in 1957 by introducing a widely adopted definition: "the set of biotic and abiotic conditions in which a species is able to persist and maintain stable population sizes.":519 The ecological niche is a central concept in the ecology of organisms and is sub-divided into the "fundamental" and the "realized" niche. The fundamental niche is the set of environmental conditions under which a species is able to persist. The realized niche is the set of environmental plus ecological conditions under which a species persists. The Hutchinsonian niche is defined more technically as a "Euclidean hyperspace whose "dimensions" are defined as environmental variables and whose "size" is a function of the number of values that the environmental values may assume for which an organism has "positive fitness".":71
Biogeographical patterns and range distributions are explained or predicted through knowledge of a species' traits and niche requirements. Species have functional traits that are uniquely adapted to the ecological niche. A trait is a measurable property, phenotype, or characteristic of an organism that may influence its survival. Genes play an important role in the interplay of development and environmental expression of traits. Resident species evolve traits that are fitted to the selection pressures of their local environment. This tends to afford them a competitive advantage and discourages similarly adapted species from having an overlapping geographic range. The competitive exclusion principle states that two species cannot coexist indefinitely by living off the same limiting resource; one will always outcompete the other. When similarly adapted species overlap geographically, closer inspection reveals subtle ecological differences in their habitat or dietary requirements. Some models and empirical studies, however, suggest that disturbances can stabilize the coevolution and shared niche occupancy of similar species inhabiting species-rich communities. The habitat plus the niche is called the ecotope, which is defined as the full range of environmental and biological variables affecting an entire species.
Niche construction.
Organisms are subject to environmental pressures, but they also modify their habitats. The regulatory feedback between organisms and their environment can affect conditions from local (e.g., a beaver pond) to global scales, over time and even after death, such as decaying logs or silica skeleton deposits from marine organisms. The process and concept of ecosystem engineering is related to niche construction, but the former relates only to the physical modifications of the habitat whereas the latter also considers the evolutionary implications of physical changes to the environment and the feedback this causes on the process of natural selection. Ecosystem engineers are defined as: "organisms that directly or indirectly modulate the availability of resources to other species, by causing physical state changes in biotic or abiotic materials. In so doing they modify, maintain and create habitats.":373
The ecosystem engineering concept has stimulated a new appreciation for the influence that organisms have on the ecosystem and evolutionary process. The term "niche construction" is more often used in reference to the under-appreciated feedback mechanisms of natural selection imparting forces on the abiotic niche. An example of natural selection through ecosystem engineering occurs in the nests of social insects, including ants, bees, wasps, and termites. There is an emergent homeostasis or homeorhesis in the structure of the nest that regulates, maintains and defends the physiology of the entire colony. Termite mounds, for example, maintain a constant internal temperature through the design of air-conditioning chimneys. The structure of the nests themselves are subject to the forces of natural selection. Moreover, a nest can survive over successive generations, so that progeny inherit both genetic material and a legacy niche that was constructed before their time.
Biome.
Biomes are larger units of organization that categorize regions of the Earth's ecosystems, mainly according to the structure and composition of vegetation. There are different methods to define the continental boundaries of biomes dominated by different functional types of vegetative communities that are limited in distribution by climate, precipitation, weather and other environmental variables. Biomes include tropical rainforest, temperate broadleaf and mixed forest, temperate deciduous forest, taiga, tundra, hot desert, and polar desert. Other researchers have recently categorized other biomes, such as the human and oceanic microbiomes. To a microbe, the human body is a habitat and a landscape. Microbiomes were discovered largely through advances in molecular genetics, which have revealed a hidden richness of microbial diversity on the planet. The oceanic microbiome plays a significant role in the ecological biogeochemistry of the planet's oceans.
Biosphere.
The largest scale of ecological organization is the biosphere: the total sum of ecosystems on the planet. Ecological relationships regulate the flux of energy, nutrients, and climate all the way up to the planetary scale. For example, the dynamic history of the planetary atmosphere's CO2 and O2 composition has been affected by the biogenic flux of gases coming from respiration and photosynthesis, with levels fluctuating over time in relation to the ecology and evolution of plants and animals. Ecological theory has also been used to explain self-emergent regulatory phenomena at the planetary scale: for example, the Gaia hypothesis is an example of holism applied in ecological theory. The Gaia hypothesis states that there is an emergent feedback loop generated by the metabolism of living organisms that maintains the core temperature of the Earth and atmospheric conditions within a narrow self-regulating range of tolerance.
Population ecology.
Population ecology studies the dynamics of specie populations and how these populations interact with the wider environment. A population consists of individuals of the same species that live, interact and migrate through the same niche and habitat.
A primary law of population ecology is the Malthusian growth model which states, "a population will grow (or decline) exponentially as long as the environment experienced by all individuals in the population remains constant.":18 Simplified population models usually start with four variables: death, birth, immigration, and emigration.
An example of an introductory population model describes a closed population, such as on an island, where immigration and emigration does not take place. Hypotheses are evaluated with reference to a null hypothesis which states that random processes create the observed data. In these island models, the rate of population change is described by:
where "N" is the total number of individuals in the population, "b" and "d" are the per capita rates of birth and death respectively, and "r" is the per capita rate of population change.
Using these modelling techniques, Malthus' population principle of growth was later transformed into a model known as the logistic equation:
where "N" is the number of individuals measured as biomass density, "a" is the maximum per-capita rate of change, and "K" is the carrying capacity of the population. The formula states that the rate of change in population size ("dN/dT") is equal to growth ("aN") that is limited by carrying capacity (1 – "N"/"K").
Population ecology builds upon these introductory models to further understand demographic processes in real study populations. Commonly used types of data include life history, fecundity, and survivorship, and these are analysed using mathematical techniques such as matrix algebra. The information is used for managing wildlife stocks and setting harvest quotas. In cases where basic models are insufficient, ecologists may adopt different kinds of statistical methods, such as the Akaike information criterion, or use models that can become mathematically complex as "several competing hypotheses are simultaneously confronted with the data."
Metapopulations and migration.
The concept of metapopulations was defined in 1969 as "a population of populations which go extinct locally and recolonize".:105 Metapopulation ecology is another statistical approach that is often used in conservation research. Metapopulation models simplify the landscape into patches of varying levels of quality, and metapopulations are linked by the migratory behaviours of organisms. Animal migration is set apart from other kinds of movement because it involves the seasonal departure and return of individuals from a habitat. Migration is also a population-level phenomenon, as with the migration routes followed by plants as they occupied northern post-glacial environments. Plant ecologists use pollen records that accumulate and stratify in wetlands to reconstruct the timing of plant migration and dispersal relative to historic and contemporary climates. These migration routes involved an expansion of the range as plant populations expanded from one area to another. There is a larger taxonomy of movement, such as commuting, foraging, territorial behaviour, stasis, and ranging. Dispersal is usually distinguished from migration because it involves the one way permanent movement of individuals from their birth population into another population.
In metapopulation terminology, migrating individuals are classed as emigrants (when they leave a region) or immigrants (when they enter a region), and sites are classed either as sources or sinks. A site is a generic term that refers to places where ecologists sample populations, such as ponds or defined sampling areas in a forest. Source patches are productive sites that generate a seasonal supply of juveniles that migrate to other patch locations. Sink patches are unproductive sites that only receive migrants; the population at the site will disappear unless rescued by an adjacent source patch or environmental conditions become more favourable. Metapopulation models examine patch dynamics over time to answer potential questions about spatial and demographic ecology. The ecology of metapopulations is a dynamic process of extinction and colonization. Small patches of lower quality (i.e., sinks) are maintained or rescued by a seasonal influx of new immigrants. A dynamic metapopulation structure evolves from year to year, where some patches are sinks in dry years and are sources when conditions are more favourable. Ecologists use a mixture of computer models and field studies to explain metapopulation structure.
Community ecology.
Community ecology examines how interactions among species and their environment affect the abundance, distribution and diversity of species within communities.
Johnson & Stinchcomb (2007):250
Community ecology is the study of the interactions among a collections of species that inhabit the same geographic area. Research in community ecology might measure primary production in a wetland in relation to decomposition and consumption rates. This requires an understanding of the community connections between plants (i.e., primary producers) and the decomposers (e.g., fungi and bacteria), or the analysis of predator-prey dynamics affecting amphibian biomass. Food webs and trophic levels are two widely employed conceptual models used to explain the linkages among species.
Ecosystem ecology.
These ecosystems, as we may call them, are of the most various kinds and sizes. They form one category of the multitudinous physical systems of the universe, which range from the universe as a whole down to the atom.
Tansley (1935):299
Ecosystems are habitats within biomes that form an integrated whole and a dynamically responsive system having both physical and biological complexes. The underlying concept can be traced back to 1864 in the published work of George Perkins Marsh ("Man and Nature"). Within an ecosystem, organisms are linked to the physical and biological components of their environment to which they are adapted. Ecosystems are complex adaptive systems where the interaction of life processes form self-organizing patterns across different scales of time and space. Ecosystems are broadly categorized as terrestrial, freshwater, atmospheric, or marine. Differences stem from the nature of the unique physical environments that shapes the biodiversity within each. A more recent addition to ecosystem ecology are technoecosystems, which are affected by or primarily the result of human activity.
Food webs.
A food web is the archetypal ecological network. Plants capture solar energy and use it to synthesize simple sugars during photosynthesis. As plants grow, they accumulate nutrients and are eaten by grazing herbivores, and the energy is transferred through a chain of organisms by consumption. The simplified linear feeding pathways that move from a basal trophic species to a top consumer is called the food chain. The larger interlocking pattern of food chains in an ecological community creates a complex food web. Food webs are a type of concept map or a heuristic device that is used to illustrate and study pathways of energy and material flows.
Food webs are often limited relative to the real world. Complete empirical measurements are generally restricted to a specific habitat, such as a cave or a pond, and principles gleaned from food web microcosm studies are extrapolated to larger systems. Feeding relations require extensive investigations into the gut contents of organisms, which can be difficult to decipher, or stable isotopes can be used to trace the flow of nutrient diets and energy through a food web. Despite these limitations, food webs remain a valuable tool in understanding community ecosystems.
Food webs exhibit principles of ecological emergence through the nature of trophic relationships: some species have many weak feeding links (e.g., omnivores) while some are more specialized with fewer stronger feeding links (e.g., primary predators). Theoretical and empirical studies identify non-random emergent patterns of few strong and many weak linkages that explain how ecological communities remain stable over time. Food webs are composed of subgroups where members in a community are linked by strong interactions, and the weak interactions occur between these subgroups. This increases food web stability. Step by step lines or relations are drawn until a web of life is illustrated.
Trophic levels.
A trophic level (from Greek "troph", τροφή, trophē, meaning "food" or "feeding") is "a group of organisms acquiring a considerable majority of its energy from the adjacent level nearer the abiotic source.":383 Links in food webs primarily connect feeding relations or trophism among species. Biodiversity within ecosystems can be organized into trophic pyramids, in which the vertical dimension represents feeding relations that become further removed from the base of the food chain up toward top predators, and the horizontal dimension represents the abundance or biomass at each level. When the relative abundance or biomass of each species is sorted into its respective trophic level, they naturally sort into a 'pyramid of numbers'.
Species are broadly categorized as autotrophs (or primary producers), heterotrophs (or consumers), and Detritivores (or decomposers). Autotrophs are organisms that produce their own food (production is greater than respiration) by photosynthesis or chemosynthesis. Heterotrophs are organisms that must feed on others for nourishment and energy (respiration exceeds production). Heterotrophs can be further sub-divided into different functional groups, including primary consumers (strict herbivores), secondary consumers (carnivorous predators that feed exclusively on herbivores) and tertiary consumers (predators that feed on a mix of herbivores and predators). Omnivores do not fit neatly into a functional category because they eat both plant and animal tissues. It has been suggested that omnivores have a greater functional influence as predators, because compared to herbivores they are relatively inefficient at grazing.
Trophic levels are part of the holistic or complex systems view of ecosystems. Each trophic level contains unrelated species that are grouped together because they share common ecological functions, giving a macroscopic view of the system. While the notion of trophic levels provides insight into energy flow and top-down control within food webs, it is troubled by the prevalence of omnivory in real ecosystems. This has led some ecologists to "reiterate that the notion that species clearly aggregate into discrete, homogeneous trophic levels is fiction.":815 Nonetheless, recent studies have shown that real trophic levels do exist, but "above the herbivore trophic level, food webs are better characterized as a tangled web of omnivores.":612
Keystone species.
A keystone species is a species that is connected to a disproportionately large number of other species in the food-web. Keystone species have lower levels of biomass in the trophic pyramid relative to the importance of their role. The many connections that a keystone species holds means that it maintains the organization and structure of entire communities. The loss of a keystone species results in a range of dramatic cascading effects that alters trophic dynamics, other food web connections, and can cause the extinction of other species.
Sea otters ("Enhydra lutris") are commonly cited as an example of a keystone species because they limit the density of sea urchins that feed on kelp. If sea otters are removed from the system, the urchins graze until the kelp beds disappear and this has a dramatic effect on community structure. Hunting of sea otters, for example, is thought to have indirectly led to the extinction of the Steller's Sea Cow ("Hydrodamalis gigas"). While the keystone species concept has been used extensively as a conservation tool, it has been criticized for being poorly defined from an operational stance. It is difficult to experimentally determine what species may hold a keystone role in each ecosystem. Furthermore, food web theory suggests that keystone species may not be common, so it is unclear how generally the keystone species model can be applied.
Ecological complexity.
Complexity is understood as a large computational effort needed to piece together numerous interacting parts exceeding the iterative memory capacity of the human mind. Global patterns of biological diversity are complex. This biocomplexity stems from the interplay among ecological processes that operate and influence patterns at different scales that grade into each other, such as transitional areas or ecotones spanning landscapes. Complexity stems from the interplay among levels of biological organization as energy and matter is integrated into larger units that superimpose onto the smaller parts. "What were wholes on one level become parts on a higher one.":209 Small scale patterns do not necessarily explain large scale phenomena, otherwise captured in the expression (coined by Aristotle) 'the sum is greater than the parts'.[E]
"Complexity in ecology is of at least six distinct types: spatial, temporal, structural, process, behavioral, and geometric.":3 From these principles, ecologists have identified emergent and self-organizing phenomena that operate at different environmental scales of influence, ranging from molecular to planetary, and these require different explanations at each integrative level. Ecological complexity relates to the dynamic resilience of ecosystems that transition to multiple shifting steady-states directed by random fluctuations of history. Long-term ecological studies provide important track records to better understand the complexity and resilience of ecosystems over longer temporal and broader spatial scales. These studies are managed by the International Long Term Ecological Network (LTER). The longest experiment in existence is the Park Grass Experiment, which was initiated in 1856. Another example is the Hubbard Brook study, which has been in operation since 1960.
Holism.
Holism remains a critical part of the theoretical foundation in contemporary ecological studies. Holism addresses the biological organization of life that self-organizes into layers of emergent whole systems that function according to nonreducible properties. This means that higher order patterns of a whole functional system, such as an ecosystem, cannot be predicted or understood by a simple summation of the parts. "New properties emerge because the components interact, not because the basic nature of the components is changed.":8
Ecological studies are necessarily holistic as opposed to reductionistic. Holism has three scientific meanings or uses that identify with ecology: 1) the mechanistic complexity of ecosystems, 2) the practical description of patterns in quantitative reductionist terms where correlations may be identified but nothing is understood about the causal relations without reference to the whole system, which leads to 3) a metaphysical hierarchy whereby the causal relations of larger systems are understood without reference to the smaller parts. Scientific holism differs from mysticism that has appropriated the same term. An example of metaphysical holism is identified in the trend of increased exterior thickness in shells of different species. The reason for a thickness increase can be understood through reference to principles of natural selection via predation without need to reference or understand the biomolecular properties of the exterior shells.
Relation to evolution.
Ecology and evolution are considered sister disciplines of the life sciences. Natural selection, life history, development, adaptation, populations, and inheritance are examples of concepts that thread equally into ecological and evolutionary theory. Morphological, behavioural and genetic traits, for example, can be mapped onto evolutionary trees to study the historical development of a species in relation to their functions and roles in different ecological circumstances. In this framework, the analytical tools of ecologists and evolutionists overlap as they organize, classify and investigate life through common systematic principals, such as phylogenetics or the Linnaean system of taxonomy. The two disciplines often appear together, such as in the title of the journal "Trends in Ecology and Evolution". There is no sharp boundary separating ecology from evolution and they differ more in their areas of applied focus. Both disciplines discover and explain emergent and unique properties and processes operating across different spatial or temporal scales of organization. While the boundary between ecology and evolution is not always clear, ecologists study the abiotic and biotic factors that influence evolutionary processes, and evolution can be rapid, occurring on ecological timescales as short as one generation.
Behavioural ecology.
All organisms can exhibit behaviours. Even plants express complex behaviour, including memory and communication. Behavioural ecology is the study of an organism's behaviour in its environment and its ecological and evolutionary implications. Ethology is the study of observable movement or behaviour in animals. This could include investigations of motile sperm of plants, mobile phytoplankton, zooplankton swimming toward the female egg, the cultivation of fungi by weevils, the mating dance of a salamander, or social gatherings of amoeba.
Adaptation is the central unifying concept in behavioural ecology. Behaviours can be recorded as traits and inherited in much the same way that eye and hair colour can. Behaviours can evolve by means of natural selection as adaptive traits conferring functional utilities that increases reproductive fitness.
Predator-prey interactions are an introductory concept into food-web studies as well as behavioural ecology. Prey species can exhibit different kinds of behavioural adaptations to predators, such as avoid, flee or defend. Many prey species are faced with multiple predators that differ in the degree of danger posed. To be adapted to their environment and face predatory threats, organisms must balance their energy budgets as they invest in different aspects of their life history, such as growth, feeding, mating, socializing, or modifying their habitat. Hypotheses posited in behavioural ecology are generally based on adaptive principles of conservation, optimization or efficiency. For example, "[t]he threat-sensitive predator avoidance hypothesis predicts that prey should assess the degree of threat posed by different predators and match their behaviour according to current levels of risk" or "[t]he optimal flight initiation distance occurs where expected postencounter fitness is maximized, which depends on the prey's initial fitness, benefits obtainable by not fleeing, energetic escape costs, and expected fitness loss due to predation risk."
Elaborate sexual displays and posturing are encountered in the behavioural ecology of animals. The birds of paradise, for example, sing and display elaborate ornaments during courtship. These displays serve a dual purpose of signalling healthy or well-adapted individuals and desirable genes. The displays are driven by sexual selection as an advertisement of quality of traits among suitors.
Cognitive ecology.
Cognitive ecology integrates theory and observations from evolutionary ecology and neurobiology, primarily cognitive science, in order to understand the effect that animal interaction with their habitat has on their cognitive systems and how those systems restrict behavior within an ecological and evolutionary framework. "Until recently, however, cognitive scientists have not paid sufficient attention to the fundamental fact that cognitive traits evolved under particular natural settings. With consideration of the selection pressure on cognition, cognitive ecology can contribute intellectual coherence to the multidisciplinary study of cognition." As a study involving the 'coupling' or interactions between organism and environment, cognitive ecology is closely related to enactivism, a field based upon the view that "...we must see the organism and environment as bound together in reciprocal specification and selection...".
Social ecology.
Social ecological behaviours are notable in the social insects, slime moulds, social spiders, human society, and naked mole-rats where eusocialism has evolved. Social behaviours include reciprocally beneficial behaviours among kin and nest mates and evolve from kin and group selection. Kin selection explains altruism through genetic relationships, whereby an altruistic behaviour leading to death is rewarded by the survival of genetic copies distributed among surviving relatives. The social insects, including ants, bees and wasps are most famously studied for this type of relationship because the male drones are clones that share the same genetic make-up as every other male in the colony. In contrast, group selectionists find examples of altruism among non-genetic relatives and explain this through selection acting on the group, whereby it becomes selectively advantageous for groups if their members express altruistic behaviours to one another. Groups with predominantly altruistic members beat groups with predominantly selfish members.
Coevolution.
Ecological interactions can be classified broadly into a host and an associate relationship. A host is any entity that harbours another that is called the associate. Relationships within a species that are mutually or reciprocally beneficial are called mutualisms. Examples of mutualism include fungus-growing ants employing agricultural symbiosis, bacteria living in the guts of insects and other organisms, the fig wasp and yucca moth pollination complex, lichens with fungi and photosynthetic algae, and corals with photosynthetic algae. If there is a physical connection between host and associate, the relationship is called symbiosis. Approximately 60% of all plants, for example, have a symbiotic relationship with arbuscular mycorrhizal fungi living in their roots forming an exchange network of carbohydrates for mineral nutrients.
Indirect mutualisms occur where the organisms live apart. For example, trees living in the equatorial regions of the planet supply oxygen into the atmosphere that sustains species living in distant polar regions of the planet. This relationship is called commensalism because many others receive the benefits of clean air at no cost or harm to trees supplying the oxygen. If the associate benefits while the host suffers, the relationship is called parasitism. Although parasites impose a cost to their host (e.g., via damage to their reproductive organs or propagules, denying the services of a beneficial partner), their net effect on host fitness is not necessarily negative and, thus, becomes difficult to forecast. Coevolution is also driven by competition among species or among members of the same species under the banner of reciprocal antagonism, such as grasses competing for growth space. The Red Queen Hypothesis, for example, posits that parasites track down and specialize on the locally common genetic defence systems of its host that drives the evolution of sexual reproduction to diversify the genetic constituency of populations responding to the antagonistic pressure.
Biogeography.
Biogeography (an amalgamation of "biology" and "geography") is the comparative study of the geographic distribution of organisms and the corresponding evolution of their traits in space and time. The "Journal of Biogeography" was established in 1974. Biogeography and ecology share many of their disciplinary roots. For example, the theory of island biogeography, published by the mathematician Robert MacArthur and ecologist Edward O. Wilson in 1967 is considered one of the fundamentals of ecological theory.
Biogeography has a long history in the natural sciences concerning the spatial distribution of plants and animals. Ecology and evolution provide the explanatory context for biogeographical studies. Biogeographical patterns result from ecological processes that influence range distributions, such as migration and dispersal. and from historical processes that split populations or species into different areas. The biogeographic processes that result in the natural splitting of species explains much of the modern distribution of the Earth's biota. The splitting of lineages in a species is called vicariance biogeography and it is a sub-discipline of biogeography. There are also practical applications in the field of biogeography concerning ecological systems and processes. For example, the range and distribution of biodiversity and invasive species responding to climate change is a serious concern and active area of research in the context of global warming.
r/K-Selection theory.
A population ecology concept is r/K selection theory,[D] one of the first predictive models in ecology used to explain life-history evolution. The premise behind the r/K selection model is that natural selection pressures change according to population density. For example, when an island is first colonized, density of individuals is low. The initial increase in population size is not limited by competition, leaving an abundance of available resources for rapid population growth. These early phases of population growth experience "density-independent" forces of natural selection, which is called "r"-selection. As the population becomes more crowded, it approaches the island's carrying capacity, thus forcing individuals to compete more heavily for fewer available resources. Under crowded conditions, the population experiences density-dependent forces of natural selection, called "K"-selection.
In the "r/K"-selection model, the first variable "r" is the intrinsic rate of natural increase in population size and the second variable "K" is the carrying capacity of a population. Different species evolve different life-history strategies spanning a continuum between these two selective forces. An "r"-selected species is one that has high birth rates, low levels of parental investment, and high rates of mortality before individuals reach maturity. Evolution favours high rates of fecundity in "r"-selected species. Many kinds of insects and invasive species exhibit "r"-selected characteristics. In contrast, a "K"-selected species has low rates of fecundity, high levels of parental investment in the young, and low rates of mortality as individuals mature. Humans and elephants are examples of species exhibiting "K"-selected characteristics, including longevity and efficiency in the conversion of more resources into fewer offspring.
Molecular ecology.
The important relationship between ecology and genetic inheritance predates modern techniques for molecular analysis. Molecular ecological research became more feasible with the development of rapid and accessible genetic technologies, such as the polymerase chain reaction (PCR). The rise of molecular technologies and influx of research questions into this new ecological field resulted in the publication "Molecular Ecology" in 1992. Molecular ecology uses various analytical techniques to study genes in an evolutionary and ecological context. In 1994, John Avise also played a leading role in this area of science with the publication of his book, "Molecular Markers, Natural History and Evolution". Newer technologies opened a wave of genetic analysis into organisms once difficult to study from an ecological or evolutionary standpoint, such as bacteria, fungi and nematodes. Molecular ecology engendered a new research paradigm for investigating ecological questions considered otherwise intractable. Molecular investigations revealed previously obscured details in the tiny intricacies of nature and improved resolution into probing questions about behavioural and biogeographical ecology. For example, molecular ecology revealed promiscuous sexual behaviour and multiple male partners in tree swallows previously thought to be socially monogamous. In a biogeographical context, the marriage between genetics, ecology and evolution resulted in a new sub-discipline called phylogeography.
Human ecology.
The history of life on Earth has been a history of interaction between living things and their surroundings. To a large extent, the physical form and the habits of the earth's vegetation and its animal life have been molded by the environment. Considering the whole span of earthly time, the opposite effect, in which life actually modifies its surroundings, has been relatively slight. Only within the moment of time represented by the present century has one species man acquired significant power to alter the nature of his world.
Rachel Carson, "Silent Spring"
Ecology is as much a biological science as it is a human science. Human ecology is an interdisciplinary investigation into the ecology of our species. "Human ecology may be defined: (1) from a bio-ecological standpoint as the study of man as the ecological dominant in plant and animal communities and systems; (2) from a bio-ecological standpoint as simply another animal affecting and being affected by his physical environment; and (3) as a human being, somehow different from animal life in general, interacting with physical and modified environments in a distinctive and creative way. A truly interdisciplinary human ecology will most likely address itself to all three.":3 The term was formally introduced in 1921, but many sociologists, geographers, psychologists, and other disciplines were interested in human relations to natural systems centuries prior, especially in the late 19th century.
The ecological complexities human beings are facing through the technological transformation of the planetary biome has brought on the Anthropocene. The unique set of circumstances has generated the need for a new unifying science called coupled human and natural systems that builds upon, but moves beyond the field of human ecology. Ecosystems tie into human societies through the critical and all encompassing life-supporting functions they sustain. In recognition of these functions and the incapability of traditional economic valuation methods to see the value in ecosystems, there has been a surge of interest in social-natural capital, which provides the means to put a value on the stock and use of information and materials stemming from ecosystem goods and services. Ecosystems produce, regulate, maintain, and supply services of critical necessity and beneficial to human health (cognitive and physiological), economies, and they even provide an information or reference function as a living library giving opportunities for science and cognitive development in children engaged in the complexity of the natural world. Ecosystems relate importantly to human ecology as they are the ultimate base foundation of global economics as every commodity and the capacity for exchange ultimately stems from the ecosystems on Earth.
Restoration and management.
Ecosystem management is not just about science nor is it simply an extension of traditional resource management; it offers a fundamental reframing of how humans may work with nature.
Grumbine (1994):27
Ecology is an employed science of restoration, repairing disturbed sites through human intervention, in natural resource management, and in environmental impact assessments. Edward O. Wilson predicted in 1992 that the 21st century "will be the era of restoration in ecology". Ecological science has boomed in the industrial investment of restoring ecosystems and their processes in abandoned sites after disturbance. Natural resource managers, in forestry, for example, employ ecologists to develop, adapt, and implement ecosystem based methods into the planning, operation, and restoration phases of land-use. Ecological science is used in the methods of sustainable harvesting, disease and fire outbreak management, in fisheries stock management, for integrating land-use with protected areas and communities, and conservation in complex geo-political landscapes.
Relation to the environment.
The environment of ecosystems includes both physical parameters and biotic attributes. It is dynamically interlinked, and contains resources for organisms at any time throughout their life cycle. Like "ecology," the term "environment" has different conceptual meanings and overlaps with the concept of "nature." Environment "... includes the physical world, the social world of human relations and the built world of human creation.":62 The physical environment is external to the level of biological organization under investigation, including abiotic factors such as temperature, radiation, light, chemistry, climate and geology. The biotic environment includes genes, cells, organisms, members of the same species (conspecifics) and other species that share a habitat.
The distinction between external and internal environments, however, is an abstraction parsing life and environment into units or facts that are inseparable in reality. There is an interpenetration of cause and effect between the environment and life. The laws of thermodynamics, for example, apply to ecology by means of its physical state. With an understanding of metabolic and thermodynamic principles, a complete accounting of energy and material flow can be traced through an ecosystem. In this way, the environmental and ecological relations are studied through reference to conceptually manageable and isolated material parts. After the effective environmental components are understood through reference to their causes, however, they conceptually link back together as an integrated whole, or "holocoenotic" system as it was once called. This is known as the dialectical approach to ecology. The dialectical approach examines the parts, but integrates the organism and the environment into a dynamic whole (or umwelt). Change in one ecological or environmental factor can concurrently affect the dynamic state of an entire ecosystem.
Disturbance and resilience.
Ecosystems are regularly confronted with natural environmental variations and disturbances over time and geographic space. A disturbance is any process that removes biomass from a community, such as a fire, flood, drought, or predation. Disturbances occur over vastly different ranges in terms of magnitudes as well as distances and time periods, and are both the cause and product of natural fluctuations in death rates, species assemblages, and biomass densities within an ecological community. These disturbances create places of renewal where new directions emerge from the patchwork of natural experimentation and opportunity. Ecological resilience is a cornerstone theory in ecosystem management. Biodiversity fuels the resilience of ecosystems acting as a kind of regenerative insurance.
Metabolism and the early atmosphere.
Metabolism – the rate at which energy and material resources are taken up from the environment, transformed within an organism, and allocated to maintenance, growth and reproduction – is a fundamental physiological trait.
Ernest et al.:991
The Earth was formed approximately 4.5 billion years ago. As it cooled and a crust and oceans formed, its atmosphere transformed from being dominated by hydrogen to one composed mostly of methane and ammonia. Over the next billion years, the metabolic activity of life transformed the atmosphere into a mixture of carbon dioxide, nitrogen, and water vapor. These gases changed the way that light from the sun hit the Earth's surface and greenhouse effects trapped heat. There were untapped sources of free energy within the mixture of reducing and oxidizing gasses that set the stage for primitive ecosystems to evolve and, in turn, the atmosphere also evolved.
Throughout history, the Earth's atmosphere and biogeochemical cycles have been in a dynamic equilibrium with planetary ecosystems. The history is characterized by periods of significant transformation followed by millions of years of stability. The evolution of the earliest organisms, likely anaerobic methanogen microbes, started the process by converting atmospheric hydrogen into methane (4H2 + CO2 → CH4 + 2H2O). Anoxygenic photosynthesis reduced hydrogen concentrations and increased atmospheric methane, by converting hydrogen sulfide into water or other sulfur compounds (for example, 2H2S + CO2 + h"v" → CH2O + H2O + 2S). Early forms of fermentation also increased levels of atmospheric methane. The transition to an oxygen-dominant atmosphere (the "Great Oxidation") did not begin until approximately 2.4–2.3 billion years ago, but photosynthetic processes started 0.3 to 1 billion years prior.
Radiation: heat, temperature and light.
The biology of life operates within a certain range of temperatures. Heat is a form of energy that regulates temperature. Heat affects growth rates, activity, behaviour and primary production. Temperature is largely dependent on the incidence of solar radiation. The latitudinal and longitudinal spatial variation of temperature greatly affects climates and consequently the distribution of biodiversity and levels of primary production in different ecosystems or biomes across the planet. Heat and temperature relate importantly to metabolic activity. Poikilotherms, for example, have a body temperature that is largely regulated and dependent on the temperature of the external environment. In contrast, homeotherms regulate their internal body temperature by expending metabolic energy.
There is a relationship between light, primary production, and ecological energy budgets. Sunlight is the primary input of energy into the planet's ecosystems. Light is composed of electromagnetic energy of different wavelengths. Radiant energy from the sun generates heat, provides photons of light measured as active energy in the chemical reactions of life, and also acts as a catalyst for genetic mutation. Plants, algae, and some bacteria absorb light and assimilate the energy through photosynthesis. Organisms capable of assimilating energy by photosynthesis or through inorganic fixation of H2S are autotrophs. Autotrophs — responsible for primary production — assimilate light energy which becomes metabolically stored as potential energy in the form of biochemical enthalpic bonds.
Physical environments.
Water.
Wetland conditions such as shallow water, high plant productivity, and anaerobic substrates provide a suitable environment for important physical, biological, and chemical processes. Because of these processes, wetlands play a vital role in global nutrient and element cycles.
Cronk & Fennessy (2001):29
Diffusion of carbon dioxide and oxygen is approximately 10,000 times slower in water than in air. When soils are flooded, they quickly lose oxygen, becoming hypoxic (an environment with O2 concentration below 2 mg/liter) and eventually completely anoxic where anaerobic bacteria thrive among the roots. Water also influences the intensity and spectral composition of light as it reflects off the water surface and submerged particles. Aquatic plants exhibit a wide variety of morphological and physiological adaptations that allow them to survive, compete and diversify in these environments. For example, their roots and stems contain large air spaces (aerenchyma) that regulate the efficient transportation of gases (for example, CO2 and O2) used in respiration and photosynthesis. Salt water plants (halophytes) have additional specialized adaptations, such as the development of special organs for shedding salt and osmoregulating their internal salt (NaCl) concentrations, to live in estuarine, brackish, or oceanic environments. Anaerobic soil microorganisms in aquatic environments use nitrate, manganese ions, ferric ions, sulfate, carbon dioxide and some organic compounds; other microorganisms are facultative anaerobes and use oxygen during respiration when the soil becomes drier. The activity of soil microorganisms and the chemistry of the water reduces the oxidation-reduction potentials of the water. Carbon dioxide, for example, is reduced to methane (CH4) by methanogenic bacteria. The physiology of fish is also specially adapted to compensate for environmental salt levels through osmoregulation. Their gills form electrochemical gradients that mediate salt excretion in salt water and uptake in fresh water.
Gravity.
The shape and energy of the land is significantly affected by gravitational forces. On a large scale, the distribution of gravitational forces on the earth is uneven and influences the shape and movement of tectonic plates as well as influencing geomorphic processes such as orogeny and erosion. These forces govern many of the geophysical properties and distributions of ecological biomes across the Earth. On the organismal scale, gravitational forces provide directional cues for plant and fungal growth (gravitropism), orientation cues for animal migrations, and influence the biomechanics and size of animals. Ecological traits, such as allocation of biomass in trees during growth are subject to mechanical failure as gravitational forces influence the position and structure of branches and leaves. The cardiovascular systems of animals are functionally adapted to overcome pressure and gravitational forces that change according to the features of organisms (e.g., height, size, shape), their behaviour (e.g., diving, running, flying), and the habitat occupied (e.g., water, hot deserts, cold tundra).
Pressure.
Climatic and osmotic pressure places physiological constraints on organisms, especially those that fly and respire at high altitudes, or dive to deep ocean depths. These constraints influence vertical limits of ecosystems in the biosphere, as organisms are physiologically sensitive and adapted to atmospheric and osmotic water pressure differences. For example, oxygen levels decrease with decreasing pressure and are a limiting factor for life at higher altitudes. Water transportation by plants is another important ecophysiological parameter affected by osmotic pressure gradients. Water pressure in the depths of oceans requires that organisms adapt to these conditions. For example, diving animals such as whales, dolphins and seals are specially adapted to deal with changes in sound due to water pressure differences. Differences between hagfish species provide another example of adaptation to deep-sea pressure through specialized protein adaptations.
Wind and turbulence.
Turbulent forces in air and water affect the environment and ecosystem distribution, form and dynamics. On a planetary scale, ecosystems are affected by circulation patterns in the global trade winds. Wind power and the turbulent forces it creates can influence heat, nutrient, and biochemical profiles of ecosystems. For example, wind running over the surface of a lake creates turbulence, mixing the water column and influencing the environmental profile to create thermally layered zones, affecting how fish, algae, and other parts of the aquatic ecosystem are structured. Wind speed and turbulence also influence evapotranspiration rates and energy budgets in plants and animals. Wind speed, temperature and moisture content can vary as winds travel across different land features and elevations. For example, the westerlies come into contact with the coastal and interior mountains of western North America to produce a rain shadow on the leeward side of the mountain. The air expands and moisture condenses as the winds increase in elevation; this is called orographic lift and can cause precipitation. This environmental process produces spatial divisions in biodiversity, as species adapted to wetter conditions are range-restricted to the coastal mountain valleys and unable to migrate across the xeric ecosystems (e.g., of the Columbia Basin in western North America) to intermix with sister lineages that are segregated to the interior mountain systems.
Fire.
Plants convert carbon dioxide into biomass and emit oxygen into the atmosphere. By approximately 350 million years ago (the end of the Devonian period), photosynthesis had brought the concentration of atmospheric oxygen above 17%, which allowed combustion to occur. Fire releases CO2 and converts fuel into ash and tar. Fire is a significant ecological parameter that raises many issues pertaining to its control and suppression. While the issue of fire in relation to ecology and plants has been recognized for a long time, Charles Cooper brought attention to the issue of forest fires in relation to the ecology of forest fire suppression and management in the 1960s.
Native North Americans were among the first to influence fire regimes by controlling their spread near their homes or by lighting fires to stimulate the production of herbaceous foods and basketry materials. Fire creates a heterogeneous ecosystem age and canopy structure, and the altered soil nutrient supply and cleared canopy structure opens new ecological niches for seedling establishment. Most ecosystems are adapted to natural fire cycles. Plants, for example, are equipped with a variety of adaptations to deal with forest fires. Some species (e.g., "Pinus halepensis") cannot germinate until after their seeds have lived through a fire or been exposed to certain compounds from smoke. Environmentally triggered germination of seeds is called serotiny. Fire plays a major role in the persistence and resilience of ecosystems.
Soils.
Soil is the living top layer of mineral and organic dirt that covers the surface of the planet. It is the chief organizing centre of most ecosystem functions, and it is of critical importance in agricultural science and ecology. The decomposition of dead organic matter (for example, leaves on the forest floor), results in soils containing minerals and nutrients that feed into plant production. The whole of the planet's soil ecosystems is called the pedosphere where a large biomass of the Earth's biodiversity organizes into trophic levels. Invertebrates that feed and shred larger leaves, for example, create smaller bits for smaller organisms in the feeding chain. Collectively, these organisms are the detritivores that regulate soil formation. Tree roots, fungi, bacteria, worms, ants, beetles, centipedes, spiders, mammals, birds, reptiles, amphibians and other less familiar creatures all work to create the trophic web of life in soil ecosystems. Soils form composite phenotypes where inorganic matter is enveloped into the physiology of a whole community. As organisms feed and migrate through soils they physically displace materials, an ecological process called bioturbation. This aerates soils and stimulates heterotrophic growth and production. Soil microorganisms are influenced by and feed back into the trophic dynamics of the ecosystem. No single axis of causality can be discerned to segregate the biological from geomorphological systems in soils. Paleoecological studies of soils places the origin for bioturbation to a time before the Cambrian period. Other events, such as the evolution of trees and the colonization of land in the Devonian period played a significant role in the early development of ecological trophism in soils.
Biogeochemistry and climate.
Ecologists study and measure nutrient budgets to understand how these materials are regulated, flow, and recycled through the environment. This research has led to an understanding that there is global feedback between ecosystems and the physical parameters of this planet, including minerals, soil, pH, ions, water and atmospheric gases. Six major elements (hydrogen, carbon, nitrogen, oxygen, sulfur, and phosphorus; H, C, N, O, S, and P) form the constitution of all biological macromolecules and feed into the Earth's geochemical processes. From the smallest scale of biology, the combined effect of billions upon billions of ecological processes amplify and ultimately regulate the biogeochemical cycles of the Earth. Understanding the relations and cycles mediated between these elements and their ecological pathways has significant bearing toward understanding global biogeochemistry.
The ecology of global carbon budgets gives one example of the linkage between biodiversity and biogeochemistry. It is estimated that the Earth's oceans hold 40,000 gigatonnes (Gt) of carbon, that vegetation and soil hold 2070 Gt, and that fossil fuel emissions are 6.3 Gt carbon per year. There have been major restructurings in these global carbon budgets during the Earth's history, regulated to a large extent by the ecology of the land. For example, through the early-mid Eocene volcanic outgassing, the oxidation of methane stored in wetlands, and seafloor gases increased atmospheric CO2 (carbon dioxide) concentrations to levels as high as 3500 ppm.
In the Oligocene, from 25 to 32 million years ago, there was another significant restructuring of the global carbon cycle as grasses evolved a new mechanism of photosynthesis, C4 photosynthesis, and expanded their ranges. This new pathway evolved in response to the drop in atmospheric CO2 concentrations below 550 ppm. The relative abundance and distribution of biodiversity alters the dynamics between organisms and their environment such that ecosystems can be both cause and effect in relation to climate change. Human-driven modifications to the planet's ecosystems (e.g., disturbance, biodiversity loss, agriculture) contributes to rising atmospheric greenhouse gas levels. Transformation of the global carbon cycle in the next century is projected to raise planetary temperatures, lead to more extreme fluctuations in weather, alter species distributions, and increase extinction rates. The effect of global warming is already being registered in melting glaciers, melting mountain ice caps, and rising sea levels. Consequently, species distributions are changing along waterfronts and in continental areas where migration patterns and breeding grounds are tracking the prevailing shifts in climate. Large sections of permafrost are also melting to create a new mosaic of flooded areas having increased rates of soil decomposition activity that raises methane (CH4) emissions. There is concern over increases in atmospheric methane in the context of the global carbon cycle, because methane is a greenhouse gas that is 23 times more effective at absorbing long-wave radiation than CO2 on a 100-year time scale. Hence, there is a relationship between global warming, decomposition and respiration in soils and wetlands producing significant climate feedbacks and globally altered biogeochemical cycles.
History.
Early beginnings.
Ecology has a complex origin, due in large part to its interdisciplinary nature. Ancient Greek philosophers such as Hippocrates and Aristotle were among the first to record observations on natural history. However, they viewed life in terms of essentialism, where species were conceptualized as static unchanging things while varieties were seen as aberrations of an idealized type. This contrasts against the modern understanding of ecological theory where varieties are viewed as the real phenomena of interest and having a role in the origins of adaptations by means of natural selection. Early conceptions of ecology, such as a balance and regulation in nature can be traced to Herodotus (died "c". 425 BC), who described one of the earliest accounts of mutualism in his observation of "natural dentistry". Basking Nile crocodiles, he noted, would open their mouths to give sandpipers safe access to pluck leeches out, giving nutrition to the sandpiper and oral hygiene for the crocodile. Aristotle was an early influence on the philosophical development of ecology. He and his student Theophrastus made extensive observations on plant and animal migrations, biogeography, physiology, and on their behaviour, giving an early analogue to the modern concept of an ecological niche.
Ecological concepts such as food chains, population regulation, and productivity were first developed in the 1700s, through the published works of microscopist Antoni van Leeuwenhoek (1632–1723) and botanist Richard Bradley (1688?–1732). Biogeographer Alexander von Humboldt (1769–1859) was an early pioneer in ecological thinking and was among the first to recognize ecological gradients, where species are replaced or altered in form along environmental gradients, such as a cline forming along a rise in elevation. Humboldt drew inspiration from Isaac Newton as he developed a form of "terrestrial physics." In Newtonian fashion, he brought a scientific exactitude for measurement into natural history and even alluded to concepts that are the foundation of a modern ecological law on species-to-area relationships. Natural historians, such as Humboldt, James Hutton and Jean-Baptiste Lamarck (among others) laid the foundations of the modern ecological sciences. The term "ecology" (German: "Oekologie, Ökologie") is of a more recent origin and was first coined by the German biologist Ernst Haeckel in his book "Generelle Morphologie der Organismen" (1866). Haeckel was a zoologist, artist, writer, and later in life a professor of comparative anatomy.
 By ecology, we mean the whole science of the relations of the organism to the environment including, in the broad sense, all the "conditions of existence."... Thus the theory of evolution explains the housekeeping relations of organisms mechanistically as the necessary consequences of effectual causes and so forms the monistic groundwork of ecology.
 Ernst Haeckel (1866):140 [B]
 Ernst Haeckel (left) and Eugenius Warming (right), two founders of ecology
Opinions differ on who was the founder of modern ecological theory. Some mark Haeckel's definition as the beginning; others say it was Eugenius Warming with the writing of Oecology of Plants: An Introduction to the Study of Plant Communities (1895), or Carl Linnaeus' principles on the economy of nature that matured in the early 18th century. Linnaeus founded an early branch of ecology that he called the economy of nature. His works influenced Charles Darwin, who adopted Linnaeus' phrase on the "economy or polity of nature" in "The Origin of Species". Linnaeus was the first to frame the balance of nature as a testable hypothesis. Haeckel, who admired Darwin's work, defined ecology in reference to the economy of nature, which has led some to question whether ecology and the economy of nature are synonymous.
From Aristotle until Darwin, the natural world was predominantly considered static and unchanging. Prior to "The Origin of Species", there was little appreciation or understanding of the dynamic and reciprocal relations between organisms, their adaptations, and the environment. An exception is the 1789 publication "Natural History of Selborne" by Gilbert White (1720–1793), considered by some to be one of the earliest texts on ecology. While Charles Darwin is mainly noted for his treatise on evolution, he was one of the founders of soil ecology, and he made note of the first ecological experiment in "The Origin of Species". Evolutionary theory changed the way that researchers approached the ecological sciences.
Nowhere can one see more clearly illustrated what may be called the sensibility of such an organic complex,--expressed by the fact that whatever affects any species belonging to it, must speedily have its influence of some sort upon the whole assemblage. He will thus be made to see the impossibility of studying any form completely, out of relation to the other forms,--the necessity for taking a comprehensive survey of the whole as a condition to a satisfactory understanding of any part.
Stephen Forbes (1887)
Since 1900.
Modern ecology is a young science that first attracted substantial scientific attention toward the end of the 19th century (around the same time that evolutionary studies were gaining scientific interest). Notable scientist Ellen Swallow Richards may have first introduced the term "oekology" (which eventually morphed into home economics) in the U.S. as early 1892.
In the early 20th century, ecology transitioned from a more descriptive form of natural history to a more analytical form of "scientific natural history". Frederic Clements published the first American ecology book in 1905, presenting the idea of plant communities as a superorganism. This publication launched a debate between ecological holism and individualism that lasted until the 1970s. Clements' superorganism concept proposed that ecosystems progress through regular and determined stages of seral development that are analogous to the developmental stages of an organism. The Clementsian paradigm was challenged by Henry Gleason, who stated that ecological communities develop from the unique and coincidental association of individual organisms. This perceptual shift placed the focus back onto the life histories of individual organisms and how this relates to the development of community associations.
The Clementsian superorganism theory was an overextended application of an idealistic form of holism. The term "holism" was coined in 1926 by Jan Christiaan Smuts, a South African general and polarizing historical figure who was inspired by Clements' superorganism concept.[C] Around the same time, Charles Elton pioneered the concept of food chains in his classical book "Animal Ecology". Elton defined ecological relations using concepts of food chains, food cycles, and food size, and described numerical relations among different functional groups and their relative abundance. Elton's 'food cycle' was replaced by 'food web' in a subsequent ecological text. Alfred J. Lotka brought in many theoretical concepts applying thermodynamic principles to ecology.
In 1942, Raymond Lindeman wrote a landmark paper on the trophic dynamics of ecology, which was published posthumously after initially being rejected for its theoretical emphasis. Trophic dynamics became the foundation for much of the work to follow on energy and material flow through ecosystems. Robert E. MacArthur advanced mathematical theory, predictions and tests in ecology in the 1950s, which inspired a resurgent school of theoretical mathematical ecologists. Ecology also has developed through contributions from other nations, including Russia's Vladimir Vernadsky and his founding of the biosphere concept in the 1920s and Japan's Kinji Imanishi and his concepts of harmony in nature and habitat segregation in the 1950s. Scientific recognition of contributions to ecology from non-English-speaking cultures is hampered by language and translation barriers.
This whole chain of poisoning, then, seems to rest on a base of minute plants which must have been the original concentrators. But what of the opposite end of the food chain—the human being who, in probable ignorance of all this sequence of events, has rigged his fishing tackle, caught a string of fish from the waters of Clear Lake, and taken them home to fry for his supper?
Rachel Carson (1962):48
Ecology surged in popular and scientific interest during the 1960–1970s environmental movement. There are strong historical and scientific ties between ecology, environmental management, and protection. The historical emphasis and poetic naturalistic writings advocating the protection of wild places by notable ecologists in the history of conservation biology, such as Aldo Leopold and Arthur Tansley, have been seen as far removed from urban centres where, it is claimed, the concentration of pollution and environmental degradation is located. Palamar (2008) notes an overshadowing by mainstream environmentalism of pioneering women in the early 1900s who fought for urban health ecology (then called euthenics) and brought about changes in environmental legislation. Women such as Ellen Swallow Richards and Julia Lathrop, among others, were precursors to the more popularized environmental movements after the 1950s.
In 1962, marine biologist and ecologist Rachel Carson's book "Silent Spring" helped to mobilize the environmental movement by alerting the public to toxic pesticides, such as DDT, bioaccumulating in the environment. Carson used ecological science to link the release of environmental toxins to human and ecosystem health. Since then, ecologists have worked to bridge their understanding of the degradation of the planet's ecosystems with environmental politics, law, restoration, and natural resources management.

</doc>
<doc id="9631" url="http://en.wikipedia.org/wiki?curid=9631" title="Country dance terminology">
Country dance terminology

An alphabetic list of modern Country dance terminology;
Active Couple - for long-ways sets with more than one couple dancing, the active couple is the couple doing the more complicated movement during any given portion of the dance. For duple dances, that is every other couple, and for triple dances, every third couple is the active couple. The term is applicable to triplet dances, where typically the active couple is the only couple that is active. In the seventeenth and eighteenth centuries, only the active couple—the "1st couple"—initiated the action, other couples supporting their movements and joining in as needed, until they also took their turn as leading couples.
Arm right (or left) - couples link right (or left) arms and move forward in a circle, returning to their starting positions.
Back to back - facing another person, move forward "passing" right shoulders and "fall back" to place passing left. May also start by passing left and falling back right. Called a do si do in other dance forms (and "dos-à-dos" in France).
Balance back - a "single" backward.
Both hands - two dancers face each other and give hands right to left and left to right.
Cast - turn outward and dance up or down outside the set, as directed. The instruction "cast off" is frequently synonymous with "cast down".
Changes (starting right or left) - like the "circular hey", but dancers give hands as they pass (handing hey). The number of changes is given first (e.g. two changes, three changes, etc.). 
Chassé - slipping step to right or left as directed.
Circular hey - dancers face partners or along the line and "pass" right and left alternating a stated number of changes. Usually done without hands, the circular hey may also be done by more than two couples facing alternately and moving in opposite directions - usually to their original places. This name for the figure was invented by Cecil Sharp and does not appear in sources pre-1900.Nonetheless, some early country dances calling for heys have been interpreted in modern times using circular heys. In early dances, where the hey is called a "double hey", it works to interpret this as an oval hey, like the modern circular hey but adapted to the straight sides of a longways formation.
Clockwise - in a ring, move to one's left. In a "turn single" turn to the right.
Contrary - your contrary is not your partner. In Playford's original notation, this term meant the same thing that "Corner" (or sometimes "Opposite") means today.
Corner - in a two-couple set, the dancer diagonally opposite, i.e., the first man and the second woman, first woman and second man.
Counter-clockwise - the opposite of clockwise - in a ring, move right. In a "turn single", turn to the left.
Cross hands - face and give left to left and right to right.
Cross over or Pass - change places with another dancer moving forward and passing by the right shoulder, unless otherwise directed.
Cross and go below - cross as above and go outside below one couple, ending improper.
Double - four steps forward or back, closing the feet on the 4th step (see "Single" below).
Fall (back) - dance backwards.
Figure of 8 - a weaving figure in which dancers pass between two standing people and move around them in a figure 8 pattern. A Full Figure of 8 returns the dancer to original position; a Half Figure of 8 leaves the dancer on the opposite side of the set from original position. In doing this figure, the man lets his partner pass in front of him in some communities; others prefer the rule of "the dancer coming from the left-hand side has right of way". A double figure of 8 involves four dancers tracing a whole figure of eight around the (now unocccupied) positions of the other couple; half the dancers typically start going around the outside first.
Forward - "lead" or move in the direction you are facing.
Gypsy - two dancers move around each other in a circular path while facing each other.
Hands across - right or left hands are given to "corners", and dancers move in the direction they face.
Hands three, four etc.. - the designated number of dancers form a ring and move around in the direction indicated, usually first to the left and back to the right.
Hey - a weaving figure in which two groups of dancers move in single file and in opposite directions (see "circular hey" and "straight hey").
Honour - couples step forward and right, close, shift weight, and curtsey or bow, then repeat to their left. In the time of Playford's original manual, a woman's curtsey was similar to the modern one, but a man's honour (or reverence) kept the upper body upright and involved sliding the left leg forward while bending the right knee.
Lead - couples join inside hands and walk up or down the set.
"Mad Robin" figure - a back to back with your neighbor while maintaining eye-contact with your partner across the set. Men take one step forward and then slide to the right passing in front of their neighbour, then step backwards and slide left behind their neighbour. Conversely women take one step backwards and then slide to the left passing behind of their neighbour, then step forwards and slide right in front of their neighbour. In some versions, the dancer who is going outside the set at the moment casts out to begin that motion. The term "Mad Robin" comes from the name of a dance which has the move and was adopted into contra dancing (as a move for all four dancers, unlike the original English dance, where only one couple does it a time) before being readmitted as an all-four figure into English in modern dances.
Neighbour - the person you are standing beside, but not your partner.
Opposite - the person you are facing.
Poussette - two dancers face, give both hands and change places as a couple with two adjacent dancers. One pair moves a "double" toward one wall, the other toward the other wall. In this half-poussette, couples pass around each other diagonally. To complete the poussette, move in the opposite direction. Dancers end in their original places. In a similar movement, the Draw Poussette, the dancing pairs move on a U-shaped track with one dancer of the pair always moving forwards.
Proper - with the man on the left and the woman on the right, from the perspective of someone facing the music. Improper is the opposite.
Right & left - like the "circular hey", but dancers give hands as they pass (handing hey).
Set - a dancer steps right, closes with left foot and shifts weight to it, then steps back to the right foot (right-together-step); then repeats the process mirror-image (left-together-step). In some areas it is done starting to the left. It may be done in place or advancing. Often followed by a turn single.
Siding - two dancers, partners by default if not otherwise specified, go forward in four counts to meet side by side, then back in four counts to where they started the figure. As depicted by Feuillet, this is done right side by right side the first time, left by left the second time.
Single - two steps in any direction closing feet on the second step (the second step tends to be interpreted as a closing action in which weight usually stays on the same foot as before, consistent with descriptions from Renaissance sources).
Slipping circle (left or right) - dancers take hands in a circle (facing in) and chassé left or right.
Hands across or Star - some number of dancers (usually four) join right or left hands in the middle of their circle (facing either CW or CCW). The dancers circle in the direction they face.
Straight hey for four - dancers face alternately, the two in the middle facing out. Dancers pass right shoulders on either end and weave to the end opposite. If the last pass at the end is by the right. the dancer turns right and reenters the line by the same shoulder; vice versa if the last pass was to the left. Dancers end in their original places.
Straight hey for three - the first dancer faces the other two and "passes" right shoulders with the second dancer, left shoulder with the third - the other dancers moving and passing the indicated shoulder. On making the last pass, each dancer makes a whole turn on the end, bearing right if the last pass was by the right shoulder or left if last pass was by the left, and reenters the figure returning to place. Each dancer describes a figure of eight pattern.
Swing - a "turn" with two hands, but moving faster and making more than one revolution.
Turn - face, give "both hands", and make a complete circular, clockwise turn to place.
Turn by right or left - dancers join right (or left) hands and turn around, separate, and "fall" to places.
Turn single - dancers turn around in four steps. 'Turn single right shoulder' is a clockwise turn; 'turn single left shoulder' is a counterclockwise turn.
Up a double and back - common combination in which dancers (usually having linked hands in a line) advance a double and then retire another double.

</doc>
<doc id="9632" url="http://en.wikipedia.org/wiki?curid=9632" title="Ecosystem">
Ecosystem

An ecosystem is a community of living organisms (plants, animals and microbes) in conjunction with the nonliving components of their environment (things like air, water and mineral soil), interacting as a system. These biotic and abiotic components are regarded as linked together through nutrient cycles and energy flows. As ecosystems are defined by the network of interactions among organisms, and between organisms and their environment, they can be of any size but usually encompass specific, limited spaces (although some scientists say that the entire planet is an ecosystem).
Energy, water, nitrogen and soil minerals are other essential abiotic components of an ecosystem. The energy that flows through ecosystems is obtained primarily from the sun. It generally enters the system through photosynthesis, a process that also captures carbon from the atmosphere. By feeding on plants and on one another, animals play an important role in the movement of matter and energy through the system. They also influence the quantity of plant and microbial biomass present. By breaking down dead organic matter, decomposers release carbon back to the atmosphere and facilitate nutrient cycling by converting nutrients stored in dead biomass back to a form that can be readily used by plants and other microbes.
Ecosystems are controlled both by external and internal factors. External factors such as climate, the parent material which forms the soil and topography, control the overall structure of an ecosystem and the way things work within it, but are not themselves influenced by the ecosystem. Other external factors include time and potential biota. Ecosystems are dynamic entities—invariably, they are subject to periodic disturbances and are in the process of recovering from some past disturbance. Ecosystems in similar environments that are located in different parts of the world can have very different characteristics simply because they contain different species. The introduction of non-native species can cause substantial shifts in ecosystem function. Internal factors not only control ecosystem processes but are also controlled by them and are often subject to feedback loops. While the resource inputs are generally controlled by external processes like climate and parent material, the availability of these resources within the ecosystem is controlled by internal factors like decomposition, root competition or shading. Other internal factors include disturbance, succession and the types of species present. Although humans exist and operate within ecosystems, their cumulative effects are large enough to influence external factors like climate.
Biodiversity affects ecosystem function, as do the processes of disturbance and succession. Ecosystems provide a variety of goods and services upon which people depend; the principles of ecosystem management suggest that rather than managing individual species, natural resources should be managed at the level of the ecosystem itself. Classifying ecosystems into ecologically homogeneous units is an important step towards effective ecosystem management, but there is no single, agreed-upon way to do this.
History and development.
The term "ecosystem" was first used in a publication by British ecologist Arthur Tansley. Tansley devised the concept to draw attention to the importance of transfers of materials between organisms and their environment. He later refined the term, describing it as "The whole system, ... including not only the organism-complex, but also the whole complex of physical factors forming what we call the environment". Tansley regarded ecosystems not simply as natural units, but as mental isolates. Tansley later defined the spatial extent of ecosystems using the term ecotope.
G. Evelyn Hutchinson, a pioneering limnologist who was a contemporary of Tansley's, combined Charles Elton's ideas about trophic ecology with those of Russian geochemist Vladimir Vernadsky to suggest that mineral nutrient availability in a lake limited algal production which would, in turn, limit the abundance of animals that feed on algae. Raymond Lindeman took these ideas one step further to suggest that the flow of energy through a lake was the primary driver of the ecosystem. Hutchinson's students, brothers Howard T. Odum and Eugene P. Odum, further developed a "systems approach" to the study of ecosystems, allowing them to study the flow of energy and material through ecological systems.
Ecosystem processes.
Energy and carbon enter ecosystems through photosynthesis, are incorporated into living tissue, transferred to other organisms that feed on the living and dead plant matter, and eventually released through respiration. Most mineral nutrients, on the other hand, are recycled within ecosystems.
Ecosystems are controlled both by external and internal factors. External factors, also called state factors, control the overall structure of an ecosystem and the way things work within it, but are not themselves influenced by the ecosystem. The most important of these is climate. Climate determines the biome in which the ecosystem is embedded. Rainfall patterns and temperature seasonality determine the amount of water available to the ecosystem and the supply of energy available (by influencing photosynthesis). Parent material, the underlying geological material that gives rise to soils, determines the nature of the soils present, and influences the supply of mineral nutrients. Topography also controls ecosystem processes by affecting things like microclimate, soil development and the movement of water through a system. This may be the difference between the ecosystem present in wetland situated in a small depression on the landscape, and one present on an adjacent steep hillside.
Other external factors that play an important role in ecosystem functioning include time and potential biota. Ecosystems are dynamic entities—invariably, they are subject to periodic disturbances and are in the process of recovering from some past disturbance. Time plays a role in the development of soil from bare rock and the recovery of a community from disturbance. Similarly, the set of organisms that can potentially be present in an area can also have a major impact on ecosystems. Ecosystems in similar environments that are located in different parts of the world can end up doing things very differently simply because they have different pools of species present. The introduction of non-native species can cause substantial shifts in ecosystem function.
Unlike external factors, internal factors in ecosystems not only control ecosystem processes, but are also controlled by them. Consequently, they are often subject to feedback loops. While the resource inputs are generally controlled by external processes like climate and parent material, the availability of these resources within the ecosystem is controlled by internal factors like decomposition, root competition or shading. Other factors like disturbance, succession or the types of species present are also internal factors. Human activities are important in almost all ecosystems. Although humans exist and operate within ecosystems, their cumulative effects are large enough to influence external factors like climate.
Primary production.
Primary production is the production of organic matter from inorganic carbon sources. Overwhelmingly, this occurs through photosynthesis. The energy incorporated through this process supports life on earth, while the carbon makes up much of the organic matter in living and dead biomass, soil carbon and fossil fuels. It also drives the carbon cycle, which influences global climate via the greenhouse effect.
Through the process of photosynthesis, plants capture energy from light and use it to combine carbon dioxide and water to produce carbohydrates and oxygen. The photosynthesis carried out by all the plants in an ecosystem is called the gross primary production (GPP). About 48–60% of the GPP is consumed in plant respiration. The remainder, that portion of GPP that is not used up by respiration, is known as the net primary production (NPP). Total photosynthesis is limited by a range of environmental factors. These include the amount of light available, the amount of leaf area a plant has to capture light (shading by other plants is a major limitation of photosynthesis), rate at which carbon dioxide can be supplied to the chloroplasts to support photosynthesis, the availability of water, and the availability of suitable temperatures for carrying out photosynthesis.
Energy flow.
Left: Energy flow diagram of a frog. The frog represents a node in an extended food web. The energy ingested is utilized for metabolic processes and transformed into biomass. The energy flow continues on its path if the frog is ingested by predators, parasites, or as a decaying carcass in soil. This energy flow diagram illustrates how energy is lost as it fuels the metabolic process that transforms the energy and nutrients into biomass.<br> Right: An expanded three link energy food chain (1. plants, 2. herbivores, 3. carnivores) illustrating the relationship between food flow diagrams and energy transformity. The transformity of energy becomes degraded, dispersed, and diminished from higher quality to lesser quantity as the energy within a food chain flows from one trophic species into another. Abbreviations: I=input, A=assimilation, R=respiration, NU=not utilized, P=production, B=biomass.
The carbon and energy incorporated into plant tissues (net primary production) is either consumed by animals while the plant is alive, or it remains uneaten when the plant tissue dies and becomes detritus. In terrestrial ecosystems, roughly 90% of the NPP ends up being broken down by decomposers. The remainder is either consumed by animals while still alive and enters the plant-based trophic system, or it is consumed after it has died, and enters the detritus-based trophic system. In aquatic systems, the proportion of plant biomass that gets consumed by herbivores is much higher.
In trophic systems photosynthetic organisms are the primary producers. The organisms that consume their tissues are called primary consumers or secondary producers—herbivores. Organisms which feed on microbes (bacteria and fungi) are termed microbivores. Animals that feed on primary consumers—carnivores—are secondary consumers. Each of these constitutes a trophic level. The sequence of consumption—from plant to herbivore, to carnivore—forms a food chain. Real systems are much more complex than this—organisms will generally feed on more than one form of food, and may feed at more than one trophic level. Carnivores may capture some prey which are part of a plant-based trophic system and others that are part of a detritus-based trophic system (a bird that feeds both on herbivorous grasshoppers and earthworms, which consume detritus). Real systems, with all these complexities, form food webs rather than food chains.
Decomposition.
The carbon and nutrients in dead organic matter are broken down by a group of processes known as decomposition. This releases nutrients that can then be re-used for plant and microbial production, and returns carbon dioxide to the atmosphere (or water) where it can be used for photosynthesis. In the absence of decomposition, dead organic matter would accumulate in an ecosystem and nutrients and atmospheric carbon dioxide would be depleted. Approximately 90% of terrestrial NPP goes directly from plant to decomposer.
Decomposition processes can be separated into three categories—leaching, fragmentation and chemical alteration of dead material. As water moves through dead organic matter, it dissolves and carries with it the water-soluble components. These are then taken up by organisms in the soil, react with mineral soil, or are transported beyond the confines of the ecosystem (and are considered "lost" to it). Newly shed leaves and newly dead animals have high concentrations of water-soluble components, and include sugars, amino acids and mineral nutrients. Leaching is more important in wet environments, and much less important in dry ones.
Fragmentation processes break organic material into smaller pieces, exposing new surfaces for colonization by microbes. Freshly shed leaf litter may be inaccessible due to an outer layer of cuticle or bark, and cell contents are protected by a cell wall. Newly dead animals may be covered by an exoskeleton. Fragmentation processes, which break through these protective layers, accelerate the rate of microbial decomposition. Animals fragment detritus as they hunt for food, as does passage through the gut. Freeze-thaw cycles and cycles of wetting and drying also fragment dead material.
The chemical alteration of dead organic matter is primarily achieved through bacterial and fungal action. Fungal hyphae produce enzymes which can break through the tough outer structures surrounding dead plant material. They also produce enzymes which break down lignin, which allows to them access to both cell contents and to the nitrogen in the lignin. Fungi can transfer carbon and nitrogen through their hyphal networks and thus, unlike bacteria, are not dependent solely on locally available resources.
Decomposition rates vary among ecosystems. The rate of decomposition is governed by three sets of factors—the physical environment (temperature, moisture and soil properties), the quantity and quality of the dead material available to decomposers, and the nature of the microbial community itself. Temperature controls the rate of microbial respiration; the higher the temperature, the faster microbial decomposition occurs. It also affects soil moisture, which slows microbial growth and reduces leaching. Freeze-thaw cycles also affect decomposition—freezing temperatures kill soil microorganisms, which allows leaching to play a more important role in moving nutrients around. This can be especially important as the soil thaws in the Spring, creating a pulse of nutrients which become available.
Decomposition rates are low under very wet or very dry conditions. Decomposition rates are highest in wet, moist conditions with adequate levels of oxygen. Wet soils tend to become deficient in oxygen (this is especially true in wetlands), which slows microbial growth. In dry soils, decomposition slows as well, but bacteria continue to grow (albeit at a slower rate) even after soils become too dry to support plant growth. When the rains return and soils become wet, the osmotic gradient between the bacterial cells and the soil water causes the cells to gain water quickly. Under these conditions, many bacterial cells burst, releasing a pulse of nutrients. Decomposition rates also tend to be slower in acidic soils. Soils which are rich in clay minerals tend to have lower decomposition rates, and thus, higher levels of organic matter. The smaller particles of clay result in a larger surface area that can hold water. The higher the water content of a soil, the lower the oxygen content and consequently, the lower the rate of decomposition. Clay minerals also bind particles of organic material to their surface, making them less accessibly to microbes. Soil disturbance like tilling increase decomposition by increasing the amount of oxygen in the soil and by exposing new organic matter to soil microbes.
The quality and quantity of the material available to decomposers is another major factor that influences the rate of decomposition. Substances like sugars and amino acids decompose readily and are considered "labile". Cellulose and hemicellulose, which are broken down more slowly, are "moderately labile". Compounds which are more resistant to decay, like lignin or cutin, are considered "recalcitrant". Litter with a higher proportion of labile compounds decomposes much more rapidly than does litter with a higher proportion of recalcitrant material. Consequently, dead animals decompose more rapidly than dead leaves, which themselves decompose more rapidly than fallen branches. As organic material in the soil ages, its quality decreases. The more labile compounds decompose quickly, leaving an increasing proportion of recalcitrant material. Microbial cell walls also contain recalcitrant materials like chitin, and these also accumulate as the microbes die, further reducing the quality of older soil organic matter.
Nutrient cycling.
Ecosystems continually exchange energy and carbon with the wider environment; mineral nutrients, on the other hand, are mostly cycled back and forth between plants, animals, microbes and the soil. Most nitrogen enters ecosystems through biological nitrogen fixation, is deposited through precipitation, dust, gases or is applied as fertilizer. Since most terrestrial ecosystems are nitrogen-limited, nitrogen cycling is an important control on ecosystem production.
Until modern times, nitrogen fixation was the major source of nitrogen for ecosystems. Nitrogen fixing bacteria either live symbiotically with plants, or live freely in the soil. The energetic cost is high for plants which support nitrogen-fixing symbionts—as much as 25% of GPP when measured in controlled conditions. Many members of the legume plant family support nitrogen-fixing symbionts. Some cyanobacteria are also capable of nitrogen fixation. These are phototrophs, which carry out photosynthesis. Like other nitrogen-fixing bacteria, they can either be free-living or have symbiotic relationships with plants. Other sources of nitrogen include acid deposition produced through the combustion of fossil fuels, ammonia gas which evaporates from agricultural fields which have had fertilizers applied to them, and dust. Anthropogenic nitrogen inputs account for about 80% of all nitrogen fluxes in ecosystems.
When plant tissues are shed or are eaten, the nitrogen in those tissues becomes available to animals and microbes. Microbial decomposition releases nitrogen compounds from dead organic matter in the soil, where plants, fungi and bacteria compete for it. Some soil bacteria use organic nitrogen-containing compounds as a source of carbon, and release ammonium ions into the soil. This process is known as nitrogen mineralization. Others convert ammonium to nitrite and nitrate ions, a process known as nitrification. Nitric oxide and nitrous oxide are also produced during nitrification. Under nitrogen-rich and oxygen-poor conditions, nitrates and nitrites are converted to nitrogen gas, a process known as denitrification.
Other important nutrients include phosphorus, sulfur, calcium, potassium, magnesium and manganese. Phosphorus enters ecosystems through weathering. As ecosystems age this supply diminishes, making phosphorus-limitation more common in older landscapes (especially in the tropics). Calcium and sulfur are also produced by weathering, but acid deposition is an important source of sulfur in many ecosystems. Although magnesium and manganese are produced by weathering, exchanges between soil organic matter and living cells account for a significant portion of ecosystem fluxes. Potassium is primarily cycled between living cells and soil organic matter.
Function and biodiversity.
Ecosystem processes are broad generalizations that actually take place through the actions of individual organisms. The nature of the organisms—the species, functional groups and trophic levels to which they belong—dictates the sorts of actions these individuals are capable of carrying out, and the relative efficiency with which they do so. Thus, ecosystem processes are driven by the number of species in an ecosystem, the exact nature of each individual species, and the relative abundance organisms within these species. Biodiversity plays an important role in ecosystem functioning.
Ecological theory suggests that in order to coexist, species must have some level of limiting similarity—they must be different from one another in some fundamental way, otherwise one species would competitively exclude the other. Despite this, the cumulative effect of additional species in an ecosystem is not linear—additional species may enhance nitrogen retention, for example, but beyond some level of species richness, additional species may have little additive effect. The addition (or loss) of species which are ecologically similar to those already present in an ecosystem tends to only have a small effect on ecosystem function. Ecologically distinct species, on the other hand, have a much larger effect. Similarly, dominant species have a large impact on ecosystem function, while rare species tend to have a small effect. Keystone species tend to have an effect on ecosystem function that is disproportionate to their abundance in an ecosystem.
Ecosystem goods and services.
Ecosystems provide a variety of goods and services upon which people depend. Ecosystem goods include the "tangible, material products" of ecosystem processes—food, construction material, medicinal plants—in addition to less tangible items like tourism and recreation, and genes from wild plants and animals that can be used to improve domestic species. Ecosystem services, on the other hand, are generally "improvements in the condition or location of things of value". These include things like the maintenance of hydrological cycles, cleaning air and water, the maintenance of oxygen in the atmosphere, crop pollination and even things like beauty, inspiration and opportunities for research. While ecosystem goods have traditionally been recognized as being the basis for things of economic value, ecosystem services tend to be taken for granted. While Gretchen Daily's original definition distinguished between ecosystem goods and ecosystem services, Robert Costanza and colleagues' later work and that of the Millennium Ecosystem Assessment lumped all of these together as ecosystem services.
Ecosystem management.
When natural resource management is applied to whole ecosystems, rather than single species, it is termed ecosystem management. A variety of definitions exist: F. Stuart Chapin and coauthors define it as "the application of ecological science to resource management to promote long-term sustainability of ecosystems and the delivery of essential ecosystem goods and services", while Norman Christensen and coauthors defined it as "management driven by explicit goals, executed by policies, protocols, and practices, and made adaptable by monitoring and research based on our best understanding of the ecological interactions and processes necessary to sustain ecosystem structure and function" and Peter Brussard and colleagues defined it as "managing areas at various scales in such a way that ecosystem services and biological resources are preserved while appropriate human use and options for livelihood are sustained".
Although definitions of ecosystem management abound, there is a common set of principles which underlie these definitions. A fundamental principle is the long-term sustainability of the production of goods and services by the ecosystem; "intergenerational sustainability [is] a precondition for management, not an afterthought". It also requires clear goals with respect to future trajectories and behaviors of the system being managed. Other important requirements include a sound ecological understanding of the system, including connectedness, ecological dynamics and the context in which the system is embedded. Other important principles include an understanding of the role of humans as components of the ecosystems and the use of adaptive management. While ecosystem management can be used as part of a plan for wilderness conservation, it can also be used in intensively managed ecosystems (see, for example, agroecosystem and close to nature forestry).
Ecosystem dynamics.
Ecosystems are dynamic entities—invariably, they are subject to periodic disturbances and are in the process of recovering from some past disturbance. When an ecosystem is subject to some sort of perturbation, it responds by moving away from its initial state. The tendency of a system to remain close to its equilibrium state, despite that disturbance, is termed its resistance. On the other hand, the speed with which it returns to its initial state after disturbance is called its resilience.
From one year to another, ecosystems experience variation in their biotic and abiotic environments. A drought, an especially cold winter and a pest outbreak all constitute short-term variability in environmental conditions. Animal populations vary from year to year, building up during resource-rich periods and crashing as they overshoot their food supply. These changes play out in changes in NPP, decomposition rates, and other ecosystem processes. Longer-term changes also shape ecosystem processes—the forests of eastern North America still show legacies of cultivation which ceased 200 years ago, while methane production in eastern Siberian lakes is controlled by organic matter which accumulated during the Pleistocene.
Disturbance also plays an important role in ecological processes. F. Stuart Chapin and coauthors define disturbance as "a relatively discrete event in time and space that alters the structure of populations, communities and ecosystems and causes changes in resources availability or the physical environment". This can range from tree falls and insect outbreaks to hurricanes and wildfires to volcanic eruptions and can cause large changes in plant, animal and microbe populations, as well soil organic matter content. Disturbance is followed by succession, a "directional change in ecosystem structure and functioning resulting from biotically driven changes in resources supply."
The frequency and severity of disturbance determines the way it impacts ecosystem function. Major disturbance like a volcanic eruption or glacial advance and retreat leave behind soils that lack plants, animals or organic matter. Ecosystems that experience disturbances that undergo primary succession. Less severe disturbance like forest fires, hurricanes or cultivation result in secondary succession. More severe disturbance and more frequent disturbance result in longer recovery times. Ecosystems recover more quickly from less severe disturbance events.
The early stages of primary succession are dominated by species with small propagules (seed and spores) which can be dispersed long distances. The early colonizers—often algae, cyanobacteria and lichens—stabilize the substrate. Nitrogen supplies are limited in new soils, and nitrogen-fixing species tend to play an important role early in primary succession. Unlike in primary succession, the species that dominate secondary succession, are usually present from the start of the process, often in the soil seed bank. In some systems the successional pathways are fairly consistent, and thus, are easy to predict. In others, there are many possible pathways—for example, the introduced nitrogen-fixing legume, "Myrica faya", alter successional trajectories in Hawaiian forests.
The theoretical ecologist Robert Ulanowicz has used information theory tools to describe the structure of ecosystems, emphasizing mutual information (correlations) in studied systems. Drawing on this methodology and prior observations of complex ecosystems, Ulanowicz depicts approaches to determining the stress levels on ecosystems and predicting system reactions to defined types of alteration in their settings (such as increased or reduced energy flow, and eutrophication.
Ecosystem ecology.
Ecosystem ecology studies "the flow of energy and materials through organisms and the physical environment". It seeks to understand the processes which govern the stocks of material and energy in ecosystems, and the flow of matter and energy through them. The study of ecosystems can cover 10 orders of magnitude, from the surface layers of rocks to the surface of the planet.
There is no single definition of what constitutes an ecosystem. German ecologist Ernst-Detlef Schulze and coauthors defined an ecosystem as an area which is "uniform regarding the biological turnover, and contains all the fluxes above and below the ground area under consideration." They explicitly reject Gene Likens' use of entire river catchments as "too wide a demarcation" to be a single ecosystem, given the level of heterogeneity within such an area. Other authors have suggested that an ecosystem can encompass a much larger area, even the whole planet. Schulze and coauthors also rejected the idea that a single rotting log could be studied as an ecosystem because the size of the flows between the log and its surroundings are too large, relative to the proportion cycles within the log. Philosopher of science Mark Sagoff considers the failure to define "the kind of object it studies" to be an obstacle to the development of theory in ecosystem ecology.
Ecosystems can be studied through a variety of approaches—theoretical studies, studies monitoring specific ecosystems over long periods of time, those that look at differences between ecosystems to elucidate how they work and direct manipulative experimentation. Studies can be carried out at a variety of scales, from and mesocosms which serve as simplified representations of ecosystems, through whole-ecosystem studies. American ecologist Stephen R. Carpenter has argued that microcosm experiments can be "irrelevant and diversionary" if they are not carried out in conjunction with field studies carried out at the ecosystem scale, because microcosm experiments often fail to accurately predict ecosystem-level dynamics.
The Hubbard Brook Ecosystem Study, established in the White Mountains, New Hampshire in 1963, was the first successful attempt to study an entire watershed as an ecosystem. The study used stream chemistry as a means of monitoring ecosystem properties, and developed a detailed biogeochemical model of the ecosystem. Long-term research at the site led to the discovery of acid rain in North America in 1972, and was able to document the consequent depletion of soil cations (especially calcium) over the next several decades.
Classification.
Classifying ecosystems into ecologically homogeneous units is an important step towards effective ecosystem management. A variety of systems exist, based on vegetation cover, remote sensing, and bioclimatic classification systems. American geographer Robert Bailey defines a hierarchy of ecosystem units ranging from microecosystems (individual homogeneous sites, on the order of 10 km2 in area), through mesoecosystems (landscape mosaics, on the order of 1000 km2) to macroecosystems (ecoregions, on the order of 100000 km2).
Bailey outlined five different methods for identifying ecosystems: "gestalt" ("a whole that is not derived through considerable of its parts"), in which regions are recognized and boundaries drawn intuitively; a map overlay system where different layers like geology, landforms and soil types are overlain to identify ecosystems; multivariate clustering of site attributes; digital image processing of remotely sensed data grouping areas based on their appearance or other spectral properties; or by a "controlling factors method" where a subset of factors (like soils, climate, vegetation physiognomy or the distribution of plant or animal species) are selected from a large array of possible ones are used to delineate ecosystems. In contrast with Bailey's methodology, Puerto Rico ecologist Ariel Lugo and coauthors identified ten characteristics of an effective classification system: that it be based on georeferenced, quantitative data; that it should minimize subjectivity and explicitly identify criteria and assumptions; that it should be structured around the factors that drive ecosystem processes; that it should reflect the hierarchical nature of ecosystems; that it should be flexible enough to conform to the various scales at which ecosystem management operates; that it should be tied to reliable measures of climate so that it can "anticipat[e] global climate change; that it be applicable worldwide; that it should be validated against independent data; that it take into account the sometimes complex relationship between climate, vegetation and ecosystem functioning; and that it should be able to adapt and improve as new data become available".
Anthropogenic threats.
As human populations grow, so do the resource demands imposed on ecosystems and the impacts of the human ecological footprint. Natural resources are not invulnerable and infinitely available. The environmental impacts of anthropogenic actions, which are processes or materials derived from human activities, are becoming more apparent—air and water quality are increasingly compromised, oceans are being overfished, pests and diseases are extending beyond their historical boundaries, and deforestation is exacerbating flooding downstream. It has been reported that approximately 40–50% of Earth's ice-free land surface has been heavily transformed or degraded by anthropogenic activities, 66% of marine fisheries are either overexploited or at their limit, atmospheric CO2 has increased more than 30% since the advent of industrialization, and nearly 25% of Earth's bird species have gone extinct in the last two thousand years. Society is increasingly becoming aware that ecosystem services are not only limited, but also that they are threatened by human activities. The need to better consider long-term ecosystem health and its role in enabling human habitation and economic activity is urgent. To help inform decision-makers, many ecosystem services are being assigned economic values, often based on the cost of replacement with anthropogenic alternatives. The ongoing challenge of prescribing economic value to nature, for example through biodiversity banking, is prompting transdisciplinary shifts in how we recognize and manage the environment, social responsibility, business opportunities, and our future as a species.
Literature cited.
</dl>

</doc>
<doc id="9633" url="http://en.wikipedia.org/wiki?curid=9633" title="E (mathematical constant)">
E (mathematical constant)

The number e is an important mathematical constant that is the base of the natural logarithm. It is approximately equal to 2.71828, and is the limit of (1 + 1/"n")"n" as n approaches infinity, an expression that arises in the study of compound interest. It can also be calculated as the sum of the infinite series
The constant can be defined in many ways. For example, e can be defined as the unique positive number a such that the graph of the function "y" = "a""x" has unit slope at "x" = 0. The function "f"("x") = "e""x" is called the exponential function, and its inverse is the natural logarithm, or logarithm to base e. The natural logarithm of a positive number "k" can also be defined directly as the area under the curve "y" = 1/"x" between "x" = 1 and "x" = "k", in which case, e is the number whose natural logarithm is 1. There are alternative characterizations.
Sometimes called Euler's number after the Swiss mathematician Leonhard Euler, e is not to be confused with γ—the Euler–Mascheroni constant, sometimes called simply "Euler's constant". The number e is also known as Napier's constant, but Euler's choice of the symbol e is said to have been retained in his honor. The constant was discovered by the Swiss mathematician Jacob Bernoulli while studying compound interest.
The number e is of eminent importance in mathematics, alongside 0, 1, π and i. All five of these numbers play important and recurring roles across mathematics, and are the five constants appearing in one formulation of Euler's identity. Like the constant π, e is irrational: it is not a ratio of integers; and it is transcendental: it is not a root of "any" non-zero polynomial with rational coefficients. The numerical value of e truncated to 50 decimal places is
History.
The first references to the constant were published in 1618 in the table of an appendix of a work on logarithms by John Napier. However, this did not contain the constant itself, but simply a list of logarithms calculated from the constant. It is assumed that the table was written by William Oughtred. The discovery of the constant itself is credited to Jacob Bernoulli, who attempted to find the value of the following expression (which is in fact e):
The first known use of the constant, represented by the letter "b", was in correspondence from Gottfried Leibniz to Christiaan Huygens in 1690 and 1691. Leonhard Euler introduced the letter e as the base for natural logarithms, writing in a letter to Christian Goldbach of 25 November 1731. Euler started to use the letter e for the constant in 1727 or 1728, in an unpublished paper on explosive forces in cannons, and the first appearance of e in a publication was Euler's "Mechanica" (1736). While in the subsequent years some researchers used the letter "c", e was more common and eventually became the standard.
Applications.
Compound interest.
Jacob Bernoulli discovered this constant by studying a question about compound interest:
If the interest is credited twice in the year, the interest rate for each 6 months will be 50%, so the initial $1 is multiplied by 1.5 twice, yielding $1.00×1.52 = $2.25 at the end of the year. Compounding quarterly yields $1.00×1.254 = $2.4414..., and compounding monthly yields $1.00×(1+1/12)12 = $2.613035... If there are "n" compounding intervals, the interest for each interval will be 100%/"n" and the value at the end of the year will be $1.00×(1 + 1/"n")"n".
Bernoulli noticed that this sequence approaches a limit (the force of interest) with larger "n" and, thus, smaller compounding intervals. Compounding weekly ("n" = 52) yields $2.692597..., while compounding daily ("n" = 365) yields $2.714567..., just two cents more. The limit as "n" grows large is the number that came to be known as e; with "continuous" compounding, the account value will reach $2.7182818... More generally, an account that starts at $1 and offers an annual interest rate of "R" will, after "t" years, yield "e""Rt" dollars with continuous compounding. (Here "R" is a fraction, so for 5% interest, "R" = 5/100 = 0.05)
Bernoulli trials.
The number e itself also has applications to probability theory, where it arises in a way not obviously related to exponential growth. Suppose that a gambler plays a slot machine that pays out with a probability of one in "n" and plays it "n" times. Then, for large "n" (such as a million) the probability that the gambler will lose every bet is (approximately) 1/"e". For "n" = 20 it is already approximately 1/2.79.
This is an example of a Bernoulli trials process. Each time the gambler plays the slots, there is a one in one million chance of winning. Playing one million times is modelled by the binomial distribution, which is closely related to the binomial theorem. The probability of winning "k" times out of a million trials is;
In particular, the probability of winning zero times ("k" = 0) is
This is very close to the following limit for 1/"e":
Derangements.
Another application of e, also discovered in part by Jacob Bernoulli along with Pierre Raymond de Montmort is in the problem of derangements, also known as the "hat check problem": "n" guests are invited to a party, and at the door each guest checks his hat with the butler who then places them into "n" boxes, each labelled with the name of one guest. But the butler does not know the identities of the guests, and so he puts the hats into boxes selected at random. The problem of de Montmort is to find the probability that "none" of the hats gets put into the right box. The answer is:
As the number "n" of guests tends to infinity, "p""n" approaches 1/"e". Furthermore, the number of ways the hats can be placed into the boxes so that none of the hats is in the right box is "n"!/"e" rounded to the nearest integer, for every positive "n".
Asymptotics.
The number e occurs naturally in connection with many problems involving asymptotics. A prominent example is Stirling's formula for the asymptotics of the factorial function, in which both the numbers e and π enter:
A particular consequence of this is
Standard normal distribution.
The simplest case of a normal distribution is known as the "standard normal distribution", described by this probability density function:
The factor formula_10 in this expression ensures that the total area under the curve "ϕ"("x") is equal to one[proof]. The 1/2 in the exponent ensures that the distribution has unit variance (and therefore also unit standard deviation). This function is symmetric around "x"=0, where it attains its maximum value formula_10; and has inflection points at +1 and −1.
e in calculus.
The principal motivation for introducing the number e, particularly in calculus, is to perform differential and integral calculus with exponential functions and logarithms. A general exponential function has derivative given as the limit:
The limit on the far right is independent of the variable "x": it depends only on the base "a". When the base is e, this limit is equal to 1, and so e is symbolically defined by the equation:
Consequently, the exponential function with base e is particularly suited to doing calculus. Choosing e, as opposed to some other number, as the base of the exponential function makes calculations involving the derivative much simpler.
Another motivation comes from considering the base-"a" logarithm. Considering the definition of the derivative of log"a" "x" as the limit:
where the substitution was made in the last step. The last limit appearing in this calculation is again an undetermined limit that depends only on the base a, and if that base is e, the limit is equal to 1. So symbolically,
The logarithm in this special base is called the natural logarithm and is represented as ln; it behaves well under differentiation since there is no undetermined limit to carry through the calculations.
There are thus two ways in which to select a special number . One way is to set the derivative of the exponential function "a""x" to "a""x", and solve for "a". The other way is to set the derivative of the base "a" logarithm to 1/"x" and solve for "a". In each case, one arrives at a convenient choice of base for doing calculus. In fact, these two solutions for "a" are actually "the same", the number e.
Alternative characterizations.
Other characterizations of e are also possible: one is as the limit of a sequence, another is as the sum of an infinite series, and still others rely on integral calculus. So far, the following two (equivalent) properties have been introduced:
The following three characterizations can be proven equivalent:
Properties.
Calculus.
As in the motivation, the exponential function "e""x" is important in part because it is the unique nontrivial function (up to multiplication by a constant) which is its own derivative
and therefore its own antiderivative as well:
Exponential-like functions.
The global maximum for the function
occurs at . Similarly, is where the global minimum occurs for the function
defined for positive "x". More generally, is where the global minimum occurs for the function
for any n > 0. The infinite tetration
converges if and only if "e"−"e" ≤ "x" ≤ "e"1/"e" (or approximately between 0.0660 and 1.4447), due to a theorem of Leonhard Euler.
Number theory.
The real number e is irrational. Euler proved this by showing that its simple continued fraction expansion is infinite. (See also Fourier's proof that e is irrational.)
Furthermore, by the Lindemann–Weierstrass theorem, e is transcendental, meaning that it is not a solution of any non-constant polynomial equation with rational coefficients. It was the first number to be proved transcendental without having been specifically constructed for this purpose (compare with Liouville number); the proof was given by Charles Hermite in 1873.
It is conjectured that e is normal, meaning that when e is expressed in any base the possible digits in that base are uniformly distributed (occur with equal probability in any sequence of given length).
Complex numbers.
The exponential function "e""x" may be written as a Taylor series
Because this series keeps many important properties for "e""x" even when "x" is complex, it is commonly used to extend the definition of "e""x" to the complex numbers. This, with the Taylor series for sin and cos "x", allows one to derive Euler's formula:
which holds for all "x". The special case with is Euler's identity:
from which it follows that, in the principal branch of the logarithm,
Furthermore, using the laws for exponentiation,
which is de Moivre's formula.
The expression
is sometimes referred to as cis("x").
Differential equations.
The general function
is the solution to the differential equation:
Representations.
The number e can be represented as a real number in a variety of ways: as an infinite series, an infinite product, a continued fraction, or a limit of a sequence. The chief among these representations, particularly in introductory calculus courses is the limit
given above, as well as the series
given by evaluating the above power series for "e""x" at .
Less common is the continued fraction (sequence in OEIS).
which written out looks like
This continued fraction for e converges three times as quickly:
which written out looks like
Many other series, sequence, continued fraction, and infinite product representations of e have been developed.
Stochastic representations.
In addition to exact analytical expressions for representation of e, there are stochastic techniques for estimating e. One such approach begins with an infinite sequence of independent random variables "X"1, "X"2..., drawn from the uniform distribution on [0, 1]. Let "V" be the least number "n" such that the sum of the first "n" observations exceeds 1:
Then the expected value of "V" is e: .
Known digits.
The number of known digits of e has increased substantially during the last decades. This is due both to the increased performance of computers and to algorithmic improvements.
In computer culture.
In contemporary internet culture, individuals and organizations frequently pay homage to the number e.
For instance, in the IPO filing for Google in 2004, rather than a typical round-number amount of money, the company announced its intention to raise $2,718,281,828, which is e billion dollars rounded to the nearest dollar. Google was also responsible for a billboard that appeared in the heart of Silicon Valley, and later in Cambridge, Massachusetts; Seattle, Washington; and Austin, Texas. It read "{first 10-digit prime found in consecutive digits of e}.com". Solving this problem and visiting the advertised (now defunct) web site led to an even more difficult problem to solve, which in turn led to Google Labs where the visitor was invited to submit a resume. The first 10-digit prime in e is 7427466391, which starts at the 99th digit.
In another instance, the computer scientist Donald Knuth let the version numbers of his program Metafont approach e. The versions are 2, 2.7, 2.71, 2.718, and so forth. Similarly, the version numbers of his TeX program approach π.

</doc>
<doc id="9637" url="http://en.wikipedia.org/wiki?curid=9637" title="Euler–Maclaurin formula">
Euler–Maclaurin formula

In mathematics, the Euler–Maclaurin formula provides a powerful connection between integrals (see calculus) and sums. It can be used to approximate integrals by finite sums, or conversely to evaluate finite sums and infinite series using integrals and the machinery of calculus. For example, many asymptotic expansions are derived from the formula, and Faulhaber's formula for the sum of powers is an immediate consequence.
The formula was discovered independently by Leonhard Euler and Colin Maclaurin around 1735 (and later generalized as Darboux's formula). Euler needed it to compute slowly converging infinite series while Maclaurin used it to calculate integrals.
The formula.
If "m" and "n" are natural numbers and "f"("x") is an analytic function of exponential type < 2π defined for all real numbers "x" in the interval formula_1, then the integral
can be approximated by the sum (or vice versa)
(see trapezoidal rule). The Euler–Maclaurin formula provides expressions for the difference between the sum and the integral in terms of the higher derivatives "ƒ"("k") at the end points of the interval "m" and "n". Explicitly, for any natural number "p", we have
where "B"1 = +1/2, "B"2 = 1/6, "B"3 = 0, "B"4 = −1/30, "B"5 = 0, "B"6 = 1/42, "B"7 = 0, "B"8 = −1/30, … are the Bernoulli numbers, and "R" is an error term which is normally small for suitable values of "p" and depends on "n, m, p" and "f".
The formula is often written with the subscript taking only even values, since the odd Bernoulli numbers are zero except for "B"1, in which case we have
The remainder term.
The remainder term "R" is most easily expressed using the periodic Bernoulli polynomials "P""n"("x"). The Bernoulli polynomials "B""n"("x"), "n" = 0, 1, 2, … are defined recursively as
Then the periodic Bernoulli functions "P""n" are defined as
where formula_8 denotes the largest integer that
is not greater than "x". Then, in terms of "P""n"("x"), the remainder
term "R" can be written as
or equivalently, integrating by parts, assuming "ƒ"(2"p") is differentiable again and recalling that all odd Bernoulli numbers (but the first one) are zero:
When "n" > 0, it can be shown that
where "ζ" denotes the Riemann zeta function (see Lehmer; one approach to prove the inequality is to obtain the Fourier series for the polynomials "B""n"). The bound is achieved for even "n" when "x" is zero. Using this inequality, the size of the remainder term can be estimated using
Applicable formula.
In the end, we get the following simple formula:
Where 'N' is the number of points in the interval of integration, from formula_14 to formula_15.
This is just the trapezoid rule with correction terms.
Applications.
The Basel problem.
The Basel problem asks to determine the sum
Euler computed this sum to 20 decimal places with only a few terms of the Euler–Maclaurin formula in 1735. This probably convinced him that the sum equals π2 / 6, which he proved in the same year. Parseval's identity for the Fourier series of "f"("x") = "x" gives the same result.
Sums involving a polynomial.
If "f" is a polynomial and "p" is big enough, then the remainder term vanishes. For instance, if "f"("x") = "x"3, we can choose "p" = 2 to obtain after simplification
(see Faulhaber's formula).
Numerical integration.
The Euler–Maclaurin formula is also used for detailed error analysis in numerical quadrature. It explains the superior performance of the trapezoidal rule on smooth periodic functions and is used in certain extrapolation methods. Clenshaw–Curtis quadrature is essentially a change of variables to cast an arbitrary integral in terms of integrals of periodic functions where the Euler–Maclaurin approach is very accurate (in that particular case the Euler–Maclaurin formula takes the form of a discrete cosine transform). This technique is known as a periodizing transformation.
Asymptotic expansion of sums.
In the context of computing asymptotic expansions of sums and series, usually the most useful form of the Euler–Maclaurin formula is
where "a" and "b" are integers. Often the expansion remains valid even after taking the limits formula_19 or formula_20, or both. In many cases the integral on the right-hand side can be evaluated in closed form in terms of elementary functions even though the sum on the left-hand side cannot. Then all the terms in the asymptotic series can be expressed in terms of elementary functions. For example,
Here the left-hand side is equal to formula_22, namely the first-order polygamma function defined through formula_23; the gamma function formula_24 is equal to formula_25 if formula_26 is a positive integer. This results in an asymptotic expansion for formula_22. That expansion, in turn, serves as the starting point for one of the derivations of precise error estimates for Stirling's approximation of the factorial function.
Proofs.
Derivation by mathematical induction.
We follow the argument given in Apostol.
The Bernoulli polynomials "Bn"("x"), "n" = 0, 1, 2, … may be defined recursively as follows:
The first several of these are
The values "Bn"(0) are the Bernoulli numbers. Notice that for "n" ≠ 1 we have
For "n" = 1,
We define the periodic Bernoulli functions "P""n" by
where formula_35 denotes the largest integer that is not greater than "x". So "P""n" agree with the Bernoulli polynomials on the interval (0, 1) and are periodic with period 1. Thus,
Let "k" be an integer, and consider the integral
where
Integrating by parts, we get
Summing the above from "k" = 0 to "k" = "n" − 1, we get
Adding ("f"("n") - "f"(0))/2 to both sides and rearranging, we have
The last two terms therefore give the error when the integral is taken to approximate the sum.
Next, consider
where
Integrating by parts again, we get
Then summing from "k" = 0 to "k" = "n" − 1, and then replacing the last integral in (1) with what we have thus shown to be equal to it, we have
By now the reader will have guessed that this process can be iterated. In this way we get a proof of the Euler–Maclaurin summation formula by mathematical induction, in which the induction step relies on integration by parts and on the identities for periodic Bernoulli functions.
References.
</dl>

</doc>
<doc id="9638" url="http://en.wikipedia.org/wiki?curid=9638" title="Epimenides paradox">
Epimenides paradox

The Epimenides paradox reveals a problem with self-reference in logic.
It is named after the Cretan philosopher Epimenides of Knossos (alive circa 600 BC) who is credited with the original statement.
A typical description of the problem is given in the book "Gödel, Escher, Bach", by Douglas Hofstadter
A paradox of self-reference arises when one considers whether it is possible for Epimenides to have spoken the truth.
Logical paradox.
Thomas Fowler (1869) states the paradox as follows: "Epimenides the Cretan says, 'that all the Cretans are liars,' but Epimenides is himself a Cretan; therefore he is himself a liar. But if he be a liar, what he says is untrue, and consequently the Cretans are veracious; but Epimenides is a Cretan, and therefore what he says is true; saying the Cretans are liars, Epimenides is himself a liar, and what he says is untrue. Thus we may go on alternately proving that Epimenides and the Cretans are truthful and untruthful."
The Epimenides paradox in this form can however be solved, meaning that the statement can be assigned truth-value in a way that is not self-contradicting. Namely, if the statement "all Cretans are liars" (stated by Epimenides, himself a Cretan) is true, then Epimenides, being a Cretan, would be a liar; making the assumption that liars only make false statements, the statement should be false. So assuming the statement is true leads us to conclude that the statement is false and cannot be accepted. However, if we assume the statement is false, then its correct negation, "there exists a Cretan who is honest", is true. This does not lead to contradiction, since it is not required that this Cretan be Epimenides, meaning that Epimenides can tell false statements (honest people tell only true statements) and thus be a liar (Epimenides knows at least one honest Cretan and lies about it). Hence, from the assumption that the statement is false it does not follow that the statement is true. So we can avoid a paradox as seeing the statement "all Cretans are liars" as a false statement, which is made by a lying Cretan, Epimenides. The mistake made by Thomas Fowler (and many other people) above is to think that the negation of "all Cretans are liars" is "all Cretans are honest", when in fact the negation is "there exists a Cretan who is honest". The Epimenides paradox can be slightly modified as to not allow the kind of solution described above, like it was in the first paradox of Eubulides but instead leading to a non-avoidable self-contradiction. Paradoxical versions of the Epimenides problem are closely related to a class of more difficult logical problems, including the liar paradox, Socratic paradox, and the Burali-Forti paradox, all of which have self-reference in common with Epimenides. Indeed, the Epimenides paradox is usually classified as a variation on the liar paradox, and sometimes the two are not distinguished. The study of self-reference led to important developments in logic and mathematics in the twentieth century.
Origin of the phrase.
Epimenides was a 6th-century BC philosopher and religious prophet who, against the general sentiment of Crete, proposed that Zeus was immortal, as in the following poem:
 They fashioned a tomb for thee, O holy and high one<br>The Cretans, always liars, evil beasts, idle bellies!<br>But thou art not dead: thou livest and abidest forever,<br>For in thee we live and move and have our being.
 — Epimenides, "Cretica"
Denying the immortality of Zeus, then, was the lie of the Cretans.
The phrase "Cretans, always liars" was quoted by the poet Callimachus in his "Hymn to Zeus", with the same theological intent as Epimenides:
 O Zeus, some say that thou wert born on the hills of Ida;<br> Others, O Zeus, say in Arcadia;<br> Did these or those, O Father lie? -- “Cretans are ever liars.” <br>Yea, a tomb, O Lord, for thee the Cretans builded;<br> But thou didst not die, for thou art for ever.
 — Callimachus, "Hymn I to Zeus"
Emergence as a logical contradiction.
The logical inconsistency of a Cretan asserting all Cretans are always liars may not have occurred to Epimenides, nor to Callimachus, who both used the phrase to emphasize their point, without irony.
Later, in the 1st century AD, the quote is given similar credence as it had in the past:
 One of Crete's own prophets has said it: 'Cretans are always liars, evil brutes, idle bellies'.<br>He has surely told the truth. For this reason correct them sternly, that they may be sound in faith instead of paying attention to Jewish fables and to commandments of people who turn their backs on the truth.
 — Epistle to Titus, 1:12–13
Nor does Clement of Alexandria, in the late 2nd century AD, indicate that the concept of logical paradox is an issue:
 In his epistle to Titus, Apostle Paul wants to warn Titus that Cretans don't believe in the one truth of Christianity, because "Cretans are always liars". To justify his claim, Apostle Paul cites Epimenides. 
 — Stromata 1.14
During the early 4th century, Saint Augustine restates the closely related liar paradox in "Against the Academicians" (III.13.29), but without mentioning Epimenides.
In the Middle Ages, many forms of the liar paradox were studied under the heading of insolubilia, but these were not explicitly associated with Epimenides.
Finally, in 1740, the second volume of Pierre Bayle's "Dictionnaire Historique et Critique" explicitly connects Epimenides with the paradox, though Bayle labels the paradox a "sophisme".
References by other authors.
All of the works of Epimenides are now lost, and known only through quotations by other authors. The quotation from the "Cretica" of Epimenides is given by R.N. Longenecker, "Acts of the Apostles", in volume 9 of "The Expositor's Bible Commentary", Frank E. Gaebelein, editor (Grand Rapids, Michigan: Zondervan Corporation, 1976–1984), page 476. Longenecker in turn cites M.D. Gibson, "Horae Semiticae X" (Cambridge: Cambridge University Press, 1913), page 40, "in Syriac". Longenecker states the following in a footnote:
An oblique reference to Epimenides in the context of logic appears in "The Logical Calculus" by W. E. Johnson, "Mind" (New Series), volume 1, number 2 (April, 1892), pages 235–250. Johnson writes in a footnote,
The Epimenides paradox appears explicitly in "Mathematical Logic as Based on the Theory of Types", by Bertrand Russell, in the "American Journal of Mathematics", volume 30, number 3 (July, 1908), pages 222–262, which opens with the following:
In that article, Russell uses the Epimenides paradox as the point of departure for discussions of other problems, including the Burali-Forti paradox and the paradox now called Russell's paradox. Since Russell, the Epimenides paradox has been referenced repeatedly in logic. Typical of these references is "Gödel, Escher, Bach" by Douglas Hofstadter, which accords the paradox a prominent place in a discussion of self-reference.

</doc>
<doc id="9640" url="http://en.wikipedia.org/wiki?curid=9640" title="Engine">
Engine

An engine, or motor, is a machine designed to convert one form of energy into mechanical energy. Heat engines, including internal combustion engines and external combustion engines (such as steam engines) burn a fuel to create heat, which then creates a force. Electric motors convert electrical energy into mechanical motion, pneumatic motors use compressed air and others—such as clockwork motors in wind-up toys—use elastic energy. In biological systems, molecular motors, like myosins in muscles, use chemical energy to create forces and eventually motion.
Terminology.
"Engine" was originally a term for any mechanical device that converts force into motion. Hence, pre-industrial weapons such as catapults, trebuchets and battering rams were called "siege engines". The word "gin," as in "cotton gin", is short for "engine." The word derives from Old French "engin", from the Latin "ingenium", which is also the root of the word "ingenious". Most mechanical devices invented during the industrial revolution were described as engines—the steam engine being a notable example.
In modern usage, the term "engine" typically describes devices, like steam engines and internal combustion engines, that burn or otherwise consume fuel to perform mechanical work by exerting a torque or linear force (usually in the form of thrust). Examples of engines which exert a torque include the familiar automobile gasoline and diesel engines, as well as turboshafts. Examples of engines which produce thrust include turbofans and rockets.
When the internal combustion engine was invented, the term "motor" was initially used to distinguish it from the steam engine—which was in wide use at the time, powering locomotives and other vehicles such as steam rollers. "Motor" and "engine" later came to be used interchangeably in casual discourse. However, technically, the two words have different meanings. An "engine" is a device that burns or otherwise consumes fuel, changing its chemical composition, whereas a motor is a device driven by electricity, which does not change the chemical composition of its energy source.
A heat engine may also serve as a "prime mover"—a component that transforms the flow or changes in pressure of a fluid into mechanical energy. An automobile powered by an internal combustion engine may make use of various motors and pumps, but ultimately all such devices derive their power from the engine. Another way of looking at it is that a motor receives power from an external source, and then converts it into mechanical energy, while an engine creates power from pressure (derived directly from the explosive force of combustion or other chemical reaction, or secondarily from the action of some such force on other substances such as air, water, or steam).
Devices converting heat energy into motion are commonly referred to simply as "engines".
History.
Antiquity.
Simple machines, such as the club and oar (examples of the lever), are prehistoric. More complex engines using human power, animal power, water power, wind power and even steam power date back to antiquity. Human power was focused by the use of simple engines, such as the capstan, windlass or treadmill, and with ropes, pulleys, and block and tackle arrangements; this power was transmitted usually with the forces multiplied and the speed reduced. These were used in cranes and aboard ships in Ancient Greece, as well as in mines, water pumps and siege engines in Ancient Rome. The writers of those times, including Vitruvius, Frontinus and Pliny the Elder, treat these engines as commonplace, so their invention may be more ancient. By the 1st century AD, cattle and horses were used in mills, driving machines similar to those powered by humans in earlier times.
According to Strabo, a water powered mill was built in Kaberia of the kingdom of Mithridates during the 1st century BC. Use of water wheels in mills spread throughout the Roman Empire over the next few centuries. Some were quite complex, with aqueducts, dams, and sluices to maintain and channel the water, along with systems of gears, or toothed-wheels made of wood and metal to regulate the speed of rotation. More sophisticated small devices, such as the Antikythera Mechanism used complex trains of gears and dials to act as calendars or predict astronomical events. In a poem by Ausonius in the 4th century AD, he mentions a stone-cutting saw powered by water. Hero of Alexandria is credited with many such wind and steam powered machines in the 1st century AD, including the Aeolipile and the vending machine, often these machines were associated with worship, such as animated altars and automated temple doors.
Medieval.
Medieval Muslim engineers employed gears in mills and water-raising machines, and used dams as a source of water power to provide additional power to watermills and water-raising machines. In the medieval Islamic world, such advances made it possible to mechanize many industrial tasks previously carried out by manual labour.
In 1206, al-Jazari employed a crank-conrod system for two of his water-raising machines. A rudimentary steam turbine device was described by Taqi al-Din in 1551 and by Giovanni Branca in 1629.
In the 13th century, the solid rocket motor was invented in China. Driven by gunpowder, this, the simplest form of internal combustion engine was unable to deliver sustained power, but was useful for propelling weaponry at high speeds towards enemies in battle and for fireworks. After invention, this innovation spread throughout Europe.
Industrial Revolution.
The Watt steam engine was the first type of steam engine to make use of steam at a pressure just above atmospheric to drive the piston helped by a partial vacuum. Improving on the design of the 1712 Newcomen steam engine, the Watt steam engine, developed sporadically from 1763 to 1775, was a great step in the development of the steam engine. Offering a dramatic increase in fuel efficiency, James Watt's design became synonymous with steam engines, due in no small part to his business partner, Matthew Boulton. It enabled rapid development of efficient semi-automated factories on a previously unimaginable scale in places where waterpower was not available. Later development led to steam locomotives and great expansion of railway transportation.
As for internal combustion piston engines, these were tested in France in 1807 by de Rivaz and independently, by the Niépce brothers . They were theoretically advanced by Carnot in 1824. In 1853-57 Eugenio Barsanti and Felice Matteucci invented and patented an engine using the free-piston principle that was possibly the first 4-cycle engine. The Otto cycle in 1877 was capable of giving a far higher power to weight ratio than steam engines and worked much better for many transportation applications such as cars and aircraft.
Automobiles.
The first commercially successful automobile, created by Karl Benz, added to the interest in light and powerful engines. The lightweight petrol internal combustion engine, operating on a four-stroke Otto cycle, has been the most successful for light automobiles, while the more efficient Diesel engine is used for trucks and buses. However, in recent years, turbo Diesel engines have become increasingly popular, especially outside of the United States, even for quite small cars.
Horizontally opposed pistons.
In 1896, Karl Benz was granted a patent for his design of the first engine with horizontally opposed pistons. His design created an engine in which the corresponding pistons move in horizontal cylinders and reach top dead center simultaneously, thus automatically balancing each other with respect to their individual momentum. Engines of this design are often referred to as flat engines because of their shape and lower profile. They are or were used in: the Volkswagen Beetle, some Porsche and Subaru cars, many BMW and Honda motorcycles, and aircraft engines (for propeller driven aircraft), etc.
Advancement.
Continuance of the use of the internal combustion engine for automobiles is partly due to the improvement of engine control systems (onboard computers providing engine management processes, and electronically controlled fuel injection). Forced air induction by turbocharging and supercharging have increased power outputs and engine efficiencies. Similar changes have been applied to smaller diesel engines giving them almost the same power characteristics as petrol engines. This is especially evident with the popularity of smaller diesel engine propelled cars in Europe. Larger diesel engines are still often used in trucks and heavy machinery, although they require special machining not available in most factories. Diesel engines produce lower hydrocarbon and CO2 emissions, but greater particulate and NOx pollution, than gasoline engines. Diesel engines are also 40% more fuel efficient than comparable gasoline engines.
Increasing power.
The first half of the 20th century saw a trend to increasing engine power, particularly in the American models. Design changes incorporated all known methods of raising engine capacity, including increasing the pressure in the cylinders to improve efficiency, increasing the size of the engine, and increasing the rate at which the engine produces work. The higher forces and pressures created by these changes created engine vibration and size problems that led to stiffer, more compact engines with V and opposed cylinder layouts replacing longer straight-line arrangements.
Combustion efficiency.
The design principles favoured in Europe, because of economic and other restraints such as smaller and twistier roads, leant toward smaller cars and corresponding to the design principles that concentrated on increasing the combustion efficiency of smaller engines. This produced more economical engines with earlier four-cylinder designs rated at 40 horsepower (30 kW) and six-cylinder designs rated as low as 80 horsepower (60 kW), compared with the large volume V-8 American engines with power ratings in the range from 250 to 350 hp, some even over 400 hp (190 to 260 kW).
Engine configuration.
Earlier automobile engine development produced a much larger range of engines than is in common use today. Engines have ranged from 1- to 16-cylinder designs with corresponding differences in overall size, weight, engine displacement, and cylinder bores. Four cylinders and power ratings from 19 to 120 hp (14 to 90 kW) were followed in a majority of the models. Several three-cylinder, two-stroke-cycle models were built while most engines had straight or in-line cylinders. There were several V-type models and horizontally opposed two- and four-cylinder makes too. Overhead camshafts were frequently employed. The smaller engines were commonly air-cooled and located at the rear of the vehicle; compression ratios were relatively low. The 1970s and 1980s saw an increased interest in improved fuel economy, which caused a return to smaller V-6 and four-cylinder layouts, with as many as five valves per cylinder to improve efficiency. The Bugatti Veyron 16.4 operates with a W16 engine, meaning that two V8 cylinder layouts are positioned next to each other to create the W shape sharing the same crankshaft.
The largest internal combustion engine ever built is the Wärtsilä-Sulzer RTA96-C, a 14-cylinder, 2-stroke turbocharged diesel engine that was designed to power the "Emma Mærsk", the largest container ship in the world. This engine weighs 2,300 tons, and when running at 102 RPM produces 109,000 bhp (80,080 kW) consuming some 13.7 tons of fuel each hour.
Types.
An engine can be put into a category according to two criteria: the form of energy it accepts in order to create motion, and the type of motion it outputs.
Heat engine.
Combustion engine.
Combustion engines are heat engines driven by the heat of a combustion process.
Internal combustion engine.
The internal combustion engine is an engine in which the combustion of a fuel (generally, fossil fuel) occurs with an oxidizer (usually air) in a combustion chamber. In an internal combustion engine the expansion of the high temperature and high pressure gases, which are produced by the combustion, directly applies force to components of the engine, such as the pistons or turbine blades or a nozzle, and by moving it over a distance, generates useful mechanical energy.
External combustion engine.
An external combustion engine (EC engine) is a heat engine where an internal working fluid is heated by combustion of an external source, through the engine wall or a heat exchanger. The fluid then, by expanding and acting on the mechanism of the engine produces motion and usable work. The fluid is then cooled, compressed and reused (closed cycle), or (less commonly) dumped, and cool fluid pulled in (open cycle air engine).
"Combustion" refers to burning fuel with an oxidizer, to supply the heat. Engines of similar (or even identical) configuration and operation may use a supply of heat from other sources such as nuclear, solar, geothermal or exothermic reactions not involving combustion; but are not then strictly classed as external combustion engines, but as external thermal engines.
The working fluid can be a gas as in a Stirling engine, or steam as in a steam engine or an organic liquid such as n-pentane in an Organic Rankine cycle. The fluid can be of any composition; gas is by far the most common, although even single-phase liquid is sometimes used. In the case of the steam engine, the fluid changes phases between liquid and gas.
Air-breathing combustion engines.
Air-breathing combustion engines are combustion engines that use the oxygen in atmospheric air to oxidise ('burn') the fuel, rather than carrying an oxidiser, as in a rocket. Theoretically, this should result in a better specific impulse than for rocket engines.
A continuous stream of air flows through the air-breathing engine. This air is compressed, mixed with fuel, ignited and expelled as the exhaust gas.
Typical air-breathing engines include:
Environmental effects.
The operation of engines typically has a negative impact upon air quality and ambient sound levels. There has been a growing emphasis on the pollution producing features of automotive power systems. This has created new interest in alternate power sources and internal-combustion engine refinements. Though a few limited-production battery-powered electric vehicles have appeared, they have not proved competitive owing to costs and operating characteristics. In the 21st century the diesel engine has been increasing in popularity with automobile owners. However, the gasoline engine and the Diesel engine, with their new emission-control devices to improve emission performance, have not yet been significantly challenged. A number of manufacturers have introduced hybrid engines, mainly involving a small gasoline engine coupled with an electric motor and with a large battery bank, but these too have yet to make much of an inroad into the market shares of gasoline and Diesel engines.
Air quality.
Exhaust from a spark ignition engine consists of the following: nitrogen 70 to 75% (by volume), water vapor 10 to 12%, carbon dioxide 10 to 13.5%, hydrogen 0.5 to 2%, oxygen 0.2 to 2%, carbon monoxide: 0.1 to 6%, unburnt hydrocarbons and partial oxidation products (e.g. aldehydes) 0.5 to 1%, nitrogen monoxide 0.01 to 0.4%, nitrous oxide <100 ppm, sulfur dioxide 15 to 60 ppm, traces of other compounds such as fuel additives and lubricants, also halogen and metallic compounds, and other particles. Carbon monoxide is highly toxic, and can cause carbon monoxide poisoning, so it is important to avoid any build-up of the gas in a confined space. Catalytic converters can reduce toxic emissions, but not completely eliminate them. Also, resulting greenhouse gas emissions, chiefly carbon dioxide, from the widespread use of engines in the modern industrialized world is contributing to the global greenhouse effect – a primary concern regarding global warming.
Non-combusting heat engines.
Some engines convert heat from noncombustive processes into mechanical work, for example a nuclear power plant uses the heat from the nuclear reaction to produce steam and drive a steam engine, or a gas turbine in a rocket engine may be driven by decomposing hydrogen peroxide. Apart from the different energy source, the engine is often engineered much the same as an internal or external combustion engine. Another group of noncombustive engines includes thermoacoustic heat engines (sometimes called "TA engines") which are thermoacoustic devices which use high-amplitude sound waves to pump heat from one place to another, or conversely use a heat difference to induce high-amplitude sound waves. In general, thermoacoustic engines can be divided into standing wave and travelling wave devices.
Non-thermal chemically powered motor.
Non-thermal motors usually are powered by a chemical reaction, but are not heat engines. Examples include:
Electric motor.
An electric motor uses electrical energy to produce mechanical energy, usually through the interaction of magnetic fields and current-carrying conductors. The reverse process, producing electrical energy from mechanical energy, is accomplished by a generator or dynamo. Traction motors used on vehicles often perform both tasks. Electric motors can be run as generators and vice versa, although this is not always practical.
Electric motors are ubiquitous, being found in applications as diverse as industrial fans, blowers and pumps, machine tools, household appliances, power tools, and disk drives. They may be powered by direct current (for example a battery powered portable device or motor vehicle), or by alternating current from a central electrical distribution grid. The smallest motors may be found in electric wristwatches. Medium-size motors of highly standardized dimensions and characteristics provide convenient mechanical power for industrial uses. The very largest electric motors are used for propulsion of large ships, and for such purposes as pipeline compressors, with ratings in the thousands of kilowatts. Electric motors may be classified by the source of electric power, by their internal construction, and by their application.
The physical principle of production of mechanical force by the interactions of an electric current and a magnetic field was known as early as 1821. Electric motors of increasing efficiency were constructed throughout the 19th century, but commercial exploitation of electric motors on a large scale required efficient electrical generators and electrical distribution networks.
To reduce the electric energy consumption from motors and their associated carbon footprints, various regulatory authorities in many countries have introduced and implemented legislation to encourage the manufacture and use of higher efficiency electric motors. A well-designed motor can convert over 90% of its input energy into useful power for decades. When the efficiency of a motor is raised by even a few percentage points, the savings, in kilowatt hours (and therefore in cost), are enormous. The electrical energy efficiency of a typical industrial induction motor can be improved by: 1) reducing the electrical losses in the stator windings (e.g., by increasing the cross-sectional area of the conductor, improving the winding technique, and using materials with higher electrical conductivities, such as copper), 2) reducing the electrical losses in the rotor coil or casting (e.g., by using materials with higher electrical conductivities, such as copper), 3) reducing magnetic losses by using better quality magnetic steel, 4) improving the aerodynamics of motors to reduce mechanical windage losses, 5) improving bearings to reduce friction losses, and 6) minimizing manufacturing tolerances. "For further discussion on this subject, see Premium efficiency and Copper in energy efficient motors.)"
By convention, "electric engine" refers to a railroad electric locomotive, rather than an electric motor.
Physically powered motor.
Some motors are powered by potential or kinetic energy, for example some funiculars, gravity plane and ropeway conveyors have used the energy from moving water or rocks, and some clocks have a weight that falls under gravity. Other forms of potential energy include compressed gases (such as pneumatic motors), springs (clockwork motors) and elastic bands.
Historic military siege engines included large catapults, trebuchets, and (to some extent) battering rams were powered by potential energy.
Pneumatic motor.
A pneumatic motor is a machine that converts potential energy in the form of compressed air into mechanical work. Pneumatic motors generally convert the compressed air to mechanical work though either linear or rotary motion. Linear motion can come from either a diaphragm or piston actuator, while rotary motion is supplied by either a vane type air motor or piston air motor. Pneumatic motors have found widespread success in the hand-held tool industry and continual attempts are being made to expand their use to the transportation industry. However, pneumatic motors must overcome efficiency deficiencies before being seen as a viable option in the transportation industry.
Hydraulic motor.
A hydraulic motor is one that derives its power from a pressurized fluid. This type of engine can be used to move heavy loads or produce motion.
Performance.
Engine speed.
In the case of engines outputting shaft power, engine speed is measured in revolutions per minute (RPM). Engines may be classified as low-speed, medium-speed or high-speed but these terms are inexact and depend on the type of engine being described. Generally, diesel engines operate at lower speed compared to gasoline engines. Electric motors and turboshafts are capable of very high speeds. In the case of engines producing thrust, it is rather inaccurate to talk of an 'engine speed' since what is moving is not the engine, but the working medium that the engine is accelerating; in this case one talks of an exhaust velocity, which is exactly the Isp outside of a gravitational field and therefore makes one jump straight to a discussion of efficiency; see the article on specific impulse for more information.
Thrust.
Thrust is the force arising from the interaction between two masses which exert equal but opposite forces on each other due to their speed. The force F can be measured either in newtons (N, SI units) or in pounds-thrust (lbf, imperial units).
Torque.
Torque is the force being exerted on a theoretical lever connected to the output shaft of an engine. This is expressed by the formula:
where "r" is the length of the lever, "F" is the force applied on it, and r×F is the vector cross product.
Torque is measured typically either in newton-metres (N·m, SI units) or in foot-pounds (ft·lb, imperial units).
Power.
Power is the amount of work being done, or energy being produced, per unit of time. This is expressed by the formula:
With a quick demonstration, it can be shown that:
This formula with linear forces and speeds can be used equally well for both engines outputting thrust and engines exerting torque.
When considering propulsive engines, typically only the raw force of the core mass flow is considered, leading to such engines having their 'power' rated in any of the units discussed above for forces.
If the engine in question outputs its power on a shaft, then:
This is the reason why any engine outputting its power on a rotating shaft always informs, along with its rated power, the rotational speed at which that rated power is developed.
Typically, among engines driving a rotating shaft, combustion engines have their power rated in horsepower (hp), while electric engines have their power rated in watts (W, not to be confused with the mathematical symbol for work) or multiples thereof.
Efficiency.
Depending on the type of engine employed, different rates of efficiency are attained.
For heat engines, efficiency cannot be greater than the Carnot efficiency.
Sound levels.
In the case of sound levels, engine operation is of greatest impact with respect to mobile sources such as automobiles and trucks. Engine noise is a particularly large component of mobile source noise for vehicles operating at lower speeds, where aerodynamic and tire noise is less significant. Generally speaking, petrol and diesel engines emit less noise than turboshafts of equivalent power output; electric motors very often emit less noise than their fossil fuel-powered equivalents. Thrust-outputting engines, such as turbofans, turbojets and rockets emit the greatest amount of noise because their method of producing thrust is directly related to the production of sound.
Various methods have been devised to reduce noise. Petrol and diesel engines are fitted with mufflers (silencers); newer turbofans often have outsized fans (the so-called high-bypass technology) in order to reduce the proportion of noisy, hot exhaust from the integrated turboshaft in the exhaust stream, and hushkits exist for older, low-bypass turbofans. No known methods exist for reducing the noise output of rockets without a corresponding reduction in thrust.
Engines by use.
Particularly notable kinds of engines include:
References.
</dl>

</doc>
<doc id="9643" url="http://en.wikipedia.org/wiki?curid=9643" title="Economic and monetary union">
Economic and monetary union

An economic and monetary union is a type of trade bloc which is composed of an economic union (common market and customs union) with a monetary union. It is to be distinguished from a mere monetary union (e.g. the Latin Monetary Union in the 19th century), which does not involve a common market. This is the sixth stage of economic integration. EMU is established through a currency-related trade pact. An intermediate step between pure EMU and a complete economic integration is the fiscal union.
List of economic and monetary unions.
Additionally the autonomous and dependent territories, such as some of the EU member state special territories, are sometimes treated as separate customs territory from their mainland state or have varying arrangements of formal or de facto customs union, common market and currency union (or combinations thereof) with the mainland and in regards to third countries through the trade pacts signed by the mainland state.

</doc>
<doc id="9644" url="http://en.wikipedia.org/wiki?curid=9644" title="European Environment Agency">
European Environment Agency

The European Environment Agency (EEA) is the agency of the European Union (EU) that provides independent information on the environment, thereby helping those involved in developing, adopting, implementing and evaluating environmental policy, as well as informing the general public. The agency is governed by a management board composed of representatives of the governments of its 33 member states, a European Commission representative and two scientists appointed by the European Parliament, assisted by a committee of scientists.
The EEA was established by the European Economic Community (EEC) Regulation 1210/1990 (amended by EEC Regulation 933/1999 and EC Regulation 401/2009) and became operational in 1994. It is headquartered in Copenhagen, Denmark.
The current Executive Director of the agency is Professor Hans Bruyninckx, who has been appointed for a five-year term. He is the successor of Professor Jacqueline McGlade.
The member states of the union are members; however the Council Regulation establishing it provided that other states may become members of it by means of agreements concluded between them and the EU. 
It was the first EU body to open its membership to the 13 candidate countries (pre-2004 enlargement).
The EEA has 33 member countries and six cooperating countries. The European environment information and observation network (Eionet) is a partnership network of the EEA and the countries. The EEA is responsible for developing the network and coordinating its activities. To do so, the EEA works closely together with national focal points, typically national environment agencies or environment ministries. They are responsible for coordinating national networks involving many institutions (about 350 in all). 
The 33 member countries include the 28 European Union Member States together with Iceland, Liechtenstein, Norway, Switzerland and Turkey.
The six Balkans countries are cooperating countries: Albania, Bosnia and Herzegovina, the Republic of Macedonia, Montenegro, Serbia as well as Kosovo under the UN Security Council Resolution 1244/99. These cooperation activities are integrated into Eionet and are supported by the European Union under the Instrument for Pre-Accession Assistance.
The EEA is an active member of the EPA Network.
Member countries.
The 33 member countries include the 28 European Union member states together with Iceland, Liechtenstein, Norway, Switzerland and Turkey. The six Western Balkan countries are cooperating countries: Albania, Bosnia and Herzegovina, the Republic of Macedonia, Montenegro, Serbia as well as Kosovo under the UN Security Council Resolution 1244/99.
European environment information and observation network.
The European environment information and observation network (Eionet) is a partnership network of the EEA and its member and cooperating countries. The EEA is responsible for developing the network and coordinating its activities. To do this, the EEA works closely together with the National Focal Points (NFPs), typically national environment agencies or environment ministries in the member countries.
The NFPs are responsible for coordinating networks of the National Reference Centres (NRCs), bringing altogether around 1000 experts from over 350 national institutions and other bodies dealing with environmental information.
Apart from the NFPs and NRCs, Eionet currently covers six European Topic Centres (ETCs) in the areas of air and climate change, biological diversity, climate change impacts, vulnerability and adaptation, water, land use and spatial information and analysis and sustainable consumption and production.
Annual discharge process.
On February 2012, the European Parliament's Committee on Budgetary Control published a draft report identifying potential areas of concern in the use of funds and influence for the 2010 budget. The EEA's Executive Director refuted allegations of irregularities in a public hearing 
Members of the European Parliament (MEPs) voted on the report on 27 March 2012 and commended the cooperation between the Agency and NGOs working in the environmental area. On 23 October 2012, the European Parliament voted and granted the discharge to the European Environment Agency for its 2010 budget.
On 17 April 2013, the European Parliament (MEPs) voted and granted the discharge to the European Environment Agency for its 2011 budget.
International cooperation.
In addition to its 33 members and six Balkan cooperating countries, the EEA also cooperates and fosters partnerships with its neighbours and other countries and regions, mostly in the context of the European Neighbourhood Policy:
Additionally the EEA cooperates with multiple international organizations and the corresponding agencies of the following countries:
Official languages.
The 26 official languages used by the EEA are: Bulgarian, Czech, Croatian, Danish, German, Greek, English, Spanish, Estonian, Finnish, French, Hungarian, Icelandic, Italian, Lithuanian, Latvian, Malti, Dutch, Norwegian, Polish, Portuguese, Romanian, Slovak, Slovene, Swedish and Turkish.

</doc>
<doc id="9645" url="http://en.wikipedia.org/wiki?curid=9645" title="EV">
EV

EV and similar forms may refer to:

</doc>
<doc id="9646" url="http://en.wikipedia.org/wiki?curid=9646" title="Erlang (programming language)">
Erlang (programming language)

Erlang ( ) is a general-purpose, concurrent, garbage-collected programming language and runtime system. The sequential subset of Erlang is almost a functional language (excluding certain BIFs such as those manipulating the process dictionary), with eager evaluation, single assignment, and dynamic typing. It was designed by Ericsson to support distributed, fault-tolerant, soft-real-time, non-stop applications. It supports hot swapping, so that code can be changed without stopping a system.
While threads require external library support in most languages, Erlang provides language-level features for creating and managing processes with the aim of simplifying concurrent programming. Though all concurrency is explicit in Erlang, processes communicate using message passing instead of shared variables, which removes the need for explicit locks (a locking scheme is still used internally by the VM).
The first version was developed by Joe Armstrong, Robert Virding and Mike Williams in 1986. It was originally a proprietary language within Ericsson, but was released as open source in 1998.
History.
The name "Erlang", attributed to Bjarne Däcker, has been presumed by those working on the telephony switches (for whom the language was designed) to be a reference to Danish mathematician and engineer Agner Krarup Erlang or the ubiquitous use of the unit named for him, and (initially at least) simultaneously as a syllabic abbreviation of "Ericsson Language".
Erlang was designed with the aim of improving the development of telephony applications. The initial version of Erlang was implemented in Prolog and was influenced by the programming language PLEX used in earlier Ericsson exchanges. According to Armstrong, the language went from lab product to real applications following the collapse of the next-generation AXE exchange named "AXE-N" in 1995. As a result, Erlang was chosen for the next ATM exchange "AXD".
In 1998 Ericsson announced the AXD301 switch, containing over a million lines of Erlang and reported to achieve an availability of nine "9"s. Shortly thereafter, Ericsson Radio Systems banned the in-house use of Erlang for new products, citing a preference for non-proprietary languages. The ban caused Armstrong and others to leave Ericsson. The implementation was open-sourced at the end of the year. Ericsson eventually lifted the ban; it re-hired Armstrong in 2004.
In 2006, native symmetric multiprocessing support was added to the runtime system and virtual machine.
As Tim Bray, director of Web Technologies at Sun Microsystems, expressed in his keynote at OSCON in July 2008:
Functional programming examples.
An Erlang function that uses recursion to count to ten: 
A factorial algorithm implemented in Erlang:
A Fibonacci algorithm implemented in Erlang (Note: This is only for demonstrating the Erlang syntax. This algorithm is rather slow.):
A sorting algorithm (similar to quicksort):
The above example recursively invokes the function codice_1 until nothing remains to be sorted. The expression codice_2 is a list comprehension, meaning “Construct a list of elements codice_3 such that codice_3 is a member of codice_5, and codice_3 is less than codice_7.” codice_8 is the list concatenation operator.
A comparison function can be used for more complicated structures for the sake of readability.
The following code would sort lists according to length:
Here again, a codice_7 is taken from the first parameter given to codice_10 and the rest of codice_11 is named codice_5. Note that the expression
is no different in form from
(in the previous example) except for the use of a comparison function in the last part, saying “Construct a list of elements codice_13 such that codice_13 is a member of codice_5, and codice_16 is true", with codice_16 being defined earlier as
Note also that the anonymous function is named codice_16 in the parameter list of the second definition of codice_1 so that it can be referenced by that name within that function. It is not named in the first definition of codice_1, which deals with the base case of an empty list and thus has no need of this function, let alone a name for it.
Data types.
Erlang has eight primitive data types:
And two compound data types:
Two forms of syntactic sugar are provided:
Erlang has no method of defining classes, although there are external libraries available.
Concurrency and distribution orientation.
Erlang's main strength is support for concurrency. It has a small but powerful set of primitives to create processes and communicate among them. Processes are the primary means to structure an Erlang application. Erlang's concurrency implementation is the Actor model. They are neither operating system processes nor operating system threads, but lightweight processes. Like operating system processes (but unlike operating system threads), they share no state with each other. The estimated minimal overhead for each is 300 words. Thus, many processes can be created without degrading performance. A benchmark with 20 million processes has been successfully performed. Erlang has supported symmetric multiprocessing since release R11B of May 2006.
Inter-process communication works via a shared-nothing asynchronous message passing system: every process has a "mailbox", a queue of messages that have been sent by other processes and not yet consumed. A process uses the codice_34 primitive to retrieve messages that match desired patterns. A message-handling routine tests messages in turn against each pattern, until one of them matches. When the message is consumed and removed from the mailbox the process resumes execution. A message may comprise any Erlang structure, including primitives (integers, floats, characters, atoms), tuples, lists, and functions.
The code example below shows the built-in support for distributed processes:
As the example shows, processes may be created on remote nodes, and communication with them is transparent in the sense that communication with remote processes works exactly as communication with local processes.
Concurrency supports the primary method of error-handling in Erlang. When a process crashes, it neatly exits and sends a message to the controlling process which can take action.
Implementation.
The Ericsson Erlang implementation loads virtual machine bytecode which is converted to threaded code at load time. It also includes a native code compiler on most platforms, developed by the High Performance Erlang Project (HiPE) at Uppsala University. Since October 2001 the HiPE system is fully integrated in Ericsson's Open Source Erlang/OTP system. It also supports interpreting, directly from source code via abstract syntax tree, via script as of R11B-5 release of Erlang.
Hot code loading and modules.
Erlang supports language-level Dynamic Software Updating. To implement this, code is loaded and managed as "module" units; the module is a compilation unit. The system can keep two versions of a module in memory at the same time, and processes can concurrently run code from each. The versions are referred to as the "new" and the "old" version. A process will not move into the new version until it makes an external call to its module.
An example of the mechanism of hot code loading:
For the second version, we add the possibility to reset the count to zero.
Only when receiving a message consisting of the atom 'code_switch' will the loop execute an external call to codeswitch/1 (codice_35 is a preprocessor macro for the current module). If there is a new version of the "counter" module in memory, then its codeswitch/1 function will be called. The practice of having a specific entry-point into a new version allows the programmer to transform state to what is required in the newer version. In our example we keep the state as an integer.
In practice, systems are built up using design principles from the Open Telecom Platform which leads to more code upgradable designs. Successful hot code loading is a tricky subject; Code needs to be written to make use of Erlang's facilities.
Distribution.
In 1998, Ericsson released Erlang as open source to ensure its independence from a single vendor and to increase awareness of the language. Erlang, together with libraries and the real-time distributed database Mnesia, forms the Open Telecom Platform (OTP) collection of libraries. Ericsson and a few other companies offer commercial support for Erlang.
Since the open source release, Erlang has been used by several firms worldwide, including Nortel and T-Mobile. Although Erlang was designed to fill a niche and has remained an obscure language for most of its existence, its popularity is growing due to demand for concurrent services.
Erlang has found some use in fielding MMORPG servers.
Projects using Erlang.
Projects using Erlang include:
Companies using Erlang.
Companies using Erlang in their production systems include:
Further reading.
</dl>

</doc>
<doc id="9647" url="http://en.wikipedia.org/wiki?curid=9647" title="Euphoria (programming language)">
Euphoria (programming language)

Euphoria is a programming language originally created by Robert Craig of Rapid Deployment Software in Toronto. Initially developed (though not publicly released) on the Atari ST, the first commercial release was for the 16-bit MS-DOS platform and was proprietary. In 2006 (with the release of version 3), Euphoria became open source and the openEuphoria Group continues to administer and develop the project. In December 2010, the openEuphoria Group released version 4 of openEuphoria along with a new identity and mascot for the project. OpenEuphoria is currently available for Microsoft Windows, Linux, OS X and three flavors of *BSD.
Euphoria is a general-purpose high-level imperative/procedural interpreted language. A translator generates C source code and the GCC and Open Watcom compilers are supported. Alternatively, Euphoria programs may be bound with the interpreter to create stand-alone executables. A number of GUI libraries are supported including Win32lib and wrappers for wxWidgets, GTK+ and IUP. Euphoria has a simple built-in database and wrappers for a variety of other databases.
Overview.
The Euphoria language is a general purpose procedural language that focuses on simplicity, legibility, rapid development and performance.
History.
Developed as a personal project to invent a programming language from scratch, Euphoria was created by Robert Craig on an Atari Mega-ST. Many design ideas for the language came from Craig's Master's Thesis in Computer Science at the University of Toronto. Craig's thesis was heavily influenced by the work of John Backus on functional programming (FP) languages.
Craig ported his original Atari implementation to the 16-bit MS-DOS platform and Euphoria was first released (version 1.0) in July, 1993 under a proprietary licence. The original Atari implementation is described by Craig as "primitive" and has not been publicly released. Euphoria continued to be developed and released by Craig via his company Rapid Deployment Software (RDS) and website rapideuphoria.com. In October, 2006 RDS released version 3 of Euphoria and announced that henceforth Euphoria would be freely distributed under an open source licence.
RDS continued to develop Euphoria, culminating with the release of version 3.1.1 in August, 2007. Subsequently, RDS ceased unilateral development of Euphoria and the openEuphoria Group took over ongoing development. The openEuphoria Group released version 4 in December, 2010 along with a new logo and mascot for the openEuphoria project.
Version 3.1.1 remains an important milestone release, being the last version of Euphoria which supports the MS-DOS platform.
Euphoria is an acronym for "End-User Programming with Hierarchical Objects for Robust Interpreted Applications" although there is some suspicion that this is a backronym.
The Euphoria language interpreter was originally written in C. With the release of version 2.5 in November, 2004 the Euphoria interpreter was split into two sections: the front-end parser and the back-end interpreter. The front-end (which is also used with the Euphoria-to-C translator and the Binder) is now written in Euphoria. The main back-end and run time library are written in C.
Features.
Euphoria was conceived and developed with the following design goals and features:
Use.
Euphoria is designed to readily facilitate the handling of dynamic collections of data of varying types and is particularly useful for string and image processing. Euphoria has been used in artificial intelligence experiments, the study of mathematics, for teaching programming, and to implement fonts involving thousands of characters. A large part of the Euphoria interpreter is written in Euphoria.
Data types.
Euphoria has two basic data types:
Euphoria has two additional data types predefined:
There is no character string data type. Strings are represented by a sequence of integer values. However, because literal strings are so commonly used in programming, Euphoria interprets double-quote enclosed characters as a sequence of integers. Thus
 "ABC"
is seen as if the coder had written:
which is the same as:
Hello World.
 puts(1, "Hello World!\n")
Examples.
Note: Comments start with a double dash "--" and go through the end of line.
The following code looks for an old item in a group of items. If found, it removes it by concatenating all the elements prior to it with all the elements after it. Note that the first element in a sequence has the index one [1] and that $ refers to the length (i.e. total number of elements) of the sequence.
 global function delete_item( object old, sequence group )
 integer pos
 -- Code begins --
 pos = find( old, group )
 if pos > 0 then
 group = group[1 .. pos-1] & group[pos+1 .. $]
 end if
 return group
 end function
The following modification to the above example replaces an old item with a new item. As the variables "old" and "new" have been defined as objects, they could be atoms or sequences. Type checking is not required as the function will work with any sequence of data of any type and requires no external libraries.
 global function replace_item( object old, object new, sequence group )
 integer pos
 -- Code begins --
 pos = find( old, group )
 if pos > 0 then
 group[pos] = new
 end if
 return group
 end function
Furthermore, no pointers are involved and subscripts are automatically checked. Thus the function cannot access memory out-of-bounds. There is no need to allocate or deallocate memory explicitly and no chance of a memory leak.
The line
shows some of the sequence handling facilities. A sequence may contain a collection of any types, and this can be sliced (to take a subset of the data in a sequence) and concatenated in expressions with no need for special functions.
Parameter passing.
Arguments to routines are always passed by value; there is no pass-by-reference facility. However, parameters are allowed to be modified "locally" (i.e. within the callee) which is implemented very efficiently as sequences have automatic copy-on-write semantics. In other words, when you pass a sequence to a routine, initially only a reference to it is passed, but at the point the routine modifies this sequence parameter the sequence is copied and the routine updates only a copy of the original.
External links.
Free downloads of Euphoria for the various platforms, packages, Windows IDE, Windows API libraries, a GTK+ wrapper for Linux, graphics libraries (DOS, OpenGL, etc.).

</doc>
<doc id="9649" url="http://en.wikipedia.org/wiki?curid=9649" title="Energy">
Energy

In physics, energy is a property of objects which can be transferred to other objects or converted into different forms, but cannot be created or destroyed. The ability of a system to perform work is a common description. But, it is difficult to give a comprehensive definition of energy because of its many forms. In SI units, energy is measured in joules, the energy transferred to an object by the mechanical work of moving it 1 metre against a force of 1 newton.
All of the many forms of energy are convertible to other kinds of energy, and obey the conservation of energy. Common energy forms include the kinetic energy of a moving object, the radiant energy carried by light, the potential energy stored by an object's position in a force field (gravitational, electric or magnetic), elastic energy stored by stretching solid objects, chemical energy released when a fuel burns, and the thermal energy due to an object's temperature.
According to mass–energy equivalence, any object that has mass when stationary,(called rest mass) also has an equivalent amount of energy whose form is called rest energy. Conversely, any additional energy above the rest energy will increase an object's mass. For example, if you had a sensitive enough scale, you could measure an increase in mass after heating an object. Our Sun transforms nuclear potential energy to other forms of energy; its total mass does not decrease due to that in itself (since it still contains the same total energy even if in different forms), but its mass does decrease when the energy escapes out to its surroundings, largely as radiant energy.
For closed systems, the first law of thermodynamics states that a system's energy is constant unless energy is transferred in or out by work or heat, and that no energy is lost in transfer. This means that it is impossible to create or destroy energy. The second law of thermodynamics states that all systems doing work always lose some energy as waste heat. This creates a limit to the amount of energy that can do work by a heating process, a limit called the available energy. Mechanical and other forms of energy can be transformed in the other direction into thermal energy without such limitations. The total energy of a system can be calculated by adding up all forms of energy in the system. Examples of energy transfer and transformation include generating or making use of electric energy, performing chemical reactions, or lifting an object. Lifting against gravity performs work on the object and stores gravitational potential energy; if it falls, gravity does work on the object which transforms the potential energy to the kinetic energy associated with its speed.
Living organisms require available energy to stay alive, such as the energy humans get from food. Civilisation gets the energy it needs from energy resources such as fossil fuels. The processes of Earth's climate and ecosystem are driven by the radiant energy Earth receives from the sun and the geothermal energy contained within the earth. While total energy is never lost, energy conservation refers to using less available energy, which may be considered lost when it changes to a less useful form, such as waste heat.
Forms.
The total energy of a system can be subdivided and classified in various ways. For example, classical mechanics distinguishes between kinetic energy, which is determined by an object's movement through space, and potential energy, which is a function of the position of an object within a field. It may also be convenient to distinguish gravitational energy, thermal energy, several types of nuclear energy (which utilize potentials from the nuclear force and the weak force), electric energy (from the electric field), and magnetic energy (from the magnetic field), among others. Many of these classifications overlap; for instance, thermal energy usually consists partly of kinetic and partly of potential energy. Some types of energy are a varying mix of both potential and kinetic energy. An example is mechanical energy which is the sum of (usually macroscopic) kinetic and potential energy in a system. Elastic energy in materials is also dependent upon electrical potential energy (among atoms and molecules), as is chemical energy, which is stored and released from a reservoir of electrical potential energy between electrons, and the molecules or atomic nuclei that attract them. .The list is also not necessarily complete. Whenever physical scientists discover that a certain phenomenon appears to violate the law of energy conservation, new forms are typically added that account for the discrepancy.
Heat and work are special cases in that they are not properties of systems, but are instead properties of "processes" that transfer energy. In general we cannot measure how much heat or work are present in an object, but rather only how much energy is transferred among objects in certain ways during the occurrence of a given process. Heat and work are measured as positive or negative depending on which side of the transfer we view them from.
Potential energies are often measured as positive or negative depending on whether they are greater or less than the energy of a specified base state or configuration such as two interacting bodies being infinitely far apart. Wave energies (such as radiant or sound energy), kinetic energy, and rest energy are each greater than or equal to zero because they are measured in comparison to a base state of zero energy: "no wave", "no motion", and "no inertia", respectively.
The distinctions between different kinds of energy is not always clear-cut. As Richard Feynman points out:
 These notions of potential and kinetic energy depend on a notion of length scale. For example, one can speak of "macroscopic" potential and kinetic energy, which do not include thermal potential and kinetic energy. Also what is called chemical potential energy is a macroscopic notion, and closer examination shows that it is really the sum of the potential "and kinetic" energy on the atomic and subatomic scale. Similar remarks apply to nuclear "potential" energy and most other forms of energy. This dependence on length scale is non-problematic if the various length scales are decoupled, as is often the case ... but confusion can arise when different length scales are coupled, for instance when friction converts macroscopic work into microscopic thermal energy.
Some examples of different kinds of energy:
History.
The word "energy" derives from the Ancient Greek: ἐνέργεια "energeia" "activity, operation", which possibly appears for the first time in the work of Aristotle in the 4th century BC. In contrast to the modern definition, energeia was a qualitative philosophical concept, broad enough to include ideas such as happiness and pleasure.
In the late 17th century, Gottfried Leibniz proposed the idea of the Latin: "vis viva", or living force, which defined as the product of the mass of an object and its velocity squared; he believed that total "vis viva" was conserved. To account for slowing due to friction, Leibniz theorized that thermal energy consisted of the random motion of the constituent parts of matter, a view shared by Isaac Newton, although it would be more than a century until this was generally accepted. The modern analog of this property, kinetic energy, differs from "vis viva" only by a factor of two.
In 1807, Thomas Young was possibly the first to use the term "energy" instead of "vis viva", in its modern sense. Gustave-Gaspard Coriolis described "kinetic energy" in 1829 in its modern sense, and in 1853, William Rankine coined the term "potential energy". The law of conservation of energy, was also first postulated in the early 19th century, and applies to any isolated system. It was argued for some years whether heat was a physical substance, dubbed the caloric, or merely a physical quantity, such as momentum. In 1845 James Prescott Joule discovered the link between mechanical work and the generation of heat.
These developments led to the theory of conservation of energy, formalized largely by William Thomson (Lord Kelvin) as the field of thermodynamics. Thermodynamics aided the rapid development of explanations of chemical processes by Rudolf Clausius, Josiah Willard Gibbs, and Walther Nernst. It also led to a mathematical formulation of the concept of entropy by Clausius and to the introduction of laws of radiant energy by Jožef Stefan. According to Noether's theorem, the conservation of energy is a consequence of the fact that the laws of physics do not change over time. Thus, since 1918, theorists have understood that the law of conservation of energy is the direct mathematical consequence of the translational symmetry of the quantity conjugate to energy, namely time.
Measurement and units.
Energy, like mass, is a scalar physical quantity. The joule is the International System of Units (SI) unit of measurement for energy. It is a derived unit of energy, work, or amount of heat. It is equal to the energy expended (or work done) in applying a force of one newton through a distance of one metre. However energy is also expressed in many other units such as ergs, calories, British Thermal Units, kilowatt-hours and kilocalories for instance. There is always a conversion factor for these to the SI unit; for instance; one kWh is equivalent to 3.6 million joules.
The SI unit of power (energy per unit time) is the watt, which is simply a joule per second. Thus, a joule is a watt-second, so 3600 joules equal a watt-hour. The CGS energy unit is the erg, and the imperial and US customary unit is the foot pound. Other energy units such as the electron volt, food calorie or thermodynamic kcal (based on the temperature change of water in a heating process), and BTU are used in specific areas of science and commerce and have unit conversion factors relating them to the joule.
Because energy is defined as the ability to do work on objects, there is no absolute measure of energy. Only the transition of a system from one state into another can be defined and thus energy is measured in relative terms. The choice of a baseline or zero point is often arbitrary and can be made in whatever way is most convenient for a problem.
For example in the case of measuring the energy deposited by X-rays as shown in the accompanying diagram, conventionally the technique most often employed is calorimetry. This is a thermodynamic technique that relies on the measurement of temperature using a thermometer or of intensity of radiation using a bolometer.
Energy density is a term used for the amount of useful energy stored in a given system or region of space per unit volume. For fuels, the energy per unit volume is sometimes a useful parameter. In a few applications, comparing, for example, the effectiveness of hydrogen fuel to gasoline it turns out that hydrogen has a higher specific energy than does gasoline, but, even in liquid form, a much lower energy "density".
Scientific use.
Classical mechanics.
In classical mechanics, energy is a conceptually and mathematically useful property, as it is a conserved quantity. Several formulations of mechanics have been developed using energy as a core concept.
Work, a form of energy, is force times distance.
This says that the work (formula_2) is equal to the line integral of the force F along a path "C"; for details see the mechanical work article. Work and thus energy is frame dependent. For example, consider a ball being hit by a bat. In the center-of-mass reference frame, the bat does no work on the ball. But, in the reference frame of the person swinging the bat, considerable work is done on the ball.
The total energy of a system is sometimes called the Hamiltonian, after William Rowan Hamilton. The classical equations of motion can be written in terms of the Hamiltonian, even for highly complex or abstract systems. These classical equations have remarkably direct analogs in nonrelativistic quantum mechanics.
Another energy-related concept is called the Lagrangian, after Joseph-Louis Lagrange. This formalism is as fundamental as the Hamiltonian, and both can be used to derive the equations of motion or be derived from them. It was invented in the context of classical mechanics, but is generally useful in modern physics. The Lagrangian is defined as the kinetic energy "minus" the potential energy. Usually, the Lagrange formalism is mathematically more convenient than the Hamiltonian for non-conservative systems (such as systems with friction).
Noether's theorem (1918) states that any differentiable symmetry of the action of a physical system has a corresponding conservation law. Noether's theorem has become a fundamental tool of modern theoretical physics and the calculus of variations. A generalisation of the seminal formulations on constants of motion in Lagrangian and Hamiltonian mechanics (1788 and 1833, respectively), it does not apply to systems that cannot be modeled with a Lagrangian; for example, dissipative systems with continuous symmetries need not have a corresponding conservation law.
Chemistry.
In the context of chemistry, energy is an attribute of a substance as a consequence of its atomic, molecular or aggregate structure. Since a chemical transformation is accompanied by a change in one or more of these kinds of structure, it is invariably accompanied by an increase or decrease of energy of the substances involved. Some energy is transferred between the surroundings and the reactants of the reaction in the form of heat or light; thus the products of a reaction may have more or less energy than the reactants. A reaction is said to be exergonic if the final state is lower on the energy scale than the initial state; in the case of endergonic reactions the situation is the reverse. Chemical reactions are invariably not possible unless the reactants surmount an energy barrier known as the activation energy. The "speed" of a chemical reaction (at given temperature "T") is related to the activation energy "E", by the Boltzmann's population factor e−"E"/"kT" – that is the probability of molecule to have energy greater than or equal to "E" at the given temperature "T". This exponential dependence of a reaction rate on temperature is known as the Arrhenius equation.The activation energy necessary for a chemical reaction can be in the form of thermal energy.
Biology.
In biology, energy is an attribute of all biological systems from the biosphere to the smallest living organism. Within an organism it is responsible for growth and development of a biological cell or an organelle of a biological organism. Energy is thus often said to be stored by cells in the structures of molecules of substances such as carbohydrates (including sugars), lipids, and proteins, which release energy when reacted with oxygen in respiration. In human terms, the human equivalent (H-e) (Human energy conversion) indicates, for a given amount of energy expenditure, the relative quantity of energy needed for human metabolism, assuming an average human energy expenditure of 12,500kJ per day and a basal metabolic rate of 80 watts. For example, if our bodies run (on average) at 80 watts, then a light bulb running at 100 watts is running at 1.25 human equivalents (100 ÷ 80) i.e. 1.25 H-e. For a difficult task of only a few seconds' duration, a person can put out thousands of watts, many times the 746 watts in one official horsepower. For tasks lasting a few minutes, a fit human can generate perhaps 1,000 watts. For an activity that must be sustained for an hour, output drops to around 300; for an activity kept up all day, 150 watts is about the maximum. The human equivalent assists understanding of energy flows in physical and biological systems by expressing energy units in human terms: it provides a "feel" for the use of a given amount of energy
Sunlight is also captured by plants as "chemical potential energy" in photosynthesis, when carbon dioxide and water (two low-energy compounds) are converted into the high-energy compounds carbohydrates, lipids, and proteins. Plants also release oxygen during photosynthesis, which is utilized by living organisms as an electron acceptor, to release the energy of carbohydrates, lipids, and proteins. Release of the energy stored during photosynthesis as heat or light may be triggered suddenly by a spark, in a forest fire, or it may be made available more slowly for animal or human metabolism, when these molecules are ingested, and catabolism is triggered by enzyme action.
Any living organism relies on an external source of energy—radiation from the Sun in the case of green plants; chemical energy in some form in the case of animals—to be able to grow and reproduce. The daily 1500–2000 Calories (6–8 MJ) recommended for a human adult are taken as a combination of oxygen and food molecules, the latter mostly carbohydrates and fats, of which glucose (C6H12O6) and stearin (C57H110O6) are convenient examples. The food molecules are oxidised to carbon dioxide and water in the mitochondria
and some of the energy is used to convert ADP into ATP
The rest of the chemical energy in the carbohydrate or fat is converted into heat: the ATP is used as a sort of "energy currency", and some of the chemical energy it contains when split and reacted with water, is used for other metabolism (at each stage of a metabolic pathway, some chemical energy is converted into heat). Only a tiny fraction of the original chemical energy is used for work:
It would appear that living organisms are remarkably inefficient (in the physical sense) in their use of the energy they receive (chemical energy or radiation), and it is true that most real machines manage higher efficiencies. In growing organisms the energy that is converted to heat serves a vital purpose, as it allows the organism tissue to be highly ordered with regard to the molecules it is built from. The second law of thermodynamics states that energy (and matter) tends to become more evenly spread out across the universe: to concentrate energy (or matter) in one specific place, it is necessary to spread out a greater amount of energy (as heat) across the remainder of the universe ("the surroundings"). Simpler organisms can achieve higher energy efficiencies than more complex ones, but the complex organisms can occupy ecological niches that are not available to their simpler brethren. The conversion of a portion of the chemical energy to heat at each step in a metabolic pathway is the physical reason behind the pyramid of biomass observed in ecology: to take just the first step in the food chain, of the estimated 124.7 Pg/a of carbon that is fixed by photosynthesis, 64.3 Pg/a (52%) are used for the metabolism of green plants, i.e. reconverted into carbon dioxide and heat.
Earth sciences.
In geology, continental drift, mountain ranges, volcanoes, and earthquakes are phenomena that can be explained in terms of energy transformations in the Earth's interior., while meteorological phenomena like wind, rain, hail, snow, lightning, tornadoes and hurricanes, are all a result of energy transformations brought about by solar energy on the atmosphere of the planet Earth.
Sunlight may be stored as gravitational potential energy after it strikes the Earth, as (for example) water evaporates from oceans and is deposited upon mountains (where, after being released at a hydroelectric dam, it can be used to drive turbines or generators to produce electricity). Sunlight also drives many weather phenomena, save those generated by volcanic events. An example of a solar-mediated weather event is a hurricane, which occurs when large unstable areas of warm ocean, heated over months, give up some of their thermal energy suddenly to power a few days of violent air movement.
In a slower process, radioactive decay of atoms in the core of the Earth releases heat. This thermal energy drives plate tectonics and may lift mountains, via orogenesis. This slow lifting represents a kind of gravitational potential energy storage of the thermal energy, which may be later released to active kinetic energy in landslides, after a triggering event. Earthquakes also release stored elastic potential energy in rocks, a store that has been produced ultimately from the same radioactive heat sources. Thus, according to present understanding, familiar events such as landslides and earthquakes release energy that has been stored as potential energy in the Earth's gravitational field or elastic strain (mechanical potential energy) in rocks. Prior to this, they represent release of energy that has been stored in heavy atoms since the collapse of long-destroyed supernova stars created these atoms.
Cosmology.
In cosmology and astronomy the phenomena of stars, nova, supernova, quasars and gamma ray bursts are the universe's highest-output energy transformations of matter. All stellar phenomena (including solar activity) are driven by various kinds of energy transformations. Energy in such transformations is either from gravitational collapse of matter (usually molecular hydrogen) into various classes of astronomical objects (stars, black holes, etc.), or from nuclear fusion (of lighter elements, primarily hydrogen). The nuclear fusion of hydrogen in the Sun also releases another store of potential energy which was created at the time of the Big Bang. At that time, according to theory, space expanded and the universe cooled too rapidly for hydrogen to completely fuse into heavier elements. This meant that hydrogen represents a store of potential energy that can be released by fusion. Such a fusion process is triggered by heat and pressure generated from gravitational collapse of hydrogen clouds when they produce stars, and some of the fusion energy is then transformed into sunlight.
Quantum mechanics.
In quantum mechanics, energy is defined in terms of the energy operator
as a time derivative of the wave function. The Schrödinger equation equates the energy operator to the full energy of a particle or a system. In results can be considered as a definition of measurement of energy in quantum mechanics. The Schrödinger equation describes the space- and time-dependence of slow changing (non-relativistic) wave function of quantum systems. The solution of this equation for bound system is discrete (a set of permitted states, each characterized by an energy level) which results in the concept of quanta. In the solution of the Schrödinger equation for any oscillator (vibrator) and for electromagnetic waves in a vacuum, the resulting energy states are related to the frequency by Planck's relation: formula_3 (where formula_4 is the Planck's constant and formula_5 the frequency). In the case of electromagnetic wave these energy states are called quanta of light or photons.
Relativity.
When calculating kinetic energy (work to accelerate a mass from zero speed to some finite speed) relativistically - using Lorentz transformations instead of Newtonian mechanics, Einstein discovered an unexpected by-product of these calculations to be an energy term which does not vanish at zero speed. He called it rest mass energy - energy which every mass must possess even when being at rest. The amount of energy is directly proportional to the mass of body:
where
For example, consider electron–positron annihilation, in which the rest mass of individual particles is destroyed, but the inertia equivalent of the system of the two particles (its invariant mass) remains (since all energy is associated with mass), and this inertia and invariant mass is carried off by photons which individually are massless, but as a system retain their mass. This is a reversible process - the inverse process is called pair creation - in which the rest mass of particles is created from energy of two (or more) annihilating photons. In this system the matter (electrons and positrons) is destroyed and changed to non-matter energy (the photons). However, the total system mass and energy do not change during this interaction.
In general relativity, the stress–energy tensor serves as the source term for the gravitational field, in rough analogy to the way mass serves as the source term in the non-relativistic Newtonian approximation.
It is not uncommon to hear that energy is "equivalent" to mass. It would be more accurate to state that every energy has an inertia and gravity equivalent, and because mass is a form of energy, then mass too has inertia and gravity associated with it.
In classical physics, energy is a scalar quantity, the canonical conjugate to time. In special relativity energy is also a scalar (although not a Lorentz scalar but a time component of the energy–momentum 4-vector). In other words, energy is invariant with respect to rotations of space, but not invariant with respect to rotations of space-time (= boosts).
Transformation.
Energy may be transformed between different forms at various efficiencies. Items that transform between these forms are called transducers. Examples of transducers include a battery, from chemical energy to electric energy; a dam: gravitational potential energy to kinetic energy of moving water (and the blades of a turbine) and ultimately to electric energy through an electric generator.
There are strict limits to how efficiently energy can be converted into other forms of energy via work, and heat as described by Carnot's theorem and the second law of thermodynamics. These limits are especially evident when an engine is used to perform work. However, some energy transformations can be quite efficient. The direction of transformations in energy (what kind of energy is transformed to what other kind) is often determined by entropy (equal energy spread among all available degrees of freedom) considerations. In practice all energy transformations are permitted on a small scale, but certain larger transformations are not permitted because it is statistically unlikely that energy or matter will randomly move into more concentrated forms or smaller spaces.
Energy transformations in the universe over time are characterized by various kinds of potential energy that has been available since the Big Bang, later being "released" (transformed to more active types of energy such as kinetic or radiant energy), when a triggering mechanism is available. Familiar examples of such processes include nuclear decay, in which energy is released that was originally "stored" in heavy isotopes (such as uranium and thorium), by nucleosynthesis, a process ultimately using the gravitational potential energy released from the gravitational collapse of supernovae, to store energy in the creation of these heavy elements before they were incorporated into the solar system and the Earth. This energy is triggered and released in nuclear fission bombs or in civil nuclear power generation. Similarly, in the case of a chemical explosion, chemical potential energy is transformed to kinetic energy and thermal energy in a very short time. Yet another example is that of a pendulum. At its highest points the kinetic energy is zero and the gravitational potential energy is at maximum. At its lowest point the kinetic energy is at maximum and is equal to the decrease of potential energy. If one (unrealistically) assumes that there is no friction or other losses, the conversion of energy between these processes would be perfect, and the pendulum would continue swinging forever.
Energy is also transferred from potential energy (formula_7) to kinetic energy (formula_8) and then back to potential energy constantly. This is referred to as conservation of energy. In this closed system, energy cannot be created or destroyed; therefore, the initial energy and the final energy will be equal to each other. This can be demonstrated by the following:
The equation can then be simplified further since formula_9 (mass times acceleration due to gravity times the height) and formula_10 (half mass times velocity squared). Then the total amount of energy can be found by adding formula_11.
Conservation of energy and mass in transformation.
Energy gives rise to weight when it is trapped in a system with zero momentum, where it can be weighed. It is also equivalent to mass, and this mass is always associated with it. Mass is also equivalent to a certain amount of energy, and likewise always appears associated with it, as described in mass-energy equivalence. The formula "E" = "mc"², derived by Albert Einstein (1905) quantifies the relationship between rest-mass and rest-energy within the concept of special relativity. In different theoretical frameworks, similar formulas were derived by J. J. Thomson (1881), Henri Poincaré (1900), Friedrich Hasenöhrl (1904) and others (see Mass-energy equivalence#History for further information).
Matter may be converted to energy (and vice versa), but mass cannot ever be destroyed; rather, mass/energy equivalence remains a constant for both the matter and the energy, during any process when they are converted into each other. However, since formula_12 is extremely large relative to ordinary human scales, the conversion of ordinary amount of matter (for example, 1 kg) to other forms of energy (such as heat, light, and other radiation) can liberate tremendous amounts of energy (~formula_13 joules = 21 megatons of TNT), as can be seen in nuclear reactors and nuclear weapons. Conversely, the mass equivalent of a unit of energy is minuscule, which is why a loss of energy (loss of mass) from most systems is difficult to measure by weight, unless the energy loss is very large. Examples of energy transformation into matter (i.e., kinetic energy into particles with rest mass) are found in high-energy nuclear physics.
Reversible and non-reversible transformations.
Thermodynamics divides energy transformation into two kinds: reversible processes and irreversible processes. An irreversible process is one in which energy is dissipated (spread) into empty energy states available in a volume, from which it cannot be recovered into more concentrated forms (fewer quantum states), without degradation of even more energy. A reversible process is one in which this sort of dissipation does not happen. For example, conversion of energy from one type of potential field to another, is reversible, as in the pendulum system described above. In processes where heat is generated, quantum states of lower energy, present as possible excitations in fields between atoms, act as a reservoir for part of the energy, from which it cannot be recovered, in order to be converted with 100% efficiency into other forms of energy. In this case, the energy must partly stay as heat, and cannot be completely recovered as usable energy, except at the price of an increase in some other kind of heat-like increase in disorder in quantum states, in the universe (such as an expansion of matter, or a randomisation in a crystal).
As the universe evolves in time, more and more of its energy becomes trapped in irreversible states (i.e., as heat or other kinds of increases in disorder). This has been referred to as the inevitable thermodynamic heat death of the universe. In this heat death the energy of the universe does not change, but the fraction of energy which is available to do work through a heat engine, or be transformed to other usable forms of energy (through the use of generators attached to heat engines), grows less and less.
Conservation of energy.
According to conservation of energy, energy can neither be created (produced) nor destroyed by itself. It can only be transformed. The total inflow of energy into a system must equal the total outflow of energy from the system, plus the change in the energy contained within the system. Energy is subject to a strict global conservation law; that is, whenever one measures (or calculates) the total energy of a system of particles whose interactions do not depend explicitly on time, it is found that the total energy of the system always remains constant.
Richard Feynman said during a 1961 lecture:
There is a fact, or if you wish, a "law", governing all natural phenomena that are known to date. There is no known exception to this law—it is exact so far as we know. The law is called the "conservation of energy". It states that there is a certain quantity, which we call energy, that does not change in manifold changes which nature undergoes. That is a most abstract idea, because it is a mathematical principle; it says that there is a numerical quantity which does not change when something happens. It is not a description of a mechanism, or anything concrete; it is just a strange fact that we can calculate some number and when we finish watching nature go through her tricks and calculate the number again, it is the same.—"The Feynman Lectures on Physics"
Most kinds of energy (with gravitational energy being a notable exception) are subject to strict local conservation laws as well. In this case, energy can only be exchanged between adjacent regions of space, and all observers agree as to the volumetric density of energy in any given space. There is also a global law of conservation of energy, stating that the total energy of the universe cannot change; this is a corollary of the local law, but not vice versa.
This law is a fundamental principle of physics. As shown rigorously by Noether's theorem, the conservation of energy is a mathematical consequence of translational symmetry of time, a property of most phenomena below the cosmic scale that makes them independent of their locations on the time coordinate. Put differently, yesterday, today, and tomorrow are physically indistinguishable. This is because energy is the quantity which is canonical conjugate to time. This mathematical entanglement of energy and time also results in the uncertainty principle - it is impossible to define the exact amount of energy during any definite time interval. The uncertainty principle should not be confused with energy conservation - rather it provides mathematical limits to which energy can in principle be defined and measured.
Each of the basic forces of nature is associated with a different type of potential energy, and all types of potential energy (like all other types of energy) appears as system mass, whenever present. For example, a compressed spring will be slightly more massive than before it was compressed. Likewise, whenever energy is transferred between systems by any mechanism, an associated mass is transferred with it.
In quantum mechanics energy is expressed using the Hamiltonian operator. On any time scales, the uncertainty in the energy is by
which is similar in form to the Heisenberg Uncertainty Principle (but not really mathematically equivalent thereto, since "H" and "t" are not dynamically conjugate variables, neither in classical nor in quantum mechanics).
In particle physics, this inequality permits a qualitative understanding of virtual particles which carry momentum, exchange by which and with real particles, is responsible for the creation of all known fundamental forces (more accurately known as fundamental interactions). Virtual photons (which are simply lowest quantum mechanical energy state of photons) are also responsible for electrostatic interaction between electric charges (which results in Coulomb law), for spontaneous radiative decay of exited atomic and nuclear states, for the Casimir force, for van der Waals bond forces and some other observable phenomena.
Transfer between systems.
Closed systems.
Energy transfer usually refers to movements of energy between systems which are closed to transfers of matter. The portion of the energy which is transferred by conservative forces over a distance is measured as the work the source system does on the receiving system. The portion of the energy which does not do work doing during the transfer is called heat. Energy can be transferred between systems in a variety of ways. Examples include the transmission of electromagnetic energy via photons, physical collisions which transfer kinetic energy, and the conductive transfer of thermal energy.
Energy is strictly conserved and is also locally conserved wherever it can be defined. Mathematically, the process of energy transfer is described by the first law of thermodynamics:
where formula_15 is the amount of energy transferred, formula_2  represents the work done on the system, and formula_17 represents the heat flow into the system. As a simplification, the heat term, formula_17, is sometimes ignored, especially when the thermal efficiency of the transfer is high.
This simplified equation is the one used to define the joule, for example.
Open systems.
There are other ways in which an open system can gain or lose energy. In chemical systems, energy can be added to a system by means of adding substances with different chemical potentials, which potentials are then extracted (both of these process are illustrated by fueling an auto, a system which gains in energy thereby, without addition of either work or heat). These terms may be added to the above equation, or they can generally be subsumed into a quantity called "energy addition term formula_15" which refers to "any" type of energy carried over the surface of a control volume or system volume. Examples may be seen above, and many others can be imagined (for example, the kinetic energy of a stream of particles entering a system, or energy from a laser beam adds to system energy, without either being either work-done or heat-added, in the classic senses).
Where formula_15 in this general equation represents other additional advected energy terms not covered by work done on a system, or heat added to it.
Thermodynamics.
Internal energy.
Internal energy is the sum of all microscopic forms of energy of a system. It is the energy needed to create the system. It is related to the potential energy, e.g., molecular structure, crystal structure, and other geometric aspects, as well as the motion of the particles, in form of kinetic energy. Thermodynamics is chiefly concerned with changes in internal energy and not its absolute value, which is impossible to determine with thermodynamics alone.
First law of thermodynamics.
The first law of thermodynamics asserts that energy (but not necessarily thermodynamic free energy) is always conserved and that heat flow is a form of energy transfer. For homogeneous systems, with a well-defined temperature and pressure, a commonly used corollary of the first law is that, for a system subject only to pressure forces and heat transfer (e.g., a cylinder-full of gas), the differential change in the internal energy of the system (with a "gain" in energy signified by a positive quantity) is given as
where the first term on the right is the heat transferred into the system, expressed in terms of temperature "T" and entropy "S" (in which entropy increases and the change d"S" is positive when the system is heated), and the last term on the right hand side is identified as work done on the system, where pressure is "P" and volume "V" (the negative sign results since compression of the system requires work to be done on it and so the volume change, d"V", is negative when work is done on the system).
This equation is highly specific, ignoring all chemical, electrical, nuclear, and gravitational forces, effects such as advection of any form of energy other than heat and pV-work. The general formulation of the first law (i.e., conservation of energy) is valid even in situations in which the system is not homogeneous. For these cases the change in internal energy of a "closed" system is expressed in a general form by
where formula_23 is the heat supplied to the system and formula_24 is the work applied to the system.
Equipartition of energy.
The energy of a mechanical harmonic oscillator (a mass on a spring) is alternatively kinetic and potential. At two points in the oscillation cycle it is entirely kinetic, and alternatively at two other points it is entirely potential. Over the whole cycle, or over many cycles, net energy is thus equally split between kinetic and potential. This is called equipartition principle; total energy of a system with many degrees of freedom is equally split among all available degrees of freedom.
This principle is vitally important to understanding the behaviour of a quantity closely related to energy, called entropy. Entropy is a measure of evenness of a distribution of energy between parts of a system. When an isolated system is given more degrees of freedom (i.e., given new available energy states that are the same as existing states), then total energy spreads over all available degrees equally without distinction between "new" and "old" degrees. This mathematical result is called the second law of thermodynamics.
Further reading.
</dl>

</doc>
<doc id="9653" url="http://en.wikipedia.org/wiki?curid=9653" title="Expected value">
Expected value

In probability theory, the expected value of a random variable is intuitively the long-run average value of repetitions of the experiment it represents. For example, the expected value of a dice roll is 3.5 because, roughly speaking, the average of an extremely large number of dice rolls is practically always nearly equal to 3.5. Less roughly, the law of large numbers guarantees that the arithmetic mean of the values almost surely converges to the expected value as the number of repetitions goes to infinity. The expected value is also known as the expectation, mathematical expectation, EV, mean, or first moment.
More practically, the expected value of a discrete random variable is the probability-weighted average of all possible values. In other words, each possible value the random variable can assume is multiplied by its probability of occurring, and the resulting products are summed to produce the expected value. The same works for continuous random variables, except the sum is replaced by an integral and the probabilities by probability densities. The formal definition subsumes both of these and also works for distributions which are neither discrete nor continuous: the expected value of a random variable is the integral of the random variable with respect to its probability measure.
The expected value does not exist for random variables having some distributions with large "tails", such as the Cauchy distribution. For random variables such as these, the long-tails of the distribution prevent the sum/integral from converging.
The expected value is a key aspect of how one characterizes a probability distribution; it is one type of location parameter. By contrast, the variance is a measure of dispersion of the possible values of the random variable around the expected value. The variance itself is defined in terms of two expectations: it is the expected value of the squared deviation of the variable's value from the variable's expected value.
The expected value plays important roles in a variety of contexts. In regression analysis, one desires a formula in terms of observed data that will give a "good" estimate of the parameter giving the effect of some explanatory variable upon a dependent variable. The formula will give different estimates using different samples of data, so the estimate it gives is itself a random variable. A formula is typically considered good in this context if it is an unbiased estimator—that is, if the expected value of the estimate (the average value it would give over an arbitrarily large number of separate samples) can be shown to equal the true value of the desired parameter.
In decision theory, and in particular in choice under uncertainty, an agent is described as making an optimal choice in the context of incomplete information. For risk neutral agents, the choice involves using the expected values of uncertain quantities, while for risk averse agents it involves maximizing the expected value of some objective function such as a von Neumann-Morgenstern utility function. One example of using expected value in reaching optimal decisions is the Gordon-Loeb Model of information security investment. According to the model, one can conclude that the amount a firm spends to protect information should generally be only a small fraction of the expected loss (i.e., the expected value of the loss resulting from a cyber/information security breach).
Definition.
Univariate discrete random variable, finite case.
Suppose random variable "X" can take value "x"1 with probability "p"1, value "x"2 with probability "p"2, and so on, up to value "xk" with probability "pk". Then the expectation of this random variable "X" is defined as
Since all probabilities "pi" add up to one ("p"1 + "p"2 + ... + "pk" = 1), the expected value can be viewed as the weighted average, with "pi"’s being the weights:
If all outcomes "xi" are equally likely (that is, "p"1 = "p"2 = ... = "pk"), then the weighted average turns into the simple average. This is intuitive: the expected value of a random variable is the average of all values it can take; thus the expected value is what one expects to happen "on average". If the outcomes "xi" are not equally probable, then the simple average must be replaced with the weighted average, which takes into account the fact that some outcomes are more likely than the others. The intuition however remains the same: the expected value of "X" is what one expects to happen "on average".
Example 1. Let "X" represent the outcome of a roll of a fair six-sided die. More specifically, "X" will be the number of pips showing on the top face of the die after the toss. The possible values for "X" are 1, 2, 3, 4, 5, and 6, all equally likely (each having the probability of 1/6). The expectation of "X" is
If one rolls the die "n" times and computes the average (arithmetic mean) of the results, then as "n" grows, the average will almost surely converge to the expected value, a fact known as the strong law of large numbers. One example sequence of ten rolls of the die is 2, 3, 1, 2, 5, 6, 2, 2, 2, 6, which has the average of 3.1, with the distance of 0.4 from the expected value of 3.5. The convergence is relatively slow: the probability that the average falls within the range 3.5 ± 0.1 is 21.6% for ten rolls, 46.1% for a hundred rolls and 93.7% for a thousand rolls. See the figure for an illustration of the averages of longer sequences of rolls of the die and how they converge to the expected value of 3.5. More generally, the rate of convergence can be roughly quantified by e.g. Chebyshev's inequality and the Berry-Esseen theorem.
Example 2. The roulette game consists of a small ball and a wheel with 38 numbered pockets around the edge. As the wheel is spun, the ball bounces around randomly until it settles down in one of the pockets. Suppose random variable "X" represents the (monetary) outcome of a $1 bet on a single number ("straight up" bet). If the bet wins (which happens with probability 1/38), the payoff is $35; otherwise the player loses the bet. The expected profit from such a bet will be
Univariate discrete random variable, countable case.
Let "X" be a discrete random variable taking values "x", "x", ... with probabilities "p", "p", ... respectively. Then the expected value of this random variable is the infinite sum
provided that this series converges absolutely (that is, the sum must remain finite if we were to replace all "x"'s with their absolute values). If this series does not converge absolutely, we say that the expected value of "X" does not exist.
For example, suppose random variable "X" takes values 1, −2, 3, −4, ..., with respective probabilities "c"/12, "c"/22, "c"/32, "c"/42, ..., where is a normalizing constant that ensures the probabilities sum up to one. Then the infinite sum
converges and its sum is equal to ln(2) ≃ 0.69315. However it would be incorrect to claim that the expected value of "X" is equal to this number—in fact E["X"] does not exist, as this series does not converge absolutely (see harmonic series).
Univariate continuous random variable.
If the probability distribution of formula_7 admits a probability density function formula_8, then the expected value can be computed as
General definition.
In general, if "X" is a random variable defined on a probability space (Ω, Σ, "P"), then the expected value of "X", denoted by E["X"], 〈"X"〉, "X" or E["X"], is defined as the Lebesgue integral 
When this integral exists, it is defined as the expectation of "X". Note that not all random variables have a finite expected value, since the integral may not converge absolutely; furthermore, for some it is not defined at all (e.g., Cauchy distribution). Two variables with the same probability distribution will have the same expected value, if it is defined.
It follows directly from the discrete case definition that if "X" is a constant random variable, i.e. "X" = "b" for some fixed real number "b", then the expected value of "X" is also "b".
The expected value of a measurable function of "X", "g"("X"), given that "X" has a probability density function "f"("x"), is given by the inner product of "f" and "g":
This is sometimes called the law of the unconscious statistician. Using representations as Riemann–Stieltjes integral and integration by parts the formula can be restated as
As a special case let "α" denote a positive real number. Then
In particular, if α = 1 and Pr["X" ≥ 0] = 1, then this reduces to
where "F" is the cumulative distribution function of "X". This last identity is an instance of what, in a non-probabilistic setting, has been called the layer cake representation.
The law of the unconscious statistician applies also to a measurable function "g" of several random variables "X"1, ... "X"n having a joint density "f":
Properties.
Constants.
The expected value of a constant is equal to the constant itself; i.e., if "c" is a constant, then .
Monotonicity.
If "X" and "Y" are random variables such that "X" ≤ "Y" almost surely, then E["X"] ≤ E["Y"].
Linearity.
The expected value operator (or expectation operator) E is linear in the sense that
Note that the second result is valid even if "X" is not statistically independent of "Y". Combining the results from previous three equations, we can see that
for any two random variables "X" and "Y" (which need to be defined on the same probability space) and any real numbers "a", "b" and "c".
Iterated expectation.
Iterated expectation for discrete random variables.
For any two discrete random variables "X", "Y" one may define the conditional expectation:
which means that E["X"|"Y" = "y"] is a function of "y". Let "g"("y") be that function of "y"; then the notation E["X"|"Y"] is then a random variable in its own right, equal to "g"("Y").
Lemma. Then the expectation of "X" satisfies:
Proof.
The left-hand side of this equation is referred to as the "iterated expectation". The equation is sometimes called the "tower rule" or the "tower property"; it is treated under law of total expectation.
Iterated expectation for continuous random variables.
In the continuous case, the results are completely analogous. The definition of conditional expectation would use inequalities, density functions, and integrals to replace equalities, mass functions, and summations, respectively. However, the main result still holds:
Inequality.
If a random variable "X" is always less than or equal to another random variable "Y", the expectation of "X" is less than or equal to that of "Y":
If "X" ≤ "Y", then E["X"] ≤ E["Y"].
In particular, if we set Y to |"X"| we know "X" ≤ "Y" and −"X" ≤ "Y". Therefore we know E["X"] ≤ E["Y"] and E[−"X"] ≤ E["Y"]. From the linearity of expectation we know −E["X"] ≤ E["Y"]. Therefore the absolute value of expectation of a random variable is less than or equal to the expectation of its absolute value:
Non-multiplicativity.
If one considers the joint probability density function of "X" and "Y", say "j(x,y)", then the expectation of "XY" is
In general, the expected value operator is not multiplicative, i.e. E["XY"] is not necessarily equal to E["X"]·E["Y"]. In fact, the amount by which multiplicativity fails is called the covariance:
Thus multiplicativity holds precisely when Cov("X", "Y") = 0, in which case "X" and "Y" are said to be uncorrelated (independent variables are a notable case of uncorrelated variables).
Now if "X" and "Y" are independent, then by definition where "f" and "g" are the marginal PDFs for "X" and "Y". Then
and Cov("X", "Y") = 0.
Observe that independence of "X" and "Y" is required only to write "j"("x", "y") = "f"("x")"g"("y"), and this is required to establish the second equality above. The third equality follows from a basic application of the Fubini-Tonelli theorem.
Functional non-invariance.
In general, the expectation operator and functions of random variables do not commute; that is
A notable inequality concerning this topic is Jensen's inequality, involving expected values of convex (or concave) functions.
Uses and applications.
It is possible to construct an expected value equal to the probability of an event by taking the expectation of an indicator function that is one if the event has occurred and zero otherwise. This relationship can be used to translate properties of expected values into properties of probabilities, e.g. using the law of large numbers to justify estimating probabilities by frequencies.
The expected values of the powers of "X" are called the moments of "X"; the moments about the mean of "X" are expected values of powers of "X" − E["X"]. The moments of some random variables can be used to specify their distributions, via their moment generating functions.
To empirically estimate the expected value of a random variable, one repeatedly measures observations of the variable and computes the arithmetic mean of the results. If the expected value exists, this procedure estimates the true expected value in an unbiased manner and has the property of minimizing the sum of the squares of the residuals (the sum of the squared differences between the observations and the estimate). The law of large numbers demonstrates (under fairly mild conditions) that, as the size of the sample gets larger, the variance of this estimate gets smaller.
This property is often exploited in a wide variety of applications, including general problems of statistical estimation and machine learning, to estimate (probabilistic) quantities of interest via Monte Carlo methods, since most quantities of interest can be written in terms of expectation, e.g. formula_27 where formula_28 is the indicator function for set formula_29, i.e. formula_30.
 In classical mechanics, the center of mass is an analogous concept to expectation. For example, suppose "X" is a discrete random variable with values "xi" and corresponding probabilities "pi". Now consider a weightless rod on which are placed weights, at locations "xi" along the rod and having masses "pi" (whose sum is one). The point at which the rod balances is E["X"].
Expected values can also be used to compute the variance, by means of the computational formula for the variance
A very important application of the expectation value is in the field of quantum mechanics. The expectation value of a quantum mechanical operator formula_32 operating on a quantum state vector formula_33 is written as formula_34. The uncertainty in formula_32 can be calculated using the formula formula_36.
Expectation of matrices.
If "X" is an "m" × "n" matrix, then the expected value of the matrix is defined as the matrix of expected values:
This is utilized in covariance matrices.
Formulas for special cases.
Discrete distribution taking only non-negative integer values.
When a random variable takes only values in {0, 1, 2, 3, ...} we can use the following formula for computing its expectation (even when the expectation is infinite):
Proof.
Interchanging the order of summation, we have
This result can be a useful computational shortcut. For example, suppose we toss a coin where the probability of heads is "p". How many tosses can we expect until the first heads (not including the heads itself)? Let "X" be this number. Note that we are counting only the tails and not the heads which ends the experiment; in particular, we can have "X" = 0. The expectation of "X" may be computed by formula_41. This is because, when the first "i" tosses yield tails, the number of tosses is at least "i". The last equality used the formula for a geometric progression, formula_42 where formula_43.
Continuous distribution taking non-negative values.
Analogously with the discrete case above, when a continuous random variable "X" takes only non-negative values, we can use the following formula for computing its expectation (even when the expectation is infinite):
Proof: It is first assumed that "X" has a density "fX"("x"). We present two techniques:
and the bracket vanishes because (see Cumulative distribution function#Derived functions)
formula_46 as formula_47
In case no density exists, it is seen that
History.
The idea of the expected value originated in the middle of the 17th century from the study of the so-called problem of points, which seeks to divide the stakes "in a fair way" between two players who have to end their game before it's properly finished. This problem had been debated for centuries, and many conflicting proposals and solutions had been suggested over the years, when it was posed in 1654 to Blaise Pascal by French writer and amateur mathematician Chevalier de Méré. de Méré claimed that this problem couldn't be solved and that it showed just how flawed mathematics was when it came to its application to the real world. Pascal, being a mathematician, was provoked and determined to solve the problem once and for all. He began to discuss the problem in a now famous series of letters to Pierre de Fermat. Soon enough they both independently came up with a solution. They solved the problem in different computational ways but their results were identical because their computations were based on the same fundamental principle. The principle is that the value of a future gain should be directly proportional to the chance of getting it. This principle seemed to have come naturally to both of them. They were very pleased by the fact that they had found essentially the same solution and this in turn made them absolutely convinced they had solved the problem conclusively. However, they did not publish their findings. They only informed a small circle of mutual scientific friends in Paris about it.
Three years later, in 1657, a Dutch mathematician Christiaan Huygens, who had just visited Paris, published a treatise (see ) "De ratiociniis in ludo aleæ" on probability theory. In this book he considered the problem of points and presented a solution based on the same principle as the solutions of Pascal and Fermat. Huygens also extended the concept of expectation by adding rules for how to calculate expectations in more complicated situations than the original problem (e.g., for three or more players). In this sense this book can be seen as the first successful attempt of laying down the foundations of the theory of probability.
In the foreword to his book, Huygens wrote: "It should be said, also, that for some time some of the best mathematicians of France have occupied themselves with this kind of calculus so that no one should attribute to me the honour of the first invention. This does not belong to me. But these savants, although they put each other to the test by proposing to each other many questions difficult to solve, have hidden their methods. I have had therefore to examine and go deeply for myself into this matter by beginning with the elements, and it is impossible for me for this reason to affirm that I have even started from the same principle. But finally I have found that my answers in many cases do not differ from theirs." (cited by ). Thus, Huygens learned about de Méré's Problem in 1655 during his visit to France; later on in 1656 from his correspondence with Carcavi he learned that his method was essentially the same as Pascal's; so that before his book went to press in 1657 he knew about Pascal's priority in this subject.
Neither Pascal nor Huygens used the term "expectation" in its modern sense. In particular, Huygens writes: "That my Chance or Expectation to win any thing is worth just such a Sum, as wou'd procure me in the same Chance and Expectation at a fair Lay. ... If I expect a or b, and have an equal Chance of gaining them, my Expectation is worth a+b/2." More than a hundred years later, in 1814, Pierre-Simon Laplace published his tract "Théorie analytique des probabilités", where the concept of expected value was defined explicitly:
… this advantage in the theory of chance is the product of the sum hoped for by the probability of obtaining it; it is the partial sum which ought to result when we do not wish to run the risks of the event in supposing that the division is made proportional to the probabilities. This division is the only equitable one when all strange circumstances are eliminated; because an equal degree of probability gives an equal right for the sum hoped for. We will call this advantage "mathematical hope".
The use of the letter "E" to denote expected value goes back to W.A. Whitworth in 1901, who used a script E. The symbol has become popular since for English writers it meant "Expectation", for Germans "Erwartungswert", and for French "Espérance mathématique".

</doc>
<doc id="9656" url="http://en.wikipedia.org/wiki?curid=9656" title="Electric light">
Electric light

An electric light is a device that produces visible light by the flow of electric current. It is the most common form of artificial lighting and is essential to modern society, providing interior lighting for buildings and exterior light for evening and nighttime activities. Before electric lighting became common in the early 20th century, people used candles, gas lights, oil lamps, and fires. Most electric lighting is powered by centrally generated electric power, but lighting may also be powered by mobile or standby electric generators or battery systems. Battery-powered lights, usually called "flashlights" or "torches", are used for portability and as backups when the main lights fail.
The two main categories of electric lights are "incandescent lamps", which produce light by a filament heated white-hot by electric current, and "gas-discharge lamps", which produce light by means of an electric arc through a gas. The energy efficiency of electric lighting has increased radically since the first demonstration of arc lamps and the incandescent light bulb of the 19th century. Modern electric light sources come in a profusion of types and sizes adapted to a myriad of applications. The word "lamp" can refer either to a light source or an or the appliance that holds the source.
Types.
Types of electric lighting include:
Different types of lights have vastly differing efficiencies and color of light. 
The most efficient source of electric light is the low-pressure sodium lamp. It produces, for all practical purposes, a monochromatic orange/yellow light, which gives a similarly monochromatic perceprtion of any illuminated scene. For this reason, it is generally reserved for outdoor public lighting usages. Low-pressure sodium lights are favoured for public lighting by astronomers, since the light pollution that they generate can be easily filtered, contrary to broadband or continuous spectra.
Incandescent light bulb.
The modern incandescent lightbulb, with a coiled filament of tungsten, was commercialized in the 1920s developed from the carbon filament lamp introduced in about 1880. As well as bulbs for normal illumination, there is a very wide range, including low voltage, low-power types often used as components in equipment, but now largely displaced by LEDs
There is currently interest in banning some types of filament lamp in some countries, such as Australia planning to ban standard incandescent light bulbs by 2010, because they are inefficient at converting electricity to light. Sri Lanka has already banned importing filament bulbs because of high use of electricity and less light. Less than 3% of the input energy is converted into usable light. Nearly all of the input energy ends up as heat that, in warm climates, must then be removed from the building by ventilation or air conditioning, often resulting in more energy consumption. In colder climates where heating and lighting is required during the cold and dark winter months, the heat byproduct has at least some value.
Halogen lamp.
Halogen lamps are usually much smaller than standard incandescents, because for successful operation a bulb temperature over 200 °C is generally necessary. For this reason, most have a bulb of fused silica (quartz), but sometimes aluminosilicate glass. This is often sealed inside an additional layer of glass. The outer glass is a safety precaution, reducing UV emission and because halogen bulbs can occasionally explode during operation. One reason is if the quartz bulb has oily residue from fingerprints. The risk of burns or fire is also greater with bare bulbs, leading to their prohibition in some places unless enclosed by the luminaire.
Those designed for 12 V or 24 V operation have compact filaments, useful for good optical control, also they have higher efficiencies (lumens per watt) and better lives than non halogen types. The light output remains almost constant throughout life.
Fluorescent lamp.
Fluorescent lamps consist of a glass tube that contains mercury vapour or argon under low pressure. Electricity flowing through the tube causes the gases to give off ultraviolet energy. The inside of the tubes are coated with phosphors that give off visible light when struck by ultraviolet energy. have much higher efficiency than Incandescent lamps. For the same amount of light generated, they typically use around one-quarter to one-third the power of an incandescent.
LED lamp.
Solid state LEDs have been popular as indicator lights since the 1970s. In recent years, efficacy and output have risen to the point where LEDs are now being used in niche lighting applications.
Indicator LEDs are known for their extremely long life, up to 100,000 hours, but lighting LEDs are operated much less conservatively (due to high LED cost per watt), and consequently have much shorter lives.
Due to the relatively high cost per watt, LED lighting is most useful at very low powers, typically for lamp assemblies of under 10 W. LEDs are currently most useful and cost-effective in low power applications, such as nightlights and flashlights. Colored LEDs can also be used for accent lighting, such as for glass objects, and even in fake ice cubes for drinks at parties. They are also being increasingly used as holiday lighting.
LED efficiencies vary over a very wide range. Some have lower efficiency than filament lamps, and some significantly higher. LED performance in this respect is prone to being misinterpreted, as the inherent directionality of LEDs gives them a much higher light intensity in one direction per given total light output.
Single color LEDs are well developed technology, but white LEDs at time of writing still have some unresolved issues.
LED technology is useful for lighting designers because of its low power consumption, low heat generation, instantaneous on/off control, and in the case of single color LEDs, continuity of color throughout the life of the diode and relatively low cost of manufacture.
In the last few years, software has been developed to merge lighting and video by enabling lighting designers to stream video content to their LED fixtures, creating low resolution video walls.
For general domestic lighting, total cost of ownership of LED lighting is still much higher than for other well established lighting types. 
Carbon arc lamp.
Carbon arc lamps consist of two carbon rod electrodes in open air, supplied by a current-limiting ballast. The electric arc is struck by touching the rods then separating them. The ensuing arc heats the carbon tips to white heat. These lamps have higher efficiency than filament lamps, but the carbon rods are short lived and require constant adjustment in use. The lamps produce significant ultra-violet output, they require ventilation when used indoors, and due to their intensity they need protecting from direct sight.
Invented by Humphry Davy around 1805, the carbon arc was the first practical electric light. They were used commercially beginning in the 1870s for large building and street lighting until they were superseded in the early 20th century by the incandescent light. Carbon arc lamps operate at high powers and produce high intensity white light. They also are a point source of light. They remained in use in limited applications that required these properties, such as movie projectors, stage lighting, and searchlights, until after World War 2.
Discharge lamp.
A discharge lamp has a glass or silica envelope containing two metal electrodes separated by a gas. Gases used include, neon, argon, xenon, sodium, metal halide, and mercury.
The core operating principle is much the same as the carbon arc lamp, but the term 'arc lamp' is normally used to refer to carbon arc lamps, with more modern types of gas discharge lamp normally called discharge lamps.
With some discharge lamps, very high voltage is used to strike the arc. This requires an electrical circuit called an igniter, which is part of the ballast circuitry. After the arc is struck, the internal resistance of the lamp drops to a low level, and the ballast limits the current to the operating current. Without a ballast, excess current would flow, causing rapid destruction of the lamp.
Some lamp types contain a little neon, which permits striking at normal running voltage, with no external ignition circuitry. Low pressure sodium lamps operate this way.
The simplest ballasts are just an inductor, and are chosen where cost is the deciding factor, such as street lighting. More advanced electronic ballasts may be designed to maintain constant light output over the life of the lamp, may drive the lamp with a square wave to maintain completely flicker-free output, and shut down in the event of certain faults.
Lamp life expectancy.
Life expectancy is defined as the number of hours of operation for a lamp until 50% of them fail. This means that it is possible for some lamps to fail after a short amount of time and for some to last significantly longer than the rated lamp life. This is an average (median) life expectancy. Production tolerances as low as 1% can create a variance of 25% in lamp life. For LEDs, lamp life is when 50% of lamps have lumen output drop to 70% or less.
Lamps are also sensitive to switching cycles. The rapid heating of a lamp filament or electrodes when a lamp is turned on is the most stressful event on the lamp. Most test cycles have the lamps on for 3 hours and then off for 20 minutes. (Some standard had to be used since it is unknown how the lamp will be used by consumers.) This switching cycle repeats until the lamps fail and the data is recorded. If switching is increased to only 1 hour on, the lamp life is usually reduced because the number of times the lamp has been turned on has increased. Rooms with frequent switching (bathroom, bedrooms, etc.) can expect much shorter lamp life than what is printed on the box.
Public lighting.
The total amount of artificial light (especially from street light) is sufficient for cities to be easily visible at night from the air, and from space. This light is the source of light pollution that burdens astronomers and others.
 <br>

</doc>
<doc id="9657" url="http://en.wikipedia.org/wiki?curid=9657" title="Edgar Rice Burroughs">
Edgar Rice Burroughs

Edgar Rice Burroughs (September 1, 1875 – March 19, 1950) was an American writer best known for his creations of the jungle hero Tarzan and the heroic Mars adventurer John Carter, although he produced works in many genres.
Biography.
Early life.
Burroughs was born on September 1, 1875, in Chicago, Illinois (he later lived for many years in the suburb of Oak Park), the fourth son of businessman and Civil War veteran Major George Tyler Burroughs (1833–1913) and his wife Mary Evaline (Zieger) Burroughs (1840–1920). His middle name is from his paternal grandmother, Mary Rice Burroughs (1802–ca. 70).
Burroughs was educated at a number of local schools, and during the Chicago influenza epidemic in 1891, he spent a half year at his brother's ranch on the Raft River in Idaho. He then attended the Phillips Academy in Andover, Massachusetts, and then the Michigan Military Academy. Graduating in 1895, and failing the entrance exam for the United States Military Academy (West Point), he ended up as an enlisted soldier with the 7th U.S. Cavalry in Fort Grant, Arizona Territory. After being diagnosed with a heart problem and thus ineligible to serve, he was discharged in 1897.
Adulthood.
After his discharge, Burroughs worked a number of different jobs. He drifted and worked on a ranch in Idaho. Then, Burroughs found work at his father's firm in 1899. He married his childhood sweetheart Emma Hulbert (1876-1944) in January 1900. In 1904, he left his job and worked less regularly, first in Idaho, then in Chicago.
By 1911, after seven years of low wages, he was working as a pencil-sharpener wholesaler and began to write fiction. By this time, Burroughs and Emma had two children, Joan (1908–72), who would later marry Tarzan film actor James Pierce, and Hulbert (1909–91). During this period, he had copious spare time and he began reading many pulp fiction magazines. In 1929 he recalled thinking that
...if people were paid for writing rot such as I read in some of those magazines, that I could write stories just as rotten. As a matter of fact, although I had never written a story, I knew absolutely that I could write stories just as entertaining and probably a whole lot more so than any I chanced to read in those magazines.
In the 1920s Burroughs became a pilot, purchasing an Security Airster S-1, encourages his family to learn to fly.
Burroughs divorced Emma in 1934, and in 1935 he married former actress Florence Gilbert Dearholt, the former wife of his friend, Ashton Dearholt. Burroughs adopted the Dearholts' two children. He and Florence divorced in 1942.
Burroughs was in his late 60s and a resident of Hawaii at the time of the Japanese attack on Pearl Harbor. Despite his age, he applied for and received permission to become a war correspondent, becoming one of the oldest U.S. war correspondents during World War II. This period of his life is mentioned in William Brinkley's bestselling novel "Don't Go Near the Water".
American film director Wes Anderson is Burroughs' great-grandson.
American actor Reid Markel is Burroughs' great-great- grandson.
The Science Fiction Hall of Fame inducted Burroughs in 2003.
Death.
After the war ended, Burroughs moved back to Encino, California, where, after many health problems, he died of a heart attack on March 19, 1950, having written almost 80 novels.
Literary career.
Aiming his work at the pulps, Burroughs had his first story, "Under the Moons of Mars", serialized by Frank Munsey in the February to July 1912 issues of "The All-Story" – under the name "Norman Bean" to protect his reputation. "Under the Moons of Mars" inaugurated the Barsoom series and earned Burroughs US$400 ($<br>{Inflation} - Amount must not have "" prefix: 400.   today). It was first published as a book by A. C. McClurg of Chicago in 1917, entitled "A Princess of Mars", after three Barsoom sequels had appeared as serials, and McClurg had published the first four serial Tarzan novels as books.
Burroughs soon took up writing full-time and by the time the run of "Under the Moons of Mars" had finished he had completed two novels, including "Tarzan of the Apes", published from October 1912 and one of his most successful series. In 1913, Burroughs and Emma had their third and last child, John Coleman Burroughs (1913–79).
Burroughs also wrote popular science fiction and fantasy stories involving Earthly adventurers transported to various planets (notably Barsoom, Burroughs's fictional name for Mars, and Amtor, his fictional name for Venus), lost islands, and into the interior of the hollow earth in his "Pellucidar" stories, as well as westerns and historical romances. Along with "All-Story", many of his stories were published in "The Argosy" magazine.
Tarzan was a cultural sensation when introduced. Burroughs was determined to capitalize on Tarzan's popularity in every way possible. He planned to exploit Tarzan through several different media including a syndicated Tarzan comic strip, movies and merchandise. Experts in the field advised against this course of action, stating that the different media would just end up competing against each other. Burroughs went ahead, however, and proved the experts wrong – the public wanted Tarzan in whatever fashion he was offered. Tarzan remains one of the most successful fictional characters to this day and is a cultural icon.
In either 1915 or 1919, Burroughs purchased a large ranch north of Los Angeles, California, which he named "Tarzana." The citizens of the community that sprang up around the ranch voted to adopt that name when their community, Tarzana, California was formed in 1927. Also, the unincorporated community of Tarzan, Texas, was formally named in 1927 when the US Postal Service accepted the name, reputedly coming from the popularity of the first (silent) "Tarzan of the Apes" film, starring Elmo Lincoln, and an early "Tarzan" comic strip.
In 1923 Burroughs set up his own company, Edgar Rice Burroughs, Inc., and began printing his own books through the 1930s.
Literary criticism.
In a "Paris Review" interview, Ray Bradbury said of Burroughs that "Edgar Rice Burroughs never would have looked upon himself as a social mover and shaker with social obligations. But as it turns out – and I love to say it because it upsets everyone terribly – Burroughs is probably the most influential writer in the entire history of the world." Bradbury continued that "By giving romance and adventure to a whole generation of boys, Burroughs caused them to go out and decide to become special."
Few critical books have arisen concerning Burroughs. From an academic standpoint, the most helpful are Erling Holtsmark's two books: "Tarzan and Tradition" and "Edgar Rice Burroughs"; Stan Galloway's "The Teenage Tarzan: A Literary Analysis of Edgar Rice Burroughs' "Jungle Tales of Tarzan; and Richard Lupoff's two books: "Master of Adventure: Edgar Rice Burroughs" and "Barsoom: Edgar Rice Burroughs and the Martian Vision". Galloway was identified by James Gunn (author) as "one of the half-dozen finest Burroughs scholars in the world"; Galloway called Holtsmark his "most important predecessor."
Selected works.
"Moon" series.
These three texts have been published by various houses in one or two volumes. Adding to the confusion, some editions have the original (significantly longer) introduction to Part I from the first publication as a magazine serial, and others have the shorter version from the first book publication, which included all three parts under the title "The Moon Maid".

</doc>
<doc id="9658" url="http://en.wikipedia.org/wiki?curid=9658" title="Eugène Viollet-le-Duc">
Eugène Viollet-le-Duc

Eugène Emmanuel Viollet-le-Duc (27 January 1814 – 17 September 1879) was a French architect and theorist, famous for his interpretive "restorations" of medieval buildings. Born in Paris, he was a major Gothic Revival architect.
His works were largely restorative and few of his independent building designs were ever realised. Strongly contrary to the prevailing Beaux-Arts architectural trend of his time, much of his design work was largely derided by his contemporaries. He was the architect hired to design the internal structure of the Statue of Liberty, but died before the project was completed.
Early years.
Viollet-le-Duc's father was a civil servant living in Paris who collected books; his mother's Friday salons were attended by Stendhal and Sainte-Beuve. His mother's brother, Étienne-Jean Delécluze, "a painter in the mornings, a scholar in the evenings", was largely in charge of the young man's education. Viollet-le-Duc was trendy philosophically: republican, anti-clerical, rebellious, he built a barricade in the July Revolution of 1830 and refused to enter the École des Beaux-Arts. Instead he opted in favor of direct practical experience in the architectural offices of Jacques-Marie Huvé and Achille Leclère.
Architectural restorer.
Restoration work.
During the early 1830s, a popular sentiment for the restoration of medieval buildings developed in France. Viollet-le-Duc, returning during 1835 from study in Italy, was commissioned by Prosper Mérimée to restore the Romanesque abbey of Vézelay. This work was the first of a long series of restorations; Viollet-le-Duc's restorations at Notre Dame de Paris brought him national attention. His other main works include Mont Saint-Michel, Carcassonne, Roquetaillade castle and Pierrefonds.
Viollet-le-Duc's "restorations" frequently combined historical fact with creative modification. For example, under his supervision, Notre Dame was not only cleaned and restored but also "updated", gaining its distinctive third tower (a type of flèche) in addition to other smaller changes. Another of his most famous restorations, the medieval fortified town of Carcassonne, was similarly enhanced, gaining atop each of its many wall towers a set of pointed roofs that are actually more typical of northern France.
At the same time, in the cultural atmosphere of the Second Empire theory necessarily became diluted in practice: Viollet-le-Duc provided a Gothic reliquary for the relic of the Crown of Thorns at Notre-Dame in 1862, and yet Napoleon III also commissioned designs for a luxuriously appointed railway carriage from Viollet-le-Duc, in 14th-century Gothic style.
Among his restorations were:
Restoration of the Château de Pierrefonds, reinterpreted by Viollet-le-Duc for Napoleon III, was interrupted by the departure of the Emperor in 1870.
Influence on historic preservation.
Basic intervention theories of historic preservation are framed in the dualism of the retention of the status quo versus a "restoration" that creates something that never actually existed in the past. John Ruskin was a strong proponent of the former sense, while his contemporary, Viollet-le-Duc, advocated for the latter instance. Viollet-le-Duc wrote that restoration is a "means to reestablish [a building] to a finished state, which may in fact never have actually existed at any given time." The type of restoration employed by Viollet-le-Duc, in its English form as Victorian restoration, was decried by Ruskin as "a destruction out of which no remnants can be gathered: a destruction accompanied with false description of the thing destroyed."
This argument is still a current one when restoration is being considered for a building or landscape. In removing layers of history from a building, information and age value are also removed which can never be recreated. However, adding features to a building, as Viollet-le-Duc also did, can be more appealing to modern viewers.
Publications.
Throughout his career Viollet-le-Duc made notes and drawings, not only for the buildings he was working on, but also on Romanesque, Gothic and Renaissance buildings that were to be soon demolished. His notes were helpful in his published works. His study of medieval and Renaissance periods was not limited to architecture, but extended to furniture, clothing, musical instruments, armament, geology and so forth.
All this work was published, first in serial, and then as full-scale books, as:
Architectural theory and new building projects.
Viollet-le-Duc is considered by many to be the first theorist of modern architecture. Sir John Summerson wrote that "there have been two supremely eminent theorists in the history of European architecture - Leon Battista Alberti and Eugène Viollet-le-Duc."
His architectural theory was largely based on finding the ideal forms for specific materials, and using these forms to create buildings. His writings centered on the idea that materials should be used 'honestly'. He believed that the outward appearance of a building should reflect the rational construction of the building. In "Entretiens sur l'architecture", Viollet-le-Duc praised the Greek temple for its rational representation of its construction. For him, "Greek architecture served as a model for the correspondence of structure and appearance." There is speculation that this philosophy was heavily influenced by the writings of John Ruskin, who championed honesty of materials as one of the seven main emphases of architecture.
In several unbuilt projects for new buildings, Viollet-le-Duc applied the lessons he had derived from Gothic architecture, applying its rational structural systems to modern building materials such as cast iron. He also examined organic structures, such as leaves and animal skeletons, for inspiration. He was especially interested in the wings of bats, an influence represented by his Assembly Hall project.
Viollet-le-Duc's drawings of iron trusswork were innovative for the time. Many of his designs emphasizing iron would later influence the Art Nouveau movement, most noticeably in the work of Hector Guimard, Victor Horta, Antoni Gaudí or Hendrik Petrus Berlage. His writings inspired some American architects, including Frank Furness, John Wellborn Root, Louis Sullivan, and Frank Lloyd Wright.
Military career and influence.
Viollet-le-Duc had a second career in the military, primarily in the defence of Paris during the Franco-Prussian War (1870-1). He was so influenced by the conflict that during his later years he described the idealized defense of France by the analogy of the military history of Le Roche-Pont, an imaginary castle, in his work "Histoire d'une Forteresse" ("Annals of a Fortress", twice translated into English). Accessible and well researched, it is partly fictional.
"Annals of a Fortress" strongly influenced French military defensive thinking. Viollet-le-Duc's critique of the effect of artillery (applying his practical knowledge from the 1870–1871 war) is so complete that it accurately describes the principles applied to the defence of France until World War II. The physical results of his theories are present in the fortification of Verdun prior to World War I and the Maginot Line prior to World War II. His theories are also represented by the French military theory of "Deliberate Advance", such that artillery and a strong system of fortresses in the rear of an army are essential.
Legacy.
Some of his restorations, such as that of the Château de Pierrefonds, have become very controversial because they were not intended so much to recreate a historical situation accurately as to create a "perfect building" of medieval style: "to restore an edifice", he observed in the "Dictionnaire raisonné", "is not to maintain it, repair or rebuild it, but to re-establish it in a complete state that may never have existed at a particular moment". The idea and the very word "restoration" applied to architecture Viollet-le-Duc considered part of a modern innovation. Modern conservation practice considers Viollet-le-Duc's restorations too free, too personal, too interpretive, but some of the monuments he restored might have been lost otherwise.
The English architect Benjamin Bucknall (1833–95) was a devotee of Viollet-le-Duc and during 1874 to 1881 translated several of his publications into English to popularise his principles in Great Britain. The later works of the English designer and architect William Burges were greatly influenced by Viollet-le-Duc, most strongly in Burges's designs for his own home, The Tower House in London's Holland Park district and Burges's designs for Castell Coch near Cardiff, Wales.
An exhibition, "Eugène Viollet-le-Duc 1814–1879" was presented in Paris, 1965, and a larger, centennial exhibition, 1980.
Viollet-le-Duc was the subject of a Google Doodle on January 27, 2014.
Later life.
In 1874 Viollet-le-Duc resigned as diocesan architect of Paris, and was succeeded by his contemporary, Paul Abadie. In his old age, Viollet-le-Duc relocated to Lausanne, Switzerland, where he constructed a villa (since destroyed). He died there in 1879.

</doc>
<doc id="9659" url="http://en.wikipedia.org/wiki?curid=9659" title="Endocarditis">
Endocarditis

Endocarditis is an inflammation of the inner layer of the heart, the endocardium. It usually involves the heart valves. Other structures that may be involved include the interventricular septum, the chordae tendineae, the mural endocardium, or the surfaces of intracardiac devices. Endocarditis is characterized by lesions, known as "vegetations", which is a mass of platelets, fibrin, microcolonies of microorganisms, and scant inﬂammatory cells. In the subacute form of infective endocarditis, the vegetation may also include a center of granulomatous tissue, which may fibrose or calcify.
There are several ways to classify endocarditis. The simplest classification is based on cause: either "infective" or "non-infective", depending on whether a microorganism is the source of the inflammation or not. Regardless, the diagnosis of endocarditis is based on clinical features, investigations such as an echocardiogram, and blood cultures demonstrating the presence of endocarditis-causing microorganisms. Signs and symptoms include: fever, chills, sweating, malaise, weakness, anorexia, weight loss, splenomegaly, flu like feeling, cardiac murmur, heart failure, patechia of anterior trunk, Janeway's lesions, etc.
Cause.
Infective.
Since the valves of the heart do not receive any dedicated blood supply, defensive immune mechanisms (such as white blood cells) cannot directly reach the valves via the bloodstream. If an organism (such as bacteria) attaches to a valve surface and forms a vegetation, the host immune response is blunted. The lack of blood supply to the valves also has implications on treatment, since drugs also have difficulty reaching the infected valve.
Normally, blood flows smoothly past these valves. If they have been damaged (from rheumatic fever, for example) the risk of bacteria attachment is increased.
Rheumatic fever is common worldwide and responsible for many cases of damaged heart valves. Chronic rheumatic heart disease is characterized by repeated inflammation with fibrinous resolution. The cardinal anatomic changes of the valve include leaflet thickening, commissural fusion, and shortening and thickening of the tendinous cords. The recurrence of rheumatic fever is relatively common in the absence of maintenance of low dose antibiotics, especially during the first three to five years after the first episode. Heart complications may be long-term and severe, particularly if valves are involved. While rheumatic fever since the advent of routine penicillin administration for Strep throat has become less common in developed countries, in the older generation and in much of the less-developed world, valvular disease (including mitral valve prolapse, reinfection in the form of valvular endocarditis, and valve rupture) from undertreated rheumatic fever continues to be a problem.
In an Indian hospital between 2004 and 2005, 4 of 24 endocarditis patients failed to demonstrate classic vegetation. All had rheumatic heart disease and presented with prolonged fever. All had severe eccentric mitral regurgitation. (One had severe aortic regurgitation also.) One had flail posterior mitral leaflet.
Non-infective.
Nonbacterial thrombotic endocarditis (NBTE), also called marantic endocarditis is most commonly found on previously undamaged valves. As opposed to infective endocarditis, the vegetations in NBTE are small, sterile, and tend to aggregate along the edges of the valve or the cusps. Also unlike infective endocarditis, NBTE does not cause an inflammation response from the body. NBTE usually occurs during a hypercoagulable state such as system wide bacterial infection, or pregnancy, though it is also sometimes seen in patients with venous catheters. NBTE may also occur in patients with cancers, particularly mucinous adenocarcinoma where Trousseau syndrome can be encountered. Typically NBTE does not cause many problems on its own, but parts of the vegetations may break off and embolize to the heart or brain, or they may serve as a focus where bacteria can lodge, thus causing infective endocarditis.
Another form of sterile endocarditis, is termed Libman-Sacks endocarditis; this form occurs more often in patients with lupus erythematosus and is thought to be due to the deposition of immune complexes. Like NBTE, Libman-Sacks endocarditis involves small vegetations, while infective endocarditis is composed of large vegetations. These immune complexes precipitate an inflammation reaction, which helps to differentiate it from NBTE. Also unlike NBTE, Libman-Sacks endocarditis does not seem to have a preferred location of deposition and may form on the undersurfaces of the valves or even on the endocardium.
Diagnostics.
Examination of suspected infective endocarditis includes a detailed examination of the patient, complete history taking, and especially careful cardiac auscultation, various blood tests, ECG, cardiac ultrasound (echocardiography). In the overall analysis of blood revealed the typical signs of inflammation (increased erythrocyte sedimentation rate, leukocytosis). It is also necessary to sow twice venous blood in order to identify the specific pathogen (this requires two samples of blood). Negative blood cultures, however, does not exclude the diagnosis of infective endocarditis. The decisive role played by echocardiography in the diagnosis (through the anterior chest wall or transesophageal), with which you can reliably establish the presence of microbial vegetation, the degree of valvular and violations of the pumping function of the heart.
External links.
 at DMOZ

</doc>
<doc id="9660" url="http://en.wikipedia.org/wiki?curid=9660" title="Euler's sum of powers conjecture">
Euler's sum of powers conjecture

Euler's conjecture is a disproved conjecture in mathematics related to Fermat's last theorem. It was proposed by Leonhard Euler in 1769. It states that for all integers "n" and "k" greater than 1, if the sum of "n" "k"th powers of non-zero integers is itself a "k"th power, then "n" is greater than or equal to "k".
In symbols, the conjecture falsely states that if
formula_1
where formula_2 and formula_3 are non-zero integers, then formula_4.
The conjecture represents an attempt to generalize Fermat's last theorem, which is the special case "n" = 2: if formula_5, then formula_6.
Although the conjecture holds for the case "k" = 3 (which follows from Fermat's last theorem for the third powers), it was disproved for "k" = 4 and "k" = 5. It is unknown whether the conjecture fails or holds for any value "k" ≥ 6.
Background.
Euler had an equality for four fourth powers formula_7 this however is not a counterexample because no term is isolated on one side of the equation. He also provided a complete solution to the four cubes problem as in Plato's number formula_8 or the taxicab number 1729. The general solution for:
is
where formula_12 and formula_13 are any integers.
Counterexamples.
Euler's conjecture was disproven by L. J. Lander and T. R. Parkin in 1966 when, through a direct computer search on a CDC 6600, they found a counterexample for "k" = 5. A total of three primitive (that is, in which the summands do not all have a common factor) counterexamples are known:
In 1986, Noam Elkies found a method to construct an infinite series of counterexamples for the "k" = 4 case. His smallest counterexample was
A particular case of Elkies' solutions can be reduced to the identity
where
This is an elliptic curve with a rational point at "v"1 = −31/467. From this initial rational point, one can compute an infinite collection of others. Substituting "v"1 into the identity and removing common factors gives the numerical example cited above.
In 1988, Roger Frye found the smallest possible counterexample 
for "k" = 4 by a direct computer search using techniques suggested by Elkies. This solution is the only one with values of the variables below 1,000,000.
Generalizations.
In 1967, L. J. Lander, T. R. Parkin, and John Selfridge conjectured that if "k" > 3 and formula_14, where "ai" ≠ "bj" are positive integers for all 1 ≤ "i" ≤ "n" and 1 ≤ "j" ≤ "m", then "m"+"n" ≥ "k". In the special case "m" = 1, the conjecture states that if
(under the conditions given above) then "n" ≥ "k" − 1.
The special case may be described as the problem of giving a partition of a perfect power into few like powers. For "k" = 4, 5, 7, 8 and "n" = "k" or "k" − 1, there are many known solutions. Some of these are listed below. There are no solutions for "k" = 6 where "b" ≤ 272580.

</doc>
<doc id="9662" url="http://en.wikipedia.org/wiki?curid=9662" title="Book of Exodus">
Book of Exodus

The Book of Exodus or, simply, Exodus (from Greek ἔξοδος, "exodos", meaning "going out"; Hebrew: שמות‎, "Sh'mot", "Names"), is the second book of the Torah and the Hebrew Bible (the Old Testament).
The book tells how the Israelites leave slavery in Egypt through the strength of Yahweh, the God who has chosen Israel as his people. Led by their prophet Moses they journey through the wilderness to Mount Sinai, where Yahweh promises them the land of Canaan (the "Promised Land") in return for their faithfulness. Israel enters into a covenant with Yahweh who gives them their laws and instructions for the Tabernacle, the means by which he will dwell with them and lead them to the land, and give them peace. 
Traditionally ascribed to Moses himself, modern scholarship sees the book as initially a product of the Babylonian exile (6th century BCE), with final revisions in the Persian post-exilic period (5th century BCE). Carol Meyers in her commentary on Exodus suggests that it is arguably the most important book in the Bible, as it presents the defining features of Israel's identity: memories of a past marked by hardship and escape, a binding covenant with God, who chooses Israel, and the establishment of the life of the community and the guidelines for sustaining it.
Structure.
There is no agreement among scholars on the structure of Exodus. One strong possibility is that it is a diptych (i.e., divided into two parts), with the division between parts 1 and 2 at the crossing of the Red Sea or at the beginning of the theophany (appearance of God) in chapter 19. On this plan, the first part tells of God's rescue of his people from Egypt and their journey under his care to Sinai (chapters 1–19) and the second tells of the covenant between them (chapters 20–40).
Summary.
Egypt's Pharaoh, fearful of the Israelites' numbers, orders that all newborn boys be thrown into the Nile. A Levite woman saves her baby by setting him adrift on the river Nile in an ark of bulrushes. Pharaoh's daughter finds the child, names him Moses, and brings him up as her own. But Moses is aware of his origins, and one day, when grown, he kills an Egyptian overseer who is beating a Hebrew slave and has to flee into Midian. There he marries the daughter of Jethro a priest of Midian, and encounters God in a burning bush. Moses asks God for his name: God replies: "I AM that I AM." God tells Moses to return to Egypt and lead the Hebrews into Canaan, the land promised to Abraham.
Moses returns to Egypt and fails to convince the Pharaoh to release the Israelites. God smites the Egyptians with terrible plagues (Plagues of Egypt) including a river of blood, many frogs, and the death of first-born sons. Moses leads the Israelites out of bondage after a final chase scene ensues when the Pharaoh reneges on his coerced consent (Crossing the Red Sea). The desert proves arduous, and the Israelites complain and long for Egypt, but God provides manna and miraculous water for them. The Israelites arrive at the mountain of God, where Moses' father-in-law Jethro visits Moses; at his suggestion Moses appoints judges over Israel. God asks whether they will agree to be his people. They accept. The people gather at the foot of the mountain, and with thunder and lightning, fire and clouds of smoke, and the sound of trumpets, and the trembling of the mountain, God appears on the peak, and the people see the cloud and hear the voice [or possibly "sound"] of God. Moses and Aaron are told to ascend the mountain. God pronounces the Ten Commandments (the Ethical Decalogue) in the hearing of all Israel. Moses goes up the mountain into the presence of God, who pronounces the Covenant Code (a detailed code of ritual and civil law), and promises Canaan to them if they obey. Moses comes down the mountain and writes down God's words and the people agree to keep them. God calls Moses up the mountain with Aaron and the elders of Israel, and they feast in the presence of God. God calls Moses up the mountain to receive a set of stone tablets containing the law, and he and Joshua go up, leaving Aaron below.
God gives Moses instructions for the construction of the tabernacle so that God could dwell permanently among his chosen people, as well as instructions for the priestly vestments, the altar and its appurtenances, the procedure to be used to ordain the priests, and the daily sacrifices to be offered. Aaron is appointed as the first hereditary high priest. God gives Moses the two tablets of stone containing the words of the ten commandments, written with the "finger of God".
While Moses is with God, Aaron makes a golden calf, which the people worship. God informs Moses of their apostasy and threatens to kill them all, but relents when Moses pleads for them. Moses comes down from the mountain, smashes the stone tablets in anger, and commands the Levites to massacre the unfaithful Israelites. God commands Moses to make two new tablets on which He will personally write the words that were on the first tablets. Moses ascends the mountain, God dictates the Ten Commandments (the Ritual Decalogue), and Moses writes them on the tablets.
Moses descends from the mountain, and his face is transformed, so that from that time onwards he has to hide his face with a veil. Moses assembles the Hebrews and repeats to them the commandments he has received from God, which are to keep the Sabbath and to construct the Tabernacle. "And all the construction of the Tabernacle of the Tent of Meeting was finished, and the children of Israel did according to everything that God had commanded Moses", and from that time God dwelt in the Tabernacle and ordered the travels of the Hebrews.
Composition.
Authorship.
Jewish and Christian tradition viewed Moses as the author of Exodus and the entire Pentateuch, but by the end of the 19th century the increasing awareness of the discrepancies, inconsistencies, repetitions and other features of the Pentateuch had led scholars to abandon this idea. According to current thinking, a first draft (the Yahwist) was probably written in the 6th century BCE during the Babylonian exile; this was supplemented and completed as a post-Exilic final edition (the Priestly source) at the very end of the 6th century or during the 5th century, and further adjustments and minor revisions continued down to the end of the 4th century.
Genre and sources.
The Book of Exodus is not historical narrative in any modern sense. Modern history writing requires the critical evaluation of sources, and does not accept God as a cause of events. But in Exodus, everything is presented as the work of God, who appears frequently in person, and the historical setting is only very hazily sketched. The purpose of the book is not to record what really happened, but to reflect the historical experience of the exile community in Babylon and later Jerusalem, facing foreign captivity and the need to come to terms with their understanding of God.
Although mythical elements are not so prominent in Exodus as in Genesis, the echoes of ancient legends are crucial to understanding the book's origins and purpose: for example, the story of the infant Moses's salvation from the Nile has its basis in an earlier legend of king Sargon, while the story of the parting of the Red Sea trades on Mesopotamian creation mythology. Similarly, the Covenant Code (the law code in Exodus 20:22–23:33) has notable similarities in both content and structure with the Laws of Hammurabi. These influences serve to reinforce the conclusion that the Book of Exodus originated in the exiled Jewish community of 6th-century Babylon, but not all the sources are Mesopotamian: the story of Moses's flight to Midian following the murder of the Egyptian overseer may draw on the Egyptian "Story of Sinuhe".
Themes.
Salvation.
Biblical scholars describe the Bible's theologically-motivated history writing as "salvation history", meaning a history of God's saving actions that give identity to Israel – the promise of offspring and land to the ancestors, the exodus from Egypt ( in which God saves Israel from slavery), the wilderness wandering, the revelation at Sinai, and the hope for the future life in the promised land.
Theophany.
A theophany is a manifestation (appearance) of a god – in the Bible, an appearance of the God of Israel, accompanied by storms – the earth trembles, the mountains quake, the heavens pour rain, thunder peals and lightning flashes. The theophany in Exodus begins "the third day" from their arrival at Sinai in chapter 19: Yahweh and the people meet at the mountain, God appears in the storm and converses with Moses, giving him the Ten Commandments while the people listen. The theophany is therefore a public experience of divine law.
The second half of Exodus marks the point at which, and describes the process through which, God's theophany becomes a permanent presence for Israel via the Tabernacle. That so much of the book (chapters 25–31, 35–40) is spent describing the plans of the Tabernacle demonstrates the importance it played in the perception of Second Temple Judaism at the time of the text's redaction by the Priestly writers: the Tabernacle is the place where God is physically present, where, through the priesthood, Israel could be in direct, literal communion with him.
Covenant.
The heart of Exodus is the Sinaitic covenant. A covenant is a legal document binding two parties to take on certain obligations towards each other. There are several covenants in the Bible, and in each case they exhibit at least some of the elements found in real-life treaties of the ancient Middle East: a preamble, historical prologue, stipulations, deposition and reading, list of witnesses, blessings and curses, and ratification by animal sacrifice. Biblical covenants, in contrast to Eastern covenants in general, are between a god, Yahweh, and a people, Israel, instead of between a strong ruler and a weaker vassal.
Election of Israel.
Israel is elected for salvation because the "sons of Israel" are "the firstborn son" of the God of Israel, descended through Shem and Abraham to the chosen line of Jacob whose name is changed to Israel. The theme of election by birth will later narrow still further, to the line of David, the descendant of Judah, and further in Christianity to Jesus. The goal of the divine plan as revealed in Exodus is a return to humanity's state in Eden, so that God can dwell with the Israelites as he had with Adam and Eve through the Ark and Tabernacle, which together form a model of the universe; in later Abrahamic religions this came to be interpreted as Israel being the guardian of God's plan for humanity, to bring "God's creation blessing to mankind" begun in Adam. Later, the "obedient son", Jesus, ultimately brings about a new creation where the "old Adam and old Israel failed".

</doc>
<doc id="9663" url="http://en.wikipedia.org/wiki?curid=9663" title="Electronics">
Electronics

Electronics is the science of how to control electric energy, energy in which the electrons have a fundamental role. Electronics deals with electrical circuits that involve active electrical components such as vacuum tubes, transistors, diodes and integrated circuits, and associated passive electrical components and interconnection technologies. Commonly, electronic devices contain circuitry consisting primarily or exclusively of active semiconductors supplemented with passive elements; such a circuit is described as an electronic circuit. 
The nonlinear behaviour of active components and their ability to control electron flows makes amplification of weak signals possible, and electronics is widely used in information processing, telecommunication, and signal processing. The ability of electronic devices to act as switches makes digital information processing possible. Interconnection technologies such as circuit boards, electronics packaging technology, and other varied forms of communication infrastructure complete circuit functionality and transform the mixed components into a regular working system.
Electronics is distinct from electrical and electro-mechanical science and technology, which deal with the generation, distribution, switching, storage, and conversion of electrical energy to and from other energy forms using wires, motors, generators, batteries, switches, relays, transformers, resistors, and other passive components. This distinction started around 1906 with the invention by Lee De Forest of the triode, which made electrical amplification of weak radio signals and audio signals possible with a non-mechanical device. Until 1950 this field was called "radio technology" because its principal application was the design and theory of radio transmitters, receivers, and vacuum tubes.
Today, most electronic devices use semiconductor components to perform electron control. The study of semiconductor devices and related technology is considered a branch of solid-state physics, whereas the design and construction of electronic circuits to solve practical problems come under electronics engineering. This article focuses on engineering aspects of electronics.
Branches of Electronics.
Electronics has branches as follows:
1.Digital electronics 
2.Analogue electronics 
3.Microelectronics 
4.Fuzzy electronics
5.Circuit Design
6.Integrated circuit
7.Optoelectronics
8.Semiconductor
9.Semiconductor device
Electronic devices and components.
An electronic component is any physical entity in an electronic system used to affect the electrons or their associated fields in a manner consistent with the intended function of the electronic system. Components are generally intended to be connected together, usually by being soldered to a printed circuit board (PCB), to create an electronic circuit with a particular function (for example an amplifier, radio receiver, or oscillator). Components may be packaged singly, or in more complex groups as integrated circuits. Some common electronic components are capacitors, inductors, resistors, diodes, transistors, etc. Components are often categorized as active (e.g. transistors and thyristors) or passive (e.g. resistors, diodes, inductors and capacitors).
History of electronic components.
Vacuum tubes (Thermionic valves) were one of the earliest electronic components. They were almost solely responsible for the electronics revolution of the first half of the Twentieth Century. They took electronics from parlor tricks and gave us radio, television, phonographs, radar, long distance telephony and much more. They played a leading role in the field of microwave and high power transmission as well as television receivers until the middle of the 1980s. Since that time, solid state devices have all but completely taken over. Vacuum tubes are still used in some specialist applications such as high power RF amplifiers, cathode ray tubes, specialist audio equipment, guitar amplifiers and some microwave devices.
In April 1955 the IBM 608 was the first IBM product to use transistor circuits without any vacuum tubes and is believed to be the world's first all-transistorized calculator to be manufactured for the commercial market. The 608 contained more than 3,000 germanium transistors. Thomas J. Watson Jr. ordered all future IBM products to use transistors in their design. From that time on transistors were almost exclusively used for computer logic and peripherals.
Types of circuits.
Circuits and components can be divided into two groups: analog and digital. A particular device may consist of circuitry that has one or the other or a mix of the two types.
Analog circuits.
Most analog electronic appliances, such as radio receivers, are constructed from combinations of a few types of basic circuits. Analog circuits use a continuous range of voltage or current as opposed to discrete levels as in digital circuits.
The number of different analog circuits so far devised is huge, especially because a 'circuit' can be defined as anything from a single component, to systems containing thousands of components.
Analog circuits are sometimes called linear circuits although many non-linear effects are used in analog circuits such as mixers, modulators, etc. Good examples of analog circuits include vacuum tube and transistor amplifiers, operational amplifiers and oscillators.
One rarely finds modern circuits that are entirely analog. These days analog circuitry may use digital or even microprocessor techniques to improve performance. This type of circuit is usually called "mixed signal" rather than analog or digital.
Sometimes it may be difficult to differentiate between analog and digital circuits as they have elements of both linear and non-linear operation. An example is the comparator which takes in a continuous range of voltage but only outputs one of two levels as in a digital circuit. Similarly, an overdriven transistor amplifier can take on the characteristics of a controlled switch having essentially two levels of output. In fact, many digital circuits are actually implemented as variations of analog circuits similar to this example—after all, all aspects of the real physical world are essentially analog, so digital effects are only realized by constraining analog behavior.
Digital circuits.
Digital circuits are electric circuits based on a number of discrete voltage levels. Digital circuits are the most common physical representation of Boolean algebra, and are the basis of all digital computers. To most engineers, the terms "digital circuit", "digital system" and "logic" are interchangeable in the context of digital circuits.
Most digital circuits use a binary system with two voltage levels labeled "0" and "1". Often logic "0" will be a lower voltage and referred to as "Low" while logic "1" is referred to as "High". However, some systems use the reverse definition ("0" is "High") or are current based. Quite often the logic designer may reverse these definitions from one circuit to the next as he sees fit to facilitate his design. The definition of the levels as "0" or "1" is arbitrary.
Ternary (with three states) logic has been studied, and some prototype computers made.
Computers, electronic clocks, and programmable logic controllers (used to control industrial processes) are constructed of digital circuits. Digital signal processors are another example.
Building blocks:
Highly integrated devices:
Heat dissipation and thermal management.
Heat generated by electronic circuitry must be dissipated to prevent immediate failure and improve long term reliability. Heat dissipation is mostly achieved by passive conduction/convection. Means to achieve greater dissipation include heat sinks and fans for air cooling, and other forms of computer cooling such as water cooling. These techniques use convection, conduction, and radiation of heat energy.
Noise.
Electronic noise is defined as unwanted disturbances superposed on a useful signal that tend to obscure its information content. Noise is not the same as signal distortion caused by a circuit. Noise is associated with all electronic circuits. Noise may be electromagnetically or thermally generated, which can be decreased by lowering the operating temperature of the circuit. Other types of noise, such as shot noise cannot be removed as they are due to limitations in physical properties.
Electronics theory.
Mathematical methods are integral to the study of electronics. To become proficient in electronics it is also necessary to become proficient in the mathematics of circuit analysis.
Circuit analysis is the study of methods of solving generally linear systems for unknown variables such as the voltage at a certain node or the current through a certain branch of a network. A common analytical tool for this is the SPICE circuit simulator.
Also important to electronics is the study and understanding of electromagnetic field theory.
Electronics lab.
Due to the complex nature of electronics theory, laboratory experimentation is an important part of the development of electronic devices. These experiments are used to test or verify the engineer’s design and detect errors. Historically, electronics labs have consisted of electronics devices and equipment located in a physical space, although in more recent years the trend has been towards electronics lab simulation software, such as CircuitLogix, Multisim, and PSpice.
Computer aided design (CAD).
Today's electronics engineers have the ability to design circuits using premanufactured building blocks such as power supplies, semiconductors (i.e. semiconductor devices, such as transistors), and integrated circuits. Electronic design automation software programs include schematic capture programs and printed circuit board design programs. Popular names in the EDA software world are NI Multisim, Cadence (ORCAD), EAGLE PCB and Schematic, Mentor (PADS PCB and LOGIC Schematic), Altium (Protel), LabCentre Electronics (Proteus), gEDA, KiCad and many others.
Construction methods.
Many different methods of connecting components have been used over the years. For instance, early electronics often used point to point wiring with components attached to wooden breadboards to construct circuits. Cordwood construction and wire wrap were other methods used. Most modern day electronics now use printed circuit boards made of materials such as FR4, or the cheaper (and less hard-wearing) Synthetic Resin Bonded Paper (SRBP, also known as Paxoline/Paxolin (trade marks) and FR2) - characterised by its brown colour. Health and environmental concerns associated with electronics assembly have gained increased attention in recent years, especially for products destined to the European Union, with its Restriction of Hazardous Substances Directive (RoHS) and Waste Electrical and Electronic Equipment Directive (WEEE), which went into force in July 2006.
Degradation.
Rasberry crazy ants have been known to consume the insides of electrical wiring, and nest inside of electronics; they prefer DC to AC currents. This behavior is not well understood by scientists. 

</doc>
<doc id="9664" url="http://en.wikipedia.org/wiki?curid=9664" title="Erewhon">
Erewhon

Erewhon: or, Over the Range (Ĕ-rĕ-whŏn) is a novel by Samuel Butler which was first published anonymously in 1872. The title is also the name of a country, supposedly discovered by the protagonist. In the novel, it is not revealed where Erewhon is, but it is clear that it is a fictional country. Butler meant the title to be read as "nowhere" backwards even though the letters "h" and "w" are transposed, as it would have been pronounced in his day (and still is in some dialects of English). The book is a satire on Victorian society.
The first few chapters of the novel dealing with the discovery of Erewhon are in fact based on Butler's own experiences in New Zealand where, as a young man, he worked as a sheep farmer on Mesopotamia Station for about four years (1860–1864), and explored parts of the interior of the South Island and which he wrote about in his "A First Year in Canterbury Settlement" (1863).
Content.
The greater part of the book consists of a description of Erewhon. The nature of this nation is intended to be ambiguous. At first glance, Erewhon appears to be a Utopia, yet it soon becomes clear that this is far from the case. Yet for all the failings of Erewhon, it is also clearly not a dystopia, such as that depicted in George Orwell's "Nineteen Eighty-Four". As a satirical utopia, "Erewhon" has sometimes been compared to "Gulliver's Travels" (1726), a classic novel by Jonathan Swift; the image of Utopia in this latter case also bears strong parallels with the self-view of the British Empire at the time. It can also be compared to the William Morris novel, "News from Nowhere".
"Erewhon" satirises various aspects of Victorian society, including criminal punishment, religion and anthropocentrism. For example, according to Erewhonian law, offenders are treated as if they were ill, whereas ill people are looked upon as criminals. Another feature of Erewhon is the absence of machines; this is due to the widely shared perception by the Erewhonians that they are potentially dangerous. This last aspect of "Erewhon" reveals the influence of Charles Darwin's evolution theory; Butler had read "On the Origin of Species" soon after it was published in 1859.
The Book of the Machines.
Butler developed the three chapters of "Erewhon" that make up "The Book of the Machines" from a number of articles that he had contributed to "The Press", which had just begun publication in Christchurch, New Zealand, beginning with "Darwin among the Machines" (1863). Butler was the first to write about the possibility that machines might develop consciousness by Darwinian Selection. Many dismissed this as a joke; but, in his preface to the second edition, Butler wrote, "I regret that reviewers have in some cases been inclined to treat the chapters on Machines as an attempt to reduce Mr. Darwin's theory to an absurdity. Nothing could be further from my intention, and few things would be more distasteful to me than any attempt to laugh at Mr. Darwin."
Reception.
After its first release, this book sold far better than any of Butler's other works, perhaps because the British public assumed that the anonymous author was some better-known figure (the favourite being Lord Lytton, who had published "The Coming Race" two years previously). In a 1945 broadcast, George Orwell praised the book and said that when Butler wrote "Erewhon" it needed "imagination of a very high order to see that machinery could be dangerous as well as useful." He recommended the novel, though not its sequel, "Erewhon Revisited".
Influence and legacy.
Today scientists and philosophers seriously debate whether computers and robots could develop a kind of consciousness (artificial intelligence, AI), and organic interaction (artificial life) similar to or exceeding that of human beings. This is also a popular theme in science-fiction novels and movies; some raise the same question ("Dune's" "Butlerian Jihad", for example, which was named such as a reference to Erewhon), while others explore what the relationship between human beings and machines with artificial intelligence would be, and even whether AI is desirable. However, it should be noted that Butler wrote of machines developing consciousness by "natural selection", not artificially, although machine algorithms are approaching a level of autonomy which could be considered natural. 
The French philosopher Gilles Deleuze used ideas from Butler's book at various points in the development of his philosophy of difference. In "Difference and Repetition" (1968), Deleuze refers to what he calls "Ideas" as "erewhons." "Ideas are not concepts," he explains, but rather "a form of eternally positive differential multiplicity, distinguished from the identity of concepts." "Erewhon" refers to the "nomadic distributions" that pertain to simulacra, which "are not universals like the categories, nor are they the "hic et nunc" or "now here", the diversity to which categories apply in representation." "Erewhon," in this reading, is "not only a disguised "no-where" but a rearranged "now-here"."
In his collaboration with Félix Guattari, "Anti-Oedipus" (1972), Deleuze draws on Butler's "The Book of the Machines" to "go beyond" the "usual polemic between vitalism and mechanism" as it relates to their concept of "desiring-machines":
For one thing, Butler is not content to say that machines extend the organism, but asserts that they are really limbs and organs lying on the body without organs of a society, which men will appropriate according to their power and their wealth, and whose poverty deprives them as if they were mutilated organisms. For another, he is not content to say that organisms are machines, but asserts that they contain such an abundance of parts that they must be compared to very different parts of distinct machines, each relating to the others, engendered in combination with the others..."He shatters the vitalist argument by calling in question the specific or personal unity of the organism, and the mechanist argument even more decisively, by calling in question the structural unity of the machine.—Deleuze and Guattari, "Anti-Œdipus"
A reference to "Erewhon" and specifically "The Book of Machines" opens Miguel de Unamuno's short story, "Mecanópolis,". This story was written in Spanish and tells of a man who visits a city (called Mecanópolis) which is inhabited solely by machines.
George B. Dyson uses the heading of Butler's original article in "Darwin Among the Machines: The Evolution of Global Intelligence" (1998) ISBN 0-7382-0030-1.
Fritz Leiber's extensive tales of "Fafhrd and the Gray Mouser", written mostly in the 1960s and 1970s, take place on a world called Nehwon ("No When" backwards), a homage to Butler as well as a reference to the occasional contemporary and futuristic elements added to the medieval milieu of the stories.
In Anne McCaffrey's 1988 novel "Nimisha's Ship", the heroine Nimisha is pulled through a wormhole to the far side of the galaxy, and names the planet she settles on "Erehwon" in reference to an "old earth story" that several characters try, but fail, to remember. McCaffrey does not transpose the "h" and "w" as did Butler.
In David Weber's Honorverse series, the planet Erewhon was initially settled by interstellar criminals as a front for organised crime, with many of its place names referencing 21st century laundry appliances.
In 1994, a group of ex-Yugoslavian writers in Amsterdam, who had established the Pen centre of Yugoslav Writers in Exile, published a single issue of a literary journal "Erewhon".
One of New Zealand's largest sheep stations located near where Butler lived is named "Erewhon" in his honour.

</doc>
<doc id="9665" url="http://en.wikipedia.org/wiki?curid=9665" title="Ectopia (medicine)">
Ectopia (medicine)

In medicine, an ectopia is a displacement or malposition of an organ or other body part. Most ectopias are congenital, but some may happen later in life.

</doc>
<doc id="9667" url="http://en.wikipedia.org/wiki?curid=9667" title="Entorhinal cortex">
Entorhinal cortex

The entorhinal cortex (EC) (ento = interior, rhino = nose, entorhinal = interior to the rhinal sulcus) is an area of the brain located in the medial temporal lobe and functioning as a hub in a widespread network for memory and navigation. The EC is the main interface between the hippocampus and neocortex. The EC-hippocampus system plays an important role in declarative (autobiographical/episodic/semantic) memories and in particular spatial memories including memory formation, memory consolidation, and memory optimization in sleep. The EC is also responsible for the pre-processing (familiarity) of the input signals in the reflex nictitating membrane response of classical trace conditioning, the association of impulses from the eye and the ear occurs in the entorhinal cortex.
Anatomy.
In rodents, the EC is located at the caudal end of the temporal lobe. In primates it is located at the rostral end of the temporal lobe and stretches dorsolaterally. It is usually divided into medial and lateral regions with three bands with distinct properties and connectivity running perpendicular across the whole area. A distinguishing characteristic of the EC is the lack of cell bodies where layer IV should be; this layer is called the "lamina dissecans".
Inputs and outputs.
The superficial layers - layers II and III - of EC project to the dentate gyrus and hippocampus: Layer II projects primarily to dentate gyrus and hippocampal region CA3; layer III projects primarily to hippocampal region CA1 and the subiculum. These layers receive input from other cortical areas, especially associational, perirhinal, and parahippocampal cortices, as well as prefrontal cortex. EC as a whole, therefore, receives highly processed input from every sensory modality, as well as input relating to ongoing cognitive processes, though it should be stressed that, within EC, this information remains at least partially segregated.
The deep layers, especially layer V, receive one of the three main outputs of the hippocampus and, in turn, reciprocate connections from other cortical areas that project to superficial EC.
The rodent entorhinal cortex shows a modular organization, with different properties and connections in different areas.
Neuron information processing.
In 2005, it was discovered that entorhinal cortex contains a neural map of the spatial environment in rats. In 2014, John O'Keefe, May-Britt Moser and Edvard Moser received the Nobel Prize for Physiology or Medicine for this discovery.
Neurons in the lateral entorhinal cortex exhibit little spatial selectivity, whereas neurons of the medial entorhinal cortex (MEA), exhibit multiple "place fields" that are arranged in an hexagonal pattern, and are, therefore, called "grid cells". These fields and spacing between fields increase from the dorso-lateral MEA to the ventro-medial MEA.
Single-unit recording of neurons in humans playing video games find path cells in the EC, the activity of which indicates whether a person is taking a clockwise or counterclockwise path. Such EC "direction" path cells show this directional activity irrespective of the location of where a person experiences themselves, which contrasts them to place cells in the hippocampus, which are activated by specific locations.
EC neurons process general information such as directional activity in the environment, which contrasts to that of the hippocampal neurons, which usually encode information about specific places. This suggests that EC encodes general properties about current contexts that are then used by hippocampus to create unique representations from combinations of these properties.
Alzheimer's Disease.
The entorhinal cortex is the first area of the brain to be affected in Alzheimer's Disease; a recent functional magnetic resonance imaging study has localised the area to the lateral entorhinal cortex. Besides, Lopez "et al." have showed, in a nice multimodal study, that there are differences in the volume of the LEFT entorhinal cortex between progressing (to Alzheimer's Disease) and stable mild cognitive impairment patients. These authors also found that the volume of the left entorhinal cortex inversely correlates with the level of alpha band phase synchronization between the right anterior cingulate and temporo-occipital regions.
In 2012, neuroscientists at UCLA conducted an experiment using a virtual taxi video game connected to seven epilepsy patients with electrodes already implanted in their brains, allowing the researchers to monitor neuronal activity whenever memories were being formed. As the researchers stimulated the nerve fibers of each of the patients' entorhinal cortex as they were learning, they were then able to better navigate themselves through various routes and recognize landmarks more quickly. This signified an improvement in the patients' spatial memory.
External links.
For delineating the Entorhinal cortex, see Desikan RS, Ségonne F, Fischl B, Quinn BT, Dickerson BC, Blacker D, Buckner RL, Dale AM, Maguire RP, Hyman BT, Albert MS, Killiany RJ. An automated labeling system for subdividing the human cerebral cortex on MRI scans into gyral based regions of interest. Neuroimage. 2006 Jul 1;31(3):968-80.

</doc>
<doc id="9668" url="http://en.wikipedia.org/wiki?curid=9668" title="Ernst Haeckel">
Ernst Haeckel

Ernst Heinrich Philipp August Haeckel (]; 16 February 1834 – 9 August 1919) was a German biologist, naturalist, philosopher, physician, professor, and artist who discovered, described and named thousands of new species, mapped a genealogical tree relating all life forms, and coined many terms in biology, including "anthropogeny", "ecology", "phylum", "phylogeny", "stem cell", and "Protista." Haeckel promoted and popularised Charles Darwin's work in Germany and developed the influential but no longer widely held recapitulation theory ("ontogeny recapitulates phylogeny") claiming that an individual organism's biological development, or ontogeny, parallels and summarises its species' evolutionary development, or phylogeny.
The published artwork of Haeckel includes over 100 detailed, multi-colour illustrations of animals and sea creatures (see: "Kunstformen der Natur", "Art Forms of Nature"). As a philosopher, Ernst Haeckel wrote "Die Welträtsel" (1895–1899, in English, "The Riddle of the Universe", 1901), the genesis for the term "world riddle" ("Welträtsel"); and "Freedom in Science and Teaching" to support teaching evolution.
Life.
Ernst Haeckel was born on 16 February 1834, in Potsdam (then part of Prussia).
In 1852, Haeckel completed studies at the "Domgymnasium", the cathedral high school of Merseburg.
He then studied medicine in Berlin and Würzburg, particularly with Albert von Kölliker, Franz Leydig, Rudolf Virchow (with whom he later worked briefly as assistant), and with the anatomist-physiologist Johannes Peter Müller (1801–1858). Together with Hermann Steudner he attended botany lectures in Würzburg. In 1857, Haeckel attained a doctorate in medicine, M.D.), and afterwards he received a license to practice medicine. The occupation of physician appeared less worthwhile to Haeckel, after contact with suffering patients.
Haeckel studied under Karl Gegenbaur at the University of Jena for three years, earning a doctorate in zoology, before becoming a professor of comparative anatomy at the University of Jena, where he remained for 47 years, from 1862 to 1909. Between 1859 and 1866, Haeckel worked on many invertebrate groups, including radiolarians, poriferans (sponges) and annelids (segmented worms). During a trip to the Mediterranean, Haeckel named nearly 150 new species of radiolarians.
From 1866 to 1867, Haeckel made an extended journey to the Canary Islands with Hermann Fol and during this period, met with Charles Darwin, in 1866 at Down House in Kent, Thomas Huxley and Charles Lyell. In 1867, he married Agnes Huschke. Their son Walter was born in 1868, their daughters Elizabeth in 1871 and Emma in 1873. In 1869, he traveled as a researcher to Norway, in 1871 to Croatia (lived on the island of Hvar in a monastery), and in 1873 to Egypt, Turkey, and to Greece. Haeckel retired from teaching in 1909, and in 1910 he withdrew from the Evangelical church.
Haeckel's wife, Agnes, died in 1915, and Haeckel became substantially frailer, with a broken leg (thigh) and broken arm. He sold his "Villa Medusa" in Jena in 1918 to the Carl Zeiss foundation, and it presently contains a historic library. Haeckel died on 9 August 1919.
Politics.
Haeckel's political beliefs were influenced by his affinity for the German Romantic movement coupled with his acceptance of a form of Lamarckism. Rather than being a strict Darwinian, Haeckel believed that racial characteristics were acquired through interactions with the environment and that ontogeny directly followed phylogeny. He believed the social sciences to be instances of "applied biology". Most of these arguments have been shown to be over-generalisations at best and flatly incorrect at worst in modern biology and social studies.
In 1905, Haeckel founded a group called the "Monist League" to promote his religious and political beliefs. This
group lasted until 1933 and included such notable members as Wilhelm Ostwald, Georg von Arco, Helene Stöcker
and Walter Arthur Berendsohn.
"First World War".
Haeckel was the first person known to use the term "First World War". Shortly after the start of the war Haeckel wrote:
There is no doubt that the course and character of the feared "European War"...will become the first world war in the full sense of the word.—"Indianapolis Star", 20 September 1914
The "European War" became known as "The Great War", and it was not until 1920, in the book "The First World War 1914–1918" by Charles à Court Repington, that the term "First World War" was used as the official name for the conflict.
Research.
Haeckel was a zoologist, an accomplished artist and illustrator, and later a professor of comparative anatomy. Although Haeckel's ideas are important to the history of evolutionary theory, and although he was a competent invertebrate anatomist most famous for his work on radiolaria, many speculative concepts that he championed are now considered incorrect. For example, Haeckel described and named hypothetical ancestral microorganisms that have never been found.
He was one of the first to consider psychology as a branch of physiology. He also proposed the kingdom "Protista" in 1866. His chief interests lay in evolution and life development processes in general, including development of nonrandom form, which culminated in the beautifully illustrated "Kunstformen der Natur" ("Art forms of nature"). Haeckel did not support natural selection, rather believing in Lamarckism.
Haeckel advanced a version of the earlier recapitulation theory previously set out by Étienne Serres in the 1820s and supported by followers of Étienne Geoffroy Saint-Hilaire including Robert Edmond Grant. It proposed a link between ontogeny (development of form) and phylogeny (evolutionary descent), summed up by Haeckel in the phrase "ontogeny recapitulates phylogeny". His concept of recapitulation has been refuted in the form he gave it (now called "strong recapitulation"), in favour of the ideas first advanced by Karl Ernst von Baer. The strong recapitulation hypothesis views ontogeny as repeating forms of the ancestors, while weak recapitulation means that what is repeated (and built upon) is the ancestral embryonic development process. Haeckel supported the theory with embryo drawings that have since been shown to be oversimplified and in part inaccurate, and the theory is now considered an oversimplification of quite complicated relationships. Haeckel introduced the concept of heterochrony, the change in timing of embryonic development over the course of evolution.
Haeckel was a flamboyant figure, who sometimes took great, non-scientific leaps from available evidence. For example, at the time when Darwin published "On the Origin of Species by Means of Natural Selection" (1859), Haeckel postulated that evidence of human evolution would be found in the Dutch East Indies (now Indonesia). At that time, no remains of human ancestors had yet been found. He described these theoretical remains in great detail and even named the as-yet unfound species, "Pithecanthropus alalus", and instructed his students such as Richard and Oskar Hertwig to go and find it.
One student did find some remains: a Dutchman named Eugene Dubois searched the East Indies from 1887 to 1895, discovering the remains of Java Man in 1891, consisting of a skullcap, thighbone, and a few teeth. These remains are among the oldest hominid remains ever found. Dubois classified Java Man with Haeckel's "Pithecanthropus" label, though they were later reclassified as "Homo erectus". Some scientists of the day suggested Dubois' Java Man as a potential intermediate form between modern humans and the common ancestor we share with the other great apes. The current consensus of anthropologists is that the direct ancestors of modern humans were African populations of "Homo erectus" (possibly "Homo ergaster"), rather than the Asian populations exemplified by Java Man and Peking Man.
Polygenism and racial theory.
The creationist polygenism of Samuel George Morton and Louis Agassiz, which presented human races as separately created species, was rejected by Charles Darwin, who argued for the monogenesis of the human species and the African origin of modern humans. In contrast to most of Darwin's supporters, Haeckel put forward a doctrine of evolutionary polygenism based on the ideas of the linguist August Schleicher, in which several different language groups had arisen separately from speechless prehuman "Urmenschen", which themselves had evolved from simian ancestors. These separate languages had completed the transition from animals to man, and, under the influence of each main branch of languages, humans had evolved – in a kind of Lamarckian use-inheritance – as separate species, which could be subdivided into races. From this Haeckel drew the implication that languages with the most potential formed human species with the most potential, led by the Semitic and Indo-Germanic groups, with Berber, Jewish, Greco-Roman and Germanic varieties to the fore. As Haeckel stated:
Haeckel's view can be seen as a forerunner of the views of Carleton Coon, who also believed that human races evolved independently and in parallel with each other. These ideas eventually fell from favour.
Haeckel also applied the hypothesis of polygenism to the modern diversity of human groups. He became a key figure in social darwinism and leading proponent of scientific racism, stating for instance:
Haeckel divided human beings into ten races, of which the Caucasian was the highest and the primitives were doomed to extinction. Haeckel claimed that Negros have stronger and more freely movable toes than any other race which is evidence that Negros are related to apes because when apes stop climbing in trees they hold on to the trees with their toes, Haeckel compared Negros to "four-handed" apes. Haeckel also believed Negros were savages and that Whites were the most civilised.
However, Robert J. Richards notes: "Haeckel, on his travels to Ceylon and Indonesia, often formed closer and more intimate relations with natives, even members of the untouchable classes, than with the European colonials."
In his "Ontology and Phylogeny" Harvard paleontologist Stephen Jay Gould wrote: "[Haekel's] evolutionary racism; his call to the German people for racial purity and unflinching devotion to a "just" state; his belief that harsh, inexorable laws of evolution ruled human civilization and nature alike, conferring upon favored races the right to dominate others . . . all contributed to the rise of Nazism."
In Alfred Rosenberg's The Myth of the Twentieth Century, Rosenberg explicitly mentions having read Haeckel, "As can be seen from the World Riddle, National Socialist orthodoxy was never meant to be as monolithic nor as all embracing as that of Marx and Lenin."
In the same line of thought, historian Daniel Gasman states that Haeckel's ideology stimulated the birth of Fascist ideology in Italy and France.
Asia hypothesis.
Haeckel claimed the origin of humanity was to be found in Asia: he believed that Hindustan (South Asia) was the actual location where the first humans had evolved. Haeckel argued that humans were closely related to the primates of Southeast Asia and rejected Darwin's hypothesis of Africa.
Haeckel later claimed that the missing link was to be found on the lost continent of Lemuria located in the Indian Ocean, he believed that Lemuria was the home of the first humans and that Asia was the home of many of the earliest primates, he thus supported that Asia was the cradle of hominid evolution. Haeckel also claimed that Lemuria connected Asia and Africa which allowed the migration of humans to the rest of the world.
In Haeckel’s book "The History of Creation" (1884) he included migration routes which he thought the first humans had used outside of Lemuria.
Embryology and recapitulation theory.
When Haeckel was a student in the 1850s he showed great interest in embryology, attending the rather unpopular lectures twice and in his notes sketched the visual aids: textbooks had few illustrations, and large format plates were used to show students how to see the tiny forms under a reflecting microscope, with the translucent tissues seen against a black background. Developmental series were used to show stages within a species, but inconsistent views and stages made it even more difficult to compare different species. It was agreed by all European evolutionists that all vertebrates looked very similar at an early stage, in what was thought of as a common ideal type, but there was a continuing debate from the 1820s between the Romantic recapitulation theory that human embryos developed through stages of the forms of all the major groups of adult animals, literally manifesting a sequence of organisms on a linear chain of being, and Karl Ernst von Baer's opposing view that the early general forms diverged into four major groups of specialised forms without ever resembling the adult of another species, showing affinity to an archetype but no relation to other types or any transmutation of species. By the time Haeckel was teaching he was able to use a textbook with woodcut illustrations written by his own teacher Albert von Kölliker, which purported to explain human development while also using other mammalian embryos to claim a coherent sequence. Despite the significance to ideas of transformism, this was not really polite enough for the new popular science writing, and was a matter for medical institutions and for experts who could make their own comparisons.:264–267
Darwin, Naturphilosophie and Lamarck.
Darwin's "On the Origin of Species", which made a powerful impression on Haeckel when he read it in 1864, was very cautious about the possibility of ever reconstructing the history of life, but did include a section reinterpreting von Baer's embryology and revolutionising the field of study, concluding that "Embryology rises greatly in interest, when we thus look at the embryo as a picture, more or less obscured, of the common parent-form of each great class of animals." It mentioned von Baer's 1828 anecdote (misattributing it to Louis Agassiz) that at an early stage embryos were so similar that it could be impossible to tell whether an unlabelled specimen was of a mammal, a bird, or of a reptile, and Darwin's own research using embryonic stages of barnacles to show that they are crustaceans, while cautioning against the idea that one organism or embryonic stage is "higher" or "lower", or more or less evolved. Haeckel disregarded such caution, and in a year wrote his massive and ambitious "Generelle Morphologie", published in 1866, presenting a revolutionary new synthesis of Darwin's ideas with the German tradition of "Naturphilosophie" going back to Goethe and with the progressive evolutionism of Lamarck in what he called "Darwinismus". He used morphology to reconstruct the evolutionary history of life, in the absence of fossil evidence using embryology as evidence of ancestral relationships. He invented new terms, including ontogeny and phylogeny, to present his evolutionised recapitulation theory that "ontogeny recapitulated phylogeny". The two massive volumes sold poorly, and were heavy going: with his limited understanding of German, Darwin found them impossible to read. Haeckel's publisher turned down a proposal for a ""strictly scholarly" and "objective"" second edition.:269–270
Embryological drawings.
Haeckel's aim was a reformed morphology with evolution as the organising principle of a cosmic synthesis unifying science, religion, and art. He was giving successful "popular lectures" on his ideas to students and townspeople in Jena, in an approach pioneered by his teacher Rudolf Virchow. To meet his publisher's need for a popular work he used a student's transcript of his lectures as the basis of his "Natürliche Schöpfungsgeschichte" of 1868, presenting a comprehensive presentation of evolution. In the Spring of that year he drew figures for the book, synthesising his views of specimens in Jena and published pictures to represent types. After publication he told a colleague that the images "are completely exact, partly copied from nature, partly assembled from all illustrations of these early stages that have hitherto become known." There were various styles of embryological drawings at that time, ranging from more schematic representations to "naturalistic" illustrations of specific specimens. Haeckel believed privately that his figures were both exact and synthetic, and in public asserted that they were schematic like most figures used in teaching. The images were reworked to match in size and orientation, and though displaying Haeckel's own views of essential features, they support von Baer's concept that vertebrate embryos begin similarly and then diverge. Relating different images on a grid conveyed a powerful evolutionary message. As a book for the general public, it followed the common practice of not citing sources.:270–274
The book sold very well, and while some anatomical experts hostile to Haeckel's evolutionary views expressed some private concerns that certain figures had been drawn rather freely, the figures showed what they already knew about similarities in embryos. The first published concerns came from Ludwig Rütimeyer, a professor of zoology and comparative anatomy at the University of Basel who had placed fossil mammals in an evolutionary lineage early in the 1860s and had been sent a complimentary copy. At the end of 1868 his review in the "Archiv für Anthropologie" wondered about the claim that the work was "popular and scholarly", doubting whether the second was true, and expressed horror about such public discussion of man's place in nature with illustrations such as the evolutionary trees being shown to non-experts. Though he made no suggestion that embryo illustrations should be directly based on specimens, to him the subject demanded the utmost "scrupulosity and conscientiousness" and an artist must "not arbitrarily model or generalise his originals for speculative purposes" which he considered proved by comparison with works by other authors. In particular, "one and the same, moreover incorrectly interpreted woodcut, is presented to the reader three times in a row and with three different captions as [the] embryo of the dog, the chick, [and] the turtle." He accused Haeckel of "playing fast and loose with the public and with science", and failing to live up to the obligation to the truth of every serious researcher. Haeckel responded with angry accusations of bowing to religious prejudice, but in the second (1870) edition changed the duplicated embryo images to a single image captioned "embryo of a mammal or bird". Duplication using galvanoplastic stereotypes (clichés) was a common technique in textbooks, but not on the same page to represent different eggs or embryos. In 1891 Haeckel made the excuse that this "extremely rash foolishness" had occurred in undue haste but was "bona ﬁde", and since repetition of incidental details was obvious on close inspection, it is unlikely to have been intentional deception.:275–276;282–286
The revised 1870 second edition of 1,500 copies attracted more attention, being quickly followed by further revised editions with larger print runs as the book became a prominent part of the optimistic, nationalist, anticlerical "culture of progress" in Otto von Bismarck's new German Empire. The similarity of early vertebrate embryos became common knowledge, and the illustrations were praised by experts such as Michael Foster of the University of Cambridge. In the introduction to his 1871 "The Descent of Man, and Selection in Relation to Sex", Darwin gave particular praise to Haeckel, writing that if "Natürliche Schöpfungsgeschichte" "had appeared before my essay had been written, I should probably never have completed it." The first chapter included an illustration: "As some of my readers may never have seen a drawing of an embryo, I have given one of man and another of a dog, at about the same early stage of development, carefully copied from two works of undoubted accuracy" with a footnote citing the sources and noting that "Häckel has also given analogous drawings in his "Schöpfungsgeschichte."" The fifth edition of Haeckel's book appeared in 1874, with its frontispiece a heroic portrait of Haeckel himself, replacing the previous controversial image of the heads of apes and humans.:285–288
Controversy.
Later in 1874, Haeckel's simpliﬁed embryology textbook "Anthropogenie" made the subject into a battleground over Darwinism aligned with Bismarck's "Kulturkampf" ("culture struggle") against the Catholic Church. Haeckel took particular care over the illustrations, changing to the leading zoological publisher Wilhelm Engelmann of Leipzig and obtaining from them use of illustrations from their other textbooks as well as preparing his own drawings including a dramatic double page illustration showing "early", "somewhat later" and "still later" stages of 8 different vertebrates. Though Haeckel's views had attracted continuing controversy, there had been little dispute about the embryos and he had many expert supporters, but Wilhelm His revived the earlier criticisms and introduced new attacks on the 1874 illustrations. Others joined in, both expert anatomists and Catholic priests and supporters were politically opposed to Haeckel's views.:288–296
While it has been widely claimed that Haeckel was charged with fraud by five professors and convicted by a university court at Jena, there does not appear to be an independently verifiable source for this claim.
Recent analyses (Richardson 1998, Richardson and Keuck 2002) have found that some of the criticisms of Haeckel's embryo drawings were legitimate, but others were unfounded.
There were multiple versions of the embryo drawings, and Haeckel rejected the claims of fraud. It was later said that "there is evidence of sleight of hand" on both sides of the feud between Haeckel and Wilhelm His. Robert J. Richards, in a paper published in 2008, defends the case for Haeckel, shedding doubt against the fraud accusations based on the material used for comparison with what Haeckel could access at the time. The controversy involves several different issues (see more details at: recapitulation theory).
Awards and honours.
He was awarded the title of Excellency by Kaiser Wilhelm II in 1907 and the Linnean Society of London's prestigious Darwin-Wallace Medal in 1908. In the United States, "Mount Haeckel", a 13418 ft summit in the Eastern Sierra Nevada, overlooking the Evolution Basin, is named in his honour, as is another "Mount Haeckel", a 2941 m summit in New Zealand; and the asteroid 12323 Haeckel.
Publications.
Darwin's 1859 book "On the Origin of Species" had immense popular influence, but although its sales exceeded its publisher's hopes it was a technical book rather than a work of popular science: long, difficult and with few illustrations. One of Haeckel's books did a great deal to explain his version of "Darwinism" to the world. It was a bestselling, provocatively illustrated book in German, titled "Natürliche Schöpfungsgeschichte", published in Berlin in 1868, and translated into English as "The History of Creation" in 1876. It was frequently reprinted until 1926.
Haeckel argued that human evolution consisted of precisely 22 phases, the 21st – the "missing link" — being a halfway step between apes and humans. He even formally named this missing link "Pithecanthropus alalus", translated as "ape man without speech."
Haeckel's entire literary output was extensive, working as a professor at the University of Jena for 47 years, and even at the time of the celebration of his 60th birthday at Jena in 1894, Haeckel had produced 42 works with nearly 13,000 pages, besides numerous scientific memoirs and illustrations.
Haeckel's monographs include: 
As well as several "Challenger" reports:
Among his many books, Ernst Haeckel wrote: 
Books of travel:
For a extensive list of works of and about Haeckel, see his entry in the .
The standard author abbreviation Haeckel is used to indicate this individual as the author when citing a botanical name.

</doc>
<doc id="9670" url="http://en.wikipedia.org/wiki?curid=9670" title="Evolutionism">
Evolutionism

Evolutionism was a common 19th century belief that organisms inherently improve themselves through progressive inherited change over time, and increase in complexity through evolution. The belief went on to include cultural evolution and social evolution. In the 1970s the term Neo-Evolutionism was used to describe the idea "that human beings sought to preserve a familiar style of life unless change was forced on them by factors that were beyond their control".
The term is sometimes also colloquially used to refer to acceptance of the modern evolutionary synthesis, a scientific theory that describes how biological evolution occurs. In addition, the term is used in a broader sense to cover a world-view on a wide variety of topics, including chemical evolution as an alternative term for abiogenesis or for nucleosynthesis of chemical elements, galaxy formation and evolution, stellar evolution, spiritual evolution, technological evolution and universal evolution, which seeks to explain every aspect of the world in which we live.
Since the overwhelming majority of scientists accept the modern evolutionary synthesis as the best explanation of current data, the term is seldom used in the scientific community; to say someone is a scientist implies acceptance of evolutionary views, unless specifically noted otherwise. In the creation-evolution controversy, creationists often call those who accept the validity of the modern evolutionary synthesis "evolutionists" and the theory itself as "evolutionism." Some creationists and creationist organizations, such as the Institute of Creation Research, use these terms in an effort to make it appear that evolutionary biology is a form of secular religion.
19th-century use.
Evolution originally was used to refer to an orderly sequence of events with the outcome somehow contained at the start. Darwin did not use the term in "Origin of Species" until its sixth edition in 1872, (though earlier editions did use the word "evolved") by which time Herbert Spencer had given it scientific currency with a broad definition of progression in complexity in 1862. Edward B. Tylor and Lewis H Morgan brought the term "evolution" to anthropology though they tended toward the older pre-Spencerian definition helping to form the concept of unilineal evolution used during the later part of what Trigger calls the Antiquarianism-Imperial Synthesis period (c1770-c1900).
Modern use.
In modern times, the term "evolution" is widely used, but the terms "evolutionism" and "evolutionist" are seldom used in the scientific community to refer to the biological discipline as the term is considered both redundant and anachronistic, though it has been used by creationists in discussing the creation-evolution controversy. Apart from the use of the word by creationists, it is also often used by theists to imply a worldview that is essentially naturalistic.
The Institute for Creation Research, in order to treat evolution as a category of religions, including atheism, fascism, humanism and occultism, commonly uses the words "evolutionism" and "evolutionist" to describe the consensus of mainstream science and the scientists subscribing to it, thus implying through language that the issue is a matter of religious belief. The goal of this argument is to equate the validity of the theory of evolution with the pseudoscientific concept of Intelligent Design.
The BioLogos Foundation, an organization that promotes the idea of theistic evolution, uses the term "evolutionism" to describe "the atheistic worldview that so often accompanies the acceptance of biological evolution in public discourse." It views this as a subset of scientism.

</doc>
<doc id="9672" url="http://en.wikipedia.org/wiki?curid=9672" title="Entscheidungsproblem">
Entscheidungsproblem

In mathematics and computer science, the Entscheidungsproblem (], German for 'decision problem') is a challenge posed by David Hilbert in 1928. The Entscheidungsproblem asks for an algorithm that takes as input a statement of a first-order logic (possibly with a finite number of axioms beyond the usual axioms of first-order logic) and answers "Yes" or "No" according to whether the statement is "universally valid", i.e., valid in every structure satisfying the axioms. By the completeness theorem of first-order logic, a statement is universally valid if and only if it can be deduced from the axioms, so the Entscheidungsproblem can also be viewed as asking for an algorithm to decide whether a given statement is provable from the axioms using the rules of logic.
In 1936, Alonzo Church and Alan Turing published independent papers showing that a general solution to the Entscheidungsproblem is impossible, assuming that the intuitive notion of "effectively calculable" is captured by the functions computable by a Turing machine (or equivalently, by those expressible in the lambda calculus). This assumption is now known as the Church–Turing thesis.
History of the problem.
The origin of the Entscheidungsproblem goes back to Gottfried Leibniz, who in the seventeenth century, after having constructed a successful mechanical calculating machine, dreamt of building a machine that could manipulate symbols in order to determine the truth values of mathematical statements. He realized that the first step would have to be a clean formal language, and much of his subsequent work was directed towards that goal. In 1928, David Hilbert and Wilhelm Ackermann posed the question in the form outlined above.
In continuation of his "program," Hilbert posed three questions at an international conference in 1928, the third of which became known as "Hilbert's Entscheidungsproblem." As late as 1930, he believed that there would be no such thing as an unsolvable problem.
Negative answer.
Before the question could be answered, the notion of "algorithm" had to be formally defined. This was done by Alonzo Church in 1936 with the concept of "effective calculability" based on his λ calculus and by Alan Turing in the same year with his concept of Turing machines. Turing immediately recognized that these are equivalent models of computation. 
The negative answer to the "Entscheidungsproblem" was then given by Alonzo Church in 1935–36 and independently shortly thereafter by Alan Turing in 1936. Church proved that there is no computable function which decides for two given λ-calculus expressions whether they are equivalent or not. He relied heavily on earlier work by Stephen Kleene. Turing reduced the halting problem for Turing machines to the Entscheidungsproblem. The work of both authors was heavily influenced by Kurt Gödel's earlier work on his incompleteness theorem, especially by the method of assigning numbers (a Gödel numbering) to logical formulas in order to reduce logic to arithmetic.
The Entscheidungsproblem is related to Hilbert's tenth problem, which asks for an algorithm to decide whether Diophantine equations have a solution. The non-existence of such an algorithm, established by Yuri Matiyasevich in 1970, also implies a negative answer to the Entscheidungsproblem.
Some first-order theories are algorithmically decidable; examples of this include Presburger arithmetic, real closed fields and static type systems of many programming languages. The general first-order theory of the natural numbers expressed in Peano's axioms cannot be decided with such an algorithm, however.
Practical decision procedures.
Having practical decision procedures for classes of logical formulas is of considerable interest for program verification and circuit verification. Pure Boolean logical formulas are usually decided using SAT-solving techniques based on the DPLL algorithm. Conjunctive formulas over linear real or rational arithmetic can be decided using the simplex algorithm, formulas in linear integer arithmetic (Presburger arithmetic) can be decided using Cooper's algorithm or William Pugh's Omega test. Formulas with negations, conjunctions and disjunctions combine the difficulties of satisfiability testing with that of decision of conjunctions; they are generally decided nowadays using SMT-solving technique, which combine SAT-solving with decision procedures for conjunctions and propagation techniques. Real polynomial arithmetic, also known as the theory of real closed fields, is decidable, for instance using the cylindrical algebraic decomposition; unfortunately the complexity of that algorithm is excessive for most practical uses.
References.
</dl>

</doc>
<doc id="9674" url="http://en.wikipedia.org/wiki?curid=9674" title="Einhard">
Einhard

Einhard (also Eginhard or Einhart; c. 775 – March 14, 840) was a Frankish scholar and courtier. Einhard was a dedicated servant of Charlemagne and his son Louis the Pious; his main work is a biography of Charlemagne, the "Vita Karoli Magni", "one of the most precious literary bequests of the early Middle Ages."
Public life.
Einhard was from the eastern German-speaking part of the Frankish Kingdom. Born into a family of relatively low status, his parents sent him to be educated by the monks of Fulda - one of the most impressive centres of learning in the Frank lands - perhaps due to his small stature (Einhard referred to himself as a "tiny manlet") which restricted his riding and sword-fighting ability, Einhard concentrated his energies towards scholarship and especially to the mastering of Latin. Despite such humble origins, he was accepted into the hugely wealthy court of Charlemagne around 791 or 792. Charlemagne actively sought to amass scholarly men around him and established a royal school led by the Northumbrian scholar Alcuin. Einhard evidently was a talented builder and construction manager, because Charlemagne put him in charge of the completion of several palace complexes including Aachen and Ingelheim. Despite the fact that Einhard was on intimate terms with Charlemagne, he never achieved office in his reign. In 814, on Charlemagne's death his son Louis the Pious made Einhard his private secretary. Einhard retired from court during the time of the disputes between Louis and his sons in the spring of 830.
He died at Seligenstadt in 840.
Private life.
Einhard was married to Imma, of whom (as of most laywomen of the period) little is known. There is a possibility that their marriage bore a son, Vussin. Their marriage also appears to have been exceptionally liberal for the period, with Imma being as active as Einhard, if not more so, in the handling of their property. It is said that in the later years of their marriage Imma and Einhard abstained from sexual relations, choosing instead to focus their attentions on their many religious commitments. Though he was undoubtedly devoted to her, Einhard wrote nothing of his wife until after her death on 13 December 835, when he wrote to a friend that he was reminded of her loss in ‘every day, in every action, in every undertaking, in all the administration of the house and household, in everything needing to be decided upon and sorted out in my religious and earthly responsibilities’.
Religious beliefs.
Einhard made numerous references to himself as a "sinner", a description of himself that shows his Augustinian influenced world view. To assuage such feelings of guilt he erected churches at both of his estates in Michelstadt and Mulinheim. In Michelstadt he also saw fit to build a basilica completed in 827 and then sent a servant, Ratleic, to Rome with an end to find relics for the new building. Once in Rome, Ratleic robbed a catacomb of the bones of the Martyrs Marcellinus and Peter and had them translated to Michelstadt. Once there, the relics made it known they were unhappy with their new tomb and thus had to be moved again to Mulinheim. Once established there, they proved to be miracle workers. Although unsure as to why these saints should choose such a "sinner" as their patron, Einhard nonetheless set about ensuring they continued to receive a resting place fitting of their honour. Between 831 and 834 he founded a Benedictine Monastery and, after the death of his wife, served as its Abbot until his own death in 840.
Local lore.
Local lore from Seligenstadt portrays Einhard as the lover of Emma, one of Charlemagne's daughters, and has the couple elope from court. Charlemagne found them at Seligenstadt (then called Obermühlheim) and forgave them. This account is used to explain the name "Seligenstadt" by folk etymology. Einhard and his wife were originally buried in one sarcophagus in the choir of the church in Seligenstadt, but in 1810 the sarcophagus was presented by the Grand Duke of Hesse to the count of Erbach, who claims descent from Einhard as the husband of Imma, the reputed daughter of Charlemagne. The count put it in the famous chapel of his castle at Erbach in the Odenwald.
Works.
The most famous of Einhard's works is his biography of Charlemagne, the "Vita Karoli Magni", "The Life of Charlemagne" (c. 817–836), which provides much direct information about Charlemagne's life and character, written sometime between 817 and 830. In composing this he relied heavily upon the Royal Frankish Annals. Einhard's literary model was the classical work of the Roman historian Suetonius, the "Lives of the Caesars", though it is important to stress that the work is very much Einhard's own, that is to say he adapts the models and sources for his own purposes. His work was written as a praise of Charlemagne, whom he regarded as a foster-father ("nutritor") and to whom he was a debtor "in life and death". The work thus contains an understandable degree of bias, Einhard taking care to exculpate Charlemagne in some matters, not mention others, and to gloss over certain issues which would be of embarrassment to Charlemagne, such as the morality of his daughters; by contrast, other issues are curiously not glossed over, like his concubines.
Einhard is also responsible for three other extant works: a collection of letters, "On the Translations and the Miracles of SS. Marcellinus and Petrus", and "On the Adoration of the Cross". The latter dates from ca. 830 and was not rediscovered until 1885, when Ernst Dümmler identified a text in a manuscript in Vienna as the missing "Libellus de adoranda cruce", which Einhard had dedicated to his pupil Lupus Servatus.

</doc>
<doc id="9675" url="http://en.wikipedia.org/wiki?curid=9675" title="Ester">
Ester

In chemistry, esters are chemical compounds derived from an acid (organic or inorganic) in which at least one -OH (hydroxyl) group is replaced by an -O-alkyl (alkoxy) group. Usually, esters are derived from a carboxylic acid and an alcohol. Esters comprise most naturally occurring fats and oils. An important case are glycerides, which are fatty acid esters of glycerol. Esters with low molecular weight are commonly used as fragrances and found in essential oils and pheromones. Phosphoesters form the backbone of DNA molecules. Nitrate esters, such as nitroglycerin, are known for their explosive properties, while polyesters are important plastics, with monomers linked by ester moieties.
Nomenclature.
Etymology.
The word 'ester' was coined in 1848 by German chemist Leopold Gmelin, probably as a contraction of the German Essigäther, "acetic ether".
IUPAC nomenclature.
Ester names are derived from the parent alcohol and the parent acid, where the latter may be organic or inorganic. Esters derived from the simplest carboxylic acids are commonly named according to the more traditional, so-called "trivial names" e.g. as formate, acetate, propionate, and butyrate, as opposed to the IUPAC nomenclature methanoate, ethanoate, propanoate and butanoate. Esters derived from more complex carboxylic acids are, on the other hand, more frequently named using the systematic IUPAC name, based on the name for the acid followed by the suffix "-oate". For example the ester hexyl octanoate, also known under the trivial name hexyl caprylate, has the formula CH3(CH2)6CO2(CH2)5CH3.
The chemical formulas of organic esters usually take the form RCO2R', where R and R' are the hydrocarbon parts of the carboxylic acid and the alcohol, respectively. For example butyl acetate (systematically butyl ethanoate), derived from butanol and acetic acid (systematically ethanoic acid) would be written CH3CO2C4H9. Alternative presentations are common including BuOAc and CH3COOC4H9.
Cyclic esters are called lactones, regardless of whether they are derived from an organic or an inorganic acid. One example of a (organic) lactone is "gamma"-valerolactone.
Orthoesters.
An uncommon class of organic esters are the orthoesters, which have the formula RC(OR')3. Triethylorthoformate (HC(OC2H5)3) is derived, in terms of its name (but not its synthesis) from orthoformic acid (HC(OH)3) and ethanol.
Inorganic esters.
Esters can also be derived from an inorganic acid and an alcohol. Thus, the nomenclature extends to inorganic oxo acids, e.g. phosphoric acid, sulfuric acid, nitric acid and boric acid. For example, triphenyl phosphate is the ester derived from phosphoric acid and phenol. Organic carbonates are derived from carbonic acid; for example, ethylene carbonate is derived from carbonic acid and ethylene glycol.
Structure and bonding.
Esters contain a carbonyl center, which gives rise to 120 °C-C-O and O-C-O angles. Unlike amides, esters are structurally flexible functional groups because rotation about the C-O-C bonds has a low barrier. Their flexibility and low polarity is manifested in their physical properties; they tend to be less rigid (lower melting point) and more volatile (lower boiling point) than the corresponding amides. The pKa of the alpha-hydrogens on esters is around 25.
Physical properties and characterization.
Esters are more polar than ethers but less polar than alcohols. They participate in hydrogen bonds as hydrogen-bond acceptors, but cannot act as hydrogen-bond donors, unlike their parent alcohols. This ability to participate in hydrogen bonding confers some water-solubility. Because of their lack of hydrogen-bond-donating ability, esters do not self-associate. Consequently esters are more volatile than carboxylic acids of similar molecular weight.
Characterization and analysis.
Esters are generally identified by gas chromatography, taking advantage of their volatility. IR spectra for esters feature an intense sharp band in the range 1730–1750 cm−1 assigned to νC=O. This peak changes depending on the functional groups attached to the carbonyl. For example, a benzene ring or double bond in conjugation with the carbonyl will bring the wavenumber down about 30 cm−1.
Applications and occurrence.
Esters are widespread in nature and are widely used in industry. In nature, fats are, in general, triesters derived from glycerol and fatty acids. Esters are responsible for the aroma of many fruits, including apples, durians, pears, bananas, pineapples, and strawberries. Several billion kilograms of polyesters are produced industrially annually, important products being polyethylene terephthalate, acrylate esters, and cellulose acetate.
Preparation.
Esterification is the general name for a chemical reaction in which two reactants (typically an alcohol and an acid) form an ester as the reaction product. Esters are common in organic chemistry and biological materials, and often have a characteristic pleasant, fruity odor. This leads to their extensive use in the fragrance and flavor industry. Ester bonds are also found in many polymers.
Esterification of carboxylic acids.
The classic synthesis is the Fischer esterification, which involves treating a carboxylic acid with an alcohol in the presence of a dehydrating agent:
The equilibrium constant for such reactions is about 5 for typical esters, e.g., ethyl acetate. The reaction is slow in the absence of a catalyst. Sulfuric acid is a typical catalyst for this reaction. Many other acids are also used such as polymeric sulfonic acids. Since esterification is highly reversible, the yield of the ester can be improved using Le Chatelier's principle:
Reagents are known that drive the dehydration of mixtures of alcohols and carboxylic acids. One example is the Steglich esterification, which is a method of forming esters under mild conditions. The method is popular in peptide synthesis, where the substrates are sensitive to harsh conditions like high heat. DCC (dicyclohexylcarbodiimide) is used to activate the carboxylic acid to further reaction. DMAP (4-dimethylaminopyridine) is used as an acyl-transfer catalyst.
Another method for the dehydration of mixtures of alcohols and carboxylic acids is the Mitsunobu reaction:
Carboxylic acids can be esterified using diazomethane:
Using this diazomethane, mixtures of carboxylic acids can be converted to their methyl esters in near quantitative yields, e.g., for analysis by gas chromatography. The method is useful in specialized organic synthetic operations but is considered too expensive for large scale applications.
Alcoholysis of acyl chlorides and acid anhydrides.
Alcohols react with acyl chlorides and acid anhydrides to give esters:
The reactions are irreversible simplifying work-up. Since acyl chlorides and acid anhydrides also react with water, anhydrous conditions are preferred. The analogous acylations of amines to give amides are less sensitive because amines are stronger nucleophiles and react more rapidly than does water. This method is employed only for laboratory-scale procedures, as it is expensive.
Alkylation of carboxylate salts.
Although not widely employed for esterifications, salts of carboxylate anions can be alkylating agent with alkyl halides to give esters. In the case that an alkyl chloride is used, an iodide salt can catalyze the reaction (Finkelstein reaction). The carboxylate salt is often generated "in situ". In difficult cases, the silver carboxylate may be used, since the silver ion coordinates to the halide aiding its departure and improving the reaction rate. This reaction can suffer from anion availability problems and, therefore, can benefit from the addition of phase transfer catalysts or highly polar aprotic solvents such as DMF.
Transesterification.
Transesterification, which involves changing one ester into another one, is widely practiced:
Like the hydrolysation, transesterification is catalysed by acids and bases. The reaction is widely used for degrading triglycerides, e.g. in the production of fatty acid esters and alcohols. Poly(ethylene terephthalate) is produced by the transesterification of dimethyl terephthalate and ethylene glycol: 
Carbonylation.
Alkenes undergo "hydroesterification" in the presence of metal carbonyl catalysts. Esters of propionic acid are produced commercially by this method:
The carbonylation of methanol yields methyl formate, which is the main commercial source of formic acid. The reaction is catalyzed by sodium methoxide:
Addition of carboxylic acids to alkenes.
In the presence of palladium-based catalysts, ethylene, acetic acid, and oxygen react to give vinyl acetate:
Direct routes to this same ester are not possible because vinyl alcohol is unstable.
Reactions.
Esters react with nucleophiles at the carbonyl carbon. The carbonyl is weakly electrophilic but is attacked by strong nucleophilies (amines, alkoxides, hydride sources, organolithium compounds, etc.). The C-H bonds adjacent to the carbonyl are weakly acidic but undergo deprotonation with strong bases. This process is the one that usually initiates condensation reactions. The carbonyl oxygen is weakly basic (less so than in amides) but forms adducts.
Addition of nucleophiles at carbonyl.
Esterification is a reversible reaction. Esters undergo hydrolysis under acid and basic conditions. Under acidic conditions, the reaction is the reverse reaction of the Fischer esterification. Under basic conditions, hydroxide acts as a nucleophile, while an alkoxide is the leaving group. This reaction, saponification, is the basis of soap making.
The alkoxide group may also be displaced by stronger nucleophiles such as ammonia or primary or secondary amines to give amides: (ammonolysis reaction)
This reaction is not usually reversible. Hydrazines and hydroxylamine can be used in place of amines. Esters can be converted to isocyanates through intermediate hydroxamic acids in the Lossen rearrangement.
Sources of carbon nucleophiles, e.g., Grignard reagents and organolithium compounds, add readily to the carbonyl.
Reduction.
Compared to ketones and aldehydes, esters are relatively resistant to reduction. The introduction of catalytic hydrogenation in the early part of the 20th century was a breakthrough; esters of fatty acids are hydrogenated to fatty alcohols.
A typical catalyst is copper chromite. Prior to the development of catalytic hydrogenation, esters were reduced on a large scale using the Bouveault-Blanc reduction. This method, which is largely obsolete, uses sodium in the presence of proton sources.
Especially for fine chemical syntheses, lithium aluminium hydride is used to reduce esters to two primary alcohols. The related reagent sodium borohydride is slow in this reaction. DIBAH reduces esters to aldehydes.
Direct reduction to give the corresponding ether is difficult as the intermediate hemiacetal tends to decompose to give an alcohol and an aldehyde (which is rapidly reduced to give a second alcohol). The reaction can be achieved using triethylsilane with a variety of Lewis acids.
Claisen condensation and related reactions.
As for aldehydes, the hydrogen atoms on the carbon adjacent ("α to") the carboxyl group in esters are sufficiently acidic to undergo deprotonation, which in turn leads to a variety of useful reactions. Deprotonation requires relatively strong bases, such as alkoxides. Deprotonation gives a nucleophilic enolate, which can further react, e.g., the Claisen condensation and its intramolecular equivalent, the Dieckmann condensation. This conversion is exploited in the malonic ester synthesis, wherein the diester of malonic acid reacts with an electrophile (e.g., alkyl halide), and is subsequently decarboxylated. Another variation is the Fráter–Seebach alkylation.
Protecting groups.
As a class, esters serve as protecting groups for carboxylic acids. Protecting a carboxylic acid is useful in peptide synthesis, to prevent self-reactions of the bifunctional amino acids. Methyl and ethyl esters are commonly available for many amino acids; the "t"-butyl ester tends to be more expensive. However, "t"-butyl esters are particularly useful because, under strongly acidic conditions, the "t"-butyl esters undergo elimination to give the carboxylic acid and isobutylene, simplifying work-up.
List of ester odorants.
Many esters have distinctive fruit-like odors, and many occur naturally in the essential oils of plants. This has also led to their commonplace use in artificial flavorings and fragrances when those odors aim to be mimicked.

</doc>
<doc id="9677" url="http://en.wikipedia.org/wiki?curid=9677" title="Endosymbiont">
Endosymbiont

An endosymbiont is any organism that lives within the body or cells of another organism, i.e. forming an endosymbiosis (Greek: ἔνδον "endon" "within", σύν "syn" "together" and βίωσις "biosis" "living"). Examples are nitrogen-fixing bacteria (called rhizobia), which live in root nodules on legume roots, single-cell algae inside reef-building corals, and bacterial endosymbionts that provide essential nutrients to about 10–15% of insects.
Many instances of endosymbiosis are obligate; that is, either the endosymbiont or the host cannot survive without the other, such as the gutless marine worms of the genus "Riftia", which get nutrition from their endosymbiotic bacteria. The most common examples of obligate endosymbioses are mitochondria and chloroplasts. Some human parasites, e.g. "Wuchereria bancrofti" and "Mansonella perstans", thrive in their intermediate insect hosts because of an obligate endosymbiosis with "Wolbachia spp." They can both be eliminated from said hosts by treatments that target this bacterium. However, not all endosymbioses are obligate. Also, some endosymbioses can be harmful to either of the organisms involved.
It is generally agreed that certain organelles of the eukaryotic cell, especially mitochondria and plastids such as chloroplasts, originated as bacterial endosymbionts. This theory is called the endosymbiotic theory, and was first articulated by the Russian botanist Konstantin Mereschkowski in 1910, even though the first paper that referenced this theory was published in 1905.
Endosymbiosis theory and mitochondria and chloroplasts.
The endosymbiosis theory explains the origins of organelles such as mitochondria and chloroplasts in eukaryotic cells. The theory proposes that chloroplasts and mitochondria evolved from certain types of bacteria that eukaryotic cells engulfed through endophagocytosis. These cells and the bacteria trapped inside them entered a symbiotic relationship, a close association between different types of organisms over an extended time. However, to be specific, the relationship was endosymbiotic, meaning that one of the organisms (the bacteria) lived within the other (the eukaryotic cells).
According to endosymbiosis theory, an anaerobic cell probably ingested an aerobic bacterium but failed to digest it. The aerobic bacterium flourished within the cell because the cell's cytoplasm was abundant in half-digested food molecules. The bacterium digested these molecules with oxygen and gained great amounts of energy. Because the bacterium had so much energy, it probably leaked some of it as adenosine triphosphate into the cell's cytoplasm. This benefited the anaerobic cell because it was now able to breathe aerobically, which means more potential for energy gain. Eventually, the aerobic bacterium could no longer live independently from the cell, and it, therefore, became a mitochondrion. The origin of the chloroplast is very similar to that of the mitochondrion. A cell must have captured a photosynthetic cyanobacterium and failed to digest it. The cyanobacterium thrived in the cell and eventually evolved into the first chloroplast. Other eukaryotic organelles may have also evolved through endosymbiosis; it has been proposed that cilia, flagella, centrioles, and microtubules may have originated from a symbiosis between a Spirochaete bacterium and an early eukaryotic cell, but this is not widely accepted among biologists.
There are several examples of evidence that support endosymbiosis theory. Mitochondria and chloroplasts contain their own small supply of DNA, which may be remnants of the genome the organelles had when they were independent aerobic bacteria. The single most convincing evidence of the descent of organelles from bacteria is the position of mitochondria and plastid DNA sequences in phylogenetic trees of bacteria. Mitochondria have sequences that clearly indicate origin from a group of bacteria called the alphaproteobacteria. Plastids have DNA sequences that indicate origin from the cyanobacteria (blue-green algae). In addition, there are organisms alive today, called living intermediates, that are in a similar endosymbiotic condition to the prokaryotic cells and the aerobic bacteria. Living intermediates show that the evolution proposed by the endosymbiont theory is possible. For example, the giant amoeba "Pelomyxa" lacks mitochondria but has aerobic bacteria that carry out a similar role. A variety of corals, clams, snails, and one species of "Paramecium" permanently host algae in their cells. Many of the insect endosymbionts have been shown to have ancient associations with their hosts, involving strictly vertical inheritance. In addition, these insect symbionts have similar patterns of genome evolution to those found in true organelles: genome reduction, rapid rates of gene evolution, and bias in nucleotide base composition favoring adenine and thymine, at the expense of guanine and cytosine.
Further evidence of endosymbiosis are the prokaryotic ribosomes found within chloroplasts and mitochondria as well as the double-membrane enclosing them. It used to be widely assumed that the inner membrane is the original membrane of the once independent prokaryote, while the outer one is the food vacuole (phagosomal membrane) it was enclosed in initially. However, this view neglects the fact that i) both modern cyanobacteria and alpha-proteobacteria are Gram-negative bacteria, which are surrounded by double membranes; ii) the outer membranes of the endosymbiotic organelles (chloroplasts and mitochondria) are very similar to those of these bacteria in their lipid and protein compositions. Accumulating biochemical data strongly suggests that the double-membrane-enclosing chloroplasts and mitochondria derived from those of the ancestral bacteria, and the phagosomal membrane disappeared during organelle evolution. Triple or quadruple membranes are found among certain algae, probably resulting from repeated endosymbiosis (although little else was retained of the engulfed cell).
These modern organisms with endosymbiotic relationships with aerobic bacteria have verified the endosymbiotic theory, which explains the origin of mitochondria and chloroplasts from bacteria. Researchers in molecular and evolutionary biology no longer question this theory, although some of the details, such as the mechanisms for loss of genes from organelles to host nuclear genomes, are still being worked out.
Bacterial endosymbionts in marine invertebrates.
Extracellular endosymbionts are also represented in all four extant classes of Echinodermata (Crinoidea, Ophiuroidea, Echinoidea, and Holothuroidea). Little is known of the nature of the association (mode of infection, transmission, metabolic requirements, etc.) but phylogenetic analysis indicates that these symbionts belong to the alpha group of the class Proteobacteria, relating them to "Rhizobium" and "Thiobacillus". Other studies indicate that these subcuticular bacteria may be both abundant within their hosts and widely distributed among the Echinoderms in general.
Some marine oligochaeta (e.g., Olavius or Inanidrillus) have obligate extracellular endosymbionts that fill the entire body of their host. These marine worms are nutritionally dependent on their symbiotic chemoautotrophic bacteria lacking any digestive or excretory system (no gut, mouth, or nephridia).
"Symbiodinium" dinoflagellate endosymbionts in marine metazoa and protists.
Dinoflagellate endosymbionts of the genus "Symbiodinium", commonly known as zooxanthellae, are found in corals, mollusks (esp. giant clams, the "Tridacna"), sponges, and foraminifera. These endosymbionts drive the formation of coral reefs by capturing sunlight and providing their hosts with energy for carbonate deposition.
Previously thought to be a single species, molecular phylogenetic evidence over the past couple decades has shown there to be great diversity in "Symbiodinium". In some cases, there is specificity between host and "Symbiodinium" clade. More often, however, there is an ecological distribution of "Symbiodinium", the symbionts switching between hosts with apparent ease. When reefs become environmentally stressed, this distribution of symbionts is related to the observed pattern of coral bleaching and recovery. Thus, the distribution of "Symbiodinium" on coral reefs and its role in coral bleaching presents one of the most complex and interesting current problems in reef ecology.
Endosymbionts in protists.
"Mixotricha paradoxa" is a protozoan that lacks mitochondria. However, spherical bacteria live inside the cell and serve the function of the mitochondria. "Mixotricha" also has three other species of symbionts that live on the surface of the cell.
"Paramecium bursaria", a species of ciliate, has a mutualistic symbiotic relationship with green alga called Zoochlorella. The algae live inside the cell, in the cytoplasm.
"Paulinella chromatophora" is a freshwater amoeboid which has recently (evolutionarily speaking) taken on a cyanobacterium as an endosymbiont.
Bacterial endosymbionts in insects.
Scientists classify insect endosymbionts in two broad categories, 'Primary' and 'Secondary'. Primary endosymbionts (sometimes referred to as P-endosymbionts) have been associated with their insect hosts for many millions of years (from 10 to several hundred million years in some cases). They form obligate associations (see below), and display cospeciation with their insect hosts. Secondary endosymbionts exhibit a more recently developed association, are sometimes horizontally transferred between hosts, live in the hemolymph of the insects (not specialized bacteriocytes, see below), and are not obligate.
Among primary endosymbionts of insects, the best-studied are the pea aphid ("Acyrthosiphon pisum") and its endosymbiont "Buchnera sp." APS, the tsetse fly "Glossina morsitans morsitans" and its endosymbiont "Wigglesworthia glossinidia brevipalpis" and the endosymbiotic protists in lower termites. As with endosymbiosis in other insects, the symbiosis is obligate in that neither the bacteria nor the insect is viable without the other. Scientists have been unable to cultivate the bacteria in lab conditions outside of the insect. With special nutritionally-enhanced diets, the insects can survive, but are unhealthy, and at best survive only a few generations.
In some insect groups, these endosymbionts live in specialized insect cells called bacteriocytes (also called "mycetocytes"), and are maternally-transmitted, i.e. the mother transmits her endosymbionts to her offspring. In some cases, the bacteria are transmitted in the egg, as in "Buchnera"; in others like "Wigglesworthia", they are transmitted via milk to the developing insect embryo. In termites, the endosymbionts reside within the hindguts and are transmitted through trophallaxis among colony members.
The primary endosymbionts are thought to help the host either by providing nutrients that the host cannot obtain itself or by metabolizing insect waste products into safer forms. For example, the putative primary role of "Buchnera" is to synthesize essential amino acids that the aphid cannot acquire from its natural diet of plant sap. Likewise, the primary role of "Wigglesworthia", it is presumed, is to synthesize vitamins that the tsetse fly does not get from the blood that it eats. In lower termites, the endosymbiotic protists play a major role in the digestion of lignocellulosic materials that constitute a bulk of the termites' diet.
Bacteria benefit from the reduced exposure to predators and competition from other bacterial species, the ample supply of nutrients and relative environmental stability inside the host.
Genome sequencing reveals that obligate bacterial endosymbionts of insects have among the smallest of known bacterial genomes and have lost many genes that are commonly found in closely related bacteria. Several theories have been put forth to explain the loss of genes. It is presumed that some of these genes are not needed in the environment of the host insect cell. A complementary theory suggests that the relatively small numbers of bacteria inside each insect decrease the efficiency of natural selection in 'purging' deleterious mutations and small mutations from the population, resulting in a loss of genes over many millions of years. Research in which a parallel phylogeny of bacteria and insects was inferred supports the belief that the primary endosymbionts are transferred only vertically (i.e., from the mother), and not horizontally (i.e., by escaping the host and entering a new host).
Attacking obligate bacterial endosymbionts may present a way to control their insect hosts, many of which pests or carriers of human disease. For example aphids are crop pests and the tsetse fly carries the organism "Trypanosoma brucei" that causes African sleeping sickness. Other motivations for their study is to understand symbiosis, and to understand how bacteria with severely depleted genomes are able to survive, thus improving our knowledge of genetics and molecular biology.
Less is known about secondary endosymbionts. The pea aphid ("Acyrthosiphon pisum") is known to contain at least three secondary endosymbionts, "Hamiltonella defensa", "Regiella insecticola", and "Serratia symbiotica". "H. defensa" aids in defending the insect from parasitoids. "Sodalis glossinidius" is a secondary endosymbiont of tsetse flies that lives inter- and intracellularly in various host tissues, including the midgut and hemolymph. Phylogenetic studies have not indicated a correlation between evolution of "Sodalis" and tsetse. Unlike tsetse's P-symbiont "Wigglesworthia", though, "Sodalis" has been cultured "in vitro".
Viral endosymbionts and endogenous retrovirus.
During pregnancy in viviparous mammals, endogenous retroviruses (ERVs) are activated and produced in high quantities during the implantation of the embryo. On one hand, they act as immunodepressors, and protect the embryo from the immune system of the mother, and on the other hand viral fusion proteins cause the formation of the placental syncytium in order to limit the exchange of migratory cells between the developing embryo and the body of the mother, where an epithelium will not be adequate because certain blood cells are specialized to be able to insert themselves between adjacent epithelial cells. The ERV is a virus similar to HIV (the virus causing AIDS in humans). The immunodepressive action was the initial normal behavior of the virus, similar to HIV. The fusion proteins was a way to spread the infection to other cells by simply merging them with the infected one (similar to HIV). It is believed that the ancestors of modern viviparous mammals evolved after an accidental infection of an ancestor with this virus, which permitted the fetus to survive the immune system of the mother.
The human genome project found several thousand ERVs, which are organized into 24 families.

</doc>
<doc id="9678" url="http://en.wikipedia.org/wiki?curid=9678" title="Exponential function">
Exponential function

The term exponential function is almost exclusively used to mean the natural exponential function "e""x", where "e" is Euler's number, a number (approximately 2.718281828) such that the function "e""x" is its own derivative. The exponential function is used to model a relationship in which a constant change in the independent variable gives the same proportional change (i.e. percentage increase or decrease) in the dependent variable. The function is often written as exp("x"), especially when it is impractical to write the independent variable as a superscript. The exponential function is widely used in physics, chemistry, engineering, mathematical biology, economics and mathematics.
The graph of "y" = "e""x" is upward-sloping, and increases faster as "x" increases. The graph always lies above the "x"-axis but can get arbitrarily close to it for negative "x"; thus, the "x"-axis is a horizontal asymptote. The slope of the tangent to the graph at each point is equal to its "y" coordinate at that point. The inverse function is the natural logarithm ln("x"); because of this, some old texts refer to the exponential function as the antilogarithm.
In general, the variable "x" can be any real or complex number or even an entirely different kind of mathematical object; see the formal definition below.
Formal definition.
The exponential function "e""x" can be characterized in a variety of equivalent ways. In particular it may be defined by the following power series:
Using an alternate definition for the exponential function leads to the same result when expanded as a Taylor series.
Less commonly, "e""x" is defined as the solution "y" to the equation
It is also the following limit:
Overview.
The exponential function arises whenever a quantity grows or decays at a rate proportional to its current value. One such situation is continuously compounded interest, and in fact it was this that led Jacob Bernoulli in 1683 to the number
now known as "e". Later, in 1697, Johann Bernoulli studied the calculus of the exponential function.
If a principal amount of 1 earns interest at an annual rate of "x" compounded monthly, then the interest earned each month is "x"/12 times the current value, so each month the total value is multiplied by (1+"x"/12), and the value at the end of the year is (1+"x"/12)12. If instead interest is compounded daily, this becomes (1+"x"/365)365. Letting the number of time intervals per year grow without bound leads to the limit definition of the exponential function,
first given by Euler.
This is one of a number of characterizations of the exponential function; others involve series or differential equations.
From any of these definitions it can be shown that the exponential function obeys the basic exponentiation identity,
which is why it can be written as "e""x".
The derivative (rate of change) of the exponential function is the exponential function itself. More generally, a function with a rate of change "proportional" to the function itself (rather than equal to it) is expressible in terms of the exponential function. This function property leads to exponential growth and exponential decay.
The exponential function extends to an entire function on the complex plane. Euler's formula relates its values at purely imaginary arguments to trigonometric functions. The exponential function also has analogues for which the argument is a matrix, or even an element of a Banach algebra or a Lie algebra.
Derivatives and differential equations.
The importance of the exponential function in mathematics and the sciences stems mainly from properties of its derivative. In particular,
Proof:
formula_8
That is, "e""x" is its own derivative and hence is a simple example of a Pfaffian function. Functions of the form "ce""x" for constant "c" are the only functions with that property (by the Picard–Lindelöf theorem). Other ways of saying the same thing include:
If a variable's growth or decay rate is proportional to its size—as is the case in unlimited population growth (see Malthusian catastrophe), continuously compounded interest, or radioactive decay—then the variable can be written as a constant times an exponential function of time. Explicitly for any real constant "k", a function "f": R→R satisfies "f"′ = "kf" if and only if "f"("x") = "ce""kx" for some constant "c".
Furthermore for any differentiable function "f"("x"), we find, by the chain rule:
Continued fractions for "e""x".
A continued fraction for "e""x" can be obtained via an identity of Euler:
The following generalized continued fraction for "e""z" converges more quickly:
or, by applying the substitution "z" = "x"⁄"y":
with a special case for "z" = 2:
This formula also converges, though more slowly, for "z" > 2. For example:
Complex plane.
As in the real case, the exponential function can be defined on the complex plane in several equivalent forms. One such definition parallels the power series definition for real numbers, where the real variable is replaced by a complex one:
The exponential function is periodic with imaginary period formula_16 and can be written as
where "a" and "b" are real values and on the right the real functions must be used if used as a definition (see also Euler's formula). This formula connects the exponential function with the trigonometric functions and to the hyperbolic functions.
When considered as a function defined on the complex plane, the exponential function retains the properties
for all "z" and "w".
The exponential function is an entire function as it is holomorphic over the whole complex plane. It takes on every complex number excepting 0 as value; that is, 0 is a lacunary value of the exponential function. This is an example of Picard's little theorem that any non-constant entire function takes on every complex number as value with at most one value excepted.
Extending the natural logarithm to complex arguments yields the complex logarithm log "z", which is a multivalued function.
We can then define a more general exponentiation:
for all complex numbers "z" and "w". This is also a multivalued function, even when "z" is real. This distinction is problematic, as the multivalued functions log "z" and "z""w" are easily confused with their single-valued equivalents when substituting a real number for "z". The rule about multiplying exponents for the case of positive real numbers must be modified in a multivalued context:
See failure of power and logarithm identities for more about problems with combining powers.
The exponential function maps any line in the complex plane to a logarithmic spiral in the complex plane with the center at the origin. Two special cases might be noted: when the original line is parallel to the real axis, the resulting spiral never closes in on itself; when the original line is parallel to the imaginary axis, the resulting spiral is a circle of some radius.
Computation of "a""b" where both "a" and "b" are complex.
Complex exponentiation "a""b" can be defined by converting "a" to polar coordinates and using the identity ("e"ln("a"))"b" = "a""b":
However, when "b" is not an integer, this function is multivalued, because "θ" is not unique (see failure of power and logarithm identities).
Matrices and Banach algebras.
The power series definition of the exponential function makes sense for square matrices (for which the function is called the matrix exponential) and more generally in any Banach algebra "B". In this setting, "e"0 = 1, and "e""x" is invertible with inverse "e"−"x" for any "x" in "B". If "xy" ="yx", then "e""x"+"y" = "e""x""e""y", but this identity can fail for noncommuting "x" and "y".
Some alternative definitions lead to the same function. For instance, "e""x" can be defined as
formula_27
Or "e""x" can be defined as "f"(1), where "f": R→"B" is the solution to the differential equation "f"′("t") = "xf"("t") with initial condition "f"(0) = 1.
Lie algebras.
Given a Lie group "G" and its associated Lie algebra formula_28, the exponential map is a map formula_29 satisfying similar properties. In fact, since R is the Lie algebra of the Lie group of all positive real numbers under multiplication, the ordinary exponential function for real arguments is a special case of the Lie algebra situation. Similarly, since the Lie group GL("n",R) of invertible "n" × "n" matrices has as Lie algebra M("n",R), the space of all "n" × "n" matrices, the exponential function for square matrices is a special case of the Lie algebra exponential map.
The identity exp("x" + "y") = exp("x")exp("y") can fail for Lie algebra elements "x" and "y" that do not commute; the Baker–Campbell–Hausdorff formula supplies the necessary correction terms.
Double exponential function.
The term double exponential function can have two meanings:
Factorials grow faster than exponential functions, but slower than double-exponential functions. Fermat numbers, generated by formula_30 and double Mersenne numbers generated by formula_31 are examples of double exponential functions.
Similar properties of "e" and the function "e""z".
The function "e""z" is not in C("z") (i.e., is not the quotient of two polynomials with complex coefficients).
For "n" distinct complex numbers {"a"1, …, "a""n"}, the set {"e""a"1"z", …, "e""a""n""z"} is linearly independent over C("z").
The function "e""z" is transcendental over C("z").

</doc>
