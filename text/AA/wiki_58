<doc id="8204" url="http://en.wikipedia.org/wiki?curid=8204" title="December 31">
December 31

December 31 is the day of the year in the Gregorian calendar. It is widely known as New Year's Eve since the following day is New Year's Day. It is the last day of the year. The following day is January 1 of the following year.

</doc>
<doc id="8205" url="http://en.wikipedia.org/wiki?curid=8205" title="Deng Xiaoping">
Deng Xiaoping

Deng Xiaoping (Simplified Chinese 邓小平, Traditional Chinese 鄧小平, pinyin "dèng xiǎopíng", ]); 22 August 1904 – 19 February 1997), transliterated as "Tong Shau-ping", was a Chinese revolutionary and statesman. He was the leader of China from 1978 until his retirement in 1992. After Mao Zedong's death, Deng led his country through far-reaching market economic reforms. While Deng never held office as the head of state, head of government or General Secretary (that is, the leader of the Communist Party), he nonetheless was considered the "paramount leader" of the People's Republic of China from December 1978 to 1992. As the core of the second-generation leaders, Deng shared his power with several powerful older politicians commonly known as the Eight Elders.
Born into a peasant background in Guang'an, Sichuan province, Deng studied and worked in France in the 1920s, where he was influenced by Marxism-Leninism. He joined the Communist Party of China in 1923. Upon his return to China he worked as a political commissar for the military in rural regions and was considered a "revolutionary veteran" of the Long March. Following the founding of the People's Republic of China in 1949, Deng worked in Tibet and other southwestern regions to consolidate Communist control.
Deng was a major supporter of Mao Zedong in the early 1950s. As the party's Secretary-General, Deng became instrumental in China's economic reconstruction following the Great Leap Forward in the early 1960s. His economic policies, however, were at odds with Mao's political ideologies. As a result, he was purged twice during the Cultural Revolution, but regained prominence in 1978 by outmaneuvering Mao's chosen successor, Hua Guofeng.
Inheriting a country fraught with social and institutional woes resulting from the Cultural Revolution and other mass political movements of the Mao era, Deng became the pre-eminent figure of the "second generation" of Chinese leadership. He is considered "the architect" of a new brand of socialist thinking, combining the Communist Party's socialist ideology with a pragmatic adoption of market economic practices. Deng opened China to foreign investment, the global market and limited private competition. He is generally credited with developing China into one of the fastest-growing economies in the world for over 35 years and raising the standard of living of hundreds of millions of Chinese.
Early life and family.
Deng was born into an ethnically Hakka Han family in the village of Paifang (牌坊村), in the township of Xiexing (协兴镇), Guang'an County in Sichuan province, approximately 160 km from Chongqing (formerly spelled "Chungking"). Deng's ancestors can be traced back to Mei County, Guangdong, a prominent ancestral area for the Hakka people, and had been settled in Sichuan for several generations.
Deng had the name Deng Xiansheng, with the given name meaning "early/first" "sage/saint." All of his siblings had given names beginning with "xian." Deng's father, Deng Wenming, was a middle-level landowner and had studied at the University of Law and Political Science in Chengdu. His mother, surnamed Dan, died early in Deng's life, leaving Deng, his three brothers and three sisters. At the age of five Deng was sent to a traditional Chinese-style private primary school, followed by a more modern primary school at the age of seven.
Deng's first wife, one of his schoolmates from Moscow, died aged 24 a few days after giving birth to Deng's first child, a baby girl who also died. His second wife, Jin Weiying, left him after Deng came under political attack in 1933. His third wife Zhuo Lin was the daughter of an industrialist in Yunnan Province. She became a member of the Communist Party in 1938, and married Deng a year later in front of Mao's cave dwelling in Yan'an. They had five children: three daughters (Deng Lin, Deng Nan and Deng Rong) and two sons (Deng Pufang and Deng Zhifang).
Education and early career.
When Deng first attended school, his tutor objected to him having the given name "Xiansheng" (a homophone for an honorific analogous to "mister" or "sir"), calling him "Xixian", which includes the characters "to aspire to" and "goodness", with overtones of wisdom.
In the summer of 1919 Deng Xiaoping graduated from the Chongqing School. He and 80 schoolmates travelled by ship to France (traveling steerage) to participate in the Diligent Work-Frugal Study Movement, a work-study program in which 4,001 Chinese would participate by 1927. Deng, the youngest of all the Chinese students in the group, had just turned 15. Wu Yuzhang, local leader of the Movement in Chongqing, enrolled Deng and his paternal uncle, Deng Shaosheng, in the program. Deng's father strongly supported his son's exile in the work-study abroad program. The night before his departure, Deng's father took his son aside and asked him what he hoped to learn in France. He repeated the words he had learned from his teachers: "To learn knowledge and truth from the West in order to save China." Deng was aware that China was suffering greatly, and that the Chinese people must have a modern education to save their country.
In December 1920 a French packet ship, the "André Lyon", sailed into Marseille with 210 Chinese students aboard including Deng. The sixteen-year-old Deng briefly attended middle schools in Bayeux and Châtillon, but he spent most of his time in France working. His first job was as a fitter at the Le Creusot Iron and Steel Plant in La Garenne-Colombes, a south-western suburb of Paris where he moved in April 1921. Ironically, when Deng Xiaoping's later political fortunes were down and he was sent to work in a tractor factory in 1974 he found himself a fitter again, and proved to still be a master of the skill.
In La Garenne-Colombes Deng met Zhou Enlai, Nie Rongzhen, Cai Hesen, Zhao Shiyan and Li Wenhai. Under the influence of these older Chinese students in France, Deng began to study Marxism and engaged in political dissemination work. In 1921 he joined the Chinese Communist Youth League in Europe. In the second half of 1924 he joined the Chinese Communist Party and became one of the leading members of the General Branch of the Youth League in Europe. In 1926 Deng traveled to the Soviet Union and studied at Moscow Sun Yat-sen University, where one of his classmates was Chiang Ching-kuo, the son of Chiang Kai Shek.
Return to China.
In late 1927, Deng left Moscow to return to China, where he joined the army of Feng Yuxiang, a military leader in northwest China, who had requested assistance from the Soviet Union in his struggle with other local leaders in the region. At that time, the Soviet Union, through the Comintern, an international organization supporting the communist movements in the world, supported the Communists' alliance with the Nationalists of the Kuomintang (KMT) party founded by Sun Yat-sen.
He arrived in Xi'an, the stronghold of Feng Yuxiang, in March 1927. He was part of the Fengtian clique's attempt to prevent the break of the alliance between the KMT and the Communists. This split resulted in part from Chiang Kai-shek's forcing them to flee areas controlled by the KMT. After the breakup of the alliance between communists and nationalists, Feng Yuxiang stood on the side of Chiang Kai-shek and the Communists who participated in their army, as Deng Xiaoping, were forced to flee. In 1929 Deng led the Baise Uprising in Guangxi province against the Kuomintang (KMT) government. The uprising failed and Deng went to the Central Soviet Area in Jiangxi province.
Political rise.
Although Deng got involved in the Marxist revolutionary movement in China, the historian Mobo Gao has argued that "Deng Xiaoping and many like him [in the Chinese Communist Party] were not really Marxists, but basically revolutionary nationalists who wanted to see China standing on equal terms with the great global powers. They were primarily nationalists and they participated in the Communist revolution because that was the only viable route they could find to Chinese nationalism."
Activism in Shanghai and Wuhan.
After leaving the army of Feng Yuxiang in the northwest, Deng ended up in the city of Wuhan, where the Communists at that time had their headquarters. At that time, he began using the nickname "Xiaoping" and occupied prominent positions in the party apparatus. He participated in the historic emergency session on 7 August 1927 in which, by Soviet instruction, the Party dismissed its founder Chen Duxiu, and Qu Qiubai became the secretary general. In Wuhan, Deng first established contact with Mao Zedong, who was then little valued by militant pro-Soviet leaders of the party.
Between 1927 and 1929, Deng lived in Shanghai, where he helped organize protests that would be harshly persecuted by the Kuomintang authorities. The death of many Communist militants in those years led to a decrease in the number of members of the Communist Party, which enabled Deng to quickly move up the ranks. During this stage in Shanghai, Deng married for the first time with a woman he met in Moscow, Zhang Xiyuan.
Military campaign in Guangxi.
Beginning in 1929, he participated in the struggle against the Kuomintang in Guangxi. The superiority of the forces of Chiang Kai-shek caused a huge number of casualties in the Communist ranks. The confrontational strategy of the party leadership was a failure that killed many militants. The response to this defeat catalyzed one of the most confusing episodes in the biography of Deng: in March 1931, he left the Communist Army seventh battalion to appear some time later in Shanghai.
His official biography states that Deng had been charged by his superiors with deserting from the battle zone before fleeing to Shanghai, where there were leaders of the underground Communist Party. Although he was not punished in Shanghai, this episode in his biography remains unclear and would be used against him to question his devotion to the Communist Party during the Cultural Revolution era.
At the Jiangxi Soviet.
After returning to Shanghai, Deng discovered that his wife and daughter had died during childbirth. In addition, he discovered that many of his former comrades had died as a result of the Kuomintang's crackdown against the Communists.
The campaigns against the Communists in the cities represented a setback for the party and in particular to the Comintern Soviet advisers, who saw the mobilization of the urban proletariat as the force for the advancement of communism. Contrary to the urban vision of the revolution, based on the Soviet experience, the Communist leader Mao Zedong saw the rural peasants as the revolutionary force in China. In a mountainous area of Jiangxi province, where Mao went to establish a communist system, there developed the embryo of a future state of China under communism which adopted the official name of the Chinese Soviet Republic, but was better known as the "Jiangxi Soviet".
In one of the most important cities in the Soviet zone, Ruijin, Deng took over as secretary of the Party Committee in the summer of 1931. A year later, in the winter of 1932, Deng went on to play the same position in the nearby district of Huichang. In 1933 he became director of the propaganda department of the Provincial Party Committee in Jiangxi. It was then that he married for the second time to a young woman named Jin Weiying, whom he had met in Shanghai.
The successes of the Soviet in Jiangxi made the party leaders decide to move to Jiangxi from Shanghai. The confrontation between the ideas of Mao and the party leaders and their Soviet advisers were increasingly tense and the struggle for power between the two factions led to the removal of Deng, who favored the ideas of Mao, from his position in the propaganda department. Despite the internal strife within the party, the Jiangxi Soviet became the first successful experiment of communist rule in rural China. It even issued stamps and paper money under the letterhead of the Soviet Republic of China, and the army of Chiang Kai-shek finally decided to attack the communist area.
The Long March.
Surrounded by the more powerful army of the Republic of China, the Communists fled Jiangxi in October 1934. Thus began the epic movement that would mark a turning point in the development of Chinese communism. The evacuation was difficult, because the Army of the Republic had taken positions in all areas occupied by the Communists. Advancing through remote and mountainous terrain, some 10,000 men (and some women) managed to escape Jiangxi starting a long strategic retreat through the interior of China which ended one year later when between 8,000 and 9,000 survivors reached the northern province of Shaanxi.
During the Zunyi Conference at the beginning of the Long March, the so-called 28 Bolsheviks, led by Bo Gu and Wang Ming, were ousted from power and Mao Zedong, to the dismay of the Soviet Union, had become the new leader of the Communist Party of China. The pro-Soviet Communist Party of China had ended and a new rural-inspired party emerged under the leadership of Mao. Deng had once again become a leading figure in the party, when the north ended up winning the civil war against the Kuomintang.
The confrontation between the two parties was temporarily interrupted, however, by the Japanese invasion, forcing the Kuomintang to form an alliance for the second time with the Communists to defend the nation against external aggression.
Japanese Invasion.
The invasion of Japanese troops in 1937 marked the beginning of the Second Sino-Japanese War. During the invasion, Deng remained in the area controlled by the Communists in the north, where he assumed the role of deputy political director of the three divisions of the restructured Communist army. From September 1937 until January 1938, he lived in Buddhist monasteries and temples in the Wutai Mountains. In January 1938, he was appointed as Political Commissar of the 129th division of the Eighth Route Army commanded by Liu Bocheng, starting a long-lasting partnership with Liu.
Deng stayed for most of the conflict with the Japanese in the war front in the area bordering the provinces of Shanxi, Henan and Hebei, then traveled several times to the city of Yan'an, where Mao had established the basis for Communist Party leadership. In one of his trips to Yan'an in 1939, he married, for the third and last time in his life, Zhuo Lin, a young native of Kunming, who, like other young idealists of the time, had traveled to Yan'an to join the Communists.
Resumed war against the Nationalists.
After Japan's defeat in World War II, Deng traveled to Chongqing, the city in which Chiang Kai-shek established his government during the blue Japanese invasion, to participate in peace talks between the Kuomintang and the Communist Party. The results of those negotiations were not positive and military confrontation between the two antagonistic parties resumed shortly after the meeting in Chongqing.
While Chiang Kai-shek re-established the government in Nanjing, the capital of the Republic of China, the Communists were fighting for control in the field. Following up with guerrilla tactics from their positions in rural areas against cities under the control of the government of Chiang and their supply lines, the Communists were increasing the territory under their control, and incorporating more and more soldiers who had deserted the Nationalist army.
In the final phase of the war, Deng again exercised a key role as political leader and propaganda master as Political Commissar of the 2nd Field Army commanded by Liu Bocheng. He also participated in disseminating the ideas of Mao Zedong, which turned into the ideological foundation of the Communist Party. His work in political and ideological work, along with his status as a veteran of the Long March, placed him in a privileged position within the party to occupy positions of power after the Communist Party managed to defeat Chiang Kai-shek and found the People's Republic of China.
Political career under Mao.
As Mayor of Chongqing.
On 1 October 1949, Deng attended the proclamation of the People's Republic of China in Beijing. At that time, the Communist Party controlled the entire north, but there were still parts of the south held by the Kuomintang regime. He became responsible for leading the liberation of southwest China, in his capacity as first secretary of the Department of the Southwest. This organization had the task of managing the final takeover of that part of the country where still held by the Kuomintang, while, on the other hand, most of Tibet was a de facto independent for many years.
The Kuomintang government after being forced to leave Guangzhou, and then had to establish a new provisional capital of Chongqing, the capital during the Japanese occupation. There, Chiang Kai-shek with his son Chiang Ching-kuo, former classmate of Deng in Moscow, were anxious to stop the Communist advance.
Under the political control of Deng, the Communist army won in Chongqing in late November 1949 and entered a few days later in Chengdu, the last bastion of power of Chiang Kai-shek. At that time, Deng took over as mayor of Chongqing; in addition to being the leader of the Communist Party in the southwest, where the Communist army, now known as the People's Liberation Army, had to suppress resistance loyal to the old Kuomintang regime. In 1950, the new state also seized control over Tibet.
Deng Xiaoping would spend three years in Chongqing, the city where he had studied in his teenage years before going to France. In 1952 he moved to Beijing, where he occupied different positions in the central government.
Political rise in Beijing.
In July 1952, Deng came to Beijing to assume the posts of Vice Premier and Deputy Chair of the Committee on Finance. Soon after, he took the posts of Minister of Finance and Director of the Office of Communications. In 1954, he was removed from all these positions, holding only the post of Deputy Premier. In 1956, he became Head of the Communist Party's Organization Department and Vice Chairman of the Central Military Commission.
After officially supporting Mao Zedong in his Anti-Rightist Movement of 1957, Deng acted as Secretary General of the Secretariat and ran the country's daily affairs with President Liu Shaoqi and Premier Zhou Enlai. Deng and Liu's policies emphasized economics over ideological dogma, an implicit departure from the mass hysteria of the Great Leap Forward.
Both Liu and Deng supported Mao in the mass campaigns of the 1950s, in which they attacked the bourgeois and capitalists, and promoted Mao's ideology. However, the economic failure of the Great Leap Forward was seen as an indictment on the ability of Mao to manage the economy. Peng Dehuai openly criticized Mao, while Liu Shaoqi and Deng, though more cautious, began to take charge of economic policy, leaving Mao out of day-to-day affairs of the party and state. Mao agreed to cede the presidency of the People's Republic (China's head of state position) to Liu Shaoqi, while retaining his positions as leader of the party and the army.
In 1963, Deng traveled to Moscow to lead a meeting of the Chinese delegation with Stalin's successor, Nikita Khrushchev. Relations between the People's Republic and the Soviet Union had worsened since the death of Stalin. After this meeting, no agreement was reached and the Sino–Soviet split was consummated; there was an almost total suspension of relations between the two major communist powers of the time.
Liu and Deng's economic reforms of the early 1960s were generally popular and restored many of the economic institutions previously dismantled during the Great Leap Forward. Mao, sensing his loss of prestige, took action to regain control of the state. Appealing to his revolutionary spirit, Mao launched the Cultural Revolution, which encouraged the masses to root out the right-wing capitalists who have "infiltrated the party". Deng was labelled the second-in-command of the "capitalist-roaders" faction ("Zouzipai").
Target of two purges.
Cultural Revolution.
Mao feared that the reformist economic policies of Deng and Liu could lead to restoration of capitalism and end the Chinese Revolution. For this and other reasons, Mao launched the Cultural Revolution in 1966, during which Deng fell out of favor and was forced to retire from all his positions.
During the Cultural Revolution, he and his family were targeted by Red Guards, who imprisoned Deng's eldest son, Deng Pufang. Deng Pufang was tortured and jumped out the window of a four-story building in 1968, becoming a paraplegic. In October 1969 Deng Xiaoping was sent to the Xinjian County Tractor Factory in rural Jiangxi province to work as a regular worker. In his four years there, Deng spent his spare time writing. He was purged nationally, but to a lesser scale than Liu Shaoqi.
After Lin Biao was killed in an air crash (according to official reports he was trying to flee from China after unsuccessfully trying to stage a coup against Mao), Deng Xiaoping (who had led a large field army during the civil war) became the most influential of the remaining army leaders. When Premier Zhou Enlai fell ill with cancer, Deng became Zhou's choice as successor, and Zhou was able to convince Mao to bring Deng back into politics in 1974 as First Vice-Premier, in practice running daily affairs. Deng focused on reconstructing the country's economy and stressed unity as the first step by raising production. He remained careful, however, to avoid contradicting Maoist ideology, at least on paper.
The Cultural Revolution was not yet over, and a radical leftist political group known as the Gang of Four, led by Mao's wife Jiang Qing, competed for power within the Party. The Gang saw Deng as their greatest challenge to power. Mao, too, was suspicious that Deng would destroy the positive reputation of the Cultural Revolution, which Mao considered one of his greatest policy initiatives. Beginning in late 1975, Deng was asked to draw up a series of self-criticisms. Although he admitted to having taken an "inappropriate ideological perspective" while dealing with state and party affairs, he was reluctant to admit that his policies were wrong in essence. His antagonism with the Gang of Four became increasingly clear, and Mao seemed to sway in the Gang's favour. Mao refused to accept Deng's self-criticisms and asked the party's Central Committee to "discuss Deng's mistakes thoroughly".
'Criticize Deng' campaign.
Zhou Enlai died in January 1976, to an outpouring of national grief. Zhou was a very important figure in Deng's political life, and his death eroded his remaining support within the Party's Central Committee. After delivering Zhou's official eulogy at the state funeral, the Gang of Four, with Mao's permission, began the so-called "Criticize Deng and Oppose the Rehabilitation of Right-leaning Elements" campaign. Hua Guofeng, not Deng, was selected to become Zhou's successor.
On 2 February 1976, the Central Committee issued a Top-Priority Directive, officially transferring Deng to work on "external affairs" and thus removing Deng from the party's power apparatus. Deng stayed at home for several months, awaiting his fate. The political turmoil halted the economic progress Deng had laboured for in the past year. On 3 March, Mao issued a directive reaffirming the legitimacy of the Cultural Revolution and specifically pointed to Deng as an internal, rather than external, problem. This was followed by a Central Committee directive issued to all local party organs to study Mao's directive and criticize Deng.
Deng's reputation as a reformer suffered a severe blow when the Qingming Festival, after the mass public mourning of Zhou on a traditional Chinese holiday culminated the Tiananmen Incident of 1976, an event the Gang of Four branded as counter-revolutionary and threatening to their power. Furthermore, the Gang deemed Deng the mastermind behind the incident, and Mao himself wrote that "the nature of things has changed". This prompted Mao to remove Deng from all leadership positions, although he retained his party membership.
Re-emergence post-Cultural Revolution.
Following Mao's death on 9 September 1976 and the purge of the Gang of Four in October 1976, Deng gradually emerged as the "de facto" leader of China. Prior to Mao's death, the only governmental position he held was that of First Vice Premier of the State Council, but Hua Guofeng wanted to rid the Party of extremists and successfully marginalised the Gang of Four. On 22 July 1977, Deng was restored to the posts of Vice-Chairman of the Central Committee, Vice-Chairman of the Military Commission and Chief of the General Staff of the People's Liberation Army.
By carefully mobilizing his supporters within the party, Deng outmaneuvered Hua, who had pardoned him, then ousted Hua from his top leadership positions by 1980. In contrast to previous leadership changes, Deng allowed Hua to retain membership in the Central Committee and quietly retire, helping to set the precedent that losing a high-level leadership struggle would not result in physical harm.
Deng repudiated the Cultural Revolution and, in 1977, launched the "Beijing Spring", which allowed open criticism of the excesses and suffering that had occurred during the period. Meanwhile, he was the impetus for the abolition of the class background system. Under this system, the CPC removed employment barriers to Chinese deemed to be associated with the former landlord class; its removal allowed a faction favoring the restoration of the private market to enter the Communist Party.
Deng gradually outmaneuvered his political opponents. By encouraging public criticism of the Cultural Revolution, he weakened the position of those who owed their political positions to that event, while strengthening the position of those like himself who had been purged during that time. Deng also received a great deal of popular support. As Deng gradually consolidated control over the CPC, Hua was replaced by Zhao Ziyang as premier in 1980, and by Hu Yaobang as party chief in 1981, despite the fact that Hua was Mao Zedong's designated successor as the "paramount leader" of the Communist Party of China and the People's Republic of China.
Important decisions were always taken in Deng's home with a caucus of eight senior party cadres, called "Eight Elders", especially with Chen Yun and Li Xiannian. Deng ruled as "paramount leader" although he never held the top title of the party, and was able to successively remove three party leaders, including Hu Yaobang. Deng remained the most influential of the CPC cadre, although after 1987 his only official posts were as chairman of the state and Communist Party Central Military Commissions.
The ideal sought a situation where the party developed policy, implemented by the state.
Deng's elevation to China's new number-one figure meant that the historical and ideological questions around Mao Zedong had to be addressed properly. Because Deng wished to pursue deep reforms, it was not possible for him to continue Mao's hard-line "class struggle" policies and mass public campaigns. In 1982 the Central Committee of the Communist Party released a document entitled "On the Various Historical Issues since the Founding of the People's Republic of China". Mao retained his status as a "great Marxist, proletarian revolutionary, militarist, and general", and the undisputed founder and pioneer of the country and the People's Liberation Army. "His accomplishments must be considered before his mistakes", the document declared. Deng personally commented that Mao was "seven parts good, three parts bad." The document also steered the prime responsibility of the Cultural Revolution away from Mao (although it did state that "Mao mistakenly began the Cultural Revolution") to the "counter-revolutionary cliques" of the Gang of Four and Lin Biao.
Opening up.
In November 1978, after the country had stabilized following political turmoil, Deng visited Bangkok, Kuala Lumpur and Singapore and met with Singapore's Prime Minister Lee Kuan Yew. Deng was very impressed with Singapore's economic development, greenery and housing, and later sent tens of thousands of Chinese to Singapore to learn from their experience. Lee, on the other hand, advised Deng to stop exporting Communist ideologies in Southeast Asia, an advice which Deng later followed.
Thanks to the support of other party leaders who had already recovered their official positions, in 1978 the rise to power of Deng was inevitable. Even though Hua Guofeng formally monopolized the top positions in the People's Republic, his position, with little support, was becoming increasingly difficult. In December 1978, during the Third Plenum of the 11th Central Committee Congress of the Communist Party of China, Deng took over the reins of power.
Beginning in 1979, the economic reforms accelerated the market model, while the leaders maintained old Communist-style rhetoric. The commune system was gradually dismantled and the peasants began to have more freedom to manage the land they cultivated and sell their products on the market. At the same time, China's economy opened to foreign trade. On 1 January of that year, the United States recognized the People's Republic of China, leaving the Republic of China's Nationalist government to one side, and business contacts between China and the West began to grow. In late 1978, the aerospace company Boeing announced the sale of 747 aircraft to various airlines in the PRC, and the beverage company Coca-Cola made public their intention to open a production plant in Shanghai.
In early 1979, Deng undertook an official visit to the United States, meeting President Jimmy Carter in Washington as well as several Congressmen. The Chinese insisted that ex-President Richard Nixon be invited to the formal White House reception, a symbolic indication of their assertiveness on the one hand, and their desire to continue with the Nixon initiatives on the other. During the visit, Deng visited the Johnson Space Center in Houston, as well as the headquarters of Coca-Cola and Boeing in Atlanta and Seattle, respectively. With these visits so significant, Deng made it clear that the new Chinese regime's priorities were economic and technological development.
Sino-Japanese relations also improved significantly. Deng used Japan as an example of a rapidly progressing power that set a good example for China economically.
True to his famous 1961 pronouncement "it doesn't matter whether a cat is black or white, if it catches mice it is a good cat", which had caused so much criticism, Deng, along with his closest collaborators, such as Zhao Ziyang, who in 1980 relieved Hua Guofeng as premier, and Hu Yaobang, who in 1981 did the same with the post of party chairman, took the reins of power and the purpose of advancing the "four modernizations" (economy, agriculture, scientific and technological development and national defense), and announced an ambitious plan of opening and liberalizing the economy. The last position of power retained by Hua Guofeng, chairman of the Central Military Commission, was taken by Deng in 1981.
From 1980, Deng led the expansion of the economy and in political terms, took over negotiations with the United Kingdom to return the territory of Hong Kong, meeting personally with British Prime Minister Margaret Thatcher. The result of these negotiations was the Sino-British Joint Declaration, signed on 19 December 1984, which formally outlined the United Kingdom's return of Hong Kong to China by 1997. The Chinese government pledged to respect the economic system and civil liberties of the then British colony for 50 years after the return.
In 1987, Portugal, under pressure from the Chinese authorities, agreed to arrange the return of its colony of Macau by 1999, with an agreement roughly equal to that of Hong Kong. The return of these two territories was based on political principle formulated by Deng himself called "one country, two systems", which refers to the coexistence under one political authority areas with different economic systems, communism and capitalism. Although this theory was applied to the cases of Hong Kong and Macau, it seems that Deng intended to also present it as an attractive option to the people of Taiwan for eventual incorporation of that island, claimed as Chinese territory.
China's rapid economic growth presented several problems. The 1982 population census revealed the extraordinary growth of the population, which already exceeded one billion people. Deng continued the plans initiated by Hua Guofeng to restrict birth to only one child, limiting women to one child under pain of administrative penalty. Yet increasing economic freedom was being translated into a greater freedom of opinion and critics began to arise with the system, including the famous dissident Wei Jingsheng, who coined the term "fifth modernization" in reference to democracy as a missing element in the renewal plans of Deng Xiaoping. In late 1980s, dissatisfaction with the authoritarian regime and the growing inequalities caused the biggest crisis to Deng's leadership.
In October 1987, at the Plenary Session of the National People's Congress, Deng was re-elected as Chairman of Central Military Commission, but he resigned as Chairman of the Central Advisory Commission and he was succeeded by Chen Yun. He continued to chair and developed the reform and opening up as the main policy, put forward the three steps suitable for China's economic development strategy within 70 years: the first step, to double the 1980 GNP and ensure that the people have enough food and clothing, was attained by the end of the 1980s; second step, to quadruple the 1980 GNP by the end of the 20th century, was achieved in 1995 ahead of schedule; the third step, to increase per capita GNP to the level of the medium-developed countries by 2050, at which point, the Chinese people will be fairly well-off and modernization will be basically realized.
Deng, however, did little to improve relations with the Soviet Union; he continued to adhere the Maoist line of the Sino–Soviet split era that the Soviet Union was a superpower as "hegemonic" as the United States, but even more threatening to China because of its geographic proximity.
Economic reforms.
Improving relations with the outside world was the second of two important philosophical shifts outlined in Deng's program of reform termed "Gaige Kaifang" ("lit." Reforms and Openness). China's domestic social, political, and most notably, economic systems would undergo significant changes during Deng's time as leader. The goals of Deng's reforms were summed up by the Four Modernizations, those of agriculture, industry, science and technology and the military.
The strategy for achieving these aims of becoming a modern, industrial nation was the socialist market economy. Deng argued that China was in the primary stage of socialism and that the duty of the party was to perfect so-called "socialism with Chinese characteristics", and "seek truth from facts". (This somewhat resembles the Leninist theoretical justification of the New Economic Policy (NEP) in the 1920s, which argued that the Soviet Union had not gone deeply enough into the capitalist phase and therefore needed limited capitalism in order to fully evolve its means of production.)
This interpretation of Maoism reduced the role of ideology in economic decision-making. Downgrading communitarian values, but not necessarily the criticising the ideology of Marxism-Leninism, Deng emphasized that "socialism does not mean shared poverty". His theoretical justification for allowing market forces was given as such:
"Planning and market forces are not the essential difference between socialism and capitalism. A planned economy is not the definition of socialism, because there is planning under capitalism; the market economy happens under socialism, too. Planning and market forces are both ways of controlling economic activity."
Unlike Hua Guofeng, Deng believed that no policy should be rejected outright simply because it was not associated with Mao. Unlike more conservative leaders such as Chen Yun, Deng did not object to policies on the grounds that they were similar to ones which were found in capitalist nations.
This political flexibility towards the foundations of socialism is strongly supported by quotes such as:
We mustn't fear to adopt the advanced management methods applied in capitalist countries (...) The very essence of socialism is the liberation and development of the productive systems (...) Socialism and market economy are not incompatible (...) We should be concerned about right-wing deviations, but most of all, we must be concerned about left-wing deviations.
Although Deng provided the theoretical background and the political support to allow economic reform to occur, it is in general consensus amongst historians that few of the economic reforms that Deng introduced were originated by Deng himself. Premier Zhou Enlai, for example, pioneered the Four Modernizations years before Deng. In addition, many reforms would be introduced by local leaders, often not sanctioned by central government directives. If successful and promising, these reforms would be adopted by larger and larger areas and ultimately introduced nationally. An often cited example is the household-responsibility system, which was first secretly implemented by a poor rural village at the risk of being convicted as "counter-revolutionary." This experiment proved very successful. Deng openly supported it and it was later adopted nationally. Many other reforms were influenced by the experiences of the East Asian Tigers.
This is in sharp contrast to the pattern in the "perestroika" undertaken by Mikhail Gorbachev in which most of the major reforms were originated by Gorbachev himself. The bottom-up approach of the Deng reforms, in contrast to the top-down approach of "perestroika", was likely a key factor in the success of the former.
Deng's reforms actually included the introduction of planned, centralized management of the macro-economy by technically proficient bureaucrats, abandoning Mao's mass campaign style of economic construction. However, unlike the Soviet model, management was indirect through market mechanisms. Deng sustained Mao's legacy to the extent that he stressed the primacy of agricultural output and encouraged a significant decentralization of decision making in the rural economy teams and individual peasant households. At the local level, material incentives, rather than political appeals, were to be used to motivate the labor force, including allowing peasants to earn extra income by selling the produce of their private plots at free market.
Export focus.
In the move toward market allocation, local municipalities and provinces were allowed to invest in industries that they considered most profitable, which encouraged investment in light manufacturing. Thus, Deng's reforms shifted China's development strategy to an emphasis on light industry and export-led growth. Light industrial output was vital for a developing country coming from a low capital base. With the short gestation period, low capital requirements, and high foreign-exchange export earnings, revenues generated by light manufacturing were able to be reinvested in technologically more advanced production and further capital expenditures and investments.
However, in sharp contrast to the similar, but much less successful reforms in the Socialist Federal Republic of Yugoslavia and the People's Republic of Hungary, these investments were not government mandated. The capital invested in heavy industry largely came from the banking system, and most of that capital came from consumer deposits. One of the first items of the Deng reforms was to prevent reallocation of profits except through taxation or through the banking system; hence, the reallocation in state-owned industries was somewhat indirect, thus making them more or less independent from government interference. In short, Deng's reforms sparked an industrial revolution in China.
These reforms were a reversal of the Maoist policy of economic self-reliance. China decided to accelerate the modernization process by stepping up the volume of foreign trade, especially the purchase of machinery from Japan and the West. By participating in such export-led growth, China was able to step up the Four Modernizations by attaining certain foreign funds, market, advanced technologies and management experiences, thus accelerating its economic development. Deng attracted foreign companies to a series of Special Economic Zones, where foreign investment and market liberalization were encouraged.
The reforms sought to improve labor productivity. New material incentives and bonus systems were introduced. Rural markets selling peasants' homegrown products and the surplus products of communes were revived. Not only did rural markets increase agricultural output, they stimulated industrial development as well. With peasants able to sell surplus agricultural yields on the open market, domestic consumption stimulated industrialization as well and also created political support for more difficult economic reforms.
There are some parallels between Deng's market socialism especially in the early stages, and Vladimir Lenin's NEP as well as those of Nikolai Bukharin's economic policies, in that both foresaw a role for private entrepreneurs and markets based on trade and pricing rather than central planning. An interesting anecdote on this note is the first meeting between Deng and Armand Hammer. Deng pressed the industrialist and former investor in Lenin's Soviet Union for as much information on the NEP as possible.
Role in the Tiananmen Square protests.
The Tiananmen Square protests of 1989, culminating in the June Fourth Massacre, were a series of demonstrations in and near Tiananmen Square in the People's Republic of China (PRC) between 15 April and 4 June 1989, a year in which many other socialist governments collapsed.
The protests were sparked by the death of Hu Yaobang, a reformist official backed by Deng but ousted by the Eight Elders and the conservative wing of the politburo. Many people were dissatisfied with the party's slow response and relatively subdued funeral arrangements. Public mourning began on the streets of Beijing and universities in the surrounding areas. In Beijing this was centered on the Monument to the People's Heroes in Tiananmen Square. The mourning became a public conduit for anger against perceived nepotism in the government, the unfair dismissal and early death of Hu, and the behind-the-scenes role of the "old men". By the eve of Hu's funeral, the demonstration had reached 100,000 people on Tiananmen Square. While the protests lacked a unified cause or leadership, participants raised the issue of corruption within the government and some voiced calls for economic liberalization and democratic reform within the structure of the government while others called for a less authoritarian and less centralized form of socialism.
During the demonstrations, Deng's pro-market ally general secretary Zhao Ziyang supported the demonstrators and distanced himself from the Politburo. Martial law was declared on 20 May by the socialist hardliner Li Peng, but the initial military advance on the city was blocked by residents. The movement lasted seven weeks. On 3–4 June, over two hundred thousand soldiers in tanks and helicopters were sent into the city to quell the protests by force, resulting in thousands of casualties. Many ordinary people in Beijing believed that Deng had ordered the intervention, but political analysts do not know who was actually behind the order. However, Deng's daughter defends the actions that occurred as a collective decision by the party leadership.
To purge sympathizers of Tiananmen demonstrators, the Communist Party initiated a one-and-a-half-year-long program similar to Anti-Rightist Movement. Old-timers like Deng Fei aimed to deal "strictly with those inside the party with serious tendencies toward bourgeois liberalization" and more than 30,000 communist officers were deployed to the task.
Zhao was placed under house arrest by hardliners and Deng himself was forced to make concessions to them. He soon declared that "the entire imperialist Western world plans to make all socialist countries discard the socialist road and then bring them under the monopoly of international capital and onto the capitalist road". A few months later he said that the "United States was too deeply involved" in the student movement, referring to foreign reporters who had given financial aid to the student leaders and later helped them escape to various Western countries, primarily the United States through Hong Kong and Taiwan.
Although he at first made concessions to the socialist hardliners, he soon resumed his reforms after his 1992 southern tour. After his tour, he was able to stop the attacks of the socialist hardliners on the reforms through their "named capitalist or socialist?" campaign.
Deng privately told Canadian Prime Minister Pierre Trudeau that factions of the Communist Party could have grabbed army units and the country had risked a civil war. Two years later, Deng endorsed Zhu Rongji, a Shanghai Mayor, as a vice-premier candidate. Zhu Rongji had refused to declare martial law in Shanghai during the demonstrations even though socialist hardliners had pressured him.
Resignation and 1992 southern tour.
Officially, Deng decided to retire from top positions when he stepped down as Chairman of the Central Military Commission in 1989, and retired from the political scene in 1992. China, however, was still in the "era of Deng Xiaoping". He continued to be widely regarded as the "paramount leader" of the country, believed to have backroom control. Deng was recognized officially as "the chief architect of China's economic reforms and China's socialist modernization". To the Communist Party, he was believed to have set a good example for communist cadres who refused to retire at old age. He broke earlier conventions of holding offices for life. He was often referred to as simply "Comrade Xiaoping", with no title attached.
Because of the Tiananmen Square protests of 1989, Deng's power had been significantly weakened and there was a growing formalist faction opposed to Deng's reforms within the Communist Party. To reassert his economic agenda, in the spring of 1992, Deng made his famous southern tour of China, visiting Guangzhou, Shenzhen, Zhuhai and spending the New Year in Shanghai, using his travels as a method of reasserting his economic policy after his retirement from office.
On his tour, Deng made various speeches and generated large local support for his reformist platform. He stressed the importance of economic reform in China, and criticized those who were against further reform and opening up. Although there was a debate on whether or not Deng actually said it, his perceived catchphrase, "To get rich is glorious" (致富光荣), unleashed a wave of personal entrepreneurship that continues to drive China's economy today. He stated that the "leftist" elements of Chinese society were much more dangerous than "rightist" ones. Deng was instrumental in the development of Shanghai's Pudong New Area, revitalizing the city as China's economic hub.
His southern tour was at first ignored by the Beijing and national media, which were then under the control of Deng's political rivals. Jiang Zemin, General Secretary of the Communist Party of China since 1989, showed little support. Challenging their media control, Shanghai's "Liberation Daily" newspaper published several articles supporting reforms authored by "Huangfu Ping", which quickly gained support amongst local officials and populace. Deng's new wave of policy rhetoric gave way to a new political storm between factions in the Politburo. Jiang eventually sided with Deng, and the national media finally reported Deng's southern tour several months after it occurred. Observers suggest that Jiang's submission to Deng's policies had solidified his position as Deng's heir apparent. Behind the scenes, Deng's southern tour aided his reformist allies' climb to the apex of national power, and permanently changed China's direction toward economic development. In addition, the eventual outcome of the southern tour proved that Deng was still the most powerful man in China.
Deng's insistence on economic openness aided in the phenomenal growth levels of the coastal areas, especially the "Golden Triangle" region surrounding Shanghai. Deng reiterated that "some areas must get rich before others", and asserted that the wealth from coastal regions will eventually be transferred to aid economic construction inland. The theory, however, faced numerous challenges when put into practice, as provincial governments moved to protect their own interests.
Death and reaction.
Deng died on 19 February 1997 from a lung infection and Parkinson's disease. Even though his successor Jiang Zemin was in firm control, government policies maintained Deng's political and economic philosophies. Officially, Deng was eulogized as a "great Marxist, great Proletarian Revolutionary, statesman, military strategist, and diplomat; one of the main leaders of the Communist Party of China, the People's Liberation Army of China, and the People's Republic of China; The great architect of China's socialist opening-up and modernized construction; the founder of Deng Xiaoping Theory".
Although the public was largely prepared for Deng's death, as rumors had been circulating for a long time, the death of Deng was followed by the greatest publicly sanctioned display of grief for any Chinese leader since Mao Zedong. However, in contrast, Deng's death in the media was announced without any titles attached (Mao was called the Great Leader and teacher, Deng was simply "Comrade"), or any emotional overtones from the news anchors that delivered the message.
At 10:00 on the morning of 24 February, people were asked by Premier Li Peng to pause in silence for three minutes. The nation's flags flew at half-mast for over a week. The nationally televised funeral, which was a simple and relatively private affair attended by the country's leaders and Deng's family, was broadcast on all cable channels. Jiang's tearful eulogy to the late reformist leader declared, "The Chinese people love, thank, mourn and cherish the memory of Comrade Deng Xiaoping because he devoted his life-long energies to the Chinese people, performed immortal feats for the independence and liberation of the Chinese nation." Jiang vowed to continue Deng's policies.
After the funeral, his organs were donated to medical research, the remains were cremated, and his ashes were subsequently scattered at sea, according to his wishes. For the next two weeks, Chinese state media ran news stories and documentaries related to Deng's life and death, with the regular 19:00 "National News" program in the evening lasting almost two hours over the regular broadcast time.
Certain segments of the Chinese population, notably the modern Maoists and radical reformers (the far left and the far right), had negative views on Deng. In the year that followed, songs like "Story of Spring" by Dong Wenhua, which were created in Deng's honour shortly after Deng's southern tour in 1992, once again were widely played.
There was a significant amount of international reaction to Deng's death: UN Secretary-General Kofi Annan said Deng was to be remembered "in the international community at large as a primary architect of China's modernization and dramatic economic development". French President Jacques Chirac said "In the course of this century, few men have, as much as Deng, led a vast human community through such profound and determining changes"; British Prime Minister John Major commented about Deng's key role in the return of Hong Kong to Chinese control; Canadian Prime Minister Jean Chrétien called Deng a "pivotal figure" in Chinese history. The Kuomintang chair in Taiwan also sent its condolences, saying it longed for peace, cooperation, and prosperity. The Dalai Lama voiced regret.
Memorials.
When compared to the memorials of other former CPC leaders, those dedicated to Deng have been relatively low profile, in keeping with Deng's pragmatism. Likewise, he was cremated after death, as opposed to being embalmed like Mao.
There are a few public displays of Deng in the country. A bronze statue of Deng was erected on 14 November 2000, at the grand plaza of Lianhua Mountain Park () of Shenzhen. This statue is dedicated to Deng's role as a great planner and contributor to the development of the Shenzhen Special Economic Zone, starting in 1979. The statue is 6 m high, with an additional 3.68-meter base. The statue shows Deng striding forward confidently. Many CPC high level leaders visit the statue frequently. In addition, in many coastal areas and on the island province of Hainan, Deng is seen on large roadside billboards with messages emphasizing economic reform or his policy of One country, two systems.
Another bronze statue of Deng was dedicated 13 August 2004 in the city of Guang'an, Deng's hometown, in southwest China's Sichuan Province. The statue was erected to commemorate Deng's 100th birthday. The statue shows Deng, dressed casually, sitting on a chair and smiling. The Chinese characters for "Statue of Deng Xiaoping" are inscribed on the pedestal. The original calligraphy was written by Jiang, then Chairman of the Central Military Commission.
Deng Xiaoping's Former Residence in his hometown of Paifang Village in Sichuan province has been preserved as a museum telling Deng's life story.
In Bishkek, capital of the Republic of Kyrgyzstan, there is a six-lane boulevard, 25 m wide and 3.5 km long, the "Deng Xiaoping Prospekt", which was dedicated on 18 June 1997. A two-meter high red granite monument stands at the east end of this route. The epigraph in memory of Deng is written in Chinese, Russian and Kirghiz.
A documentary on Deng entitled "Deng Xiaoping" was released by CCTV in January 1997, prior to Deng's death, and chronicles his life from his days as a student in France to his "Southern Tour" of 1993. In 2014, a TV series commemorating Deng entitled "Deng Xiaoping at History's Crossroads" was released by CCTV in anticipation of the 110th anniversary of the birth of Deng Xiaoping.
References.
</dl>
Further reading.
</dl>
External links.
class="wikitable succession-box" style="margin:0.5em auto; font-size:95%;clear:both;"

</doc>
<doc id="8206" url="http://en.wikipedia.org/wiki?curid=8206" title="HM Prison Dartmoor">
HM Prison Dartmoor

HM Prison Dartmoor is a Category C men's prison, located in Princetown, high on Dartmoor in the English county of Devon. Its high granite walls dominate this area of the moor. The prison is owned by the Duchy of Cornwall, and is operated by Her Majesty's Prison Service.
History.
In 1805, Great Britain was at war with Napoleonic France; a conflict during which thousands of prisoners were taken and confined in prison "hulks" or derelict ships. This was considered unsafe, partially due to the proximity of the Royal Naval dockyard at Devonport (then called Plymouth Dock), and as living conditions were appalling in the extreme, a prisoner of war depot was planned in the remote isolation of Dartmoor. Construction started in 1806, taking three years to complete. In 1809 the first French prisoners arrived, and were joined by American POWs taken in the War of 1812. At one time the prison population numbered almost 6,000. Many prisoners died and were buried on the moor. Both French and American wars were concluded in 1815, and repatriations began. The prison then lay empty until 1850, when it was largely rebuilt and commissioned as a convict gaol. With the establishment of the prison farm in about 1852, all the prisoners' remains were exhumed and re-interred in two cemeteries behind the prison.
Early history.
Designed by Daniel Asher Alexander and constructed originally between 1806 and 1809 by local labour, to hold prisoners of the Napoleonic Wars, it was also used to hold American prisoners from the War of 1812. Although the war ended with the Treaty of Ghent in December 1814, many American prisoners of war still remained in Dartmoor.
From the spring of 1813 until March 1815 about 6,500 American sailors were imprisoned at Dartmoor. These were naval prisoners, and impressed American seamen discharged from British vessels. Whilst the British were in charge of the prison, the prisoners created their own governance and culture. They had courts which meted out punishments, there was an in-prison market, a theatre and a gambling room. About 1,000 of the prisoners were black Americans.
After the prisoners heard of the Peace of Ghent, they expected immediate release, but the British government refused to let them go on parole or take any steps until the treaty was ratified by the United States Senate, 17 February 1815. It took several weeks for the American agent to secure ships for their transportation home, and the men grew very impatient. On 4 April, a food contractor attempted to work off some damaged hardtack on them in place of soft bread and was forced to yield by their insurrection. The commandant, Captain T. G. Shortland, suspected them of a design to break out of the gaol. This was the reverse of the truth in general, as they would lose their chance of going on the ships, but a few had made threats of the sort, and the commandant was very uneasy.
About 6:00 pm of the 6th, Shortland discovered a hole from one of the five prisons to the barrack yard near the gun racks. Some prisoners were outside the fence, noisily pelting each other with turf, and many more were near the breach (and the gambling tables), though the signal for return to prisons had sounded. Shortland was convinced of a plot, and rang the alarm bell to collect the officers and have the men ready. This precaution brought back a crowd just going to quarters. Just then a prisoner broke a gate chain with an iron bar and a number of the prisoners pressed through to the prison market square. After attempts at persuasion, Shortland ordered a charge which drove some of the prisoners in. Those near the gate, however, hooted at and taunted the soldiery, who fired a volley over their heads. The crowd yelled louder and threw stones, and the soldiers, probably without orders, fired a direct volley which killed and wounded a large number. Then they continued firing at the prisoners, many of whom were now struggling to get back inside the blocks.
Finally the captain, a lieutenant and the hospital surgeon (the other officers being at dinner) succeeded in stopping the shooting and caring for the wounded – about 60, 30 seriously, besides seven killed outright. The affair was examined by a joint commission, Charles King for the United States and F. S. Larpent for Great Britain, which exonerated Shortland, justified the initial shooting and blamed the subsequent deaths on unknown culprits. The British government provided for the families of the killed, pensioned the disabled and promoted Shortland.
A memorial has been erected to the 271 POWs (mostly seamen) who are buried in the prison grounds.
Dartmoor was reopened in 1851 as a civilian prison, but was closed again in 1917, when it was converted into a Home Office Work Centre for certain conscientious objectors granted release from prison; cells were unlocked, inmates wore their own clothes, and could visit the village in their off-duty time. It was reopened as a prison in 1920, and then contained some of Britain's most serious offenders.
Dartmoor mutiny.
On 24 January 1932, there was a major disturbance at the prison. The cause of the riots is generally attributed to the food, not generally but just on specific days when it was suspected it had been tampered with prior to the disturbance. There had also been other instances of disobedience prior to this, according to the official Du Parcq report into the incident such as a model prisoner attacking a popular guard with a razor blade and rough treatment of a prisoner being removed to solitary. At the parade later that day, 50 prisoners refused orders, and the rest were marched back to their cells but refused to enter. At this point, the prison governor and his staff fled to an unused part of the prison and secured themselves in there. The prisoners then released those held in solitary. There was extensive damage to property and a prisoner was shot by one of the staff but no prison staff were injured. According to Fitzgerald (1977) "Reinforcements arrived, and within fifteen minutes these 'vicious brutes', who for some two hours had terrorized well-armed prison staff, and effectively controlled the prison, had surrendered and been locked up again".
Post 2000 history.
In 2001 a Board of Visitors report condemned sanitation at Dartmoor as well as highlighting a list of urgent repairs needed at the prison. A year later Dartmoor was converted to a Category C prison for less violent offenders.
In 2002 the Prison Reform Trust warned that Dartmoor Prison may be breaching the Human Rights Act 1998 due to severe overcrowding at the jail. A year later, however, the Chief Inspector of Prisons declared that Dartmoor had made substantial improvements to its management and regime.
In March 2008 staff at the prison passed a vote of no confidence in the governor Serena Watts, claiming they felt bullied by managers and unsafe.
Previous governor Tony Corcoran is now governor of HMP Haverigg, taking up the post in early 2013.
The prison today.
Dartmoor still has a misplaced reputation for being a high-security prison that is escape-proof. Now a Category C prison, Dartmoor houses mainly non-violent offenders and white-collar criminals. It also holds sex offenders and offers sex offender treatment programmes intended to make the offender realise their behaviour is unacceptable. Some subsequently volunteer for behaviour changing treatment with medication under a scheme being piloted at HMP Whatton which has had encouraging results.
Dartmoor offers cellular accommodation on 6 wings. Education is available at the prison (full and part-time), and ranges from basic educational skills to Open University courses. Vocational training includes electronics, brickwork and carpentry courses up to City & Guilds and NVQ level, Painting and Decorating courses, industrial cleaning and desktop publishing. Full-time employment is also available in catering, farming, gardening, laundry, textiles, Braille, contract services, furniture manufacturing and polishing. Employment is supported with NVQ or City & Guilds vocational qualifications. All courses and qualifications at Dartmoor are operated by South Gloucestershire and Stroud College and Cornwall College.
The 'Dartmoor Jailbreak' is a yearly event, in which members of the public 'escape' from the prison and must travel as far as possible in 4 days, without directly paying for transport, raising money for charity.
Future.
In September 2013 it was announced that discussions would commence with the Duchy of Cornwall about the long-term future of HMP Dartmoor. In January 2014 it was stated on the BBC news website that the notice period with the Duchy for closing is 10 years.
Dartmoor Prison Museum.
The Dartmoor Prison Museum, located in the old dairy buildings, focuses on the history of HMP Dartmoor. Exhibits include the prison's role in housing prisoners of war from the Napoleonic Wars and the War of 1812, manacles and weapons, memorabilia, clothing and uniforms, famous prisoners, and the changed focus of the prison. It also sells (2010) garden ornaments, plus postcards and fridge magnets, and similar mementoes.

</doc>
<doc id="8207" url="http://en.wikipedia.org/wiki?curid=8207" title="Dilation and curettage">
Dilation and curettage

Dilation (or dilatation) and curettage (D&C) refers to the dilation (widening/opening) of the cervix and surgical removal of part of the lining of the uterus and/or contents of the uterus by scraping and scooping (curettage). It is a therapeutic gynecological procedure as well as the most often used method of first trimester abortion.
D&C normally refers to a procedure involving a curette, also called "sharp curettage". However, some sources use the term D&C to refer more generally to any procedure that involves the processes of dilation and removal of uterine contents, which includes the more common "suction curettage" procedures of manual and electric vacuum aspiration.
Procedure.
The first step in a D&C is to dilate the cervix, usually done a few hours before the surgery. The woman is usually put under general anesthesia before the procedure begins. A curette, a metal rod with a handle on one end and a sharp loop on the other, is inserted into the uterus through the dilated cervix. The curette is used to gently scrape the lining of the uterus and remove the tissue in the uterus. This tissue is examined for completeness (in the case of abortion or miscarriage treatment) or pathologically for abnormalities (in the case of treatment for abnormal bleeding).
Clinical uses.
D&Cs are commonly performed for the diagnosis of gynecological conditions leading to 'abnormal uterine bleeding'; to resolve abnormal uterine bleeding (too much, too often or too heavy a menstrual flow); to remove the excess uterine lining in women who have conditions such as polycystic ovary syndrome (which cause a prolonged buildup of tissue with no natural period to remove it); to remove tissue in the uterus that may be causing abnormal vaginal bleeding, including postpartum retained placenta; to remove retained tissue (also known as retained POC or retained products of conception) in the case of a missed or incomplete miscarriage; and as a method of abortion that is now uncommon. In contrast, D&C remains 'standard care' for missed and incomplete miscarriage in many countries despite the existence of alternatives currently used for abortions.
Because medical and non-invasive methods of abortion now exist, and because D&C requires heavy sedation or general anesthesia and has higher risks of complication, the procedure has been declining as a method of abortion. The World Health Organization recommends D&C as a method of surgical abortion only when manual vacuum aspiration is unavailable. Most D&Cs are now carried out for miscarriage management and other indications such as diagnosis.
Hysteroscopy is a valid alternative to D&C for many surgical indications from diagnosis of uterine pathology to the removal of fibroids and even retained products of conception. It poses less risk because the doctor has a view inside the uterus during surgery, unlike with blind D&C.
Medical management of miscarriage and medical abortion using drugs such as misoprostol and mifepristone are safe, non-invasive and cheaper alternatives to D&C.
Complications.
Complications may arise from either the introduction or spreading of infection, adverse reaction to general anesthesia required during the surgery or from instrumentation itself, if the procedure is performed blindly (without the use of any imaging technique such as ultrasound or hysteroscopy).
One risk of sharp curettage is uterine perforation. Although normally no treatment is required for uterine perforation, a laparoscopy may be done to verify that bleeding has stopped on its own. Infection of the uterus or fallopian tubes is also a possible complication, especially if the woman has an untreated sexually transmitted infection.
Another risk is intrauterine adhesions, or Asherman's syndrome. One study found that in women who had one or two sharp curettage procedures for miscarriage, 14-16% developed some adhesions. Women who underwent three sharp curettage procedures for miscarriage had a 32% risk of developing adhesions. The risk of Asherman's syndrome was found to be 30.9% in women who had D&C following a missed miscarriage, and 25% in those who had a D&C 1–4 weeks postpartum. Untreated Asherman's syndrome, especially if severe, also increases the risk of complications in future pregnancies, such as ectopic pregnancy, miscarriage, and abnormal placentation (e.g.placenta previa and placenta accreta). According to recent case reports, use of vacuum aspiration can also lead to intrauterine adhesions. Yet, in terms of long-term reproductive outcome after miscarriage, a review in 2013 came to the conclusion that there were no studies reporting a link to D&C, while similar pregnancy outcomes were reported subsequent to surgical management (including D&C), medical management or conservative management (that is, watchful waiting). After miscarriage, there is an association between surgical intervention in the uterus and the development of intrauterine adhesions, and between intrauterine adhesions and adverse pregnancy outcomes, but there is still no clear evidence of any method of prevention of adverse pregnancy outcomes related to intrauterine adhesions.

</doc>
<doc id="8209" url="http://en.wikipedia.org/wiki?curid=8209" title="Doctor Who">
Doctor Who

Doctor Who is a British science-fiction television programme produced by the BBC from 1963 to the present day. The programme depicts the adventures of the Doctor, a Time Lord—a time-travelling humanoid alien. He explores the universe in his TARDIS, a sentient time-travelling space ship. Its exterior appears as a blue British police box, which was a common sight in Britain in 1963 when the series first aired. Along with a succession of companions, the Doctor combats a variety of foes while working to save civilisations and help people in need.
The show is a significant part of British popular culture, and elsewhere it has become a cult television favourite. The show has influenced generations of British television professionals, many of whom grew up watching the series. The programme originally ran from 1963 to 1989. After an unsuccessful attempt to revive regular production in 1996 with a backdoor pilot in the form of a television film, the programme was relaunched in 2005 by Russell T Davies who was showrunner and head writer for the first five years of its revival, produced in-house by BBC Wales in Cardiff. The first series of the 21st century, featuring Christopher Eccleston in the title role, was produced by the BBC. Series two and three had some development money contributed by the Canadian Broadcasting Corporation (CBC), which was credited as a co-producer. "Doctor Who" also spawned spin-offs in multiple media, including "Torchwood" (2006–11) and "The Sarah Jane Adventures" (2007–11), both created by Russell T Davies; "K-9" (2009–10); and a single pilot episode of "K-9 and Company" (1981). There also have been many spoofs and cultural references of the character in other media.
Twelve actors have headlined the series as the Doctor. The transition from one actor to another, and the differing approach to the role that they bring, is written into the plot of the show as regeneration into a new incarnation, a life process of Time Lords through which the character of the Doctor takes on a new body and, to some extent, new personality, which occurs after sustaining injury which would be fatal to most other species. While each actor's portrayal differs, they are all intended to be aspects of the same character, and form part of the same storyline. The time-travelling nature of the plot means that on occasion, story arcs have involved different Doctors meeting each other. Peter Capaldi took on the role after Matt Smith's exit in the 2013 Christmas special, "The Time of the Doctor".
Premise.
"Doctor Who" follows the adventures of the primary character, a rogue Time Lord from the planet Gallifrey who simply goes by the name "The Doctor". He fled from Gallifrey in a stolen Mark I Type 40 TARDIS time machine - "Time and Relative Dimension in Space" - which allows him to travel across time and space. Due to a malfunction of the TARDIS' "chameleon circuit", which normally allows the TARDIS to take on the appearance of local objects to disguise it from others, the Doctor's TARDIS remains fixed as a blue British Police box. The Doctor rarely travels alone, and often brings one or more companions to share these adventures with, typically humans as he has found a fascination with the planet Earth. He often finds events that pique his curiosity while trying to prevent evil forces from harming innocent people or changing history, using only his ingenuity and minimal resources, such as his versatile sonic screwdriver. As a Time Lord, the Doctor has the ability to regenerate when his body is mortally damaged, taking on a new appearance and personality. The Doctor has gained numerous reoccurring enemies during his travels, including the Daleks, the Cybermen, and another renegade Time Lord, the Master.
History.
"Doctor Who" first appeared on BBC1 television at 17:16:20 GMT, eighty seconds after the scheduled programme time, 5:15 pm, on Saturday, 23 November 1963. It was to be a regular weekly programme, each episode 25 minutes of transmission length. Discussions and plans for the programme had been in progress for a year. The head of drama, Canadian Sydney Newman, was mainly responsible for developing the programme, with the first format document for the series being written by Newman along with the head of the script department (later head of serials) Donald Wilson and staff writer C. E. Webber. Writer Anthony Coburn, story editor David Whitaker and initial producer Verity Lambert also heavily contributed to the development of the series. The programme was originally intended to appeal to a family audience, as an educational programme using time travel as a means to explore scientific ideas and famous moments in history. On 31 July 1963 Whitaker commissioned Terry Nation to write a story under the title "The Mutants". As originally written, the Daleks and Thals were the victims of an alien neutron bomb attack but Nation later dropped the aliens and made the Daleks the aggressors. When the script was presented to Newman and Wilson it was immediately rejected as the programme was not permitted to contain any "bug-eyed monsters". The first serial had been completed and the BBC believed it was crucial that the next one be a success, however, "The Mutants" was the only script ready to go so the show had little choice but to use it. According to producer Verity Lambert; "We didn't have a lot of choice — we only had the Dalek serial to go ... We had a bit of a crisis of confidence because Donald [Wilson] was so adamant that we shouldn't make it. Had we had anything else ready we would have made that." Nation's script became the second "Doctor Who" serial – "The Daleks" (a.k.a. "The Mutants"). The serial introduced the eponymous aliens that would become the series' most popular monsters, and was responsible for the BBC's first merchandising boom.
The BBC drama department's serials division produced the programme for 26 seasons, broadcast on BBC 1. Falling viewing numbers, a decline in the public perception of the show and a less-prominent transmission slot saw production suspended in 1989 by Jonathan Powell, controller of BBC 1. Although (as series co-star Sophie Aldred reported in the documentary "Doctor Who: More Than 30 Years in the TARDIS") it was effectively, if not formally, cancelled with the decision not to commission a planned 27th series of the show for transmission in 1990, the BBC repeatedly affirmed that the series would return.
While in-house production had ceased, the BBC hoped to find an independent production company to relaunch the show. Philip Segal, a British expatriate who worked for Columbia Pictures' television arm in the United States, had approached the BBC about such a venture as early as July 1989, while the 26th series was still in production. Segal's negotiations eventually led to a "Doctor Who" television film, broadcast on the Fox Network in 1996 as a co-production between Fox, Universal Pictures, the BBC and BBC Worldwide. Although the film was successful in the UK (with 9.1 million viewers), it was less so in the United States and did not lead to a series.
Licensed media such as novels and audio plays provided new stories, but as a television programme "Doctor Who" remained dormant until 2003. In September of that year, BBC Television announced the in-house production of a new series after several years of attempts by BBC Worldwide to find backing for a feature film version. The executive producers of the new incarnation of the series were writer Russell T Davies and BBC Cymru Wales head of drama Julie Gardner.
"Doctor Who" finally returned with the episode "Rose" on BBC One on 26 March 2005. There have since been eight further series in 2006–2008 and 2010–2014, and Christmas Day specials every year since 2005. No full series was filmed in 2009, although four additional specials starring Tennant were made. In 2010, Steven Moffat replaced Davies as head writer and executive producer.
The 2005 version of "Doctor Who" is a direct plot continuation of the original 1963–1989 series, as is the 1996 telefilm. This differs from other series relaunches that have either been reimaginings or reboots (for example, "Battlestar Galactica" and "Bionic Woman") or series taking place in the same universe as the original but in a different period and with different characters (for example, "" and spin-offs).
The programme has been sold to many other countries worldwide (see Viewership).
Public consciousness.
It has been suggested that the transmission of the first episode was delayed by ten minutes due to extended news coverage of the assassination of US President John F. Kennedy the previous day; whereas in fact, it went out after a delay of eighty seconds. Because it was believed that the coverage of the events of the assassination as well as a series of power blackouts across the country may have caused too many viewers to miss this introduction to a new series, the BBC broadcast it again on 30 November 1963, just before the broadcast of episode two.
The programme soon became a national institution in the United Kingdom, with a large following among the general viewing audience. Many renowned actors asked for, or were offered and accepted, guest-starring roles in various stories.
With popularity came controversy over the show's suitability for children. Morality campaigner Mary Whitehouse repeatedly complained to the BBC in the 1970s over what she saw as the show's frightening and gory content. John Nathan-Turner, who produced the series during the 1980s, was heard to say that he looked forward to Whitehouse's comments, as the show's ratings would increase soon after she had made them.
The phrase "Hiding behind (or 'watching from behind') the sofa" became coined and entered British pop culture, signifying in humour the stereotypical early-series behaviour of children who wanted to avoid seeing frightening parts of a television programme while remaining in the room to watch the remainder of it. The phrase retains this association with Doctor Who, to the point that in 1991 the Museum of the Moving Image in London named their exhibition celebrating the programme "Behind the Sofa". The electronic theme music too was perceived as eerie, novel, and frightening, at the time. A 2012 article placed this childhood juxtaposition of fear and thrill "at the center of many people's relationship with the show", and a 2011 online vote at Digital Spy deemed the series the "scariest TV show of all time".
During Jon Pertwee's second series as the Doctor, in the serial "Terror of the Autons" (1971), images of murderous plastic dolls, daffodils killing unsuspecting victims, and blank-featured policemen marked the apex of the show's ability to frighten children. Other notable moments in that decade include a disembodied brain falling to the floor in "The Brain of Morbius" and the Doctor apparently being drowned by Chancellor Goth in "The Deadly Assassin" (both 1976).
A BBC audience research survey conducted in 1972 found that, by their own definition of violence ("any act[s] which may cause physical and/or psychological injury, hurt or death to persons, animals or property, whether intentional or accidental") "Doctor Who" was the most violent of the drama programmes the corporation produced at the time. The same report found that 3% of the surveyed audience regarded the show as "very unsuitable" for family viewing. Responding to the findings of the survey in "The Times" newspaper, journalist Philip Howard maintained that, "to compare the violence of "Dr Who", sired by a horse-laugh out of a nightmare, with the more realistic violence of other television series, where actors who look like human beings bleed paint that looks like blood, is like comparing Monopoly with the property market in London: both are fantasies, but one is meant to be taken seriously."
The image of the TARDIS has become firmly linked to the show in the public's consciousness; BBC scriptwriter Anthony Coburn, who lived in the resort of Herne Bay, Kent, was one of the people who conceived the idea of a police box as a time machine. In 1996, the BBC applied for a trade mark to use the TARDIS' blue police box design in merchandising associated with "Doctor Who". In 1998, the Metropolitan Police Authority filed an objection to the trade mark claim; but in 2002, the Patent Office ruled in favour of the BBC.
The programme's broad appeal attracts audiences of children and families as well as science fiction fans.
The 21st century revival of the programme has become the centrepiece of BBC One's Saturday schedule, and has, "defined the channel". Since its return, "Doctor Who" has consistently received high ratings, both in number of viewers and as measured by the Appreciation Index. In 2007, Caitlin Moran, television reviewer for "The Times", wrote that "Doctor Who" is, "quintessential to being British". Director Steven Spielberg has commented that, "the world would be a poorer place without "Doctor Who"".
On 4 August 2013, a live programme titled "Doctor Who Live: The Next Doctor" was broadcast on BBC One, during which the actor playing the Twelfth Doctor was revealed. The show was simultaneously broadcast in the US and Australia.
Episodes.
"Doctor Who" originally ran for 26 seasons on BBC One, from 23 November 1963 until 6 December 1989. During the original run, each weekly episode formed part of a story (or "serial") — usually of four to six parts in earlier years and three to four in later years. Notable exceptions were: "The Daleks' Master Plan", which aired in 12 episodes (plus an earlier one-episode teaser, "Mission to the Unknown", featuring none of the regular cast); almost an entire season of seven-episode serials (season 7); the 10-episode serial "The War Games"; and "The Trial of a Time Lord", which ran for 14 episodes (albeit divided into three production codes and four narrative segments) during season 23. Occasionally serials were loosely connected by a storyline, such as season 8 being devoted to the Doctor battling a rogue Time Lord called The Master, season 16's quest for The Key to Time, season 18's journey through E-Space and the theme of entropy, and season 20's Black Guardian Trilogy.
The programme was intended to be educational and for family viewing on the early Saturday evening schedule. Initially, it alternated stories set in the past, which were intended to teach younger audience members about history, with stories set either in the future or in outer space to teach them about science. This was also reflected in the Doctor's original companions, one of whom was a science teacher and another a history teacher.
However, science fiction stories came to dominate the programme and the "historicals", which were not popular with the production team, were dropped after "The Highlanders" (1967). While the show continued to use historical settings, they were generally used as a backdrop for science fiction tales, with one exception: "Black Orchid" set in 1920s England.
The early stories were serial-like in nature, with the narrative of one story flowing into the next, and each episode having its own title, although produced as distinct stories with their own production codes. Following "The Gunfighters" (1966), however, each serial was given its own title, with the individual parts simply being assigned episode numbers.
Of the programme's many writers, Robert Holmes was the most prolific, while Douglas Adams became the most well-known outside "Doctor Who" itself, due to the popularity of his "Hitchhiker's Guide to the Galaxy".
The serial format changed for the 2005 revival, with each series usually consisting of 13 45-minute, self-contained episodes (60 minutes with adverts, on overseas commercial channels), and an extended episode broadcast on Christmas Day. Each series includes several standalone and multi-part stories, linked with a loose story arc that resolves in the series finale. As in the early "classic" era, each episode, whether standalone or part of a larger story, has its own title. Occasionally, regular-series episodes will exceed the 45-minute run time; notably, the episodes "Journey's End" from 2008 and "The Eleventh Hour" from 2010 exceeded an hour in length.
813 "Doctor Who" instalments have been televised since 1963, ranging between 25-minute episodes (the most common format), 45-minute episodes (for "Resurrection of the Daleks" in the 1984 series, a single season in 1985, and the revival), two feature-length productions (1983's "The Five Doctors" and the 1996 television film), eight Christmas specials (most of 60 minutes' duration, one of 72 minutes), and four additional specials ranging from 60 to 75 minutes in 2009, 2010 and 2013. Four mini-episodes, running about eight minutes each, were also produced for the 1993, 2005 and 2007 Children in Need charity appeals, while another mini-episode was produced in 2008 for a "Doctor Who"-themed edition of The Proms. The 1993 2-part story, entitled "Dimensions in Time", was made in collaboration with the cast of the BBC soap-opera "EastEnders" and was filmed partly on the "EastEnders" set. A two-part mini-episode was also produced for the 2011 edition of Comic Relief. Starting with the 2009 special "Planet of the Dead", the series was filmed in 1080i for HDTV, and broadcast simultaneously on BBC One and BBC HD.
To celebrate the 50th anniversary of the show, a special 3D episode, "The Day of the Doctor", was broadcast in 2013. In March 2013, it was announced that Tennant and Piper would be returning, and that the episode would have a limited cinematic release worldwide.
In April 2015, Steven Moffat confirmed that "Doctor Who" would run for at least another five years, extending the show until 2020.
Missing episodes.
Between about 1964 and 1973, large amounts of older material stored in the BBC's various video tape and film libraries were either destroyed, wiped, or suffered from poor storage which led to severe deterioration from broadcast quality. This included many old episodes of "Doctor Who", mostly stories featuring the first two Doctors: William Hartnell and Patrick Troughton. In all, 97 of 253 episodes produced during the first six years of the programme are not held in the BBC's archives (most notably seasons 3, 4, & 5, from which 79 episodes are missing). In 1972, almost all episodes then made were known to exist at the BBC, while by 1978 the practice of wiping tapes and destroying "spare" film copies had been brought to a stop.
No 1960s episodes exist on their original videotapes (all surviving prints being film transfers), though some were transferred to film for editing before transmission, and exist in their broadcast form.
Some episodes have been returned to the BBC from the archives of other countries who bought prints for broadcast, or by private individuals who acquired them by various means. Early colour videotape recordings made off-air by fans have also been retrieved, as well as excerpts filmed from the television screen onto 8 mm cine film and clips that were shown on other programmes. Audio versions of all of the lost episodes exist from home viewers who made tape recordings of the show. Short clips from every story with the exception of "Marco Polo", "Mission to the Unknown" and "The Massacre of St Bartholomew's Eve" also exist.
In addition to these, there are off-screen photographs made by photographer John Cura, who was hired by various production personnel to document many of their programmes during the 1950s and 1960s, including "Doctor Who". These have been used in fan reconstructions of the serials. These amateur reconstructions have been tolerated by the BBC, provided they are not sold for profit and are distributed as low-quality VHS copies.
One of the most sought-after lost episodes is part four of the last William Hartnell serial, "The Tenth Planet" (1966), which ends with the First Doctor transforming into the Second. The only portion of this in existence, barring a few poor-quality silent 8 mm clips, is the few seconds of the regeneration scene, as it was shown on the children's magazine show "Blue Peter". With the approval of the BBC, efforts are now under way to restore as many of the episodes as possible from the extant material.
"Official" reconstructions have also been released by the BBC on VHS, on MP3 CD-ROM, and as special features on DVD. The BBC, in conjunction with animation studio Cosgrove Hall, reconstructed the missing episodes 1 and 4 of "The Invasion" (1968), using remastered audio tracks and the comprehensive stage notes for the original filming, for the serial's DVD release in November 2006. The missing episodes of "The Reign of Terror" were animated by animation company Theta-Sigma, in collaboration with Big Finish, and became available for purchase in May 2013 through Amazon.com. Subsequent animations made in 2013 include "The Tenth Planet", "The Ice Warriors" and "The Moonbase".
In April 2006, "Blue Peter" launched a challenge to find missing Doctor Who episodes with the promise of a full-scale Dalek model as a reward.
In December 2011, it was announced that part 3 of "Galaxy 4" and part 2 of "The Underwater Menace" had been returned to the BBC by a fan who had purchased them in the mid-1980s without realising that the BBC did not hold copies of them.
On 10 October 2013, the BBC announced that films of eleven episodes, including nine missing episodes, had been found in a Nigerian television relay station in Jos. Six of the eleven films discovered were the six-part serial "The Enemy of the World", from which all but the third episode had been missing. The remaining films were from another six-part serial, "The Web of Fear", and included the previously missing episodes 2, 4, 5, and 6. Episode 3 of "The Web of Fear" is still missing.
Characters.
The Doctor.
The character of the Doctor was initially shrouded in mystery. All that was known about him in the programme's early days was that he was an eccentric alien traveller of great intelligence who battled injustice while exploring time and space in an unreliable time machine, the "TARDIS" (an acronym for time and relative dimension(s) in space), which notably appears much larger on the inside than on the outside (a quality referred to as "dimensional transcendentality").
The initially irascible and slightly sinister Doctor quickly mellowed into a more compassionate figure. It was eventually revealed that he had been on the run from his own people, the Time Lords of the planet Gallifrey.
Changes of appearance.
Producers introduced the concept of regeneration to permit the recasting of the main character. This was first prompted by original star William Hartnell's poor health. The actual term "regeneration" was not initially conceived of until the Doctor's third on-screen regeneration however; Hartnell's Doctor had merely described undergoing a "renewal," and the Second Doctor underwent a "change of appearance". The device has allowed for the recasting of the actor various times in the show's history, as well as the depiction of alternative Doctors either from the Doctor's relative past or future.
The serials "The Deadly Assassin" and "Mawdryn Undead" and the 1996 TV film would later establish that a Time Lord can only regenerate 12 times, for a total of 13 incarnations. This line became stuck in the public consciousness despite not often being repeated, and was recognised by producers of the show as a plot obstacle for when the show finally had to regenerate the Doctor a thirteenth time. The episode "The Time of the Doctor" depicted the Doctor acquiring a new cycle of regenerations, starting from the Twelfth Doctor, due to the Eleventh Doctor being the product of the Doctor's twelfth regeneration from his original set.
In addition to those actors who have headlined the series, others have portrayed versions of the Doctor in guest roles. Notably, in 2013, John Hurt guest-starred as a hitherto unknown incarnation of the Doctor known as the War Doctor in the run-up to the show's 50th anniversary special "The Day of the Doctor". He is shown in mini-episode "The Night of the Doctor" to have been retroactively inserted into the show's fictional chronology between McGann and Eccleston's Doctors, although his introduction was written so as not to disturb the established numerical naming of the Doctors. Another example is from the 1986 serial "The Trial of a Time Lord", where Michael Jayston portrayed the Valeyard, who is described as an amalgamation of the darker sides of the Doctor's nature, somewhere between his twelfth and final incarnation.
On rare occasions, other actors have stood in for the lead. In "The Five Doctors", Richard Hurndall played the First Doctor due to William Hartnell's death in 1975. In "Time and the Rani", Sylvester McCoy briefly played the Sixth Doctor during the regeneration sequence, carrying on as the Seventh. For more information, see the list of actors who have played the Doctor. In other media, the Doctor has been played by various other actors, including Peter Cushing in two films.
The casting of a new Doctor has often inspired debate and speculation: in particular, the desirability or possibility of a new Doctor being played by a woman. In October 2010, the "Sunday Telegraph" revealed that the series' co-creator, Sydney Newman, had urged the BBC to recast the role of the Doctor as a female "Time Lady" during the ratings crisis of the late 1980s.
Meetings of different incarnations.
There have been instances of actors returning at later dates to reprise the role of their specific Doctor. In 1973's "The Three Doctors", William Hartnell and Patrick Troughton returned alongside Jon Pertwee. For 1983's "The Five Doctors", Troughton and Pertwee returned to star with Peter Davison, and Tom Baker appeared in previously unseen footage from the uncompleted Shada episode. For this episode, Richard Hurndall replaced William Hartnell. Patrick Troughton again returned in 1985's "The Two Doctors" with Colin Baker. In 2007, Peter Davison returned in the Children in Need short "Time Crash" alongside David Tennant, and most recently in 2013's 50th anniversary special episode, "The Day of the Doctor", David Tennant's Tenth Doctor appeared alongside Matt Smith as the Eleventh Doctor and John Hurt as the War Doctor, as well as brief footage from all of the previous actors. In addition, the Doctor has occasionally encountered himself in the form of his own incarnation, from the near future or past. The First Doctor encounters himself in the story "The Space Museum" (albeit frozen and as an exhibit), the Third Doctor encounters and interacts with himself in the story "Day of the Daleks", the Fourth Doctor encounters and interacts with the future incarnation of himself (the 'Watcher') in the story "Logopolis", the Ninth Doctor observes a former version of his current incarnation in "Father's Day", and the Eleventh Doctor briefly comes face to face with himself in "The Big Bang". In "The Almost People" the Doctor comes face-to-face with himself although it is found out that this incarnation is in fact just a flesh replica. In "The Name of the Doctor", the Eleventh Doctor meets an unknown incarnation of himself, whom he refers to as "his secret" and who is subsequently revealed to be the War Doctor.
Additionally, multiple Doctors have returned in new adventures together in audio dramas based on the series. Peter Davison, Colin Baker and Sylvester McCoy appeared together in the 1999 audio adventure "The Sirens of Time". To celebrate the 40th anniversary in 2003, an audio drama titled "Zagreus" featuring Paul McGann, Colin Baker, Sylvester McCoy and Peter Davison was released with additional archive recordings of Jon Pertwee. Again in 2003, Colin Baker and Sylvester McCoy appeared together in the audio adventure "Project: Lazarus". In 2010, Peter Davison, Colin Baker, Sylvester McCoy and Paul McGann came together again to star in the audio drama "The Four Doctors".
Revelations about the Doctor.
Throughout the programme's long history, there have been revelations about the Doctor that have raised additional questions. In "The Brain of Morbius" (1976), it was hinted that the First Doctor may not have been the first incarnation (although the other faces depicted may have been incarnations of the Time Lord Morbius). In subsequent stories the First Doctor was depicted as the earliest incarnation of the Doctor. In "Mawdryn Undead" (1983), the Fifth Doctor explicitly confirmed that he was then currently in his fifth incarnation. Later that same year, during 1983's 20th Anniversary special "The Five Doctors", the First Doctor enquires as to the Fifth Doctor's regeneration; when the Fifth Doctor confirms "Fourth", the First Doctor excitedly replies "Goodness me. So there are five of me now." In 2010, the Eleventh Doctor similarly calls himself "the Eleventh" in "The Lodger". In the 2013 episode "The Time of the Doctor," the Eleventh Doctor clarified he was the product of the twelfth regeneration, due to a previous incarnation which he chose not to count and one other aborted regeneration. The name Eleventh is still used for this incarnation; the same episode depicts the prophesied "Fall of the Eleventh" which had been trailed throughout the series.
During the Seventh Doctor's era, it was hinted that the Doctor was more than just an ordinary Time Lord. In the 1996 television film, the Eighth Doctor describes himself as being "half human". The BBC's FAQ for the programme notes that "purists tend to disregard this", instead focusing on his Gallifreyan heritage.
The programme's first serial, "An Unearthly Child", shows that the Doctor has a granddaughter, Susan Foreman. In the 1967 serial, "Tomb of the Cybermen", when Victoria Waterfield doubts the Doctor can remember his family because of, "being so ancient", the Doctor says that he can when he really wants to—"The rest of the time they sleep in my mind". The 2005 series reveals that the Ninth Doctor thought he was the last surviving Time Lord, and that his home planet had been destroyed; in "The Empty Child" (2005), Dr. Constantine states that, "Before the war even began, I was a father and a grandfather. Now I am neither." The Doctor remarks in response, "Yeah, I know the feeling." In "Smith and Jones" (2007), when asked if he had a brother, he replied, "No, not any more." In both "Fear Her" (2006) and "The Doctor's Daughter" (2008), he states that he had, in the past, been a father.
In "The Wedding of River Song" (2011), it is implied that the Doctor's true name is a secret that must never be revealed; this is explored further in "The Name of the Doctor" (2013), when River Song speaking his name allows the Great Intelligence to enter his tomb, and in "The Time of the Doctor" (2013) where speaking his true name becomes the signal by which the Time Lords would know they can safely return to the universe, an event opposed by many species.
Companions.
The companion figure - generally a human - has been a constant feature in "Doctor Who" since the programme's inception in 1963. One of the roles of the companion is to remind the Doctor of his "moral duty". The Doctor's first companions seen on screen were his granddaughter Susan Foreman (Carole Ann Ford) and her teachers Barbara Wright (Jacqueline Hill) and Ian Chesterton (William Russell). These characters were intended to act as audience surrogates, through which the audience would discover information about the Doctor who was to act as a mysterious father figure. The only story from the original series in which the Doctor travels alone is "The Deadly Assassin". Notable companions from the earlier series included Romana (Mary Tamm and Lalla Ward), a Time Lady; Sarah Jane Smith (Elisabeth Sladen); and Jo Grant (Katy Manning). Dramatically, these characters provide a figure with whom the audience can identify, and serve to further the story by requesting exposition from the Doctor and manufacturing peril for the Doctor to resolve. The Doctor regularly gains new companions and loses old ones; sometimes they return home or find new causes — or loves — on worlds they have visited. Some have died during the course of the series. Companions are usually human, or humanoid aliens.
Since the 2005 revival, The Doctor generally travels with a primary female companion, who occupies a larger narrative role. Steven Moffat described the companion as the main character of the show, as the story begins anew with each companion and she undergoes more change than the Doctor. The primary companions of the Ninth and Tenth Doctors were Rose Tyler (Billie Piper), Martha Jones (Freema Agyeman), and Donna Noble (Catherine Tate) with Mickey Smith (Noel Clarke), Jackie Tyler (Camille Coduri) and Jack Harkness (John Barrowman) recurring as secondary companion figures. The Eleventh Doctor became the first to travel with a married couple (Amy Pond (Karen Gillan) and Rory Williams (Arthur Darvill)) whilst out-of-sync meetings with River Song (Alex Kingston) and Clara Oswald (Jenna Coleman) provided ongoing story arcs.
Some companions have gone on to re-appear either in the main series, or in spin-offs. Sarah Jane Smith became the central character in "The Sarah Jane Adventures" (2007-2011) following a return to "Doctor Who" in 2006. Guest stars in the series included former companions Jo Grant, K-9, and Brigadier Lethbridge-Stewart (Nicholas Courtney). The character of Jack Harkness also served to launch a spin-off, "Torchwood", (2006-2011) in which Martha Jones also appeared.
Adversaries.
When Sydney Newman commissioned the series, he specifically did not want to perpetuate the cliché of the "bug-eyed monster" of science fiction. However, monsters were popular with audiences and so became a staple of "Doctor Who" almost from the beginning.
With the show's 2005 revival, executive producer Russell T Davies stated his intention to reintroduce classic icons of "Doctor Who" one step at a time: the Autons with the Nestene Consciousness and Daleks in series 1, Cybermen in series 2, the Macra and the Master in series 3, the Sontarans and Davros in series 4, and the Time Lords (Rassilon) in the 2009–10 Specials. Davies' successor, Steven Moffat, has continued the trend by reviving the Silurians in series 5, Cybermats in series 6, the Great Intelligence and the Ice Warriors in Series 7, and Zygons in the 50th Anniversary Special. Since its 2005 return, the series has also introduced new recurring aliens: Slitheen (Raxacoricofallapatorian), Ood, Judoon, Weeping Angels and the Silence.
Besides infrequent appearances by the Ice Warriors, Ogrons, the Rani, and Black Guardian, three adversaries have become particularly iconic: the Daleks, the Cybermen, and the Master.
Daleks.
The Dalek race, which first appeared in the show's second serial in 1963, are "Doctor Who"‍ '​s oldest villains. The Daleks were Kaleds from the planet Skaro, mutated by the scientist Davros and housed in tank-like mechanical armour shells for mobility. The actual creatures resemble octopuses with large, pronounced brains. Their armour shells contain a single eye-stalk to allow them vision, a sink-plunger-like device that serves the purpose of a hand, and a directed-energy weapon. Their main weakness is their eyestalk; most attacks on them, including those from guns and baseball bats, will blind them, making them go mad. Their chief role in the plot of the series, as they frequently remark in their instantly recognisable metallic voices, is to "exterminate" all non-Dalek beings, even attacking the Time Lords in the Time War, which was not shown until the 50th Anniversary celebrating the show, where some snippets of the Time War are shown. The Daleks' most recent appearance was in the 2014 episode "Into the Dalek". They continue to be a recurring 'monster' within the Doctor Who franchise. Davros himself has also been a recurring figure since his debut in "Genesis of the Daleks", although played by several different actors.
The Daleks were created by writer Terry Nation (who intended them to be an allegory of the Nazis) and BBC designer Raymond Cusick. The Daleks' début in the programme's second serial, "The Daleks" (1963–64), made both the Daleks and "Doctor Who" very popular. A Dalek appeared on a postage stamp celebrating British popular culture in 1999, photographed by Lord Snowdon. In the new series, Daleks come in a range of colours; the colour of a Dalek denotes its role within the species. 
In the 2012 episode "Asylum of the Daleks", every generation of the Dalek species made an appearance.
Cybermen.
Cybermen were originally a wholly organic species of humanoids originating on Earth's twin planet Mondas that began to implant more and more artificial parts into their bodies. This led to the race becoming coldly logical and calculating cyborgs, with emotions usually only shown when naked aggression was called for. With the demise of Mondas, they acquired Telos as their new home planet. They continue to be a recurring 'monster' within the "Doctor Who" franchise.
The 2006 series introduced a totally new variation of Cybermen. These Cybus Cybermen were created in a parallel universe by the mad inventor John Lumic; he was attempting to preserve the life of a human by transplanting their brains into powerful metal bodies, sending them orders using a mobile phone network and inhibiting their emotions with an electronic chip.
The Master.
The Master is the Doctor's archenemy, a renegade Time Lord who desires to rule the universe. Conceived as "Professor Moriarty to the Doctor's Sherlock Holmes", the character first appeared in 1971. As with the Doctor, the role has been portrayed by several actors, since the Master is a Time Lord as well and able to regenerate; the first of these actors was Roger Delgado, who continued in the role until his death in 1973. The Master was briefly played by Peter Pratt and Geoffrey Beevers until Anthony Ainley took over and continued to play the character until Doctor Who's hiatus in 1989. The Master returned in the 1996 television movie of "Doctor Who", and was played by American actor Eric Roberts.
Following the series revival in 2005, Derek Jacobi provided the character's re-introduction in the 2007 episode "Utopia". During that story the role was then assumed by John Simm who returned to the role multiple times through the Tenth Doctor's tenure.
As of the 2014 episode "Dark Water," it was revealed that the Master had become a female incarnation or "Time Lady," going by the name of "Missy" (short for Mistress, the feminine equivalent of "Master"). This incarnation is played by Michelle Gomez.
Music.
Theme music.
The "Doctor Who" theme music was one of the first electronic music signature tunes for television, and after five decades remains one of the most easily recognised. It has been often called both memorable and frightening, priming the viewer for what was to follow. During the 1970s, the "Radio Times", the BBC's own listings magazine, announced that a child's mother said the theme music terrified her son. The "Radio Times" was apologetic, but the theme music remained.
The original theme was composed by Ron Grainer and realised by Delia Derbyshire of the BBC Radiophonic Workshop, with assistance from Dick Mills. The various parts were built up using musique concrète techniques, by creating tape loops of an individually struck piano string and individual test oscillators and filters. The Derbyshire arrangement served, with minor edits, as the theme tune up to the end of season 17 (1979–80). It is regarded as a significant and innovative piece of electronic music, recorded well before the availability of commercial synthesisers or multitrack mixers. Each note was individually created by cutting, splicing, speeding up and slowing down segments of analogue tape containing recordings of a single plucked string, white noise, and the simple harmonic waveforms of test-tone oscillators, intended for calibrating equipment and rooms, not creating music. New techniques were invented to allow mixing of the music, as this was before the era of multitrack tape machines. On hearing the finished result, Grainer asked, "Did I write that?"
A different arrangement was recorded by Peter Howell for season 18 (1980), which was in turn replaced by Dominic Glynn's arrangement for the season-long serial "The Trial of a Time Lord" in season 23 (1986). Keff McCulloch provided the new arrangement for the Seventh Doctor's era which lasted from season 24 (1987) until the series' suspension in 1989. American composer John Debney created a new arrangement of Ron Grainer's original theme for "Doctor Who" in 1996. For the return of the series in 2005, Murray Gold provided a new arrangement which featured samples from the 1963 original with further elements added; in the 2005 Christmas episode "The Christmas Invasion", Gold introduced a modified closing credits arrangement that was used up until the conclusion of the 2007 series.
A new arrangement of the theme, once again by Gold, was introduced in the 2007 Christmas special episode, "Voyage of the Damned"; Gold returned as composer for the 2010 season. He was responsible for a new version of the theme which was reported to have had a hostile reception from some viewers. In 2011, the theme tune charted at number 228 of radio station Classic FM's Hall of Fame, a survey of classical music tastes. A revised version of Gold's 2010 arrangement had its debut over the opening titles of the 2012 Christmas special "The Snowmen", and a further revision of the arrangement was made for the 50th Anniversary special "The Day of the Doctor" in November 2013.
Versions of the "Doctor Who Theme" have also been released as pop music over the years. In the early 1970s, Jon Pertwee, who had played the Third Doctor, recorded a version of the Doctor Who theme with spoken lyrics, titled, "Who Is the Doctor". In 1978 a disco version of the theme was released in the UK, Denmark and Australia by the group Mankind, which reached number 24 in the UK charts. In 1988 the band The Justified Ancients of Mu Mu (later known as The KLF) released the single "Doctorin' the Tardis" under the name The Timelords, which reached No. 1 in the UK and No. 2 in Australia; this version incorporated several other songs, including "Rock and Roll Part 2" by Gary Glitter (who recorded vocals for some of the CD-single remix versions of "Doctorin' the Tardis"). Others who have covered or reinterpreted the theme include Orbital, Pink Floyd, the Australian string ensemble Fourplay, New Zealand punk band Blam Blam Blam, The Pogues, Thin Lizzy, Dub Syndicate, and the comedians Bill Bailey and Mitch Benn. Both the theme and obsessive fans were satirised on "The Chaser's War on Everything". The theme tune has also appeared on many compilation CDs, and has made its way into mobile-phone ringtones. Fans have also produced and distributed their own remixes of the theme. In January 2011 the Mankind version was released as a digital download on the album "Gallifrey And Beyond".
Incidental music.
Most of the innovative incidental music for "Doctor Who" has been specially commissioned from freelance composers, although in the early years some episodes also used stock music, as well as occasional excerpts from original recordings or cover versions of songs by popular music acts such as The Beatles and The Beach Boys. Since its 2005 return, the series has featured occasional use of excerpts of pop music from the 1970s to the 2000s.
The incidental music for the first "Doctor Who" adventure, "An Unearthly Child", was written by Norman Kay. Many of the stories of the William Hartnell period were scored by electronic music pioneer Tristram Cary, whose "Doctor Who" credits include "The Daleks", "Marco Polo", "The Daleks' Master Plan", "The Gunfighters" and "The Mutants". Other composers in this early period included Richard Rodney Bennett, Carey Blyton and Geoffrey Burgon.
The most frequent musical contributor during the first 15 years was Dudley Simpson, who is also well known for his theme and incidental music for "Blake's 7", and for his haunting theme music and score for the original 1970s version of "The Tomorrow People". Simpson's first "Doctor Who" score was "Planet of Giants" (1964) and he went on to write music for many adventures of the 1960s and 1970s, including most of the stories of the Jon Pertwee/Tom Baker periods, ending with "The Horns of Nimon" (1979). He also made a cameo appearance in "The Talons of Weng-Chiang" (as a Music hall conductor).
In 1980 starting with the serial "The Leisure Hive" the task of creating incidental music was assigned to the Radiophonic Workshop. Paddy Kingsland and Peter Howell contributed many scores in this period and other contributors included Roger Limb, Malcolm Clarke and Jonathan Gibbs.
The Radiophonic Workshop was dropped after 1986's "The Trial of a Time Lord" series, and Keff McCulloch took over as the series' main composer until the end of its run, with Dominic Glynn and Mark Ayres also contributing scores.
All the incidental music for the 2005 revived series has been composed by Murray Gold and Ben Foster and has been performed by the BBC National Orchestra of Wales from the 2005 Christmas episode "The Christmas Invasion" onwards. A concert featuring the orchestra performing music from the first two series took place on 19 November 2006 to raise money for Children in Need. David Tennant hosted the event, introducing the different sections of the concert. Murray Gold and Russell T Davies answered questions during the interval and Daleks and Cybermen appeared whilst music from their stories was played. The concert aired on BBCi on Christmas Day 2006. A Doctor Who Prom was celebrated on 27 July 2008 in the Royal Albert Hall as part of the annual BBC Proms. The BBC Philharmonic and the London Philharmonic Choir performed Murray Gold's compositions for the series, conducted by Ben Foster, as well as a selection of classics based on the theme of space and time. The event was presented by Freema Agyeman and guest-presented by various other stars of the show with numerous monsters participating in the proceedings. It also featured the specially filmed mini-episode "Music of the Spheres", written by Russell T Davies and starring David Tennant.
Six soundtrack releases have been released since 2005. The featured tracks from the first two series, the and featured music from the third and fourth series respectively. The was released on 4 October 2010 as a two disc special edition and contained music from the 2008–2010 specials ("The Next Doctor" to "End of Time Part 2"). The was released on 8 November 2010. In February 2011, a soundtrack was released for the 2010 Christmas special: "A Christmas Carol", and in December 2011 the was released, both by Silva Screen Records.
Logo history.
Below is a collection of current and past "Doctor Who" logos from the classic and current series. The different doctors have been named below the logos that have appeared during their tenure (e.g. The diamond logo from 1973-1980 was used for the Third Doctor in his final season (5 serials). However, he is more commonly associated with the 1970-1973 logo used for four seasons (7-10).
The original logo used for the First Doctor (and briefly for the Second Doctor) was reused in a slightly modified format for the 50th anniversary special "The Day of the Doctor" during the Eleventh Doctor's run. The logo used in the television movie featuring the Eighth Doctor was an edited version of the logo used for the Third Doctor. The logo from 1973–80 was used for the Third Doctor's final season and for the majority of the Fourth Doctor's tenure. The following logo, while most associated with the Fifth Doctor, was also used for the Fourth Doctor's final season. The logo used for the Ninth Doctor was slightly edited for the Tenth Doctor, but it retained the same general appearance. The logo used for the Eleventh Doctor had the "DW" TARDIS portion removed in 2012, but the same font remained (hence the date for until 2014), and the font was later altered for the Twelfth Doctor. As of 2014, the logo used for the Third and Eighth Doctors is the primary logo used on all media and merchandise relating to past Doctors, and the current "Doctor Who" logo is used for all merchandise relating to the current Doctor.
Viewership.
United Kingdom.
Premiering the day after the assassination of John F. Kennedy, the first episode of "Doctor Who" was repeated with the second episode the following week. "Doctor Who" has always appeared initially on the BBC's mainstream BBC One channel, where it is regarded as a family show, drawing audiences of many millions of viewers; episodes are now repeated on BBC Three. The programme's popularity has waxed and waned over the decades, with three notable periods of high ratings. The first of these was the "Dalekmania" period (circa 1964–1965), when the popularity of the Daleks regularly brought "Doctor Who" ratings of between 9 and 14 million, even for stories which did not feature them. The second was the late 1970s, when Tom Baker occasionally drew audiences of over 12 million.
During the ITV network strike of 1979, viewership peaked at 16 million. Figures remained respectable into the 1980s, but fell noticeably after the programme's 23rd series was postponed in 1985 and the show was off the air for 18 months. Its late 1980s performance of three to five million viewers was seen as poor at the time and was, according to the BBC Board of Control, a leading cause of the programme's 1989 suspension. Some fans considered this disingenuous, since the programme was scheduled against the soap opera "Coronation Street", the most popular show at the time. After the series' revival in 2005 (the third notable period of high ratings), it has consistently had high viewership levels for the evening on which the episode is broadcast.
The BBC One broadcast of "Rose", the first episode of the 2005 revival, drew an average audience of 10.81 million, third highest for BBC One that week and seventh across all channels. The current revival also garners the highest audience Appreciation Index of any drama on television.
International.
"Doctor Who" has been broadcast internationally outside of the United Kingdom since 1964, a year after the show first aired. As of 1 January 2013, the modern series has been or is currently broadcast weekly in more than 50 countries.
"Doctor Who" is one of the five top grossing titles for BBC Worldwide, the BBC's commercial arm. BBC Worldwide CEO John Smith has said that "Doctor Who" is one of a small number of "Superbrands" which Worldwide will promote heavily.
Only four episodes have ever had their premiere showings on channels other than BBC One. The 1983 20th anniversary special "The Five Doctors" had its début on 23 November (the actual date of the anniversary) on a number of PBS stations two days prior to its BBC One broadcast. The 1988 story "Silver Nemesis" was broadcast with all three episodes airing back to back on TVNZ in New Zealand in November, after the first episode had been shown in the UK but before the final two instalments had aired there. Finally, the 1996 television film premièred on 12 May 1996 on CITV in Edmonton, Canada, 15 days before the BBC One showing, and two days before it aired on Fox in the United States.
Oceania.
New Zealand was the first country outside the United Kingdom to screen "Doctor Who", beginning in September 1964, and continued to screen the series for many years, including the new series from 2005.
In Australia, the show has had a strong fan base since its inception, having been exclusively first run by the Australian Broadcasting Corporation (ABC) since January 1965. The ABC has periodically repeated episodes; of note were the weekly screenings of all available classic episodes starting in 2003, for the show's 40th anniversary. The ABC broadcasts the modern series first run on ABC1, with repeats on ABC2. The ABC also provided partial funding for the 20th anniversary special "The Five Doctors" in 1983. Repeats of both the classic and modern series have also been shown on subscription television channels BBC UKTV, SF and later on SyFy upon SF's closure.
North America.
The series also has a fan base in the United States, where it was shown in syndication from the 1970s to the 1990s, particularly on PBS stations.
TVOntario picked up the show in 1976 beginning with "The Three Doctors" and aired each series (several years late) through to series 24 in 1991. From 1979 to 1981, TVO airings were bookended by science-fiction writer Judith Merril who would introduce the episode and then, after the episode concluded, try to place it in an educational context in keeping with TVO's status as an educational channel. Its airing of "The Talons of Weng-Chiang" was cancelled as a result of accusations that the story was racist; the story was later broadcast in the 1990s on cable station YTV. CBC began showing the series again in 2005. The series moved to the Canadian cable channel Space in 2009.
For the Canadian broadcast, Christopher Eccleston recorded special video introductions for each episode (including a trivia question as part of a viewer contest) and excerpts from the "Doctor Who Confidential" documentary were played over the closing credits; for the broadcast of "The Christmas Invasion" on 26 December 2005, Billie Piper recorded a special video introduction. CBC began airing series two on 9 October 2006 at 20:00 E/P (20:30 in Newfoundland and Labrador), shortly after that day's CFL double header on Thanksgiving in most of the country.
Series three began broadcasting on CBC on 18 June 2007 followed by the second Christmas special, "The Runaway Bride" at midnight, and the Sci Fi Channel began on 6 July 2007 starting with the second Christmas special at 8:00 pm E/P followed by the first episode.
Series four aired in the United States on the Sci Fi Channel (now known as Syfy), beginning in April 2008. It aired on CBC beginning 19 September 2008, although the CBC did not air the "Voyage of the Damned" special. The Canadian cable network Space broadcast "The Next Doctor" (in March 2009) and all subsequent series and specials.
Other countries.
In Latin America, the original series — known as "Doctor Misterio" – was shown in Venezuela from 1967; Mexico (Canal 13) from 1968, then later syndicated from 1979; and Chile from 1969.
A special logo has been designed for the Japanese broadcast with the katakana "ドクター・フー" (romanised as "Dokutā Fū"). The series has apparently "mystified" viewers in Japan where it has been broadcast in a late evening time slot, leading to some not realising it is a family show.
DVD and video.
A wide selection of serials are available from BBC Video on DVD, on sale in the United Kingdom, Australia, Canada and the United States. Every fully extant serial has been released on VHS, and BBC Worldwide continues to regularly release serials on DVD. The 2005 series is also available in its entirety on UMD for the PlayStation Portable. Eight original series serials have been released on Laserdisc and many have also been released on Betamax tape and Video 2000. One episode of Doctor Who The Infinite Quest was released on VCD. So far only the new series from 2009 onwards are available on Blu-ray. The 1970 classic series story "Spearhead from Space" was released on Blu-ray in July 2013. Many early releases have been re-released on special edition with more bonus features.
Adaptations and other appearances.
"Doctor Who" films.
There are two "Doctor Who" feature films: "Dr. Who and the Daleks", released in 1965 and "" in 1966. Both are retellings of existing television stories (specifically, the first two Dalek serials, "The Daleks" and "The Dalek Invasion of Earth" respectively) with a larger budget and alterations to the series concept.
In these films, Peter Cushing plays a human scientist named "Dr. Who", who travels with his granddaughter and niece and other companions in a time machine he has invented. The Cushing version of the character reappears in both comic strips and a short story, the latter attempting to reconcile the film continuity with that of the series.
In addition, several planned films were proposed, including a sequel, "The Chase", loosely based on the original series story, for the Cushing Doctor, plus many attempted television movie and big screen productions to revive the original "Doctor Who", after the original series was cancelled.
Paul McGann starred in the only television film as the eighth incarnation of the Doctor. After the film, he continued the role in audio books and was confirmed as the eighth incarnation through flashback footage and a mini episode in the 2005 revival, effectively linking the two series and the television movie.
In 2011, David Yates announced that he had started work with the BBC on a "Doctor Who" film, a project that would take three or more years to complete. Yates indicated that the film would take a different approach to "Doctor Who", although the current "Doctor Who" showrunner Steven Moffat stated later that any such film would not be a reboot of the series and a film should be made by the BBC team and star the current TV Doctor.
Spin-offs.
"Doctor Who" has appeared on stage numerous times. In the early 1970s, Trevor Martin played the role in "Doctor Who and the Daleks in the Seven Keys to Doomsday". In the late 1980s, Jon Pertwee and Colin Baker both played the Doctor at different times during the run of a play titled "Doctor Who – The Ultimate Adventure". For two performances, while Pertwee was ill, David Banks (better known for playing Cybermen) played the Doctor. Other original plays have been staged as amateur productions, with other actors playing the Doctor, while Terry Nation wrote "The Curse of the Daleks", a stage play mounted in the late 1960s, but without the Doctor.
A pilot episode ("A Girl's Best Friend") for a potential spinoff series, "K-9 and Company", was aired in 1981 with Elisabeth Sladen reprising her role as companion Sarah Jane Smith and John Leeson as the voice of K-9, but was not picked up as a regular series.
Concept art for an animated "Doctor Who" series was produced by animation company Nelvana in the 1980s, but the series was not produced.
Following the success of the 2005 series produced by Russell T Davies, the BBC commissioned Davies to produce a 13-part spin-off series titled "Torchwood" (an anagram of "Doctor Who"), set in modern-day Cardiff and investigating alien activities and crime. The series debuted on BBC Three on 22 October 2006. John Barrowman reprised his role of Jack Harkness from the 2005 series of "Doctor Who". Two other actresses who appeared in Doctor Who also star in the series; Eve Myles as Gwen Cooper, who also played the similarly named servant girl Gwyneth in the 2005 "Doctor Who" episode "The Unquiet Dead", and Naoko Mori who reprised her role as Toshiko Sato first seen in "Aliens of London". A second series of "Torchwood" aired in 2008; for three episodes, the cast was joined by Freema Agyeman reprising her "Doctor Who" role of Martha Jones. A third series was broadcast from 6 to 10 July 2009, and consisted of a single five-part story called "Children of Earth" which was set largely in London. A fourth series, "" jointly produced by BBC Wales, BBC Worldwide and the American entertainment company Starz debuted in 2011. The series was predominantly set in the United States, though Wales remained part of the show's setting.
"The Sarah Jane Adventures", starring Elisabeth Sladen who reprised her role as investigative journalist Sarah Jane Smith, was developed by CBBC; a special aired on New Year's Day 2007 and a full series began on 24 September 2007. A second series followed in 2008, notable for (as noted above) featuring the return of Brigadier Lethbridge-Stewart. A third in 2009 featured a crossover appearance from the main show by David Tennant as the Tenth Doctor. In 2010, a further such appearance featured Matt Smith as the Eleventh Doctor alongside former companion actress Katy Manning reprising her role as Jo Grant. A final, three-story fifth series was transmitted in autumn 2011 – uncompleted due to the death of Elisabeth Sladen in early 2011.
An animated serial, "The Infinite Quest", aired alongside the 2007 series of "Doctor Who" as part of the children's television series "Totally Doctor Who". The serial featured the voices of series regulars David Tennant and Freema Agyeman but is not considered part of the 2007 series. A second animated serial, "Dreamland", aired in six parts on the BBC Red Button service, and the official "Doctor Who" website in 2009.
Numerous other spin-off series have been created not by the BBC but by the respective owners of the characters and concepts. Such spin-offs include the novel and audio drama series "Faction Paradox", "Iris Wildthyme" and "Bernice Summerfield"; as well as the made-for-video series "P.R.O.B.E."; the Australian-produced television series "K-9", which aired a 26-episode first season on Disney XD; and the audio spin-off "Counter-Measures".
Charity episodes.
In 1983, coinciding with the series' 20th anniversary, a charity special titled "The Five Doctors" was produced in aid of Children in Need, featuring three of the first five Doctors, a new actor to replace the deceased William Hartnell, and unused footage to represent Tom Baker. This was a full-length, 90-minute film, the longest single episode of "Doctor Who" produced to date (the television movie ran slightly longer on broadcast where it included commercial breaks).
In 1993, for the franchise's 30th anniversary, another charity special, titled "Dimensions in Time" was produced for Children in Need, featuring all of the surviving actors who played the Doctor and a number of previous companions. It also featured a crossover with the soap opera "EastEnders", the action taking place in the latter's Albert Square location and around Greenwich. The special was one of several special 3D programmes the BBC produced at the time, using a 3D system that made use of the Pulfrich effect requiring glasses with one darkened lens; the picture would look normal to those viewers who watched without the glasses.
In 1999, another special, "Doctor Who and the Curse of Fatal Death", was made for Comic Relief and later released on VHS. An affectionate parody of the television series, it was split into four segments, mimicking the traditional serial format, complete with cliffhangers, and running down the same corridor several times when being chased (the version released on video was split into only two episodes). In the story, the Doctor (Rowan Atkinson) encounters both the Master (Jonathan Pryce) and the Daleks. During the special the Doctor is forced to regenerate several times, with his subsequent incarnations played by, in order, Richard E. Grant, Jim Broadbent, Hugh Grant and Joanna Lumley. The script was written by Steven Moffat, later to be head writer and executive producer to the revived series.
Since the return of "Doctor Who" in 2005, the franchise has produced two original "mini-episodes" to support Children in Need. The first, aired in November 2005, was an which introduced David Tennant as the Tenth Doctor. It was followed in November 2007 by "Time Crash", a 7-minute scene which featured the Tenth Doctor meeting the Fifth Doctor (played once again by Peter Davison).
A set of two mini-episodes, titled "Space" and "Time" respectively, were produced to support Comic Relief. They were aired during the Comic Relief 2011 event.
During 2011 Children in Need, an exclusively-filmed segment showed the Doctor addressing the viewer, attempting to persuade them to purchase items of his clothing, which were going up for auction for Children in Need. The 2012 edition of CiN featured the mini-episode "The Great Detective".
Spoofs and cultural references.
"Doctor Who" has been satirised and spoofed on many occasions by comedians including Spike Milligan (a Dalek invades his bathroom — Milligan, naked, hurls a soap sponge at it) and Lenny Henry. Jon Culshaw frequently impersonates the Fourth Doctor in the BBC "Dead Ringers" series. "Doctor Who" fandom has also been lampooned on programs such as "Saturday Night Live", "The Chaser's War on Everything", "Mystery Science Theater 3000", "Family Guy", "American Dad!", "Futurama", "South Park",
"Community" as Inspector Spacetime, "The Simpsons" and "The Big Bang Theory".
The Doctor in his fourth incarnation has been represented on several episodes of "The Simpsons" and Matt Groening's other animated series "Futurama".
There have also been many references to "Doctor Who" in popular culture and other science fiction, including " (") and "Leverage". In the Channel 4 series "Queer as Folk" (created by later "Doctor Who" executive producer Russell T Davies), the character of Vince was portrayed as an avid "Doctor Who" fan, with references appearing many times throughout in the form of clips from the programme. In a similar manner, the character of Oliver on "Coupling" (created and written by current show runner Steven Moffat) is portrayed as a "Doctor Who" collector and enthusiast.
References to "Doctor Who" have also appeared in the young adult fantasy novels "Brisingr" and "High Wizardry", the video game "Rock Band", the soap opera "EastEnders", the Adult Swim comedy show "Robot Chicken", the "Family Guy" episodes "Blue Harvest" and "420", and the game RuneScape.
"Doctor Who" has been a reference in several political cartoons, from a 1964 cartoon in the "Daily Mail" depicting Charles de Gaulle as a Dalek to a 2008 edition of "This Modern World" by Tom Tomorrow in which the Tenth Doctor informs an incredulous character from 2003 that the Democratic Party will nominate an African-American as its presidential candidate.
The word "TARDIS" is an entry in the Shorter Oxford English Dictionary and the iOS dictionary.
As part of the 50th anniversary programmes, former Fifth Doctor Peter Davison created, wrote and co-starred in a parody "The Five(ish) Doctors Reboot" featuring cameos from several other former Doctors, companions and people involved in the programme.
Museums and exhibitions.
There have been various exhibitions of "Doctor Who" in the United Kingdom, including the now closed exhibitions at:
There is an exhibition open currently in Cardiff (the city where the series is filmed)
Merchandise.
Since its beginnings, "Doctor Who" has generated hundreds of products related to the show, from toys and games to collectible picture cards and postage stamps. These include board games, card games, gamebooks, computer games, roleplaying games, action figures and a pinball game. Many games have been released that feature the Daleks, including Dalek computer games.
Audios.
The Doctor has also appeared in webcasts and in audio plays; among the latter were those produced by Big Finish Productions, who were responsible for a range of audio plays released on CD, as well as 2006's eight-part BBC 7 series starring Paul McGann. Big Finish's productions began with the release of "The Sirens of Time" in July 1999. These audios feature Doctors 4–8. As well as this, Big Finish also release a range of other audio books read by both Doctors and Companions.
Books.
"Doctor Who" books have been published from the mid-sixties through to the present day. From 1965 to 1991 the books published were primarily novelised adaptations of broadcast episodes; beginning in 1991 an extensive line of original fiction was launched, the Virgin New Adventures and Virgin Missing Adventures. Since the relaunch of the programme in 2005, a new range of novels have been published by BBC Books. Numerous non-fiction books about the series, including guidebooks and critical studies, have also been published, and a dedicated "Doctor Who Magazine" with newsstand circulation has been published regularly since 1979. There is also a "Doctor Who Adventures" magazine published by the BBC.
See also:
Video games.
Numerous "Doctor Who" video games have been created from the mid-80s through to the present day. One of the recent ones is a match-3 game released in November 2013 for iOS and Android, called "Doctor Who: Legacy". It has been continually updated from its release to February 2015 and features all of the Doctors as playable characters.
Another video game installment is LEGO Dimensions - in which Doctor Who is one of the many "Level Packs" in the game. At the moment, the pack contains the Twelfth Doctor, K9, the TARDIS and a Victorian London adventure level area. The game and pack are due later in 2015. 
Chronology and canonicity.
Since the creation of the "Doctor Who" character by BBC Television in the early 1960s, a myriad of stories have been published about "Doctor Who", in different media: apart from the actual television episodes that continue to be produced by the BBC, there have also been novels, comics, short stories, audio books, radio plays, interactive video games, game books, webcasts, DVD extras, and even stage performances. In this respect it is noteworthy that the BBC takes no position on the canonicity of any of such stories, and recent producers of the show have expressed distaste for the idea.
Awards.
The show has received recognition as one of Britain's finest television programmes, winning the 2006 British Academy Television Award for Best Drama Series and five consecutive (2005–2010) awards at the National Television Awards during Russell T Davies' tenure as executive producer. In 2011, Matt Smith became the first Doctor to be nominated for a BAFTA Television Award for Best Actor. In 2013, the Peabody Awards honoured "Doctor Who" with an Institutional Peabody "for evolving with technology and the times like nothing else in the known television universe." The programme is listed in "Guinness World Records" as the longest-running science fiction television show in the world, the "most successful" science fiction series of all time—based on its over-all broadcast ratings, DVD and book sales, and iTunes traffic— and for the largest ever simulcast of a TV drama with its 50th anniversary special. During its original run, it was recognised for its imaginative stories, creative low-budget special effects, and pioneering use of electronic music (originally produced by the BBC Radiophonic Workshop).
In 1975, Season 11 of the series won a Writers' Guild of Great Britain award for Best Writing in a Children's Serial. In 1996, BBC television held the "Auntie Awards" as the culmination of their "TV60" series, celebrating 60 years of BBC television broadcasting, where "Doctor Who" was voted as the "Best Popular Drama" the corporation had ever produced, ahead of such ratings heavyweights as "EastEnders" and "Casualty". In 2000, "Doctor Who" was ranked third in a list of the 100 Greatest British Television Programmes of the 20th century, produced by the British Film Institute and voted on by industry professionals. In 2005, the series came first in a survey by SFX magazine of "The Greatest UK Science Fiction and Fantasy Television Series Ever". Also, in the 100 Greatest Kids' TV shows (a Channel 4 countdown in 2001), the 1963–1989 run was placed at number eight.
The revived series has received recognition from critics and the public, across various awards ceremonies. It won five BAFTA TV Awards, including Best Drama Series, the highest-profile and most prestigious British television award for which the series has ever been nominated. It was very popular at the BAFTA Cymru Awards, with 25 wins overall including Best Drama Series (twice), Best Screenplay/Screenwriter (thrice) and Best Actor. It was also nominated for 7 Saturn Awards, winning the only Best International Series in the ceremony's history. In 2009, "Doctor Who" was voted the 3rd greatest show of the 2000s by Channel 4, behind "Top Gear" and "The Apprentice". The episode "Vincent and the Doctor" was shortlisted for a Mind Award at the 2010 Mind Mental Health Media Awards for its "touching" portrayal of Vincent van Gogh.
It has won the Short Form of the Hugo Award for Best Dramatic Presentation, the oldest science fiction/fantasy award for films and series, six times (every year since 2006, except for 2009, 2013 and 2014). The winning episodes were "The Empty Child"/"The Doctor Dances" (2006), "The Girl in the Fireplace" (2007), "Blink" (2008), "The Waters of Mars" (2010), "The Pandorica Opens"/"The Big Bang" (2011), and "The Doctor's Wife" (2012). Doctor Who star Matt Smith won Best Actor in the 2012 National Television awards alongside Karen Gillan who won Best Actress. "Doctor Who" has been nominated for over 200 awards and has won over a hundred of them.
As a British series, the majority of its nominations and awards have been for national competitions such as the BAFTAs, but it has occasionally received nominations in mainstream American awards, most notably a nomination for "Favorite Sci-Fi Show" in the 2008 People's Choice Awards and the series has been nominated multiple times in the Spike Scream Awards, with Smith winning Best Science Fiction Actor in 2011. The Canadian Constellation Awards have also recognised the series.
References.
Cited texts.
</dl>
External links.
Listen to this article ()
This audio file was created from a revision of the "Doctor Who" article dated 2011-01-02, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="8211" url="http://en.wikipedia.org/wiki?curid=8211" title="Democritus">
Democritus

Democritus (; Greek: Δημόκριτος "Dēmókritos", meaning "chosen of the people"; c. 460 – c. 370 BC) was an influential Ancient Greek pre-Socratic philosopher primarily remembered today for his formulation of an atomic theory of the universe.
Democritus was born in Abdera, Thrace around 460 BC. His exact contributions are difficult to disentangle from those of his mentor Leucippus, as they are often mentioned together in texts. Their speculation on atoms, taken from Leucippus, bears a passing and partial resemblance to the nineteenth-century understanding of atomic structure that has led some to regard Democritus as more of a scientist than other Greek philosophers; however, their ideas rested on very different bases. Largely ignored in ancient Athens, Democritus was nevertheless well known to his fellow northern-born philosopher Aristotle. Plato is said to have disliked him so much that he wished all his books burned. Many consider Democritus to be the "father of modern science".
Life.
Democritus was born in the city of Abdera in Thrace, an Ionian colony of Teos, although some called him a Milesian. He was born in the 80th Olympiad (460–457 BC) according to Apollodorus of Athens, and although Thrasyllus placed his birth in 470 BC, the later date is probably more likely. John Burnet has argued that the date of 460 is "too early" since, according to Diogenes Laërtius ix.41, Democritus said that he was a "young man ("neos")" during Anaxagoras' old age (circa 440–428). It was said that Democritus' father was so wealthy that he received Xerxes on his march through Abdera. Democritus spent the inheritance which his father left him on travels into distant countries, to satisfy his thirst for knowledge. He traveled to Asia, and was even said to have reached India and Ethiopia.
It is known that he wrote on Babylon and Meroe; he visited Egypt, and Diodorus Siculus states that he lived there for five years. He himself declared that among his contemporaries none had made greater journeys, seen more countries, and met more scholars than himself. He particularly mentions the Egyptian mathematicians, whose knowledge he praises. Theophrastus, too, spoke of him as a man who had seen many countries. During his travels, according to Diogenes Laërtius, he became acquainted with the Chaldean magi. "Ostanes", one of the magi accompanying Xerxes, was also said to have taught him.
After returning to his native land he occupied himself with natural philosophy. He traveled throughout Greece to acquire a better knowledge of its cultures. He mentions many Greek philosophers in his writings, and his wealth enabled him to purchase their writings. Leucippus, the founder of atomism, was the greatest influence upon him. He also praises Anaxagoras. Diogenes Laertius says that he was friends with Hippocrates. He may have been acquainted with Socrates, but Plato does not mention him and Democritus himself is quoted as saying, "I came to Athens and no one knew me." Aristotle placed him among the pre-Socratic natural philosophers.
The many anecdotes about Democritus, especially in Diogenes Laërtius, attest to his disinterest, modesty, and simplicity, and show that he lived exclusively for his studies. One story has him deliberately blinding himself in order to be less disturbed in his pursuits; it may well be true that he lost his sight in old age. He was cheerful, and was always ready to see the comical side of life, which later writers took to mean that he always laughed at the foolishness of people.
He was highly esteemed by his fellow-citizens, "because," as Diogenes Laërtius says, "he had foretold them some things which events proved to be true," which may refer to his knowledge of natural phenomena. According to Diodorus Siculus, Democritus died at the age of 90, which would put his death around 370 BC, but other writers have him living to 104, or even 109. Marcus Aurelius, in his book "Meditations", says that Democritus was eaten by lice or vermin. 
Popularly known as the Laughing Philosopher (for laughing at human follies), the terms "Abderitan laughter", which means scoffing, incessant laughter, and Abderite, which means a scoffer, are derived from Democritus. To his fellow citizens he was also known as "The Mocker".
Philosophy and science.
Most sources say that Democritus followed in the tradition of Leucippus, and they carried on the scientific rationalist philosophy associated with Miletus. Both were thoroughly materialist, believing everything to be the result of natural laws. Unlike Aristotle or Plato, the atomists attempted to explain the world without reasoning as to "purpose", "prime mover", or "final cause". For the atomists questions of physics should be answered with a mechanistic explanation ("What earlier circumstances caused this event?"), while their opponents search for explanations which, in addition to the material and mechanistic, also included the formal and teleological ("What purpose did this event serve?").
Aesthetics.
Democritus wrote theoretically on poetry and fine art. Thrasyllus identified specifically six works in his oevre which would belong to aesthetics as a discipline, but only fragments of the relevant works are extant, hence of all his outpourings on these things, only a few of his ideas are available to know. Proceeding Grecian historians consider him to have established aesthetics as a subject of investigation and study.
Atomic hypothesis.
The theory of Democritus and Leucippus held that everything is composed of "atoms", which are physically, but not geometrically, indivisible; that between atoms, there lies empty space; that atoms are indestructible; have always been, and always will be, in motion; that there are an infinite number of atoms, and kinds of atoms, which differ in shape, and size. Of the mass of atoms, Democritus said "The more any indivisible exceeds, the heavier it is." But his exact position on weight of atoms is disputed.
Leucippus is widely credited with being the first to develop the theory of atomism, although Isaac Newton preferred to credit the obscure Mochus the Phoenician (whom he believed to be the biblical Moses) as the inventor of the idea on the authority of Posidonius and Strabo. However, the "Stanford Encyclopedia of Philosophy" notes, "This theologically motivated view does not seem to claim much historical evidence, however."
Democritus, along with Leucippus and Epicurus, proposed the earliest views on the shapes and connectivity of atoms. They reasoned that the solidness of the material corresponded to the shape of the atoms involved. Thus, iron atoms are solid and strong with hooks that lock them into a solid; water atoms are smooth and slippery; salt atoms, because of their taste, are sharp and pointed; and air atoms are light and whirling, pervading all other materials. Using analogies from our sense experiences, he gave a picture or an image of an atom that distinguished them from each other by their shape, their size, and the arrangement of their parts. Moreover, connections were explained by material links in which single atoms were supplied with attachments: some with hooks and eyes others with balls and sockets. The Democritean atom is an inert solid (merely excluding other bodies from its volume) that interacts with other atoms mechanically. In contrast, modern, quantum-mechanical atoms interact via electric and magnetic force fields and are far from inert.
The theory of the atomists appears to be more nearly aligned with that of modern science than any other theory of antiquity. However, the similarity with modern concepts of science can be confusing when trying to understand where the hypothesis came from. It is obvious that classical atomists would never have had a solid empirical basis for our modern concepts of atoms and molecules. Bertrand Russell states that they just hit on a lucky hypothesis, only recently confirmed by evidence.
However, Lucretius, describing atomism in his "De rerum natura", gives very clear and compelling empirical arguments for the original atomist theory. He observes that any material is subject to irreversible decay. Through time, even hard rocks are slowly worn down by drops of water. Things have the tendency to get mixed up: mix water with soil and you get mud, that will usually not un-mix by itself. Wood decays. However, we see in nature and technology that there are mechanisms to recreate "pure" materials like water, air, and metals. The seed of an oak will grow out into an oak tree, made of similar wood as historical oak trees, the wood of which has already decayed. The conclusion is that many properties of materials must derive from something inside, that will itself never decay, something that stores for eternity the same inherent, indivisible properties. The basic question is: why has everything in the world not yet decayed, and how can exactly the same materials, plants, animals be recreated again and again? One obvious solution to explain how indivisible properties can be conveyed in a way not easily visible to human senses, is to hypothesize the existence of "atoms". These classical "atoms" are nearer to our modern concept of "molecule" than to the atoms of modern science. The other big point of classical atomism is that there must be a lot of open space between these "atoms": the void. Lucretius gives reasonable arguments that the void is absolutely necessary to explain how gasses and liquids can change shape, flow, while metals can be molded, without changing the basic material properties.
Void hypothesis.
The atomistic void hypothesis was a response to the paradoxes of Parmenides and Zeno, the founders of metaphysical logic, who put forth difficult to answer arguments in favor of the idea that there can be no movement. They held that any movement would require a void—which is nothing—but a nothing cannot exist. The Parmenidean position was "You say there "is" a void; therefore the void is not nothing; therefore there is not the void." The position of Parmenides appeared validated by the observation that where there seems to be nothing there is air, and indeed even where there is not matter there is "something", for instance light waves.
The atomists agreed that motion required a void, but simply ignored the argument of Parmenides on the grounds that motion was an observable fact. Therefore, they asserted, there must be a void. This idea survived in a refined version as Newton's theory of absolute space, which met the logical requirements of attributing reality to not-being. Einstein's theory of relativity provided a new answer to Parmenides and Zeno, with the insight that space by itself is relative and cannot be separated from time as part of a generally curved space-time manifold. Consequently, Newton's refinement is now considered superfluous.
Epistemology.
The knowledge of truth, according to Democritus, is difficult, since the perception through the senses is subjective. As from the same senses derive different impressions for each individual, then through the sense-impressions we cannot judge the truth. We can only interpret the sense data through the intellect and grasp the truth, because the truth is at the bottom.
There are two kinds of knowing, the one he calls "legitimate" (γνησίη, "gnesie", "genuine") and the other "bastard" (σκοτίη, "skotie", "secret"). The "bastard" knowledge is concerned with the perception through the senses, therefore it is insufficient and subjective. The reason is that the sense-perception is due to the effluences of the atoms from the objects to the senses. When these different shapes of atoms come to us, they stimulate our senses according to their shape, and our sense-impressions arise from those stimulations.
The second sort of knowledge, the "legitimate" one, can be achieved through the intellect, in other words, all the sense-data from the "bastard" must be elaborated through reasoning. In this way one can get away from the false perception of the "bastard" knowledge and grasp the truth through the inductive reasoning. After taking into account the sense-impressions, one can examine the causes of the appearances, draw conclusions about the laws that govern the appearances, and discover the causality (αἰτιολογία, "aetiologia") by which they are related. This is the procedure of thought from the parts to the whole or else from the apparent to non-apparent (inductive reasoning). This is one example of why Democritus is considered to be an early scientific thinker. The process is reminiscent of that by which science gathers its conclusions.
Ethics and politics.
The ethics and politics of Democritus come to us mostly in the form of maxims. As such, the Stanford Encyclopedia of Philosophy has gone as far to say that: "despite the large number of ethical sayings, it is difficult to construct a coherent account of Democritus' ethical views" and noting that there is a "difficulty of deciding which fragments are genuinely Democritean".
He says that "Equality is everywhere noble," but he is not encompassing enough to include women or slaves in this sentiment. Poverty in a democracy is better than prosperity under tyrants, for the same reason one is to prefer liberty over slavery. Those in power should "take it upon themselves to lend to the poor and to aid them and to favor them, then is there pity and no isolation but companionship and mutual defense and concord among the citizens and other good things too many to catalogue." Money when used with sense leads to generosity and charity, while money used in folly leads to a common expense for the whole society— excessive hoarding of money for one's children is avarice. While making money is not useless, he says, doing so as a result of wrongdoing is the "worst of all things." He is on the whole ambivalent towards wealth, and values it much less than self-sufficiency. He disliked violence but was not a pacifist: he urged cities to be prepared for war, and believed that a society had the right to execute a criminal or enemy so long as this did not violate some law, treaty, or oath.
Goodness, he believed, came more from practice and discipline than from innate human nature. He believed that one should distance oneself from the wicked, stating that such association increases disposition to vice. Anger, while difficult to control, must be mastered in order for one to be rational. Those who take pleasure from the disasters of their neighbors fail to understand that their fortunes are tied to the society in which they live, and they rob themselves of any joy of their own. Democritus believed that happiness was a property of the soul. He advocated a life of contentment with as little grief as possible, which he said could not be achieved through either idleness or preoccupation with worldly pleasures. Contentment would be gained, he said, through moderation and a measured life; to be content one must set their judgment on the possible and be satisfied with what one has—giving little thought to envy or admiration. Democritus approved of extravagance on occasion, as he held that feasts and celebrations were necessary for joy and relaxation. He considers education to be the noblest of pursuits, but cautioned that learning without sense leads to error.
Mathematics.
Democritus was also a pioneer of mathematics and geometry in particular. We only know this through citations of his works (titled "On Numbers", "On Geometrics", "On Tangencies", "On Mapping", and "On Irrationals") in other writings, since most of Democritus' body of work did not survive the Middle Ages. Democritus was among the first to observe that a cone or pyramid has one-third the volume of a cylinder or prism respectively with the same base and height.
Anthropology, biology, and cosmology.
His work on nature is known through citations of his books on the subjects, "On the Nature of Man", "On Flesh" (two books), "On Mind, On the Senses", "On Flavors", "On Colors", "Causes concerned with Seeds and Plants and Fruits", and "Causes concerned with Animals" (three books). He spent much of his life experimenting with and examining plants and minerals, and wrote at length on many scientific topics. Democritus thought that the first humans lived an anarchic and animal sort of life, going out to forage individually and living off the most palatable herbs and the fruit which grew wild on the trees. They were driven together into societies for fear of wild animals, he said. He believed that these early people had no language, but that they gradually began to articulate their expressions, establishing symbols for every sort of object, and in this manner came to understand each other. He says that the earliest men lived laboriously, having none of the utilities of life; clothing, houses, fire, domestication, and farming were unknown to them. Democritus presents the early period of mankind as one of learning by trial and error, and says that each step slowly led to more discoveries; they took refuge in the caves in winter, stored fruits that could be preserved, and through reason and keenness of mind came to build upon each new idea.
Democritus held that the Earth was round (spherical), and stated that originally the universe was composed of nothing but tiny atoms churning in chaos, until they collided together to form larger units—including the earth and everything on it. He surmised that there are many worlds, some growing, some decaying; some with no sun or moon, some with several. He held that every world has a beginning and an end, and that a world could be destroyed by collision with another world. To epitomize Democritus's cosmology, Russell calls on Shelley: "Worlds on worlds are rolling ever / From creation to decay, / Like the bubbles on a river / Sparkling, bursting, borne away."
Numismatics.
Democritus was depicted on the following contemporary coins/banknotes:

</doc>
<doc id="8212" url="http://en.wikipedia.org/wiki?curid=8212" title="Disc golf">
Disc golf

Disc golf, also known as Frolf, Folf, Frisbee disc or frisbee golf, is a flying disc game, as well as a precision and accuracy sport, in which individual players throw a flying disc at a target. According to Paul Ince of the Professional Disc Golf Association, "the object of the game is to traverse a course from beginning to end in the fewest number of throws of the disc." In just 8 years (2000-2008), the number of disc golf courses doubled. The game is played in about 40 countries around the world.
History.
The early history of disc golf is closely tied to the history of the recreational flying disc (especially as popularized by the trademarked Frisbee) and may have been invented in the early 1900s. The first known instance of anyone playing golf with a flying disc occurred in Bladworth, Saskatchewan, Canada in 1926. Ronald Gibson and a group of his Bladworth Elementary School buddies played a game throwing tin plates at targets such as trees and fence posts. They called the game Tin Lid Golf and played on a fairly regular basis on a disc golf course they laid out on their school grounds. But, after they grew older and went their separate ways, the game came to an end. It wasn't until the 1970s that disc golf would be reintroduced to Canadians at the Canadian Open Frisbee Championships in Toronto.
Modern disc golf started in the early 1960s, when it seems to have been invented in many places and by many people independently. Students at Rice University in Houston, Texas, for example, held tournaments with trees as targets as early as 1963, and in the early 1960s, players in Pendleton King Park in Augusta, Georgia would toss Frisbees in 50-gallon barrel trash cans designated as targets.
A true pioneer of the sport of Frisbee Golf is Kevin Donnelly, who, until 2011, was unknown for his accomplishment. Kevin began playing a form of Frisbee golf in 1959 called Street Frisbee Golf. In 1961, while a Recreation Leader and then Recreation Supervisor for the City of Newport Beach, California, he formulated and then began organizing Frisbee golf tournaments at nine of the city's playgrounds he supervised. This culminated in 1965 with a fully documented, Wham-O sponsored, city-wide Frisbee Golf tournament. This highly publicized tournament included hula hoops as holes, with published rules, hole lengths, pars, and penalties, Wham-O prizes and, an event in which Fred Morrison, the Frisbee inventor, was in attendance. In 1967, two years after conducting the first-ever organized Frisbee Golf Tournament, Kevin, then the Coordinator of the Parks and Recreation Section at Fresno State College, California, organized and then taught the first ever college level Frisbee Golf activity course, in which George Sappenfield was registered.
Two of the best-known figures in the sport are "Steady Ed" Headrick, who introduced the first formal disc golf target with chains and a basket, and Dave Dunipace who invented the modern golf disc in 1983, with his revolutionary change to add a beveled edge rim, this gave the disc a greater distance and accuracy. Dave was one of the founders of Innova, a well-known disc manufacturer. In 1976, Headrick formed the DGA, then later the PDGA and the RDGA. Ted Smethers took over the PDGA in 1982 to be run independently and to officiate the standard rules of play for the sport. The sport has grown at a rate of 12-15 percent annually for more than the past decade, with nearly 4000 courses in the US and about 5000 globally. The game is now played in more than 40 countries worldwide, primarily in the United States, Canada, Central and Western Europe, Japan, South Korea, New Zealand and Australia.
George Sappenfield and early object courses.
In 1965, George Sappenfield, from Fresno California, was a recreation counselor during summer break from college. While playing golf one afternoon he realized that it might be fun for the kids on his playground if they played "golf" with frisbees. He set up an object course for his kids to play on. Other early courses were also of this type, using anything from lamp poles to fire hydrants as targets. When he finished college in 1968, Sappenfield became the Parks and Recreation Supervisor for Conejo Recreation and Park District in Thousand Oaks, California. George introduced the game to many adults by planning a disc golf tournament as part of a recreation project. He contacted Wham-O Manufacturing and asked them for help with the event. Wham-O supplied frisbees for throwing, and hula hoops for use as targets. However, it would not be until the early 1970s that courses began to crop up in various places in the Midwest and the East Coast (some perhaps through Sappenfield's promotion efforts, others probably independently envisioned). Some of Sappenfield's acquaintances are known to have brought the game to UC Berkeley. It quickly became popular on campus, with a permanent course laid out in 1970.
"Steady Ed" Headrick and the growth of the modern game.
"Steady Ed" Headrick began thinking about the sport during his time at Wham-o toys where he designed and patented the modern day Frisbee. Headrick, who is now regarded as the "Father of Disc Golf," designed and installed the first standardized target course in what was then known as Oak Grove Park in La Cañada Flintridge, California. (Today the park is known as Hahamongna Watershed Park). The park is immediately south of the Jet Propulsion Laboratory, which supplied at least a few of the earliest players. Ed worked for the San Gabriel, California-based Wham-O Corporation and is credited for pioneering the modern era of disc sports. While at Wham-O, Headrick redesigned the Pluto Platter reworking the rim height, disc shape, diameter, weight and plastics, creating a controllable disc that could be thrown accurately. Headrick marketed and pushed the professional model Frisbee and "Frisbee" as a sport. Ed Founded "The International Frisbee Association (IFA)" and began establishing standards for various sports using the Frisbee such as Distance, Flying disc freestyle and Guts.
Headrick coined and trademarked the term "Disc Golf" when formalizing the sport and patented the Disc Pole Hole, the first disc golf target to incorporate chains and a basket on a pole. He started designing the target because he was tired of arguing over what counted as a scoring disc with his friends.
Headrick founded the Professional Disc Golf Association (PDGA)and Recreational Disc Golf Association (RDGA)for competitive and family-oriented play, respectively, and worked on standardizing the rules and the equipment for the growing sport. Headrick abandoned his trademark on the term "Disc Golf," and turned over control and administration of the PDGA to the growing body of disc golf players in order to focus on his passion for building and inventing equipment for the sport.
Basic rules.
For the complete rules of disc golf, one can read the .
Safety.
All players should always keep in mind that disc golf discs can be potentially very dangerous if not thrown with safety and common sense always in mind. Never throw a disc in the area someone occupies unless they are completely aware you are throwing it in their direction. Players should always wait until the player or group in front of them have completed a hole and moved out of the way until they throw. If you come upon a hole where the target is blind and you can potentially not see a group ahead it is common procedure to call out "clear on (# of hole you are on)". If there is no answer you should again call out and before throwing say "coming down on (# of hole you are on)".
Elements.
Disc types.
The golf discs used today are much smaller and heavier than traditional flying discs, typically 8-9 inches (20-23 cm) in diameter and weighing between 120 and 180 grams. The PDGA prohibits any disc to be heavier than 200 grams. Discs used for disc golf are designed and shaped for control, speed, and accuracy, while general-purpose flying discs, such as those used for playing guts or ultimate, have a more traditional shape, similar to a catch disc. There is a wide variety of discs used in disc golf and they are generally divided into three categories: putters, all-purpose mid-range discs, and drivers.
Putter.
Putters are similar to the discs used in simple games of catch, such as the Wham-o brand Frisbee. They are designed to fly straight, predictably, and very slowly compared to mid-range discs and drivers. They are typically used for tight, controlled shots that are close to the basket, although some players use them for short drives where trees or other obstacles come into play. Usually a pro carries 1-7 putters depending on their flight characteristics.
Mid-range.
Mid-range discs have slightly sharper edges that enable them to cut through the air better. These discs are usually faster, more stable, and have a longer range than a putter. Some players will use mid-ranges as drivers, and there are tournaments that require players to use only mid-range discs. They are good all-around discs and are suitable for a first time player.
Driver.
Drivers are usually recognized by their sharp, bevelled edge and have most of their mass concentrated on the outer rim of the disc rather than distributed equally throughout. Drivers are often divided into different categories. For example, Innova Discs divides their discs into Distance Drivers and Fairway Drivers, with a fairway driver being somewhere between a distance driver and a mid-range disc. Discraft divides their drivers into 3 categories: Long Drivers, Extra Long Drivers, and Maximum Distance Drivers. Because the physics of a disc require "snap" or "flick", which means putting spin on the disc, new players generally find that throwing a distance driver accurately can be somewhat difficult and will require experience with golf disc response. This is why it is better for players to begin with fairway drivers, long drivers, or even mid-ranges, and incorporate maximum distance drivers as their strength and disc control increases. Most players that are starting off will be most likely throwing lighter discs. Another type of driver, used less frequently, is a roller. As the name indicates, it has an edge designed to roll rather than fly.
Stability.
Stability is the measurement of a disc's tendency to bank laterally during its flight. A disc that is over-stable will tend to track left (for a right handed, backhand throw), whereas a disc that is under-stable will tend to track right (also for a right handed, backhand throw). The stability rating of the discs differs depending on the manufacturer of the disc. Innova Discs rate stability as "turn" and "fade". "Turn" references how the disc will fly at high speed during the beginning and middle of its flight, and is rated on a scale of +1 to −5, where +1 is the most overstable and −5 is the most understable. "Fade" references how the disc will fly at lower speeds towards the end of its flight, and is rated on a scale of 0 to 5, where 0 has the least fade, and 5 has the most fade. For example, a disc with a turn of -5 and fade of +1 will fly to the right for (right handed, backhand throw) the majority of its flight then curl back minimally left at the end. A disc with a turn of -1 and a fade of +3 will turn slightly right during the middle of its flight and turn hard left as it slows down. These ratings can be found on the discs themselves or from the manufacturer's web site. Discraft prints the stability rating on all discs and also provides this information on their web site. The stability ranges from 3 to −2 for Discraft discs; however Discraft's ratings are more of a combination of turn and fade with the predominance being fade.
Throw styles.
While there are many different grips and styles to throwing the disc, there are two basic throwing techniques, backhand and forehand (or sidearm). These two techniques are different and effective in different circumstances. Their understanding and mastery can greatly improve a players' game, and offer diverse options in maneuvering to the basket with greater efficacy. Many players use what is referred to as a run-up during their drive. This is practiced to build more forward disc momentum and distance. Throwing styles vary from player to player, and there is no standard throwing style.
All discs when thrown will naturally fall to a certain direction, this direction is termed "Hyzer", the natural fall of the disc, or "Anhyzer", making the disc fall against its natural flight pattern. For a right-handed, back-hand thrower (RHBH), the disc will naturally fall to the left. For a right-handed fore-hand thrower (RHFH), the disc will naturally fall to the right. For a left-handed, back-hand thrower (LHBH), the disc will naturally fall to the right. For a left-handed, fore-hand thrower (LHFH), the disc will naturally fall to the left.
Backhand.
To perform this throw, the disc is rapidly drawn from across the front of the body, and released towards a forward aimpoint. Due to the potential snap available with this technique, one can expect greater distance and accuracy than with its counterpart. It is important to initiate momentum from the feet and allow it to travel up the body, hips and shoulders, culminating in the transfer of energy to the disc.
Forehand.
The forehand (sidearm) throw is performed by drawing the disc from behind and partially across the front of the body: similar to a sidearm throw in baseball. The term sidearm actually predates the descriptor forehand, which is seemingly in use today as a simpler means to communicate the technique: equating to a tennis forehand.
Alternative throws.
The following examples of throws may be used to better deliver a disc where the former common two throws would be impeded by obstacles (bushes, trees, boulders, structures, etc.
Common alternative styles
Other alternative styles
Course components.
While the roots of the game are very casual and laid back, the newer generations of players are taking course design as well as the other elements of the game to a new level. Though early on targets were trees or fence posts in the woods, now courses are being cut out and under-utilized parts of parks, schools, and private land are being used to make some of the most challenging and strategic courses around. All courses share the same basic elements; targets, tee pads, signage, topography, and most important, safety.
Targets.
The first incarnation of targets were known as tonal poles because of the sound they made when hit. These consisted of a metal pipe placed on a smaller pipe that when struck with the disc made a gong type sound. While these were much more accurate than a tree, arguments and disagreements led to the invention of the Disc Pole Hole by Ed Headrick in 1975. The basket (as it is now known in most circles) is the standard for disc golf courses.
Tee pads.
The tee pad is where a player begins the hole. A solid base is a must for any successful course, and where early courses had plain dirt pads, modern courses use concrete, or more cost effective materials such as mulch, decomposed granite, or other natural materials. In recent years recycled rubber mats have been developed and are starting to catch on. While many alternatives have been created, concrete is the standard.
Signage.
Signage is critical to any good course. Knowing distances, par count, out-of-bounds, and layout for each hole will give a player the information they need to make a great shot. Many courses have a main layout sign at the beginning of the course to show details of the course as a whole, as well as any needed information about the course. Hole signs give specific details about the hole the player is on, such as mandatory paths, out-of-bounds, and length. Not only are hole information signs critical, but way-finding signs and informational signs can make a good course great, and the absence of these can make a good course bad.
Topography.
What makes disc golf unique is the utilization of natural elements, using trees and shrubs as obstacles and elevation changes to make the course challenging. Keeping the raw and environmentally conscious elements gives each course its own personality and strategy.
Safety.
Safety is one of the most important elements of course design and actual play because most courses are in public parks: non-players are routinely found in the course environment. Paramount to the planning of a quality course is the detailing and minimizing possible points of interaction with non-players.
Because of all of these elements and the importance of each one to the success of the course, seeking out a qualified, experienced course designer will help to insure that all of these factors are kept at the forefront. Discs can be thrown fast and when hit, mainly the head, can cause serious injury.
Scoring.
Medal play is the most common scoring method used in the sport but there are many other forms. These include match play, skins, speed golf and captain's choice, which in disc golf is referred to as "doubles" (not to be confused with partner or team play).
Regardless of which form of play the participants choose, the main objectives of disc golf are conceptually the same as traditional golf in the sense that players follow the same scorekeeping technique.
Scoreboard:
Doubles play is a unique style of play that many local courses offer on a weekly basis. In this format, teams of two golfers are determined. Sometimes this is done by random draw, and other times it is a pro-am format. On the course, it is a "best-disc" scramble, meaning both players throw their tee shot; and then decide which lie they would like to play. Both players then play from the same lie, again choosing which lie is preferable. The World Amateur Doubles Format includes best shot, alternate shot, best score (players play singles and take the best result from the hole) and worst shot (both players must sink the putt).
Tournaments.
Tournaments are held nation-wide and year long in the USA. Sanctioned Tournament play is communicated through the Professional Disc Golf Association Membership. The PDGA provides international, professional, and amateur disc golf tournaments as well as communicates event results, opinions and other information beneficial to the sport via electronic and printed media. In 1982 the PDGA hosted the first World Championship Tournament. Since then the World Championships have been held in 17 different American states, as well as Toronto, ON 
Disc golf tournaments are popular around the world. As with traditional golf, there are many championship tournaments. One of the largest is the United States Disc Golf Championship.
Every year, the largest teams tournament in the world is held in Austin, Texas, by John Houck.
To prove the year-round sustainability of the sport, annual winter tournaments known as Ice Bowls are held at courses around the world. Using the motto "No Wimps, No Whiners", Ice Bowls collectively are designed to create sport awareness, and are considered charity events that typically benefit a food bank local to a given tournament location. The official Web site reports that the 2010 Ice Bowls raised over $250,000 and donated over 67,000 pounds of food in the 222 tournaments for the year. 
Women.
While there are more male than female players, the Women's Disc Golf Association exists to encourage female players and arrange women's tournaments. A PDGA survey states that out of its 11,302 members in 2006, 8% are female, or about 900. In PDGA competition, women have the option to play in gender-protected divisions. The women's field has grown rapidly in the past five years, as many women-only tournaments have grown in popularity around the world.
Several companies have started programs and websites to help attract women to the sport. The PDGA Women's Committee is "Dedicated to Attract, Encourage, and Retain Female Participation in Organized Disc Golf Events". The PDGA Women's Committee set historical records on 12 May 2012 by running the Inaugural Women's Global Event that attracted 636 female players in 24 states and 4 countries. The Women's Global Event is expected to become a bi-annual event returning in 2014 with hopes of setting the bar even higher with the amount of participants.
There are also Disc golf companies such as Disc-Diva, that have started up with a primary, though not exclusive, focus on women in the sport, promoting accessories geared towards women and using catch phrases like "you wish you threw like a girl". Sassy Pants is another group that focuses on getting more involvement from women in the sport, advocating for sponsorship of women to enter tournaments.
Women's disc golf teams are involved in the National Collegiate Disc Golf Championship, and the Mississippi State Women's Team were the inaugural champions.
Disc golf hall of fame.
Inductees:

</doc>
<doc id="8214" url="http://en.wikipedia.org/wiki?curid=8214" title="Decimal">
Decimal

The decimal numeral system (also called base ten or occasionally denary) has ten as its base. It is the numerical base most widely used by modern civilizations.
Decimal notation often refers to a base-10 positional notation such as the Hindu-Arabic numeral system or rod calculus; however, it can also be used more generally to refer to non-positional systems such as Roman or Chinese numerals which are also based on powers of ten.
A decimal number, or just decimal, refers to any number written in decimal notation, although it is more commonly used to refer to numbers that have a fractional part separated from the integer part with a decimal separator (e.g. 11.25).
A decimal may be a terminating decimal, which has a finite fractional part (e.g. 15.600); a repeating decimal, which has an infinite (non-terminating) fractional part made up of a repeating sequence of digits (e.g. 5.8144); or an infinite decimal, which has a fractional part that neither terminates nor has an infinitely repeating pattern (e.g. 3.14159265...). Decimal fractions have terminating decimal representations, whereas irrational numbers have infinite decimal representations. 
Decimal notation.
Decimal notation is the writing of numbers in a base-10 numeral system. Examples are Greek numerals, Roman numerals, Brahmi numerals, and Chinese numerals, as well as the Hindu-Arabic numerals used by speakers of many European languages. Roman numerals have symbols for the decimal powers (1, 10, 100, 1000) and secondary symbols for half these values (5, 50, 500). Brahmi numerals have symbols for the nine numbers 1–9, the nine decades 10–90, plus a symbol for 100 and another for 1000. Chinese numerals have symbols for 1–9, and additional symbols for powers of 10, which in modern usage reach 1072.
However, when people who use Hindu-Arabic numerals speak of decimal notation, they often mean not just decimal numeration, as above, but also decimal fractions, all conveyed as part of a positional system. Positional decimal systems include a zero and use symbols (called digits) for the ten values (0, 1, 2, 3, 4, 5, 6, 7, 8, and 9) to represent any number, no matter how large or how small. These digits are often used with a decimal separator which indicates the start of a fractional part, and with a symbol such as the plus sign + (for positive) or minus sign − (for negative) adjacent to the numeral to indicate whether it is greater or less than zero, respectively.
Positional notation uses positions for each power of ten: units, tens, hundreds, thousands, etc. The position of each digit within a number denotes the multiplier (power of ten) multiplied with that digit—each position has a value ten times that of the position to its right. There were at least two presumably independent sources of positional decimal systems in ancient civilization: the Chinese counting rod system and the Hindu-Arabic numeral system (the latter descended from Brahmi numerals).
Ten is the number which is the count of fingers and thumbs on both hands (or toes on the feet). The English word digit as well as its translation in many languages is also the anatomical term for fingers and toes. In English, decimal (decimus < Lat.) means "tenth", decimate means "reduce by a tenth", and denary (denarius < Lat.) means "the unit of ten".
The symbols for the digits in common use around the globe today are called Arabic numerals by Europeans and Indian numerals by Arabs, the two groups' terms both referring to the culture from which they learned the system. However, the symbols used in different areas are not identical; for instance, Western Arabic numerals (from which the European numerals are derived) differ from the forms used by other Arab cultures.
Decimal fractions.
A decimal fraction is a fraction the denominator of which is a power of ten.
Decimal fractions are commonly expressed in decimal notation rather than fraction notation by discarding the denominator and inserting the decimal separator into the numerator at the position from the right corresponding to the power of ten of the denominator and filling the gap with leading zeros if needed, e.g. decimal fractions 8/10, 1489/100, 24/100000, and 58900/10000 are expressed in decimal notation as 0.8, 14.89, 0.00024, 5.8900 respectively. In English-speaking, some Latin American and many Asian countries, a period (.) or raised period (·) is used as the decimal separator; in many other countries, particularly in Europe, a comma (,) is used.
The integer part, or integral part of a decimal number is the part to the left of the decimal separator. (See also truncation.) The part from the decimal separator to the right is the "fractional part". It is usual for a decimal number that consists only of a fractional part (mathematically, a "proper fraction") to have a leading zero in its notation (its "numeral"). This helps disambiguation between a decimal sign and other punctuation, and especially when the negative number sign is indicated, it helps visualize the sign of the numeral as a whole.
Trailing zeros after the decimal point are not necessary, although in science, engineering and statistics they can be retained to indicate a required precision or to show a level of confidence in the accuracy of the number: Although 0.080 and 0.08 are numerically equal, in engineering 0.080 suggests a measurement with an error of up to one part in two thousand (±0.0005), while 0.08 suggests a measurement with an error of up to one in two hundred (see "significant figures").
Other rational numbers.
Any rational number with a denominator whose only prime factors are 2 and/or 5 may be precisely expressed as a decimal fraction and has a finite decimal expansion.
If the rational number's denominator has any prime factors other than 2 or 5, it cannot be expressed as a finite decimal fraction, and has a unique eventually repeating infinite decimal expansion.
100 − 1 = 99 = 9 × 11:
1000 − 1 = 9 × 111 = 27 × 37:
also:
That a rational number must have a finite or recurring decimal expansion can be seen to be a consequence of the long division algorithm, in that there are at most q-1 possible nonzero remainders on division by q, so that the recurring pattern will have a period less than q. For instance, to find 3/7 by long division:
 0.4 2 8 5 7 1 4 ...
 7 ) 3.0 0 0 0 0 0 0 0
 2 8 30/7 = 4 with a remainder of 2
 2 0
 1 4 20/7 = 2 with a remainder of 6
 6 0
 5 6 60/7 = 8 with a remainder of 4
 4 0
 3 5 40/7 = 5 with a remainder of 5
 5 0
 4 9 50/7 = 7 with a remainder of 1
 1 0
 7 10/7 = 1 with a remainder of 3
 3 0
 2 8 30/7 = 4 with a remainder of 2
 2 0
 etc.
The converse to this observation is that every recurring decimal represents a rational number "p"/"q". This is a consequence of the fact that the recurring part of a decimal representation is, in fact, an infinite geometric series which will sum to a rational number. For instance,
Real numbers.
Every real number has a (possibly infinite) decimal representation; i.e., it can be written as
where
Such a sum converges as more and more negative values of "i" are included, even if there are infinitely many non-zero "ai".
Rational numbers (e.g., p/q) with prime factors in the denominator other than 2 and 5 (when reduced to simplest terms) have a unique recurring decimal representation.
Non-uniqueness of decimal representation.
Consider those rational numbers which have only the factors 2 and 5 in the denominator, i.e., which can be written as "p"/(2"a"5"b"). In this case there is a terminating decimal representation. For instance, 1/1 = 1, 1/2 = 0.5, 3/5 = 0.6, 3/25 = 0.12 and 1306/1250 = 1.0448. Such numbers are the only real numbers which do not have a unique decimal representation, as they can also be written as a representation that has a recurring 9, for instance 1 = 0.99999…, 1/2 = 0.499999…, etc. The number 0 = 0/1 is special in that it has no representation with recurring 9.
This leaves the irrational numbers. They also have unique infinite decimal representations, and can be characterised as the numbers whose decimal representations neither terminate nor recur.
So in general the decimal representation is unique, if one excludes representations that end in a recurring 9.
The same trichotomy holds for other base-"n" positional numeral systems:
A version of this even holds for irrational-base numeration systems, such as golden mean base representation.
Decimal computation.
Decimal computation was carried out in ancient times in many ways, typically in rod calculus, with decimal multiplication table used in ancient China and with sand tables in India and Middle East or with a variety of abaci.
Modern computer hardware and software systems commonly use a binary representation internally (although many early computers, such as the ENIAC or the IBM 650, used decimal representation internally).
For external use by computer specialists, this binary representation is sometimes presented in the related octal or hexadecimal systems.
For most purposes, however, binary values are converted to or from the equivalent decimal values for presentation to or input from humans; computer programs express literals in decimal by default. (123.1, for example, is written as such in a computer program, even though many computer languages are unable to encode that number precisely.)
Both computer hardware and software also use internal representations which are effectively decimal for storing decimal values and doing arithmetic. Often this arithmetic is done on data which are encoded using some variant of binary-coded decimal,
especially in database implementations, but there are other decimal representations in use (such as in the new IEEE 754 Standard for Floating-Point Arithmetic).
Decimal arithmetic is used in computers so that decimal fractional results can be computed exactly, which is not possible using a binary fractional representation.
This is often important for financial and other calculations.
History.
Many ancient cultures calculated with numerals based on ten: Egyptian hieroglyphs, in evidence since around 3000 BC, used a purely decimal system, just as the Cretan hieroglyphs (ca. 1625−1500 BC) of the Minoans whose numerals are closely based on the Egyptian model. The decimal system was handed down to the consecutive Bronze Age cultures of Greece, including Linear A (ca. 18th century BC−1450 BC) and Linear B (ca. 1375−1200 BC) — the number system of classical Greece also used powers of ten, including, like the Roman numerals did, an intermediate base of 5. Notably, the polymath Archimedes (ca. 287–212 BC) invented a decimal positional system in his Sand Reckoner which was based on 108 and later led the German mathematician Carl Friedrich Gauss to lament what heights science would have already reached in his days if Archimedes had fully realized the potential of his ingenious discovery. The Hittites hieroglyphs (since 15th century BC), just like the Egyptian and early numerals in Greece, was strictly decimal.
The Egyptian hieratic numerals, the Greek alphabet numerals, the Roman numerals, the Chinese numerals and early Indian Brahmi numerals are all non-positional decimal systems, and required large numbers of symbols. For instance, Egyptian numerals used different symbols for 10, 20, to 90, 100, 200, to 900, 1000, 2000, 3000, 4000, to 10,000.
The world's earliest positional decimal system was the Chinese rod calculus
History of decimal fractions.
According to Joseph Needham and Lam Lay Yong, decimal fractions were first developed and used by the Chinese in the 1st century BC, and then spread to the Middle East and from there to Europe. The written Chinese decimal fractions were non-positional. However, counting rod fractions were positional.
Qin Jiushao in his book Mathematical Treatise in Nine Sections (1247) denoted 0.96644 by
The Jewish mathematician Immanuel Bonfils invented decimal fractions around 1350, anticipating Simon Stevin, but did not develop any notation to represent them.
The Persian mathematician Jamshīd al-Kāshī claimed to have discovered decimal fractions himself in the 15th century, though J. Lennart Berggren notes that positional decimal fractions were used five centuries before him by Arab mathematician Abu'l-Hasan al-Uqlidisi as early as the 10th century.Al Khwarizmi introduced fraction to Islamic countries in the early 9th century, his fraction presentation was an exact copy of traditional Chinese mathematical fraction from The Mathematical Classic of Sunzi. This form of fraction with numerator on top and denominator at bottom without a horizontal bar was also used by 10th century Abu'l-Hasan al-Uqlidisi and 15th century Jamshīd al-Kāshī's work "Arithmetic Key".
A forerunner of modern European decimal notation was introduced by Simon Stevin in the 16th century.
Natural languages.
Telugu language uses a straightforward decimal system. Other Dravidian languages such as Tamil and Malayalam have replaced the number nine "tondu" with 'onpattu' ("one to ten") during the early Middle Ages, while Telugu preserved the number nine as "tommidi".
The Hungarian language also uses a straightforward decimal system. All numbers between 10 and 20 are formed regularly (e.g. 11 is expressed as "tízenegy" literally "one on ten"), as with those between 20-100 (23 as "huszonhárom" = "three on twenty").
A straightforward decimal rank system with a word for each order 10十,100百,1000千,10000万, and in which 11 is expressed as "ten-one" and 23 as "two-ten-three", and 89345 is expressed as 8 (ten thousands) 万9 (thousand) 千3 (hundred) 百4 (tens) 十 5 is found in Chinese languages, and in Vietnamese with a few irregularities. Japanese, Korean, and Thai have imported the Chinese decimal system. Many other languages with a decimal system have special words for the numbers between 10 and 20, and decades. For example in English 11 is "eleven" not "ten-one" or "one-teen".
Incan languages such as Quechua and Aymara have an almost straightforward decimal system, in which 11 is expressed as "ten with one" and 23 as "two-ten with three".
Some psychologists suggest irregularities of the English names of numerals may hinder children's counting ability.
Other bases.
Some cultures do, or did, use other bases of numbers.

</doc>
<doc id="8216" url="http://en.wikipedia.org/wiki?curid=8216" title="Dorians">
Dorians

The Dorians (; Greek: Δωριεῖς, "Dōrieis", singular Δωριεύς, "Dōrieus") were one of the four major Greek "ethnē" into which the Greeks, or Hellenes, of the ancient period considered themselves divided (along with the Aeolians, Achaeans and Ionians). "Ethnos" has the sense of ethnic group. Herodotus uses the word with regard to them. They are almost always referred to as just "the Dorians", as they are in the earliest literary mention of them in "Odyssey", where they already can be found inhabiting the island of Crete.
They were diverse in way of life and social organization, varying from the populous trade center of the city of Corinth, known for its ornate style in art and architecture, to the isolationist, military state of Sparta. And yet, all Hellenes knew which localities were Dorian, and which were not. Dorian states at war could more likely, but not always, count on the assistance of other Dorian states. Dorians were distinguished by the Doric Greek dialect and by characteristic social and historical traditions.
In the 5th century BC, Dorians and Ionians were the two most politically important Greek "ethne", whose ultimate clash resulted in the Peloponnesian War. The degree to which fifth-century Hellenes self-identified as "Ionian" or "Dorian" has itself been disputed. At one extreme Édouard Will concludes that there was no true ethnic component in fifth-century Greek culture, in spite of anti-Dorian elements in Athenian propaganda. At the other extreme John Alty reinterprets the sources to conclude that ethnicity did motivate fifth-century actions. Moderns viewing these ethnic identifications through the fifth- and fourth-century BC literary tradition have been profoundly influenced by their own social politics. Also, according to E.N. Tigerstedt, nineteenth-century European admirers of virtues they considered "Dorian" identified themselves as "Laconophile" and found responsive parallels in the culture of their day as well; their biases contribute to the traditional modern interpretation of "Dorians".
Origin of the Dorians.
Accounts vary as to the Dorians’ place of origin. One theory, widely believed in ancient times, is that they originated in the north, north-western mountainous regions of Greece, ancient Macedonia and Epirus, whence obscure circumstances brought them south into the Peloponnese, to certain Aegean islands, Magna Graecia, Lapithos and Crete. Mythology gave them a Greek origin and eponymous founder, Dorus son of Hellen, the mythological patriarch of the Hellenes.
Peloponnesian dialect replacement.
The origin of the Dorians is a multi-faceted concept. In modern scholarship the term often has meant the location of the population disseminating the Doric Greek dialect within a hypothetical Proto-Greek speaking population. This dialect is known from records of classical northwest Greece, the Peloponnesus and Crete and some of the islands. The geographic and ethnic information found in the west's earliest known literary work, the "Iliad", combined with the administrative records of the former Mycenaean states, prove to universal satisfaction that East Greek speakers were once dominant in the Peloponnesus but suffered a setback there and were replaced at least in official circles by West Greek speakers. A historical event is associated with the overthrow, called anciently the Return of the Heracleidai and by moderns the Dorian Invasion.
This theory of a return or invasion presupposes that West Greek speakers resided in northwest Greece but overran the Peloponnesus replacing the East Greek there with their own dialect. No other records than Mycenaean are known to have existed in the Bronze Age, so a West Greek of that time and place cannot be proved or disproved. West Greek speakers were in western Greece in classical times. Unlike the East Greeks, they are not associated with any evidence of displacement events. This provides circumstantial evidence that the Doric dialect disseminated among the Hellenes of northwest Greece, a highly mountainous and somewhat isolated region.
The Dorian invasion.
The Dorian invasion is a modern historical concept attempting to account for:
On the whole, none of the objectives were met, but the investigations served to rule out various speculative hypotheses. Most scholars doubt that the Dorian invasion was the main cause of the collapse of the Mycenean civilization. The source of the West Greek speakers in the Peloponnesus remains unattested by any solid evidence.
Post-migrational distribution of the Dorians.
Though most of the Doric invaders settled in the Peloponnese, they also settled on Rhodes and Sicily, in what is now southern Italy. In Asia Minor existed the Dorian Hexapolis (the six great Dorian cities): Halikarnassos (Halicarnassus) and Knidos (Cnidus) in Asia Minor, Kos, and Lindos, Kameiros, and Ialyssos on the island of Rhodes. These six cities would later become rivals with the Ionian cities of Asia Minor. The Dorians also invaded Crete. These origin traditions remained strong into classical times: Thucydides saw the Peloponnesian War in part as "Ionians fighting against Dorians" and reported the tradition that the Syracusans in Sicily were of Dorian descent. Other such "Dorian" colonies, originally from Corinth, Megara, and the Dorian islands, dotted the southern coasts of Sicily from Syracuse to Selinus. ("EB" 1911).
Dorian identity.
Name of the Dorians.
The Dorian of Bronze Age Pylos.
A man's name, Dōrieus, occurs in the Linear B tablets at Pylos, one of the regions later invaded and subjugated by the Dorians. Pylos tablet Fn867 records it in the dative case as do-ri-je-we, *Dōriēwei, a third or consonant declension noun with stem ending in w. An unattested nominative plural, *Dōriēwes, would have become Dōrieis by loss of the w and contraction. The tablet records the grain rations issued to the servants of "religious dignitaries" celebrating a religious festival of Potnia, the mother goddess.
The nominative singular, Dōrieus, remained the same in the classical period. Many Linear B names of servants were formed from their home territory or the places where they came into Mycenaean ownership. According to Carl Darling Buck, the -eus suffix was very productive. One of its uses was to convert a toponym to an anthroponym; for example, Megareus, "Megarian," from Megara. A Dōrieus would be from Dōris, the only classical Greek state to serve as the basis for the name of the Dorians. The state is a small one in the mountains of west central Greece. However, classical Doris may not have been the same as Mycenaean Doris.
The Dorians of upland Doris.
A number of credible etymologies by noted scholars have been proposed. Julius Pokorny derives Δωριεύς, "Dōrieus" from δωρίς, "dōris", "woodland" (which can also mean upland). The "dōri-" segment is from the o-grade (either "ō" or "o") of Proto-Indo-European "*deru-", "tree", which also gives the Homeric Δούρειος Ἵππος ("Doureios Hippos", "Wooden Horse"). This derivation has the advantage of naming the people after their wooded, mountainous country. 
The lancers.
A second popular derivation was given by the French linguist, Émile Boisacq, from the same root, but from Greek δόρυ ("doru") 'spear-shaft' (which was made of wood); i.e., "the people of the spear" or "spearmen." In this case the country would be named after the people, as in Saxony from the Saxons. However, R. S. P. Beekes doubted the validity of this derivation and asserted that no good etymology exists.
The chosen Greeks.
It sometimes happens that different derivations of an Indo-European word exploit similar-sounding Indo-European roots. Greek "doru", "lance," is from the o-grade of Indo-European *"deru", "solid," in the sense of wood. It is similar to an extended form, *"dō-ro-", of "*dō-", (give), as can be seen in the modern Greek imperative δώσε ("dose", "give [sing.]!") appearing in Greek as δῶρον ("dōron", "gift"). This is the path taken by Jonathan Hall, relying on elements taken from the myth of the Return of the Herakeidai.
Hall cites the tradition, based on a fragment of the poet, Tyrtaeus, that "Sparta is a divine gift granted by Zeus and Hera" to the Heracleidae. In another version, Tyndareus gives his kingdom to Heracles in gratitude for restoring him to the throne, but Heracles "asks the Spartan king to safeguard the gift until his descendants might claim it."
Hall therefore proposes that the Dorians are the people of the gift. They assumed the name on taking possession of Lacedaemon. Doris was subsequently named after them. Hall makes comparisons of Spartans to Hebrews as a chosen people maintaining a covenant with God and being assigned a Holy Land. To arrive at this conclusion, Hall relies on Herodotus' version of the myth (see below) that the Hellenes under Dorus did not take his name until reaching the Peloponnesus. In other versions the Heracleidae enlisted the help of their Dorian neighbors. Hall does not address the problem of the Dorians not calling Lacedaemon Doris, but assigning that name to some less holy and remoter land. Similarly, he does not mention the Dorian servant at Pylos, whose sacred gift, if such it was, was still being ruled by the Achaean Atreid family at Lacedaemon.
Distinctions of language.
The Doric dialect was spoken in northwest Greece, Peloponnese, Crete, southwest Asia Minor, the southernmost islands in the Aegean Sea, and various cities of Southern Italy and Sicily. After the classical period it was mainly replaced by the Attic, upon which the Koine or common Greek language of the Hellenistic period was based. The main characteristic of Doric was the preservation of Indo-European [aː], long ⟨α⟩, which in Attic-Ionic became ], ⟨η⟩; as an example, the famous last farewell before the battle by Spartan mothers to their warrior sons giving them their shields "Ἤ τὰν ἤ ἐπὶ τὰς" (E tan e epi tas: either with it or on it - either you return with the shield or you are carried back dead on it) would have been "Ἤ τήν ἤ ἐπὶ τῆς" (E ten e epi tes) if it had been uttered by an Attic-Ionic speaker, such as an Athenian mother. Tsakonian Greek, a descendant of Doric Greek, is still spoken in some regions of the Southern Argolid coast of the Peloponnese, on the coast of the modern prefecture of Arcadia.
Other cultural distinctions.
Culturally, in addition to their Doric dialect of Greek, Doric colonies retained their characteristic Doric calendar revolving round a cycle of festivals of which the Hyacinthia and the Carneia were especially important.
The Dorian mode in music also was attributed to Doric societies and was associated by classical writers with martial qualities.
The Doric order of architecture in the tradition inherited by Vitruvius included the Doric column, noted for its simplicity and strength.
Dorian women had a distinctive dress, a tunic (plain dress) not needing to be pinned with brooches, which was once common to all the Hellenes. The Ionian women adopted a new dress with a brooch.
The Dorians seem to have offered the central mainland cultus for Helios. The scattering of cults of the sun god in Sicyon, Argos, Ermioni, Epidaurus and Laconia, and his holy livestock flocks at Taenarum, seem to suggest that the deity was considerably important in Dorian religion, compared to other parts of ancient Greece. Additionally, it may have been the Dorians to import his worship to Rhodes.
Ancient traditions.
In Greek historiography, the Dorians are mentioned by many authors. The chief classical authors to relate their origins are Herodotus, Thucydides and Pausanias. The most copious authors, however, lived in Hellenistic and Roman times, long after the main events. This apparent paradox does not necessarily discredit the later writers, who were relying on earlier works that did not survive. The customs of the Spartan state and its illustrious individuals are detailed at great length in such authors as Plutarch and Diodorus Siculus.
Homer.
The "Odyssey" has one reference to the Dorians:"There is a land called Crete, in the midst of the wine-dark sea, a fair, rich land, begirt with water, and therein are many men, past counting, and ninety cities. They have not all the same speech, but their tongues are mixed. There dwell Achaeans, there great-hearted native Cretans, there Cydonians, and Dorians of waving plumes, and goodly Pelasgians."
The reference is not compatible with a Dorian invasion that brought Dorians to Crete only after the fall of the Mycenaean states. In the "Odyssey", Odysseus and his relatives visit those states. Two solutions are possible, either the "Odyssey" is anachronistic or Dorians were on Crete in Mycenaean times. The uncertain nature of the Dorian invasion defers a definitive answer until more is known about it.
Tyrtaeus.
Tyrtaeus, a lame Athenian warrior-poet, became advisor of the Lacedaemonians in their mid-7th-century war to suppress a rebellion of the Messenians. The latter were a remnant of the Achaeans conquered "two generations before," which suggests a rise to supremacy at the end of the Dark Age rather than during and after the fall of Mycenae. The Messenian population was reduced to serfdom.
Only a few fragments of Tyrtaeus' five books of martial verse survive. His is the earliest mention of the three Dorian tribes: Pamphyli, Hylleis, Dymanes. He also says: "For Cronus' Son Himself, Zeus the husband of fair-crowned Hera, hath given this city to the children of Heracles, with whom we came into the wide isle of Pelops from windy Erineus."
Erineus was a village of Doris. He helped to establish the Spartan constitution, giving the kings and elders, among other powers, the power to dismiss the assembly. He established a rigorous military training program for the young including songs and poems he wrote himself, such as the "Embateria or Songs of the Battle-Charge which are also called Enoplia or Songs-under-Arms." These were chants used to establish the timing of standard drills under arms. He stressed patriotism:"For 'tis a fair thing for a good man to fall and die fighting in the van for his native land, ... let us fight with a will for this land, and die for our children and never spare our lives."
Herodotus.
Herodotus was from Halicarnassus, a Dorian colony on the southwest coast of Asia Minor; following the literary tradition of the times he wrote in Ionic Greek, being one of the last authors to do so. He described the Persian Wars, giving a thumbnail account of the histories of the antagonists, Greeks and Persians.
Herodotus gives a general account of the events termed "the Dorian Invasion," presenting them as transfers of population. Their original home was in northern central Greece next to Thessaly:"the Pelasgians ... were once neighbors of the people now called Dorians, and at that time inhabited the country which now is called Thessalian."
He goes on to expand in mythological terms, giving some of the geographic details of the myth:"56. Of all the answers that had reached him, this pleased him far the best, for it seemed incredible that a mule should ever come to be king of the Medes, and so he concluded that the sovereignty would never depart from himself or his seed after him. Afterwards he turned his thoughts to the alliance which he had been recommended to contract, and sought to ascertain by inquiry which was the most powerful of the Grecian states. His inquiries pointed out to him two states as pre-eminent above the rest. These were the Lacedaemonians and the Athenians, the former of Doric the latter of Ionic blood. And indeed these two nations had held from very early times the most distinguished place in Greece, the one being a Pelasgic the other a Hellenic people, and the one having never quitted its original seats, while the other had been excessively migratory; for during the reign of Deucalion, Phthiotis was the country in which the Hellenes dwelt, but under Dorus, the son of Hellen, they moved to the tract at the base of Ossa and Olympus, which is called Histiaeotis; forced to retire from that region by the Cadmeians,[1] they settled, under the name of Macedni, in the chain of Pindus. Hence they once more removed and came to Dryopis; and from Dryopis having entered the Peloponnese in this way, they became known as Dorians.
57. What the language of the Pelasgi was I cannot say with any certainty. If, however, we may form a conjecture from the tongue spoken by the Pelasgi of the present day, - those, for instance, who live at Creston above the Tyrrhenians, who formerly dwelt in the district named Thessaliotis, and were neighbours of the people now called the Dorians, - or those again who founded Placia and Scylace upon the Hellespont, who had previously dwelt for some time with the Athenians, - or those, in short, of any other of the cities which have dropped the name but are in fact Pelasgian; if, I say, we are to form a conjecture from any of these, we must pronounce that the Pelasgi spoke a barbarous language. If this were really so, and the entire Pelasgic race spoke the same tongue, the Athenians, who were certainly Pelasgi, must have changed their language at the same time that they passed into the Hellenic body; for it is a certain fact that the people of Creston speak a language unlike any of their neighbours, and the same is true of the Placianians, while the language spoken by these two people is the same; which shows that they both retain the idiom which they brought with them into the countries where they are now settled.
58. The Hellenic race has never, since its first origin, changed its speech. This at least seems evident to me. It was a branch of the Pelasgic, which separated from the main body, and at first was scanty in numbers and of little power; but it gradually spread and increased to a multitude of nations, chiefly by the voluntary entrance into its ranks of numerous tribes of barbarians. The Pelasgi, on the other hand, were, as I think, a barbarian race which never greatly multiplied.
Thus, according to Herodotus, the Dorians did not name themselves after Dorus until they had reached Peloponnesus. Herodotus does not explain the contradictions of the myth; for example, how Doris, located outside the Peloponnesus, acquired its name. However, his goal, as he relates in the beginning of the first book, is only to report what he had heard from his sources without judgement. In the myth, the Achaeans displaced from the Peloponnesus gathered at Athens under a leader Ion and became identified as "Ionians".
Herodotus' list of Dorian states is as follows. From northeastern Greece were Phthia, Histiaea and Macedon. In central Greece were Doris (the former Dryopia) and in the south Peloponnesus, specifically the states of Lacedaemon, Corinth, Sicyon, Epidaurus and Troezen. Hermione was not Dorian but had joined the Dorians. Overseas were the islands of Rhodes, Cos, Nisyrus and the Anatolian cities of Cnidus, Halicarnassus, Phaselis and Calydna. Dorians also colonised Crete including founding of such towns as Lato, Dreros and Olous. The Cynurians were originally Ionians but had become Dorian under the influence of their Argive masters.
Thucydides.
Thucydides professes little of Greece before the Trojan War except to say that it was full of barbarians and that there was no distinction between barbarians and Greeks. The Hellenes came from Phthiotis. The whole country indulged in and suffered from piracy and was not settled. After the Trojan War, "Hellas was still engaged in removing and settling."
Some 60 years after the Trojan War the Boeotians were driven out of Arne by the Thessalians into Boeotia and 20 years later "the Dorians and the Heraclids became masters of the Peloponnese." So the lines were drawn between the Dorians and the Aeolians (here Boeotians) with the Ionians (former Peloponnesians).
Other than these few brief observations Thucydides names but few Dorians. He does make it clear that some Dorian states aligned or were forced to align with the Athenians while some Ionians went with the Lacedaemonians and that the motives for alignment were not always ethnic but were diverse. Among the Dorians was Lacedaemon, Corcyra, Corinth and Epidamnus, Leucadia, Ambracia, Potidaea, Rhodes, Cythera, Argos, Carystus, Syracuse, Gela, Acragas (later Agrigentum), Acrae, Casmenae.
He does explain with considerable dismay what happened to incite ethnic war after the unity between the Greek states during the Battle of Thermopylae. The Congress of Corinth, formed prior to it, "split into two sections." Athens headed one and Lacedaemon the other:"For a short time the league held together, till the Lacedaemonians and Athenians quarreled, and made war upon each other with their allies, a duel into which all the Hellenes sooner or later were drawn."
He adds: "the real cause I consider to be ... the growth of the power of Athens and the alarm which this inspired in Lacedaemon..."
Pausanias.
The "Description of Greece" by Pausanias relates that the Achaeans of the Peloponnesus were driven from their lands by Dorians coming from Oeta, a mountainous region bordering on Thessaly. They were led by Hyllus, a son of Heracles, but were defeated by the Achaeans. Under other leadership they managed to be victorious over the Achaeans and remain in the Peloponnesus, a mythic theme called "the return of the Heracleidae." They had built ships at Naupactus in which to cross the Gulf of Corinth. This invasion is viewed by the tradition of Pausanias as a return of the Dorians to the Peloponnesus, apparently meaning a return of families ruling in Aetolia and northern Greece to a land in which they had once had a share. The return is described in detail: there were "disturbances" throughout the Peloponnesus except in Arcadia, and new Dorian settlers. Pausanias goes on to describe the conquest and resettlement of Laconia, Messenia, Argos and elsewhere, and the emigration from there to Crete and the coast of Asia Minor.
Diodorus Siculus.
Diodorus is a rich source of traditional information concerning the mythology and history of the Dorians, especially the "Library of History". He does not make any such distinction but the fantastic nature of the earliest material marks it as mythical or legendary. The myths do attempt to justify some Dorian operations, suggesting that they were in part political.
Heracles was a Perseid, a member of the ruling family of Greece. His mother Alcmene had both Perseids and Pelopids in her ancestry. A princess of the realm, she received Zeus thinking he was Amphitryon. Zeus intended his son to rule Greece but according to the rules of succession Eurystheus, born slightly earlier, preempted the right. Attempts to kill Heracles as a child failed. On adulthood he was forced into the service of Eurystheus, who commanded him to perform 12 labors.
Heracles became a warrior without a home, wandering from place to place assisting the local rulers with various problems. He took a retinue of Arcadians with him acquiring also over time a family of grown sons, the Heraclidae. He continued this mode of life even after completing the 12 labors. The legend has it that he became involved with Achaean Sparta when the family of king Tyndareus was unseated and driven into exile by Hippocoön and his family, who in the process happened to kill the son of a friend of Heracles. The latter and his retinue assaulted Sparta, taking it back from Hippocoön. He recalled Tyndareus, set him up as a guardian regent, and instructed him to turn the kingdom over to any descendants of his that should claim it. Heracles went on with the way of life to which he had become accustomed, which was by today's standards that of a mercenary, as he was being paid for his assistance. Subsequently he founded a colony in Aetolia, then in Trachis.
After displacing the Dryopes, he went to the assistance of the Dorians, who lived in a land called Hestiaeotis under king Aegimius and were campaigning against the numerically superior Lapithae. The Dorians promised him 1/3 of Doris (which they did not yet possess). He asked Aegimius to keep his share of the land "in trust" until it should be claimed by a descendant. He went on to further adventures but was poisoned by his jealous wife, Deianeira. He immolated himself in full armor dressed for combat and "passed from among men into the company of the gods."
Strabo.
Strabo, who depends of course on the books available to him, goes on to elaborate:
Of these peoples, according to Staphylus, the Dorians occupy the part toward the east, the Cydonians the western part, the Eteo-Cretans the southern; and to these last belongs the town Praisos, where is the temple of the Dictaean Zeus; whereas the other peoples, since they were more powerful, dwelt in the plains. Now it is reasonable to suppose that the Eteo-Cretans and the Cydonians were autochthonous, and that the others were foreigners ...
Beside this sole reference to Dorians in Crete, the mention of the "Iliad" on the Heraclid Tlepolemus, a warrior on the side of Achaeans and colonist of three important Dorian cities in Rhodes has been also regarded as a later interpolation
See also.
Language
Mythology
History
List of Dorian states

</doc>
<doc id="8217" url="http://en.wikipedia.org/wiki?curid=8217" title="Declaration of the Rights of Man and of the Citizen">
Declaration of the Rights of Man and of the Citizen

The Declaration of the Rights of Man and of the Citizen (French: "Déclaration des droits de l'homme et du citoyen"), passed by France's National Constituent Assembly in August 1789, is a fundamental document of the French Revolution and in the history of human rights. The Declaration was directly influenced by Thomas Jefferson, working with General Lafayette, who introduced it. Influenced also by the doctrine of "natural right", the rights of man are held to be universal: valid at all times and in every place, pertaining to human nature itself. It became the basis for a nation of free individuals protected equally by law. It is included in the preamble of the constitutions of both the Fourth French Republic (1946) and Fifth Republic (1958) and is still current. Inspired in part by the American Revolution, and also by the Enlightenment philosophers, the Declaration was a core statement of the values of the French revolution and had a major impact on the development of liberty and democracy in Europe and worldwide.
The declaration, together with the American Declaration of Independence, Constitution, and Bill of Rights, inspired the 1948 United Nations Universal Declaration of Human Rights for a large part.
History.
The inspiration and content of the document emerged largely from the ideals of the American Revolution.
The key drafts were prepared by Lafayette, working at times with his close friend Thomas Jefferson, who drew heavily upon The Virginia Declaration of Rights, drafted in May 1776 by George Mason (which was based in part on the English Bill of Rights 1689), as well as Jefferson's own drafts for the American Declaration of Independence. In August 1789, Honoré Mirabeau played a central role in conceptualizing and drafting the Declaration of the Rights of Man and of the Citizen.
The last article of the Declaration of the Rights of Man and the Citizen was adopted on 26 August 1789 by the National Constituent Assembly, during the period of the French Revolution, as the first step toward writing a constitution for France. Inspired by the Enlightenment, the original version of the Declaration was discussed by the representatives on the basis of a 24 article draft proposed by , led by . The draft was later modified during the debates. A second and lengthier declaration, known as the Declaration of the Rights of Man and Citizen of 1793, was written in 1793 but never formally adopted.
Philosophical and theoretical context.
The concepts in the Declaration come from the philosophical and political duties of the Enlightenment, such as individualism, the general will, the social contract as theorized by the French philosopher Rousseau, and the separation of powers espoused by the Baron de Montesquieu. As can be seen in the texts, the French declaration is heavily influenced by the political philosophy of the Enlightenment, by Enlightenment principles of human rights, and by the U.S. Declaration of Independence which preceded it (4 July 1776). Thomas Jefferson—the primary author of the U.S. Declaration of Independence—was at the time in France as a U.S. diplomat, and worked closely with Lafayette in designing a bill of rights for France. In the ratification by the states of the U.S. Constitution in 1788, critics had demanded a written Bill of Rights. In response, James Madison's proposal for a U.S. Bill of Rights was introduced in New York on 8 June 1789, 11 weeks before the French declaration. Considering the 6 to 8 weeks it took news to cross the Atlantic, it is possible that the French knew of the American text. But, as Lafebvre notes, both texts emerged from the same shared intellectual heritage. The same people took part in shaping both documents; Lafayette admired Jefferson, and Jefferson in turn found Lafayette useful, writing in 1787 that Lafayette was "a most valuable auxiliary to me. His zeal is unbounded, & his weight with those in power, great." Historian Iain McLean concludes that Jefferson worked hard to influence the French Declaration and that Lafayette was "the ideal tool for Jefferson's interests as they broadened from American trade to French politics."
The declaration is in the spirit of "secular natural law", which does not base itself on religious doctrine or authority, in contrast with traditional natural law theory, which does.
The declaration defines a single set of individual and collective rights for all men. Influenced by the doctrine of natural rights, these rights are held to be universal and valid in all times and places. For example, "Men are born and remain free and equal in rights. Social distinctions may be founded only upon the general good." They have certain natural rights to property, to liberty, and to life. According to this theory, the role of government is to recognize and secure these rights. Furthermore, government should be carried on by elected representatives.
At the time of writing, the rights contained in the declaration were only awarded to men. Furthermore, the declaration was a statement of vision rather than reality. The declaration was not deeply rooted in either the practice of the West or even France at the time. The declaration emerged in the late 18th century out of war and revolution. It encountered opposition as democracy and individual rights were frequently regarded as synonymous with anarchy and subversion. The declaration embodies ideals and aspirations towards which France pledged to struggle in the future.
Substance.
The Declaration is introduced by a preamble describing the fundamental characteristics of the rights which are qualified as being "natural, unalienable and sacred" and consisting of "simple and incontestable principles" on which citizens could base their demands. In the second article, "the natural and imprescriptible rights of man" are defined as "liberty, property, security and resistance to oppression". It called for the destruction of aristocratic privileges by proclaiming an end to feudalism and to exemptions from taxation, freedom and equal rights for all human beings (referred to as "Men"), and access to public office based on talent. The monarchy was restricted, and all citizens were to have the right to take part in the legislative process. Freedom of speech and press were declared, and arbitrary arrests outlawed.
The Declaration also asserted the principles of popular sovereignty, in contrast to the divine right of kings that characterized the French monarchy, and social equality among citizens, "All the citizens, being equal in the eyes of the law, are equally admissible to all public dignities, places, and employments, according to their capacity and without distinction other than that of their virtues and of their talents," eliminating the special rights of the nobility and clergy.
Articles:
Article I - Men are born and remain free and equal in rights. Social distinctions can be founded only on the common good.
Article II - The goal of any political association is the conservation of the natural and imprescriptible rights of man. These rights are liberty, property, safety and resistance against oppression.
Article III - The principle of any sovereignty resides essentially in the Nation. No body, no individual can exert authority which does not emanate expressly from it.
Article IV - Liberty consists of doing anything which does not harm others: thus, the exercise of the natural rights of each man has only those borders which assure other members of the society the enjoyment of these same rights. These borders can be determined only by the law.
Article V - The law has the right to forbid only actions harmful to society. Anything which is not forbidden by the law cannot be impeded, and no one can be constrained to do what it does not order.
Article VI - The law is the expression of the general will. All the citizens have the right of contributing personally or through their representatives to its formation. It must be the same for all, either that it protects, or that it punishes. All the citizens, being equal in its eyes, are equally admissible to all public dignities, places and employments, according to their capacity and without distinction other than that of their virtues and of their talents.
Article VII - No man can be accused, arrested nor detained but in the cases determined by the law, and according to the forms which it has prescribed. Those who solicit, dispatch, carry out or cause to be carried out arbitrary orders, must be punished; but any citizen called or seized under the terms of the law must obey at once; he renders himself culpable by resistance.
Article VIII - The law should establish only penalties that are strictly and evidently necessary, and no one can be punished but under a law established and promulgated before the offense and legally applied.
Article IX - Any man being presumed innocent until he is declared culpable, if it is judged indispensible to arrest him, any rigor which would not be necessary for the securing of his person must be severely reprimanded by the law.
Article X - No one may be disturbed for his opinions, even religious ones, provided that their manifestation does not trouble the public order established by the law.
Article XI - The free communication of thoughts and of opinions is one of the most precious rights of man: any citizen thus may speak, write, print freely, except to respond to the abuse of this liberty, in the cases determined by the law.
Article XII - The guarantee of the rights of man and of the citizen necessitates a public force: this force is thus instituted for the advantage of all and not for the particular utility of those in whom it is trusted.
Article XIII - For the maintenance of the public force and for the expenditures of administration, a common contribution is indispensable; it must be equally distributed between all the citizens, according to their ability to pay.
Article XIV - Each citizen has the right to ascertain, by himself or through his representatives, the need for a public tax, to consent to it freely, to know the uses to which it is put, and of determining the proportion, basis, collection, and duration.
Article XV - The society has the right of requesting account from any public agent of its administration.
Article XVI - Any society in which the guarantee of rights is not assured, nor the separation of powers determined, has no Constitution.
Article XVII - Property being an inviolable and sacred right, no one can be deprived of private usage, if it is not when the public necessity, legally noted, evidently requires it, and under the condition of a just and prior indemnity.
Active and passive citizenship.
While the French Revolution provided rights to a larger portion of the population, there remained a distinction between those who obtained the political rights in the Declaration of the Rights of Man and Citizen and those who did not. Those who were deemed to hold these political rights were called active citizens. Active citizenship was granted to men who were French, at least 25 years old, paid taxes equal to three days work, and could not be defined as servants . This meant that at the time of the Declaration only male property owners held these rights. The deputies in the National Assembly believed that only those who held tangible interests in the nation could make informed political decisions. This distinction directly affects articles 6, 12, 14, and 15 of the Declaration of the Rights of Man and Citizen as each of these rights is related to the right to vote and to participate actively in the government. With the decree of 29 October 1789, the term active citizen became embedded in French politics.
The concept of passive citizens was created to encompass those populations that had been excluded from political rights in the Declaration of the Rights of Man and Citizen. Because of the requirements set down for active citizens, the vote was granted to approximately 4.3 million Frenchmen. out of a population of around 29 million. These omitted groups included women, slaves, children, and foreigners. As these measures were voted upon by the General Assembly, they limited the rights of certain groups of citizens while implementing the democratic process of the new French Republic (1792–1804). This legislation, passed in 1789, was amended by the creators of the Constitution of 1795 in order to eliminate the label of active citizen. The power to vote was then, however, to be granted solely to substantial property owners.
Tensions arose between active and passive citizens throughout the Revolution. This happened when passive citizens started to call for more rights, or when they openly refused to listen to the ideals set forth by active citizens. This clearly demonstrates the difference that existed between the active and passive citizens along with the tensions associated with such differences. In the cartoon, a passive citizen is holding a spade and a wealthy landowning active citizen is ordering the passive citizens to go to work. The act appears condescending to the passive citizen and it revisits the reasons why the French Revolution began in the first place.
Women, in particular, were strong passive citizens who played a significant role in the Revolution. Olympe de Gouges penned her "Declaration of the Rights of Woman and the Female Citizen" in 1791 and drew attention to the need for gender equality. By supporting the ideals of the French Revolution and wishing to expand them to women, she represented herself as a revolutionary citizen. Madame Roland also established herself as an influential figure throughout the Revolution. She saw women of the French Revolution as holding three roles; "inciting revolutionary action, formulating policy, and informing others of revolutionary events." By working with men, as opposed to working separate from men, she may have been able to further the fight of revolutionary women. As players in the French Revolution, women occupied a significant role in the civic sphere by forming social movements and participating in popular clubs, allowing them societal influence, despite their lack of direct political influence.
Women's rights.
The Declaration recognized many rights as belonging to citizens (who could only be male). This was despite the fact that after The March on Versailles on 5 October 1789, women presented the Women's Petition to the National Assembly in which they proposed a decree giving women equal rights. In 1790, Nicolas de Condorcet and Etta Palm d'Aelders unsuccessfully called on the National Assembly to extend civil and political rights to women. Condorcet declared that "he who votes against the right of another, whatever the religion, color, or sex of that other, has henceforth abjured his own". The French Revolution did not lead to a recognition of women's rights and this prompted Olympe de Gouges to publish the Declaration of the Rights of Woman and the Female Citizen in September 1791.
The Declaration of the Rights of Woman and the Female Citizen is modelled on the Declaration of the Rights of Man and of the Citizen and is ironic in formulation and exposes the failure of the French Revolution, which had been devoted to equality. It states that:
“This revolution will only take effect when all women become fully aware of their deplorable condition, and of the rights they have lost in society”.
The Declaration of the Rights of Woman and the Female Citizen follows the seventeen articles of the Declaration of the Rights of Man and of the Citizen point for point and has been described by Camille Naish as "almost a parody... of the original document". The first article of the Declaration of the Rights of Man and of the Citizen proclaims that "Men are born and remain free and equal in rights. Social distinctions may be based only on common utility." The first article of Declaration of the Rights of Woman and the Female Citizen replied: "Woman is born free and remains equal to man in rights. Social distinctions may only be based on common utility".
De Gouges also draws attention to the fact that under French law women were fully punishable, yet denied equal rights, declaring "Women have the right to mount the scaffold, they must also have the right to mount the speaker's rostrum".
Slavery.
The declaration did not revoke the institution of slavery, as lobbied for by Jacques-Pierre Brissot's "Les Amis des Noirs" and defended by the group of colonial planters called the Club Massiac because they met at the Hôtel Massiac. Despite the lack of explicit mention of slavery in the Declaration, slave uprisings in Saint-Domingue in the Haitian Revolution took inspiration from its words, as discussed in C. L. R. James' history of the Haitian Revolution, "The Black Jacobins".
Deplorable conditions for the thousands of slaves in Saint-Domingue, the most profitable slave colony in the world, led to the uprisings which would be known as the first successful slave revolt in the New World. Slavery in the French colonies was abolished by the Convention dominated by the Jacobins in 1794. However, Napoleon reinstated it in 1802. In 1804, the colony of Saint-Domingue became an independent state, the Republic of Haiti.
Legacy.
The Declaration has also influenced and inspired rights-based liberal democracy throughout the world. It was translated as soon as 1793–1794 by Colombian Antonio Nariño, who published it despite the Inquisition and was sentenced to be imprisoned for ten years for doing so. In 2003, the document was listed on UNESCO's Memory of the World register.
Constitution of the French Fifth Republic.
According to the preamble of the Constitution of the French Fifth Republic (adopted on 4 October 1958, and the current constitution), the principles set forth in the Declaration have constitutional value. Many laws and regulations have been cancelled because they did not comply with those principles as interpreted by the Conseil Constitutionnel ("Constitutional Council of France") or by the "Conseil d'État" ("Council of State").

</doc>
<doc id="8218" url="http://en.wikipedia.org/wiki?curid=8218" title="Dennis Ritchie">
Dennis Ritchie

Dennis MacAlistair Ritchie (September 9, 1941 – October 12, 2011) was an American computer scientist. He created the C programming language and, with long-time colleague Ken Thompson, the Unix operating system. Ritchie and Thompson received the Turing Award from the ACM in 1983, the Hamming Medal from the IEEE in 1990 and the National Medal of Technology from President Clinton in 1999. Ritchie was the head of Lucent Technologies System Software Research Department when he retired in 2007. He was the "R" in K&R C and commonly known by his username dmr.
Personal life.
Ritchie was born in Bronxville, New York. His father was Alistair E. Ritchie, a longtime Bell Labs scientist and co-author of "The Design of Switching Circuits" on switching circuit theory. He moved with his family to Summit, New Jersey, as a child, where he graduated from Summit High School. He graduated from Harvard University with degrees in physics and applied mathematics.
Career.
In 1967, Ritchie began working at the Bell Labs Computing Sciences Research Center, and in 1968, he defended his PhD thesis on "Program Structure and Computational Complexity" at Harvard under the supervision of Patrick C. Fischer. However, Ritchie never officially received his PhD degree. 
Ritchie was best known as the creator of the C programming language, a key developer of the Unix operating system, and co-author of the book "The C Programming Language", and was the 'R' in "K&R" (a common reference to the book's authors Kernighan and Ritchie). Ritchie worked together with Ken Thompson, the scientist credited with writing the original Unix; one of Ritchie's most important contributions to Unix was its porting to different machines and platforms. They were so influential on Research Unix that Doug McIlroy later wrote, "The names of Ritchie and Thompson may safely be assumed to be attached to almost everything not otherwise attributed."
The C language is widely used today in application, operating system, and embedded system development, and its influence is seen in most modern programming languages. Unix has also been influential, establishing concepts and principles that are now precepts of computing.
Views on computing.
In an interview from 1999, Dennis Ritchie clarifies that he sees Linux and BSD operating systems as a continuation of the basis of the Unix operating system, and as derivatives of Unix:
I think the Linux phenomenon is quite delightful, because it draws so strongly on the basis that Unix provided. Linux seems to be among the healthiest of the direct Unix derivatives, though there are also the various BSD systems as well as the more official offerings from the workstation and mainframe manufacturers.
In the same interview, he states that he views both Unix and Linux as "the continuation of ideas that were started by Ken and me and many others, many years ago."
Awards.
In 1983, Ritchie and Thompson received the Turing Award for their development of generic operating systems theory and specifically for the implementation of the UNIX operating system. Ritchie's Turing Award lecture was titled "Reflections on Software Research". In 1990, both Ritchie and Thompson received the IEEE Richard W. Hamming Medal from the Institute of Electrical and Electronics Engineers (IEEE), "for the origination of the UNIX operating system and the C programming language".
In 1997, both Ritchie and Thompson were made Fellows of the Computer History Museum, "for co-creation of the UNIX operating system, and for development of the C programming language."
On April 21, 1999, Thompson and Ritchie jointly received the National Medal of Technology of 1998 from President Bill Clinton for co-inventing the UNIX operating system and the C programming language which, according to the citation for the medal, "led to enormous advances in computer hardware, software, and networking systems and stimulated growth of an entire industry, thereby enhancing American leadership in the Information Age".
In 2005, the Industrial Research Institute awarded Ritchie with its Achievement Award in recognition of his contribution to science and technology, and to society generally, with his development of the Unix operating system.
In 2011, Ritchie, along with Thompson, was awarded the Japan Prize for Information and Communications for his work in the development of the Unix operating system.
Death and legacy.
Ritchie was found dead on October 12, 2011, at the age of 70 at his home in Berkeley Heights, New Jersey, where he lived alone. First news of his death came from his former colleague, Rob Pike. The cause and exact time of death have not been disclosed. He had been in frail health for several years following treatment for prostate cancer and heart disease. His death came a week after the death of Steve Jobs but did not receive as much media coverage.
Following Ritchie's death, computer historian Paul E. Ceruzzi stated:
Ritchie was under the radar. His name was not a household name at all, but... if you had a microscope and could look in a computer, you'd see his work everywhere inside.
In an interview shortly after Ritchie's death, long time colleague Brian Kernighan said Ritchie never expected C to be so significant.
Kernighan reminded readers of how important a role C and UNIX had played in the development of later high-profile projects, like the iPhone.
Other testimonials to his influence followed.
The Fedora 16 Linux distribution, which was released about a month after he died, was dedicated to his memory. FreeBSD 9.0, released January 12, 2012 was also dedicated in his memory.
External links.
Listen to this article ()
This audio file was created from a revision of the "Dennis Ritchie" article dated June 17, 2006, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="8219" url="http://en.wikipedia.org/wiki?curid=8219" title="December 16">
December 16

December 16 is the day of the year in the Gregorian calendar.

</doc>
<doc id="8220" url="http://en.wikipedia.org/wiki?curid=8220" title="Doctrine and Covenants">
Doctrine and Covenants

The Doctrine and Covenants (sometimes abbreviated and cited as D&C or D. and C.) is a part of the open scriptural canon of several denominations of the Latter Day Saint movement. Originally published in 1835 as Doctrine and Covenants of the Church of the Latter Day Saints: Carefully Selected from the Revelations of God, editions of the book continue to be printed mainly by The Church of Jesus Christ of Latter-day Saints (LDS Church) and the Community of Christ (formerly the Reorganized Church of Jesus Christ of Latter Day Saints (RLDS Church)).
The book originally contained two parts: a sequence of lectures setting forth basic church doctrine, followed by a compilation of important revelations, or "covenants" of the church: thus the name "Doctrine and Covenants". The "doctrine" portion of the book, however, has been removed by both the LDS Church and the Community of Christ. The remaining portion of the book contains revelations on numerous topics, most of which were dictated by the movement's founder Joseph Smith, supplemented by materials periodically added by each denomination.
Controversy has existed between the two largest denominations of the Latter Day Saint movement over some sections added to the 1876 LDS edition, attributed to founder Smith. Whereas the LDS Church believes these sections to have been revelations to Smith, the RLDS Church traditionally disputed their authenticity.
History.
The Doctrine and Covenants was first published in 1835 as a later version of the Book of Commandments, which had been partially printed in 1833. This earlier book contained 65 early revelations to church leaders, including Joseph Smith and Oliver Cowdery. Before many copies of the book could be printed, the printing press and most of the printed copies were destroyed by a mob in Missouri.
On September 24, 1834, a committee was appointed by the general assembly of the church to organize a new volume containing the most significant revelations. This committee of Presiding Elders, consisting of Smith, Cowdery, Sidney Rigdon, and Frederick G. Williams, began to review and revise numerous revelations for inclusion in the new work. The committee eventually organized the book into two parts: a "Doctrine" part and a "Covenants" part.
The "Doctrine" part of the book consisted of a theological course now called the "Lectures on Faith". The lectures were a series of doctrinal courses used in the School of the Prophets which had recently been completed in Kirtland, Ohio. According to the committee, these lectures were included in the compilation "in consequence of their embracing the important doctrine of salvation."
The "Covenants" part of the book, labeled "Covenants and Commandments of the Lord, to his servants of the church of the Latter Day Saints", contained a total of 103 revelations. These 103 revelations were said to "contain items or principles for the regulation of the church, as taken from the revelations which have been given since its organization, as well as from former ones." Each of the 103 revelations was assigned a "section number"; however, section 66 was mistakenly used twice. Thus, the sections of the original work were numbered only to 102.
On February 17, 1835, after the committee had selected the book's contents, the committee wrote that the resulting work represents "our belief, and when we say this, humbly trust, the faith and principles of this society as a body."
The book was first introduced to the church body in a general conference on August 17, 1835. Smith and Williams, two of the Presiding Elders on the committee, were absent, but Cowdery and Rigdon were present. The church membership at the time had not yet seen the Doctrine and Covenants manuscript as it had been compiled and revised solely by the committee; however, various church members who were familiar with the work "bore record" of the book's truth. At the end of the conference, the church "by a unanimous vote" agreed to accept the compilation as "the doctrine and covenants of their faith" and to make arrangements for its printing.
In 1835, the book was printed and published under the title "Doctrine and Covenants of the Church of the Latter Day Saints: Carefully Selected from the Revelations of God".
The Church of Jesus Christ of Latter-day Saints editions.
In The Church of Jesus Christ of Latter-day Saints (LDS Church), The Doctrine and Covenants of The Church of Jesus Christ of Latter-day Saints stands alongside the Bible, the Book of Mormon, and The Pearl of Great Price as scripture. Together the LDS Church's scriptures are referred to as the "standard works". The LDS Church's version of the Doctrine and Covenants is described by the church as "containing revelations given to Joseph Smith, the Prophet, with some additions by his successors in the Presidency of the Church."
Sections added to LDS edition.
The 138 sections in LDS Church's Doctrine and Covenants break down as follows:
The following sections are not revelations, but letters, reports, statements, and other similar documents: 102, 123, 127–131, 134, and 135
In 1844, the church added eight sections not included in the 1835 edition. In the current edition, these added sections are numbered 103, 105, 112, 119, 124, 127, 128, and 135.
In 1876, a new LDS Church edition renumbered most of the sections in a roughly chronological order instead of the earlier topical order, and included 26 sections not included in previous editions, now numbered as sections 2, 13, 77, 85, 87, 108–111, 113–118, 120–123, 125, 126, 129–132, and 136. Previous editions had been divided into verses with the early versifications generally following the paragraph structure of the original text. It was with the 1876 edition that the currently used versification was first employed.
During the 1880s, five foreign editions contained two revelations to John Taylor that were received in 1882 and 1883; these revelations "set in order" the priesthood, gave more clarification about the roles of priesthood offices—especially the seventy—and required "men who...preside over my priesthood" to live plural marriage in order to qualify to hold their church positions. Probably due to the LDS Church's change in attitude to polygamy in 1890, these sections were not included in future English editions of the Doctrine and Covenants.
In 1930, a small volume edited by apostle James E. Talmage titled "Latter-day Revelations" was published, which was a highly edited selective version of the Doctrine and Covenants. Talmage wrote that the book's purpose was "to make the strictly doctrinal parts of the Doctrine and Covenants of easy access and reduce its bulk" by including only "the sections comprising scriptures of general and enduring value". Ninety-five of the sections of the Doctrine and Covenants were completely omitted—most notably section 132 on plural and celestial marriage—along with parts of 21 others. Twenty complete sections were retained along with parts of 21 others. Fundamentalist Mormons were offended, particularly at the exclusion of section 132, and accused the church of "changing the scriptures." As a result, church president Heber J. Grant ordered the withdrawal of the book from sale with the remaining copies shredded in order to "avoid further conflict with the fundamentalists".
Sections 137 and 138 were added to the LDS Church's 1981 edition of the Doctrine and Covenants, which is the edition currently in use by the church. These were accounts of two visions, one from Joseph Smith in 1837 and the other from his nephew, Joseph F. Smith, in 1918. The revelations were earlier accepted as scripture when added to the Pearl of Great Price in April 1976. No new revelatory sections have been added since 1981.
The LDS Church's 1981 edition also contains two "Official Declarations" at the book's conclusion. The 1890 Official Declaration 1 ended the church-authorized practice of plural marriage, and the 1978 Official Declaration 2 announces the opening of priesthood ordination to all worthy male members without regard to race or color. The two Official Declarations are not revelations, but they serve as the formal announcements that a revelation was received. In neither case is the entire revelation included in the Doctrine and Covenants. The text of Official Declaration 1 has been included in every LDS Church printing of the Doctrine and Covenants since 1908.
Portions removed from the LDS edition.
In 1876, section 101 from the 1835 edition (and subsequent printings) was removed. Section 101 was a "Statement on Marriage" as adopted by a, 1835 conference of the church, and contained the following text: 
Inasmuch as this Church of Christ has been reproached with the crime of fornication and polygamy, we declare that we believe that one man should have one wife, and one woman but one husband, except in the case of death, when either is at liberty to marry again.
This section was removed because it had been superseded by section 132 of the modern LDS edition, recorded in 1843, which contains a revelation received by Joseph Smith on eternal marriage and plural marriage, the principles of which can be dated to as early as 1831.
In 1921, the LDS Church removed the "Lectures on Faith" portion of the book, with an explanation that the lectures "were never presented to nor accepted by the Church as being otherwise than theological lectures or lessons". The lectures contain theology concerning the Godhead and emphasize the importance of faith and works.
Until 1981, editions of the book used code names for certain people and places in those sections that dealt with the United Order. The 1981 LDS edition replaced these with the real names, relegating the code names to footnotes. The Community of Christ edition still uses the code names.
Community of Christ editions.
Officials of Community of Christ (formerly known as the Reorganized Church of Jesus Christ of Latter Day Saints) first published an edition of the Doctrine and Covenants in 1864, based on the previous 1844 edition. A general conference of the church in 1878 approved a resolution that declared that the revelations of the Prophet-President Joseph Smith III had equal standing to those previously included in the work. Since that time, the church has continued to add sections to its edition of the Doctrine and Covenants, containing the revelations of succeeding Prophet–Presidents. The most recent addition was formally authorized on April 14, 2010, after being presented to the church for informal consideration on January 17, 2010. The numbers of the sections and versification differ from the edition published by the LDS Church and both modern editions differ from the original 1835 edition numeration.
Regarding the contents of the Doctrine and Covenants, the church has stated: "As with other books of scripture, the various passages vary in their enduring quality."
Sections added to Community of Christ edition.
The 166 sections of the Community of Christ's Doctrine and Covenants break down as follows:
The following sections are not revelations, but letters, reports, statements, and other similar documents: 99, 108A, 109–113, and 123.
Based on the above, the number of revelations (accounting for sections that are not revelations) presented by each Community of Christ president, are as follows:
Portions removed from the Community of Christ edition.
Community of Christ removed the "Lectures on Faith" in 1897. The 1970 World Conference concluded that several sections that had been added between the 1835 and 1844 editions—mainly dealing with the subjects of temple worship and baptism for the dead—had been published without proper approval of a church conference. As a result, the World Conference removed sections 107, 109, 110, 113, and 123 to a historical appendix, which also includes documents that were never published as sections. Of these, only section 107 was a revelation. The World Conference of 1990 subsequently removed the entire appendix from the Doctrine and Covenants. Section 108A contained the minutes of a business meeting, which, because of its historical nature, was moved to the Introduction in the 1970s. After 1990, the Introduction was updated, and what was section 108A was removed entirely.
Doctrinal developments in the Community of Christ edition.
The ongoing additions to the Community of Christ edition provide a record of the leadership changes and doctrinal developments within the denomination. When W. Grant McMurray became Prophet-President, he declared that instruction specific to leadership changes would no longer be included, so that the focus of the work could be more doctrinal in nature, and less administrative. The record of these leadership changes are still maintained in the form of published "letters of counsel." Prophet–President Stephen M. Veazey has conformed to this pattern. Although these letters are not formally published in the Doctrine and Covenants, they are still deemed to be inspired, and are dealt with in the same manner that revelations are (that is, they must be deliberated and approved by the voting members of a World Conference).
A modern revelation that resulted in some "disaffection" and "led to intense conflict in scattered areas of the RLDS Church" is contained in the Community of Christ version's section 156, presented by Prophet–President Wallace B. Smith and added in 1984, which called for the ordination of women to the priesthood and set out the primary purpose of temples to be "the pursuit of peace". A resulting schism over the legitimacy of these change led to the formation of the Restoration Branches movement, the Restoration Church of Jesus Christ of Latter Day Saints and the Remnant Church of Jesus Christ of Latter Day Saints.
While some of the prose in the new revelations seems designed to guide the denomination on matters of church governance and doctrine, others are seen as inspirational. One such example can be cited from section 161, presented as counsel to the church by W. Grant McMurray in 1996: "Become a people of the Temple—those who see violence but proclaim peace, who feel conflict yet extend the hand of reconciliation, who encounter broken spirits and find pathways for healing."
Editions used by other denominations.
The Church of Jesus Christ of Latter Day Saints (Strangite) uses the 1846 edition that was published in Nauvoo, Illinois; this version is virtually identical to the 1844 edition. Most recently a facsimile reprint was produced for the church at Voree, Wisconsin by Richard Drew in 1993.
The Church of Christ (Temple Lot) contends that the thousands of changes made to the original revelations as published in the Book of Commandments (including the change of the church's name) are not doctrinal and result from Joseph Smith's fall from his original calling. As a result, the Church of Christ (Temple Lot) prefers to use reprints of the Book of Commandments text.
The Church of Jesus Christ (Cutlerite) accepts the 1844 edition of the Doctrine and Covenants, including the Lectures on Faith, which it insists are as much inspired as the revelations themselves.
The Restoration Branches generally use the older RLDS Church Doctrine and Covenants, typically sections 1–144.
The Remnant Church of Jesus Christ of Latter Day Saints also uses the older RLDS Church version of the Doctrine and Covenants up to section 144, and also contains new revelations from their prophet–president Frederick Niels Larsen. These new sections are R 145–R 153.
Chart comparison of editions.
The following chart compares the current editions of the Doctrine and Covenants used by the LDS Church (LDS ed.) and the Community of Christ (CofC ed.) with the 1833 Book of Commandments (BofC), the 1835 edition published in Kirtland, and the 1844 edition published in Nauvoo. Unless otherwise specified, the document is styled a "revelation" of the person delivering it.

</doc>
<doc id="8221" url="http://en.wikipedia.org/wiki?curid=8221" title="Death">
Death

Death is the termination of all biological functions that sustain a living organism. Phenomena which commonly bring about death include biological aging (), predation, malnutrition, disease, suicide, homicide, starvation, dehydration, and accidents or trauma resulting in terminal injury. Bodies of living organisms begin to decompose shortly after death. Death has commonly been considered a sad or unpleasant occasion, due to the termination of bonds with or affection for the being that has died, or having fear of death, necrophobia, anxiety, sorrow, grief, emotional pain, depression, sympathy, compassion, solitude, or saudade.
Etymology.
The word death comes from Old English deað, which in turn comes from Proto-Germanic *dauthuz (reconstructed by etymological analysis). This comes from the Proto-Indo-European stem *dheu- meaning the "Process, act, condition of dying".
Associated terms.
The concept and symptoms of death, and varying degrees of delicacy used in discussion in public forums, have generated numerous scientific, legal, and socially acceptable terms or euphemisms for death. When a person has died, it is also said they have "passed away", "passed on", "expired", or are "gone", among numerous other socially accepted, religiously specific, slang, and irreverent terms. Bereft of life, the dead person is then a "corpse", "cadaver", a "body", a "set of remains", and when all flesh has rotted away, a skeleton. The terms "carrion" and "carcass" can also be used, though these more often connote the remains of non-human animals. As a polite reference to a dead person, it has become common practice to use the participle form of "decease", as in "the deceased"; the noun form is "decedent". The ashes left after a cremation are sometimes referred to by the neologism "cremains", a portmanteau of "cremation" and "remains".
Senescence.
Almost all animals who survive external hazards to their biological functioning eventually die from biological aging, known in life sciences as “”. Some organisms experience negligible senescence, even exhibiting biological immortality. These include the jellyfish "Turritopsis dohrnii", the hydra, and the planarian. Unnatural causes of death include suicide and homicide. From all causes, roughly 150,000 people die around the world each day.<ref name="doi10.2202/1941-6008.1011"></ref> Of these, two thirds die directly or indirectly due to senescence, but in industrialized countries—such as the United States, the United Kingdom, and Germany—the rate approaches 90%, i.e., nearly nine out of ten of all deaths are related to senescence.
Physiological death is now seen as a process, more than an event: conditions once considered indicative of death are now reversible. Where in the process a dividing line is drawn between life and death depends on factors beyond the presence or absence of vital signs. In general, clinical death is neither necessary nor sufficient for a determination of legal death. A patient with working heart and lungs determined to be brain dead can be pronounced legally dead without clinical death occurring. As scientific knowledge and medicine advance, a precise medical definition of death becomes more problematic.
Signs of biological death.
Signs of death or strong indications that a warm-blooded animal is no longer alive are:
Diagnosis.
Problems of definition.
The concept of death is a key to human understanding of the phenomenon. There are many scientific approaches to the concept. For example, brain death, as practiced in medical science, defines death as a point in time at which brain activity ceases.
One of the challenges in defining death is in distinguishing it from life. As a point in time, death would seem to refer to the moment at which life ends. However, determining when death has occurred requires drawing precise conceptual boundaries between life and death. This is problematic because there is little consensus over how to define life. This general problem applies to the particular challenge of defining death in the context of medicine.
It is possible to define life in terms of consciousness. When consciousness ceases, a living organism can be said to have died. One of the notable flaws in this approach, however, is that there are many organisms which are alive but probably not conscious (for example, single-celled organisms). Another problem is in defining consciousness, which has many different definitions given by modern scientists, psychologists and philosophers. Additionally, many religious traditions, including Abrahamic and Dharmic traditions, hold that death does not (or may not) entail the end of consciousness. In certain cultures, death is more of a process than a single event. It implies a slow shift from one spiritual state to another.
Other definitions for death focus on the character of cessation of something. In this context "death" describes merely the state where something has ceased, for example, life. Thus, the definition of "life" simultaneously defines death.
Historically, attempts to define the exact moment of a human's death have been problematic. Death was once defined as the cessation of heartbeat (cardiac arrest) and of breathing, but the development of CPR and prompt defibrillation have rendered that definition inadequate because breathing and heartbeat can sometimes be restarted. Events which were causally linked to death in the past no longer kill in all circumstances; without a functioning heart or lungs, life can sometimes be sustained with a combination of life support devices, organ transplants and artificial pacemakers.
Today, where a definition of the moment of death is required, doctors and coroners usually turn to "brain death" or "biological death" to define a person as being dead; people are considered dead when the electrical activity in their brain ceases. It is presumed that an end of electrical activity indicates the end of consciousness. However, suspension of consciousness must be permanent, and not transient, as occurs during certain sleep stages, and especially a coma. In the case of sleep, EEGs can easily tell the difference.
However, the category of "brain death" is seen by some scholars to be problematic. For instance, Dr. Franklin Miller, senior faculty member at the Department of Bioethics, National Institutes of Health, notes: "By the late 1990s, however, the equation of brain death with death of the human being was increasingly challenged by scholars, based on evidence regarding the array of biological functioning displayed by patients correctly diagnosed as having this condition who were maintained on mechanical ventilation for substantial periods of time. These patients maintained the ability to sustain circulation and respiration, control temperature, excrete wastes, heal wounds, fight infections and, most dramatically, to gestate fetuses (in the case of pregnant "brain-dead" women)."
Those people maintaining that only the neo-cortex of the brain is necessary for consciousness sometimes argue that only electrical activity should be considered when defining death. Eventually it is possible that the criterion for death will be the permanent and irreversible loss of cognitive function, as evidenced by the death of the cerebral cortex. All hope of recovering human thought and personality is then gone given current and foreseeable medical technology. However, at present, in most places the more conservative definition of death – irreversible cessation of electrical activity in the whole brain, as opposed to just in the neo-cortex – has been adopted (for example the Uniform Determination Of Death Act in the United States). In 2005, the Terri Schiavo case brought the question of brain death and artificial sustenance to the front of American politics.
Even by whole-brain criteria, the determination of brain death can be complicated. EEGs can detect spurious electrical impulses, while certain drugs, hypoglycemia, hypoxia, or hypothermia can suppress or even stop brain activity on a temporary basis. Because of this, hospitals have protocols for determining brain death involving EEGs at widely separated intervals under defined conditions.
Legal.
The death of a person has legal consequences that may vary between different jurisdictions.
A death certificate is issued in most jurisdictions, either by a doctor himself or by an administrative office upon presentation of a doctor's declaration of death.
Misdiagnosed.
There are many anecdotal references to people being declared dead by physicians and then "coming back to life", sometimes days later in their own coffin, or when embalming procedures are about to begin. From the mid-18th century onwards, there was an upsurge in the public's fear of being mistakenly buried alive, and much debate about the uncertainty of the signs of death. Various suggestions were made to test for signs of life before burial, ranging from pouring vinegar and pepper into the corpse's mouth to applying red hot pokers to the feet or into the rectum. Writing in 1895, the physician J.C. Ouseley claimed that as many as 2,700 people were buried prematurely each year in England and Wales, although others estimated the figure to be closer to 800.
In cases of electric shock, cardiopulmonary resuscitation (CPR) for an hour or longer can allow stunned nerves to recover, allowing an apparently dead person to survive. People found unconscious under icy water may survive if their faces are kept continuously cold until they arrive at an emergency room. This "diving response", in which metabolic activity and oxygen requirements are minimal, is something humans share with cetaceans called the mammalian diving reflex.
As medical technologies advance, ideas about when death occurs may have to be re-evaluated in light of the ability to restore a person to vitality after longer periods of apparent death (as happened when CPR and defibrillation showed that cessation of heartbeat is inadequate as a decisive indicator of death). The lack of electrical brain activity may not be enough to consider someone scientifically dead. Therefore, the concept of information-theoretic death has been suggested as a better means of defining when true death occurs, though the concept has few practical applications outside of the field of cryonics.
There have been some scientific attempts to bring dead organisms back to life, but with limited success. In science fiction scenarios where such technology is readily available, real death is distinguished from reversible death.
Cause.
The leading cause of human death in developing countries is infectious disease. The leading causes in developed countries are atherosclerosis (heart disease and stroke), cancer, and other diseases related to obesity and aging. By extremely wide margin, the largest unifying cause of death in the developed world is biological aging, leading to various complications known as aging-associated diseases. These conditions cause loss of homeostasis, leading to cardiac arrest, causing loss of oxygen and nutrient supply, causing irreversible deterioration of the brain and other tissues. Of the roughly 150,000 people who die each day across the globe, about two thirds die of age-related causes. In industrialized nations, the proportion is much higher, approaching 90%. With improved medical capability, dying has become a condition to be managed. Home deaths, once commonplace, are now rare in the developed world.
In developing nations, inferior sanitary conditions and lack of access to modern medical technology makes death from infectious diseases more common than in developed countries. One such disease is tuberculosis, a bacterial disease which killed 1.7M people in 2004. Malaria causes about 400–900M cases of fever and 1–3M deaths annually. AIDS death toll in Africa may reach 90–100M by 2025.
According to Jean Ziegler (United Nations Special Reporter on the Right to Food, 2000—Mar 2008), mortality due to malnutrition accounted for 58% of the total mortality rate in 2006. Ziegler says worldwide approximately 62M people died from all causes and of those deaths more than 36M died of hunger or diseases due to deficiencies in micronutrients.
Tobacco smoking killed 100 million people worldwide in the 20th century and could kill 1 billion people around the world in the 21st century, a WHO Report warned.
Many leading developed world causes of death can be postponed by diet and physical activity, but the accelerating incidence of disease with age still imposes limits on human longevity. The evolutionary cause of aging is, at best, only just beginning to be understood. It has been suggested that direct intervention in the aging process may now be the most effective intervention against major causes of death.
In 2012, suicide overtook car crashes for leading causes of human injury deaths in America, followed by poisoning, falls and murder. Causes of death are different in different parts of the world. In high-income and middle income countries nearly half up to more than two thirds of all people live beyond the age of 70 and predominantly die of chronic diseases. In low-income countries, where less than one in five of all people reach the age of 70, and more than a third of all deaths are among children under 15, people predominantly die of infectious diseases.
Autopsy.
An autopsy, also known as a "postmortem examination" or an "obduction", is a medical procedure that consists of a thorough examination of a human corpse to determine the cause and manner of a person's death and to evaluate any disease or injury that may be present. It is usually performed by a specialized medical doctor called a pathologist.
Autopsies are either performed for legal or medical purposes. A forensic autopsy is carried out when the cause of death may be a criminal matter, while a clinical or academic autopsy is performed to find the medical cause of death and is used in cases of unknown or uncertain death, or for research purposes. Autopsies can be further classified into cases where external examination suffices, and those where the body is dissected and an internal examination is conducted. Permission from next of kin may be required for internal autopsy in some cases. Once an internal autopsy is complete the body is generally reconstituted by sewing it back together. Autopsy is important in a medical environment and may shed light on mistakes and help improve practices.
A "necropsy" is an older term for a postmortem examination, unregulated, and not always a medical procedure. In modern times the term is more often used in the postmortem examination of the corpses of animals.
Life extension.
Life extension refers to an increase in maximum or average lifespan, especially in humans, by slowing down or reversing the processes of aging. Average lifespan is determined by vulnerability to accidents and age or lifestyle-related afflictions such as cancer, or cardiovascular disease. Extension of average lifespan can be achieved by good diet, exercise and avoidance of hazards such as smoking. Maximum lifespan is also determined by the rate of aging for a species inherent in its genes. Currently, the only widely recognized method of extending maximum lifespan is calorie restriction. Theoretically, extension of maximum lifespan can be achieved by reducing the rate of aging damage, by periodic replacement of damaged tissues, or by molecular repair or rejuvenation of deteriorated cells and tissues.
A United States poll found that religious people and irreligious people, as well as men and women and people of different economic classes have similar rates of support for life extension, while Africans and Hispanics have higher rates of support than white people. 38 percent of the polled said they would desire to have their aging process cured.
Researchers of life extension are a subclass of biogerontologists known as "biomedical gerontologists". They try to understand the nature of aging and they develop treatments to reverse aging processes or to at least slow them down, for the improvement of health and the maintenance of youthful vigor at every stage of life. Those who take advantage of life extension findings and seek to apply them upon themselves are called "life extensionists" or "longevists". The primary life extension strategy currently is to apply available anti-aging methods in the hope of living long enough to benefit from a complete cure to aging once it is developed.
Location.
Before about 1930, most people in Western countries died in their own homes, surrounded by family, and comforted by clergy, neighbors, and doctors making house calls. By the mid-20th century, half of all Americans died in a hospital. By the start of the 21st century, only about 20 to 25% of people in developed countries died outside of a medical institution. The shift away from dying at home, towards dying in a professionalized medical environment, has been termed the "Invisible Death".
Society and culture.
In society, the nature of death and humanity's awareness of its own mortality has for millennia been a concern of the world's religious traditions and of philosophical inquiry. This includes belief in resurrection or an afterlife (associated with Abrahamic religions), reincarnation or rebirth (associated with Dharmic religions), or that consciousness permanently ceases to exist, known as eternal oblivion (associated with atheism).
Commemoration ceremonies after death may include various mourning, funeral practices and ceremonies of honouring the deceased. The physical remains of a person, commonly known as a "corpse" or "body", are usually interred whole or cremated, though among the world's cultures there are a variety of other methods of mortuary disposal. In the English language, blessings directed towards a dead person include "rest in peace", or its initialism RIP.
Death is the center of many traditions and organizations; customs relating to death are a feature of every culture around the world. Much of this revolves around the care of the dead, as well as the afterlife and the disposal of bodies upon the onset of death. The disposal of human corpses does, in general, begin with the last offices before significant time has passed, and ritualistic ceremonies often occur, most commonly interment or cremation. This is not a unified practice; in Tibet, for instance, the body is given a sky burial and left on a mountain top. Proper preparation for death and techniques and ceremonies for producing the ability to transfer one's spiritual attainments into another body (reincarnation) are subjects of detailed study in Tibet. Mummification or embalming is also prevalent in some cultures, to retard the rate of decay.
Legal aspects of death are also part of many cultures, particularly the settlement of the deceased estate and the issues of inheritance and in some countries, inheritance taxation.
Capital punishment is also a culturally divisive aspect of death. In most jurisdictions where capital punishment is carried out today, the death penalty is reserved for premeditated murder, espionage, treason, or as part of military justice. In some countries, sexual crimes, such as adultery and sodomy, carry the death penalty, as do religious crimes such as apostasy, the formal renunciation of one's religion. In many retentionist countries, drug trafficking is also a capital offense. In China, human trafficking and serious cases of corruption are also punished by the death penalty. In militaries around the world courts-martial have imposed death sentences for offenses such as cowardice, desertion, insubordination, and mutiny.
Death in warfare and in suicide attack also have cultural links, and the ideas of dulce et decorum est pro patria mori, mutiny punishable by death, grieving relatives of dead soldiers and death notification are embedded in many cultures. Recently in the western world, with the increase in terrorism following the September 11 attacks, but also further back in time with suicide bombings, kamikaze missions in World War II and suicide missions in a host of other conflicts in history, death for a cause by way of suicide attack, and martyrdom have had significant cultural impacts.
Suicide in general, and particularly euthanasia, are also points of cultural debate. Both acts are understood very differently in different cultures. In Japan, for example, ending a life with honor by seppuku was considered a desirable death, whereas according to traditional Christian and Islamic cultures, suicide is viewed as a sin. Death is personified in many cultures, with such symbolic representations as the Grim Reaper, Azrael, the Hindu God Yama and Father Time.
In Brazil, a human death is counted officially when it is registered by existing family members at a cartório, a government-authorized registry. Before being able to file for an official death, the deceased must have been registered for an official birth at the cartório. Though a Public Registry Law guarantees all Brazilian citizens the right to register deaths, regardless of their financial means, of their family members (often children), the Brazilian government has not taken away the burden, the hidden costs and fees, of filing for a death. For many impoverished families, the indirect costs and burden of filing for a death lead to a more appealing, unofficial, local, cultural burial, which in turn raises the debate about inaccurate mortality rates.
Talking about death and witnessing it is a difficult issue with most cultures. Western societies may like to treat the dead with the utmost material respect, with an official embalmer and associated rites. Eastern societies (like India) may be more open to accepting it as a "fait accompli", with a funeral procession of the dead body ending in an open air burning-to-ashes of the same.
Death and consciousness.
Much interest and debate surround the question of what happens to one's consciousness as one's body dies. The belief in the permanent loss of consciousness after death is often called "eternal oblivion". Belief that consciousness is preserved after physical death is described by the term "afterlife".
In biology.
After death the remains of an organism become part of the biogeochemical cycle. Animals may be consumed by a predator or a scavenger. Organic material may then be further decomposed by detritivores, organisms which recycle detritus, returning it to the environment for reuse in the food chain. Examples of detritivores include earthworms, woodlice and dung beetles.
Microorganisms also play a vital role, raising the temperature of the decomposing matter as they break it down into yet simpler molecules. Not all materials need to be decomposed fully, however. Coal, a fossil fuel formed over vast tracts of time in swamp ecosystems, is one example.
Natural selection.
Contemporary evolutionary theory sees death as an important part of the process of natural selection. It is considered that organisms less adapted to their environment are more likely to die having produced fewer offspring, thereby reducing their contribution to the gene pool. Their genes are thus eventually bred out of a population, leading at worst to extinction and, more positively, making the process possible, referred to as speciation. Frequency of reproduction plays an equally important role in determining species survival: an organism that dies young but leaves numerous offspring displays, according to Darwinian criteria, much greater fitness than a long-lived organism leaving only one.
Extinction.
Extinction is the cessation of existence of a species or group of taxa, reducing biodiversity. The moment of extinction is generally considered to be the death of the last individual of that species (although the capacity to breed and recover may have been lost before this point). Because a species' potential range may be very large, determining this moment is difficult, and is usually done retrospectively. This difficulty leads to phenomena such as Lazarus taxa, where species presumed extinct abruptly "reappear" (typically in the fossil record) after a period of apparent absence. New species arise through the process of speciation, an aspect of evolution. New varieties of organisms arise and thrive when they are able to find and exploit an ecological niche – and species become extinct when they are no longer able to survive in changing conditions or against superior competition.
Evolution of aging and mortality.
Inquiry into the evolution of aging aims to explain why so many living things and the vast majority of animals weaken and die with age (notable exceptions being "Hydra" and the already cited jellyfish "Turritopsis dohrnii", which research shows to be biologically immortal). The evolutionary origin of senescence remains one of the fundamental puzzles of biology. Gerontology specializes in the science of human aging processes.
Organisms showing only asexual reproduction (e.g. bacteria, some protists, like the euglenoids and many amoebozoans) and unicellular organisms with sexual reproduction (colonial or not, like the volvocine algae "Pandorina" and "Chlamydomonas") are "immortal" at some extent, dying only due to external hazards, like being eaten or meeting with a fatal accident. However, in multicellular organisms (and also in multinucleate ciliates), with a Weismannist development, that is, with a division of labor between mortal somatic (body) cells and "immortal" germ (reproductive) cells, death becomes an essential part of life, at least for the somatic line.
The "Volvox" algae are among the simplest organisms to exhibit that division of labor between two completely different cell types, and as a consequence include death of somatic line as a regular, genetically regulated part of its life history.
References.
</dl>

</doc>
<doc id="8222" url="http://en.wikipedia.org/wiki?curid=8222" title="Deseret alphabet">
Deseret alphabet

The Deseret alphabet () (Deseret: 𐐔𐐯𐑅𐐨𐑉𐐯𐐻 or 𐐔𐐯𐑆𐐲𐑉𐐯𐐻) is a phonemic English spelling reform developed in the mid-19th century by the board of regents of the University of Deseret (later the University of Utah) under the direction of Brigham Young, second president of The Church of Jesus Christ of Latter-day Saints.
In public statements, Young claimed the alphabet was intended to replace the traditional Latin alphabet with an alternative, more phonetically accurate alphabet for the English language. This would offer immigrants an opportunity to learn to read and write English, he said, the orthography of which is often less phonetically consistent than those of many other languages. Similar experiments were not uncommon during the period, the most well-known of which is the Shavian alphabet.
Young also prescribed the learning of Deseret to the school system, stating "It will be the means of introducing uniformity in our orthography, and the years that are now required to learn to read and spell can be devoted to other studies".
Development and use.
The Deseret alphabet was developed primarily by a committee made up of the university's board of regents and church leaders Parley P. Pratt and Heber C. Kimball. The two main contributors to the alphabet's character development were Pratt and George D. Watt, a local expert on shorthand systems. In addition, a Frenchman visiting Utah at the time the alphabet was being developed reported that William W. Phelps "worked out the letters." Assistant Church Historian, Andrew Jenson, also reported that the alphabet was produced by a committee composed of Orson Pratt, Parley P. Pratt, Wilford Woodruff, George D. Watt, Robert L. Campbell, and others.
The Deseret alphabet may have been inspired by the phonetic alphabet published by Michael Hull Barton in Boston and the Shaker community at Harvard, Massachusetts from 1830 to 1833. Originally a Quaker, Barton was baptized a Mormon in Portsmouth, New Hampshire around October 1831 (during his phonetic alphabet experiment), but within a few months then converted to Shakerism, although he continued to meet with early Mormon leaders until at least 1844. The alphabet went through at least three major revisions during its first few years.
At least four books were published in the new alphabet: "The First Deseret Alphabet Reader", "The Second Deseret Alphabet Reader", The Book of Mormon, and a Book of Mormon excerpt called "First Nephi–Omni". Additionally published in the "Deseret News" were various articles and passages from the New Testament, which were printed on a press obtained by Orson Pratt.
Considerable non-printed material in the Deseret alphabet still exists, including one replica headstone in Cedar City, some coinage, letters, diaries, and meeting minutes. Pratt supervised the transcription of the complete Bible and the Doctrine and Covenants. One of the more curious items found in the Deseret alphabet is an English-Hopi dictionary. 
Despite heavy promotion, the Deseret alphabet was never widely adopted. This reluctance was partly due to prohibitive costs; Pratt estimated that the cost of printing a regular library would be over one million dollars. With modern computer systems lowering the costs associated with typesetting, new material in the Deseret alphabet occasionally appears. Students in a small UC Berkeley linguistic class learned the alphabet in two months. 
The Alphabet.
Although Deseret is an alphabet with case, the only difference between the minuscule and majuscule forms is that the majuscule forms are larger. The Unicode values for each glyph can be found below.
A degree of free spelling is allowed to accommodate dialectal differences in English.
Representation of /ə/.
The Deseret alphabet does not have a distinct symbol for the mid-central vowel. This sound is written as what it would be if the syllable that contains it were stressed. For example, the word "enough" is commonly pronounced [əˈnɐf], but when it is stressed (as in a declaration of irritation) it is pronounced [iˈnɐf]. The Deseret spelling of the word, 𐐨𐑌𐐲𐑁, reflects that stressed pronunciation.
If /ə/ does not have an inherent stressed value in a word, as is often the case before /r/, then it is written as 𐐲.
Syllabic values.
Each letter in the Deseret alphabet has a name, and when a letter is written on its own it has the value of that name. This allows some short words to be written with a single letter, and is called a letter's "syllabic value". The most common word in English, "the", is written simply 𐑄, as the letter's name is /ðiː/ and that is the stressed pronunciation of the word. The consonants with syllabic values are 𐐶 (woo), 𐐷 (ye/Yi), 𐐸 (ha, or haw in dialects with the cot-caught merger), 𐐹 (pee), 𐐺 (be/bee), 𐐻 (tee/tea), 𐐽 (qi), 𐐾 (gee), 𐑀 (gay), and 𐑄 (the/thee). Syllabic values do not apply within words (though this was formerly the case; the coin on the right actually reads "holin(e)ss to the Lord", using the syllabic value of the letter 𐑅). Though "bee" is written 𐐺, "bees" is written 𐐺𐐨𐑆.
Unicode.
The Deseret alphabet (U+10400–U+1044F) was added to the Unicode Standard in March 2001 with the release of version 3.1.
The letters Oi and Ew were added to the Unicode Standard in April 2003 with the release of version 4.0.
References.
General references.
</dl>

</doc>
<doc id="8226" url="http://en.wikipedia.org/wiki?curid=8226" title="Danish">
Danish

Danish may refer to:

</doc>
<doc id="8227" url="http://en.wikipedia.org/wiki?curid=8227" title="Danish language">
Danish language

Danish ("dansk" ]; "dansk sprog", ]) is a North Germanic language spoken by around six million people, principally in Denmark and in the region of Southern Schleswig in northern Germany, where it holds minority language status. There are also minor Danish-speaking communities in Norway, Sweden, the United States, Canada, Brazil and Argentina. Due to immigration and language shift in urban areas, around 15–20% of the population of Greenland speaks Danish as their home language.
Along with the other North Germanic languages, Danish is a descendant of Old Norse, the common language of the Germanic peoples living in Scandinavia during the Viking Era. Danish, together with Swedish, derives from the East Norse dialect group, while the Old Norwegian dialects before the influence of Danish and Norwegian Bokmål is classified as a West Norse language together with Faroese and Icelandic. A more recent classification based on mutual intelligibility separates modern spoken Danish, Norwegian and Swedish into a "Mainland Scandinavian" group while Icelandic and Faroese are placed in a separate category labelled "Insular Scandinavian".
Up until the 16th century Danish was a continuum of vernacular dialects spoken from Schleswig to Scania; There was no standard variety or orthographic conventions. With the Reformation and the introduction of printing, a standard language was developed based on the educated Copenhagen dialect, and it spread through its use in the education system and administration. German and Latin continued to be the most important written languages well into the 17th century. Following the loss of territory to Germany and Sweden, a nationalist movement adopted the language as a rallying point for Danish identity, and the language experienced a strong surge in use and popularity with major works of literature produced in the 18th and 19th century. Today the traditional Danish dialects have all but disappeared, although there are some regional pronunciation variation of the Standard language. The main parameters of linguistic variation today is between generations, youth language being particularly innovative.
Danish has a very large vowel inventory comprising 27 phonemically distinctive vowels, and its prosody is characterized by the distinctive stød phenomenon - a kind of laryngeal phonation type. The grammar is moderately inflective with strong (irregular) and weak (regular) conjugations and inflections. Nouns and demonstrative pronouns distinguish common and neutral gender. As English, Danish only has vestiges of case, particularly in the pronouns, and it has lost all any person marking on verbs. Its syntax is V2, with the finite verb always occupying the second slot in the sentence.
Classification.
Danish is a Germanic language of the North Germanic branch, other names for this group are the Nordic or Scandinavian languages. Along with Swedish, Danish descends from the Eastern dialects of the Old Norse language, Danish and Swedish are also classified as East Scandinavian or East Nordic languages.
Scandinavian languages are often considered a dialect continuum, where there are no sharp dividing lines between the different vernacular languages. 
Just as Norwegian and Swedish, Danish has been significantly influenced from Low German in the Middle Ages, and from English since the turn of the 20th century. 
Danish itself can be divided into three main dialect areas: West Danish (Jutlandic), Insular Danish (including the Standard variety), and East Danish (including Bornholmian and Scanian. Under the view that Scandinavian is a dialect continuum, East Danish can be considered intermediary between Danish and Swedish, and Scanian can be considered a Swedified East Danish dialect, and Bornholmsk is its closest relative.
Mutual intelligibility.
Danish is largely mutually intelligible with Norwegian and Swedish. Proficient speakers of any of the three languages can often understand the others fairly well, though studies have shown that speakers of Norwegian generally understand both Danish and Swedish far better than Swedes or Danes understand each other. Both Swedes and Danes also understand Norwegian better than they understand each other's languages.
History.
Dǫnsk tunga: The Old Norse period.
"Móðir Dyggva var Drótt, dóttir Danps konungs, sonar Rígs er fyrstr var konungr kallaðr á danska tungu".<br> "Dyggvi's mother was Drott, the daughter of king Danp, Ríg's son, who was the first to be called king in the Danish tongue."
“
 Heimskringla by Snorri Sturluson
By the 8th century, the common Germanic language of Scandinavia, Proto-Norse, had undergone some changes and evolved into Old Norse. 
This language was generally called the "Danish tongue" ("Dǫnsk tunga"), or "Norse language" ("Norrœnt mál"). Norse was written in the runic alphabet, first with the elder futhark and from the 9th century with the younger futhark. 
From the 7th century the common Norse language began to undergo new changes that did not spread to all of Scandinavia, resulting in the appearance of two dialect areas, "Old West Norse" (Norway and Iceland) and "Old East Norse" (Denmark and Sweden). Most of the changes separating East Norse from West Norse started as innovations in Denmark, that spread through Scania into Sweden and by maritime contact to southern Norway. A change that separated Old East Norse (Runic Swedish/Danish) from Old West Norse was the change of the diphthong "æi" (Old West Norse "ei") to the monophthong "e", as in "stæin" to "sten". This is reflected in runic inscriptions where the older read "stain" and the later "stin". There was also a change of "au" as in "dauðr" into "ø" as in "døðr". This change is shown in runic inscriptions as a change from "tauþr" into "tuþr". Moreover, the "øy" (Old West Norse "ey") diphthong changed into "ø" as well, as in the Old Norse word for "island". This monophthongization started in Jutland and spread eastward, having spread throughout Denmark and most of Sweden by 1100.
Through Danish conquest, Old East Norse was once widely spoken in the northeast counties of England. Many words derived from Norse, such as "gate" (gade) for street, still survive in Yorkshire and the East Midlands (parts of eastern England) colonized by Danish Vikings. The city of York was once the Viking settlement of Jorvik. Several other English words also derive from Old East Norse, for example, "are" (er), "knife" (kniv), "husband" (husbond), and "egg" (æg). The suffix "-by" for 'town' is common in place names in Yorkshire and the East Midlands that is, Selby, Whitby, Derby and Grimsby. The word dale in Yorkshire and Derbyshire is commonly used in place of valley.
Old Danish: The 11th to 16th centuries.
"Fangær man saar i hor seng mæth annæns mansz kunæ. oc kumær han burt liuænd...".<br> "If one catches someone in the whore-bed with another man's wife and he comes away alive..."
”
Jutlandic Law, 1241
In the medieval period Danish emerged as a separate language from Swedish. The main written language was Latin, and the few Danish language texts preserved from this period are written in the Latin alphabet, although the runic alphabet seems to have lingered in popular usage in some areas. The main text types written in this period are laws, which were formulated in the vernacular language to be accessible also to those who were not latinate. The Jutlandic Law and Scanian Law were written in vernacular Danish in the early 13th century. Beginning in 1350 Danish began to be used as a language of administration and new types of literature began to be written in the language, such as royal letters and testaments. The orthography in this period was not standardized nor was the spoken language, and the regional laws demonstrate the dialectal differences between the regions in which they were written. 
Throughout this period Danish was in contact with Low German, and many Low German loans were introduced in this period. With the Protestant Reformation in 1536, Danish also became the language of religion, which sparked a new interest in using Danish as a literary language. It is also in this period that Danish begins to take on the linguistic traits that differentiate it from Swedish and Norwegian, such as the stød and the voicing of many stop consonants.
The first printed book in Danish dates from 1495, the "Rimkrøniken" (Rhyming Chronicle), a history book told in rhymed verses. The first complete translation of the Bible in Danish, the Bible of Christian II translated by Christiern Pedersen was published in 1550. Pedersen's orthographic choices set the de facto standard for subsequent writing in Danish.
Early Modern Danish.
"Herrer og Narre have frit Sprog".<br> "Lords and jesters have free speech."
”
 Peder Syv, proverbs 
Following the first Bible translation the development of Danish as a written language, and as a language of religion, administration and public discourse sped up. In the second half of the 17th century a number of grammarians elaborated grammars of Danish, first among them Rasmus Bartholin's 1657 Latin grammar "De studio lingvæ danicæ"; then Laurids Olufsen Kock's 1660 grammar of the Zealand dialect "Introductio ad lingvam Danicam puta selandicam"; and in 1685 the first Danish grammar written in Danish, "Den Danske Sprog-Kunst" ("The Danish Speech-art") by Peder Syv. Significant authors from this period are Thomas Kingo, poet and psalmist, and Leonora Christina Ulfeldt, whose novel Jammersminde ("Remebered Woes") is considered a literary masterpiece. Orthography was still not standardized and the principles for doign so were vigorously discussed among Danish philologists. The grammar of Jens Pedersen Høysgaard was the first to give a detailed analysis of Danish phonology and prosody, including a description of the stød. In this period scholars were also discussing whether it was best to "write as one speaks" or to "speak as one writes", including whether archaic grammatical forms that had fallen out of use in the vernacular, such as the plural form of verbs, should be conserved in writing (i.e. "han er" "he is" vs. "de ere" "they are"). 
The East Danish provinces were lost to Sweden after the Treaty of Brömsebro after which they were gradually Swedified; just as Norway was politically severed from Denmark, marking the end of Danish influence on Norwegian. With the introduction of absolutism in 1660, the Danish state was further integrated, and the language of the chancellery, a Zealandic variety with German and French influence, became the de facto official standard language, especially in writing - this was the original so-called "rigsdansk" ("Danish of the Realm"). Also beginning in the mid 18th century, the "skarre-R", the uvular R sound ([ʁ]), began spreading through Denmark, probably through influence from Parisian French and German. It affected all of the areas where Danish had been influential, including all of Denmark, Southern Sweden and coastal southern Norway.
In the 18th century Danish philology was advanced by Rasmus Rask, who pioneered the disciplines of comparative and historical linguistics and wrote the first English language grammar of Danish. And literary Danish flourished with the works of Ludvig Holberg, whose plays and historical and scientific works laid a foundation for the Danish literary canon. With the Danish colonization of Greenland by Hans Egede, Danish became the administrative and religious there. Just as Iceland and the Faroe Islands, had status as Danish colonies with Danish as an official language up until the mid 20th century.
Modern Danish as a National language.
"Moders navn er vort Hjertesprog,<br> kun løs er al fremmed Tale.<br> Det alene i mund og bog,<br> kan vække et folk a dvale."<br> "Mother's name is our hearts' tongue,<br> only idle is all foreign speech<br> It alone, in mouth or in book,<br> can rouse a people from sleep."
”
 N.F.S. Grundtvig, "Modersmaalet"
Following the loss of Schleswig to Germany, a sharp influx of German speakers moved into the area, eventually outnumbering the Danish speakers. The political loss of territory sparked a period of intense nationalism in Denmark, coinciding with the so-called "Golden Age" of Danish culture. Authors such as N.F.S. Grundtvig emphasized the role of language in creating national belonging. Some of the most cherished Danish language authors of this period are existential philosopher Søren Kierkegaard, prolific fairy tale author Hans Christian Andersen. The influence of popular literary role models, together with increased requirements of education did much to strengthen the Danish language, and also started a period of homogeneization, where the Copenhagen standard language gradually displaced the regional vernacular languages. After the Schleswig referendum in 1920 a number of Danes remained as a minority within German territories. Throughout the 19th Century Danes emigrated, establishing small expatriate communities in the Americas, particularly in the US, Canada, and Argentina where memory and some use of Danish remains today.
After the Danish occupation by Germany in World War II, the 1948 orthography reform dropped the German influenced rule of capitalizing nouns, and introduced the letter Å/å. Three 20th century Danish authors have become Nobel Prize laureates in Literature: Karl Adolph Gjellerup and Henrik Pontoppidan (joint recipients in 1917) and Johannes Vilhelm Jensen (awarded 1944). 
With the exclusive use of "rigsdansk", the High Copenhagen Standard in national broadcasting, the traditional dialects came under increased pressure. In the 20th century they have all but disappeared, and the Standard language has extended throughout the country. Minor regional pronunciation variation of the Standard language, sometimes called "regionssprog" ("regional languages") remain, and are in some cases vital. Today the major varieties of Standard Danish are High Copenhagenian , associated with elderly, well to-do and well educated people of the capital, and low-Copenhagenian associated with the working class people and youths. Also in the 21st century the influence of immigration has had linguistic consequences, such as the emergence of a so-called multiethnolect in the urban areas, an immigrant Danish variety (also known as Perkerdansk), combining elements of different immigrant languages such as Arabic, Turkish and Kurdish, as well as English and Danish.
Geographical distribution.
Danish is the national language of Denmark and one of two official languages of the Faroe Islands (alongside Faroese). Until 2009, it had also been one of two official languages of Greenland (alongside Greenlandic). Danish is widely spoken in Greenland now as "lingua franca", and an unknown portion of the native Greenlandic population has Danish as their first language; nearly all of the native Greenlandic population speak Danish as a second language since its introduction into the education system as a compulsory language in 1928. Danish was an official language in Iceland until 1944 but is today still widely used and is a mandatory subject in school.
In addition, there is a noticeable community of Danish speakers in Southern Schleswig, the portion of Germany bordering Denmark, where it is an officially recognised regional language, just as German is north of the border. Furthermore, Danish is one of the official languages of the European Union and one of the working languages of the Nordic Council. Under the Nordic Language Convention, Danish-speaking citizens of the Nordic countries have the opportunity to use their native language when interacting with official bodies in other Nordic countries without being liable for any interpretation or translation costs.
The more widespread of the two varieties of Norwegian, Bokmål, is very close to Danish, because standard Danish was used as the de facto administrative langauge until 1814. Bokmål is based on Danish unlike the other variety of Norwegian, Nynorsk, which is based on the Norwegian dialects, with Old Norwegian as an important reference point.
There is no law stipulating an official language for Denmark, making Danish the de facto language only. The Code of Civil Procedure does, however, lay down Danish as the language of the courts. Since 1997 public authorities have been obliged to observe the official spelling by way of the Orthography Law.
Development.
Education.
Danish is a mandatory subject in school in the Danish dependencies of the Faroe Islands (where it is also an official language after Faroese) and Greenland (where, however, the only official language since 2009 is Kalaallisut and the Danish is now spoken as "lingua franca"), as well as the former crown holding of Iceland.
Dialects.
Standard Danish ("rigsdansk") is the language based on dialects spoken in and around the capital, Copenhagen. Unlike Swedish and Norwegian, Danish does not have more than one regional speech norm. More than 25% of all Danish speakers live in the metropolitan area of the capital, and most government agencies, institutions, and major businesses keep their main offices in Copenhagen, something that has resulted in a very homogeneous national speech norm. In contrast, though Oslo (Norway) and Stockholm (Sweden) are quite dominant in terms of speech standards, cities like Bergen, Gothenburg and the Malmö-Lund region are large and influential enough to create secondary regional norms, making the standard language more varied than is the case with Danish. The general agreement is that Standard Danish is based on a form of Copenhagen dialect, but the specific norm, as with most language norms, is difficult to pinpoint for both laypeople and scholars. Historically Standard Danish emerged as a compromise between the dialect of Zealand and Scania. The first layers of it can be seen in east Danish provincial law texts such as Skånske Lov, just as we can recognize west Danish in laws from the same ages in Jyske Lov.
Despite the relative cultural monopoly of the capital and the centralized government, the divided geography of the country allowed distinct rural dialects to flourish during the centuries. Such "genuine" dialects were formerly spoken by a vast majority of the population, but have declined much since the 1960s. They still exist in communities out in the countryside, but most speakers in these areas generally speak a regionalized form of Standard Danish, when speaking with one who speaks to them in that same standard. Usually an adaptation of the local dialect to "rigsdansk" is spoken, though code-switching between the standard-like norm and a distinct dialect is common.
Danish is divided into three distinct dialect groups, which are further subdivided in about 30 "dialektområder", "dialect areas":
The term "Eastern Danish" is occasionally used for Bornholmian, but including the dialects of Scania (particularly in a historical context). The background for this lies in the loss of the originally Danish provinces of Blekinge, Halland and Scania to Sweden in 1658. The island of Bornholm in the Baltic Sea was ceded to Sweden the same year, but returned to Danish rule in 1660. The spoken language in this part of modern Sweden is historically a variant of Danish, but was over time influenced by written and spoken Swedish and is today considered to be a Swedish dialect. Similarly, the Norwegian language is classified as a descendant of West Norse, while the written language used by the vast majority in Norway is derived from an older variant of standard Danish. A few generations ago, the classical dialects spoken in the southern Swedish provinces could still be argued to be more Eastern Danish than Swedish, being similar to the dialect of Bornholm. Today, influx of Standard Swedish and Standard Danish vocabulary has generally meant that Scanian and Bornholmian are closer to the modern national standards of their respective host nations than to each other. The Bornholmian dialect has also maintained to this day many archaic features, such as a distinction between three grammatical genders, which this feature disappeared in the central Insular Danish dialects during the 20th century. Standard Danish has two genders and the definite form of nouns is formed by the use of suffixes, while Western Jutlandic has only one gender and the definite form of nouns uses an article before the noun itself, in the same fashion as West Germanic languages. Today, Standard Danish is most similar to the Insular Danish dialect group.
Sounds.
The sound system of Danish is in many ways unusual among the world's languages. From the perspective of the written language, the spoken is quite prone to considerable reduction and assimilation of both consonants and vowels even in very formal standard language. 
Vowels.
Although somewhat depending on analysis, most modern variants of Danish distinguish between 12 long vowels, 13 short vowels and two schwa vowels, /ə/ and /ɐ/ that only occur in unstressed syllables. This gives a total of 27 different vowel phonemes - a very large number among the world's languages. At least 19 different diphthongs also occur, all with a short first vowel and the second segment being either [i̯], [u̯] or [ɐ̯]. The table below shows the approximate distribution of the vowels as given by in Modern Standard Danish, with the symbols used in . Questions of analysis may give a slightly different inventory, for example based on whether r-colored vowels are considered distinct phonemes. gives 25 "full vowels", not counting the two unstressed schwa-vowels.
Consonants.
In distinct pronunciation it is possible to distinguish at least 19 consonants in most variants of Danish:
These consonants can be analyzed to represent 15 phonemes: /m n p t k b d ɡ f s h v j r l/. Many of these phonemes have quite different allophones in onset and coda. /p t k/ are aspirated in onset, not in coda. /d ɡ v j r/ are contoid in onset and vocoid in coda.
[ʋ, ð] often have slight frication, but are usually pronounced as approximants.
[ɕ] occurs only after /s/ or /t/. Since [j] doesn't occur after these phonemes, [ɕ] can be analyzed as /j/, which is devoiced after voiceless alveolar frication. This makes it unnecessary to postulate a /ɕ/-phoneme in Danish.
In onset /r/ is realized as a uvular approximant, [ʁ], but in coda it is either realized as a non-syllabic low central vowel, [ɐ̯] (which is almost identical to how /r/ is often pronounced in syllable-final position in German) or simply coalesces with the preceding vowel. The phenomenon is also comparable to non-rhotic pronunciations of English. The Danish pronunciation of /r/ distinguishes the language from those varieties of Norwegian and Swedish that use trilled [r].
Prosodic phenomena.
A rare feature is the presence of a prosodic feature called "stød" (lit. "push; thrust"). This is a form of laryngealization or creaky voice, only occasionally realized as a full glottal stop (especially in emphatic pronunciation). It can be the only distinguishing feature between certain words, thus creating minimal pairs (for example, "bønder" "peasants" with stød, versus "bønner" "beans" or "prayers" without). The distribution of stød in the lexicon is related to the distribution of the common Scandinavian pitch accents found in most dialects of Norwegian and Swedish. 
Stress is phonemic and distinguishes words such as "billigst" [ˈbilist] "cheapest" and "bilist" [biˈlist] "car driver".
Grammar.
The infinitive forms of Danish verbs end in a vowel, which in almost all cases is a schwa, represented in writing by the letter "e". Verbs are conjugated according to tense, but otherwise do not vary according to person or number. For example the present tense form of the Danish infinitive verb "spise" ("to eat") is "spiser"; this form is the same regardless of whether the subject is in the first, second, or third person, or whether it is singular or plural. This extreme ease of conjugating verbs is compensated by the large number of irregular verbs in the language.
Standard Danish nouns fall into only two grammatical genders: "common" and "neuter", while some dialects still often have "masculine", "feminine" and "neuter". While the majority of Danish nouns (ca. 75%) have the "common" gender, and "neuter" is often used for inanimate objects, the genders of nouns are not generally predictable and must in most cases be memorized. A distinctive feature of the Scandinavian languages, including Danish, is an enclitic definite article.
To demonstrate: The "common" gender word "a man" (indefinite) is "en mand" but "the man" (definite) is "manden". The "neuter" equivalent would be "a house" (indefinite) "et hus", "the house" (definite) "huset". Even though the definite and indefinite articles have separate origins, they have become homographs in Danish. In the plural, the definite article is "-(e)ne", as the plural endings are "- / -e / -er". The enclitic article is not used when an adjective is added to the noun; here the demonstrative pronoun is used instead: "den store mand" "the big man", "the big house", "det store hus".
Like all Germanic languages, Danish forms compound nouns. These are represented in Danish orthography as one word, as in "kvindehåndboldlandsholdet", "the female national handball team". In some cases, nouns are joined with an extra "s", originally possessive in function, like "landsmand" (from "land", "country", and "mand", "man", meaning "compatriot"), but "landmand" (from same roots, meaning "farmer"). Some words are joined with an extra "e", like "gæstebog" (from "gæst" and "bog", meaning "guest book").
Vocabulary.
The majority of Danish words are derived from the Old Norse language. However, 50-60% of Danish words hail from Middle Low German and were borrowed in the late medieval era (explaining the relative similarity of its vocabulary with modern Low Saxon and Dutch), for example, "betale" (to pay). In the 17th and 18th Centuries standard German and French superseded Low German influence and in the 20th Century English became the main supplier of loan words—especially after World War II. Although many old Nordic words remain, some were replaced with borrowed synonyms, such as can be seen with "æde" (to eat) which became less common when the Low German "spise" came into fashion. Besides borrowing new words are freely formed by compounding existing words.
Because of the shared history between Danish and English—both are Germanic languages (though Danish is a North Germanic language descended from Old Norse and English is a West Germanic language descended from Old English) and Old Norse exerted a strong influence on Old English in the early medieval period. To show their shared Germanic heritage, one merely has to note the many common words that are very similar in the two languages. For example, Danish words for commonly used nouns and prepositions are easily recognizable in their written form to English speakers, such as "have", "over", "under", "for", "give", "flag," "salt," and "kat". However, when pronounced, most of these words sound quite different from their English equivalents; not so much due to the Great Vowel Shift of English, as Danish "a" and "e" were affected similarly, but due to the Danish tendency to slur soft consonants such as "d", "g", and "v" (resulting in what sounds to English ears as "ha'e", "o'er", "un'er", "gi'e", and "flay"). Similarly, some other words are almost identical to their Scottish equivalents, e.g., "kirke" (Scottish "kirk", i.e., 'church') or "barn" (Scottish "bairn", i.e. 'child'). In addition, the word "by", meaning "village" or "town", occurs in many English place-names, such as "Whitby" and "Selby", as remnants of the Viking occupation. During the latter period, English adopted "are", the third person plural form of the verb "to be", as well as the corresponding personal pronoun form "they" from contemporary Old Norse.
Numerals.
In the word forms of numbers above 20, the units are stated before the tens, such that 21 is rendered "enogtyve", literally "one and twenty". Similar constructions are found in German, Dutch, Afrikaans, certain varieties of Norwegian, Slovene and Arabic as well as in archaic and dialect English (compare the line ""Four-and-twenty" blackbirds" in the old nursery rhyme.)
The numeral "halvanden" means 1½ (literally "half second", implying "one plus half of the second one"). The numerals "halvtredje" (2½) and "halvfjerde" (3½) are obsolete, but still implicitly used in the vigesimal system described below. Similarly, the temporal designation "klokken halv tre", literally "half three o'clock", is half past two o'clock.
One peculiar feature of the Danish language is the fact that numerals 50, 60, 70, 80 and 90 are (somewhat like the French numerals from 80 through 99) based on a vigesimal system, formerly also used in Norwegian and Swedish. This means that the score is used as a base unit in counting: "Tres" (short for "tre-sinds-tyve" meaning "three times twenty") means sixty, while "halvtreds" (short for "halvtredje-sinds-tyve" meaning "half third times twenty", implying two score plus half of the third score) is fifty. The ending "sindstyve" meaning "times twenty" is no longer included in cardinal numbers, but still used in ordinal numbers. Thus, in modern Danish fifty-two is usually rendered as "tooghalvtreds" from the now obsolete "tooghalvtredsindstyve", whereas 52nd is either "tooghalvtredsende" or "tooghalvtredsindstyvende". Twenty is called "tyve" (derived from old Danish "tiughu", a haplology of "tuttiughu", meaning 'two tens'), while thirty is "tredive" (derived from old Danish "þrjatiughu" meaning 'three tens'), and forty is called "fyrre" (derived from the old Danish "fyritiughu" meaning 'four tens' via "fyrretyve", still occasionally used in historical settings or for humorous effect). An exception to the way Danish numbers are formed is in writing cheques and legal amounts in banking, where traditionally the numbers are 10-based and spelled as they appear in numerical form; thus, "fir(e)ti" is forty (4 times 10) and "seksti-to" is sixty-two (6 times 10 plus 2).
For large numbers (one billion or larger), Danish uses the long scale, so that for example, one short scale billion (1,000,000,000) is called "milliard", and one short scale trillion (1,000,000,000,000) is "billion".
Writing system.
The oldest preserved examples of written Danish (from the Iron and Viking Ages) are in the Runic alphabet. The introduction of Christianity also brought the Latin script to Denmark, and at the end of the High Middle Ages the Runes had more or less been replaced by the Latin letters.
As in Germany, the Fraktur (blackletter) types were still commonly used in the late 19th century (until 1875, Danish children were taught to read Fraktur letters in school), and many books were printed with Fraktur typesetting even in the beginning of the 20th century, particularly by conservatives. However, the Latin script was used by modernists, for example, the Royal Danish Academy of Sciences and Letters changed style in 1799. Nouns were capitalized, as in German, until the 1948 spelling reform.
The modern Danish alphabet is similar to the English one, with three additional letters: "æ", "ø", and "å", which come at the end of the alphabet, in that order. A spelling reform in 1948 introduced the letter "å", already in use in Norwegian and Swedish, into the Danish alphabet to replace the digraph "aa"; the old usage still occurs in some personal and geographical names (for example, the name of the city of "Aalborg" is spelled with Aa following a decision by the City Council in the 1970s). When representing the "å" sound, "aa" is treated just like "å" in alphabetical sorting, even though it looks like two letters. When the letters are not available due to technical limitations (e.g., in URLs), they are often replaced by "ae" (Æ, æ), "oe" or "o" (Ø, ø), and "aa" (Å, å), respectively.
The same spelling reform changed the spelling of a few common words, such as the past tense "vilde" (would), "kunde" (could) and "skulde" (should), to their current forms of "ville", "kunne" and "skulle" (making them identical to the infinitives in writing, as they are in speech). Modern Danish and Norwegian use the same alphabet, though spelling differs slightly.

</doc>
<doc id="8228" url="http://en.wikipedia.org/wiki?curid=8228" title="Decade (Neil Young album)">
Decade (Neil Young album)

Decade is a compilation by Neil Young, originally released in 1977 as a triple album, now available on two compact discs. It contains 35 of Young's songs recorded between 1966 and 1976, among them five tracks that had been unreleased up to that point. It peaked at No. 43 on the "Billboard" Top Pop Albums chart, and was certified platinum by the RIAA in 1986.
History.
Compiled by Young himself, with his hand-written liner notes about each track, "Decade" represents almost every album from his career and various affiliations through 1977 with the exception of "Four Way Street" and "Time Fades Away". Of the previously unreleased songs, "Down to the Wire" features the New Orleans pianist Dr. John with Buffalo Springfield on an item from their shelved "Stampede" album; "Love Is a Rose" was a minor hit for Linda Ronstadt in 1975; "Winterlong" received a cover by Pixies on the Neil Young tribute album from 1989, ""; and "Campaigner" is a Young song critical of Richard Nixon. The track "Long May You Run" is a different mix to that found on the album of the same name, featuring the harmonies of the full Crosby Stills & Nash before David Crosby and Graham Nash left the recording sessions.
For many years, "Decade" was the only Neil Young compilation album available. A 1993 compilation called "Lucky Thirteen" was released, but it only covered Young's 1982–1988 output. It was not until 2004 that Reprise Records released a single-disc retrospective of his best-known tracks, titled "Greatest Hits". Throughout the 1980s and '90s, Young promised fans a follow-up to the original "Decade" collection, provisionally titled "Decade II"; eventually, this idea was scrapped in favor of a much more comprehensive anthology to be titled "Archives", spanning his entire career and ranging in size from a box set to an entire series of audio and/or video releases. The first release of archival material since "Decade" and "Lucky Thirteen" would appear in 2006, "Live at the Fillmore East", a recording from a 1970 concert featuring Crazy Horse with Danny Whitten. Several other archival live releases followed, and in 2009 the first of several planned multi-disc box sets, "The Archives Vol. 1 1963–1972", was issued.
Alternate early version.
Initially, "Decade" was to be released in 1976, but was pulled at the last minute by Young. It was shelved until the following year, where it appeared with two songs removed from the original tracklist (a live version of "Don't Cry No Tears" recorded in Japan in 1976, and a live version of "Pushed It Over the End" recorded in 1974). Also removed were the following comments on those two songs and "Time Fades Away", from Young's handwritten liner notes:
 Time Fades Away. No songs from this album are included here. It was recorded on my biggest tour ever, 65 shows in 90 days. Money hassles among everyone concerned ruined this tour and record for me but I released it anyway so you folks could see what could happen if you lose it for a while. I was becoming more interested in an audio verite approach than satistfying ["sic"] the public demands for a repetition of Harvest.
Don't Cry No Tears. Initially titled 'I Wonder,' this song was written in 1964. One of my first songs. This is a live recording from Japan with Crazy Horse.
Pushed It over the End. Recorded live on the road in Chicago, 1974. Thanks to Crosby & Nash's help on the overdubbed chorus, I was able to complete this work. I wrote it for Patty Hearst and her countless brothers and sisters. Also, I wrote it for myself and the increasing distance between me and you.
Reception.
The album has been lauded in many quarters as one of the best examples of a career retrospective for a rock artist, and as a template for the box set collections that would follow in the 1980s and beyond. However, in the original article on Young from the first edition of the "Rolling Stone Illustrated History of Rock and Roll" and a subsequent article in the "1983 Rolling Stone Record Guide", critic Dave Marsh used this album to accuse Young of deliberately manufacturing a self-mythology, arguing that while his highlights could be seen to place him on a level with other artists from his generation like Bob Dylan or The Beatles, the particulars of his catalogue did not bear this out. The magazine has since excised the article from subsequent editions of the "Illustrated History" book; a transcription of it can be found at the link below (despite his scathing view of Young's career, Marsh gave the album the highest possible rating).
Track listing.
All songs written by Neil Young.
Side six.
The CD release combined sides 1-3 onto disc one, and sides 4-6 on disc two.
Charts.
Album

</doc>
<doc id="8230" url="http://en.wikipedia.org/wiki?curid=8230" title="Demeter">
Demeter

 
In ancient Greek religion and Greek mythology, Demeter (; Attic: Δημήτηρ "Dēmḗtēr"; Doric: Δαμάτηρ "Dāmā́tēr") is the goddess of the harvest, who presided over grains and the fertility of the earth. Her cult titles include Sito (Σιτώ), "she of the Grain", as the giver of food or grain and Thesmophoros (θεσμός, "thesmos": divine order, unwritten law; "phoros": bringer, bearer), "Law-Bringer," as a mark of the civilized existence of agricultural society.
Though Demeter is often described simply as the goddess of the harvest, she presided also over the sacred law, and the cycle of life and death. She and her daughter Persephone were the central figures of the Eleusinian Mysteries that predated the Olympian pantheon. In the Linear B Mycenean Greek tablets of circa 1400–1200 BC found at Pylos, the "two mistresses and the king" may be related with Demeter, Persephone and Poseidon. Her Roman equivalent is Ceres.
Etymology.
It is possible that Demeter appears in Linear A as "da-ma-te" on three documents (AR Zf 1 and 2, and KY Za 2), all three apparently dedicated in religious situations and all three bearing just the name ("i-da-ma-te" on AR Zf 1 and 2). It is unlikely that Demeter appears as "da-ma-te" in a Linear B (Mycenean Greek) inscription (PY En 609); the word 𐀅𐀔𐀳, "da-ma-te", probably refers to "households". On the other hand 𐀯𐀵𐀡𐀴𐀛𐀊, "si-to-po-ti-ni-ja", "Potnia of the Grain", is regarded to refer to her Bronze Age predecessor or to one of her epithets.
Demeter's character as mother-goddess is identified in the second element of her name "meter" (μήτηρ) derived from Proto-Indo-European "*méh₂tēr" (mother). In antiquity, different explanations were already proffered for the first element of her name. It is possible that "Da" (Δᾶ), a word which became "Ge" (Γῆ) in Attic, is the Doric form of "De" (Δῆ), "earth", the old name of the chthonic earth-goddess, and that Demeter is "Mother-Earth". This root also appears in the Linear B inscription "E-ne-si-da-o-ne", "earth-shaker", as an aspect of the god Poseidon. However, the "dā" element in the name of Demeter, is not so simply equated with "earth" according to John Chadwick.
The element "De"- may be connected with "Deo", a surname of Demeter probably derived from the Cretan word "dea" (δηά), Ionic "zeia" (ζειά) meaning "barley", so that she is the Mother and the giver of food generally. Arcadian cult to Demeter links her to a male deity (Greek: Πάρεδρος, "Paredros"), who accompanied the Great Goddess and has been interpreted as a possible substitution for Poseidon; Demeter may therefore be related to a Minoan Great Goddess.
An alternative, Proto-Indo-European etymology comes through Potnia and Despoina; where "Des-" represents a derivative of PIE "*dem" (house, dome), and Demeter is "mother of the house" (from PIE "*dems-méh₂tēr").
Agricultural deity.
According to the Athenian rhetorician Isocrates, Demeter's greatest gifts to humankind were agriculture, particularly of cereals, and the Mysteries which give the initiate higher hopes in this life and the afterlife. These two gifts were intimately connected in Demeter's myths and mystery cults. In Homer's "Odyssey" she is the blond-haired goddess who separates the chaff from the grain. In Hesiod, prayers to Zeus-Chthonios (chthonic Zeus) and Demeter help the crops grow full and strong. Demeter's emblem is the poppy, a bright red flower that grows among the barley.
In Hesiod's Theogony, Demeter is the daughter of Cronus and Rhea. At the marriage of Cadmus and Harmonia, Demeter lured Iasion away from the other revelers. They had intercourse in a ploughed furrow in Crete, and she gave birth to a son, Ploutos. Her daughter by Zeus was Persephone, Queen of the Underworld.
Festivals and cults.
Demeter's two major festivals were sacred mysteries. Her Thesmophoria festival (11–13 October) was women-only. Her Eleusinian mysteries were open to initiates of any gender or social class. At the heart of both festivals were myths concerning Demeter as Mother and Persephone as her daughter.
Myths.
Demeter and Persephone.
Demeter's virgin daughter Persephone was abducted to the underworld by Hades. Demeter searched for her ceaselessly, preoccupied with her loss and her grief. The seasons halted; living things ceased their growth, then began to die. Faced with the extinction of all life on earth, Zeus sent his messenger Hermes to the underworld to bring Persephone back. Hades agreed to release her if she had eaten nothing while in his realm; but Persephone had eaten a small number of pomegranate seeds. This bound her to Hades and the underworld for certain months of every year, either the dry Mediterranean summer, when plant life is threatened by drought, or the autumn and winter. There are several variations on the basic myth. In the Homeric hymn to Demeter, Hecate assists in the search and later becomes Persephone's underworld attendant. In another, Persephone willingly and secretly eats the pomegranate seeds, thinking to deceive Hades, but is discovered and made to stay. In all versions, Persephone's time in the underworld corresponds with the unfruitful seasons of the ancient Greek calendar, and her return to the upper world with springtime. Demeter's descent to retrieve Persephone from the underworld is connected to the Eleusinian Mysteries.
Demeter and her daughter Persephone were usually called:
In Mycenaean Pylos, Demeter and Persephone were probably called "queens" (wa-na-ssoi).
The myth of the capture of Persephone seems to be pre-Greek. In the Greek version Ploutos (πλούτος, wealth) represents the wealth of the corn that was stored in underground silos or ceramic jars ("pithoi"). Similar subterranean "pithoi" were used in ancient times for funerary practices is fused with Persephone, the Queen of the underworld. At the beginning of the autumn, when the corn of the old crop is laid on the fields she ascends and is reunited with her mother Demeter, for at this time the old crop and the new meet each other.
According to the personal mythology of Robert Graves, Persephone is not only the younger self of Demeter, she is in turn also one of three guises of the Triple Goddess — Kore (the youngest, the maiden, signifying green young grain), Persephone (in the middle, the nymph, signifying the ripe grain waiting to be harvested), and Hecate (the eldest of the three, the crone, the harvested grain), which to a certain extent reduces the name and role of Demeter to that of group name. Before her abduction, she is called Kore; and once taken she becomes Persephone ('she who brings destruction').
Demeter at Eleusis.
Demeter's search for her daughter Persephone took her to the palace of Celeus, the King of Eleusis in Attica. She assumed the form of an old woman, and asked him for shelter. He took her in, to nurse Demophon and Triptolemus, his sons by Metanira. To reward his kindness, she planned to make Demophon immortal; she secretly anointed the boy with ambrosia and laid him in the flames of the hearth, to gradually burn away his mortal self. But Metanira walked in, saw her son in the fire and screamed in fright. Demeter abandoned the attempt. Instead, she taught Triptolemus the secrets of agriculture, and he in turn taught them to any who wished to learn them. Thus, humanity learned how to plant, grow and harvest grain. The myth has several versions; some are linked to figures such as Eleusis, Rarus and Trochilus. The Demophon element may be based on an earlier folk tale.
Demeter and Poseidon.
Demeter and Poseidon's names appear in the earliest scratched notes in Linear B found at Mycenae and Mycenaean Pylos; "e-ne-si-da-o-ne" (earth-shaker) for Poseidon, and "si-to-po-ti-ni-ja", who is probably related with Demeter. Poseidon carries frequently the title "wa-na-ka" ("wanax") in Linear B inscriptions, as king of the underworld, and his title " E-ne-si-da-o-ne" indicates his chthonic nature. In the cave of Amnisos (Crete) "Enesidaon" is related with the cult of Eileithyia, the goddess of childbirth. She was related with the annual birth of the divine child. During the Bronze Age, a goddess of nature, dominated both in Minoan and Mycenean cult, and "Wanax" ("wa-na-ka") was her male companion (paredros) in Mycenean cult. She and her paredros survived in the Eleusinian cult, where the following words were uttered : " Mighty Potnia bore a strong son" However there is no evidence that originally the name of Potnia was Demeter.
Tablets from Pylos record sacrificial goods destined for "the Two Queens and Poseidon" ("to the Two Queens and the King" :"wa-na-ssoi", "wa-na-ka-te"). The "Two Queens" may be related with Demeter and Persephone, or their precursors, goddesses who were not associated with Poseidon in later periods. An exception is the myth of isolated Arcadia in southern Greece. Despoina, is daughter of Demeter and Poseidon "Hippios", Horse-Poseidon. These myths seem to be connected with the first Greek-speaking people who came from the north during the Bronze age. Poseidon represents the river spirit of the underworld and he appears as a horse as it often happens in northern-European folklore. He pursues the mare-Demeter and she bears one daughter who obviously originally had the form or the shape of a mare too. Demeter and Despoina were closely connected with springs and animals, related to Poseidon as a God of waters and especially with Artemis, the mistress of the animals and the goddess of, among others, the Hunt.
Demeter as mare-goddess was pursued by Poseidon, and hid from him among the horses of King Onkios, but could not conceal her divinity. In the form of a stallion, Poseidon caught and covered her. Demeter was furious (erinys) at Poseidon's assault; in this furious form, she is known as Demeter Erinys. But she washed away her anger in the River Ladon, becoming "Demeter Lousia", the "bathed Demeter". "In her alliance with Poseidon," Karl Kerenyi noted, "she was Earth, who bears plants and beasts, and could therefore assume the shape of an ear of grain or a mare." She bore a daughter Despoina (Δέσποινα: the "Mistress"), whose name should not be uttered outside the Arcadian Mysteries, and a horse named Arion, with a black mane and tail.
In Arcadia, Demeter's mare-form was worshiped into historical times. Her "xoanon" of Phigaleia shows how the local cult interpreted her: a Medusa type with a horse's head with snaky hair, holding a dove and a dolphin, probably representing her power over air and water.
 The second mountain, Mt. Elaios, is about 30 stades from Phigaleia, and has a cave sacred to Demeter Melaine ["Black"]... the Phigalians say, they accounted the cave sacred to Demeter, and set up a wooden image in it. The image was made in the following fashion: it was seated on a rock, and was like a woman in all respects save the head. She had the head and hair of a horse, and serpents and other beasts grew out of her head. Her chiton reached right to her feet, and she held a dolphin in one hand, a dove in the other. Why they made the "xoanon" like this should be clear to any intelligent man who is versed in tradition. They say they named her Black because the goddess wore black clothing. However, they cannot remember who made this xoanon or how it caught fire; but when it was destroyed the Phigalians gave no new image to the goddess and largely neglected her festivals and sacrifices, until finally barrenness fell upon the land.
 — Pausanias, "Description of Greece "8.42.1ff."
Titles and functions.
Demeter's epithets show her many religious functions. She was the "Corn-Mother" who blesses the harvesters. Some cults interpreted her as "Mother-Earth". Demeter may be linked to goddess-cults of Minoan Crete, and embody aspects of a pre-Hellenic Great Goddess. It is possible that the title mistress of the labyrinth that appears in a Linear B inscription belonged originally to "Sito",the Great Mother Demeter and in the Eleusinian mysteries this title was kept by her daughter Persephone (Kore or Despoina). However there is not any evidence that the name of Potnia in Eleusis was originally Demeter. Her other epithets include:
Demeter might also be invoked in the guises of:
Theocritus, wrote of an earlier role of Demeter as a poppy goddess:
In a clay statuette from Gazi (Heraklion Museum, Kereny 1976 fig 15), the Minoan poppy goddess wears the seed capsules, sources of nourishment and narcosis, in her diadem. "It seems probable that the Great Mother Goddess, who bore the names Rhea and Demeter, brought the poppy with her from her Cretan cult to Eleusis, and it is certain that in the Cretan cult sphere, opium was prepared from poppies" (Kerenyi 1976, p 24).
Cult places.
Major cults to Demeter are known at Eleusis in Attica, Hermion (in Crete), Megara, Celeae, Lerna, Aegila, Munychia, Corinth, Delos, Priene, Akragas, Iasos, Pergamon, Selinus, Tegea, Thoricus, Dion (in Macedonia) Lykosoura, Mesembria, Enna (Sicily), and Samothrace.
An ancient Amphictyony, probably the earliest centred on the cult of Demeter at Anthele (Ἀνθήλη), which lay on the coast of Malis south of Thessaly. This was the locality of Thermopylae.
After the "First Sacred War", the Anthelan body was known thenceforth as the Delphic Amphictyony
Demeter of Mysia had a seven-day festival at Pellené in Arcadia. Pausanias passed the shrine to Demeter at Mysia on the road from Mycenae to Argos but all he could draw out to explain the archaic name was a myth of an eponymous Mysius who venerated Demeter.

</doc>
<doc id="8233" url="http://en.wikipedia.org/wiki?curid=8233" title="Death metal">
Death metal

Death metal is an extreme subgenre of heavy metal music. It typically employs heavily distorted and low tuned guitars, played with techniques such as palm muting and tremolo picking, deep growling vocals and screams, aggressive, powerful drumming featuring double kick or blast beat techniques, minor keys or atonality, abrupt tempo, key, and time signature changes and chromatic chord progressions. The lyrical themes of death metal may invoke slasher film-stylized violence, Satanism, religion, occultism, Lovecraftian horror, nature, mysticism, philosophy, science fiction, and politics, and they may describe extreme acts, including mutilation, dissection, torture, rape, cannibalism, and necrophilia.
Building from the musical structure of thrash metal and early black metal, death metal emerged during the mid-1980s. Bands such as Venom, Celtic Frost, Slayer, and Kreator, were important influences on the genre's creation. Possessed and Death, along with bands such as Obituary, Deicide, Cannibal Corpse, and Morbid Angel, are often considered pioneers of the genre. In the late 1980s and early 1990s, death metal gained more media attention as popular genre niche record labels like Combat, Earache, and Roadrunner, began to sign death metal bands at a rapid rate.
Since then, death metal has diversified, spawning several subgenres. Melodic death metal combines death metal elements with those of the New Wave of British Heavy Metal. Technical death metal is a complex style, with uncommon time signatures, atypical rhythms and unusual harmonies and melodies. Death-doom combines the deep growled vocals and double-kick drumming of death metal with the slow tempos and melancholic atmosphere of doom metal. Deathgrind, goregrind and pornogrind mix the complexity of death metal with the intensity, speed, and brevity of grindcore. Deathcore combines death metal with metalcore traits. Death 'n' roll combines death metal's growled vocals and highly distorted, detuned guitar riffs with elements of 1970s hard rock and heavy metal.
History.
Emergence and early history.
English heavy metal band Venom, from Newcastle, crystallized the elements of what later became known as thrash metal, death metal and black metal, with their 1981 album "Welcome to Hell". Their dark, blistering sound, harsh vocals, and macabre, proudly Satanic imagery proved a major inspiration for extreme metal bands. Another highly influential band, Slayer, formed in 1981. Although the band was a thrash metal act, Slayer's music was more violent than their thrash contemporaries Metallica, Megadeth, and Anthrax. Their breakneck speed and instrumental prowess combined with lyrics about death, violence, war, and Satanism won Slayer a rabid cult following. According to AllMusic, their third album "Reign in Blood" inspired the entire death metal genre. It had a big impact on the genre leaders such as Death, Obituary and Morbid Angel.
Possessed, a band that formed in the San Francisco Bay Area during 1983, is attributed by Allmusic for "connecting the dots" between thrash metal and death metal with their 1985 debut album, "Seven Churches". While attributed as having a Slayer influence, current and former members of the band had actually cited Venom and Motörhead, as well as early work by Exodus, as the main influences on their sound. Although the group had released only two studio albums and an EP in their formative years, they have been described by music journalists and musicians as either being "monumental" in developing the death metal style, or as being the first death metal band. Earache Records noted that "the likes of Trey Azagthoth and Morbid Angel based what they were doing in their formative years on the Possessed blueprint laid down on the legendary "Seven Churches" recording. Possessed arguably did more to further the cause of 'Death Metal' than any of the early acts on the scene back in the mid-late 80's."
During the same period as the dawn of Possessed, a second influential metal band was formed in Florida: Death. Death, originally called Mantas, was formed in 1983 by Chuck Schuldiner, Kam Lee, and Rick Rozz. In 1984 they released their first demo entitled "Death by Metal", followed by several more. The tapes circulated through the tape trader world, quickly establishing the band's name. With Death guitarist Schuldiner adopting vocal duties, the band made a major impact on the scene. The fast minor-key riffs and solos were complemented with fast drumming, creating a style that would catch on in tape trading circles. Schuldiner has been credited by Allmusic's Eduardo Rivadavia for being widely recognized as the "Father of Death Metal". Death's 1987 debut release, "Scream Bloody Gore", has been described by About.com's Chad Bowar as being the "evolution from thrash metal to death metal", and "the first true death metal record" by the "San Francisco Chronicle". Along with Possessed and Death, other pioneers of death metal in the United States include Necrophagia, Massacre, Autopsy, Immolation, Cannibal Corpse, and Post Mortem.
Growing popularity.
By 1989, many bands had been signed by eager record labels wanting to cash in on the subgenre, including Florida's Obituary, Morbid Angel and Deicide. This collective of death metal bands hailing from Florida are often labeled as "Florida death metal". Death metal spread to Sweden in the late 1980s, flourishing with pioneers such as Carnage, God Macabre, Entombed, Dismember and Unleashed. In the early 1990s, the rise of melodic death metal was recognized, with bands such as Dark Tranquillity, At the Gates, and In Flames.
Following the original death metal innovators, new subgenres began by the end of the decade. British band Napalm Death became increasingly associated with death metal, in particular, on 1990s "Harmony Corruption". This album displays aggressive and fairly technical guitar riffing, complex rhythmics, a sophisticated growling vocal delivery by Mark "Barney" Greenway, and socially aware lyrical subjects, leading to a merging with the "grindcore" subgenre. Other bands contributing significantly to this early movement include Britain's Bolt Thrower and Carcass, and New York's Suffocation.
To close the circle, Death released their fourth album "Human" in 1991, an example of modern death metal. Death's founder Schuldiner helped push the boundaries of uncompromising speed and technical virtuosity, mixing technical and intricate rhythm guitar work with complex arrangements and emotive guitar solos. Other examples are Carcass's "Necroticism – Descanting the Insalubrious", Suffocation's "Effigy of the Forgotten" and Entombed's "Clandestine" from 1991. At this point, all the above characteristics are present: abrupt tempo and count changes, on occasion extremely fast drumming, morbid lyrics and growling vocal delivery.
Earache Records, Relativity Records and Roadrunner Records became the genre's most important labels, with Earache releasing albums by Carcass, Napalm Death, Morbid Angel, and Entombed, and Roadrunner releasing albums by Obituary, and Pestilence. Although these labels had not been death metal labels, initially, they became the genre's flagship labels in the beginning of the 1990s. In addition to these, other labels formed as well, such as Nuclear Blast, Century Media, and Peaceville. Many of these labels would go on to achieve successes in other genres of metal throughout the 1990s.
In September 1990, Death's manager Eric Greif held one of the first North American death metal festivals, "Day of Death", in Milwaukee suburb Waukesha, Wisconsin, and featured 26 bands including Autopsy, Broken Hope, Hellwitch, Obliveon, Revenant, Viogression, Immolation, Atheist, and Cynic.
Later history.
Death metal's popularity achieved its initial peak between the 1992–93 era, with some bands such as Morbid Angel, Cannibal Corpse, and Obituary, enjoying mild commercial success. However, the genre as a whole never broke into the mainstream. The genre's mounting popularity may have been partly responsible for a strong rivalry between Norwegian black metal and Swedish death metal scenes. Fenriz of Darkthrone has noted that Norwegian black metal musicians were "fed up with the whole death metal scene" at the time. Death metal diversified in the 1990s, spawning a rich variety of subgenres which still have a large "underground" following at the present.
Characteristics.
Instrumentation.
The setup most frequently used within the death metal genre is two guitarists, a bass player, a vocalist and a drummer often using "hyper double-bass blast beats". Although this is the standard setup, bands have been known to occasionally incorporate other instruments such as electronic keyboards. The genre is often identified by fast, highly distorted and low tuned guitars, played with techniques such as palm muting and tremolo picking. The percussion is usually aggressive and powerful.
Death metal is known for its abrupt tempo, key, and time signature changes. Death metal may include chromatic chord progressions and a varied song structure. In some circumstances, the style will incorporate melodic riffs and harmonies for effect. This incorporation of melody and harmonious playing was even further used in the creation of melodic death metal. These compositions tend to emphasize an ongoing development of themes and motifs.
Vocals and lyrics.
Death metal vocals are referred to as death growls; hoarse roars/snarls. Death growling is mistakenly thought to be a form of screaming using the lowest vocal register known as vocal fry, however vocal fry is actually a form of overtone screaming, and while growling can be performed this way by experienced vocalists who use the fry screaming technique, "true" death growling is in fact created by an altogether different technique. The three major methods of harsh vocalization used in the genre are often mistaken for each other, encompassing vocal fry screaming, false chord screaming, and "true" death growls. Growling is sometimes also referred to as Cookie Monster vocals, tongue-in-cheek, due to the vocal similarity to the voice of the popular "Sesame Street" character of the same name. Although often criticized, death growls serve the aesthetic purpose of matching death metal's aggressive lyrical content. High-pitched screaming is occasionally utilized in death metal, being heard in songs by Death, Exhumed, Dying Fetus, Cannibal Corpse, and Deicide.
The lyrical themes of death metal may invoke slasher film-stylized violence, but may also extend to topics like Satanism, religion, occultism, Lovecraftian horror, nature, mysticism, philosophy, science fiction, and politics. Although violence may be explored in various other genres as well, death metal may elaborate on the details of extreme acts, including mutilation, dissection, torture, rape, cannibalism, and necrophilia. Sociologist Keith Kahn-Harris commented this apparent glamorization of violence may be attributed to a "fascination" with the human body that all people share to some degree, a fascination which mixes desire and disgust. Heavy metal author Gavin Baddeley also stated there does seem to be a connection between "how acquainted one is with their own mortality" and "how much they crave images of death and violence" via the media. Additionally, contributing artists to the genre often defend death metal as little more than an extreme form of art and entertainment, similar to horror films in the motion picture industry. This explanation has brought such musicians under fire from activists internationally, who claim that this is often lost on a large number of adolescents, who are left with the glamorization of such violence without social context or awareness of why such imagery is stimulating.
According to Alex Webster, bassist of Cannibal Corpse, "The gory lyrics are probably not, as much as people say, [what's keeping us] from being mainstream. Like, 'death metal would never go into the mainstream because the lyrics are too gory?' I think it's really the music, because violent entertainment is totally mainstream."
Origin of the term.
The most popular theory of the subgenre's christening is Possessed's 1984 demo, "Death Metal"; the song from the eponymous demo would also be featured on the band's 1985 debut album, "Seven Churches". Possessed vocalist/bassist Jeff Becerra said he coined the term in early 1983 for a high school English class assignment. Another possible origin is a fanzine called "Death Metal", started by Thomas Fischer and Martin Ain of Hellhammer and Celtic Frost. The name was later given to the 1984 compilation "Death Metal" released by Noise Records. The term might also have originated from other recordings, such as the demo released by Death in 1984, called "Death by Metal".
Subgenres.
It should be noted that cited examples are not necessarily exclusive to one particular style. Many bands can easily be placed in two or more of the following categories, and a band's specific categorization is often a source of contention due to personal opinion and interpretation.
Other fusions and subgenres.
There are other heavy metal music subgenres that have come from fusions between death metal and other non-metal genres, such as the fusion of death metal and jazz. Atheist and Cynic are two examples; the former went so far as to include jazz-style drum solos on albums, while the latter incorporated elements of jazz fusion. Nile have also incorporated Egyptian music and Middle Eastern themes into their work, while Alchemist have incorporated psychedelia along with Aboriginal music. Some groups, such as Nightfall, Septic Flesh, Fleshgod Apocalypse, and Eternal Tears of Sorrow, have incorporated keyboards and symphonic elements, creating a fusion of symphonic metal and death metal, sometimes referred to as symphonic death metal.

</doc>
<doc id="8237" url="http://en.wikipedia.org/wiki?curid=8237" title="Don Quixote">
Don Quixote

Don Quixote (; ]), fully titled The Ingenious Gentleman Don Quixote of La Mancha (Spanish: "El Ingenioso Hidalgo Don Quijote de la Mancha"), is a Spanish novel by Miguel de Cervantes Saavedra. It follows the adventures of a nameless hidalgo (at the end of Part II given the name Alonso Quixano) who reads so many chivalric romances that he loses his sanity and decides to set out to revive chivalry, undo wrongs, and bring justice to the world, under the name "Don Quixote". He recruits a simple farmer, Sancho Panza, as his squire, who often employs a unique, earthy wit in dealing with Don Quixote's rhetorical orations on antiquated knighthood. Don Quixote, in the first part of the book, does not see the world for what it is, and prefers to imagine that he is living out a knightly story. The story implements various themes, such as intertextuality, realism, metatheatre, and literary representation.
Published in two volumes, in 1606 and 1615, "Don Quixote" is considered one of the most influential works of literature from the Spanish Golden Age and the entire Spanish literary canon. As a founding work of modern Western literature and one of the earliest canonical novels, it regularly appears high on lists of the greatest works of fiction ever published, such as the Bokklubben World Library collection which cites "Don Quixote" as authors' choice for the "best literary work ever written". It has had major influence on the literary community, as evidenced by direct references in Alexandre Dumas' "The Three Musketeers" (1844) and Mark Twain's "Adventures of Huckleberry Finn" (1884), as well as the word "quixotic". Arthur Schopenhauer cited "Don Quixote" as one of the four greatest novels ever written, along with "Tristram Shandy", "La Nouvelle Héloïse", and "Wilhelm Meister".
Summary.
Miguel de Cervantes said that the first chapters are taken from "The Archive of La Mancha" and the rest translated from the Arabic from the Moorish author Cid Hamet Ben Engeli. This metafictional trick appears to be designed to give a greater credibility to the text, by implying that Don Quixote is a real character and that the story truly occurred several decades back. Yet it is obvious to the reader that such a thing is impossible, because the presence of Cide Hamete would have caused numerous temporal anomalies. It was a common method at the time because of the disapproval the novel genre was subject to at that time.
Part 1.
The First Sally (Chapters 1–5).
Alonso Quixano, the protagonist of the novel (though he is not given this name until much later in the book), is a Hidalgo (member of the lesser Spanish nobility), nearing fifty years of age, living in an unnamed section of La Mancha with his niece and housekeeper, as well as a boy who is never heard of again after the first chapter. Although Quixano is usually a rational man, his reading in excess of books of chivalry has produced the distortion of his perception and the wavering of his mental faculties. In keeping with the humorism theory of the time, not sleeping adequately – because he was reading – has caused his brain to dry; Quixano's temperament is thus choleric, the hot and dry humor. As a result, he is easily given to anger and believes every word of these fictional books of chivalry to be true.
Imitating the protagonists of these books, he decides to become a knight-errant in search of adventure. To these ends, he dons an old suit of armour, renames himself "Don Quixote", names his exhausted horse "Rocinante", and designates Aldonza Lorenzo, a neighboring farm girl, as his lady love, renaming her Dulcinea del Toboso, while she knows nothing of this. Expecting to become famous quickly, he arrives at an inn, which he believes to be a castle; calls the prostitutes he meets "ladies" (doncellas); and asks the innkeeper, whom he takes as the lord of the castle, to dub him a knight. He spends the night holding vigil over his armor, and becomes involved in a fight with muleteers who try to remove his armor from the horse trough so that they can water their mules. In a pretended ceremony, the innkeeper dubs him a knight to be rid of him, and sends him on his way.
Don Quixote next "frees" a young boy tied to a tree and beaten by his master, and makes his master swear to treat the boy fairly; but the boy's beating is continued as soon as Quixote leaves. Don Quixote then encounters traders from Toledo, who "insult" the imaginary Dulcinea. He attacks them, only to be severely beaten and left on the side of the road, and returned to his home by a neighboring peasant.
Destruction of Don Quixote's library (Chapters 6 and 7).
While Don Quixote is unconscious in his bed, his niece, the housekeeper, the parish curate, and the local barber burn most of his chivalric and other books. A large part of this section consists of the priest deciding which books deserve to be burned and which to be saved. This gives occasion for many comments on books Cervantes liked and disliked. For example, Cervantes' own pastoral novel La Galatea is saved, while the rather unbelievable romance Felixmarte de Hyrcania is burned. After the books are dealt with, they seal up the room which contained the library, later telling Don Quixote that it was the action of a wizard (encantador).
The Second Sally.
After a short period of feigning health, Don Quixote requests his neighbor, Sancho Panza, to be his squire, promising him governorship of an island, or insula. Sancho, who is both greedy and unintelligent, agrees to the offer and sneaks away with Don Quixote in the early dawn. It is here that their famous adventures begin, starting with Don Quixote's attack on windmills that he believes to be ferocious giants.
The two next encounter a group of friars accompanying a lady in a carriage. Don Quixote takes the friars to be enchanters who hold the lady captive, knocks a friar from his horse, and is immediately challenged by an armed Basque traveling with the company. As he has no shield, the Basque uses a pillow to protect himself, which saves him when Don Quixote strikes him. Cervantes chooses this point, in the middle of the battle, to say that his source ends here. Soon, however, he resumes don Quixote's adventures after a story about finding Arabic notebooks containing the rest of the story by Cide Hamete Benengeli. The combat ends with the lady leaving her carriage and commanding those traveling with her to "surrender" to Don Quixote.
The Pastoral Wanderings.
Sancho and Don Quixote fall in with a group of goatherds. Don Quixote tells Sancho and the goatherds about the "Golden Age" of man, in which property does not exist and men live in peace. The goatherds invite the Knight and Sancho to the funeral of Grisóstomo, once a student who left his studies to become a shepherd after reading pastoral novels (paralleling Don Quixote's decision to become a knight), seeking the shepherdess Marcela. At the funeral Marcela appears, vindicating herself from the bitter verses written about her by Grisóstomo, and claiming her own autonomy and freedom from expectations put on her by Pastoral clichés. She disappears into the woods, and Don Quixote and Sancho follow. Ultimately giving up, the two dismount by a pond to rest. Some Galicians arrive to water their ponies, and Rocinante (Don Quixote's horse) attempts to mate with the ponies. The Galicians hit Rocinante with clubs to dissuade him, whereupon Don Quixote tries to defend Rocinante. The Galicians beat Don Quixote and Sancho, leaving them in great pain.
The Inn.
After Don Quixote and Sancho Panza escape the muleteers, they ride to a nearby inn. Once again, Don Quixote imagines the inn is a castle, although Sancho is not quite convinced. The innkeeper finds a bed for Don Quixote in a former hayloft; Sancho sleeps on a rug next to his bed. Sharing the loft with them is a muleteer. When night comes, Don Quixote imagines the servant girl at the inn, Maritornes, to be a beautiful princess, and makes her sit on his bed with him, scaring her. When the muleteer sees what is happening, he attacks Don Quixote, causing the latter's fragile bed to break. This results in a large and chaotic fight in which Don Quixote and his faithful squire are once again badly hurt. Don Quixote's explanation for everything is that they fought with an enchanted Moor. He also believes that he can cure their wounds with a mixture he calls "the balm of Fierarbras", which only makes them sick. They then decide to leave the inn. However, there is one problem: Don Quixote, following the example of the fictional knights he emulates, refuses to pay, and leaves. Sancho, however, remains and ends up wrapped in a blanket and tossed up in the air (blanketed) by several mischievous guests at the inn, something that will constantly be referenced throughout the rest of the novel. After he is released, he and Don Quixote continue their travels.
The adventures with Cardenio and Dorotea.
After Don Quixote frees a group of galley slaves, he and Sancho wander into the Sierra Morena, and there encounter the dejected Cardenio. Cardenio relates the first part of his story, in which he falls deeply in love with his childhood friend Luscinda, and is hired as the companion to the Duke's son, leading to his friendship with the Duke's younger son, Don Fernando. Cardenio confides in Don Fernando his love for Luscinda and the delays in their engagement, caused by Cardenio's desire to keep with tradition. After reading Cardenio's poems praising Luscinda, Don Fernando falls in love with her. Don Quixote interrupts when Cardenio suggests that his beloved may have become unfaithful after the formulaic stories of spurned lovers in Chivalric novels.
In the course of their travels, the protagonists meet innkeepers, prostitutes, goatherds, soldiers, priests, escaped convicts, and scorned lovers. These characters sometimes tell tales that incorporate events from the real world, like the conquest of the Kingdom of Maynila or battles of the Eighty Years' War. These encounters are magnified by Don Quixote's imagination into chivalrous quests. Don Quixote's tendency to intervene violently in matters irrelevant to himself, and his habit of not paying debts, result in privations, injuries, and humiliations (with Sancho often the victim). Finally, Don Quixote is persuaded to return to his home village. The narrator hints that there was a third quest, but says that records of it have been lost.
Part 2.
The Third Sally.
Although the two parts are now published as a single work, "Don Quixote, Part Two" was a sequel published ten years after the original novel. While Part One was mostly farcical, the second half is more serious and philosophical about the theme of deception.
As "Part Two" begins, it is assumed that the literate classes of Spain have all read the first part of the story. Cervantes's meta-fictional device was to make even the characters in the story familiar with the publication of Part One, as well as with an actually published, fraudulent Part Two. When strangers encounter the duo in person, they already know their famous history. A Duke and Duchess, and others, deceive Don Quixote for entertainment, setting forth a string of imagined adventures resulting in a series of practical jokes. Some of these put Don Quixote's sense of chivalry and his devotion to Dulcinea through many tests. Pressed into finding Dulcinea, Sancho brings back three ragged peasant girls, and tells Don Quixote that they are Dulcinea and her ladies-in-waiting. When Don Quixote only sees the peasant girls, Sancho pretends (reversing some incidents of Part One) that their derelict appearance results from an enchantment.
Sancho later gets his comeuppance for this when, as part of one of the duke and duchess's pranks, the two are led to believe that the only method to release Dulcinea from her spell is for Sancho to give himself three thousand lashes. Sancho naturally resists this course of action, leading to friction with his master. Under the duke's patronage, Sancho eventually gets a governorship, though it is false, and proves to be a wise and practical ruler; though this ends in humiliation as well. Near the end, Don Quixote reluctantly sways towards sanity.
The lengthy untold "history" of Don Quixote's adventures in knight-errantry comes to a close after his battle with the Knight of the White Moon (a young man from Don Quixote's hometown who had previously posed as the Knight of Mirrors) on the beach in Barcelona, in which the reader finds him conquered. Bound by the rules of chivalry, Don Quixote submits to prearranged terms that the vanquished is to obey the will of the conqueror: here, is that Don Quixote is to lay down his arms and cease his acts of chivalry for the period of one year (in which he may be cured of his madness).
Upon returning to his village, Don Quixote announces his plan to retire to the countryside as a shepherd, but his housekeeper urges him to stay home. Soon after, he retires to his bed with a deathly illness, and later awakes from a dream, having fully recovered his sanity. Sancho tries to restore his faith, but Quixano (his proper name) only renounces his previous ambition and apologizes for the harm he has caused. He dictates his will, which includes a provision that his niece will be disinherited if she marries a man who reads books of chivalry. After Alonso Quixano dies, the author emphasizes that there are no more adventures to relate, and that any further books about Don Quixote would be spurious.
Part Two of Don Quixote explores the concept of a character understanding that he is written about: an idea much explored in the 20th century.
Meaning.
It was not until the time Cervantes wrote "Don Quixote" that the Muslim population was made to leave Spain. Several references are made to the forcible expulsion of the Moorish population. The book contains multiple Muslim (or Muslim converso) characters (Ricote, Cide Hamete Benengeli, Princess Zoraida). In the first book, the narrator credits the tale of Don Quixote to a (fictitious) Muslim historian, Cide Hamete Benengeli.
Harold Bloom says that "Don Quixote" is the writing of radical nihilism and anarchy, preferring the glory of fantasy over the real world which includes imminent death, being "...the first modern novel"
Edith Grossman, who wrote and published a highly acclaimed English translation of the novel in 2003, says that the book is mostly meant to move people into emotion using a systematic change of course, on the verge of both tragedy and comedy at the same time.
Themes.
The novel's structure is in episodic form. It is written in the picaresco style of the late 16th century, and features reference other picaresque novels including "Lazarillo de Tormes" and "The Golden Ass". The full title is indicative of the tale's object, as "ingenioso" (Spanish) means "quick with inventiveness" marking the transition of modern literature from Dramatic to thematic unity. The novel takes place over a long period of time, including many adventures united by common themes of the nature of reality, reading, and dialogue in general.
Although burlesque on the surface, the novel, especially in its second half, has served as an important thematic source not only in literature but also in much of art and music, inspiring works by Pablo Picasso and Richard Strauss. The contrasts between the tall, thin, fancy-struck, and idealistic Quixote and the fat, squat, world-weary Panza is a motif echoed ever since the book's publication, and Don Quixote's imaginings are the butt of outrageous and cruel practical jokes in the novel.
Even faithful and simple Sancho is forced to deceive him at certain points. The novel is considered a satire of orthodoxy, veracity, and even nationalism. In exploring the individualism of his characters, Cervantes helped move beyond the narrow literary conventions of the chivalric romance literature that he spoofed, which consists of straightforward retelling of a series of acts that redound to the Knightly Virtues of the hero. The character of Don Quixote became so well known in its time that the word "quixotic" was quickly adopted by many languages. Characters such as Sancho Panza and Don Quixote's steed, Rocinante, are emblems of Western literary culture. The phrase "tilting at windmills" to describe an act of attacking imaginary enemies, derives from an iconic scene in the book.
It stands in a unique position between medieval chivalric romance and the modern novel. The former consist of disconnected stories featuring the same characters and settings with little exploration of the inner life of even the main character. The latter are usually focused on the psychological evolution of their characters. In Part I, Quixote imposes himself on his environment. By Part II, people know about him through "having read his adventures", and so, he needs to do less to maintain his image. By his deathbed, he has regained his sanity, and is once more "Alonso Quixano the Good".
When first published, "Don Quixote" was usually interpreted as a comic novel. After the French revolution it was popular for its central ethic that individuals can be right while society is quite wrong and seen as disenchanting. In the 19th century it was seen as a social commentary, but no one could easily tell "whose side Cervantes was on". Many critics came to view the work as a tragedy in which Don Quixote's idealism and nobility are viewed by the post-chivalric world as insane, and are defeated and rendered useless by common reality. By the 20th century the novel had come to occupy a canonical space as one of the foundations of modern literature.
Background.
Sources.
Sources for "Don Quixote" include the Castillian novel "Amadis de Gaula", which had enjoyed great popularity throughout the 16th century. Another prominent source, which Cervantes evidently admires more, is "Tirant lo Blanch", which the priest describes in Chapter VI of "Quixote" as "the best book in the world." (However, the sense in which it was "best" is much debated among scholars. The passage is called since the nineteenth century "the most difficult passage of "Don Quixote"".)
The scene of the book burning gives us an excellent list of Cervantes's likes and dislikes about literature.
Cervantes makes a number of references to the Italian poem "Orlando furioso". In chapter 10 of the first part of the novel, Don Quixote says he must take the magical helmet of Mambrino, an episode from Canto I of "Orlando", and itself a reference to Matteo Maria Boiardo's "Orlando innamorato". The interpolated story in chapter 33 of Part four of the First Part is a retelling of a tale from Canto 43 of "Orlando", regarding a man who tests the fidelity of his wife.
Another important source appears to have been Apuleius's "The Golden Ass", one of the earliest known novels, a picaresque from late classical antiquity. The wineskins episode near the end of the interpolated tale "The Curious Impertinent" in chapter 35 of the first part of "Don Quixote" is a clear reference to Apuleius, and recent scholarship suggests that the moral philosophy and the basic trajectory of Apuleius's novel are fundamental to Cervantes's program. Similarly, many of both Sancho's adventures in Part II and proverbs throughout are taken from popular Spanish and Italian folklore.
Cervantes's experiences as a galley slave in Algiers also influenced "Quixote".
Spurious "Second Part" by Avellaneda.
It is not certain when Cervantes began writing "Part Two" of "Don Quixote", but he had probably not gotten much further than Chapter LIX by late July 1614. About September, however, a spurious Part Two, entitled "Second Volume of the Ingenious Gentleman Don Quixote of La Mancha: by the Licenciado (doctorate) Alonso Fernández de Avellaneda, of Tordesillas", was published in Tarragona by an unidentified Aragonese who was an admirer of Lope de Vega, rival of Cervantes.
Some modern scholars suggest that Don Quixote's fictional encounter with Avellaneda in Chapter 59 of Part II should not be taken as the date that "Cervantes" encountered it, which may have been much earlier.
Avellaneda's identity has been the subject of many theories, but there is no consensus as to who he was. In its prologue, the author gratuitously insulted Cervantes, who not surprisingly took offense and responded; the last half of Chapter LIX and most of the following chapters of Cervantes' "Segunda Parte" lend some insight into the effects upon him; Cervantes manages to work in some subtle digs at Avellaneda's own work, and in his preface to Part II, comes very near to criticizing Avellaneda directly.
In his introduction to "The Portable Cervantes", Samuel Putnam, a noted translator of Cervantes' novel, calls Avellaneda's version "one of the most disgraceful performances in history".
The second part of Cervantes' "Don Quixote", finished as a direct result of the Avellaneda book, has come to be regarded by some literary critics as superior to the first part, because of its greater depth of characterization, its discussions, mostly between Quixote and Sancho, on diverse subjects, and its philosophical insights.
Other stories.
"Don Quixote, Part One" contains a number of stories which do not directly involve the two main characters, but which are narrated by some of the picaresque figures encountered by the Don and Sancho during their travels. The longest and best known of these is "El Curioso Impertinente" (the impertinently curious man), found in Part One, Book Four. This story, read to a group of travelers at an inn, tells of a Florentine nobleman, Anselmo, who becomes obsessed with testing his wife's fidelity, and talks his close friend Lothario into attempting to seduce her, with disastrous results for all.
In "Part Two", the author acknowledges the criticism of his digressions in "Part One" and promises to concentrate the narrative on the central characters (although at one point he laments that his narrative muse has been constrained in this manner). Nevertheless, "Part Two" contains several back narratives related by peripheral characters.
Several abridged editions have been published which delete some or all of the extra tales in order to concentrate on the central narrative.
Style.
Spelling and pronunciation.
Cervantes wrote his work in an early modern form of Spanish, heavily borrowing from Old Castilian, the medieval form of the Spanish language. The language of "Don Quixote", although still containing archaisms, is far more understandable to modern Spanish readers than is, for instance, the completely medieval Spanish of the "Poema de mio Cid", a kind of Spanish that is as different from Cervantes's language as Middle English is from Modern English. The Old Castilian language was also used to show the higher class that came with being a knight errant.
In Don Quixote there are basically two different types of Castilian: Old Castilian is spoken only by Don Quixote, while the rest of the roles speak a modern version of Spanish. The Old Castilian of Don Quixote is a humoristic resource – he copies the language spoken in the chivalric books that made him mad; and many times, when he talks nobody is able to understand him because his language is too old. This humorous effect is more difficult to see nowadays because the reader must be able to distinguish the two old versions of the language, but when the book was published it was much celebrated. (English translations can get some sense of the effect by having Don Quixote use King James Bible or Shakespearian English phrases.)
In Old Castilian the letter "x" represented the sound written "sh" in modern English, so the name was originally pronounced " "[kiˈʃote]. However as Old Castilian evolved towards modern Spanish, a sound change caused it to be pronounced with a voiceless velar fricative sound (like the Scottish or German "ch"), and today the Spanish pronunciation of "Quixote" is [kiˈxote]. The original pronunciation is reflected in languages such as Astur-Leonese, Galician, Catalan, Italian, Portuguese, and French, where it is pronounced with a "sh" or "ch" sound; the French opera "Don Quichotte" is one of the best-known modern examples of this pronunciation.
Today, English speakers generally attempt something close to the modern Spanish pronunciation of "Quixote" ("Quijote"), as [dɒŋ kiːˈhoʊteɪ] , although the traditional English spelling-based pronunciation with the value of the letter x in modern English is still sometimes used, resulting in /ˈkwɪksət/ or /ˈkwɪksoʊt/. In Australian English, the preferred pronunciation amongst members of the educated classes was /ˈkwɪksət/ until well into the 1970s, as part of a tendency for the upper class to "anglicise its borrowing ruthlessly". The traditional English rendering is preserved in the pronunciation of the adjectival form "quixotic", i.e., /kwɪkˈsoʊtɨk/ or /kwɪkˈsɒtɪk/, defined by Merriam-Webster as the foolishly impractical pursuit of ideals, typically marked by rash and lofty romanticism.
Setting.
Cervantes' story takes place on the plains of La Mancha, specifically the "comarca" of Campo de Montiel.
"En un lugar de La Mancha, de cuyo nombre no quiero acordarme, no ha mucho tiempo que vivía un hidalgo de los de lanza en astillero, adarga antigua, rocín flaco y galgo corredor."(Somewhere in La Mancha, in a place whose name I do not care to remember, a gentleman lived not long ago, one of those who has a lance and ancient shield on a shelf and keeps a skinny nag and a greyhound for racing.)—Miguel de Cervantes, "Don Quixote", Volume I, Chapter I (translated by Edith Grossman)
The story also takes place in El Toboso where Don Quixote goes to seek Dulcinea's blessings.
The location of the village to which Cervantes alludes in the opening sentence of "Don Quixote" has been the subject of debate since its publication over four centuries ago. Indeed, Cervantes deliberately omits the name of the village, giving an explanation in the final chapter:
Such was the end of the Ingenious Gentleman of La Mancha, whose village Cide Hamete would not indicate precisely, in order to leave all the towns and villages of La Mancha to contend among themselves for the right to adopt him and claim him as a son, as the seven cities of Greece contended for Homer.—Miguel de Cervantes, "Don Quixote", Volume II, Chapter 74
In 2004, a multidisciplinary team of academics from Complutense University, led by Francisco Parra Luna, Manuel Fernández Nieto and Santiago Petschen Verdaguer, deduced that the village was that of Villanueva de los Infantes. Their findings were published in a paper titled "'El Quijote' como un sistema de distancias/tiempos: hacia la localización del lugar de la Mancha", which was later published as a book: "El enigma resuelto del Quijote". The result was replicated in two subsequent investigations: "La determinación del lugar de la Mancha como problema estadístico" and "The Kinematics of the Quixote and the Identity of the 'Place in La Mancha'".
Language.
Because of its widespread influence, "Don Quixote" also helped cement the modern Spanish language. The opening sentence of the book created a classic Spanish cliché with the phrase "de cuyo nombre no quiero acordarme" ("whose name I do not wish to recall"): "En un lugar de la Mancha, de cuyo nombre no quiero acordarme, no hace mucho tiempo que vivía un hidalgo de los de lanza en astillero, adarga antigua, rocín flaco y galgo corredor." ("In a village of La Mancha, whose name I do not wish to recall, there lived, not very long ago, one of those gentlemen with a lance in the lance-rack, an ancient shield, a skinny old horse, and a fast greyhound.")
The novel's farcical elements make use of punning and similar verbal playfulness. Character-naming in "Don Quixote" makes ample figural use of contradiction, inversion, and irony, such as the names "Rocinante" (a reversal) and "Dulcinea" (an allusion to illusion), and the word "quixote" itself, possibly a pun on "quijada" (jaw) but certainly "cuixot" (Catalan: thighs), a reference to a horse's rump.
As a military term, the word "quijote" refers to "cuisses", part of a full suit of plate armour protecting the thighs. The Spanish suffix "-ote" denotes the augmentative—for example, "grande" means large, but "grandote" means extra large. Following this example, "Quixote" would suggest 'The Great Quijano', a play on words that makes much sense in light of the character's delusions of grandeur.
"La Mancha" is a region of Spain, but "mancha" (Spanish word) means spot, mark, stain. Translators such as John Ormsby have declared La Mancha to be one of the most desertlike, unremarkable regions of Spain, the least romantic and fanciful place that one would imagine as the home of a courageous knight.
Publication.
In July 1604, Cervantes sold the rights of "El ingenioso hidalgo don Quixote de la Mancha" (known as "Don Quixote, Part I") to the publisher-bookseller Francisco de Robles for an unknown sum. License to publish was granted in September, the printing was finished in December, and the book came out on 16 January 1605.
The novel was an immediate success. The majority of the 400 copies of the first edition were sent to the New World, with the publisher hoping to get a better price in the Americas. Although most of them disappeared in a shipwreck near La Havana, approximately 70 copies reached Lima, from where they were sent to Cuzco in the heart of the defunct Inca Empire.
No sooner was it in the hands of the public than preparations were made to issue derivative (pirated) editions. "Don Quixote" had been growing in favour, and its author's name was now known beyond the Pyrenees. By August 1605 there were two Madrid editions, two published in Lisbon, and one in Valencia. A second edition was produced with additional copyrights for Aragon and Portugal, which publisher Francisco de Robles secured.
Sale of these publishing rights deprived Cervantes of further financial profit on "Part One." In 1607, an edition was printed in Brussels. Robles, the Madrid publisher, found it necessary to meet demand with a third edition, a seventh publication in all, in 1608. Popularity of the book in Italy was such that a Milan bookseller issued an Italian edition in 1610. Yet another Brussels edition was called for in 1611. Since then, numerous editions have been released and in total, the novel is believed to have sold more than 10 million copies worldwide.
In 1613, Cervantes published the "Novelas Ejemplares", dedicated to the Maecenas of the day, the Conde de Lemos. Eight and a half years after "Part One" had appeared, we get the first hint of a forthcoming "Segunda Parte" (Part Two). "You shall see shortly," Cervantes says, "the further exploits of Don Quixote and humours of Sancho Panza." "Don Quixote, Part Two", published by the same press as its predecessor, appeared late in 1615, and quickly reprinted in Brussels and Valencia (1616) and Lisbon (1617). Part two capitalizes on the potential of the first while developing and diversifying the material without sacrificing familiarity. Many people agree that it is richer and more profound. Parts One and Two were published as one edition in Barcelona in 1617. Historically, Cervantes's work has been said to have "smiled Spain's chivalry away", suggesting that Don Quixote as a chivalric satire contributed to the demise of Spanish Chivalry.
English editions in translation.
There are many translations of the book, and it has been adapted many times in shortened versions. Many derivative editions were also written at the time, as was the custom of envious or unscrupulous writers. Seven years after the "Parte Primera" appeared, "Don Quixote" had been translated into French, German, Italian, and English, with the first French translation of 'Part II' appearing in 1618, and the first English translation in 1620. One abridged adaptation, authored by Agustín Sánchez, runs slightly over 150 pages, cutting away about 750 pages.
Thomas Shelton's English translation of the "First Part" appeared in 1612 while Cervantes was still alive, although there is no evidence that Shelton had met the author. Although Shelton's version is cherished by some, according to John Ormsby and Samuel Putnam, it was far from satisfactory as a carrying over of Cervantes's text. Shelton's translation of the novel's "Second Part" appeared in 1620.
Near the end of the 17th century, John Phillips, a nephew of poet John Milton, published what Putnam considered the worst English translation. The translation, as literary critics claim, was not based on Cervantes' text but mostly upon a French work by Filleau de Saint-Martin and upon notes which Thomas Shelton had written.
Around 1700, a version by Pierre Antoine Motteux appeared. Motteux's translation enjoyed lasting popularity; it was reprinted as the Modern Library Series edition of the novel until recent times. Nonetheless, future translators would find much to fault in Motteux's version: Samuel Putnam criticized "the prevailing slapstick quality of this work, especially where Sancho Panza is involved, the obtrusion of the obscene where it is found in the original, and the slurring of difficulties through omissions or expanding upon the text". John Ormsby considered Motteux's version "worse than worthless", and denounced its "infusion of Cockney flippancy and facetiousness" into the original.
The proverb 'The proof of the pudding is in the eating' is widely attributed to Cervantes. The Spanish word for pudding, 'budín', however doesn't appear in the original text but
premieres in the Motteux translation. In Smolletts translation of 1755 he notes that the original text reads literally "you will see when the eggs are fried" meaning 'time will tell'.
A translation by Captain John Stevens, which revised Thomas Shelton's version, also appeared in 1700, but its publication was overshadowed by the simultaneous release of Motteux's translation.
In 1742, the Charles Jervas translation appeared, posthumously. Through a printer's error, it came to be known, and is still known, as "the Jarvis translation". It was the most scholarly and accurate English translation of the novel up to that time, but future translator John Ormsby points out in his own introduction to the novel that the Jarvis translation has been criticized as being too stiff. Nevertheless, it became the most frequently reprinted translation of the novel until about 1885. Another 18th century translation into English was that of Tobias Smollett, himself a novelist, first published in 1755. Like the Jarvis translation, it continues to be reprinted today.
Most modern translators take as their model the 1885 translation by John Ormsby. It is said that his translation was the most honest of all translations, without expansions upon the text or changing of the proverbs.
An expurgated children's version, under the title "The Story of Don Quixote", was published in 1922 (available on Project Gutenberg). It leaves out the risqué sections as well as chapters that young readers might consider dull, and embellishes a great deal on Cervantes's original text. The title page actually gives credit to the two editors as if they were the authors, and omits any mention of Cervantes.
The most widely read English-language translations of the mid-20th century are by Samuel Putnam (1949), J. M. Cohen (1950; Penguin Classics), and Walter Starkie (1957). The last English translation of the novel in the 20th century was by Burton Raffel, published in 1996. The 21st century has already seen four new translations of the novel into English. The first is by John D. Rutherford and the second by Edith Grossman. Reviewing the novel in the "New York Times", Carlos Fuentes called Grossman's translation a "major literary achievement" and another called it the "most transparent and least impeded among more than a dozen English translations going back to the 17th century."
In 2005, the year of the novel's 400th anniversary, Tom Lathrop published a new English translation of the novel, based on a lifetime of specialized study of the novel and its history.
The fourth translation of the 21st century was released in 2006 by former Spanish professor James Montgomery, 26 years after he had begun it, in an attempt to "recreate the sense of the original as closely as possible, though not at the expense of Cervantes' literary style."
See also.
General:

</doc>
<doc id="8239" url="http://en.wikipedia.org/wiki?curid=8239" title="Dylan">
Dylan

Dylan may refer to:

</doc>
<doc id="8240" url="http://en.wikipedia.org/wiki?curid=8240" title="Dada">
Dada

Dada () or Dadaism was an art movement of the European avant-garde in the early 20th century. Dada in Zurich, Switzerland, began in 1916, spreading to Berlin shortly thereafter, but the height of New York Dada was the year before, in 1915. The term anti-art, a precursor to Dada, was coined by Marcel Duchamp around 1913 when he created his first readymades. Dada, in addition to being anti-war, had political affinities with the radical left and was also anti-bourgeois.
The roots of Dada lay in pre-war avant-garde. Cubism and the development of collage, combined with Wassily Kandinsky’s theoretical writings and abstraction, detached the movement from the constraints of reality and convention. The influence of French poets and the writings of German Expressionists liberated Dada from the tight correlation between words and meaning. Avant-garde circles outside of France knew of pre-war Parisian developments. They had seen (or participated in) Cubist exhibitions held at Galería Dalmau, Barcelona (1912), Galerie Der Sturm in Berlin (1912), the Armory show in New York (1913), SVU Mánes in Prague (1914), several Jack of Diamonds exhibitions in Moscow and at De Moderne Kunstkring, Amsterdam (between 1911 and 1915). Futurism developed in response to the work of various artists. Dada subsequently combined these approaches.
Dada activities included public gatherings, demonstrations, and publication of art/literary journals; passionate coverage of art, politics, and culture were topics often discussed in a variety of media. Key figures in the movement included Hugo Ball, Emmy Hennings, Hans Arp, Raoul Hausmann, Hannah Höch, Johannes Baader, Tristan Tzara, Francis Picabia, Richard Huelsenbeck, George Grosz, John Heartfield, Marcel Duchamp, Beatrice Wood, Kurt Schwitters, Hans Richter, and Max Ernst, among others. The movement influenced later styles like the avant-garde and downtown music movements, and groups including surrealism, Nouveau Réalisme, pop art and Fluxus.
Overview.
Dada was an informal international movement, with participants in Europe and North America. The beginnings of Dada correspond to the outbreak of World War I. For many participants, the movement was a protest against the bourgeois nationalist and colonialist interests, which many Dadaists believed were the root cause of the war, and against the cultural and intellectual conformity—in art and more broadly in society—that corresponded to the war.
Many Dadaists believed that the 'reason' and 'logic' of bourgeoisie capitalist society had led people into war. They expressed their rejection of that ideology in artistic expression that appeared to reject logic and embrace chaos and irrationality. For example, George Grosz later recalled that his Dadaist art was intended as a protest "against this world of mutual destruction."
According to Hans Richter Dada was not art: it was "anti-art." Dada represented the opposite of everything which art stood for. Where art was concerned with traditional aesthetics, Dada ignored aesthetics. If art was to appeal to sensibilities, Dada was intended to offend.
As Hugo Ball expressed it, "For us, art is not an end in itself ... but it is an opportunity for the true perception and criticism of the times we live in."
A reviewer from the "American Art News" stated at the time that "Dada philosophy is the sickest, most paralyzing and most destructive thing that has ever originated from the brain of man." Art historians have described Dada as being, in large part, a "reaction to what many of these artists saw as nothing more than an insane spectacle of collective homicide."
Years later, Dada artists described the movement as "a phenomenon bursting forth in the midst of the postwar economic and moral crisis, a savior, a monster, which would lay waste to everything in its path... [It was] a systematic work of destruction and demoralization... In the end it became nothing but an act of sacrilege."
To quote Dona Budd's "The Language of Art Knowledge", Dada was born out of negative reaction to the horrors of the First World War. This international movement was begun by a group of artists and poets associated with the Cabaret Voltaire in Zurich. Dada rejected reason and logic, prizing nonsense, irrationality and intuition. The origin of the name Dada is unclear; some believe that it is a nonsensical word. Others maintain that it originates from the Romanian artists Tristan Tzara's and Marcel Janco's frequent use of the words "da, da," meaning "yes, yes" in the Romanian language. Another theory says that the name "Dada" came during a meeting of the group when a paper knife stuck into a French-German dictionary happened to point to 'dada', a French word for 'hobbyhorse'. The movement primarily involved visual arts, literature, poetry, art manifestos, art theory, theatre, and graphic design, and concentrated its anti-war politics through a rejection of the prevailing standards in art through anti-art cultural works.
History.
Zurich.
In 1916, Hugo Ball, Emmy Hennings, Tristan Tzara, Jean Arp, Marcel Janco, Richard Huelsenbeck, Sophie Täuber, and Hans Richter, along with others, discussed art and put on performances in the Cabaret Voltaire expressing their disgust with the war and the interests that inspired it.
Some sources state that Dada coalesced on October 6 at the Cabaret Voltaire. Other sources state that Dada did not originate fully in a Zurich literary salon but grew out of an already vibrant artistic tradition in Eastern Europe, particularly Romania, that transposed to Switzerland when a group of Jewish modernist artists (Tzara, Janco, Arthur Segal, and others) settled in Zurich. In the years prior to the First World War similar art had already risen in Bucharest and other Eastern European cities; it is likely that DADA's catalyst was the arrival in Zurich of artists like Tzara and Janco.
Having left Germany and Romania during the Great War, the artists found themselves in Switzerland, a country recognized for its neutrality. Inside this space of political neutrality they decided to use abstraction to fight against the social, political, and cultural ideas of that time. The dadaists believed those ideas to be a byproduct of bourgeois society, a society so apathetic it would rather fight a war against itself than challenge the "status quo".
Janco recalled, "We had lost confidence in our culture. Everything had to be demolished. We would begin again after the "tabula rasa". At the Cabaret Voltaire we began by shocking common sense, public opinion, education, institutions, museums, good taste, in short, the whole prevailing order."
The Cabaret closed its doors in early July and then at the first public soiree at Waag Hall on July 14, 1916, Ball recited the . In 1917, Tzara wrote a second considered one of the most important Dada writings, which was published in 1918. Other manifestos followed.
A single issue of the magazine "Cabaret Voltaire" was the first publication to come out of the movement.
After the cabaret closed down, Dada activities moved on to a new gallery, and Hugo Ball left for Bern. Tzara began a relentless campaign to spread Dada ideas. He bombarded French and Italian artists and writers with letters, and soon emerged as the Dada leader and master strategist. The Cabaret Voltaire re-opened, and is still in the same place at the Spiegelgasse 1 in the Niederdorf.
Zurich Dada, with Tzara at the helm, published the art and literature review "Dada" beginning in July 1917, with five editions from Zurich and the final two from Paris.
After the fighting of the First World War had ended in the armistice of November 1918, most of the Zurich Dadaists returned to their home countries, and some began Dada activities in other cities. Others, such as the Swiss native Sophie Täuber, would remain in Zurich into the 1920s.
Berlin.
"Berlin was a city of tightened stomachers, of mounting, thundering hunger, where hidden rage was transformed into a boundless money lust, and men’s minds were concentrating more and more on questions of naked existence... Fear was in everybody’s bones "- Richard Hülsenbeck
The groups in Germany were not as strongly anti-art as other groups. Their activity and art were more political and social, with corrosive manifestos and propaganda, satire, public demonstrations and overt political activities. The intensely political and war-torn environment of Berlin had a dramatic impact on the ideas of Berlin Dadaists. Conversely, New York's geographic distance from the war spawned its more theoretically-driven, less political nature.
In February 1918, while the Great War was approaching its climax, Huelsenbeck gave his first Dada speech in Berlin, and he produced a Dada manifesto later in the year. Following the October Revolution in Russia, by then out of the war, Hannah Höch and George Grosz used Dada to express communist sympathies. Grosz, together with John Heartfield, Höch and Hausmann developed the technique of photomontage during this period. After the war, the artists published a series of short-lived political magazines and held the "First International Dada Fair", 'the greatest project yet conceived by the Berlin Dadaists', in the summer of 1920. As well as work by the main members of Berlin Dada – Grosz, Raoul Hausmann, Hannah Höch, Johannes Baader, Huelsenbeck and Heartfield – the exhibition also included the work of Otto Dix, Francis Picabia, Jean Arp, Max Ernst, Rudolf Schlichter, Johannes Baargeld and others. In all, over 200 works were exhibited, surrounded by incendiary slogans, some of which also ended up written on the walls of the Nazi's "Entartete Kunst" exhibition in 1937. Despite high ticket prices, the exhibition lost money, with only one recorded sale.
The Berlin group published periodicals such as "Club Dada", "Der Dada", "Everyman His Own Football", and "Dada Almanach".
Cologne.
In Cologne, Ernst, Baargeld, and Arp launched a controversial Dada exhibition in 1920 which focused on nonsense and anti-bourgeois sentiments. Cologne's Early Spring Exhibition was set up in a pub, and required that participants walk past urinals while being read lewd poetry by a woman in a communion dress. The police closed the exhibition on grounds of obscenity, but it was re-opened when the charges were dropped.
New York.
Like Zurich, New York City was a refuge for writers and artists from the First World War. Soon after arriving from France in 1915, Marcel Duchamp and Francis Picabia met American artist Man Ray. By 1916 the three of them became the center of radical anti-art activities in the United States. American Beatrice Wood, who had been studying in France, soon joined them, along with Elsa von Freytag-Loringhoven. Arthur Cravan, fleeing conscription in France, was also present for a time. Much of their activity centered in Alfred Stieglitz's gallery, 291, and the home of Walter and Louise Arensberg.
The New Yorkers, though not particularly organized, called their activities "Dada," but they did not issue manifestos. They issued challenges to art and culture through publications such as "The Blind Man", "Rongwrong", and "New York Dada" in which they criticized the traditionalist basis for "museum" art. New York Dada lacked the disillusionment of European Dada and was instead driven by a sense of irony and humor. In his book "Adventures in the arts: informal chapters on painters, vaudeville and poets" Marsden Hartley included an essay on "".
During this time Duchamp began exhibiting "readymades" (everyday objects found or purchased and declared art) such as a bottle rack, and was active in the Society of Independent Artists. In 1917 he submitted the now famous "Fountain", a urinal signed R. Mutt, to the Society of Independent Artists exhibition only to have the piece rejected. First an object of scorn within the arts community, the "Fountain" has since become almost canonized by some as one of the most recognizable modernist works of sculpture. The committee presiding over Britain's prestigious Turner Prize in 2004, for example, called it "the most influential work of modern art." As recent scholarship documents, the work is likely more collaborative than it has been given credit for in twentieth-century art history. Duchamp indicates in a 1917 letter to his sister that a female friend was centrally involved in the conception of this work. As he writes: "One of my female friends who had adopted the pseudonym Richard Mutt sent me a porcelain urinal as a sculpture." The piece is more in line with the scatological aesthetics of Duchamp's friend and neighbour, the Baroness Elsa von Freytag-Loringhoven, than Duchamp's. In an attempt to "pay homage to the spirit of Dada" a performance artist named Pierre Pinoncelli made a crack in "The Fountain" with a hammer in January 2006; he also urinated on it in 1993.
Picabia's travels tied New York, Zurich and Paris groups together during the Dadaist period. For seven years he also published the Dada periodical "391" in Barcelona, New York City, Zurich, and Paris from 1917 through 1924.
By 1921, most of the original players moved to Paris where Dada experienced its last major incarnation.
Paris.
The French avant-garde kept abreast of Dada activities in Zurich with regular communications from Tristan Tzara (whose pseudonym means "sad in country," a name chosen to protest the treatment of Jews in his native Romania), who exchanged letters, poems, and magazines with Guillaume Apollinaire, André Breton, Max Jacob, Clément Pansaers, and other French writers, critics and artists.
Paris had arguably been the classical music capital of the world since the advent of musical Impressionism in the late 19th century. One of its practitioners, Erik Satie, collaborated with Picasso and Cocteau in a mad, scandalous ballet called "Parade". First performed by the Ballets Russes in 1917, it succeeded in creating a scandal but in a different way than Stravinsky's "Le Sacre du Printemps" had done almost five years earlier. This was a ballet that was clearly parodying itself, something traditional ballet patrons would obviously have serious issues with.
Dada in Paris surged in 1920 when many of the originators converged there. Inspired by Tzara, Paris Dada soon issued manifestos, organized demonstrations, staged performances and produced a number of journals (the final two editions of "Dada", "Le Cannibale", and "Littérature" featured Dada in several editions.)
The first introduction of Dada artwork to the Parisian public was at the Salon des Indépendants in 1921. Jean Crotti exhibited works associated with Dada including a work entitled, "Explicatif" bearing the word "Tabu". In the same year Tzara staged his Dadaist play "The Gas Heart" to howls of derision from the audience. When it was re-staged in 1923 in a more professional production, the play provoked a theatre riot (initiated by André Breton) that heralded the split within the movement that was to produce Surrealism. Tzara's last attempt at a Dadaist drama was his "ironic tragedy" "Handkerchief of Clouds" in 1924.
Netherlands.
In the Netherlands the Dada movement centered mainly around Theo van Doesburg, best known for establishing the De Stijl movement and magazine of the same name. Van Doesburg mainly focused on poetry, and included poems from many well-known Dada writers in "De Stijl" such as Hugo Ball, Hans Arp and Kurt Schwitters. Van Doesburg and Thijs Rinsema became friends of Schwitters, and together they organized the so-called "Dutch Dada campaign" in 1923, where Van Doesburg promoted a leaflet about Dada (entitled "What is Dada?"), Schwitters read his poems, Vilmos Huszàr demonstrated a mechanical dancing doll and Nelly Van Doesburg (Theo's wife), played avant-garde compositions on piano.
Van Doesburg wrote Dada poetry himself in "De Stijl", although under a pseudonym, I.K. Bonset, which was only revealed after his death in 1931. 'Together' with I.K. Bonset, he also published a short-lived Dutch Dada magazine called "Mécano".
Georgia.
Although Dada itself was unknown in Georgia until at least 1920, from 1917-1921 a group of poets called themselves "41st Degree" (referring both to the latitude of Tbilisi, Georgia and to the temperature of a high fever) organized along Dadaist lines. The most important figure in this group was Iliazd, whose radical typographical designs visually echo the publications of the Dadaists. After his flight to Paris in 1921, he collaborated with Dadaists on publications and events.
Yugoslavia.
In Yugoslavia there was heavy Dada activity between 1920 and 1922, run mainly by Dragan Aleksić and including work by Mihailo S. Petrov, Zenitist's two brothers Ljubomir Micić and Branko Ve Poljanski. Aleksić used the term "Yougo-Dada" and is known to have been in contact with Raoul Hausmann, Kurt Schwitters, and Tristan Tzara.
Italy.
The Dada movement in Italy, based in Mantua, was met with distaste and failed to make a significant impact in the world of art. It published a magazine for a short time and held an exhibition in Rome, featuring paintings, quotations from Tristan Tzara, and original epigrams such as "True Dada is against Dada". The most notable member of this group was Julius Evola, who went on to become an eminent scholar of occultism, as well as a right-wing philosopher and an assistant to Benito Mussolini.
Tokyo.
A prominent Dada group in Japan was MAVO (), founded in July 1923 by Tomoyoshi Murayama and Masamu Yanase (, ). Other prominent artists were Jun Tsuji, Eisuke Yoshiyuki, Shinkichi Takahashi () and Katsue Kitasono.
Russia.
The Russian literary group "Nichevoki" came close to the Dada ideologies. Members became famous for proposing that Vladimir Mayakovsky should go to the Pushkin monument at Tverskoy Boulevard and clean the shoes of anyone who asked, after he had declared that he would "clean up Russian poetry".
Poetry; music and sound.
Dada was not confined to the visual and literary arts; its influence reached into sound and music. Kurt Schwitters developed what he called "sound poems", while Francis Picabia and Georges Ribemont-Dessaignes composed Dada music performed at the Festival Dada in Paris on 26 May 1920. Other composers such as Erwin Schulhoff, Hans Heusser and Albert Savinio all wrote "Dada music", while members of Les Six collaborated with members of the Dada movement and had their works performed at Dada gatherings. Erik Satie also dabbled with Dadaist ideas during his career, although he is primarily associated with musical Impressionism.
In the very first Dada publication, Hugo Ball describes a "balalaika orchestra playing delightful folk-songs." African music and jazz was common at Dada gatherings, signaling a return to nature and naive primitivism.
Marc Lowenthal, in "I Am a Beautiful Monster: Poetry, Prose, and Provocation", writes:
Dada is the groundwork to abstract art and sound poetry, a starting point for performance art, a prelude to postmodernism, an influence on pop art, a celebration of antiart to be later embraced for anarcho-political uses in the 1960s and the movement that laid the foundation for Surrealism.
Legacy.
While broadly based, the movement was unstable. By 1924 in Paris, Dada was melding into surrealism, and artists had gone on to other ideas and movements, including surrealism, social realism and other forms of modernism. Some theorists argue that Dada was actually the beginning of postmodern art.
By the dawn of the Second World War, many of the European Dadaists had emigrated to the United States. Some died in death camps under Adolf Hitler, who actively persecuted the kind of "degenerate art" that he considered Dada to represent. The movement became less active as post-war optimism led to the development of new movements in art and literature.
Dada is a named influence and reference of various anti-art and political and cultural movements, including the Situationist International and culture jamming groups like the Cacophony Society. Upon breaking up in July 2012, famous anarchist pop band Chumbawamba issued a statement which compared their own legacy with that of the Dada art movement.
At the same time that the Zurich Dadaists were making noise and spectacle at the Cabaret Voltaire, Lenin was planning his revolutionary plans for Russia in a nearby apartment. Tom Stoppard used this coincidence as a premise for his play "Travesties" (1974), which includes Tzara, Lenin, and James Joyce as characters. French writer Dominique Noguez imagined Lenin as a member of the Dada group in his tongue-in-cheek "Lénine Dada" (1989).
The former building of the Cabaret Voltaire fell into disrepair until it was occupied from January to March, 2002, by a group proclaiming themselves Neo-Dadaists, led by Mark Divo. The group included Jan Thieler, Ingo Giezendanner, Aiana Calugar, Lennie Lee, and Dan Jones. After their eviction, the space was turned into a museum dedicated to the history of Dada. The work of Lee and Jones remained on the walls of the new museum.
Several notable retrospectives have examined the influence of Dada upon art and society. In 1967, a large Dada retrospective was held in Paris. In 2006, the Museum of Modern Art in New York City mounted a Dada exhibition in partnership with the National Gallery of Art in Washington D.C. and the Centre Pompidou in Paris. The LTM label has released a large number of Dada-related sound recordings, including interviews with artists such as Tzara, Picabia, Schwitters, Arp, and Huelsenbeck, and musical repertoire including Satie, Ribemont-Dessaignes, Picabia, and Nelly van Doesburg.
Art techniques developed.
Collage.
The Dadaists imitated the techniques developed during the cubist movement through the pasting of cut pieces of paper items, but extended their art to encompass items such as transportation tickets, maps, plastic wrappers, etc. to portray aspects of life, rather than representing objects viewed as still life.
Photomontage.
The Dadaists – the "monteurs" (mechanics) – used scissors and glue rather than paintbrushes and paints to express their views of modern life through images presented by the media. A variation on the collage technique, photomontage utilized actual or reproductions of real photographs printed in the press. In Cologne, Max Ernst used images from the First World War to illustrate messages of the destruction of war.
Assemblage.
The assemblages were three-dimensional variations of the collage – the assembly of everyday objects to produce meaningful or meaningless (relative to the war) pieces of work including war objects and trash. Objects were nailed, screwed or fastened together in different fashions. Assemblages could be seen in the round or could be hung on a wall.
Readymades.
Marcel Duchamp began to view the manufactured objects of his collection as objects of art, which he called "readymades". He would add signatures and titles to some, converting them into artwork that he called "readymade aided" or "rectified readymades". Duchamp wrote: "One important characteristic was the short sentence which I occasionally inscribed on the 'readymade.' That sentence, instead of describing the object like a title, was meant to carry the mind of the spectator towards other regions more verbal. Sometimes I would add a graphic detail of presentation which in order to satisfy my craving for alliterations, would be called 'readymade aided.'" One such example of Duchamp's readymade works is the urinal that was turned onto its back, signed "R. Mutt", titled "Fountain", and submitted to the Society of Independent Artists exhibition that year. The piece was not displayed during the show, a fact that unmasked the inherently biased system that was the art establishment, seeing as any artist that paid the entry fee could in theory display their art, but the work of R. Mutt was banished by the judgment of a group of artists.
Bibliography.
</dl>

</doc>
<doc id="8242" url="http://en.wikipedia.org/wiki?curid=8242" title="Debian">
Debian

Debian () is a Unix-like computer operating system and a Linux distribution that is composed entirely of free and open-source software, most of which is under the GNU General Public License, and packaged by a group of individuals known as the Debian project. Currently, the Debian project offers three branches named "stable", "testing" and "unstable".
The Debian Stable distribution is one of the most popular for personal computers and network servers, and has been used as a base for several other Linux distributions.
Debian was first announced in 1993 by Ian Murdock, and the first stable release was made in 1996. The development is carried out over the Internet by a team of volunteers guided by a project leader and three foundational documents. New distributions are updated continually, and the next candidate is released after a time-based freeze.
As one of the earliest Linux distributions, it was envisioned that Debian was to be developed openly in the spirit of Linux and GNU. This vision drew the attention and support of the Free Software Foundation, which sponsored the project from November 1994 until November 1995. Upon the ending of FSF sponsorship, the Debian project formed the non-profit organisation Software in the Public Interest.
Features.
Debian has access to online repositories that contain over 43,000 software packages. Debian officially contains only free software, but non-free software can be downloaded from the Debian repositories and installed. Debian includes popular free programs such as LibreOffice, Iceweasel web browser, Evolution mail, K3b disc burner, VLC media player, GIMP image editor and Evince document viewer. Debian is a popular choice for web servers.
The cost of developing all of the packages included in Debian 5.0 Lenny (323 million lines of code) has been estimated to be about US$, using one method based on the COCOMO model. s of 2014[ [update]], Ohloh estimates that the codebase (78 million lines of code) would cost about US$ to develop, using a different method based on the same model.
The current stable release, Debian 8.0 code-named Jessie, is officially supported on ten architecture ports. Notable changes in this release include using systemd as the default init system.
Kernels.
Debian supports two kernels, Linux and kFreeBSD, and offers GNU Hurd unofficially. GNU/kFreeBSD is released as a technology preview for IA-32 and x86-64 architectures, and still lacks the amount of software available in Debian's Linux distribution. There are several flavors of the Linux kernel for each port; for instance, the i386 port has flavors for IA-32 PCs supporting Physical Address Extension and real-time computing, for older PCs, and for x86-64 PCs. The Linux kernel does not officially contain firmware without sources, although such firmware is available in non-free packages and alternative installation media.
Installation and Live images.
Debian offers DVD and CD images for installation that can be downloaded using BitTorrent or jigdo. Physical disks can also be bought from retailers. The full sets are made up of several discs (the amd64 port consists of 10 DVDs or 69 CDs), but only the first disc is required for installation, as the installer can retrieve software not contained in the first disc image from online repositories.
Debian offers different network installation methods. A minimal install of Debian is available via the "netinst" CD, whereby Debian is installed with just a base and later additional software can be downloaded from the Internet. Another option is to boot the installer from the network.
Installation images are hybrid on some architectures and can be used to create a bootable USB drive (Live USB).
The default desktop may be chosen from the DVD boot menu among GNOME, KDE Software Compilation, Xfce and LXDE, and from special disc 1 CDs.
Desktop environments.
Debian offers CD images specifically built for GNOME (the default in Wheezy), KDE Software Compilation, Xfce and LXDE. MATE is officially supported, while Cinnamon is expected to be in the next release. Less common window managers such as Enlightenment, Openbox, Fluxbox, IceWM, Window Maker and others are available.
The default desktop environment of version 7.0 Wheezy was temporarily switched to Xfce, because GNOME 3 did not fit on the first CD of the set. The default for the next version 8.0 Jessie was not clear either: it was changed again to Xfce in November 2013, and back to GNOME in September 2014.
Debian Live.
Debian releases live install images for CDs, DVDs and USB thumb drives, for IA-32 and x86-64 architectures, and with a choice of desktop environments. These "Debian Live" images allow the user to boot from removable media and run Debian without affecting the contents of their computer.
A full install of Debian to the computer's hard drive can be initiated from the live image environment.
Personalized images can be built with the live-build tool for discs, USB drives and for network booting purposes.
Package management.
Package management operations can be performed with different tools available on Debian, from the lowest level command dpkg to graphical front-ends like Synaptic. The recommended standard for administering packages on a Debian system is the apt toolset.
dpkg provides the low-level infrastructure for package management. The dpkg database contains the list of installed software on the current system. The dpkg command tool does not know about repositories. The command can work with local .deb package files as well as information from the dpkg database.
APT tools.
An APT tool allows administration of an installed Debian system for retrieving and resolving package dependencies from repositories. APT tools share dependency information and cached packages.
GDebi and other front-ends.
GDebi is an APT tool which can be used in command-line and on the GUI. GDebi can install a local .deb file via the command line like the dpkg command, but with access to repositories to resolve dependencies. Other graphical front-ends for APT include Software Center, Synaptic and Apper.
Branches.
Three branches of Debian (also called releases, distributions or suites) are regularly maintained:
Other branches in Debian:
The snapshot archive provides older versions of the branches. They may be used to install a specific older version of some software.
Numbering scheme.
"Stable" and "Oldstable" get minor updates, called point releases; as of 2015[ [update]], the "Stable" release is version 8.0, and the "Oldstable" release is version 7.8.
The numbering scheme for the point releases up to Debian 4.0 was to include the letter "r" (for "revision") after the main version number and then the number of the point release; for example, the latest point release of version 4.0 is 4.0r9. This scheme was chosen because a new dotted version would make the old one look obsolete and vendors would have trouble selling their CDs.
From Debian 5.0, the numbering scheme of point releases was changed, conforming to the GNU version numbering standard; the first point release of Debian 5.0 was 5.0.1 instead of 5.0r1. The numbering scheme was once again changed for the first Debian 7 update, which was version 7.1. The "r" scheme is no longer in use, but point release announcements include a note about not throwing away old CDs.
Code names.
The code names of Debian releases are names of characters from the "Toy Story" films. Debian 8 was named Jessie, after the cowgirl in "Toy Story 2" and "Toy Story 3". The "Testing" branch is currently named Stretch, after the toy rubber octopus in "Toy Story 3". The trunk is permanently nicknamed Sid, after the emotionally unstable boy next door who regularly destroyed toys.
This naming tradition came about because Bruce Perens was involved in the early development of Debian while working at Pixar.
Blends.
Debian Pure Blends are subsets of a Debian release configured out-of-the-box for users with particular skills and interests. For instance, Debian Jr. is targeted at children, while Debian Science aims at researchers and scientists. The complete Debian distribution includes all available Debian Pure Blends. "Debian Blend" (without "Pure") is a term for a Debian-based distribution that strives to become part of mainstream Debian, and have its extra features included in future releases.
Logo.
The Debian "swirl" logo was designed by Raul Silva in 1999 as part of a contest to replace the semi-official logo that had been used. The winner of the contest received an @debian.org email address, as well as a set of Debian 2.1 install CDs for the architecture of their choice. There has been no official statement from the Debian project on the logo's meaning, but at the time of the logo's selection, it was suggested that the logo represented the magic smoke that made computers work.
One theory about the origin of the Debian logo is based on an interesting detail: Buzz Lightyear, the chosen character for the first named Debian release, has a swirl in his chin. Stefano Zacchiroli also suggested that this swirl is the Debian one.
Archive areas.
The Debian Free Software Guidelines (DFSG) define the distinctive meaning of the word "free" as in "free and open-source software". Packages which comply with these guidelines, usually under the GNU General Public License, Modified BSD License or Artistic License, are included inside the main area; otherwise, they are included inside the non-free and contrib areas. These last two areas are not distributed within the official installation media, but they can be adopted manually.
Non-free includes packages which do not comply with the DFSG, such as documentation with invariant sections and proprietary software, and legally questionable packages. Contrib includes packages which do comply with the DFSG but fail other requirements; for instance, they may depend on packages which are in non-free or requires such for building them.
Richard Stallman and the Free Software Foundation have criticized the Debian project for hosting the non-free repository and because the contrib and non-free areas are easily accessible, an opinion echoed by some in Debian including the former project leader Wichert Akkerman. The internal dissent in the Debian project regarding the non-free section has persisted, but the last time it came to a vote in 2004, the majority decided to keep it.
Multimedia support.
Multimedia support has been problematic in Debian regarding codecs threatened by possible patent infringements, without sources or under too restrictive licenses, and regarding technologies such as Adobe Flash. Even though packages with problems related to their distribution could go into the non-free area, software such as libdvdcss is not hosted at Debian.
There is a notable third party repository, formerly known as debian-multimedia.org, providing software not present in Debian such as Windows codecs, libdvdcss and the Adobe Flash Player. Even though this repository is maintained by Christian Marillat, a Debian developer, it is not part of the project and is not hosted on a Debian server. The repository provides packages already included in Debian, interfering with the official maintenance. Eventually, project leader Stefano Zacchiroli asked Marillat to either settle an agreement about the packaging or to stop using the "Debian" name. Marillat chose the latter and renamed the repository to deb-multimedia.org. The repository was so popular that the switchover was announced by the official blog of the Debian project.
Hardware support.
Hardware requirements.
Hardware requirements are at least those of the kernel and the GNU toolsets. Debian GNU/Linux supports uniprocessor and symmetric multiprocessor systems. Debian's recommended system requirements depend on the level of installation, which corresponds to increased numbers of installed components:
The real minimum memory requirements depend on the architecture and may be much less than the numbers listed in this table. It is possible to install Debian with 60 MB of RAM for x86-64; the installer will run in low memory mode and it is recommended to create a swap partition. The installer for z/Architecture requires about 20 MB of RAM, but relies on network hardware. Similarly, disk space requirements, which depend on the packages to be installed, can be reduced by manually selecting the packages needed. s of 2014[ [update]], there is no Pure Blend that would lower the hardware requirements easily.
It is possible to run graphical user interfaces on older or low-end systems, but the installation of window managers instead of desktop environments is recommended, as desktop environments are more resource-intensive. Requirements for individual software vary widely and must be considered as well as those of the base operating environment.
Architecture ports.
Official ports.
As of the Jessie release[ [update]], the official ports are:
Unofficial ports.
Unofficial ports are available as part of the "Unstable" distribution:
Embedded systems.
Debian supports a variety of ARM-based NAS devices. The NSLU2 was supported by the installer in Debian 4.0 and 5.0, and Martin Michlmayr is providing installation tarballs since version 6.0. Other supported NAS devices are the Buffalo Kurobox Pro, GLAN Tank, Thecus N2100 and QNAP Turbo Stations.
Devices based on the Kirkwood SoC are supported too, such as the SheevaPlug plug computer and OpenRD products. There are efforts to run Debian on mobile devices, but this is not a project goal yet since the Debian Linux kernel maintainers would not apply the necessary patches. Nevertheless, there are packages for resource-limited systems.
There are efforts to support Debian on wireless access points. Debian is known to run on set-top boxes. There is an ongoing work to support the AM335x processor, which is used in electronic point of service solutions. Debian may be customized to run on cash machines.
BeagleBoard, a low-power open-source hardware single-board computer (Produced by Texas Instruments) has switched to Debian Linux pre-loaded on its Beaglebone Black board's flash.
Support for communities.
Localization.
Several parts of Debian are translated into languages other than English, including package descriptions, configuration messages, documentation and the website. The level of software localization depends on the language, ranging from the highly supported German and French to the hardly translated Creek and Samoan. The installer is available in 73 languages.
Virtual communities.
Debian provides packages targeted at virtual communities. The Facebook and Twitter application interfaces are available to programmers; the Pidgin messaging client used a custom plugin for Facebook until the networking site added support for XMPP. Debian 5.0 Lenny was the last release supporting Tencent QQ. Communication with Skype is possible using software in the contrib area.
Policies.
Debian is known for its manifesto, social contract, and policies. Debian's policies and team efforts focus on collaborative software development and testing processes. As a result of its policies, a new major release tends to occur every two years with revision releases that fix security issues and important problems.
Organization.
Simplified organizational structure
The Debian project is a volunteer organization with three foundational documents:
Debian developers are organized in a web of trust. There are at present[ [update]] about one thousand active Debian developers, but it is possible to contribute to the project without being an official developer.
The project maintains official mailing lists and conferences for communication and coordination between developers. For issues with single packages and other tasks, a public bug tracking system is used by developers and end users. Internet Relay Chat channels (primarily on the OFTC and freenode networks) are also used for communication among developers and to provide real time help.
Debian is supported by donations made to organizations authorized by the leader. The largest supporter is Software in the Public Interest, the owner of the Debian trademark, manager of the monetary donations and umbrella organization for various other community free software projects.
A Project Leader is elected once per year by the developers. The leader has special powers, but they are not absolute, and appoints delegates to perform specialized tasks. Delegates make decisions as they think is best, taking into account technical criteria and consensus. By way of a General Resolution, the developers may recall the leader, reverse a decision made by the leader or a delegate, amend foundational documents and make other binding decisions. The voting method is based on the Cloneproof Schwartz Sequential Dropping.
Project leaders
</div Scale><div id=ScaleBarleaders style="width:1px; float:left; height:36em; padding:0; background-color:#242020" />em;
 height:4.03826086957em;
 margin-left:0em;
 width:10em;
">em; 
">Ian Murdock
em;
 height:2.88em;
 margin-left:0em;
 width:10em;
">em; 
">Bruce Perens
em;
 height:1.69043478261em;
 margin-left:0em;
 width:10em;
">em; 
">Ian Jackson
em;
 height:3.39652173913em;
 margin-left:0em;
 width:10em;
">em; 
">Wichert Akkermanem;
 height:1.5652173913em;
 margin-left:0em;
 width:10em;
">em; 
">Ben Collinsem;
 height:1.42434782609em;
 margin-left:0em;
 width:10em;
">em; 
">Bdale Garbee
em;
 height:3.27130434783em;
 margin-left:0em;
 width:10em;
">em; 
">Martin Michlmayr
em;
 height:1.5652173913em;
 margin-left:0em;
 width:10em;
">em; 
">Branden Robinsonem;
 height:1.5652173913em;
 margin-left:0em;
 width:10em;
">em; 
">Anthony Townsem;
 height:1.5652173913em;
 margin-left:0em;
 width:10em;
">em; 
">Sam Hocevar
em;
 height:3.13043478261em;
 margin-left:0em;
 width:10em;
">em; 
">Steve McIntyreem;
 height:4.69565217391em;
 margin-left:0em;
 width:10em;
">em; 
">Stefano Zacchiroli
em;
 height:3.13043478261em;
 margin-left:0em;
 width:10em;
">em; 
">Lucas Nussbaumem;
 height:1.17391304348em;
 margin-left:0em;
 width:10em;
">em; 
">Neil McGovern</div Timeline>em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
"></div containsTSN></div Legend>The Project Leader is the public face of Debian and defines its direction.</div caption></div Container>
Project leadership is distributed occasionally. Branden Robinson was helped by the Project Scud, a team of developers that assisted the leader, but there were concerns that such leadership would split Debian into two developer classes. Anthony Towns created a supplemental position, Second In Charge (2IC), that shared some powers of the leader. Steve McIntyre was 2IC and had a 2IC himself.
One important role in Debian's leadership is that of a release manager. The release team sets goals for the next release, supervises the processes and decides when to release. The team is led by the next release managers and stable release managers. Release assistants were introduced in 2003.
Developer recruitment, motivation, and resignation.
The Debian project has an influx of applicants wishing to become developers. These applicants must undergo a vetting process which establishes their identity, motivation, understanding of the project's principles, and technical competence. This process has become much harder throughout the years.
Debian developers join the project for a number of reasons; some that have been cited include:
Debian developers may resign their positions at any time or, when deemed necessary, they can be expelled. Those who follow the retiring protocol are granted the "emeritus" status and they may regain their membership through a shortened new member process.
Development procedures.
Flowchart of the life cycle of a Debian package
Each software package has a maintainer that may be either one person or a team of Debian developers and non-developer maintainers. The maintainer keeps track of upstream releases, and ensures that the package coheres with the rest of the distribution and meets the standards of quality of Debian. Packages may include modifications introduced by Debian to achieve compliance with Debian Policy, even to fix non-Debian specific bugs, although coordination with upstream developers is advised.
The maintainer releases a new version by uploading the package to the "incoming" system, which verifies the integrity of the packages and their digital signatures. If the package is found to be valid, it is installed in the package archive into an area called the "pool" and distributed every day to hundreds of mirrors worldwide. The upload must be signed using OpenPGP-compatible software. All Debian developers have individual cryptographic key pairs. Developers are responsible for any package they upload even if the packaging was prepared by another contributor.
Initially, an accepted package is only available in the "Unstable" branch. For a package to become a candidate for the next release, it must migrate to the "Testing" branch by meeting the following:
Thus, a release-critical bug in a new version of a shared library on which many packages depend may prevent those packages from entering "Testing", because the updated library must meet the requirements too. From the branch viewpoint, the migration process happens twice per day, rendering "Testing" in perpetual beta.
Periodically, the release team publishes guidelines to the developers in order to ready the release. A new release occurs after a freeze, when all important software is reasonably up-to-date in the "Testing" branch and any other significant issues are solved. At that time, all packages in the "Testing" branch become the new "Stable" branch. Although freeze dates are time-based, release dates are not, which are announced by the release managers a couple of weeks beforehand.
A version of a package can belong to more than one branch, usually "Testing" and "Unstable". It is possible for a package to keep the same version between stable releases and be part of "Oldstable", "Stable", "Testing" and "Unstable" at the same time. Each branch can be seen as a collection of pointers into the package "pool" mentioned above.
Security.
The Debian project handles security through public disclosure rather than through obscurity. Debian security advisories are compatible with the Common Vulnerabilities and Exposures dictionary, are usually coordinated with other free software vendors and are published the same day a vulnerability is made public. There used to be a security audit project that focused on packages in the stable release looking for security bugs; Steve Kemp, who started the project, retired in 2011 but resumed his activities and applied to rejoin in 2014.
The "Stable" branch is supported by the Debian security team; "Oldstable" is supported for one year. Although Squeeze is not officially supported, Debian is coordinating an effort to provide long-term support until February 2016, five years after the initial release, but only for the IA-32 and x86-64 platforms. "Testing" is supported by the "Testing" security team, but does not receive updates in as timely a manner as "Stable". "Unstable"‍ '​s security is left for the package maintainers.
The Debian project offers documentation and tools to harden a Debian installation both manually and automatically. Security-Enhanced Linux support is available but disabled by default. Debian provides an optional hardening wrapper, and does not harden its software by default using gcc features such as PIE and buffer overflow protection, unlike operating systems such as OpenBSD, but tries to build as many packages as possible with hardening flags.
2008 OpenSSL vulnerability.
In May 2008, it was revealed that a Debian developer discovered that the OpenSSL package distributed with Debian and derivatives such as Ubuntu, made a variety of security keys vulnerable to a random number generator attack, since only 32,767 different keys were generated. The security weakness was caused by changes made in 2006 by another Debian developer in response to memory debugger warnings. The complete resolution procedure was cumbersome because patching the security hole was not enough; it involved regenerating all affected keys and certificates. Being introduced by Debian, the vulnerability caused anger and embarrassment among Debian developers.
Derivatives.
Debian GNU/Linux is one of the most popular Linux distributions, and many other distributions have been created from the Debian codebase, including Ubuntu and Knoppix. s of 2014[ [update]], DistroWatch lists 135 active Debian derivatives. The Debian project provides its derivatives with guidelines for best practices and encourages derivatives to merge their work back into Debian. A sign of cooperation with Ubuntu can be seen in the Debian package tracker.
History.
Release timeline.
Debian has made twelve major stable releases:
Legend:Older version, still supportedLatest versionLatest preview versionFuture release<div style="clear: left;" />
Birth (1993–1998).
Debian was first announced on August 16, 1993, by Ian Murdock, who initially called the system "the Debian Linux Release". The word "Debian" was formed as a combination of the first name of his then-girlfriend Debra Lynn and his own first name. Prior to Debian's release, the Softlanding Linux System (SLS) had been a popular Linux distribution and the basis for Slackware. The perceived poor maintenance and prevalence of bugs in SLS motivated Murdock to launch a new distribution.
Debian 0.01, released on September 15, 1993, was the first of several internal releases. Version 0.91 was virtually the first public release, providing support through mailing lists hosted at Pixar. The release included the Debian Linux Manifesto, outlining Murdock's view for the new operating system. In it he called for the creation of a distribution to be maintained openly, in the spirit of Linux and GNU.
The Debian project released the 0.9x versions in 1994 and 1995. During this time it was sponsored by the Free Software Foundation. Ian Murdock delegated the base system, the core packages of Debian, to Bruce Perens and Murdock focused on the management of the growing project. The first ports to non-IA-32 architectures began in 1995, and Debian 1.1 was released in 1996. By that time and thanks to Ian Jackson, the dpkg package manager was already an essential part of Debian.
In 1996, Bruce Perens assumed the project leadership. Perens was a controversial leader, regarded as authoritarian and strongly attached to Debian. He drafted a social contract and edited suggestions from a month-long discussion into the Debian Social Contract and the Debian Free Software Guidelines. After the FSF withdrew their sponsorship in the midst of the free software vs. open source debate, Perens initiated the creation of the legal umbrella organization Software in the Public Interest instead of seeking renewed involvement with the FSF. He led the conversion of the project from a.out to ELF. He created the BusyBox program to make it possible to run a Debian installer on a single floppy, and wrote a new installer. By the time Debian 1.2 was released, the project had grown to nearly two hundred volunteers. Perens left the project in 1998.
Ian Jackson became the leader in 1998. Debian 2.0 introduced the second official port, m68k. During this time the first port to a non-Linux kernel, Debian GNU/Hurd, was started. On December 2, the first Debian Constitution was ratified.
Leader election (1999–2005).
From 1999, the project leader was elected yearly. The Advanced Packaging Tool was deployed with Debian 2.1. The amount of applicants was overwhelming and the project established the new member process. The first Debian derivatives, namely Libranet, Corel Linux and Stormix's Storm Linux, were started in 1999. The 2.2 release in 2000 was dedicated to Joel Klecker, a developer who died of Duchenne muscular dystrophy.
In late 2000, the project reorganized the archive with new package "pools" and created the "Testing" distribution, made up of packages considered stable, to reduce the freeze for the next release. In the same year, developers began holding an annual conference called DebConf with talks and workshops for developers and technical users. In May 2001, Hewlett-Packard announced plans to base its Linux development on Debian.
In July 2002, the project released version 3.0, code-named Woody, the first release to include cryptographic software, a free licensed KDE and internationalization. During these last release cycles, the Debian project drew considerable criticism from the free software community because of the long time between stable releases.
Some events disturbed the project while working on Sarge, as Debian servers were attacked by fire and hackers. One of the most memorable was the Vancouver prospectus. After a meeting held in Vancouver, release manager Steve Langasek announced a plan to reduce the number of supported ports to four in order to shorten future release cycles. There was a large reaction because the proposal looked more like a decision and because such a drop would damage Debian's aim to be "the universal operating system".
Sarge release (2005–present).
The 3.1 Sarge release was made in June 2005. This release updated 73% of the software and included over 9,000 new packages. A new installer with a modular design, Debian-Installer, allowed installations with RAID, XFS and LVM support, improved hardware detection, made installations easier for novice users, and was translated into almost forty languages. An installation manual and release notes were in ten and fifteen languages respectively. The efforts of Skolelinux, Debian-Med and Debian-Accessibility raised the number of packages that were educational, had a medical affiliation, and ones made for people with disabilities.
In 2006, as a result of a much-publicized dispute, Mozilla software was rebranded in Debian, with Firefox becoming Iceweasel and Thunderbird becoming Icedove. The Mozilla Corporation stated that software with unapproved modifications could not be distributed under the Firefox trademark. Two reasons that Debian modifies the Firefox software are to change the non-free artwork and to provide security patches.
A fund-raising experiment, Dunc-Tank, was created to solve the release cycle problem and release managers were paid to work full-time; in response, unpaid developers slowed down their work and the release was delayed. Debian 4.0 (Etch) was released in April 2007, featuring the x86-64 port and a graphical installer. Debian 5.0 (Lenny) was released in February 2009, supporting Marvell's Orion platform and netbooks such as the Asus Eee PC. The release was dedicated to Thiemo Seufer, a developer who died in a car accident.
In July 2009, the policy of time-based development freezes on a two-year cycle was announced. Time-based freezes are intended to blend the predictability of time based releases with Debian's policy of feature based releases, and to reduce overall freeze time. The Squeeze cycle was going to be especially short; however, this initial schedule was abandoned. In September 2010, the backports service became official, providing more recent versions of some software for the stable release.
Debian 6.0 (Squeeze) was released in February 2011, introduced Debian GNU/kFreeBSD as a technology preview, featured a dependency-based boot system, and moved problematic firmware to the non-free area. Debian 7.0 (Wheezy) was released in May 2013, featuring multiarch support and Debian 8.0 (Jessie) was released in April 2015. At present[ [update]], Debian is still in development and new packages are uploaded to "Unstable" every day.
Throughout Debian's lifetime, both the Debian distribution and its website have won various awards from different organizations, including "Server Distribution of the Year" 2011, "The best Linux distro of 2011", and a "Best of the Net" award for October 1998.

</doc>
<doc id="8243" url="http://en.wikipedia.org/wiki?curid=8243" title="Doonesbury">
Doonesbury

Doonesbury is a comic strip by American cartoonist Garry Trudeau that chronicles the adventures and lives of an array of characters of various ages, professions, and backgrounds, from the President of the United States to the title character, Michael Doonesbury, who has progressed from a college student to a youthful senior citizen over the decades.
Created in "the throes of '60s and '70s counterculture," and frequently political in nature, "Doonesbury" features characters representing a range of affiliations, but the cartoon is noted for a liberal viewpoint. The name "Doonesbury" is a combination of the word "doone" (prep school slang for someone who is clueless, inattentive, or careless) and the surname of Charles Pillsbury, Trudeau's roommate at Yale University.
"Doonesbury" is written and pencilled by Garry Trudeau, then inked and lettered by his assistant Don Carlton.
History.
"Doonesbury" began as a continuation of "Bull Tales", which appeared in the Yale University student newspaper, the "Yale Daily News", beginning in September 1968. It focused on local campus events at Yale. The executive editor of the paper in the late 1960s, Reed Hundt, who later served as chairman of the FCC, noted that the "Daily News" had a flexible policy about publishing cartoons, stating that the paper published "pretty much anything."
"Doonesbury" proper debuted as a daily strip in about two dozen newspapers on October 26, 1970 (it being the first strip from Universal Press Syndicate). A Sunday strip began on March 21, 1971. Many of the early strips were reprints of the "Bull Tales" cartoons, with some changes to the drawings and plots. BD's helmet changed from having a "Y" (for Yale) to a star (for the fictional Walden College). Mike and BD started "Doonesbury" as roommates; they were not roommates in "Bull Tales".
"Doonesbury" became well known for its social and political commentary, always timely, and peppered with wry and ironic humor. It is currently syndicated in approximately 1,400 newspapers worldwide.
Like "Li'l Abner" and "Pogo" before it, "Doonesbury" blurred the distinction between editorial cartoon and the funny pages. In May 1975, the strip won Trudeau a Pulitzer Prize for Editorial Cartooning, the first strip cartoon to be so honored. That month, Holt, Rinehart, & Winston, the publishers of collections of "Doonesbury" until the mid-1980s, took out an ad in the "New York Times Book Review", marking the occasion by saying: It's nice for Trudeau and "Doonesbury" to be so honored, "but it's quite another thing when the Establishment clutches all of Walden Commune to its bosom." That same year, then-U.S. President Gerald Ford acknowledged the stature of the comic strip, telling the Radio and Television Correspondents' Association at their annual dinner, "There are only three major vehicles to keep us informed as to what is going on in Washington: the electronic media, the print media, and "Doonesbury", not necessarily in that order."
In 1977, Trudeau wrote a script for a 26-minute animated special. "A Doonesbury Special" was produced and directed by Trudeau, along with John Hubley (who died during the storyboarding stage) and Faith Hubley. The special was first broadcast by NBC on November 27, 1977. It won a Special Jury Award at the Cannes International Film Festival for best short film, and received an Academy Award nomination (for best animated short film), both in 1978. Voice actors for the special included Barbara Harris, William Sloane Coffin, Jr., Jack Gilford and Will Jordan. Also included were two songs "sung" by the character Jimmy Thudpucker (actually actor/singer/songwriter/producer James Allen "Jimmy" Brewer), entitled "Stop in the Middle" and "I Do Believe", also part of the "Special". While the compositions and performances were credited to "Jimmy Thudpucker", they were in fact co-written and sung by Brewer, who also co-wrote and provided the vocals for "Ginny's Song", a 1976 single on the Warner Bros. Label, and "Jimmy Thudpucker's Greatest Hits", an LP released by Windsong Records, John Denver's subsidiary of RCA Records).
1983–1984 hiatus.
Trudeau took a 22-month hiatus, from January 1983 to October 1984. Before the break in the strip, the characters were eternal college students, living in a commune together near Walden College, which was modeled after Trudeau's alma mater, Yale. During the break, Trudeau helped create a Broadway musical of the strip, showing the graduation of the main characters. The Broadway adaptation opened at the Biltmore Theatre on November 21, 1983, and played 104 performances. Elizabeth Swados composed the music for Trudeau's book and lyrics.
After the hiatus.
The strip resumed some time after the events in the musical, with further changes having taken place after the end of the musical's plot. While Mike, Mark, Zonker, B.D., and Boopsie were all now graduates, B.D. and Boopsie were living in Malibu, California, where B.D. was a third-string quarterback for the Los Angeles Rams, and Boopsie was making a living from walk-on and cameo roles. Mark was living in Washington, DC, working for National Public Radio. Michael and J.J. had gotten married, and Mike had dropped out of business school to start work in an advertising agency in New York City. Zonker, still not ready for the "real world", was living with Mike and J.J. until he was accepted as a medical student at his Uncle Duke's "Baby Doc College" in Haiti.
Prior to the hiatus, the strip's characters had aged at the tectonically slow rate standard for comic strips. But when Trudeau returned to "Doonesbury," the characters began to age in something close to real time, as in "Gasoline Alley" and "For Better or for Worse." Since then, the main characters' ages and career developments have tracked that of standard media portrayals of baby boomers, with jobs in advertising, law enforcement, and the dot-com boom. Current events are mirrored through the original characters, their offspring (the "second generation"), and occasional new characters.
Garry Trudeau received the National Cartoonist Society Newspaper Comic Strip Award for 1994, and their Reuben Award for 1995 for his work on the strip.
"Alpha House" and hiatuses: 2013–.
"Doonesbury"'s syndicate, Universal Uclick, announced on May 29, 2013, that the comic strip would go on hiatus from June 10 to Labor Day of that year while Garry Trudeau worked on his streaming video comedy "Alpha House", which was picked up by Amazon Studios. "Doonesbury Flashbacks" were offered during those weeks, but due to the unusually long hiatus, some newspapers opted to run different comic strips instead. Sunday strips returned as scheduled, but the daily strip's hiatus was extended until November 2013.
After "Alpha House" was retained for a second series in February 2014, Trudeau announced that he would now only produce Sunday strips for the foreseeable future. From March 3, 2014, the strip offers reruns starting from the very beginning of its history as opposed to the recent ones that re-run when Trudeau is on vacation.
Style.
With the exception of Walden College, Trudeau has frequently used real-life settings, based on real scenarios, but with fictional results. Due to deadlines, some real-world events have rendered some of Trudeau's comics unusable, such as a 1973 series featuring John Ehrlichman, a 1989 series set in Tiananmen Square in Beijing, China, a 1993 series involving Zoë Baird, and a 2005 series involving Harriet Miers. Trudeau has also displayed fluency in various forms of jargon, including those of real estate agents, flight attendants, computer scientists, journalists, presidential aides, and soldiers in Iraq.
Walden College.
The unnamed college attended by the main characters was later given the name "Walden College", revealed to be in Connecticut (the same state as Yale), and depicted as devolving into a third-rate institution under the weight of grade inflation, slipping academic standards, and the end of tenure, issues that Trudeau has consistently revisited since the original characters graduated. Some of the second generation of "Doonesbury" characters have attended Walden, a venue Trudeau uses to advance his concerns about academic standards in America.
President King, the leader of Walden College, was originally intended as a parody of Kingman Brewster, President of Yale, but all that remains of that is a certain physical resemblance.
Use of real-life politicians as characters.
Even though "Doonesbury" frequently features real-life U.S. politicians, they are rarely depicted with their real faces. Originally, strips featuring the President of the United States would show an external view of the White House, with dialogue emerging from inside. During the Gerald Ford administration, characters would be shown speaking to Ford at press conferences, and fictional dialogue supposedly spoken by Ford would be written as coming "off-panel". Similarly, while having several characters as students in a class taught by Henry Kissinger, the dialogue made up for Kissinger would also come from "off-panel" (although Kissinger had earlier appeared as a character with his face shown in a 1972 series of strips in which he met Mark Slackmeyer while the latter was on a trip to Washington). Sometimes hands, or in rare cases, the back of heads would also be seen.
Later, personal symbols reflecting some aspect of their character came into use. For example, during the 1980s, character Ron Headrest served as a doppelgänger for Ronald Reagan and was depicted as a computer-generated artificial-intelligence, an image based on the television character "Max Headroom". Members of the Bush family have been depicted as invisible. During his term as Vice President, George H. W. Bush was first depicted as completely invisible, his words emanating from a little "voice box" in the air. This was originally a reference to Bush's perceived low profile and his denials of knowledge of the Iran-Contra Affair. (In one strip, published March 20, 1988, the vice president almost materialized, but only made it to an outline before reverting to invisibility.)
George W. Bush was symbolized by a Stetson hat atop the same invisible point, because he was Governor of Texas prior to his presidency (Trudeau accused him of being "all hat and no cattle", reiterating the characterization of Bush by columnist Molly Ivins). The point became a giant asterisk (a la Roger Maris) following the 2000 presidential elections and the controversy over vote-counting. Later, President Bush's hat was changed to a Roman military helmet (again, atop an asterisk) representing imperialism. Towards the end of his first term, the helmet became battered, with the gilt work starting to come off and with clumps of bristles missing from the top. By late 2008, the helmet had been dented almost beyond recognition. No symbol for Barack Obama has appeared in the strip; the May 30, 2009, strip had Obama and an aide wondering what the reason for this might be (off panel).
Other symbols include a waffle for the indecisive Bill Clinton (chosen by popular vote—the other possibility had been a flipping coin), an unexploded (but sometimes lit) bomb for the hot-tempered Newt Gingrich, a feather for the lightweight Dan Quayle and a giant groping hand for Arnold Schwarzenegger (who is addressed by other characters as "Herr Gröpenfuhrer", a reference to accusations of sexual assault against Schwarzenegger). Many less well-known politicians have also been represented as icons over the years, like a swastika for David Duke, but only for the purposes of a gag strip or two. Trudeau has made his use of icons something of an in joke to readers, where the first appearance of a new one is often a punchline in itself.
The long career of the series and continual use of real-life political figures, analysts note, have led to some uncanny cases of the cartoon foreshadowing a national shift in the politicians’ political fortunes. Tina Gianoulis in "St. James Encyclopedia of Pop Culture" observes that "In 1971, well before the conservative Reagan years, a forward-looking BD called Ronald Reagan his 'hero.' In 1984, almost ten years before Congressman Newt Gingrich became Speaker of the House, another character worried that he would 'wake up someday in a country run by Newt Gingrich.'" In its 2003 series "John Kerry: A Candidate in the Making" on the 2004 presidential race, the "Boston Globe" reprinted and discussed 1971 "Doonesbury" cartoons of the young Kerry's Vietnam War protest speeches.
Characters.
"Doonesbury" has a large group of recurring characters, with 24 currently listed at the strip's website. There, it notes that "readers new to "Doonesbury" sometimes experience a temporary bout of character shock," as the sheer number of characters (and the historical connections among them) can be overwhelming.
The main characters are a group who attended the fictional Walden College during the strip's first 12 years, and moved into a commune together in April 1972. Most of the other characters first appeared as family members, friends, or other acquaintances. The original Walden Commune residents were Mike Doonesbury, Zonker Harris, Mark Slackmeyer, Nicole, Bernie, and DiDi. In September 1972, Joanie Caucus joined the comic, meeting Mike and Mark in Colorado and eventually moving into the commune. They were later joined by B.D. and his girlfriend (later wife) Boopsie, upon B.D.'s return from Vietnam. Nicole, DiDi, and Bernie were mostly phased out in subsequent years, and Zonker's Uncle Duke was introduced as the most prominent character outside the Walden group, and the main link to many secondary characters.
The Walden students graduated in 1983, after which the strip began to progress in something closer to real time. Their spouses and developing families became more important after this: Joanie's daughter J.J. Caucus married Mike and they had a daughter, Alex Doonesbury. They divorced, Mike remarried Kim Rosenthal, a Vietnamese refugee (who had appeared in the strip as a baby adopted by a Jewish family just after the fall of Saigon), and J.J. married Zeke Brenner, her former boyfriend and Uncle Duke's former groundskeeper. Joanie married Rick Redfern, and they had a son, Jeff. Uncle Duke and Roland Hedley have also appeared often, frequently in more topical settings unconnected to the main characters. In more recent years the second generation has taken prominence as they have grown to college age: Jeff Redfern, Alex Doonesbury, Zonker's nephew Zipper Harris, and Uncle Duke's son Earl.
Controversies.
"Doonesbury" has delved into a number of political and social issues, causing controversies and breaking new ground on the comics pages. Among the controversies:
Criticism.
Charles M. Schulz of Peanuts called Trudeau "unprofessional" for taking a long sabbatical. Nor was the return of the strip itself greeted with universal acclaim; in 1985, "Saturday Review" listed Trudeau as one of the country's "Most Overrated People in American Arts and Letters," commenting that the "most publicized return since MacArthur's has produced a strip that is predictable, mean-spirited, and not as funny as before."
Some conservatives have intensely criticized "Doonesbury". Several examples are cited in the Milestones section of the strip's website. The strip has also met criticism from its readers almost since it began syndicated publication. For example, when Lacey Davenport's husband Dick, in the last moments before his death, calls on God, several conservative pundits called the strip blasphemous. The sequence of Dick Davenport's final bird-watching and fatal heart attack was run in November 1986.
"Doonesbury" has angered, irritated, or been rebuked by many of the political figures that have appeared or been referred to in the strip over the years. A 1984 series of strips showing then Vice President George H.W. Bush placing his manhood in a blind trust—in parody of Bush's use of that financial instrument to fend off concerns that his governmental decisions would be influenced by his investment holdings—brought the politician to complain, ""Doonesbury"'s carrying water for the opposition. Trudeau is coming out of deep left field." There have also been other politicians who did not view the way that "Doonesbury" portrayed them very favorably, including Democrats such as former U.S. House Speaker Tip O'Neill and California Governor Jerry Brown.
The strip has also met controversy over every military conflict it has dealt with, including Vietnam, Grenada, Panama and both Gulf Wars. When "Doonesbury" ran the names of soldiers who had died in Iraq since the 2003 invasion, conservative commentators accused Trudeau of using the American dead to make a profit for himself, and again demanded that the strip be removed from newspapers.
After many letter-writing campaigns demanding the removal of the strip were unsuccessful, conservatives changed their tactics, and instead of writing to newspaper editors, they began writing to one of the printers who prints the color Sunday comics. In 2005, Continental Features gave in to their demands, and refused to continue printing the Sunday "Doonesbury", causing it to disappear from the 38 Sunday papers that Continental Features printed. Of the 38, only one newspaper "The Anniston Star" in Anniston, Alabama, continued to carry the Sunday "Doonesbury", though of necessity in black and white.
Some newspapers have dealt with the criticism by moving the strip from the comics page to the editorial page, because many people believe that a politically based comic strip like "Doonesbury" does not belong in a traditionally child-friendly comics section. The "Lincoln Journal" started the trend in 1973. In some papers (such as the "Tulsa World" and "Orlando Sentinel") "Doonesbury" appears on the opinions page alongside "Mallard Fillmore", a politically conservative comic strip.

</doc>
<doc id="8244" url="http://en.wikipedia.org/wiki?curid=8244" title="Dice">
Dice

Four colored traditional dice showing all six different sides
Dice (singular die or dice; from Old French "dé"; from Latin "datum" "something which is given or played"; plural dice or occasionally dices) are small throwable objects with multiple resting positions, used for generating random numbers. Dice are suitable as gambling devices for games like craps and are also used in non-gambling tabletop games.
A traditional die is a rounded cube, with each of its six faces showing a different number of dots (pips) from 1 to 6. When thrown or rolled, the die comes to rest showing on its upper surface a random integer from one to six, each value being equally likely. A variety of similar devices are also described as dice; such specialized dice may have polyhedral or irregular shapes and may have faces marked with symbols instead of numbers. They may be used to produce results other than one through six. Loaded and crooked dice are designed to favor some results over others for purposes of cheating or amusement.
A dice tray or a dice box is a piece of gaming equipment, a tray used to contain thrown dice, for gambling or board games. One traditional form used in Flemish dice games is an octagonal shaped wooden tray, lined with fabric. A dice tray can be used to play games on its own or as an add-on for other board games, in particular to allow dice throws which do not interfere with other game pieces.
History.
Dice have been used since before recorded history, and it is uncertain where they originated. The oldest known dice were excavated as part of a 5000-year-old backgammon set at the Burnt City, an archeological site in south-eastern Iran. Other excavations from ancient tombs in the Indus Valley civilization indicate a South Asian origin. Dicing is mentioned as an Indian game in the "Rigveda", "Atharvaveda" and Buddha games list. It also plays a critical role in the great Hindu epic "Mahabharata", where Yudhisthira plays a game of dice against the Kauravas for the northern kingdom of Hastinapura, which becomes the trigger for a war. There are several biblical references to "casting lots", as in Psalm 22, indicating that dicing (or a related activity) was commonplace when the psalm was composed. Knucklebones was a skill game played by women and children; a derivative form had the four sides of the bone receive different values and count as modern dice. Gambling with two or three dice was a very popular form of amusement in Greece, especially with the upper classes, and a frequent accompaniment to symposia.
Dice were originally made from the talus of hoofed animals, colloquially known as "knucklebones". These are approximately tetrahedral, leading to the nickname "bones" for dice. Modern Mongolians still use such bones as shagai for games and fortunetelling. Besides bone, materials like ivory, wood and plastics such as cellulose acetate have been used. Dice are hard to distinguish from knucklebones in literature because ancient writers confused the two, but both were used in prehistoric times.
The Romans were passionate gamblers, especially at the peak of the Roman Empire, and dicing was common though forbidden except during the Saturnalia. Horace derided youths who wasted time on dicing instead of horse-chasing. Throwing dice for money was the cause of many special laws in Rome, one of which stated that no lawsuit could be filed by a person who allowed gambling in his house, even if he had been cheated or assaulted. Professional gamblers were common, and some of their loaded dice are preserved in museums. The public houses were the resorts of gamblers, and depictions of quarreling dicers can be seen on frescos. Twenty-sided dice date back to the 2nd century AD and late BC.
Tacitus stated that the Germans were passionately fond of dicing, so much that they would stake their personal liberty when bankrupt. During the Middle Ages, dicing became a favorite pastime of knights, who formed dicing schools and guilds. After the downfall of feudalism, the landsknechts established a reputation as the most notorious dicing gamblers of their time. In France dice were used by both knights and ladies, despite repeated legislation against gambling with dice, including interdictions on the part of St. Louis in 1254 and 1256. The markings on Chinese dominoes evolved from the markings on dice.
Usage.
Dice are thrown onto a flat surface either from the hand or from a container designed for this (such as a dice cup). The face of the die that is uppermost when it comes to rest provides the value of the throw. One typical dice game today is craps, where two dice are thrown at a time and wagers are made on the total value of the two dice. Dice are frequently used to randomize moves in board games, usually by deciding the distance through which a piece will move along the board; examples of this are backgammon and "Monopoly".
The result of a die roll is determined by the way it is thrown, according to the laws of classical mechanics. A die roll is made random by uncertainty in minor factors such as tiny movements in the thrower's hand; They are thus a crude form of hardware random number generator. Perhaps to militate against concerns that the pips on the faces of certain styles of dice cause a small bias, casinos use precision dice with flush markings.
Construction.
Arrangement.
Common dice are small cubes most commonly 1.6 cm across, whose faces are numbered from one to six, usually by patterns of round dots called pips. (While the use of Hindu-Arabic numerals is occasionally seen, such dice are less common.) Opposite sides of a die traditionally add up to seven, implying that the 1, 2 and 3 faces share a vertex; these faces may be placed clockwise or counterclockwise about this vertex. If the 1, 2 and 3 faces run counterclockwise, the die is called "right-handed", and if those faces run clockwise, the die is called "left-handed". Western dice are normally right-handed, and Chinese dice are normally left-handed.
The pips on dice are arranged in specific patterns as shown. Asian style dice bear similar patterns to Western ones, but the pips are closer to the centre of the face; in addition, the pips are differently sized on Asian style dice, and the pips are colored red on the 1 and 4 sides. One possible explanation is that red fours are of Indian origin. In some older sets, the "one" pip is a colorless depression.
Manufacturing.
Non-precision dice are manufactured via the plastic injection molding process. The pips or numbers on the dice are a part of the mold. The coloring for numbering is achieved by submerging the dice entirely in paint, which is allowed to dry, and then polished via a tumble finishing process similar to rock polishing. The abrasive agent scrapes off all of the paint except for the indents of the numbering. A finer abrasive is then used to polish the die. This process also creates the smoother, rounded edges on the dice.
Precision casino dice may have a polished or sand finish, making them transparent or translucent respectively. Casino dice have their pips drilled, then filled flush with a paint of the same density as the material used for the dice, such that the center of gravity of the dice is as close to the geometric center as possible. All such dice are stamped with a serial number to prevent potential cheaters from substituting a die. Precision backgammon dice are made the same way; they tend to be slightly smaller and have rounded corners and edges, to allow better movement inside the dice cup and stop forceful rolls from damaging the playing surface.
Terms.
While the terms "ace", "deuce", "trey", "cater", "cinque" and "sice" have been made obsolete by one to six, they are still used by some professional gamblers to designate different sides of the dice. "Ace" is from the Latin "as", meaning "a unit"; the others are 2 to 6 in old French.
Notation.
Using Unicode characters, the faces ⚀ ⚁ ⚂ ⚃ ⚄ ⚅, can be shown in text using the range U+2680 to U+2685 or using decimal codice_1 to codice_2.
In many gaming contexts, especially tabletop role-playing games, it is common to see shorthand notations representing different dice rolls. A "d" or "D" is used to indicate a die with a specific number of sides, codice_3 indicating a four-sided die, for example. If several dice of the same type are to be rolled, this is indicated by a leading number specifying the number of dice. Hence, codice_4 means the player should roll six eight-sided dice. Modifiers to a die roll can also be indicated as desired. For example, codice_5 instructs the player to roll three six-sided dice, calculate the total, and add four to it.
Loaded dice.
A loaded, weighted or crooked die is one that has been tampered with so that it will land with a specific side facing upwards more or less often than a fair die would. There are several methods for creating loaded dice, including round faces, off-square faces and weights. "Tappers" have a mercury drop in a reservoir at the center, with a capillary tube leading to another reservoir at a side; the load is activated by tapping the die so that the mercury travels to the side.
Another type of loaded die is hollow with a small weight and a semi-solid substance inside whose melting point is just lower than the temperature of the human body, allowing the cheater to change the loading of the die by applying body heat, causing the semi-solid to melt and the weight to drift down, making the chosen opposite face more likely to land up. A less common type of loaded die can be made by inserting a magnet into the die and embedding a coil of wire in the game table; running current through the coil increases the likelihood of a certain side landing on the bottom, depending on the direction of the current. Transparent acetate dice, used in all reputable casinos, are harder to tamper with than other dice.
A die may be shaved on one side, making it slightly shorter in one dimension, thus affecting its outcome. One countermeasure employed by casinos against shaved dice is to measure the dice with a micrometer before playing.
Variants.
Non-numeric.
The faces of most dice are labelled using sequences of whole numbers, usually starting at one, expressed with either pips or digits. However, there are some applications that require results other than numbers. Examples include letters for Boggle, directions for "Warhammer Fantasy Battle", playing card symbols for poker dice, and instructions for sexual acts using sex dice.
Non-cubic.
Seven- and eight-sided dice are described in the 13th century Libro de los juegos as having been invented by Alfonso X in order to speed up play in chess variants. Around the end of the 1960s, non-cubical dice became popular among players of wargames, and since have been employed extensively in role-playing games and trading card games. Reciprocally symmetric numerals like 6 and 9 are distinguished with a dot or underline.
The other four Platonic solids are the most common non-cubical dice; these can have 4, 8, 12, and 20 faces. The only other common non-cubical die is the 10-sided die. The 4-sided platonic solid is difficult to roll, and a few games like Chaupur and Daldøs use 4-sided long dice instead. Using these dice in various ways, games can closely approximate the real probability distributions of the events they simulate. For instance, 10-sided dice can be rolled in pairs to produce a uniform distribution of random percentages; and summing the values of multiple dice will produce approximations to normal distributions.
Unlike other common dice, a tetrahedral die does not have a side that faces upward when it is at rest on a surface, so it has to be read in a different way. Many such dice have the numbers printed around the points, so that when it settles, the numbers at the vertex pointing up are the same and the one counted. Less commonly, the numbers on a tetrahedral die can be placed at the middle of the edges, in which case the numbers around the base are read.
A die can be constructed in the shape of a sphere, with the addition of an internal cavity in the shape of the dual polyhedron of the desired die shape and an internal weight. The weight will settle in one of the points of the internal cavity, causing it to settle with one of the numbers uppermost. For instance, a sphere with an octahedral cavity and a small internal weight will settle with one of the 6 points of the cavity held downwards by the weight.
Standard variations.
Dice are often sold in sets, matching in color, of five or six different shapes. They are also sold frequently with a second 10-sided die of a complementary or contrasting color. Sometimes, dice are sold additionally with a die resembling the five Platonic solids, whose faces are regular polygons, or the pentagonal trapezohedron die, whose faces are ten kites, each with two different edge lengths, three different angles, and two different kinds of vertices.
Normally, the faces on a die will be numbered sequentially beginning with 1, and opposite faces will thus add up to one more than the number of faces (but in the case of a dice with 4 sides and dice with an odd-number of faces, this is simply not possible). Some dice, such as a dice with 10 sides, are usually numbered sequentially beginning with 0, in which case the opposite faces will add to one less than the number of faces.
Rarer variations.
"Uniform fair dice" are dice where equal probability of the faces follow from the symmetry of the die (as it is face-transitive), and include:
Dice with an odd number of flat faces can be made as "rolling-pin style dice". They are based on an infinite set of prisms. All the (rectangular) faces they may actually land on are congruent, so they are equally fair. (The other 2 sides of the prism are rounded or capped with a pyramid, designed so that the die never actually rests on those faces.)
Application in role-playing games.
The fantasy role-playing game "Dungeons & Dragons" (D&D) is largely credited with popularizing dice in such games. Some games use only one type, like "Exalted" which uses only ten-sided dice. Others use numerous types for different game purposes, such as D&D, which makes use of all common polyhedral dice.
Dice are used to determine the outcome of events; such usage is called a "check". Games typically determine results either as a total on one or more dice above or below a fixed number, or a certain number of rolls above a certain number on one or more dice. Due to circumstances or character skill, the initial roll may have a number added to or subtracted from the final result, or have the player roll extra or fewer dice. To keep track of rolls easily, dice notation is frequently used.
A common special case is percentile rolls, referred to as codice_6 or codice_7. Since actual hundred-sided dice are large, almost spherical, and difficult to read, percentile rolls are instead handled by rolling two ten-sided dice together, using one as the "tens" and the other as the "units". A roll of ten or zero on either die is taken as a zero, unless both are zeros or tens, in which case this is 100. Some sets of percentile dice explicitly mark one die in tens and the other in units to avoid ambiguity.
Dice for role-playing games are usually plastic; early polyhedral dice from the 1970s and 1980s were made of a soft plastic that would easily wear with use, which would gradually render them unusable. Many early dice were unmarked, and players took great care in painting them. Some twenty-sided dice then were numbered zero through nine twice; half of the numbers had to be painted a contrasting color to differentiate faces. These could double as a ten-sided die by ignoring the distinguishing coloring.
Application in divination.
Dice can be used for divination and using dice for such a purpose is called cleromancy. A pair of common dice is usual, though other forms of polyhedra can be used. Tibetan Buddhists sometimes use this method of divination. It is highly likely that the Pythagoreans used the Platonic solids as dice. They referred to such dice as "the dice of the gods" and they sought to understand the universe through an understanding of geometry in polyhedra.
Astrological dice are a specialized set of three 12-sided dice for divination; the first die represents planets, the Sun, the Moon, and the nodes of the Moon, the second die represents the 12 zodiac signs, and the third represents the 12 houses. An icosahedron provides the answers of the Magic 8-Ball, conventionally used to provide answers to yes-or-no questions.

</doc>
<doc id="8246" url="http://en.wikipedia.org/wiki?curid=8246" title="Garbage picking">
Garbage picking

Garbage picking is the practice of sifting through commercial or residential waste to find items that have been discarded by their owners, but that may prove useful to the garbage picker. Garbage picking may take place in dumpsters or in landfills. When in dumpsters, the practice is called dumpster diving in American English and skipping in British English. Dumpster diving is viewed as an effective urban foraging technique. Dumpster divers forage dumpsters for items such as clothing, furniture, food, and similar items in good working condition. Some people dumpster dive out of necessity due to poverty, while others are high end professional divers that can earn 100s of thousands of US dollars per year.
Etymology.
The dumpster-diving term originates from the best-known manufacturer of commercial trash bins, Dempster, which uses the trade name "Dumpster" for its bins, and the image of someone leaping head first into a Dumpster as if it were a swimming pool. In practice, the size and design of most Dumpsters makes it possible to retrieve many items from the outside of Dumpsters without having to "dive" into them. Alternative names for the practice include bin-diving, containering, D-mart, dumpstering, tatting, skipping, or recycled food. In Australia, garbage picking is called "skip dipping."
Performers.
The term "binner" is often used to describe individuals who collect recyclable materials for their deposit value. For example, in Vancouver, British Columbia, binners, or bottle collectors, search garbage cans and dumpsters for recyclable materials that can be redeemed for their deposit value. On average, these binners earn about $40 a day for several garbage bags full of discarded containers.
The karung guni, Zabbaleen, the rag and bone man, waste picker, junk man or bin hoker are terms for people who make their living by sorting and trading trash. A similar process known as gleaning was practiced in rural areas and some ancient agricultural societies, where the residue from farmers' fields was collected.
Some dumpster divers, who self-identify as freegans, aim to reduce their ecological footprint by living from dumpster-dived-goods, sometimes exclusively.
Overview.
The organization Same Day Dumpsters has written, "Traditionally, most people who resorted to dumpster-diving were forced to do so out of economic necessity, but this is not the case today." However, dumpster diving still occurs by some out of necessity due to problems with poverty. Some dumpster divers perform in organized groups, and some organize on various internet forums and social networking websites. By reusing, or repurposing, resources destined for the landfill, dumpster diving may be environmentalist endeavor (and is thus practiced by many pro-green communities). The wastefulness of consumer society and throw-away culture compels some individuals to rescue usable items (for example, computers) from destruction and divert them to those who can make use of the items.
A wide variety of things may be disposed while still repairable or in working condition, making salvage of them a source of potentially free items for personal use, or to sell for profit. Irregular, blemished or damaged items that are still otherwise functional are regularly thrown away. Discarded food that might have slight imperfections, near its expiration date, or that is simply being replaced by newer stock is often tossed out despite being still edible. Many retailers are reluctant to sell this stock at reduced prices because of the risks that people will buy it instead of the higher-priced newer stock, that extra handling time is required, and that there are liability risks. In the United Kingdom, cookery books have been written on the cooking and consumption of such foods, which has contributed to the popularity of skipping. Artists often use discarded materials retrieved from trash receptacles to create works of found art or assemblage.
Students have been known to partake in dumpster diving to obtain high tech items for technical projects, or simply to indulge their curiosity for unusual items. Dumpster diving can additionally be used in support of academic research. It serves as the main tool for garbologists, who study the sociology and archeology of trash in modern life. Private and government investigators may dumpster dive to obtain information for their inquiries.
Dumpster diving can be hazardous, due to potential exposure to biohazardous matter, broken glass, and overall unsanitary conditions that may exist in dumpsters.
Arguments against dumpster diving often focus on the health and cleanliness implications of people rummaging in trash. This exposes the dumpster divers to potential health risks, and, especially if the dumpster diver does not return the non-usable items to their previous location, may leave trash scattered around. Divers can also be seriously injured or killed by garbage collection vehicles; in January 2012, in La Jolla, Swiss-American gentleman Alfonso de Bourbon was killed by a truck while dumpster diving. Further, there are also concerns around the legality of taking items that may still technically belong to the person who threw them away (or to the waste management operator), and whether the taking of some items like discarded documents is a violation of privacy.
Discarded billing records may be used for identity theft. As a privacy violation, discarded medical records as trash led to a $140,000 penalty against Massachusetts billing company Goldthwait Associates and a group of pathology offices in 2013 and a $400,000 settlement between Midwest Women’s Healthcare Specialists and 1,532 clients in Kansas City in 2014.
Legal status.
Since dumpsters are usually located on private premises, divers may occasionally get in trouble for trespassing while dumpster diving, though the law is enforced with varying degrees of rigor. Some businesses may lock dumpsters to prevent pickers from congregating on their property, vandalism to their property, and to limit potential liability if a dumpster diver is injured while on their property. Dumpster diving is often not prohibited by law. Abandonment of property is another principle of law that applies to recovering materials via dumpster diving.
Police searches of dumpsters as well as similar methods are also generally not considered violations; evidence seized in this way has been permitted in many criminal trials. The doctrine is not as well established in regards to civil litigation.
Companies run by private investigators specializing in dumpster diving have emerged as a result of the need for discreet, undetected retrieval of documents and evidence for civil and criminal trials. Private investigators have also written books on "P.I. technique" in which dumpster diving or its equivalent "wastebasket recovery" figures prominently.
By country.
In 2009, a Belgian dumpster diver and eco-activist nicknamed Ollie was detained for a month for dumpster diving, and was accused of theft and burglary. On February 25, 2009, Ollie was arrested for taking food from a dumpster at an AD Delhaize supermarket in Bruges. His trial evoked protests in Belgium against restrictions from taking discarded food items.
In Ontario, Canada, the Trespass to Property Act - legislation dating back to the British North America Act of 1867 - grants property owners and security guards the power to ban anyone from their premises, for any reason, permanently. This is done by issuing a notice to the intruder, who will only be breaking the law upon return. Similar laws exist in Prince Edward Island and Saskatchewan. A recent case in Canada, which involved a police officer who retrieved a discarded weapon from a trash receptacle as evidence, created some controversy. The judge ruled the policeman's actions as legal although there was no warrant present, which led some to speculate the event as validation for any Canadian citizen to raid garbage disposals.
Dumpster diving in England and Wales may qualify as theft within the Theft Act 1968 or as common-law theft in Scotland, though there is very little enforcement in practice.
In Germany, dumpster diving has been referred to as "containern", and a dumpster's contents are regarded as the property of the dumpster's owner. Therefore, taking items from a dumpster is viewed as theft. Be that as it may, the police will routinely disregard the illegality of dumpster diving seeing as the items found are generally of low value. There has only been one known instance where divers were to be prosecuted: the individuals were arrested on assumed burglary as they had surmounted a supermarket's fence which was then followed by a theft complaint by the owner.
In Italy, a law issued in 2000 declared dumpster diving to be illegal. It is also illegal in Sweden.
In the United States, the 1988 "California v. Greenwood" case in the U.S. Supreme Court held that there is no common law expectation of privacy for discarded materials. There are, however, limits to what can legally be taken from a company's refuse. In a 1983 Minnesota case involving the theft of customer lists from a garbage can, "Tennant Company v. Advance Machine Company" (355 N.W.2d 720), the owner of the discarded information was awarded $500,000 in damages.
Items.
Dumpster diving is practiced differently in developed countries than in developing countries. 
Notable instances.
In the 1960s, Jerry Schneider, using recovered instruction manuals from The Pacific Telephone & Telegraph Company, used the company's own procedures to acquire hundreds of thousands of dollars' worth of telephone equipment over several years until his arrest.
The "Castle Infinity" game, after its shutdown, was brought back from the dead by rescuing its servers from the trash.
Food Not Bombs is an anti-hunger organization that gets a significant amount of its food from dumpster diving from the dumpsters at small markets and corporate grocery stores in the US and UK.
In 2009, pro-surfer Dane Reynolds salvaged a piece of polyester foam from a dumpster behind the Channel Islands Surfboard factory. He shaped the foam into a surfboard that, at the time, was thought to be "short, fat, and ugly." The goal of this new shape was to distribute volume to the width and thickness of the board, cutting down on the overall board length needed to use in smaller surf, while staying progressive on the face of the wave. The board was a hit and was dubbed the "dumpster diver". The board changed the way surfboard shapers designed boards for use in smaller waves.
In October 2013, in North London, three men were arrested and charged under the 1824 Vagrancy Act when they were caught taking discarded food: tomatoes, mushrooms, cheese and cakes from bins behind an Iceland supermarket. The charges were dropped on 29 January 2014 after much public criticism as well as a request by Iceland's chief executive, Malcolm Walker.

</doc>
<doc id="8247" url="http://en.wikipedia.org/wiki?curid=8247" title="Digital synthesizer">
Digital synthesizer

A digital synthesizer is a synthesizer that uses digital signal processing (DSP) techniques to make musical sounds. Older, electronic keyboards make music through analog circuitry.
History.
The very earliest digital synthesis experiments were made with general-purpose computers, as part of academic research into sound generation. In 1975, the Japanese company Yamaha licensed the algorithms for frequency modulation synthesis (FM synthesis) from John Chowning, who had experimented with it at Stanford University since 1971. Yamaha's engineers began adapting Chowning's algorithm for use in a commercial digital synthesizer, adding improvements such as the "key scaling" method to avoid the introduction of distortion that normally occurred in analog systems during frequency modulation, though it would take several years before Yamaha release their FM digital synthesizers.
Early commercial digital synthesizers used simple hard-wired digital circuitry to implement techniques such as additive synthesis and FM synthesis, becoming commercially available in the late 1970s. Other techniques, such as wavetable synthesis and physical modeling, only became possible with the advent of high-speed microprocessor and digital signal processing technology. Two of the earliest commercial digital synthesizers were the Fairlight CMI, introduced in 1979, and the New England Digital Synclavier II. The Fairlight CMI was the first sampling synthesizer, while the Synclavier was originally an FM synthesizer, not adding sampling synthesis until the 1980s. The Fairlight CMI and the Synclavier were both expensive systems, retailing for more than $20,000, in the early 1980s.
In 1980, Yamaha eventually released the first FM digital synthesizer, the Yamaha GS-1, but at an expensive retail price of $16,000. The cost of digital synthesizers soon began falling rapidly in the early 1980s. E-mu Systems introduced the Emulator sampling synthesizer in 1982 at a retail price of $7,900. Although not as flexible or powerful as either the Fairlight CMI or the Synclavier, its lower cost and portability made it popular.
Introduced in 1983, the Yamaha DX7 was the breakthrough digital synthesizer to have a major impact, both innovative and affordable, and thus spelling the decline of analog synthesizers. It used FM synthesis and, although it was incapable of the sampling synthesis of the Fairlight CMI, its price was around $2,000, putting it within range of a much larger number of musicians. The DX-7 was also known for its "key scaling" method to avoid distortion and for its recognizable bright tonalities that was partly due to an overachieving sampling rate of 57 kHz. It became indispensable to many music artists of the 1980s, and would become one of the best-selling synthesizers of all time.
In 1987, Roland released its own influential synthesizer of the time : the D-50. This popular synth broke new ground in affordably combining short samples and digital oscillators, as well as the innovation of built-in digital effects (reverb., chorus, equalizer). Roland called this Linear Arithmetic (LA) synthesis. This instrument is responsible for some of the very recognisable preset synthesizer sounds of the late 1980s, such as the Pizzagogo sound used on Enya's "Orinoco Flow."
It gradually became feasible to include high quality samples of existing instruments as opposed to synthesizing them. In 1988, Korg introduced the last of the hugely popular trio of digital synthesizers of the 1980s after the DX7 and D50, the M1. This heralded both the increasing popularisation of digital sample-based synthesis, and the rise of 'workstation' synthesizers. After this time, many popular modern digital synthesizers have been described as not being full synthesizers in the most precise sense, as they play back samples stored in their memory. However, they still include options to shape the sounds through use of envelopes, LFOs, filters and effects such as reverb. The Yamaha Motif and Roland Fantom series of keyboards are typical examples of this type, described as 'ROMplers' ; at the same time, they are also examples of "workstation" synthesizers.
With the addition of sophisticated sequencers on board, now added to built-in effects and other features, the 'workstation' synthesizer had been born. These always include a multi-track sequencer, and can often record and playback samples, and in later years full audio tracks, to be used to record an entire song. These are usually also ROMplers, playing back samples, to give a wide variety of realistic instrument and other sounds such as drums, string instruments and wind instruments to sequence and compose songs, along with popular keyboard instrument sounds such as electric pianos and organs.
As there was still interest in analog synthesizers, and with the increase of computing power, over the 1990s another type of synthesizer arose : the analog modeling, or "virtual analog" synthesizer. These use computing power to simulate traditional analog waveforms and circuitry such as envelopes and filters, with the most popular examples of this type of instrument including the Nord Lead and Access Virus.
As the cost of processing power and memory fell, new types of synthesizers emerged, offering a variety of novel sound synthesis options. The Korg Oasys was one such example, packaging multiple digital synthesizers into a single unit.
Digital synthesizers can now be completely emulated in software ("softsynth"), and run on conventional PC hardware. Such soft implementations require careful programming and a fast CPU to get the same latency response as their dedicated equivalents. To reduce latency, some professional sound card manufacturers have developed specialized Digital Signal Processing ([DSP]) hardware. Dedicated digital synthesizers have the advantage of a performance-friendly user interface (physical controls like buttons for selecting features and enabling functionality, and knobs for setting variable parameters). On the other hand, software synthesizers 
have the advantages afforded by a rich graphical display.
With focus on performance-oriented keyboards and digital computer technology, manufacturers of commercial electronic instruments created some of the earliest digital synthesizers for studio and experimental use with computers being able to handle built-in sound synthesis algorithms.
Analog vs. digital.
The main difference is that a digital synthesizer uses digital processors and analog synthesizers use analog circuitry. A digital synthesizer is in essence a computer with (often) a piano-keyboard and a LCD as an interface. An analog synthesizer is made up of sound-generating circuitry and modulators. Because computer technology is rapidly advancing, it is often possible to offer more features in a digital synthesizer than in an analog synthesizer at a given price. However, both technologies have their own merit. Some forms of synthesis, such as, for instance, sampling and additive synthesis are not feasible in analog synthesizers, while on the other hand, many musicians prefer the character of analog synthesizers over their digital equivalent.
Bands using digital synths.
The New wave era of the 1980s first brought the digital synthesizer to the public ear. Bands like Talking Heads and Duran Duran used the digitally made sounds on some of their most popular albums. Other more pop-inspired bands like Hall & Oates began incorporating the digital synthesizer into their sound in the 1980s. Through breakthroughs in technology in the 1990s many modern synthesizers use DSP.
Digital synthesis.
Working in more or less the same way, every digital synthesizer appears similar to a computer. At a steady sample rate, digital synthesis produces a stream of numbers. Sound from speakers is then produced by a conversion to analog form. Through signal generation, voice and instrument-level processing, a signal flow is created and controlled either by MIDI capabilities or voice and instrument-level controls.

</doc>
<doc id="8249" url="http://en.wikipedia.org/wiki?curid=8249" title="Definition of music">
Definition of music

An accurate and concise definition of music is fundamental to being able to discuss, categorize, and otherwise consider the phenomenon of what we understand as being music. Many have been suggested, but defining music turns out to be more difficult than might first be imagined. As this article will demonstrate, there is ongoing controversy about how to define music.
The "Oxford Universal Dictionary" defines music as, "That one of the fine arts which is concerned with the combination of sounds with a view to beauty of form and the expression of thought or feeling" . However, the music genre known as noise music, for instance, challenges these ideas about what constitutes music's essential attributes by using non-traditional elements of music . (See also musique concrète.)
A famous example of the dilemma in defining music is John Cage’s composition titled "4'33"". The written score has three movements and directs the performer(s) to appear on stage, indicate by gesture or other means when the piece begins, then make no sound and only mark sections and the end by gesture. This has form and other important attributes of music, but no sound other than whatever ambient sounds may be heard in the room. Some argue this is not music because, for example, it contains no sounds that are conventionally considered "musical" and the composer and performer(s) exert no control over the organization of the sounds heard . Others argue it is music because the conventional definitions of musical sounds are unnecessarily and arbitrarily limited, and control over the organization of the sounds is achieved by the composer and performer(s) through their division of what is heard into specific sections .
Problems of defining music also arise from differences in the conception of music in different cultures.
Concepts of music.
Because of differing fundamental concepts of music, the languages of many cultures do not contain a word that can be accurately translated as "music," as that word is generally understood by Western cultures . Inuit and most North American Indian languages do not have a general term for music. Among the Aztecs, the ancient Mexican theory of rhetorics, poetry, dance, and instrumental music used the Nahuatl term "In xochitl-in kwikatl" to refer to a complex mix of music and other poetic verbal and non-verbal elements, and reserve the word "Kwikakayotl" (or cuicacayotl) only for the sung expressions . In Africa there is no term for music in Tiv, Yoruba, Igbo, Efik, Birom, Hausa, Idoma, Eggon or Jarawa. Many other languages have terms which only partly cover what Western culture typically means by the term "music" (, ). The Mapuche of Argentina do not have a word for "music", but they do have words for instrumental versus improvised forms ("kantun"), European and non-Mapuche music ("kantun winka"), ceremonial songs ("öl"), and "tayil" .
Some languages in West Africa have no term for music but the speakers do have the concept (,). "Musiqi" is the Persian word for the science and art of music, "muzik" being the sound and performance of music (, ), though some things European-influenced listeners would include, such as Quran chanting, are excluded.
Definitions.
Organized sound.
An often-cited definition of music is that it is "organized sound", a term originally coined by modernist composer Edgard Varèse in reference to his own musical aesthetic. Varèse's concept of music as "organized sound" fits into his vision of "sound as living matter" and of "musical space as open rather than bounded" . He conceived the elements of his music in terms of "sound-masses", likening their organization to the natural phenomenon of crystalization . Varèse thought that "to stubbornly conditioned ears, anything new in music has always been called noise", and he posed the question, "what is music but organized noises?" .
The fifteenth edition of the "Encyclopædia Britannica" states that "while there are no sounds that can be described as inherently unmusical, musicians in each culture have tended to restrict the range of sounds they will admit." A human organizing element is often felt to be implicit in music (sounds produced by non-human agents, such as waterfalls or birds, are often described as "musical", but perhaps less often as "music"). The composer R. Murray states that the sound of classical music "has decays; it is granular; it has attacks; it fluctuates, swollen with impurities—and all this creates a musicality that comes before any 'cultural' musicality." However, in the view of semiologist Jean-Jacques Nattiez, "just as music is whatever people choose to recognize as such, noise is whatever is recognized as disturbing, unpleasant, or both" . (See "music as social construct" below.)""'
Language.
Levi R. Bryant defines music not as a language, but as a marked-based, problem-solving method such as mathematics .
Musical universals.
Often a definition of music lists the aspects or elements that make up music under that definition. However, in addition to a lack of consensus, Jean also points out that "any element belonging to the total musical fact can be isolated, or taken as a strategic variable of musical production."
Following Wittgenstein, cognitive psychologist Eleanor Rosch proposes that categories are not clean cut but that something may be more or less a member of a category . As such the search for musical universals would fail and would not provide one with a valid definition .
Social construct.
Many people do, however, share a general idea of music. The Websters definition of music is a typical example: "the science or art of ordering tones or sounds in succession, in combination, and in temporal relationships to produce a composition having unity and continuity" ("Webster's Collegiate Dictionary", online edition).
Subjective experience.
This approach to the definition focuses not on the "construction" but on the "experience" of music. An extreme statement of the position has been articulated by the Italian composer Luciano Berio: “Music is everything that one listens to with the intention of listening to music” . This approach permits the boundary between music and noise to change over time as the conventions of musical interpretation evolve within a culture, to be different in different cultures at any given moment, and to vary from person to person according to their experience and proclivities. It is further consistent with the subjective reality that even what would commonly be considered music is experienced as nonmusic if the mind is concentrating on other matters and thus not perceiving the sound's "essence" "as music" .
Specific definitions.
Clifton.
In his 1983 book, "Music as Heard", which sets out from the phenomenological position of Husserl, Merleau-Ponty, and Ricœur, Thomas Clifton defines music as "an ordered arrangement of sounds and silences whose meaning is presentative rather than denotative. . . . This definition distinguishes music, as an end in itself, from compositional technique, and from sounds as purely physical objects." More precisely, "music is the actualization of the possibility of any sound whatever to present to some human being a meaning which he experiences with his body—that is to say, with his mind, his feelings, his senses, his will, and his metabolism" . It is therefore "a certain reciprocal relation established between a person, his behavior, and a sounding object" .
Clifton accordingly differentiates music from nonmusic on the basis of the human behavior involved, rather than on either the nature of compositional technique or of sounds as purely physical objects. Consequently, the distinction becomes a question of what is meant by musical behavior: "a musically behaving person is one whose very being is absorbed in the significance of the sounds being experienced." However, "It is not altogether accurate to say that this person is listening "to" the sounds. First, the person is doing more than listening: he is perceiving, interpreting, judging, and feeling. Second, the preposition 'to' puts too much stress on the sounds as such. Thus, the musically behaving person experiences musical significance by means of, or through, the sounds" .
In this framework, Clifton finds that there are two things that separate music from nonmusic: (1) musical meaning is presentative, and (2) music and nonmusic are distinguished in the idea of personal involvement. "It is the notion of personal involvement which lends significance to the word "ordered" in this definition of music" . This is not to be understood, however, as a sanctification of extreme relativism, since "it is precisely the 'subjective' aspect of experience which lured many writers earlier in this century down the path of sheer opinion-mongering. Later on this trend was reversed by a renewed interest in 'objective,' scientific, or otherwise nonintrospective musical analysis. But we have good reason to believe that a musical experience is not a purely private thing, like seeing pink elephants, and that reporting about such an experience need not be subjective in the sense of it being a mere matter of opinion" .
Clifton's task, then, is to describe musical experience and the objects of this experience which, together, are called "phenomena," and the activity of describing phenomena is called "phenomenology" . It is important to stress that this definition of music says nothing about aesthetic standards. Music is not a fact or a thing in the world, but a meaning constituted by human beings. . . . To talk about such experience in a meaningful way demands several things. First, we have to be willing to let the composition speak to us, to let it reveal its own order and significance. . . . Second, we have to be willing to question our assumptions about the nature and role of musical materials. . . . Last, and perhaps most important, we have to be ready to admit that describing a meaningful experience is itself meaningful. 
Nattiez.
"Music, often an art/entertainment, is a total social fact whose definitions vary according to era and culture," according to Jean . It is often contrasted with noise. According to musicologist Jean-Jacques Nattiez: "The border between music and noise is always culturally defined—which implies that, even within a single society, this border does not always pass through the same place; in short, there is rarely a consensus... By all accounts there is no "single" and "intercultural" universal concept defining what music might be" . Given the above demonstration that "there is no limit to the number or the genre of variables that might intervene in a definition of the musical," an organization of definitions and elements is necessary.
Nattiez (1990, 17) describes definitions according to a tripartite semiological scheme similar to the following:
There are three levels of description, the poietic, the neutral, and the esthesic:
Table describing types of definitions of music :
Because of this range of definitions, the study of music comes in a wide variety of forms. There is the study of sound and vibration or acoustics, the cognitive study of music, the study of music theory and performance practice or music theory and ethnomusicology and the study of the reception and history of music, generally called musicology.
Xenakis.
Composer Iannis Xenakis in "Towards a Metamusic" (chapter 7 of "Formalized Music") defined music in the following way :

</doc>
<doc id="8253" url="http://en.wikipedia.org/wiki?curid=8253" title="Dayton, Ohio">
Dayton, Ohio

Dayton (; local pronunciation: ) is the sixth largest city in the state of Ohio and is the county seat of Montgomery County. In the 2010 census, the population was 141,527; the Dayton metropolitan area had 841,502 residents, making it the fourth-largest metropolitan area in Ohio, after only the urban agglomerations of Cleveland, Cincinnati and Columbus, and the 63rd largest in the United States. The Dayton-Springfield-Greenville Combined Statistical Area had a population of 1,080,044 in 2010 and is the 43rd largest in the United States. Dayton is situated within the Miami Valley region of Ohio just north of the Cincinnati–Northern Kentucky metropolitan area.
Ohio's borders are within 500 mi of roughly 60 percent of the country's population and manufacturing infrastructure, making the Dayton area a logistical centroid for manufacturers, suppliers, and shippers. Dayton also plays host to significant research and development in fields like industrial, aeronautical, and astronautical engineering that have led to many technological innovations. Much of this innovation is due in part to Wright-Patterson Air Force Base and its place within the community. With the decline of heavy manufacturing, Dayton's businesses have diversified into a service economy that includes insurance and legal sectors as well as healthcare and government sectors.
Other than defense and aerospace, healthcare accounts for much of the Dayton area's economy. Hospitals in the Greater Dayton area have an estimated combined employment of nearly 32,000, a yearly economic impact of $6.8 billion. It is estimated that Premier Health Partners, a hospital network, contributes more than $2 billion a year to the region through operating, employment, and capital expenditures. In 2011, Dayton was rated the No. 3 city in the nation out of the top 50 cities in the United States by HealthGrades for excellence in health care. Many hospitals in the Dayton area are consistently ranked by "Forbes", "U.S. News & World Report", and HealthGrades for clinical excellence.
Dayton is also noted for its association with aviation; the city is home to the National Museum of the United States Air Force. Orville Wright, poet Paul Laurence Dunbar, and entrepreneur John H. Patterson were born in Dayton. Dayton is also known for its many patents, inventions, and inventors that have come from the area, most notable being the Wright brothers' invention of powered flight. In 2008, 2009, and 2010, "Site Selection" magazine ranked Dayton the No. 1 mid-sized metropolitan area in the nation for economic development. Also in 2010, Dayton was named one of the best places in the United States for college graduates to find a job.
History.
Dayton was founded on April 1, 1796, by a group of 12 settlers known as "The Thompson Party." They traveled in March from Cincinnati up the Great Miami River by pirogue and landed at what is now St. Clair Street, where they found two small camps of Native Americans. Among the settlers was Benjamin Van Cleve, whose memoirs provide insights into the history of the Ohio Valley. Two other groups who were travelling overland arrived several days later.
In 1797, Daniel C. Cooper laid out Mad River Road, the first overland connection between Cincinnati and Dayton, opening the "Mad River Country" to settlement. Ohio was admitted into the Union in 1803, and the city of Dayton was incorporated in 1805. The city was named after Jonathan Dayton, a captain in the American Revolutionary War who signed the U.S. Constitution and owned a significant amount of land in the area. In 1827, construction on the Dayton-Cincinnati canal began, which would provide a better way to transport goods from Dayton to Cincinnati and contribute significantly to Dayton's economic growth during the 1800s.
Historically, Dayton has been the home for many patents and inventions since the 1870s. According to the National Park Service, citing information from the U.S. Patent Office, Dayton had granted more patents per capita than any other U.S. city in 1890 and ranked fifth in the nation as early as 1870. The Wright brothers, inventors of the world's first airplane, and Charles F. Kettering, world-renowned for his numerous inventions, hailed from Dayton. The city was also home to James Ritty's "Incorruptible Cashier", the first mechanical cash register, and Arthur E. Morgan's "hydraulic jump", a flood prevention mechanism that helped pioneer modern-day hydraulic engineering. Paul Laurence Dunbar – a famous African-American poet and novelist – penned his most famous works in the late 19th century and became an integral part of the city's history.
Innovation led to business growth in the region. In 1884, John Henry Patterson acquired James Ritty's National Manufacturing Company along with his cash register patents and formed the National Cash Register Company (NCR). The company manufactured the first mechanical cash registers and played a crucial role in the success and shaping of Dayton's reputation as an epicenter for manufacturing in the early 1900s. In 1906, Charles F. Kettering, a leading engineer at the company, helped develop the first electric cash register which propelled NCR into the national spotlight. NCR also helped develop the US Navy bombe, a code-breaking machine that helped crack the Enigma machine cipher during World War II.
A catastrophic flood occurred in March 1913, known as the Great Dayton Flood, which led to the establishment of the Miami Conservancy District, a series of dams and hydraulic jumps installed around Dayton, in 1914. Like other cities across the country, Dayton was heavily involved in the war effort during World War II. Several locations around the city hosted the Dayton Project, a branch of the larger Manhattan Project, to develop polonium triggers which were used in early atomic bombs. The war efforts led to a manufacturing boom throughout the city, including high demand for housing and other services. At one point, emergency housing was put into place due to a housing shortage in the region, much of which is still in use today.
Between the 1940s and the 1970s, the city saw significant growth in suburban areas as a result of population migration. Veterans were returning from military service in large numbers seeking industrial and manufacturing jobs, a part of the local industry that was expanding rapidly. Advancements in architecture also contributed to the suburban boom. New, modernized shopping centers and the Interstate Highway System allowed workers to commute greater distances and families to live further away from the downtown area. More than 127,000 homes were built in Montgomery County alone during the 1950s.
Since the 1980s, however, Dayton's population has been in decline, mainly due to the loss of manufacturing jobs and decentralization of metropolitan areas, as well as the national housing crisis that began in 2008. While much of the state has suffered for similar reasons, the negative impact on Dayton has been greater than most. Dayton had the third-greatest percentage loss of population in the state since the 1980s, behind only Cleveland and Youngstown. Despite this, Dayton has begun diversifying its workforce from manufacturing into other growing sectors of the local economy such as healthcare and education. New expansion downtown began in the 2000s helping to revitalize the city and encourage growth. Fifth Third Field, home of the Dayton Dragons, was built in 2000. The highly successful minor league baseball team has been an integral part of Dayton's culture. In 2001, the city's public park system, Five Rivers MetroParks, constructed an outdoor entertainment venue known as RiverScape MetroPark that attracts more than 400,000 visitors a year. A new performance arts theater, the Schuster Center, opened in 2003. A large health network in the region, Premier Health Partners, expanded its Miami Valley Hospital location with a 12-story tower addition.
In 2010, the Downtown Dayton Partnership, in cooperation with the City of Dayton and community leaders, introduced the Greater Downtown Dayton Plan. It focuses on job creation and retention, infrastructure improvements, housing, recreation, and collaboration. The plan is to be implemented through the year 2020.
Peace accords.
The Dayton Agreement, a peace accord between the parties to the hostilities of the conflict in Bosnia-Herzegovina and the former Yugoslavia, was negotiated at Wright-Patterson Air Force Base, near Fairborn, Ohio.
Negotiations took place from November 1, 1995, to November 21, 1995.
Richard Holbrooke wrote about this event in his memoirs:
 There was also a real Dayton out there, a charming Ohio city, famous as the birthplace of the Wright brothers. Its citizens energized us from the outset. Unlike the population of, say, New York City, Geneva or Washington, which would scarcely notice another conference, Daytonians were proud to be part of history. Large signs at the commercial airport hailed Dayton as the "temporary center of international peace". The local newspapers and television stations covered the story from every angle, drawing the people deeper into the proceedings. When we ventured into a restaurant or a shopping center downtown, people crowded around, saying that they were praying for us. Warren Christopher was given at least one standing ovation in a restaurant. Families on the air base placed "candles of peace" in their front windows, and people gathered in peace vigils outside the base. One day they formed a "peace chain", although it was not large enough to surround the sprawling eight-thousand-acre base. Ohio's famous ethnic diversity was on display.
Nickname.
Dayton is known as the "Gem City." The origin of the nickname is uncertain, but several theories exist. In the early 19th century, a well-known racehorse named "Gem" hailed from Dayton. In 1845, an article published in the "Cincinnati Daily Chronicle" by an author known as "T" stated: 
In a small bend of the Great Miami River, with canals on the east and south, it can be fairly said, without infringing on the rights of others, that Dayton is the gem of all our interior towns. It possesses wealth, refinement, enterprise, and a beautiful country, beautifully developed.
In the late 1840s, Major William D. Bickham of the "Dayton Journal" initiated a campaign to officially nickname Dayton as the "Gem City". The name was eventually adopted by the city's Board of Trade several years later. Paul Laurence Dunbar referred to the nickname in his poem, "Toast to Dayton", as noted in the following excerpt:
<poem>
She shall ever claim our duty,
For she shines—the brightest gem
That has ever decked with beauty
 Dear Ohio's diadem.
</poem>
Dayton also plays a role in a nickname given to the state of Ohio – "Birthplace of Aviation". Dayton is the hometown of the Wright brothers, aviation pioneers who are credited with inventing and building the world's first successful airplane. After their first manned flights in Kitty Hawk, North Carolina, which had been chosen due to its ideal weather and climate conditions, the Wrights returned to Dayton and continued testing at nearby Huffman Prairie.
Additionally, Dayton is colloquially referred to as "Little Detroit." This nickname comes from Dayton's prominence as a Midwestern manufacturing center.
Geography.
According to the United States Census Bureau, the city has a total area of 56.50 sqmi, of which 55.65 sqmi is land and 0.85 sqmi is water.
Climate.
Dayton's climate features hot, muggy summers and cold, dry winters, and is either classified as a humid subtropical climate (Köppen "Cfa"), using the -3 °C isotherm of the original Köppen scheme, or a humid continental climate (Köppen "Dfa"), using the 0 °C isotherm preferred by some climatologists. Unless otherwise noted, all normal figures quoted within the text below are from the official climatology station, Dayton International Airport, which, at an elevation of 304.8 m about 10 mi to the north of downtown Dayton, which lies within the valley of the Miami River, and thus temperatures there are typically cooler than in downtown.
At the airport, monthly mean temperatures range from 27.5 °F in January to 74.1 °F in July. The highest temperature ever recorded in Dayton was 108 °F on July 22, 1901, and the coldest was -28 °F on February 13, 1899. On average, there are 14 days of 90 °F+ highs and 4.5 nights of sub-0 °F lows annually. Snow is moderate, with a normal seasonal accumulation of 23.3 in, usually occurring from November to March, occasionally April, and rarely October. Precipitation averages 41.1 in annually, with total rainfall peaking in May.
Dayton is subject to severe weather typical of the Midwestern United States. Tornadoes are possible from the spring to the fall. Floods, blizzards and severe thunderstorms can also occur from time to time.
Demographics.
The population of Dayton had declined significantly from a peak of 262,332 residents in 1960 to only 141,527 in 2010. This was in part due to the slowdown of manufacturing in the region and the growth of Dayton's affluent suburbs including Oakwood, Englewood, Beavercreek, Springboro, Miamisburg, Kettering, and Centerville. The city's most populous ethnic group, white, declined from 78.1% in 1960 to 51.7% by 2010. However, recent census estimates show a 1.3% population increase since 2010, the first increase in five decades.
As of the 2000 census, the median income for a household in the city was $27,523, and the median income for a family was $34,978. Males had a median income of $30,816 versus $24,937 for females. The per capita income for the city was $34,724. About 18.2% of families and 23.0% of the population were below the poverty line, including 32.0% of those under age 18 and 15.3% of those age 65 or over.
2010 census.
As of the census of 2010, there were 141,527 people, 58,404 households, and 31,064 families residing in the city. The population density was 2543.2 PD/sqmi. There were 74,065 housing units at an average density of 1330.9 /sqmi. The racial makeup of the city was 51.7% White, 42.9% African American, 0.3% Native American, 0.9% Asian, 1.3% from other races, and 2.9% from two or more races. Hispanic or Latino of any race were 3.0% of the population.
There were 58,404 households of which 28.3% had children under the age of 18 living with them, 25.9% were married couples living together, 21.4% had a female householder with no husband present, 5.9% had a male householder with no wife present, and 46.8% were non-families. 38.8% of all households were made up of individuals and 11.2% had someone living alone who was 65 years of age or older. The average household size was 2.26 and the average family size was 3.03.
The median age in the city was 34.4 years. 22.9% of residents were under the age of 18; 14.2% were between the ages of 18 and 24; 25.3% were from 25 to 44; 25.8% were from 45 to 64; and 11.8% were 65 years of age or older. The gender makeup of the city was 48.7% male and 51.3% female.
2013 census population estimates.
The 2013 census population estimate shows an increasing City of Dayton population for the first time in five decades attributed to revitalization efforts downtown and the increasing downtown population.
Economy.
Dayton's economy is relatively diversified and vital to the overall economy of the state of Ohio. In 2008 and 2009, "Site Selection" magazine ranked Dayton the No. 1 medium-sized metropolitan area in the U.S. for economic development. Dayton is also among the top 100 metropolitan areas in both exports and export-related jobs, ranked 16 and 14 respectively by the Brookings Institution. The 2010 report placed the value of exports at $4.7 billion and the number of export-related jobs at 44,133. The Dayton Metropolitan Statistical Area ranks 4th in Ohio's Gross Domestic Product with a 2008 industry total of $33.78 billion. Additionally, Dayton ranks third among 11 major metropolitan areas in Ohio for exports to foreign countries. The Dayton Development Coalition is attempting to leverage the regions large water capacity, estimated to be 1.5 trillion gallons of renewable water aquifers, to attract new businesses. Moody's Investment Services revised Dayton's bond rating from A1 to the stronger rating of Aa2 as part of its global recalibration process. Standard & Poor's upgraded Dayton's rating from A+ to AA- in the summer of 2009.
"Bloomberg Businessweek" ranked Dayton in 2010 as one of the best places in the U.S. for college graduates looking for a job. Companies such as Reynolds and Reynolds, CareSource, DPL, LexisNexis, Kettering Health Network, Premier Health Partners, and Standard Register have their headquarters in Dayton. It is also the former home of the Speedwell Motor Car Company, MeadWestvaco (formerly known as the Mead Paper Company), and NCR. NCR was headquartered in Dayton for over 125 years and was a major innovator in computer technology.
Research, development, aerospace and aviation.
The Dayton region gave birth to aviation and is known for its high concentration of aerospace and aviation technology. In 2009, Governor Ted Strickland designated Dayton as Ohio's aerospace innovation hub, the first such technology hub in the state. Two major United States research and development organizations have leveraged Dayton's historical leadership in aviation and maintain their headquarters in the area: The National Air and Space Intelligence Center (NASIC) and the Air Force Research Laboratory (AFRL). NASIC is the U.S. military's primary producer of intelligence on foreign air and space forces, weapons and systems, while the AFRL provides leading-edge warfighting capabilities to keep the United States air, space and cyberspace forces the world's best. Both have their headquarters at Wright-Patterson Air Force Base. Wright-Patterson Air Force Base is one of the largest air base wings in the Air Force. The installion generated a Total Economic Impact in the Dayton area of $4.67 billion in fiscal year 2011, a decline from $5.1 billion in fiscal year 2009. In addition, state officials are working to make the Dayton region a hub and a leader for UAV research and manufacturing.
Several research organizations support NASIC, AFRL and the Dayton community. The Advanced Technical Intelligence Center, is a confederation of government, academic and industry partners that leverage advanced technical intelligence expertise. daytaOhio is a non-profit organization based at Wright State University in Dayton. The University of Dayton Research Institute (UDRI), is led by the University of Dayton. In 2004 and 2005, UDRI was ranked No. 2 in the nation by the National Science Foundation in federal and industry-funded materials research. The Cognitive Technologies Division (CTD) of Applied Research Associates, Inc., which carries out human-centered research and design, is headquartered in the Dayton suburb of Fairborn. The city of Dayton has started Tech Town, a development project intended to attract technology-based firms and revitalize the downtown area. Tech Town is home to the world's first RFID business incubator. The University of Dayton-led Institute for Development & Commercialization of Sensor Technologies (IDCAST) at TechTown is a world-class center for excellence in remote sensing and sensing technology. It is one of Dayton's technology business incubators housed in The Entrepreneurs Center building.
Healthcare.
The Kettering Health Network and Premier Health Partners have a major role on the Dayton area's economy. Hospitals in the Greater Dayton area have an estimated combined employment of nearly 32,000, a yearly economic impact of $6.8 billion. In addition, several Dayton area hospitals consistently earn top national ranking and recognition including the U.S. News & World Reports list of "America's Best Hospitals" as well as many of HealthGrades top ratings. The most notable hospitals are Miami Valley Hospital and Kettering Medical Center. In 2011, the Dayton area was rated number three in the nation out of the top 50 cities in the United States by HealthGrades for excellence in healthcare. Also in 2011, Dayton was ranked the fourth best in the nation for emergency medicine care. Then in 2013, HealthGrades ranked the Dayton region number one in the nation for the lowest hospital mortality rate.
Several key institutes and centers for health care exist in the Dayton region. The Center for Tissue Regeneration and Engineering at Dayton is a center that focuses on the science and development of human tissue regeneration. The National Center for Medical Readiness (NCMR) is also located in the Dayton area. The center includes Calamityville which is a state-of-the art disaster training facility. It is conservatively estimated that over a period of five years, Calamityville will have a regional economic impact of $374 million. Also, the Neurological Institute at Miami Valley Hospital is an institute focused on the diagnosis, treatment, and research of neurological disorders.
Top employers.
According to Dayton's 2011 Comprehensive Annual Financial Report, the top employers in the city proper are:
Government.
The Dayton City Commission is composed of the Mayor and four City Commissioners. Each City Commission member is elected at-large on a non-partisan basis for four-year, overlapping terms. All policy items are decided by the City Commission, which is empowered by the City Charter to pass ordinances and resolutions, adopt regulations and appoint the City Manager. The City Manager is responsible for budgeting and implementing policies and initiatives. Dayton was the first large American city to adopt the city manager form of municipal government, in 1913.
Cityscape.
Architecture.
Unlike many midwestern cities of its age, Dayton has very broad and straight downtown streets (generally two or three full lanes in each direction), facilitating access to the downtown even after the automobile became popular. The main reason for the broad streets was that Dayton was a marketing and shipping center from its beginning: streets were broad to enable wagons drawn by teams of three to four pairs of oxen to turn around. In addition, some of today's streets were once barge canals flanked by draw-paths.
A courthouse building was constructed in downtown Dayton in 1888 to supplement Dayton's original Neoclassical courthouse, which still stands. This second, "new" courthouse has since been replaced with new facilities as well as a park. The Old Court House has been a favored political campaign stop. On September 17, 1859, future president Abraham Lincoln delivered an address on the steps of the building. Eight other presidents have visited the courthouse, either as presidents or during presidential campaigns. They include Andrew Johnson, James Garfield, John F. Kennedy, Lyndon B. Johnson, Richard Nixon, Gerald Ford, Ronald Reagan, and Bill Clinton.
In 2009, the CareSource Management Group finished construction of a $55 million corporate headquarters in downtown Dayton. The 300000 sqft, 10-story building marks downtown's first new office tower in more than a decade.
The two tallest buildings of the Dayton skyline are the Kettering Tower at 408 ft (124 m) and the KeyBank Tower at 385 ft (117 m). Kettering Tower was originally Winters Tower, the headquarters of Winters Bank. The building was renamed after Virginia Kettering when Winters was merged into BankOne. KeyBank Tower was formerly known as the MeadWestvaco Tower before KeyBank gained naming rights to the building in 2008.
Ted Rall said over the last 5 decades Dayton has been demolishing some of its architecturally significant buildings in order to reduce the city's rental vacancy rate and thus increase the occupancy rate.
Neighborhoods.
Dayton's ten historic neighborhoods — Oregon District, Wright Dunbar, Dayton View, Grafton Hill, McPherson Town, Webster Station, Huffman, Kenilworth, St. Anne's Hill, and South Park — feature mostly single-family houses and mansions in the Neoclassical, Jacobethan, Tudor Revival, English Gothic, Chateauesque, Craftsman, Queen Anne, Georgian Revival, Colonial Revival, Renaissance Revival Architecture, Shingle Style Architecture, Prairie, Mission Revival, Eastlake/Italianate, American Foursquare, and Federal styles of architecture. Downtown Dayton is also a large area that encompasses several neighborhoods itself, and has seen a recent uplift and revival.
Suburbs.
Dayton's suburbs with a population of 10,000 or more include Beavercreek, Centerville, Clayton, Englewood, Fairborn, Harrison Township, Huber Heights, Kettering, Miami Township, Miamisburg, Oakwood, Riverside, Springboro (partial), Trotwood, Vandalia, Washington Township, West Carrollton, and Xenia.
Culture.
Fine arts.
The Dayton Region ranked within the top 10% in the nation out of 373 metropolitan areas in arts and culture. In 2012, Dayton ranked No. 2 in the country as an arts destination ranking higher than larger cities such as Atlanta, St. Louis, and Cincinnati. Dayton is the home of the Dayton Art Institute (see below).
The Benjamin and Marian Schuster Performing Arts Center in downtown Dayton is a world-class performing arts center and the home venue of the Dayton Philharmonic Orchestra, Dayton Opera, and the Dayton Ballet. In addition to Philharmonic and Opera performances, the Schuster Center hosts concerts, lectures, and traveling Broadway shows, and is a popular spot for weddings and other events. The historic Victoria Theatre, located in downtown Dayton, hosts concerts, traveling Broadway shows, ballet, a summertime classic film series, and more. The Loft Theatre, also located downtown, is the home of the Human Race Theatre Company. The Dayton Playhouse, in West Dayton, is the site of numerous plays and theatrical productions.
Dayton is the home to several ballet companies including:
Food.
Dayton is home to a variety of pizza chains that have become woven into local culture, the most notable of which are Cassano's and Marion's Piazza. Also based in Dayton is the burrito restaurant chain Hot Head Burritos, which was ranked by AOL.com in 2009 as one of America's next big chains.
Other notable Dayton-based food chains include Super Subby's and Submarine House, which specialize in submarine sandwiches; the Flying Pizza, a New York–style pizza chain; Fricker's, which specializes in chicken wings; Along with these food chains, Esther Price Candies, a candy and chocolate company, and Mike-sells, the oldest potato chip company in the United States, are also based in Dayton.
Religion.
Many major religions are represented in Dayton. Christianity is represented in Dayton by dozens of denominations and their respective churches. Notable Dayton churches include the First Lutheran Church, Sacred Heart Church and Ginghamsburg Church. Dayton's Muslim community is largely represented by the Islamic Society of Greater Dayton (ISGD), a Muslim community that includes a mosque on Josie Street. Dayton is also home to the United Theological Seminary, one of thirteen seminaries affiliated with the United Methodist Church. Judaism is represented by Temple Israel. Hinduism is represented by the Hindu Temple of Dayton.
Tourism.
Tourists visiting Montgomery County accounted for $1.7 billion in business activity in 2007. Tourism also accounts for 1 out of every 14 private sector jobs in the county. Tourism in the Dayton region is led by The National Museum of the United States Air Force at nearby Wright-Patterson Air Force Base. It is the largest and oldest military aviation museum in the world. The museum draws over 1.3 million visitors per year and is one of the single most visited tourist attractions in Ohio. The museum houses the National Aviation Hall of Fame.
Other museums also play significant roles in the tourism and economy of the Dayton area. The Dayton Art Institute, a museum of fine arts, owns collections containing more than 20,000 objects spanning 5,000 years of art and archaeological history. The Dayton Art Institute was rated one of the top 10 best art museums in the United States for children. Dayton is also home to a children's museum. The Boonshoft Museum of Discovery is a local children's museum of science with numerous exhibits, one of which includes an indoor zoo with nearly 100 different animals.
Some historical museums also have notability in the region. The Dayton Aviation Heritage National Historical Park, operated by the National Park Service, commemorates the lives and achievements of Dayton natives Orville and Wilbur Wright and Paul Laurence Dunbar. The Wright brothers' famous Wright Flyer III aircraft is housed in a museum at Carillon Historical Park. Dayton is also home to America's Packard Museum with contains many restored historical Packard vehicles. Another notable park, SunWatch Indian Village/Archaeological Park is located on the south end of Dayton. SunWatch is a partially reconstructed 12th-century prehistoric American Indian village; the village is organized around a central plaza dominated by wood posts forming an astronomical calendar. It includes a museum where visitors can learn about the Indian history of the Miami Valley.
Entertainment.
The Vectren Dayton Air Show is an annual air show that takes place at the Dayton International Airport. The Vectren Dayton Airshow is one of the largest air shows in the United States.
The Dayton area is served by Five Rivers MetroParks, encompassing 14161 acres over 23 facilities for year-round recreation, education, and conservation. In cooperation with the Miami Conservancy District, the MetroParks maintains over 70 mi miles of paved, multi-use scenic trails that connect Montgomery County with Greene, Miami, Warren and Butler Counties. From 1996 to 1998, Dayton hosted the National Folk Festival. Since then, the annual Cityfolk Festival has continued to bring the best in folk, ethnic and world music and arts to Dayton. The Five Rivers MetroParks also owns and operates the PNC Second Street Market located near downtown Dayton. The Market has more than 50 vendors selling items such as produce, cooked foods, baked goods, crafts, and flowers.
The Dayton area hosts several arenas and venues. South of Dayton in Kettering is the Fraze Pavilion, which hosts many nationally and internationally known musicians for concerts. Several notable performances have included the Backstreet Boys, Boston, and Steve Miller Band. South of downtown, on the banks of the Great Miami River, is the University of Dayton Arena, home venue for the University of Dayton Flyers basketball teams and the location of various other events and concerts. UD Arena also hosts the Winter Guard International championships, at which hundreds of percussion and color guard ensembles compete from around the world. North of Dayton is the Hara Arena that frequently hosts expo events and concerts. In addition, the Dayton Amateur Radio Association hosts the annual Dayton Hamvention, North America's largest hamfest, at Hara Arena. Up to 25,000 amateur radio operators attend this convention. The Nutter Center, which is just east of Dayton in the suburb of Fairborn, is the home arena for athletics of Wright State University and the former Dayton Bombers hockey team. This venue is used for many concerts, community events, and various national traveling shows and performances.
The Oregon District is a historic residential and commercial district in southeast downtown Dayton. The district is populated with art galleries, specialty shops, pubs, nightclubs, and coffee houses.
The City of Dayton is also host to yearly festivals. Most notably the Dayton Celtic Festival and the City Folk Festival. The Dayton Celtic Festival attracts more than 30,000 people yearly and has Irish dancing, food, crafts, and performers such as Gaelic Storm. Other festivals held in the city of Dayton include the Dayton Blues Festival, Dayton Music Fest, Urban Nights, Women in Jazz, the African American and Cultural Festival, and the Dayton Reggae Fest.
Sports.
The Dayton area is home to several minor league and semi pro teams, as well as NCAA Division I sports programs.
Baseball.
The Dayton Dragons professional baseball team is the minor league affiliate for the Cincinnati Reds. The Dayton Dragons are the first (and only) team in minor league baseball history to sell out an entire season before it began and was voted as one of the top 10 hottest tickets to get in all of professional sports by Sports Illustrated. The Dayton Dragons 815 consecutive sellouts surpassed the NBA's Portland Trail Blazers for the longest sellout streak across all professional sports in the U.S.
Collegiate.
The University of Dayton and Wright State University both host NCAA basketball. The University of Dayton Arena has hosted more games in the NCAA men's basketball tournament over its history than any other venue. UD Arena is also the site of the First Round games of the NCAA Tournament. In 2012, eight teams competed for the final four spots in the NCAA Basketball Tournament. Wright State University's NCAA men's basketball is the Wright State Raiders and the University of Dayton's NCAA men's basketball team is the Dayton Flyers.
Hockey.
The Dayton Bombers were an ECHL ice hockey team that most recently played the North Division of the ECHL's American Conference. In June 2009, it was announced that the Bombers would turn in their membership back to the league. However, hockey remained in Dayton as the Dayton Gems of the International Hockey League began play in the fall of 2009 at Hara Arena. The Gems folded after the 2011–2012 season. Shortly after the Gems folded, it was announced that a new team, the Dayton Demonz, would begin play in 2012 for the FHL.
Football.
Dayton hosted the first American Professional Football Association game (precursor to the NFL). The game was played at Triangle Park between the Dayton Triangles and the Columbus Panhandles on October 3, 1920, and is considered one of the first professional football games ever played. Football teams in the Dayton area include the Dayton Flyers and the Dayton Sharks.
Golf.
The Dayton region is also known for the many golf courses and clubs that it hosts. The Miami Valley Golf Club, Moraine Country Club, NCR Country Club, and the Pipestone Golf Course are some of the more notable courses. In addition, several PGA Championships have been held at area golf courses. The Miami Valley Golf Club hosted the 1957 PGA Championship, the Moraine Country Club hosted the 1945 PGA Championship, and the NCR Country club hosted the 1969 PGA Championship.Additionally, NCR CC hosted the 1986 U.S. Women's Open and the 2005 U.S. Senior Open. Other notable courses include the Yankee Trace Golf Club, the Beavercreek Golf Club, Dayton Meadowbrook Country Club, Heatherwoode Golf Club, Community Golf Course, and Kitty Hawk Golf Course.
Rugby Union.
The city of Dayton is the home to the Dayton Area Rugby Club. As of 2010, the club fields three squads and play their home games at Eastwood Metropark.
Media.
Dayton is served in print by "The Dayton Daily News", the city's sole remaining daily newspaper. The "Dayton Daily News" is owned by Cox Enterprises. As well as the daily print, the Dayton region's main business newspaper is the Dayton Business Journal. Nielsen Media Research ranked the 11-county Dayton television market as the No. 62 market in the United States. The market is served by stations affiliated with major American networks including: WKEF, Channel 22 – ABC, operated by Sinclair Broadcasting, WHIO-TV, Channel 7 – CBS, operated by Cox Media Group, WPTD, Channel 16 – PBS, operated by ThinkTV, which also operates WPTO, assigned to Oxford, Ohio, WDTN, Channel 2 – NBC, operated by LIN TV, WBDT, Channel 26 – The CW, operated by Acme Television, and WRGT-TV, Channel 45 – Fox/My Network TV, operated under a local marketing agreement by Sinclair Broadcasting. The nationally syndicated morning talk show "The Daily Buzz" originated from WBDT-TV, the Acme property in Miamisburg, Ohio, before moving to its current home in Florida. Dayton is also served by 42 AM and FM radio stations directly, and numerous other stations are heard from elsewhere in Southwest Ohio, which serve outlying suburbs and adjoining counties.
Transportation.
Public transit.
The Greater Dayton Regional Transit Authority (RTA) operates public bus routes in the Dayton metro area. In addition to routes covered by traditional diesel-powered buses, RTA has a number of electric trolley bus routes. The Dayton trolleybus system is the second longest-running of the six remaining trolleybus systems in the U.S., having entered service in 1933. It is the present manifestation of an electric transit service that has been operated continuously in Dayton since 1888.
Dayton operates a Greyhound Station which provides inter-city bus transportation to and from Dayton. The hub is located in the Greater Dayton Regional Transit Authority North-West hub in Trotwood, Ohio.
Airports.
Air transportation is available just north of Dayton proper via the Dayton International Airport. The airport operates 24 hours a day, seven days a week, and offers service to 21 markets through 10 airlines. In 2008, it served 2.9 million passengers. The Dayton International Airport is also a significant regional air freight hub hosting FedEx Express, UPS Airlines, United States Postal Service, and major commercial freight carriers.
The Dayton area also has several regional airports. The Dayton-Wright Brothers Airport is a general aviation airport owned by the City of Dayton located 10 mi south of the central business district of Dayton on Springboro Pike in Miami Township. It serves as the reliever airport for Dayton International Airport. The airport primarily serves corporate and personal aircraft users. The Dahio Trotwood Airport, also known as Dayton-New Lebanon Airport, is a privately owned, public-use airport located 7 mi west of the central business district of Dayton. The Moraine Airpark is a privately owned, public-use airport situated 4 mi southwest of the city of Dayton.
Major highways.
The Dayton region is primarily served by three interstates:
Other major routes for the region include:
The Ohio Department of Transportation is currently in the process of $533 million of construction to modify and reconstruct I-75 through downtown Dayton. ODOT is upgrading and widening I-75 from Edwin C Moses Blvd. to Stanley Avenue.
Rail freight.
Dayton hosts several inter-modal freight railroad terminals. Two Class I railroads both CSX and Norfolk Southern Railway, operate switching yards in the city.
Bicycling.
In cooperation with the Miami Conservancy District, Five Rivers MetroParks maintains over 70 mi miles of paved, off-road, multi-use scenic trails that connect Montgomery County with over 270 mi of trails in Greene, Miami, Warren and Butler Counties. The contiguous bike trail system extends as far east as southwest Columbus and as far south as the Ohio River just east of Cincinnati.
The League of American Bicyclists named Dayton as one of only two major cities in Ohio to be "bicycle-friendly". Dayton has also implemented "bike only" lanes downtown.
Education.
Public schools.
The Dayton Public Schools operates 34 schools that serve 16,855 students, including:
Private schools.
The city of Dayton has 35 private schools located within the city, including:
Charter schools.
Dayton has 33 charter schools. 
Three of the top five charter schools named in 2011 are K-8 schools managed by National Heritage Academies.
Colleges and universities.
Dayton is home to two major universities. The University of Dayton is a private, Catholic institution founded in 1850 by the Marianist order, which has the only American Bar Association (ABA) approved law school in the Dayton area. The University of Dayton is also Ohio's largest private university and is also home to the University of Dayton Research Institute, which ranks second in the nation for sponsored research, and the Center for Tissue Regeneration and Engineering at Dayton which focuses on human tissue regeneration.
The public Wright State University became a state university in 1967. Wright State University established the National Center for Medical Readiness, a national training program for disaster preparedness and relief. The Boonshoft School of Medicine at Wright State University is the only medical school in the Dayton area and is a leader in biomedical research.
Dayton is also home to Sinclair Community College, the largest community college at a single location in Ohio and one of the largest community colleges in the nation. Sinclair is acclaimed as one of the country's best community colleges. Sinclair was originally founded as the YMCA college in 1887.
Dayton is also home to Miami-Jacobs College, the International School of Broadcasting, and the Dayton School of Medical Massage. Other schools just outside of Dayton that shape the educational landscape are Kettering College of Medical Arts and School of Advertising Art in Kettering, DeVry University in Beavercreek (Dayton), and Clark State Community College in Springfield. Just outside of Dayton proper is the public Air Force Institute of Technology, which was founded in 1919 and serves as a graduate school for the United States Air Force. The Air Force Institute of Technology is located at the nearby Wright-Patterson Air Force Base.
The Dayton area was ranked the 10th best metropolitan area in the United States for higher education by "Forbes" in 2009.
Crime.
Dayton has experienced an improving public safety environment since 2003, with crime declining in key categories according to FBI Uniform Crime Reports and Dayton Police Department data. City officials reported in January 2008 a decline of 6.1 percent in crime for 2007 when compared to 2006. From 2003 to 2007, crime decreased by 10.7 percent. Among violent crimes (homicide, rape, robbery and aggravated assault), Dayton saw a decline of 17.3 percent over the five years ending December 31, 2007. Targeted crimes in Dayton declined 39 percent over the five-year period. In 2009, crime continued to fall in the city of Dayton. Crime in the categories of forcible rape, aggravated assault, property crime, motor vehicle theft, robbery, burglary, theft and arson all showed declines for 2009. Overall, crime in Dayton dropped 40 percent over the previous year.
Also notable, John Dillinger a famous bank robber during the early 1930s, was at one time captured and arrested by Dayton city police while visiting his girlfriend at a high-class boarding house in downtown Dayton.
Sister cities.
Dayton has five sister cities, as designated by Sister Cities International: 

</doc>
<doc id="8254" url="http://en.wikipedia.org/wiki?curid=8254" title="Diode">
Diode

In electronics, a diode is a two-terminal electronic component with asymmetric conductance; it has low (ideally zero) resistance to current in one direction, and high (ideally infinite) resistance in the other. A semiconductor diode, the most common type today, is a crystalline piece of semiconductor material with a p–n junction connected to two electrical terminals. A vacuum tube diode has two electrodes, a plate (anode) and a heated cathode. Semiconductor diodes were the first semiconductor electronic devices. The discovery of crystals' rectifying abilities was made by German physicist Ferdinand Braun in 1874. The first semiconductor diodes, called cat's whisker diodes, developed around 1906, were made of mineral crystals such as galena. Today, most diodes are made of silicon, but other semiconductors such as selenium or germanium are sometimes used.
Main functions.
The most common function of a diode is to allow an electric current to pass in one direction (called the diode's "forward" direction), while blocking current in the opposite direction (the "reverse" direction). Thus, the diode can be viewed as an electronic version of a check valve. This unidirectional behavior is called rectification, and is used to convert alternating current to direct current, including extraction of modulation from radio signals in radio receivers—these diodes are forms of rectifiers.
However, diodes can have more complicated behavior than this simple on–off action, due to their nonlinear current-voltage characteristics. Semiconductor diodes begin conducting electricity only if a certain threshold voltage or cut-in voltage is present in the forward direction (a state in which the diode is said to be "forward-biased"). The voltage drop across a forward-biased diode varies only a little with the current, and is a function of temperature; this effect can be used as a temperature sensor or voltage reference.
Semiconductor diodes' current–voltage characteristic can be tailored by varying the semiconductor materials and doping, introducing impurities into the materials. These techniques are used to create special-purpose diodes that perform many different functions. For example, diodes are used to regulate voltage (Zener diodes), to protect circuits from high voltage surges (avalanche diodes), to electronically tune radio and TV receivers (varactor diodes), to generate radio-frequency oscillations (tunnel diodes, Gunn diodes, IMPATT diodes), and to produce light (light-emitting diodes). Tunnel, Gunn and IMPATT diodes exhibit negative resistance, which is useful in microwave and switching circuits.
History.
Thermionic (vacuum tube) diodes and solid state (semiconductor) diodes were developed separately, at approximately the same time, in the early 1900s, as radio receiver detectors. Until the 1950s vacuum tube diodes were more often used in radios because the early point-contact type semiconductor diodes (cat's-whisker detectors) were less stable, and because most receiving sets had vacuum tubes for amplification that could easily have diodes included in the tube (for example the 12SQ7 double diode triode), and vacuum tube rectifiers and gas-filled rectifiers handled some high voltage/high current rectification tasks beyond the capabilities of semiconductor diodes (such as selenium rectifiers) available at the time.
Vacuum tube diodes.
In 1873, Frederick Guthrie discovered the basic principle of operation of thermionic diodes. Guthrie discovered that a positively charged electroscope could be discharged by bringing a grounded piece of white-hot metal close to it (but not actually touching it). The same did not apply to a negatively charged electroscope, indicating that the current flow was only possible in one direction.
Thomas Edison independently rediscovered the principle on February 13, 1880. At the time, Edison was investigating why the filaments of his carbon-filament light bulbs nearly always burned out at the positive-connected end. He had a special bulb made with a metal plate sealed into the glass envelope. Using this device, he confirmed that an invisible current flowed from the glowing filament through the vacuum to the metal plate, but only when the plate was connected to the positive supply.
Edison devised a circuit where his modified light bulb effectively replaced the resistor in a DC voltmeter. Edison was awarded a patent for this invention in 1884. Since there was no apparent practical use for such a device at the time, the patent application was most likely simply a precaution in case someone else did find a use for the so-called Edison effect.
About 20 years later, John Ambrose Fleming (scientific adviser to the Marconi Company
and former Edison employee) realized that the Edison effect could be used as a precision radio detector. Fleming patented the first true thermionic diode, the Fleming valve, in Britain on November 16, 1904 (followed by U.S. Patent in November 1905).
Solid-state diodes.
In 1874 German scientist Karl Ferdinand Braun discovered the "unilateral conduction" of crystals. Braun patented the crystal rectifier in 1899. Copper oxide and selenium rectifiers were developed for power applications in the 1930s.
Indian scientist Jagadish Chandra Bose was the first to use a crystal for detecting radio waves in 1894. The crystal detector was developed into a practical device for wireless telegraphy by Greenleaf Whittier Pickard, who invented a silicon crystal detector in 1903 and received a patent for it on November 20, 1906. Other experimenters tried a variety of other substances, of which the most widely used was the mineral galena (lead sulfide). Other substances offered slightly better performance, but galena was most widely used because it had the advantage of being cheap and easy to obtain. The crystal detector in these early crystal radio sets consisted of an adjustable wire point-contact (the so-called "cat's whisker"), which could be manually moved over the face of the crystal in order to obtain optimum signal. This troublesome device was superseded by thermionic diodes by the 1920s, but after high purity semiconductor materials became available, the crystal detector returned to dominant use with the advent of inexpensive fixed-germanium diodes in the 1950s. Bell Labs also developed a germanium diode for microwave reception, and AT&T used these in their microwave towers that criss-crossed the nation starting in the late 1940s, carrying telephone and network television signals. Bell Labs did not develop a satisfactory thermionic diode for microwave reception.
Etymology.
At the time of their invention, such devices were known as rectifiers. In 1919, the year tetrodes were invented, William Henry Eccles coined the term diode from the Greek roots "di" (from "δί"), meaning "two", and "ode" (from "ὁδός"), meaning "path". (However, the word "diode" itself, as well as "triode, tetrode, pentode, hexode", was already in use as a term of multiplex telegraphy; see, for example, "The telegraphic journal and electrical review", September 10, 1886, p. 252).
Rectifiers.
Although all diodes "rectify", the term 'rectifier' is normally reserved for higher currents and voltages than would normally be found in the rectification of lower power signals; examples include: 
Thermionic diodes.
A thermionic diode is a thermionic-valve device (also known as a vacuum tube, tube, or valve), consisting of a sealed evacuated glass envelope containing two electrodes: a cathode heated by a filament, and a plate (anode). Early examples were fairly similar in appearance to incandescent light bulbs.
In operation, a separate current through the filament (heater), a high resistance wire made of nichrome, heats the cathode red hot (800–1000 °C), causing it to release electrons into the vacuum, a process called thermionic emission. The cathode is coated with oxides of alkaline earth metals such as barium and strontium oxides, which have a low work function, to increase the number of electrons emitted. (Some valves use "direct heating", in which a tungsten filament acts as both heater and cathode.) The alternating voltage to be rectified is applied between the cathode and the concentric plate electrode. When the plate has a positive voltage with respect to the cathode, it electrostatically attracts the electrons from the cathode, so a current of electrons flows through the tube from cathode to plate. However when the polarity is reversed and the plate has a negative voltage, no current flows, because the cathode electrons are not attracted to it. The unheated plate does not emit any electrons itself. So electrons can only flow through the tube in one direction, from cathode to plate.
In a mercury-arc valve, an arc forms between a refractory conductive anode and a pool of liquid mercury acting as cathode. Such units were made with ratings up to hundreds of kilowatts, and were important in the development of HVDC power transmission. Some types of smaller thermionic rectifiers sometimes had mercury vapor fill to reduce their forward voltage drop and to increase current rating over thermionic hard-vacuum devices.
Throughout the vacuum tube era, valve diodes were used in analog signal applications and as rectifiers in DC power supplies in consumer electronics such as radios, televisions, and sound systems. They were replaced in power supplies beginning in the 1940s by selenium rectifiers and then by semiconductor diodes by the 1960s. Today they are still used in a few high power applications where their ability to withstand transients and their robustness gives them an advantage over semiconductor devices. The recent (2012) resurgence of interest among audiophiles and recording studios in old valve audio gear such as guitar amplifiers and home audio systems has provided a market for the legacy consumer diode valves.
Semiconductor diodes.
Electronic symbols.
The symbol used for a semiconductor diode in a circuit diagram specifies the type of diode. There are alternative symbols for some types of diodes, though the differences are minor.
Point-contact diodes.
A point-contact diode works the same as the junction diodes described below, but their construction is simpler. A block of n-type semiconductor is built, and a conducting sharp-point contact made with some group-3 metal is placed in contact with the semiconductor. Some metal migrates into the semiconductor to make a small region of p-type semiconductor near the contact. The long-popular 1N34 germanium version is still used in radio receivers as a detector and occasionally in specialized analog electronics.
Junction diodes.
p–n junction diode.
A p–n junction diode is made of a crystal of semiconductor, usually silicon, but germanium and gallium arsenide are also used. Impurities are added to it to create a region on one side that contains negative charge carriers (electrons), called n-type semiconductor, and a region on the other side that contains positive charge carriers (holes), called p-type semiconductor. When two materials i.e. n-type and p-type are attached together, a momentary flow of electrons occur from n to p side resulting in a third region where no charge carriers are present. This region is called the depletion region due to the absence of charge carriers (electrons and holes in this case). The diode's terminals are attached to the n-type and p-type regions. The boundary between these two regions, called a p–n junction, is where the action of the diode takes place. The crystal allows electrons to flow from the N-type side (called the cathode) to the P-type side (called the anode), but not in the opposite direction.
Schottky diode.
Another type of junction diode, the Schottky diode, is formed from a metal–semiconductor junction rather than a p–n junction, which reduces capacitance and increases switching speed.
Current–voltage characteristic.
A semiconductor diode's behavior in a circuit is given by its current–voltage characteristic, or I–V graph (see graph below). The shape of the curve is determined by the transport of charge carriers through the so-called "depletion layer" or "depletion region" that exists at the p–n junction between differing semiconductors. When a p–n junction is first created, conduction-band (mobile) electrons from the N-doped region diffuse into the P-doped region where there is a large population of holes (vacant places for electrons) with which the electrons "recombine". When a mobile electron recombines with a hole, both hole and electron vanish, leaving behind an immobile positively charged donor (dopant) on the N side and negatively charged acceptor (dopant) on the P side. The region around the p–n junction becomes depleted of charge carriers and thus behaves as an insulator.
However, the width of the depletion region (called the depletion width) cannot grow without limit. For each electron–hole pair that recombines, a positively charged dopant ion is left behind in the N-doped region, and a negatively charged dopant ion is left behind in the P-doped region. As recombination proceeds more ions are created, an increasing electric field develops through the depletion zone that acts to slow and then finally stop recombination. At this point, there is a "built-in" potential across the depletion zone.
If an external voltage is placed across the diode with the same polarity as the built-in potential, the depletion zone continues to act as an insulator, preventing any significant electric current flow (unless electron–hole pairs are actively being created in the junction by, for instance, light; see photodiode). This is the "reverse bias" phenomenon. However, if the polarity of the external voltage opposes the built-in potential, recombination can once again proceed, resulting in substantial electric current through the p–n junction (i.e. substantial numbers of electrons and holes recombine at the junction). For silicon diodes, the built-in potential is approximately 0.7 V (0.3 V for germanium and 0.2 V for Schottky). Thus, if an external current passes through the diode, the voltage across the diode increases logarithmic with the current such that the P-doped region is positive with respect to the N-doped region and the diode is said to be "turned on" as it has a "forward bias". The diode is commonly said to have a forward "threshold" voltage, which it conducts above and is cutoff below. However, this is only an approximation as the forward characteristic is according to the Shockley equation absolutely smooth (see graph below).
A diode's I–V characteristic can be approximated by four regions of operation:
In a small silicon diode at rated currents, the voltage drop is about 0.6 to 0.7 volts. The value is different for other diode types—Schottky diodes can be rated as low as 0.2 V, germanium diodes 0.25 to 0.3 V, and red or blue light-emitting diodes (LEDs) can have values of 1.4 V and 4.0 V respectively.
At higher currents the forward voltage drop of the diode increases. A drop of 1 V to 1.5 V is typical at full rated current for power diodes.
Shockley diode equation.
The "Shockley ideal diode equation" or the "diode law" (named after transistor co-inventor William Bradford Shockley) gives the I–V characteristic of an ideal diode in either forward or reverse bias (or no bias). The following equation is called the "Shockley ideal diode equation" when "n", the ideality factor, is set equal to 1 :
where
The thermal voltage "V"T is approximately 25.85 mV at 300 K, a temperature close to "room temperature" commonly used in device simulation software. At any temperature it is a known constant defined by:
where "k" is the Boltzmann constant, "T" is the absolute temperature of the p–n junction, and "q" is the magnitude of charge of an electron (the elementary charge).
The reverse saturation current, "I"S, is not constant for a given device, but varies with temperature; usually more significantly than "V"T, so that "V"D typically decreases as "T" increases.
The "Shockley ideal diode equation" or the "diode law" is derived with the assumption that the only processes giving rise to the current in the diode are drift (due to electrical field), diffusion, and thermal recombination–generation (R–G) (this equation is derived by setting n = 1 above). It also assumes that the R–G current in the depletion region is insignificant. This means that the "Shockley ideal diode equation" doesn't account for the processes involved in reverse breakdown and photon-assisted R–G. Additionally, it doesn't describe the "leveling off" of the I–V curve at high forward bias due to internal resistance. Introducing the ideality factor, n, accounts for recombination and generation of carriers.
Under "reverse bias" voltages the exponential in the diode equation is negligible, and the current is a constant (negative) reverse current value of −"IS". The reverse "breakdown region" is not modeled by the Shockley diode equation.
For even rather small "forward bias" voltages the exponential is very large, since the thermal voltage is very small in comparison. The subtracted '1' in the diode equation is then negligible and the forward diode current can be approximated by
The use of the diode equation in circuit problems is illustrated in the article on diode modeling.
Small-signal behavior.
For circuit design, a small-signal model of the diode behavior often proves useful. A specific example of diode modeling is discussed in the article on small-signal circuits.
Reverse-recovery effect.
Following the end of forward conduction in a p–n type diode, a reverse current can flow for a short time. The device does not attain its blocking capability until the mobile charge in the junction is depleted.
The effect can be significant when switching large currents very quickly. A certain amount of "reverse recovery time" tr (on the order of tens of nanoseconds to a few microseconds) may be required to remove the reverse recovery charge Qr from the diode. During this recovery time, the diode can actually conduct in the reverse direction. This might give rise to a large constant current in the reverse direction for a short period of time and while the diode is reverse biased. The magnitude of such reverse current is determined by the operating circuit (i.e., the series resistance) and the diode is called to be in the storage-phase. In certain real-world cases it can be important to consider the losses incurred by this non-ideal diode effect. However, when the slew rate of the current is not so severe (e.g. Line frequency) the effect can be safely ignored. For most applications, the effect is also negligible for Schottky diodes.
The reverse current ceases abruptly when the stored charge is depleted; this abrupt stop is exploited in step recovery diodes for generation of extremely short pulses.
Types of semiconductor diode.
There are several types of p–n junction diodes, which emphasize either a different physical aspect of a diode often by geometric scaling, doping level, choosing the right electrodes, are just an application of a diode in a special circuit, or are really different devices like the Gunn and laser diode and the MOSFET:
Normal (p–n) diodes, which operate as described above, are usually made of doped silicon or, more rarely, germanium. Before the development of silicon power rectifier diodes, cuprous oxide and later selenium was used; its low efficiency gave it a much higher forward voltage drop (typically 1.4 to 1.7 V per "cell", with multiple cells stacked to increase the peak inverse voltage rating in high voltage rectifiers), and required a large heat sink (often an extension of the diode's metal substrate), much larger than a silicon diode of the same current ratings would require. The vast majority of all diodes are the p–n diodes found in CMOS integrated circuits, which include two diodes per pin and many other internal diodes.
Avalanche diodes
Cat's whisker or crystal diodes
Constant current diodes
Esaki or tunnel diodes
Gunn diodes
Light-emitting diodes (LEDs)
Laser diodes
Thermal diodes
Photodiodes
PIN diodes
Schottky diodes
Super barrier diodes
Gold-doped diodes
Snap-off or Step recovery diodes
Stabistors or "Forward Reference Diodes"
Transient voltage suppression diode (TVS)
Varicap or varactor diodes
Zener diodes
Other uses for semiconductor diodes include sensing temperature, and computing analog logarithms (see Operational amplifier applications#Logarithmic output).
Numbering and coding schemes.
There are a number of common, standard and manufacturer-driven numbering and coding schemes for diodes; the two most common being the EIA/JEDEC standard and the European Pro Electron standard:
EIA/JEDEC.
The standardized 1N-series numbering "EIA370" system was introduced in the US by EIA/JEDEC (Joint Electron Device Engineering Council) about 1960. Most diodes have a 1-prefix designation (e.g., 1N4003). Among the most popular in this series were: 1N34A/1N270 (germanium signal), 1N914/1N4148 (silicon signal), 1N4001-1N4007 (silicon 1A power rectifier) and 1N54xx (silicon 3A power rectifier)
JIS.
The JIS semiconductor designation system has all semiconductor diode designations starting with "1S".
Pro Electron.
The European Pro Electron coding system for active components was introduced in 1966 and comprises two letters followed by the part code. The first letter represents the semiconductor material used for the component (A = germanium and B = silicon) and the second letter represents the general function of the part (for diodes: A = low-power/signal, B = variable capacitance, X = multiplier, Y = rectifier and Z = voltage reference), for example:
Other common numbering / coding systems (generally manufacturer-driven) include:
As well as these common codes, many manufacturers or organisations have their own systems too – for example:
Related devices.
In optics, an equivalent device for the diode but with laser light would be the Optical isolator, also known as an Optical Diode, that allows light to only pass in one direction. It uses a Faraday rotator as the main component.
Applications.
Radio demodulation.
The first use for the diode was the demodulation of amplitude modulated (AM) radio broadcasts. The history of this discovery is treated in depth in the radio article. In summary, an AM signal consists of alternating positive and negative peaks of a radio carrier wave, whose amplitude or envelope is proportional to the original audio signal. The diode (originally a crystal diode) rectifies the AM radio frequency signal, leaving only the positive peaks of the carrier wave. The audio is then extracted from the rectified carrier wave using a simple filter and fed into an audio amplifier or transducer, which generates sound waves.
Power conversion.
Rectifiers are constructed from diodes, where they are used to convert alternating current (AC) electricity into direct current (DC). Automotive alternators are a common example, where the diode, which rectifies the AC into DC, provides better performance than the commutator or earlier, dynamo. Similarly, diodes are also used in "Cockcroft–Walton voltage multipliers" to convert AC into higher DC voltages.
Over-voltage protection.
Diodes are frequently used to conduct damaging high voltages away from sensitive electronic devices. They are usually reverse-biased (non-conducting) under normal circumstances. When the voltage rises above the normal range, the diodes become forward-biased (conducting). For example, diodes are used in (stepper motor and H-bridge) motor controller and relay circuits to de-energize coils rapidly without the damaging voltage spikes that would otherwise occur. (Any diode used in such an application is called a flyback diode). Many integrated circuits also incorporate diodes on the connection pins to prevent external voltages from damaging their sensitive transistors. Specialized diodes are used to protect from over-voltages at higher power (see Diode types above).
Logic gates.
Diodes can be combined with other components to construct AND and OR logic gates. This is referred to as diode logic.
Ionizing radiation detectors.
In addition to light, mentioned above, semiconductor diodes are sensitive to more energetic radiation. In electronics, cosmic rays and other sources of ionizing radiation cause noise pulses and single and multiple bit errors.
This effect is sometimes exploited by particle detectors to detect radiation. A single particle of radiation, with thousands or millions of electron volts of energy, generates many charge carrier pairs, as its energy is deposited in the semiconductor material. If the depletion layer is large enough to catch the whole shower or to stop a heavy particle, a fairly accurate measurement of the particle's energy can be made, simply by measuring the charge conducted and without the complexity of a magnetic spectrometer, etc.
These semiconductor radiation detectors need efficient and uniform charge collection and low leakage current. They are often cooled by liquid nitrogen. For longer-range (about a centimetre) particles, they need a very large depletion depth and large area. For short-range particles, they need any contact or un-depleted semiconductor on at least one surface to be very thin. The back-bias voltages are near breakdown (around a thousand volts per centimetre). Germanium and silicon are common materials. Some of these detectors sense position as well as energy.
They have a finite life, especially when detecting heavy particles, because of radiation damage. Silicon and germanium are quite different in their ability to convert gamma rays to electron showers.
Semiconductor detectors for high-energy particles are used in large numbers. Because of energy loss fluctuations, accurate measurement of the energy deposited is of less use.
Temperature measurements.
A diode can be used as a temperature measuring device, since the forward voltage drop across the diode depends on temperature, as in a silicon bandgap temperature sensor. From the Shockley ideal diode equation given above, it might "appear" that the voltage has a "positive" temperature coefficient (at a constant current), but usually the variation of the reverse saturation current term is more significant than the variation in the thermal voltage term. Most diodes therefore have a "negative" temperature coefficient, typically −2 mV/˚C for silicon diodes. The temperature coefficient is approximately constant for temperatures above about 20 kelvins. Some graphs are given for 1N400x series, and CY7 cryogenic temperature sensor.
Current steering.
Diodes will prevent currents in unintended directions. To supply power to an electrical circuit during a power failure, the circuit can draw current from a battery. An uninterruptible power supply may use diodes in this way to ensure that current is only drawn from the battery when necessary. Likewise, small boats typically have two circuits each with their own battery/batteries: one used for engine starting; one used for domestics. Normally, both are charged from a single alternator, and a heavy-duty split-charge diode is used to prevent the higher-charge battery (typically the engine battery) from discharging through the lower-charge battery when the alternator is not running.
Diodes are also used in electronic musical keyboards. To reduce the amount of wiring needed in electronic musical keyboards, these instruments often use keyboard matrix circuits. The keyboard controller scans the rows and columns to determine which note the player has pressed. The problem with matrix circuits is that, when several notes are pressed at once, the current can flow backwards through the circuit and trigger "phantom keys" that cause "ghost" notes to play. To avoid triggering unwanted notes, most keyboard matrix circuits have diodes soldered with the switch under each key of the musical keyboard. The same principle is also used for the switch matrix in solid-state pinball machines.
Waveform Clipper.
Diodes can be used to limit the positive or negative excursion of a signal to a prescribed voltage.
Clamper.
A diode clamp circuit can take a periodic alternating current signal that oscillates between positive and negative values, and vertically displace it such that either the positive, or the negative peaks occur at a prescribed level. The clamper does not restrict the peak-to-peak excursion of the signal, it moves the whole signal up or down so as to place the peaks at the reference level.
Abbreviations.
Diodes are usually referred to as "D" for diode on PCBs. Sometimes the abbreviation "CR" for "crystal rectifier" is used.

</doc>
<doc id="8256" url="http://en.wikipedia.org/wiki?curid=8256" title="Drexel University">
Drexel University

Drexel University is a private research university with three campuses in Philadelphia and one in Sacramento, California. It was founded in 1891 by Anthony J. Drexel, a noted financier and philanthropist. Drexel offers over 70 full-time undergraduate programs and accelerated degrees. At the graduate level, the university offers over 100 masters, doctoral, and professional programs, many available part-time.
Drexel is best known for the cooperative education program (co-op). Drexel's co-op is regularly ranked as one of the best co-op programs in the United States. Participating students have a variety of opportunities to gain up to 18-months of paid full-time working experience before graduation. The university has a large network of more than 1,600 corporate, governmental, and non-profit partners in 28 states and 25 international locations. The employers include top ranked multinational law firms, banks, corporations, and many Fortune 500 companies, such as Goldman Sachs, Microsoft, and Procter & Gamble.
Shanghai Jiao Tong University’s academic ranking of world universities ranks Drexel 401-500 and Times Higher Education World University Rankings placed Drexel among the top 200 universities in the World. In "U.S. News & World Report"'s annual "America's Best Colleges List", the university has been ranked consistently among the "Best National Universities – Top Schools." The 2012 rankings place Drexel third in their list of “Up and Coming National Universities” for "promising and innovative changes in the areas of academics, faculty, and student life." In addition, the National Science Foundation and the 2009 Lombardi Report also ranked Drexel among the top 50 private comprehensive research universities. Drexel University ranks #45 among "Research Universities by Salary Potential" in the United States.
History.
Drexel University was founded in 1891 as the Drexel Institute of Art, Science and Industry by Philadelphia financier and philanthropist Anthony J. Drexel to provide educational opportunities in the “practical arts and sciences” for women and men of all backgrounds. Drexel became the Drexel Institute of Technology in 1936, and in 1970 Drexel Institute of Technology gained university status, becoming Drexel University. Although there were many changes during its first century, the university's identity has been held constant as privately controlled, non-sectarian, coeducational center of higher learning, distinguished by a commitment to preparing both men and women for future success. Drexel's cornerstone of the career preparation, the cooperative education program, was introduced in 1919. The program became integral to the university's unique educational experience. Participating students alternate periods of classroom based study with periods of full-time practical work experience related to their academic and career interests.
From 1995 to 2009, the president of Drexel University, Dr. Constantine Papadakis, led the institution towards significant change. President Papadakis oversaw Drexel's largest expansion ever, the endowment increased +471% to $540M, and total enrollment increased +102% to 18,466. The institution continued to climb in the rankings, became more selective, and obtained a more academically talented student body. During the expansion, Drexel was officially united with the former MCP Hahnemann University, creating the Drexel University College of Medicine in 2002; and in the fall of 2006, Drexel established its School of Law, which was fully accredited by American Bar Association (ABA) in 2011.
In April 2009, Dr. Constantine Papadakis died of pneumonia. His successor is Mr. John Anderson Fry, formerly the president of Franklin & Marshall College and the Executive Vice President of University of Pennsylvania.
In July 2011, Drexel acquired The Academy of Natural Sciences. The agreement created an international powerhouse for discovery in the natural and environmental sciences.
Academics.
Colleges and schools.
Drexel today includes more than a dozen academic units.
The College of Computing and Informatics and the College of Arts and Sciences are two of the most prolific colleges within Drexel; and the Drexel University College of Engineering, for which Drexel is perhaps best known. The Goodwin College of Professional Studies offers working professionals and recent high school and college graduates practical educational programs with flexible scheduling, hands-on experiences, and career preparation. Full-time programs include (but are not limited to) Sport Management, Culinary Arts, and Engineering Technology, while part-time programs include Communications & Applied Technology and Computing & Security Technology.
The Bennett S. LeBow College of Business has been ranked as the 38th best private institution in the nation. The Antoinette Westphal College of Media Arts and Design houses Design and Merchandising, Graphic Design, Interior Design, Digital Media, Architecture, Fashion Design, Product Design, Photography, Visual Studies, Performing Arts, Music Industry, Entertainment & Arts Management, Film & Video, Screenwriting & Playwriting, and Dance. The Drexel University College of Medicine is a recent addition to the university. Formerly MCP Hahnemann University, it contributes two additional campuses and a teaching medical hospital, along with the College of Nursing and Health Professions and the School of Public Health. The Pennoni Honors College, named for Drexel alumnus and trustee Dr. C.R. "Chuck" Pennoni '63, '66, Hon. '92, and his wife Annette, recognizes and promotes excellence among Drexel students.
The Drexel University College of Law, now known as the Drexel University School of Law, was originally added to Drexel University as the newest school in 2006. Serving only graduate students, the law school offers Juris Doctor degrees and provides the opportunity for all students to take part in a cooperative education program.
Most popular undergraduate majors.
Drexel University is also known for creating the world's first Engineering Degree in Appropriate Technology. Drexel is also one of only 17 U.S. universities to offer a Bachelors in Architectural Engineering, and only one of five private institutions to do so.
The Drexel Engineering Curriculum (tDEC).
The 2006 edition of U.S. News ranks the undergraduate engineering program #57 in the country and the 2007 edition of graduate schools ranks the graduate program #61. The 2008 edition ranks the University Engineering Program at #55 and in the 2009 US News Ranking, the university has moved up to the #52 position.
The engineering curriculum used by the school was originally called E4 (Enhanced Educational Experience for Engineers) which was established in 1986 and funded in part by the Engineering Directorate of the National Science Foundation. In 1988 the program evolved into tDEC (the Drexel Engineering Curriculum) which is composed of two full years of rigorous core engineering courses which encompass the freshman and sophomore years of the engineering student. The College of Engineering hasn't used the tDEC curriculum since approximately 2005.
Co-op program.
Branded as "the Ultimate Internship", Drexel's longstanding cooperative-education or "co-op" program is one of the largest and oldest in the United States. Drexel has a fully internet-based job database, where students can submit résumés and request interviews with any of the thousands of companies that offer positions. Students also have the option of obtaining an internship via independent search. A student graduating from Drexel's 5-year degree program typically has a total of 18 months of internship with up to three different companies. The majority of co-ops are paid, averaging $15,912 per 6-month period, however this figure changes with major. The working experience highly pays off as one third of Drexel graduates are offered full-time positions by their co-op employers right after graduation.
Research.
Drexel's knowledge community of researchers and scholars are socially, professionally and intellectually diverse. Many of Drexel's faculty and staff are seasoned practitioners with strong academic and private sector experiences. Drexel's intellectual climate is creative, flexible and responsive to change, thereby facilitating the emergence of innovative new lines of inquiry and exploration and seeding new avenues of creative expression. Research Centers and Institutes at Drexel include:
Online education.
Drexel University launched its first Internet-based education program, a master’s degree in Library & Information Science, in 1996. In 2001, Drexel created its wholly owned, for-profit online education subsidiary, Drexel e-Learning, Inc., better known as Drexel University Online. It was announced in October 2013 that Drexel University Online would no longer be a for-profit venture, but rather become an internal division within the University to better serve its online student population. Although headquartered in Philadelphia, Drexel announced a new Washington, D.C. location in December 2012 to serve as both an academic and outreach center, catering towards the online student population.
In an effort to create greater awareness of distance learning and to recognize exceptional leaders and best practices in the field, Drexel University Online founded National Distance Learning Week, in conjunction with the United States Distance Learning Association, in 2007. In September 2010, Drexel University Online received the Sloan-C award for institution-wide excellence in online education indicating that it had exceptional programs of "demonstrably high quality" at the regional and national levels and across disciplines. Drexel University Online won the 2008 United States Distance Learning Association's Best Practices Awards for Distance Learning Programming. In 2007, the online education subsidiary had a revenue of $40 million. In March 2013, Drexel Online had more than 7,000 unique students from all 50 states and more than 20 countries pursuing a bachelor's, master's, or certificate. As of December 2013, Drexel University Online offers more than 100 fully accredited master's degrees, bachelor's degrees and certificate programs.
Rankings.
In 2010, Times Higher Education World University Rankings placed Drexel 190th in the World and 76th in North America. The university also was placed among 96-98th best universities in the world according to the Russian based Global University Ranking.
Drexel is currently in a period of the fastest rise in term of rankings. In 2013, U.S. News & World Report has ranked Drexel 97th among all universities of the United States, and third on the U.S. News & World Report "Best Colleges: Up-and-coming National Universities" 2012 ranking. The 2009 and 2010 rankings placed Drexel 88th among all universities of the United States,
48th among the best 50 private universities in the country, and 4th on the U.S. News & World Report "Up-and-coming National Universities" ranking.
The 2008 rankings placed Drexel 108th, whereas 2006 rankings had the school at 109th. Drexel and the University of Pennsylvania are the only Philadelphia colleges in this category.
In 2007, Business Week ranked the undergraduate business program among the top 30 private institutions in the country. The 2011 rankings rate the LeBow business program as the 38th best in the nation. The Princeton Review also named Drexel 6th on their list of "2010 Top Entrepreneurial Programs: Undergraduate." In 2014, Business Insider ranked Drexel's graduate business school 19th in the country for networking, one spot beneath Yale. 
The Department of Materials Science and Engineering was ranked 10th in the US in faculty scholarly productivity in 2006, and was ranked 11th out of 88 programs in the 2011 National Research Council survey rankings. Additionally, Sierra magazine, the publication of the Sierra Club, selected Drexel as one of America's "Cool Schools." Drexel was 82nd out of 135 institutions on the publication's third annual poll of "eco-enlightened" colleges and universities. To compile the list, Sierra sent questionnaires to sustainability experts at schools across the country. The survey featured categories such as efficiency, energy, food, academics, purchasing, transportation, waste management and administration.
The iSchool at Drexel, College of Information Science and Technology has been ranked among the top 10 information schools in the nation by U.S. News & World Report. Its specialties in Library and Information Science (MS), Information systems (MSIS), Medical Librarianship, and Digital Librarianship are ranked 9th, 5th, 5th and 9th respectively.
Drexel frequently ranks among the top 25 schools in the nation for technology use according to The Princeton Review and The Intel Corporation, and was ranked first in 2001 for wireless access by Yahoo!. has been selected as one of the most useful websites by PC Magazine and Scientific American. Drexel is the third largest private engineering college in the nation.
The Drexel College of Medicine and College of Nursing & Health Professions also share accolades. The Physician Assistant program is in the nation's top 50 and the Nurse Anesthesia (CRNA) program is in the top 25.
In 2014, The Princeton Review ranked Drexel 20th in its list of worst college libraries.
Campuses.
Drexel University's campus is divided into three parts: the University City Campus, the Center City Hahnemann Campus including Hahnemann University Hospital, and the Queen Lane College of Medicine Campus.
University City Main Campus.
The 77 acre University City Main Campus of Drexel University is located just west of the Schuylkill River in the University City district of Philadelphia. It is Drexel's largest and oldest campus and contains its administrative offices and the main academic center for students. The northern residential portion of the main campus is located in the Powelton Village section of West Philadelphia. The two prominent performing stages at Drexel University are the Mandell Theater and the Main Auditorium. The Main Auditorium dates back to the founding of Drexel and construction of its main hall. It features over 1000 seats, and a pipe organ installed in 1928. The organ was purchased by Saturday Evening Post publisher Cyrus H. K. Curtis after he had donated a similar organ, the Curtis Organ, to nearby University of Pennsylvania and it was suggested that he do the same for Drexel. The 424-seat Mandell Theater was built in 1973 and features a more performance-oriented stage, including a full fly system, modern stage lighting facilities, stadium seating, and accommodations for wheelchairs. It is used for the semiannual spring musical, as well as various plays and many events.
Queen Lane Campus.
The Queen Lane Medical Campus was purchased in 2003 by Drexel University as part of its acquisition of MCP Hahnemann University. It is located in East Falls in the Northwest part of Philadelphia and is primarily utilized by first- and second-year medical students. A free shuttle is available connecting it to the Center City Hahnemann and University City Main campuses.
Center City Hahnemann Campus.
The Center City Hahnemann Campus is in the middle of Philadelphia, straddling the Vine Street Expressway and centered on Hahnemann University Hospital.
The Academy of Natural Sciences.
In 2011, The Academy of Natural Sciences entered into an agreement to become a subsidiary of Drexel University. Founded in 1812, the Academy of Natural Sciences is America's oldest natural history museum and is a world leader in biodiversity and environmental research.
Drexel University Sacramento.
On January 5 2009, Drexel University opened the Center for Graduate Studies in Sacramento, California. s of 2011[ [update]], the Sacramento Center offered an Ed.D. program in Educational Leadership and Management and master degree programs in Business Administration, Finance, Higher Education, Human Resource Development, Public Health, and Interdepartmental Medical Science. Drexel awards students at its Sacramento Center fellowships from a $10 million annual budget allocation. The first cohort of students graduated from this campus in December 2010. On March 5, 2015, Drexel University announced it was closing the Sacramento campus although current students will be allowed to complete their degrees.
Student life.
Activities.
The university has a large variety of student organizations, including charity, fraternities and sororities, political, and academic groups. [what fraternities/organizations?]
Student Government.
The Undergraduate Student Government Association of Drexel University works with administrators to solve student problems and tries to promote communication between the students and the administration.
Graduate Students Association.
As stated on their website - "Graduate Student Association advocates the interests and addresses concerns of graduate students at Drexel; strives to enhance graduate student life at the University in all aspects, from academic to campus security; and provides a formal means of communication between graduate students and the University community."
Campus Activities Board.
The Campus Activities Board (CAB) is an undergraduate student run event planning organization. CAB creates events for the undergraduate population since they are funded by the student activities fee (which is collected from only undergraduate students). To clarify, CAB is not funded by all of the student activities fee from each student, but only a portion of the fee. The student activity fee is divided to fund all organizations at Drexel. CAB is broken down into 5 committees - Special Events, Traditions, Marketing, Lectures and Diversity, and Performing and Fine Arts.
Press and radio.
Radio.
WKDU is Drexel's student-run FM radio station, with membership open to all undergraduate students. Its status as an 800-watt non-commercial station in a major market city has given it a wider audience and a higher profile than many other college radio stations.
Television.
DUTV is Drexel's Philadelphia cable television station. The student operated station is part of the Paul F. Harron Studios at Drexel University. The purpose of DUTV is to provide "the people of Philadelphia with quality educational television, and providing Drexel students the opportunity to gain experience in television management and production." The Programing includes an eclectic variety of shows from a bi-monthly news show, DNews, to old films, talk shows dealing with important current issues and music appreciation shows.
Publications.
Drexel has a number of publications to its name by both the student body and the university. "The Triangle" has been the university's newspaper since 1926. The yearbook was first published in 1911 and named the Lexerd in 1913. Prior to the publishing of a campus wide yearbook in 1911 "The Hanseatic" and "The Eccentric" were both published in 1896 as class books. Other publications include "MAYA", the undergraduate student literary and artistic magazine; "D&M Magazine", Design & Merchandising students crafted magazine; "The Smart Set from Drexel University", an online magazine founded in 2005; and "The Drexelist" a blog-style news source founded in 2010.
The Drexel Publishing Group serves as a medium for literary publishing on campus. The Drexel Publishing Group oversees "ASK" (The Journal of the College of Arts and Sciences at Drexel University), "Painted Bride Quarterly", a 36-year-old national literary magazine housed at Drexel; "The 33rd", an annual anthology of student and faculty writing at Drexel; "DPG Online Magazine", and "Maya", the undergraduate literary and artistic magazine. The Drexel Publishing Group also serves as a pedagogical organization by allowing students to intern and work on its publications.
Housing.
Drexel requires all non-commuting first and second year students to live in one of its ten residence halls, and second year students to live in "university approved housing". Kelly Hall, Myers Hall, Towers Hall, and Calhoun Hall are traditional residence halls (shared bedroom, community bathrooms), while North Hall, Caneris Hall, Race Street Residence Hall, and Van Rensselaer Hall are suite style residence halls (shared bedrooms, private bathrooms, kitchens, and common area within the suite). Millennium Hall, Drexel's newest residence hall, is a modified suite (shared bedrooms, and segmented, private bathrooms in the hallway). Drexel also leases several floors of the University Crossings apartment complex for upper class students.
The Residential Living Office (RLO) at Drexel has developed a Residential Experience Engagement Model which is designed to support residents of all class levels.
Portions of the Race Street Residence Hall formerly was reserved for students of the Pennoni Honors College. However, during the 2007 spring term, the Race Street Dormitory housed Kelly Hall residents, while Kelly Hall underwent renovation. It was recently announced that for 2010-2011 the Honors Living Learning Community will be moved to Millennium Hall and the Sophomore Year Experience moved into the Race Street Residence Hall. Van Rensselaer Hall will also be utilized by the Graduate Student Experience.
All residence halls except Caneris Hall, University Crossings, and Stiles Memorial Hall are located north of Arch Street between 34th Street and 32nd Street in the Powelton Village area.
Greek life.
Twelve percent of Drexel's undergraduate population are members of a social Greek-letter organization. There are currently 13 Interfraternity Council (IFC) chapters, six Panhellenic Council (PHC) chapters and eleven Multi-cultural Greek Council (MGC) chapters.
Three IFC Chapters have been awarded Top Chapters in 2008 by their respective national organizations; Tau Kappa Epsilon, Pi Kappa Alpha, and Alpha Chi Rho. In 2013, Sigma Phi Epsilon and Alpha Epsilon Pi were awarded the Top Chapter award by their respective national headquarters.
Each year, all social fraternities and sororities at Drexel compete in Greek Week. Greek Week actually consists of two weeks, the first week consisting of events such as Penny Wars and flag football and the second week consisting of activities such as talent show, chariot races and tug-of-war. Sigma Phi Epsilon and Phi Mu have won Greek Week two years in a row.
The week after Greek Week the Dean's Cup is presented for the previous year. The Dean's Cup is the highest award for Drexel Greeks. The winners of the Dean's Cup are determined by the highest score on the Chapter Achievement Plan (CAP) which is the annual recognition process for Drexel Greeks. The Dean's Cup is reviewed by a selected committee of Student Life faculty. The Dean of Students awards the Dean's Cup, which is awarded to the top chapter in each council in the areas of academics, leadership, brother/sisterhood and service to the community.
Student Organizations.
Drexel University recognizes over 250 student organizations in the following categories:
Honorary/Professional Organizations.
The following groups are recognized as honors or professional organizations under the Office of Campus Activities and are not considered part of social Greek life at Drexel University.
Athletics.
Drexel's school mascot is a dragon known as "Mario the Magnificent," named so in honor of Mario V. Mascioli, an alumnus and former member of the Board of Trustees. The Dragon has been the mascot of the school since around the mid-1920s; the first written reference to the Dragons occurred in 1928 when the football team was called The Dragons in The Triangle. Before becoming known as the Dragons the athletic teams had been known by such names as Blue & Gold, the Engineers, and the Drexelites. The school's sports teams, now known as the Drexel Dragons, participate in the NCAA's Division I as a member of the Colonial Athletic Association. They do not currently field a varsity football team.
Drexel is home to 33 active club teams including lacrosse, water polo, squash, triathlon, and cycling. Other club teams include soccer, baseball, rugby, field hockey, and roller hockey. The club teams operate under the direction of the Club Sports Council and the Recreational Sports Office.
Fight song.
The fight song for Drexel is the "Drexel Fight Song". The lyrics are:
Fight on for Drexel,<br>
We’ve got the stuff we need to win this game.<br>
We’re gonna fight on for Drexel,<br>
Take the Dragon on to fame.<br>
Fight on for Drexel,<br>
The gold and blue is on another spree.<br>
We’re gonna fight, fight, fight, fight for Drexel U.<br>
On to victory!
D-D-D-D<br>
R-R-R-R<br>
E-E-E-E<br>
X-EL-X-EL<br>
DREX-EL-DREX-EL<br>
FIGHT-TEAM-FIGHT<br>
Student lore and traditions.
Tradition suggests that rubbing the toe of the bronze "Waterboy" statue located in the Main Building atrium can result in receiving good grades in exams. Although the rest of the bronze statue has developed a dark brown patina over the years, the toe has remained highly polished and shines like new.
The Flame of Knowledge, a fountain once located in the main quad (relocated to the area in front of North Hall in early 2007), used to be known as the "Drexel Shaft" in the late 70s and early 80s, however the name outgrew the landmark. The "Drexel Shaft" now refers to the Penn Coach Yard chimney, the large smoke stack structure which was located east of 32nd street. Unresponsive treatment by the administration has been termed the "Drexel Shaft" by students. The smoke stack was demolished on 15 November 2009, a long-anticipated event which the students hope will improve the overall aesthetics of the university.
In popular culture.
Drexel has appeared in news and television media several times. In 2006 Drexel served as the location for ABC Family's reality show "Back on Campus." Also in that year the Epsilon Zeta chapter of Delta Zeta won ABC Daytime's Summer of Fun contest. As a result the sorority was featured in national television spots for a week and also hosted an ABC party on campus which was attended by cast members from General Hospital and All My Children.
John Langdon, adjunct professor in the Antoinette Westphal College of Media Arts & Design, created the ambigram featured on the cover of Dan Brown's Angels & Demons and a number of other ambigrams were served as the central focus of the book and film. It is believed Prof. Langdon was the inspiration for the name of the lead character played by Tom Hanks in the film.
Howard Benson, a Drexel alumnus and a music producer of Hoobastank, Creed and Kelly Clarkson, teaches a music production master class at Drexel.
Drexel University was a sponsor of Matthew Quick's novel Silver Linings Playbook which was made into a movie in 2012. Matthew Quick held several lectures at Drexel University.
In 2007 Drexel was the host of the 2008 Democratic Presidential candidate debate in Philadelphia, televised by MSNBC. In 2008 from 10 January to the 13th Drexel hosted the US Table Tennis Olympic Trials. Drexel University hosted the 2011 U.S. Open Squash Championships from 1–6 October 2011 as well as the 2012 U.S. Open Squash Championships from 4–12 October 2012.
Alumni.
Since its founding the university has graduated over 100,000 alumni. Certificate-earning alumni such as artist Violet Oakley and illustrator Frank Schoonover reflect the early emphasis on art as part of the university's curriculum. With World War II, the university's technical programs swelled, and as a result Drexel graduated alumni such as Paul Baran, one of the founding fathers of the Internet and one of the inventors of the packet switching network, and Norman Joseph Woodland the inventor of barcode technology. In addition to its emphasis on technology Drexel has graduated several notable athletes such as National Basketball Association (NBA) basketball players Michael Anderson and Malik Rose, and several notable business people such as Raj Gupta, former President and Chief executive officer (CEO) of Rohm and Haas, and Kenneth C. Dahlberg, former CEO of Science Applications International Corporation (SAIC). Alassane Dramane Ouattara, president of the Ivory Coast 
In 1991, the university's centennial anniversary, Drexel created an association called the Drexel 100, for alumni who have demonstrated excellence work, philanthropy, or public service. After the creation of the association 100 alumni were inducted in 1992 and since then the induction process has been on a biennial basis. In 2006 164 total alumni had been inducted into the association.
Awards.
Drexel University created the annual $100,000 Anthony J. Drexel Exceptional Achievement Award to recognize a faculty member from a U.S. institution whose work transforms both research and the society it serves. The first recipient was bioengineer James J. Collins of Boston University (now at MIT) and the Howard Hughes Medical Institute.
In 2004, in conjunction with BAYADA Home Health Care, Drexel University's College of Nursing and Health Professions created the BAYADA Award for Technological Innovation in Nursing Education and Practice. The award honors nursing educators and practicing nurses whose innovation leads to improved patient care or improved nursing education.

</doc>
<doc id="8258" url="http://en.wikipedia.org/wiki?curid=8258" title="Daedalus">
Daedalus

In Greek mythology, Daedalus (; Ancient Greek: Δαίδαλος "Daidalos", perhaps related to δαιδάλλω "to work artfully"; Latin: "Daedalus"; Etruscan: "Taitale") was a skillful craftsman and artist. He is the father of Icarus, the uncle of Perdix and possibly also the father of Iapyx although this is unclear.
Family.
His parentage was supplied as a later addition to the "mythos", providing him with a father in Metion, Eupalamus, or Palamaon, and a mother, Alcippe, Iphinoe, or Phrasimede. Daedalus had two sons: Icarus and Iapyx, along with a nephew either Talus or Perdix.
Athenians transferred Cretan Daedalus to make him Athenian-born, the grandson of the ancient king Erechtheus, who fled to Crete, having killed his nephew, Talos. Over time, other stories were told of Daedalus.
The Labyrinth.
Daedalus is first mentioned by Homer as the creator of a wide dancing-ground for Ariadne. He also created the Labyrinth on Crete, in which the Minotaur (part man, part bull) was kept. In the story of the labyrinth as told by the Hellenes, the Athenian hero Theseus is challenged to kill the Minotaur, finding his way with the help of Ariadne's thread. Daedalus' appearance in Homer is in an extended metaphor, "plainly not Homer's invention", Robin Lane Fox observes: "he is a point of comparison and so he belongs in stories which Homer's audience already recognized". In Bronze Age Crete, an inscription "da-da-re-jo-de" has been read as referring to a place at Knossos, and a place of worship.
In Homer's language, objects which are "daidala" are finely crafted. They are mostly objects of armor, but fine bowls and furnishings are "daidala", and on one occasion so are the "bronze-working" of "clasps, twisted brooches, earrings and necklaces" made by Hephaestus while cared for in secret by the goddesses of the sea.
Ignoring Homer, later writers envisaged the Labyrinth as an edifice rather than a single dancing path to the center and out again, and gave it numberless winding passages and turns that opened into one another, seeming to have neither beginning nor end. Ovid, in his "Metamorphoses", suggests that Daedalus constructed the Labyrinth so cunningly that he himself could barely escape it after he built it. Daedalus built the labyrinth for King Minos, who needed it to imprison his wife's son the Minotaur. The story is told that Poseidon had given a white bull to Minos so that he might use it as a sacrifice. Instead, Minos kept it for himself; and in revenge, Poseidon made his wife Pasiphaë lust for the bull with the help of Aphrodite. For Pasiphaë, as Greek mythologers interpreted it, Daedalus also built a wooden cow so she could mate with the bull, for the Greeks imagined the Minoan bull of the sun to be an actual, earthly bull, the slaying of which later required a heroic effort by Theseus.
This story thus encourages others to consider the long-term consequences of their own inventions with great care, lest those inventions do more harm than good. As in the tale of Icarus' wings, Daedalus is portrayed assisting in the creation of something that has subsequent negative consequences, in this case with his creation of the monstrous Minotaur's almost impenetrable Labyrinth which made slaying the beast an endeavour of legendary difficulty.
Daedalus and Icarus.
The most familiar literary telling explaining Daedalus' wings is a late one, that of Ovid: in his "Metamorphoses" (VIII:183-235) Daedalus was shut up in a tower to prevent his knowledge of his Labyrinth from spreading to the public. He could not leave Crete by sea, as the king kept strict watch on all vessels, permitting none to sail without being carefully searched. Since Minos controlled the land and sea routes, Daedalus set to work to fabricate wings for himself and his young son Icarus. He tied feathers together, from smallest to largest so as to form an increasing surface. He secured the feathers at their midpoints with string and at their bases with wax, and gave the whole a gentle curvature like the wings of a bird. When the work was done, the artist, waving his wings, found himself buoyed upward and hung suspended, poising himself on the beaten air. He next equipped his son in the same manner, and taught him how to fly. When both were prepared for flight, Daedalus warned Icarus not to fly too high, because the heat of the sun would melt the wax, nor too low, because the sea foam would soak the feathers.
They had passed Samos, Delos and Lebynthos by the time the boy, forgetting himself, began to soar upward toward the sun. The blazing sun softened the wax which held the feathers together and they came off. Icarus quickly fell in the sea and drowned. His father cried, bitterly lamenting his own arts, and called the land near the place where Icarus fell into the ocean Icaria in memory of his child. Some time later, the goddess Athena visited Daedalus and gave him wings, telling him to fly like a god.
An early image of winged Daedalus appears on an Etruscan jug of ca 630 BC found at Cerveteri, where a winged figure captioned "Taitale" appears on one side of the vessel, paired on the other side, uniquely, with "Metaia", Medea: "its linking of these two mythical figures is unparalleled," Robin Lane Fox observes: "The link was probably based on their wondrous, miraculous art. Magically, Daedalus could fly, and magically Medea was able to rejuvenate the old (the scene on the jug seems to show her doing just this)". The image of Daedalus demonstrates that he was already well known in the West.
Sicily.
Further to the west, Daedalus arrived safely in Sicily, in the care of King Cocalus of Kamikos on the island's south coast; there Daedalus built a temple to Apollo, and hung up his wings, an offering to the god. In an invention of Virgil ("Aeneid" VI), Daedalus flies to Cumae and founds his temple there, rather than in Sicily; long afterwards Aeneas confronts the sculpted golden doors of the temple.
Minos, meanwhile, searched for Daedalus by travelling from city to city asking a riddle. He presented a spiral seashell and asked for a string to be run through it. When he reached Kamikos, King Cocalus, knowing Daedalus would be able to solve the riddle, privately fetched the old man to him. He tied the string to an ant which, lured by a drop of honey at one end, walked through the seashell stringing it all the way through. Minos then knew Daedalus was in the court of King Cocalus and demanded he be handed over. Cocalus managed to convince Minos to take a bath first, where Cocalus' daughters killed Minos. In some versions, Daedalus himself poured boiling water on Minos and killed him.
The anecdotes are literary, and late; however, in the founding tales of the Greek colony of Gela, founded in the 680s on the southwest coast of Sicily, a tradition was preserved that the Greeks had seized cult images wrought by Daedalus from their local predecessors, the Sicani.
Daedalus and Perdix.
Daedalus was so proud of his achievements that he could not bear the idea of a rival. His sister had placed her son, named variously as Perdix, Talus, or Calos, under his charge to be taught the mechanical arts. He was an apt scholar and showed striking evidence of ingenuity. Walking on the seashore, he picked up the spine of a fish. According to Ovid, imitating it, he took a piece of iron and notched it on the edge, and thus invented the saw. He put two pieces of iron together, connecting them at one end with a rivet, and sharpening the other ends, and made a pair of compasses. Daedalus was so envious of his nephew's accomplishments that he took an opportunity and caused him to fall from the Acropolis. Athena turned Perdix into a partridge and left a scar that looks like a partridge on Daedalus' right shoulder and Daedalus left Athens due to this.
Innovator.
Such anecdotal details as these were embroideries upon the reputation of Daedalus as an innovator in many arts. In Pliny's Natural History (7.198) he is credited with inventing carpentry "and with it the saw, axe, plumb-line, drill, glue, and isinglass". Pausanias, in travelling around Greece, attributed to Daedalus numerous archaic wooden cult figures (see "xoana") that impressed him: "All the works of this artist, though somewhat uncouth to look at, nevertheless have a touch of the divine in them."
It is said he first conceived masts and sails for ships for the navy of Minos. He is said to have carved statues so well they looked as if alive; even possessing self-motion. They would have escaped if not for the chain that bound them to the wall.
Daedalus gave his name, eponymously, to any Greek artificer and to many Greek contraptions that represented dextrous skill. At Plataea there was a festival, the Daedala, in which a temporary wooden altar was fashioned, and an effigy was made from an oak-tree and dressed in bridal attire. It was carried in a cart with a woman who acted as bridesmaid. The image was called "Daedale" and the archaic ritual given an explanation through a myth to the purpose
In the period of Romanticism, Daedalus came to denote the classic artist, a skilled mature craftsman, while Icarus symbolized the romantic artist, whose impetuous, passionate and rebellious nature, as well as his defiance of formal aesthetic and social conventions, may ultimately prove to be self-destructive. Stephen Dedalus, in Joyce's "Portrait of the Artist as a Young Man" envisages his future artist-self "a winged form flying above the waves [...] a hawk-like man flying sunward above the sea, a prophecy of the end he had been born to serve”.

</doc>
<doc id="8259" url="http://en.wikipedia.org/wiki?curid=8259" title="Deception Pass">
Deception Pass

Deception Pass is a strait separating Whidbey Island from Fidalgo Island, in the northwest part of the U.S. state of Washington. It connects Skagit Bay, part of Puget Sound, with the Strait of Juan de Fuca. A pair of bridges known collectively as Deception Pass Bridge cross Deception Pass, and the bridges are on the National Register of Historic Places.
History.
The Deception Pass area has been home to various Coast Salish tribes for thousands of years. The first Europeans to see Deception Pass were members of the 1790 expedition of Manuel Quimper on the "Princesa Real". The Spanish gave it the name "Boca de Flon". A group of sailors led by Joseph Whidbey, part of the Vancouver Expedition, found and mapped Deception Pass on June 7, 1792. George Vancouver gave it the name "Deception" because it had misled him into thinking Whidbey Island was a peninsula. The "deception" was heightened due to Whidbey's failure to find the strait at first. In May 1792, Vancouver was anchored near the southern end of Whidbey Island. He sent Joseph Whidbey to explore the waters east of Whidbey Island, now known as Saratoga Passage, using small boats. Whidbey reached the northern end of Saratoga Passage and explored eastward into Skagit Bay, which is shallow and difficult to navigate. He returned south to rejoin Vancouver without having found Deception Pass. It appeared that Skagit Bay was a dead-end and that Whidbey Island and Fidalgo Island were a long peninsula attached to the mainland. In June the expedition sailed north along the west coast of Whidbey Island. Vancouver sent Joseph Whidbey to explore inlets leading to the east. The first inlet turned out to be a "very narrow and intricate channel, which...abounded with rocks above and beneath the surface of the water". This channel led to Skagit Bay, thus separating Whidbey Island from the mainland. Vancouver apparently felt he and Joseph Whidbey had been deceived by the tricky strait. Vancouver wrote of Whidbey's efforts: "This determined [the shore they had been exploring] to be an island, which, in consequence of Mr. Whidbey’s circumnavigation, I distinguished by the name of Whidbey’s Island: and this northern pass, leading into [Skagit Bay], Deception Passage".
In the waters of Deception Pass, just east of the present-day Deception Pass Bridge, is a small island known as Ben Ure Island. The island became infamous for its activity of smuggling illegal Chinese immigrants for local labor. Ure and his partner Lawrence "Pirate" Kelly were quite profitable at their smuggling business and played hide-and-seek with the United States Customs Department for years. Ure's own operation at Deception Pass in the late 1880s consisted of Ure and his Native-American wife. Local tradition has it that his wife would camp on the nearby Strawberry Island (which was visible from the open sea) and signal him with a fire on the island's summit to alert him to whether or not it was safe to bring his illegal cargo ashore. For transport, Ure would tie the illegal immigrants up in burlap bags so that if customs agents were to approach then he could easily toss the bags overboard. The tidal currents would carry the discarded immigrants' bodies to San Juan Island to the north and west of the pass and many ended up in what became known as Dead Man's Bay.
Between the years 1910 and 1914, a prison rock quarry was operated on the Fidalgo Island side of the pass. Nearby barracks housed some 40 prisoners, members of an honors program out of Walla Walla State Penitentiary and the prison population was made up of several types of prisoners, including those convicted of murder. Guards stood watch at the quarry as the prisoners cut the rock into gravel and loaded it onto barges located at the base of the cliff atop the pass's waters. The quarried rock was then taken by barge to the Seattle waterfront. The camp was dismantled in 1924 and although abandoned as a quarry, the remains of the camp can still be found. The location, however, is hazardous and over the years there have been several fatal accidents when visitors have ventured onto the steep cliffs.
Upon completion on July 31, 1935, the 976 ft span Deception Pass Bridge connected Whidbey Island to the tiny Pass Island, and Pass Island to Fidalgo Island. Prior to the bridge, travellers and businessmen would use an inter-island ferry to commute between Fidalgo and Whidbey islands. 
Currents.
Deception Pass is a dramatic seascape where the tidal flow and whirlpools beneath the twin bridges connecting Fidalgo Island to Whidbey Island move quickly. During ebb and flood tide current speed reaches about 8 kn, flowing in opposite directions between ebb and flood. This swift current can lead to standing waves, large whirlpools, and roiling eddies. This swift current phenomenon can be viewed from the twin bridges' pedestrian walkways or from the trail leading below the larger south bridge from the parking lot on the Whidbey Island side. Boats can be seen waiting on either side of the pass for the current to stop or change direction before going through. Thrill-seeking kayakers go there during large tide changes to surf the standing waves and brave the class 2 and 3 rapid conditions.
Scuba Diving.
Diving Deception Pass is dangerous and only for the most competent and prepared divers. There are a few times each year that the tides are right for a drift dive from the cove, under the bridge, and back to the cove as the tide changes. These must be planned well in advance by divers who know how to read currents and are aware of the dangerous conditions. However, because of the large tidal exchange, Deception Pass hosts some of the most spectacular colors and life in the Pacific Northwest. The walls and bottom are covered in colorful invertebrates, lingcod, greenlings, and barnacles everywhere.
Recreation.
Deception Pass is today surrounded by Deception Pass State Park, the most-visited park in Washington with over 2 million visitors each year. The park was officially established in 1923, when the original 1600 acre of a military reserve was transferred to Washington State Parks. The park's facilities were greatly enhanced in the 1930s when the Civilian Conservation Corps (CCC) built roads, trails, and buildings in order to develop the park.
The road to West Beach was created in 1950, opening up a stretch of beach to hordes of vehicles. The former fish hatchery at Bowman Bay became a part of the park in the early 1970s. The old entrance to the park was closed in 1997 when a new entrance was created at the intersection of Highway 20 and Cornet Bay road, improving access into and out of the park.
Deception Pass State Park has a number of recreational opportunities, including three campgrounds, several hiking trails, beaches, and tidepools. Several miles of the Pacific Northwest Trail are within the park, most notably including the section that crosses Deception Pass on the Highway 20 bridge. In addition, the Cornet Bay Retreat Center provides cabins and dining and recreation facilities. Cornet Bay offers boat launches and fishing opportunities, while Bowman Bay has an interpretive center that explains the story of the Civilian Conservation Corps throughout Washington state. Near the center is a CCC honor statue, which can be found in 30 different states in the country. Fishing is popular in Pass Lake, on the north side of the bridge. Boat rentals and guided tours of the park are also offered.
Included in the park are ten islands: Northwest Island, Deception Island, Pass Island, Strawberry, Ben Ure, Kiket, Skagit, Hope, and Big and Little Deadman Islands. Ben Ure Island is partially privately owned. The island is not open to the public except for a small rentable cabin available via the state park, which is only accessible by rowboat.
Looking roughly west from near the south span of the Deception Pass Bridge. Whidbey Island at left, Fidalgo Island at right, with the small Deception Island visible at center.
In popular culture.
The 2002 horror movie "The Ring" was in part filmed near the pass. 
The bridge is referenced and (incorrectly) referred to as the "Desolation Bridge" in season one of The Killing. It is also inaccurately portrayed as a toll bridge.
Seattle shoegaze act The Sight Below filmed the 2008 video for their track "Further Away" at Deception Pass, with Deception Island's scenic imagery prominently featured.
Seattle grunge band Mudhoney named a song on their 1993 EP Five Dollar Bob's Mock Cooter Stew "Deception Pass."
Seattle progressive rock band Queensrÿche filmed scenes of their video Anybody Listening near Deception Pass, and Deception Island. 

</doc>
<doc id="8262" url="http://en.wikipedia.org/wiki?curid=8262" title="Dominoes">
Dominoes

Dominoes (or dominos) is a game played with rectangular "domino" tiles. The domino gaming pieces make up a "domino set", sometimes called a "deck" or "pack". The traditional Sino-European domino set consists of 28 dominoes, colloquially nicknamed "bones", "cards", "tiles", "tickets", "stones", or "spinners". Each domino is a rectangular tile with a line dividing its "face" into two square "ends". Each end is marked with a number of "spots" (also called "pips, nips, or dobs") or is "blank". The backs of the dominoes in a set are indistinguishable, either blank or having some common design. A domino set is a generic gaming device, similar to playing cards or dice, in that a variety of games can be played with a set.
The earliest mention of dominoes is from Song Dynasty China, found in the text "Former Events in Wulin". Dominoes first appeared in Italy during the 18th century, and although it is unknown how Chinese dominoes developed into the modern game, it is speculated that Italian missionaries in China may have brought the game to Europe.
The name "domino" is from the resemblance to a kind of hood worn during the Venice carnival.
Construction and composition of domino sets.
European-style dominoes are traditionally made of bone such as ivory, or a dark hardwood such as ebony, with contrasting black or white pips (inlaid or painted). Alternatively, domino sets have been made from many different natural materials: stone (e.g., marble, granite or soapstone); other hardwoods (e.g., ash, oak, redwood and cedar); metals (e.g., brass or pewter); ceramic clay, or even frosted glass or crystal. These sets have a more novel look, and the often heavier weight makes them feel more substantial, but such materials and the resulting products are usually much more expensive than polymer materials. 
Modern commercial domino sets are usually made of synthetic materials, such as ABS or polystyrene plastics, or Bakelite and other phenolic resins; many sets approximate the look and feel of ivory while others use colored or even translucent plastics to achieve a more contemporary look. Modern sets also commonly use a different color for the dots of each different end value (one-spots might have black pips while two-spots might be green, three red, etc.) to facilitate finding matching ends. Occasionally, one may find a domino set made of card stock like that for playing cards. Such sets are lightweight, compact and inexpensive, but like cards are more susceptible to minor disturbances such as a sudden breeze. Sometimes dominoes have a metal pin (called a "spinner" or "pivot") in the middle.
The traditional set of dominoes contains one unique piece for each possible combination of two ends with zero to six spots, and is known as a "double-six" set because the highest-value piece has six pips on each end (the "double six"). The spots from one to six are generally arranged as they are on six-sided dice, but because there are also blank ends having no spots there are seven possible faces, allowing 28 unique pieces in a double-six set.
However, this is a relatively small number especially when playing with more than four people, so many domino sets are "extended" by introducing ends with greater numbers of spots, which increases the number of unique combinations of ends and thus of pieces. Each progressively larger set increases the maximum number of pips on an end by three, so the common extended sets are "double-nine", "double-twelve", "double-fifteen" and "double-eighteen". Larger sets such as "double-twenty-one" can theoretically exist but are rarely seen in retail stores, as identifying the number of pips on each domino becomes difficult, and a double-twenty-one set would have 253 pieces, far more than is normally necessary for most domino games even with eight players.
History.
The oldest confirmed written mention of dominoes in China comes from the "Former Events in Wulin" (i.e. the capital Hangzhou) written by the Yuan Dynasty (1271–1368) author Zhou Mi (1232–1298), who listed "pupai" (gambling plaques or dominoes) as well as dice as items sold by peddlers during the reign of Emperor Xiaozong of Song (r. 1162–1189). Andrew Lo asserts that Zhou Mi meant dominoes when referring to "pupai", since the Ming author Lu Rong (1436–1494) explicitly defined "pupai" as dominoes (in regards to a story of a suitor who won a maiden's hand by drawing out four winning "pupai" from a set).
The earliest known manual written about dominoes is the "《宣和牌譜》(Manual of the Xuanhe Period)" written by Qu You (1341–1437). But some Chinese scholars believe this manual is a forgery from a later time.
In the "Encyclopedia of a Myriad of Treasures", Zhang Pu (1602–1641) described the game of laying out dominoes as "pupai", although the character for "pu" had changed, yet retained the same pronunciation. Traditional Chinese domino games include Tien Gow, Pai Gow, Che Deng, and others. The thirty-two-piece Chinese domino set, made to represent each possible face of two thrown dice and thus have no blank faces, differs from the twenty-eight-piece domino set found in the West during the mid 18th century.
Many different domino sets have been used for centuries in various parts of the world to play a variety of domino games. Each domino originally represented one of the 21 results of throwing two 6-sided dice (2d6). One half of each domino is set with the pips from one die and the other half contains the pips from the second die. Chinese sets also introduce duplicates of some throws and divide the dominoes into two classes: military and civil. Chinese dominoes are also longer than typical European dominoes.
The early 18th century witnessed dominoes making their way to Europe, making their first appearance in Italy. The game changed somewhat in the translation from Chinese to the European culture. European domino sets contain neither class distinctions nor the duplicates that went with them. Instead, European sets contain seven additional dominoes, with six of these representing the values that result from throwing a single die with the other half of the tile left blank, and the seventh domino representing the blank-blank (0–0) combination.
Ivory Dominoes were routinely used in 19th century rural England in the settling of disputes over traditional grazing boundaries, and were commonly referred to as "bonesticks" (see Hartley, Land Law in West Lancashire in the mid- 19th Century, Farm Gazette, March 1984).
Tiles and suits.
Domino tiles, also known as "bones", are twice as long as they are wide and usually have a line in the middle dividing them into two squares. The value of either side is the number of spots or "pips". In the most common variant (Double Six) the values range from "blank" or 0 (no pips) to 6. The sum of the two values, i.e. the total number of pips, may be referred to as the "rank" or "weight" of a tile, and a tile with more pips may be called "heavier" than a "lighter" tile with fewer pips.
Tiles are generally named after their two values; e.g. "2–5" or "5–2" are alternative ways of describing the tile with the values 2 and 5. Tiles that have the same value on both ends are called "doubles", and are typically referred to as double-zero, double-one etc. Tiles with two different values are called "singles".
Every tile belongs to the two "suits" of its two values, e.g. "0–3" belongs both to the "blank suit" (or 0 suit) and to the 3 suit. Naturally the doubles form an exception in that each double belongs to only one suit. In 42, the doubles are treated as an additional "suit of doubles", so that, e.g., the double-six "6–6" belongs both to the 6 suit and the suit of doubles.
The most common domino sets commercially available are Double Six (with 28 tiles) and Double Nine (with 55 tiles). Larger sets exist and are popular for games involving several players or for players looking for long domino games. The number of tiles in a set has the formula formula_1 for a double-formula_2 set.
Rules.
The most popular type of play are "layout games", which fall into two main categories, "blocking games" and "scoring games".
Blocking game.
The most basic domino variant is for two players and requires a double six set. The 28 tiles are shuffled face down and form the stock or boneyard. Each player draws seven tiles; the remainder are not used. Once the players begin drawing tiles, they are typically placed on-edge before the players, so that each player can see his own tiles, but none can see the value of other players' tiles. Every player can thus see how many tiles remain in the other players' hands at all times during gameplay.
One player begins by downing (playing the first tile) one of their tiles. This tile starts the line of play, a series of tiles in which adjacent tiles touch with matching, i.e. equal, values. The players alternately extend the line of play with one tile at one of its two ends. The game ends when one player wins by playing their last tile, or when the game is blocked because neither player can play. If that occurs, whoever caused the block gets all of the remaining player points not counting their own.
Scoring game.
Players accrue points during game play for certain configurations, moves, or emptying one's hand. Most scoring games use variations of the draw game. If a player does not call "domino" before the tile is laid on the table, and another player says 'domino' after the tile is laid, the first player must pick up an extra domino.
In "Mexican train" the double zero domino is scored as 7 points.
Draw game.
In a draw game (blocking or scoring), players are additionally allowed to draw as many tiles as desired from the stock before playing a tile, and they are not allowed to pass before the stock is (nearly) empty. The score of a game is the number of pips in the losing player's hand plus the number of pips in the stock. Most rules prescribe that two tiles need to remain in the stock. The Draw game is often referred to as simply "dominoes".
Adaptations of both games can accommodate more than two players, who may play individually or in teams.
Line of play.
The line of play is the configuration of played tiles on the table. It starts with a single tile and typically grows in two opposite directions when players add matching tiles. In practice, players often play tiles at right angles when the line of play gets too close to the edge of the table.
The rules for the line of play often differ from one variant to another. In many rules, the doubles serve as spinners, i.e., they can be played on all four sides, causing the line of play to branch. Sometimes the first tile is required to be a double, which serves as the only spinner. In some games such as Chicken Foot, all sides of a spinner must be occupied before anybody is allowed to play elsewhere. Matador has unusual rules for matching. Bendomino uses curved tiles, so that one side of the line of play (or both) may be blocked for geometrical reasons.
In Mexican Train and other train games, the game starts with a spinner from which various trains branch off. Most trains are owned by a player and in most situations players are allowed to extend only their own train.
Scoring.
In "blocking games", scoring happens at the end of the game. After a player has emptied his hand, thereby winning the game for their team, the score consists of the total pip count of the losing teams' hands. In some rules, the pip count of the remaining stock is added. If a game is blocked because no player can move, the winner is often determined by adding the pips in players' hands.
In "scoring games", each individual can potentially add to the score. For example, in Bergen, players score 2 points whenever they cause a configuration in which both open ends have the same value and 3 points if additionally one open end is formed by a double. In Muggins, players score by ensuring the total pip count of the open ends is a multiple of a certain number. In variants of Muggins, the line of play may branch due to spinners.
In British public houses and social clubs, a scoring version of "5s-and-3s" is used. The game is normally played in pairs (two against two) and is played as a series of "ends". In each "end", the objective is for players to attach a domino from their hand to one end of those already played so that the sum of the end dominoes is divisible by 5 or 3. One point is scored for each time 5 or 3 can be divided into the sum of the two dominoes i.e. four at one end and 5 at the other makes 9, which is divisible by 3 three times, resulting in 3 points. Double 5 at one end and 5 at the other makes 15 which is divisible by 3 five times (5 points) and divisible by 5 three times (3 points) for a total of 8 points.
An "end" stops when one of the players is "out", i.e., has played all of his dominoes. In the event no player is able to empty his hand, then the player with the lowest domino left in hand is deemed to be 'out' and scores one point. A game consists of any number of "ends" with points scored in the "ends" accumulating towards a total. The game ends when one of the pairs' total score exceeds a set number of points. A running total score is often is kept on a cribbage board. 5s-and-3s is played in a number of competitive leagues in the British Isles.
Variations and game play.
In many versions of the game, the player with the highest double-leads with that double, for example "double-six". If no one has it, the next-highest double is called: "double-five?", then "double-four?", etc. until the highest double in any of the players hands is played. If no player has an "opening" double, the next heaviest domino in the highest suit is called - "six-five?", "six-four?". In some variants, players take turns picking dominoes from the stock until an opening double is picked and played. In other variants, the hand is reshuffled and each player picks seven dominoes. After the first hand, the winner (or winning team) of the previous hand is allowed to pick first and begins by playing any domino in his or her hand.
Playing the first bone of a hand is sometimes called "setting", "leading", "downing", or "posing" the first bone. Dominoes aficionados often call this procedure "smacking down the bone". After each hand, bones are shuffled and each player draws the number of bones required, normally seven. Play proceeds clockwise. Players in turn must play a bone with an end that matches one of the open ends of the layouts.
In some versions of the games, the pips or points on the end, and the section to be played next to it must add up to a given number. For example, in a double six set the "sum" would be six, requiring a "blank" to be played next to a "6," a "1" next to a "5", a "2" next to a "4", etc.
The stock of bones left behind, if any, is called the "bone yard", and the bones therein are said to be "sleeping". In draw games, players take part in the "bone selection", typically drawing from the bone yard when they don't have a "match" in their hand.
If a player inadvertently picks up and sees one or more extra dominoes, those dominoes becomes part of his or her hand.
A player who can play a tile may be allowed to pass anyway. Passing can be signalled by tapping twice on the table or by saying "go" or "pass".
Play continues until one of the players has played all the dominoes in his or her hand, calls "Out!", "I win", or "Domino!" and wins the hand, or until all players are blocked and no legal plays remain. This is sometimes referred to as "lock down" or "sewed up". In a common version of the game, the next player after the block picks up all the dominoes in the bone yard as if trying to find a (non-existent) match. If all the players are blocked, or "locked out" the player with the lowest hand (pip count) wins. In team play, the team with the lowest individual hand wins. In the case of a tie, the first of tied players or the first "team" in the play rotation wins.
In games where points accrue, the winning player scores a point for each pip on each bone still held by each opponent or the opposing team. If no player went out, the win is determined by the lightest hand, sometimes only the excess points held by opponents.
A game is generally played to 100 points, the tally being kept with paper and pencil. In more common games, mainly urban rules, games are played to 150, 200, or 250 points.
In some games the tally is kept by creating houses, where the beginning of the house (the first ten points) is a large +, the next ten points are O, and scoring with a 5 is a /, and are placed in the four 'corners' of the house.
In some versions, if a "lock down" occurs, the first person to call a lock-down gains the other players bones and adds the amount of the pips to their house. If a person who calls "rocks" after a call of 'lock-down' or 'domino' finds the number of pips a player called is incorrect, those points become his.
Games using more dominoes.
With bigger domino sets, especially with the Double Fifteens and Double Eighteens, it is possible to have more players.
Card games using domino sets.
Apart from the usual blocking and scoring games, there are also domino games of a very different character, such as solitaire or trick-taking games. Most of these are adaptations of card games and were once popular in certain areas to circumvent religious proscriptions against playing cards.
A very simple example is a Concentration variant played with a double-six set; two tiles are considered to match if their total pip count is 12.
A popular domino game in Texas is 42. The game is similar to the card game spades. It is played with four players paired into teams. Each player draws seven dominoes, and the dominoes are played into tricks. Each trick counts as 1 point, and any domino with a multiple of 5 dots counts toward the total of the hand. 35 points of "five count" + 7 tricks = 42 points, hence the name.
Competitive play.
Dominoes is played at a professional level, similar to poker. Numerous organisations and clubs of amateur domino players exist around the world. Some organizations, including the "Fédération Internationale de Domino (FIDO)", organize international competitions. The 2008 and 2009 Double FIDO domino world champion from the UK is Darren Elhindi.
Other uses of dominoes.
Besides playing games, another use of dominoes is the domino show, which involves standing them on end in long lines so that when the first tile is toppled, it topples the second, which topples the third, etc., resulting in all of the tiles falling. By analogy, the phenomenon of small events causing similar events leading to eventual catastrophe is called the domino effect.
Arrangements of millions of tiles have been made that have taken many minutes, even hours to fall. For large and elaborate arrangements, special blockages (also known as firebreaks) are employed at regular distances to prevent a premature toppling from undoing more than a section of the dominoes while still being able to be removed without damage.
The phenomenon also has some theoretical relevance (amplifier, digital signal, information processing), and this amounts to the theoretical possibility of building domino computers. Dominoes are also commonly used as components in Rube Goldberg machines.
The Netherlands has hosted an annual domino-toppling exhibition called Domino Day since 1986. The event held on 18 November 2005 knocked over 4 million dominoes by a team from Weijers Domino Productions. On Domino Day 2008 (14 November 2008), the Weijers Domino Productions team attempted to set 10 records:
This record attempt was held in the WTC Expo hall in Leeuwarden. The artist who toppled the first stone was the Finnish acrobat Salima Peippo.
At one time, Pressman Toys manufactured a product called Domino Rally that contained tiles and mechanical devices for setting up toppling exhibits.
In Berlin on 9 November 2009, giant dominoes were toppled in a 20th anniversary commemoration of the fall of the Berlin Wall. Former Polish president and Solidarity leader Lech Wałęsa set the toppling in motion.
Dominoes in Unicode.
Since April 2008, the character encoding standard Unicode includes characters that represent the double-six domino tiles in various orientations. All combinations of 0 through 6 pips on the left or right provides 7x7 or 49 glyphs, the same combinations vertically for another 49, and also a horizontal and a vertical "back" for a total of 100 glyphs. In this arrangement, both orientations are present: horizontally both tiles [1|6] and [6|1] exist, while a regular game set only has one such tile. The Unicode range for dominoes is U+1F030–U+1F09F. The naming pattern in Unicode is, by example, . Few fonts are known to support these glyphs.

</doc>
<doc id="8263" url="http://en.wikipedia.org/wiki?curid=8263" title="Dissociation constant">
Dissociation constant

In chemistry, biochemistry, and pharmacology, a dissociation constant (formula_1 ) is a specific type of equilibrium constant that measures the propensity of a larger object to separate (dissociate) reversibly into smaller components, as when a complex falls apart into its component molecules, or when a salt splits up into its component ions. The dissociation constant is the inverse of the association constant. In the special case of salts, the dissociation constant can also be called an ionization constant.
For a general reaction:
in which a complex formula_3 breaks down into "x" A 
subunits and "y" B subunits, the dissociation constant is defined
where [A], [B], and [AxBy] are the concentrations of A, B, and the 
complex AxBy, respectively.
One reason for the popularity of the dissociation constant in biochemistry and pharmacology is that in the frequently encountered case where x=y=1, Kd has a simple physical interpretation: when [A]=Kd, [B]=[AB] or equivalently [AB]/([B]+[AB])=1/2. That is, Kd, which has the dimensions of concentration, equals the concentration of free A at which half of the total molecules of B are associated with A. This simple interpretation does not apply for higher values of x or y. It also presumes the absence of competing reactions, though the derivation can be extended to explicitly allow for and describe competitive binding. It is useful as a quick description of the binding of a substance, in the same way that EC50 and IC50 describe the biological activities of substances.
Protein-ligand binding.
The dissociation constant is commonly used to describe the affinity between a ligand (formula_5) (such as a drug) and a protein (formula_6) i.e. how tightly a ligand binds to a particular protein. Ligand-protein affinities are influenced by non-covalent intermolecular interactions between the two molecules such as hydrogen bonding, electrostatic interactions, hydrophobic and Van der Waals forces. They can also be affected by high concentrations of other macromolecules, which causes macromolecular crowding.
The formation of a ligand-protein complex (formula_7) can be described by a two-state process
the corresponding dissociation constant is defined
where [formula_6], [formula_5] and [formula_7] represent molar concentrations of the protein, ligand and complex, respectively.
The dissociation constant has molar units (M), which correspond to the concentration of ligand [formula_5] at which the binding site on a particular protein is half occupied, i.e. the concentration of ligand, at which the concentration of protein with ligand bound [formula_7], equals the concentration of protein with no ligand bound [formula_6]. The smaller the dissociation constant, the more tightly bound the ligand is, or the higher the affinity between ligand and protein. For example, a ligand with a nanomolar (nM) dissociation constant binds more tightly to a particular protein than a ligand with a micromolar (formula_16M) dissociation constant.
Sub-picomolar dissociation constants as a result of non-covalent binding interactions between two molecules are rare. Nevertheless, there are some important exceptions. Biotin and avidin bind with a dissociation constant of roughly formula_17 M = 1 fM = 0.000001 nM.
Ribonuclease inhibitor proteins may also bind to ribonuclease with a similar formula_17 M affinity. 
The dissociation constant for a particular ligand-protein interaction can change significantly with solution conditions (e.g. temperature, pH and salt concentration). The effect of different solution conditions is to effectively modify the strength of any intermolecular interactions holding a particular ligand-protein complex together.
Drugs can produce harmful side effects through interactions with proteins for which they were not meant to or designed to interact. Therefore much pharmaceutical research is aimed at designing drugs that bind to only their target proteins (Negative Design) with high affinity (typically 0.1-10 nM) or at improving the affinity between a particular drug and its "in-vivo" protein target (Positive Design).
Antibodies.
In the specific case of antibodies (Ab) binding to antigen (Ag), usually the affinity constant is used. It is the inverted dissociation constant.
This chemical equilibrium is also the ratio of the on-rate (kforward) and off-rate (kback) constants. Two antibodies can have the same affinity, but one may have both a high on- and off-rate constant, while the other may have both a low on- and off-rate constant.
Acid–base reactions.
For the deprotonation of acids, "K" is known as "K"a, the acid dissociation constant. Stronger acids, for example sulfuric or phosphoric acid, have larger dissociation constants; weaker acids, like acetic acid, have smaller dissociation constants.
Acid dissociation constants are sometimes expressed by pformula_22, which is defined as:
This formula_25 notation is seen in other contexts as well; it is mainly used for covalent dissociations (i.e., reactions in which chemical bonds are made or broken) since such dissociation constants can vary greatly.
A molecule can have several acid dissociation constants. In this regard, that is depending on the number of the protons they can give up, we define "monoprotic", "diprotic" and "triprotic" acids. The first (e.g. acetic acid or ammonium) have only one dissociable group, the second (carbonic acid, bicarbonate, glycine) have two dissociable groups and the third (e.g. phosphoric acid) have three dissociable groups. In the case of multiple p"K" values they are designated by indices: p"K"1, p"K"2, p"K"3 and so on. For amino acids, the p"K"1 constant refers to its carboxyl (-COOH) group, p"K"2 refers to its amino (-NH3) group and the p"K"3 is the p"K" value of its side chain.
formula_26
formula_27
formula_28
Dissociation constant of water.
The dissociation constant of water is denoted "K"w:
formula_29
The concentration of water formula_30 is omitted by convention, which means that the value of "K"w differs from the value of "K"eq that would be computed using that concentration.
The value of "K"w varies with temperature, as shown in the table below. This variation must be taken into account when making precise measurements of quantities such as pH.

</doc>
<doc id="8267" url="http://en.wikipedia.org/wiki?curid=8267" title="Dimensional analysis">
Dimensional analysis

In engineering and science, dimensional analysis is the analysis of the relationships between different physical quantities by identifying their fundamental dimensions (such as length, mass, time, and electric charge) and units of measure (such as miles vs. kilometers, or pounds vs. kilograms vs. grams) and tracking these dimensions as calculations or comparisons are performed. Converting from one dimensional unit to another is often somewhat complex. Dimensional analysis, or more specifically the factor-label method, also known as the unit-factor method, is a widely used technique for performing such conversions using the rules of algebra.
Any physically meaningful equation (and any inequality and inequation) must have the same dimensions on the left and right sides. Checking this is a common application of performing dimensional analysis. Dimensional analysis is also routinely used as a check on the plausibility of derived equations and computations. It is generally used to categorize types of physical quantities and units based on their relationship to or dependence on other units.
Concrete numbers and base units.
Many parameters and measurements in the physical sciences and engineering are expressed as a concrete number – a numerical quantity and a corresponding dimensional unit. Often a quantity is expressed in terms of several other quantities; for example, speed is a combination of length and time, e.g. 60 miles per hour or 1.4 km per second. Compound relations with "per" are expressed with division, e.g. 60 mi/1 h. Other relations can involve multiplication (often shown with · or juxtaposition), powers (like m2 for square meters), or combinations thereof.
Sometimes the names of units obscure their compound nature. For example, an ampere is a measure of electrical current, which is equivalent to electrical charge per unit time and is measured in coulombs (a unit of electrical charge) per second, so 1 A = 1 C/s. One newton is 1 kg·m/s2.
A unit of measure that is not compound – it is already a base unit – is known as a base unit. For example, units for length and time are normally considered base units, since in most systems of units they are defined as such. Units for volume, however, can be factored into the base units of length (m3), thus being derived or compound units.
Percentages and derivatives.
Percentages are dimensionless quantities, since they are ratios of two quantities with the same dimensions. In other words, the % sign can be read as "1/100", since 1% = 1/100.
Derivatives with respect to a quantity add the dimensions of the variable one is differentiating with respect to on the denominator. Thus:
In economics, one distinguishes between stocks and flows: a stock has units of "units" (say, widgets or dollars), while a flow is a derivative of a stock, and has units of "units/time" (say, dollars/year).
In some contexts, dimensional quantities are expressed as dimensionless quantities or percentages by omitting some dimensions. For example, debt-to-GDP ratios are generally expressed as percentages: total debt outstanding (dimension of Currency) divided by annual GDP (dimension of Currency) – but one may argue that in comparing a stock to a flow, annual GDP should have dimensions of Currency/Time (Dollars/Year, for instance), and thus Debt to GDP should have units of years.
Conversion factor.
In dimensional analysis, a ratio which converts one unit of measure into another without changing the quantity is called a conversion factor. For example, kPa and bar are both units of pressure, and 100 kPa = 1 bar. The rules of algebra allow both sides of an equation to be divided by the same expression, so this is equivalent to 100 kPa/1 bar = 1. Since any quantity can be multiplied by 1 without changing it, the expression "100 kPa/1 bar" can be used to convert from bars to kPa by multiplying it with the quantity to be converted, including units. For example, 5 bar * 100 kPa/1 bar = 500 kPa because 5*100/1=500, and bar/bar cancels out, so 5 bar = 500 kPa.
Dimensional homogeneity.
The most basic rule of dimensional analysis is that of dimensional homogeneity. Only "commensurable" quantities (quantities with the same dimensions) may be "compared," "equated," "added," or "subtracted."
However, the dimensions form a "multiplicative group" and consequently:
For example, it makes no sense to ask if 1 hour is more, the same, or less than 1 kilometer, as these have different dimensions, nor to add 1 hour to 1 kilometer. On the other hand, if an object travels 100 km in 2 hours, one may divide these and conclude that the object's average speed was 50 km/h.
The rule implies that in a physically meaningful "expression" only quantities of the same dimension can be added, subtracted, or compared. For example, if "m"man, "m"rat and "L"man denote, respectively, the mass of some man, the mass of a rat and the length of that man, the dimensionally homogeneous expression "m"man + "m"rat is meaningful, but the heterogeneous expression "m"man + "L"man is meaningless. However, "m"man/"L"2man is fine. Thus, dimensional analysis may be used as a sanity check of physical equations: the two sides of any equation must be commensurable or have the same dimensions.
Even when two physical quantities have identical dimensions, it may nevertheless be meaningless to compare or add them. For example, although torque and energy share the dimension ML2/T2, they are fundamentally different physical quantities.
To compare, add, or subtract quantities with the same dimensions but expressed in different units, the standard procedure is first to convert them all to the same units. For example, to compare 32 metres with 35 yards, use 1 yard = 0.9144 m to convert 35 yards to 32.004 m.
A related principle is that any physical law that accurately describes the real world must be independent of the units used to measure the physical variables. For example, Newton's laws of motion must hold true whether distance is measured in miles or kilometers. This principle gives rise to the form that conversion factors must take between units that measure the same dimension: multiplication by a simple constant. It also ensures equivalence; for example, if two buildings are the same height in feet, then they must be the same height in meters.
The factor-label method for converting units.
The factor-label method is the sequential application of conversion factors expressed as fractions and arranged so that any dimensional unit appearing in both the numerator and denominator of any of the fractions can be cancelled out until only the desired set of dimensional units is obtained. For example, 10 miles per hour can be converted to meters per second by using a sequence of conversion factors as shown below:
It can be seen that each conversion factor is equivalent to the value of one. For example, starting with 1 mile = 1609 meters and dividing both sides of the equation by 1 mile yields 1 mile / 1 mile = 1609 meters / 1 mile, which when simplified yields 1 = 1609 meters / 1 mile.
So, when the units "mile" and "hour" are cancelled out and the arithmetic is done, 10 miles per hour converts to 4.47 meters per second.
As a more complex example, the concentration of nitrogen oxides (i.e., NOx) in the flue gas from an industrial furnace can be converted to a mass flow rate expressed in grams per hour (i.e., g/h) of NOx by using the following information as shown below:
After cancelling out any dimensional units that appear both in the numerators and denominators of the fractions in the above equation, the NOx concentration of 10 ppmv converts to mass flow rate of 24.63 grams per hour.
Checking equations that involve dimensions.
The factor-label method can also be used on any mathematical equation to check whether or not the dimensional units on the left hand side of the equation are the same as the dimensional units on the right hand side of the equation. Having the same units on both sides of an equation does not guarantee that the equation is correct, but having different units on the two sides of an equation does guarantee that the equation is wrong.
For example, check the Universal Gas Law equation of "P·V = n·R·T", when:
As can be seen, when the dimensional units appearing in the numerator and denominator of the equation's right hand side are cancelled out, both sides of the equation have the same dimensional units.
Limitations.
The factor-label method can convert only unit quantities for which the units are in a linear relationship intersecting at 0. Most units fit this paradigm. An example for which it cannot be used is the conversion between degrees Celsius and kelvins (or Fahrenheit). Between degrees Celsius and kelvins, there is a constant difference rather than a constant ratio, while between Celsius and Fahrenheit there is neither a constant difference nor a constant ratio. There is however an affine transform (formula_4), (rather than a linear transform (formula_5)) between them.
For instance, the freezing point of water is 0° in Celsius and 32° in Fahrenheit, and a 5° change in Celsius corresponds to a 9° change in Fahrenheit. Thus, to convert from Fahrenheit to Celsius one subtracts 32° (displacement from one point), multiplies by 5 and divides by 9 (scales by the ratio of units), and adds 0 (displacement from new point). Reversing this yields the formula for Celsius; one could have started with the equivalence between 100° Celsius and 212° Fahrenheit, though this would yield the same formula at the end.
Hence, to convert Fahrenheit to Celsius, enter the Fahrenheit value into this formula:
Note that dividing by 1.8 is the same as multiplying by 5 and dividing by 9.
And, to convert Celsius to Fahrenheit, enter the Celsius value into this formula:
Note that multiplying by 1.8 is the same as multiplying by 9 and dividing by 5.
Applications.
Dimensional analysis is most often used in physics and chemistry- and in the mathematics thereof- but finds some applications outside of those fields as well.
Mathematics.
A simple application of dimensional analysis to mathematics is in computing the form of the volume of an "n"-ball (the solid ball in "n"-dimensions), or the area of its surface, the "n"-sphere: being an "n"-dimensional figure, the volume scales as formula_6 while the surface area, being formula_7-dimensional, scales as formula_8 Thus the volume of the "n"-ball in terms of the radius is formula_9 for some constant formula_10 Determining the constant takes more involved mathematics, but the form can be deduced and checked by dimensional analysis alone.
Finance, economics, and accounting.
In finance, economics, and accounting, dimensional analysis is most commonly referred to in terms of the distinction between stocks and flows. More generally, dimensional analysis is used in interpreting various financial ratios, economics ratios, and accounting ratios.
Critics of mainstream economics, notably including adherents of Austrian economics, have claimed that it lacks dimensional consistency.
Fluid Mechanics.
Common dimensionless groups in fluid mechanics include:
Re=ρVd/μ
Fr=V/√(gL)
Eu=V/(p/ρ)
History.
James Clerk Maxwell played a major role in establishing modern use of dimensional analysis by distinguishing mass, length, and time as fundamental units, while referring to other units as derived. Although Maxwell defined length, time and mass to be "The Three Fundamental Units", he also noted that gravitational mass can be derived from length and time, viz. M=L3/T2. Maxwell then determined that the dimensions of an "Electrostatic Unit" of charge were Q=L3/2M1/2/T, which, after substituting his M=L3/T2 equation for mass, reveals that charge has the same fundamental dimensions as M, viz. Q=L3/T2.
The 19th-century French mathematician Joseph Fourier made important contributions based on the idea that physical laws like should be independent of the units employed to measure the physical variables. This led to the conclusion that meaningful laws must be homogeneous equations in their various units of measurement, a result which was eventually formalized in the Buckingham π theorem.
Dimensional analysis is also used to derive relationships between the physical quantities that are involved in a particular phenomenon that one wishes to understand and characterize. It was used for the first time in this way in 1872 by Lord Rayleigh, who was trying to understand why the sky is blue. Rayleigh first published the technique in his book "theory of sound" from 1877.
Mathematical examples.
The Buckingham π theorem describes how every physically meaningful equation involving "n" variables can be equivalently rewritten as an equation of "n" − "m" dimensionless parameters, where "m" is the rank of the dimensional matrix. Furthermore, and most importantly, it provides a method for computing these dimensionless parameters from the given variables.
A dimensional equation can have the dimensions reduced or eliminated through nondimensionalization, which begins with dimensional analysis, and involves scaling quantities by characteristic units of a system or natural units of nature. This gives insight into the fundamental properties of the system, as illustrated in the examples below.
Definition.
The dimension of a physical quantity can be expressed as a product of the basic physical dimensions mass, length, time, electric charge, and absolute temperature, represented by sans-serif symbols M, L, T, Q, and Θ, respectively, each raised to a rational power.
The SI standard recommends the usage of the following dimensions and corresponding symbols: mass (M), length (L), time (T), electrical current (I), absolute temperature (Θ), amount of substance (N) and luminous intensity (J).
The term "dimension" is more abstract than "scale" unit: "mass" is a dimension, while kilograms are a scale unit (choice of standard) in the mass dimension.
As examples, the dimension of the physical quantity speed is "length/time" (L/T or LT−1), and the dimension of the physical quantity force is "mass × acceleration" or "mass×(length/time)/time" (ML/T2 or MLT−2). In principle, other dimensions of physical quantity could be defined as "fundamental" (such as momentum or energy or electric current) in lieu of some of those shown above. Most physicists do not recognize temperature, Θ, as a fundamental dimension of physical quantity since it essentially expresses the energy per particle per degree of freedom, which can be expressed in terms of energy (or mass, length, and time). Still others do not recognize electric current, I, as a separate fundamental dimension of physical quantity, since it has been expressed in terms of mass, length, and time in unit systems such as the cgs system. There are also physicists that have cast doubt on the very existence of incompatible fundamental dimensions of physical quantity, although this does not invalidate the usefulness of dimensional analysis.
The unit of a physical quantity and its dimension are related, but not identical concepts. The units of a physical quantity are defined by convention and related to some standard; e.g. length may have units of metres, feet, inches, miles or micrometres; but any length always has a dimension of L, no matter what units of length are chosen to measure it. Two different units of the same physical quantity have conversion factors that relate them. For example, 1 in = 2.54 cm; in this case (2.54 cm/in) is the conversion factor, and is itself dimensionless. Therefore multiplying by that conversion factor does not change a quantity. Dimensional symbols do not have conversion factors.
Mathematical properties.
The dimensions that can be formed from a given collection of basic physical dimensions, such as M, L, and T, form an abelian group: The identity is written as 1; L0 = 1, and the inverse to L is 1/L or L−1. L raised to any rational power "p" is a member of the group, having an inverse of L−"p" or 1/Lp. The operation of the group is multiplication, having the usual rules for handling exponents (L"n" × L"m" = L"n"+"m").
This group can be described as a vector space over the rational numbers, with for example dimensional symbol M"i"L"j"T"k" corresponding to the vector ("i", "j", "k"). When physical measured quantities (be they like-dimensioned or unlike-dimensioned) are multiplied or divided by one other, their dimensional units are likewise multiplied or divided; this corresponds to addition or subtraction in the vector space. When measurable quantities are raised to a rational power, the same is done to the dimensional symbols attached to those quantities; this corresponds to scalar multiplication in the vector space.
A basis for a given vector space of dimensional symbols is called a set of fundamental units or fundamental dimensions, and all other vectors are called derived units. As in any vector space, one may choose different bases, which yields different systems of units (e.g., choosing whether the unit for charge is derived from the unit for current, or vice versa).
The group identity 1, the dimension of dimensionless quantities, corresponds to the origin in this vector space.
The set of units of the physical quantities involved in a problem correspond to a set of vectors (or a matrix). The kernel describes some number (e.g., m) of ways in which these vectors can be combined to produce a zero vector. These correspond to producing (from the measurements) a number of dimensionless quantities, {π1...,πm}. (In fact these ways completely span the null subspace of another different space, of powers of the measurements.) Every possible way of multiplying (and exponentiating) together the measured quantities to produce something with the same units as some derived quantity X can be expressed in the general form
Consequently, every possible commensurate equation for the physics of the system can be rewritten in the form
Knowing this restriction can be a powerful tool for obtaining new insight into the system.
Mechanics.
In mechanics, the dimension of any physical quantity can be expressed in terms of the fundamental dimensions (or "base dimensions") M, L, and T – these form a 3-dimensional vector space. This is not the only possible choice, but it is the one most commonly used. For example, one might choose force, length and mass as the base dimensions (as some have done), with associated dimensions F, L, M; this corresponds to a different basis, and one may convert between these representations by a change of basis. The choice of the base set of dimensions is, thus, partly a convention, resulting in increased utility and familiarity. It is, however, important to note that the choice of the set of dimensions cannot be chosen arbitrarily – it is not "just" a convention – because the dimensions must form a basis: they must span the space, and be linearly independent.
For example, F, L, M form a set of fundamental dimensions because they form an equivalent basis to "M," "L," "T:" the former can be expressed as [F=ML/T2],L,M while the latter can be expressed as M,L,[T=(ML/F)½].
On the other hand, using length, velocity and time ("L, V, T") as base dimensions will not work well (they do not form a set of fundamental dimensions), for two reasons:
Other fields of physics and chemistry.
Depending on the field of physics, it may be advantageous to choose one or another extended set of dimensional symbols. In electromagnetism, for example, it may be useful to use dimensions of M, L, T, and Q, where Q represents quantity of electric charge. In thermodynamics, the base set of dimensions is often extended to include a dimension for temperature, Θ. In chemistry the number of moles of substance (loosely, but not precisely, related to the number of molecules or atoms) is often involved and a dimension for this is used as well.
In the interaction of relativistic plasma with strong laser pulses, a dimensionless relativistic similarity parameter, connected with the symmetry properties of the collision-less Vlasov equation, is constructed from the plasma- electron- and critical-densities in addition to the electromagnetic vector potential. The choice of the dimensions or even the number of dimensions to be used in different fields of physics is to some extent arbitrary, but consistency in use and ease of communications are common and necessary features.
The fundamental physical constants.
The primary physical constant is the speed of light in vacuum ("c"0), which has unitary space-time dimensions of L/T, i.e. metres per second.
Another fundamental is Newton's gravitational constant ("G"), with SI dimensions of L3/T2M. When Maxwell's dimensions for mass (M=L3/T2) are substituted into the SI dimensions, the gravitational constant is shown to be dimensionless in elemental space-time, viz. L0/T0.
Planck's constant ("h"), the fundamental ratio of a quantum of energy to its wavefunction's frequency (T−1), has SI dimensions of L2M/T. Substituting Maxwell's dimensions for mass (M=L3/T2) shows that the Planck quantum of Action has fundamental dimensions of L5/T3.
Maxwell determined that the unit of elementary charge ("e") has dimensions of (L3M/T2)½. Substituting his mass dimensions (M=L3/T2) reveals that charge has the fundamental dimensions of (L6/T4)½, i.e. Q=L3/T2.
The Boltzmann constant ("k"B) is defined as the energy in Joules per degree of temperature (Θ), having SI dimensions of L2M/T2Θ. Substituting M=L3/T2 reveals the Boltzmann constant to have space-time dimensions of L5/T4 (energy) per degree K.
The Planck units.
The Planck Units are "natural units" of measurement defined exclusively in terms of five universal physical constants, viz. "c", "G", "ħ", "k"e and "k"B, such that these constants have the numerical value of 1 when expressed in terms of the Planck units.
The base spatial unit is the Planck length ("lP"), defined as the distance traveled by light in vacuum during one Planck time ("tP"). The numerical value of "lP" is calculated from ("ħ" "G" "c" −3 )½, the fundamental space-time dimensions of which resolve as (L5 T −3∙L−3 T 3 )½ = L.
The five base Planck units, viz. length, time, mass, charge and temperature, have traditionally been dimensioned in terms of the base SI units L, T, M, Q and Θ. However, Maxwell's factoring of mass and charge into the more fundamental space-time dimensions of L3 T−2 permits a deep two-dimensional analysis of the base and derived Planck units.
Since the gravitational constant "G" and the vacuum permittivity (electric constant) "ε"₀ are dimensionless in L/T space-time basis units, they can be factored out of the Planck units, thereby simplifying the dimensional analysis. For example, Planck area is defined as "ħ G" / "c"3, which simplifies to L5 T −3∙L−3 T 3 = L2. Similarly, Planck current is defined as (4π "ε"₀ "c" 6/"G" )½, which resolves to (L6 T −6 )½ = L3 T −3.
Thus, the fundamental space-time dimensions for each of the Planck units can be derived from their defining expressions. However, it is considerably easier to simply substitute L3 T −2 for M and Q in the conventional SI dimensions of the Planck quantities, as follows:
To facilitate further analysis, these quantities can be arranged into a log-log space/time matrix, whose columns represent incrementing powers of Planck length (Ln) and whose rows represent increasing powers of inverse-time (T−m):
Five mutually-orthogonal spatial dimensions are required to accommodate all the Planck units, notably the "higher dimensional" (L4, L5) quantities of momentum, force, action, energy and power. Three of the spatial dimensions are the real linear dimensions of "x,y,z" space, viz. length, breadth and height. Like the "time dimension" of special relativity, defined by Einstein (1905) as √-1∙c∙t, the two extra spatial dimensions are mathematically imaginary by virtue of their orthogonality, i.e. being Wick-rotated relative to all the other dimensions. The space/time matrix represents a complex 6-dimensional Hilbert space, with internal symmetries corresponding to the SU(3) × SU(2) × U(1) unitary group, consistent with the Standard Model.
In 2006, Wesson determined that an extra spatial coordinate "x4" could be identified as ℓ=  "Gm/c" 2, which he termed the "Einstein gauge". Formulated in terms of momentum, i.e. ℓ=  "Gp/c" 3, this gauge corresponds to the L4 spatial dimension of the space/time matrix, from which emerges Momentum, Force, and Pressure. Wesson also identified another spatial coordinate as ℓ=  "ħ/mc" (dimensionally identical to ℓ=  "ħ/qc"), which he termed the "Planck gauge". This "ħ/qc" gauge corresponds to the L5 spatial coordinate in the space/time matrix. The physical quantities of Action, Energy, Power and Intensity emerge from this imaginary dimension.
Electromagnetism.
Substitution of Maxwell's L3 T −2 for mass (M) and charge (Q) in the SI dimensions of the electromagnetic (EM) quantities effects a "flattening" of their dimensionality to just spatial length and time, as follows:
As with the Planck Units, these quantities can be arranged in a 6-dimensional space/time matrix, with columns representing powers of Planck length (Ln), and rows which represent powers of inverse-time (T−m). Noting that the differential (or gradient) of any quantity with respect to time ("d/dt") is the unit immediately below it, and that the differential of a quantity with respect to space ("d/ds" or ∇) is the unit to its left, Maxwell's equations can be discerned in the relationships between these electromagnetic quantities. Within the log-log matrix, multiplication is effected by adding the two units' dimensional indices, e.g. La T b x Lc T d = L(a + c) T (b + d), and division is performed by subtracting the denominator's space/time indices from the numerator's.
Polynomials and transcendental functions.
Scalar arguments to transcendental functions such as exponential, trigonometric and logarithmic functions, or to inhomogeneous polynomials, must be dimensionless quantities. (Note: this requirement is somewhat relaxed in Siano's orientational analysis described below, in which the square of certain dimensioned quantities are dimensionless.)
While most mathematical identities about dimensionless numbers translate in a straightforward manner to dimensional quantities, care must be taken with logarithms of ratios: the identity log(a/b) = log a − log b, where the logarithm is taken in any base, holds for dimensionless numbers a and b, but it does "not" hold if a and b are dimensional, because in this case the left-hand side is well-defined but the right-hand side is not.
Similarly, while one can evaluate monomials ("x""n") of dimensional quantities, one cannot evaluate polynomials of mixed degree with dimensionless coefficients on dimensional quantities: for "x"2, the expression (3 m)2 = 9 m2 makes sense (as an area), while for "x"2 + "x", the expression (3 m)2 + 3 m = 9 m2 + 3 m does not make sense.
However, polynomials of mixed degree can make sense if the coefficients are suitably chosen physical quantities that are not dimensionless. For example,
This is the height to which an object rises in time "t" if the acceleration of gravity is 32 feet per second per second and the initial upward speed is 500 feet per second. It is not even necessary for "t" to be in "seconds". For example, suppose "t" = 0.01 minutes. Then the first term would be
Incorporating units.
The value of a dimensional physical quantity "Z" is written as the product of a unit ["Z"] within the dimension and a dimensionless numerical factor, "n".
When like-dimensioned quantities are added or subtracted or compared, it is convenient to express them in consistent units so that the numerical values of these quantities may be directly added or subtracted. But, in concept, there is no problem adding quantities of the same dimension expressed in different units. For example, 1 meter added to 1 foot is a length, but one cannot derive that length by simply adding 1 and 1. A conversion factor, which is a ratio of like-dimensioned quantities and is equal to the dimensionless unity, is needed:
The factor formula_18 is identical to the dimensionless 1, so multiplying by this conversion factor changes nothing. Then when adding two quantities of like dimension, but expressed in different units, the appropriate conversion factor, which is essentially the dimensionless 1, is used to convert the quantities to identical units so that their numerical values can be added or subtracted.
Position vs displacement.
Some discussions of dimensional analysis implicitly describe all quantities as mathematical vectors. (In mathematics scalars are considered a special case of vectors; vectors can be added to or subtracted from other vectors, and, inter alia, multiplied or divided by scalars. If a vector is used to define a position, this assumes an implicit point of reference: an origin. While this is useful and often perfectly adequate, allowing many important errors to be caught, it can fail to model certain aspects of physics. A more rigorous approach requires distinguishing between position and displacement (or moment in time versus duration, or absolute temperature versus temperature change).
Consider points on a line, each with a position with respect to a given origin, and distances among them. Positions and displacements all have units of length, but their meaning is not interchangeable:
This illustrates the subtle distinction between "affine" quantities (ones modeled by an affine space, such as position) and "vector" quantities (ones modeled by a vector space, such as displacement).
Properly then, positions have dimension of "affine" length, while displacements have dimension of "vector" length. To assign a number to an "affine" unit, one must not only choose a unit of measurement, but also a point of reference, while to assign a number to a "vector" unit only requires a unit of measurement.
Thus some physical quantities are better modeled by vectorial quantities while others tend to require affine representation, and the distinction is reflected in their dimensional analysis.
This distinction is particularly important in the case of temperature, for which the numeric value of absolute zero is not the origin 0 in some scales. For absolute zero,
but for temperature differences,
(Here °R refers to the Rankine scale, not the Réaumur scale).
Unit conversion for temperature differences is simply a matter of multiplying by, e.g., 1 °F / 1 K (although the ratio is not a constant value). But because some of these scales have origins that do not correspond to absolute zero, conversion from one temperature scale to another requires accounting for that. As a result, simple dimensional analysis can lead to errors if it is ambiguous whether 1 K means the absolute temperature equal to −272.15 °C, or the temperature difference equal to 1 °C.
Orientation and frame of reference.
Similar to the issue of a point of reference is the issue of orientation: a displacement in 2 or 3 dimensions is not just a length, but is a length together with a "direction." (This issue does not arise in 1 dimension, or rather is equivalent to the distinction between positive and negative.) Thus, to compare or combine two dimensional quantities in a multi-dimensional space, one also needs an orientation: they need to be compared to a frame of reference.
This leads to the extensions discussed below, namely Huntley's directed dimensions and Siano's orientational analysis.
Examples.
A simple example: period of a harmonic oscillator.
What is the period of oscillation formula_19 of a mass formula_20 attached to an ideal linear spring with spring constant formula_21 suspended in gravity of strength formula_22? That period is the solution for formula_19 of some dimensionless equation in the variables formula_19, formula_20, formula_21, and formula_22.
The four quantities have the following dimensions: formula_19 [T]; formula_20 [M]; formula_21 [M/T2]; and formula_22 [L/T2]. From these we can form only one dimensionless product of powers of our chosen variables, formula_32 = formula_33 [T2 · M/T2 / M = 1], and putting formula_34 for some dimensionless constant formula_35 gives the dimensionless equation sought. The dimensionless product of powers of variables is sometimes referred to as a dimensionless group of variables; here the term "group" means "collection" rather than mathematical group. They are often called dimensionless numbers as well.
Note that the variable formula_22 does not occur in the group. It is easy to see that it is impossible to form a dimensionless product of powers that combines formula_22 with formula_21, formula_20, and formula_19, because formula_22 is the only quantity that involves the dimension L. This implies that in this problem the formula_22 is irrelevant. Dimensional analysis can sometimes yield strong statements about the "irrelevance" of some quantities in a problem, or the need for additional parameters. If we have chosen enough variables to properly describe the problem, then from this argument we can conclude that the period of the mass on the spring is independent of formula_22: it is the same on the earth or the moon. The equation demonstrating the existence of a product of powers for our problem can be written in an entirely equivalent way: formula_44, for some dimensionless constant κ (equal to formula_45 from the original dimensionless equation).
When faced with a case where dimensional analysis rejects a variable (formula_22, here) that one intuitively expects to belong in a physical description of the situation, another possibility is that the rejected variable is in fact relevant, but that some other relevant variable has been omitted, which might combine with the rejected variable to form a dimensionless quantity. That is, however, not the case here.
When dimensional analysis yields only one dimensionless group, as here, there are no unknown functions, and the solution is said to be "complete" – although it still may involve unknown dimensionless constants, such as κ.
A more complex example: energy of a vibrating wire.
Consider the case of a vibrating wire of length "ℓ" ("L") vibrating with an amplitude "A" ("L"). The wire has a linear density "ρ" ("M"/"L") and is under tension "s" ("ML"/"T"2), and we want to know the energy "E" ("ML"2/"T"2) in the wire. Let "π"1 and "π"2 be two dimensionless products of powers of the variables chosen, given by
The linear density of the wire is not involved. The two groups found can be combined into an equivalent form as an equation
where "F" is some unknown function, or, equivalently as
where "f" is some other unknown function. Here the unknown function implies that our solution is now incomplete, but dimensional analysis has given us something that may not have been obvious: the energy is proportional to the first power of the tension. Barring further analytical analysis, we might proceed to experiments to discover the form for the unknown function "f". But our experiments are simpler than in the absence of dimensional analysis. We'd perform none to verify that the energy is proportional to the tension. Or perhaps we might guess that the energy is proportional to "ℓ", and so infer that . The power of dimensional analysis as an aid to experiment and forming hypotheses becomes evident.
The power of dimensional analysis really becomes apparent when it is applied to situations, unlike those given above, that are more complicated, the set of variables involved are not apparent, and the underlying equations hopelessly complex. Consider, for example, a small pebble sitting on the bed of a river. If the river flows fast enough, it will actually raise the pebble and cause it to flow along with the water. At what critical velocity will this occur? Sorting out the guessed variables is not so easy as before. But dimensional analysis can be a powerful aid in understanding problems like this, and is usually the very first tool to be applied to complex problems where the underlying equations and constraints are poorly understood. In such cases, the answer may depend on a dimensionless number such as the Reynolds number, which may be interpreted by dimensional analysis.
Extensions.
Huntley's extension: directed dimensions.
Huntley has pointed out that it is sometimes productive to refine our concept of dimension. Two possible refinements are:
As an example of the usefulness of the first refinement, suppose we wish to calculate the distance a cannonball travels when fired with a vertical velocity component formula_51 and a horizontal velocity component formula_52, assuming it is fired on a flat surface. Assuming no use of directed lengths, the quantities of interest are then formula_52, formula_51, both dimensioned as formula_55, "R", the distance travelled, having dimension "L", and "g" the downward acceleration of gravity, with dimension formula_56
With these four quantities, we may conclude that the equation for the range "R" may be written:
Or dimensionally
from which we may deduce that formula_59 and formula_60, which leaves one exponent undetermined. This is to be expected since we have two fundamental quantities "L" and "T" and four parameters, with one equation.
If, however, we use directed length dimensions, then formula_52 will be dimensioned as formula_62, formula_51 as formula_64, "R" as formula_50 and "g" as formula_66. The dimensional equation becomes:
and we may solve completely as formula_68, formula_69 and formula_70. The increase in deductive power gained by the use of directed length dimensions is apparent.
In a similar manner, it is sometimes found useful (e.g., in fluid mechanics and thermodynamics) to distinguish between mass as a measure of inertia (inertial mass), and mass as a measure of quantity (substantial mass). For example, consider the derivation of Poiseuille's Law. We wish to find the rate of mass flow of a viscous fluid through a circular pipe. Without drawing distinctions between inertial and substantial mass we may choose as the relevant variables
There are three fundamental variables so the above five equations will yield two dimensionless variables which we may take to be formula_81 and formula_82 and we may express the dimensional equation as
where C and a are undetermined constants. If we draw a distinction between inertial mass with dimensions formula_84 and substantial mass with dimensions formula_85, then mass flow rate and density will use substantial mass as the mass parameter, while the pressure gradient and coefficient of viscosity will use inertial mass. We now have four fundamental parameters, and one dimensionless constant, so that the dimensional equation may be written:
where now only "C" is an undetermined constant (found to be equal to formula_87 by methods outside of dimensional analysis). This equation may be solved for the mass flow rate to yield Poiseuille's law.
Siano's extension: orientational analysis.
Huntley's extension has some serious drawbacks:
It also is often quite difficult to assign the "L", "L""x", "L""y", "L""z", symbols to the physical variables involved in the problem of interest. He invokes a procedure that involves the "symmetry" of the physical problem. This is often very difficult to apply reliably: It is unclear as to what parts of the problem that the notion of "symmetry" is being invoked. Is it the symmetry of the physical body that forces are acting upon, or to the points, lines or areas at which forces are being applied? What if more than one body is involved with different symmetries? Consider the spherical bubble attached to a cylindrical tube, where one wants the flow rate of air as a function of the pressure difference in the two parts. What are the Huntley extended dimensions of the viscosity of the air contained in the connected parts? What are the extended dimensions of the pressure of the two parts? Are they the same or different? These difficulties are responsible for the limited application of Huntley's addition to real problems.
Angles are, by convention, considered to be dimensionless variables, and so the use of angles as physical variables in dimensional analysis can give less meaningful results. As an example, consider the projectile problem mentioned above. Suppose that, instead of the "x"- and "y"-components of the initial velocity, we had chosen the magnitude of the velocity "v" and the angle "θ" at which the projectile was fired. The angle is, by convention, considered to be dimensionless, and the magnitude of a vector has no directional quality, so that no dimensionless variable can be composed of the four variables "g", "v", "R", and θ. Conventional analysis will correctly give the powers of "g" and "v", but will give no information concerning the dimensionless angle "θ".
Siano (1985-I, 1985-II) has suggested that the directed dimensions of Huntley be replaced by using "orientational symbols" 1"x" 1"y" 1"z" to denote vector directions, and an orientationless symbol 10. Thus, Huntley's "L""x" becomes "L" 1"x" with "L" specifying the dimension of length, and 1"x" specifying the orientation. Siano further shows that the orientational symbols have an algebra of their own. Along with the requirement that 1"i"−1 = 1"i", the following multiplication table for the orientation symbols results:
Note that the orientational symbols form a group (the Klein four-group or "Viergruppe"). In this system, scalars always have the same orientation as the identity element, independent of the "symmetry of the problem." Physical quantities that are vectors have the orientation expected: a force or a velocity in the "z"-direction has the orientation of 1"z". For angles, consider an angle "θ" that lies in the "z"-plane. Form a right triangle in the z plane with θ being one of the acute angles. The side of the right triangle adjacent to the angle then has an orientation 1"x" and the side opposite has an orientation 1"y". Then, since tan("θ") = 1"y"/1"x" = "θ" + ... we conclude that an angle in the xy plane must have an orientation 1"y"/1"x" = 1"z", which is not unreasonable. Analogous reasoning forces the conclusion that sin("θ") has orientation 1"z" while cos("θ") has orientation 10. These are different, so one concludes (correctly), for example, that there are no solutions of physical equations that are of the form  a cos("θ")+b sin("θ") , where a and b are real scalars. Note that an expression such as formula_89 is not dimensionally inconsistent since it is a special case of the sum of angles formula and should properly be written:
which for formula_91 and formula_92 yields formula_93. Physical quantities may be expressed as complex numbers (e.g. formula_94) which imply that the complex quantity "i"  has an orientation equal to that of the angle it is associated with (1"z" in the above example).
The assignment of orientational symbols to physical quantities and the requirement that physical equations be orientationally homogeneous can actually be used in a way that is similar to dimensional analysis to derive a little more information about acceptable solutions of physical problems. In this approach one sets up the dimensional equation and solves it as far as one can. If the lowest power of a physical variable is fractional, both sides of the solution is raised to a power such that all powers are integral. This puts it into "normal form". The orientational equation is then solved to give a more restrictive condition on the unknown powers of the orientational symbols, arriving at a solution that is more complete than the one that dimensional analysis alone gives. Often the added information is that one of the powers of a certain variable is even or odd.
As an example, for the projectile problem, using orientational symbols, θ, being in the "xy"-plane will thus have dimension 1"z" and the range of the projectile "R" will be of the form:
Dimensional homogeneity will now correctly yield "a" = −1 and "b" = 2, and orientational homogeneity requires that "c" be an odd integer. In fact the required function of theta will be sin("θ")cos("θ") which is a series of odd powers of "θ".
It is seen that the Taylor series of sin("θ") and cos("θ") are orientationally homogeneous using the above multiplication table, while expressions like cos("θ") + sin("θ") and exp("θ") are not, and are (correctly) deemed unphysical.
It should be clear that the multiplication rule used for the orientational symbols is not the same as that for the cross product of two vectors. The cross product of two identical vectors is zero, while the product of two identical orientational symbols is the identity element.
Dimensionless concepts.
Constants.
The dimensionless constants that arise in the results obtained, such as the C in the Poiseuille's Law problem and the formula_96 in the spring problems discussed above come from a more detailed analysis of the underlying physics, and often arises from integrating some differential equation. Dimensional analysis itself has little to say about these constants, but it is useful to know that they very often have a magnitude of order unity. This observation can allow one to sometimes make "back of the envelope" calculations about the phenomenon of interest, and therefore be able to more efficiently design experiments to measure it, or to judge whether it is important, etc.
Formalisms.
Paradoxically, dimensional analysis can be a useful tool even if all the parameters in the underlying theory are dimensionless, e.g., lattice models such as the Ising model can be used to study phase transitions and critical phenomena. Such models can be formulated in a purely dimensionless way. As we approach the critical point closer and closer, the distance over which the variables in the lattice model are correlated (the so-called correlation length, formula_97 ) becomes larger and larger. Now, the correlation length is the relevant length scale related to critical phenomena, so one can, e.g., surmise on "dimensional grounds" that the non-analytical part of the free energy per lattice site should be formula_98 where formula_99 is the dimension of the lattice.
It has been argued by some physicists, e.g., Michael Duff, that the laws of physics are inherently dimensionless. The fact that we have assigned incompatible dimensions to Length, Time and Mass is, according to this point of view, just a matter of convention, borne out of the fact that before the advent of modern physics, there was no way to relate mass, length, and time to each other. The three independent dimensionful constants: "c", "ħ", and "G", in the fundamental equations of physics must then be seen as mere conversion factors to convert Mass, Time and Length into each other.
Just as in the case of critical properties of lattice models, one can recover the results of dimensional analysis in the appropriate scaling limit; e.g., dimensional analysis in mechanics can be derived by reinserting the constants "ħ", c, and G (but we can now consider them to be dimensionless) and demanding that a nonsingular relation between quantities exists in the limit formula_100, formula_101 and formula_102. In problems involving a gravitational field the latter limit should be taken such that the field stays finite.
Dimensional equivalences.
Following are tables of commonly occurring expressions in physics, related to the dimensions of energy, momentum, and force.
Natural units.
If "c" = "ħ" = 1, where "c" = luminal speed and "ħ" = Planck's reduced constant, and a suitable fixed unit of energy is chosen, then all quantities of length "L", mass "M" and time "T" can be expressed (dimensionally) as a power of energy "E", because length, mass and time can be expressed using speed "v", action "S", and energy "E":
though speed and action are dimensionless ("v" = "c" = 1 and "S" = "ħ" = 1) – so the only remaining quantity with dimension is energy. In terms of powers of dimensions:
This is particularly useful in particle physics and high energy physics, in which case the energy unit is the electron volt (eV). Dimensional checks and estimates become very simple in this system.
However, if electric charges and currents are involved, another unit to be fixed is for electric charge, normally the electron charge "e" though other choices are possible.

</doc>
<doc id="8270" url="http://en.wikipedia.org/wiki?curid=8270" title="December 25">
December 25

December 25 is the day of the year in the Gregorian calendar. 

</doc>
<doc id="8271" url="http://en.wikipedia.org/wiki?curid=8271" title="Digital television">
Digital television

Digital television (DTV) is the transmission of audio and video by digitally processed and multiplexed signal, in contrast to the totally analog and channel separated signals used by analog television. Digital TV can support more than one program in the same channel bandwidth. It is an innovative service that represents the first significant evolution in television technology since color television in the 1950s. All countries except North Korea are replacing or would replace broadcast analog television with digital television and are allowing other uses of the television radio spectrum. Several regions of the world are in different stages of adaptation and are implementing different broadcasting standards. Below are the different widely used digital television broadcasting standards (DTB):
History.
Digital TV's roots have been tied very closely to the availability of inexpensive, high performance computers. It wasn't until the 1990s that digital TV became a real possibility.
In the mid-1980s as Japanese consumer electronics firms forged ahead with the development of HDTV technology, and as the MUSE analog format proposed by NHK, a Japanese company, was seen as a pacesetter that threatened to eclipse U.S. electronics companies. Until June 1990, the Japanese MUSE standard—based on an analog system—was the front-runner among the more than 23 different technical concepts under consideration. Then, an American company, General Instrument, demonstrated the feasibility of a digital television signal. This breakthrough was of such significance that the FCC was persuaded to delay its decision on an ATV standard until a digitally based standard could be developed.
In March 1990, when it became clear that a digital standard was feasible, the FCC made a number of critical decisions. First, the Commission declared that the new ATV standard must be more than an enhanced analog signal, but be able to provide a genuine HDTV signal with at least twice the resolution of existing television images.(7) Then, to ensure that viewers who did not wish to buy a new digital television set could continue to receive conventional television broadcasts, it dictated that the new ATV standard must be capable of being "simulcast" on different channels.(8)The new ATV standard also allowed the new DTV signal to be based on entirely new design principles. Although incompatible with the existing NTSC standard, the new DTV standard would be able to incorporate many improvements.
The final standard adopted by the FCC did not require a single standard for scanning formats, aspect ratios, or lines of resolution. This outcome resulted from a dispute between the consumer electronics industry (joined by some broadcasters) and the computer industry (joined by the film industry and some public interest groups) over which of the two scanning processes—interlaced or progressive—is superior. Interlaced scanning, which is used in televisions worldwide, scans even-numbered lines first, then odd-numbered ones. Progressive scanning, which is the format used in computers, scans lines in sequences, from top to bottom. The computer industry argued that progressive scanning is superior because it does not "flicker" in the manner of interlaced scanning. It also argued that progressive scanning enables easier connections with the Internet, and is more cheaply converted to interlaced formats than vice versa. The film industry also supported progressive scanning because it offers a more efficient means of converting filmed programming into digital formats. For their part, the consumer electronics industry and broadcasters argued that interlaced scanning was the only technology that could transmit the highest quality pictures then (and currently) feasible, i.e., 1,080 lines per picture and 1,920 pixels per line. Broadcasters also favored interlaced scanning because their vast archive of interlaced programming is not readily compatible with a progressive format.
Digital television transition started in the late 2000s. All the governments across the world set the deadline for analog shutdown by the 2010s. Initially the adoption rate was low. But soon, more and more households were converting to digital televisions. The transition is expected to be completed worldwide by mid to late 2010s.
Technical information.
Formats and bandwidth.
Digital television supports many different picture formats defined by the broadcast television systems which are a combination of size, aspect ratio (width to height ratio).
With digital terrestrial television (DTT) broadcasting, the range of formats can be broadly divided into two categories: high definition television (HDTV) for the transmission of high-definition video and standard-definition television (SDTV). These terms by themselves are not very precise, and many subtle intermediate cases exist.
One of several different HDTV formats that can be transmitted over DTV is: 1280 × 720 pixels in progressive scan mode (abbreviated "720p") or 1920 × 1080 pixels in interlaced video mode ("1080i"). Each of these uses a aspect ratio. (Some televisions are capable of receiving an HD resolution of 1920 × 1080 at a 60 Hz progressive scan frame rate — known as 1080p.) HDTV cannot be transmitted over analog television channels because of channel capacity issues.
Standard definition TV (SDTV), by comparison, may use one of several different formats taking the form of various aspect ratios depending on the technology used in the country of broadcast. For aspect-ratio broadcasts, the 640 × 480 format is used in NTSC countries, while 720 × 576 is used in PAL countries. For broadcasts, the 720 × 480 format is used in NTSC countries, while 720 × 576 is used in PAL countries. However, broadcasters may choose to reduce these resolutions to save bandwidth (e.g., many DVB-T channels in the United Kingdom use a horizontal resolution of 544 or 704 pixels per line).
Each commercial broadcasting terrestrial television DTV channel in North America is permitted to be broadcast at a bit rate up to 19 megabits per second. However, the broadcaster does not need to use this entire bandwidth for just one broadcast channel. Instead the broadcast can use the channel to include PSIP and can also subdivide across several video subchannels (a.k.a. feeds) of varying quality and compression rates, including non-video datacasting services that allow one-way high-bandwidth streaming of data to computers like National Datacast.
A broadcaster may opt to use a standard-definition (SDTV) digital signal instead of an HDTV signal, because current convention allows the bandwidth of a DTV channel (or "multiplex") to be subdivided into multiple digital subchannels, (similar to what most FM radio stations offer with HD Radio), providing multiple feeds of entirely different television programming on the same channel. This ability to provide either a single HDTV feed or multiple lower-resolution feeds is often referred to as distributing one's "bit budget" or multicasting. This can sometimes be arranged automatically, using a statistical multiplexer (or "stat-mux"). With some implementations, image resolution may be less directly limited by bandwidth; for example in DVB-T, broadcasters can choose from several different modulation schemes, giving them the option to reduce the transmission bitrate and make reception easier for more distant or mobile viewers.
Receiving digital signal.
There are several different ways to receive digital television. One of the oldest means of receiving DTV (and TV in general) is using an antenna (known as an "aerial" in some countries). This way is known as Digital terrestrial television (DTT). With DTT, viewers are limited to whatever channels the antenna picks up. Signal quality will also vary. Regardless of what sale ads try to lead the public to believe, there is no such thing as a specialized DTV antenna. ANY Over the Air antenna that worked for analog TV should work for Digital TV (But DTV signal levels are lower thus requiring actually a bigger antenna with more gain unless you are visually close to the transmitting towers).
Other ways have been devised to receive digital television. Among the most familiar to people are digital cable and digital satellite. In some countries where transmissions of TV signals are normally achieved by microwaves, digital MMDS is used. Other standards, such as Digital multimedia broadcasting (DMB) and DVB-H, have been devised to allow handheld devices such as mobile phones to receive TV signals. Another way is IPTV, that is receiving TV via Internet Protocol, relying on digital subscriber line (DSL) or optical cable line. Finally, an alternative way is to receive digital TV signals via the open Internet. For example, there is P2P (peer-to-peer) Internet television software that can be used to watch TV on a computer.
Some signals carry encryption and specify use conditions (such as "may not be recorded" or "may not be viewed on displays larger than 1 m in diagonal measure") backed up with the force of law under the WIPO Copyright Treaty and national legislation implementing it, such as the U.S. Digital Millennium Copyright Act. Access to encrypted channels can be controlled by a removable smart card, for example via the Common Interface (DVB-CI) standard for Europe and via Point Of Deployment (POD) for IS or named differently CableCard.
Disadvantages.
While poor signal analog TV quality could be evaluated by the user by the amount of noise on the screen, digital TV has no grey areas, it either works or does not when the signal is not strong enough.
Protection parameters for terrestrial DTV broadcasting.
Digital television signals must not interfere with each other, and they must also coexist with analog television until it is phased out.
The following table gives allowable signal-to-noise and signal-to-interference ratios for various interference scenarios. This table is a crucial regulatory tool for controlling the placement and power levels of stations. Digital TV is more tolerant of interference than analog TV, and this is the reason a smaller range of channels can carry an all-digital set of television stations.
Interaction.
People can interact with a DTV system in various ways. One can, for example, browse the Electronic program guide. Modern DTV systems sometimes use a return path providing feedback from the end user to the broadcaster. This is possible with a coaxial or fiber optic cable, a dialup modem, or Internet connection but is not possible with a standard antenna.
Some of these systems support video on demand using a communication channel localized to a neighborhood rather than a city (terrestrial) or an even larger area (satellite).
1-segment broadcasting.
1seg (1-segment) is a special form of ISDB. Each channel is further divided into 13 segments. The 12 segments of them are allocated for HDTV and remaining segment, the 13th, is used for narrow-band receivers such as mobile television or cell phone.
Comparison of analog vs digital.
DTV has several advantages over analog TV, the most significant being that digital channels take up less bandwidth, and the bandwidth needs are continuously variable, at a corresponding reduction in image quality depending on the level of compression as well as the resolution of the transmitted image. This means that digital broadcasters can provide more digital channels in the same space, provide high-definition television service, or provide other non-television services such as multimedia or interactivity. DTV also permits special services such as multiplexing (more than one program on the same channel), electronic program guides and additional languages (spoken or subtitled). The sale of non-television services may provide an additional revenue source.
Digital and analog signals react to interference differently. For example, common problems with analog television include ghosting of images, noise from weak signals, and many other potential problems which degrade the quality of the image and sound, although the program material may still be watchable. With digital television, the audio and video must be synchronized digitally, so reception of the digital signal must be very nearly complete; otherwise, neither audio nor video will be usable. Short of this complete failure, "blocky" video is seen when the digital signal experiences interference.
Analog TV started off with monophonic sound, and later evolved to stereophonic sound with two independent audio signal channels. DTV will allow up to 5 audio signal channels plus a sub-woofer bass channel, with broadcasts similar in quality to movie theaters and DVDs.
Compression artifacts and allocated bandwidth.
DTV images have some picture defects that are not present on analog television or motion picture cinema, because of present-day limitations of bandwidth and compression algorithms such as MPEG-2. This defect is sometimes referred to as "mosquito noise".
Because of the way the human visual system works, defects in an image that are localized to particular features of the image or that come and go are more perceptible than defects that are uniform and constant. However, the DTV system is designed to take advantage of other limitations of the human visual system to help mask these flaws, e.g. by allowing more compression artifacts during fast motion where the eye cannot track and resolve them as easily and, conversely, minimizing artifacts in still backgrounds that may be closely examined in a scene (since time allows).
Effects of poor reception.
Changes in signal reception from factors such as degrading antenna connections or changing weather conditions may gradually reduce the quality of analog TV. The nature of digital TV results in a perfectly decodable video initially, until the receiving equipment starts picking up interference that overpowers the desired signal or if the signal is too weak to decode. Some equipment will show a garbled picture with significant damage, while other devices may go directly from perfectly decodable video to no video at all or lock up. This phenomenon is known as the digital cliff effect.
For remote locations, distant channels that, as analog signals, were previously usable in a snowy and degraded state may, as digital signals, be perfectly decodable or may become completely unavailable. The use of higher frequencies will add to these problems, especially in cases where a clear line-of-sight from the receiving antenna to the transmitter is not available.
Effect on old analog technology.
Television sets with only analog tuners cannot decode digital transmissions. When analog broadcasting over the air ceases, users of sets with analog-only tuners may use other sources of programming (e.g. cable, recorded media) or may purchase set-top converter boxes to tune in the digital signals. In the United States, a government-sponsored coupon was available to offset the cost of an external converter box. Analog switch-off (of full-power stations) took place on December 11, 2006 in The Netherlands, June 12, 2009 in the United States for full-power stations, July 24, 2011 in Japan, August 31, 2011 in Canada, February 13, 2012 in Arab states, May 1, 2012 in Germany, October 24, 2012 in the United Kingdom and Ireland, October 31, 2012 in selected Indian cities, and December 10, 2013 in Australia. Completion of analog switch-off is scheduled for December 31, 2014 in the whole of India, by 2015 in the Philippines and Uruguay, by September 1, 2015 for low-power stations in the United States, and by 2017 in Costa Rica.
Disappearance of TV-audio receivers.
Prior to the conversion to digital TV, analog television broadcast audio for TV channels on a separate FM carrier frequency from the video signal. This FM audio signal could be heard using standard radios equipped with the appropriate tuning circuits.
However, after the transition of many countries to digital TV, no portable radio manufacturer has yet developed an alternative method for portable radios to play just the audio signal of digital TV channels. (DTV radio is not the same thing.)
Environmental issues.
The adoption of a broadcast standard incompatible with existing analog receivers has created the problem of large numbers of analog receivers being discarded during digital television transition. One superintendent of Public Works was quoted in 2009 as saying, "Some of the studies I’ve read in the trade magazines say up to a quarter of American households could be throwing a TV out in the next two years following the regulation change". In 2009, an estimated 99 million analog TV receivers were sitting unused in homes in the US alone and, while some obsolete receivers are being retrofitted with converters, many more are simply dumped in landfills where they represent a source of toxic metals such as lead as well as lesser amounts of materials such as barium, cadmium and chromium.
According to one campaign group, a CRT computer monitor or TV contains an average of 8 lb of lead. According to another source, the lead in glass of a CRT varies from 1.08 lb to 11.28 lb, depending on screen size and type, but the lead is in the form of "stable and immobile" lead oxide mixed into the glass. It is claimed that the lead can have long-term negative effects on the environment if dumped as landfill. However, the glass envelope can be recycled at suitably equipped facilities. Other portions of the receiver may be subject to disposal as hazardous material.
Local restrictions on disposal of these materials vary widely; in some cases second-hand stores have refused to accept working color television receivers for resale due to the increasing costs of disposing of unsold TVs. Those thrift stores which are still accepting donated TVs have reported significant increases in good-condition working used television receivers abandoned by viewers who often expect them not to work after digital transition.
In Michigan in 2009, one recycler estimated that as many as one household in four would dispose of or recycle a TV set in the following year. The digital television transition, migration to high-definition television receivers and the replacement of CRTs with flatscreens are all factors in the increasing number of discarded analog CRT-based television receivers.

</doc>
<doc id="8274" url="http://en.wikipedia.org/wiki?curid=8274" title="Declaration of Arbroath">
Declaration of Arbroath

The Declaration of Arbroath is a declaration of Scottish independence, made in 1320. It is in the form of a letter in Latin submitted to Pope John XXII, dated 6 April 1320, intended to confirm Scotland's status as an independent, sovereign state and defending Scotland's right to use military action when unjustly attacked.
Generally believed to have been written in the Arbroath Abbey by Bernard of Kilwinning, then Chancellor of Scotland and Abbot of Arbroath, and sealed by fifty-one magnates and nobles, the letter is the sole survivor of three created at the time. The others were a letter from the King of Scots, Robert I, and a letter from four Scottish bishops which all presumably made similar points.
Overview.
The Declaration was part of a broader diplomatic campaign which sought to assert Scotland's position as an independent kingdom, rather than being a feudal land controlled by England's Norman kings, as well as lift the excommunication of Robert the Bruce. The Pope had recognised Edward I of England's claim to overlordship of Scotland in 1305 and Bruce was excommunicated by the Pope for murdering John Comyn before the altar in Greyfriars Church in Dumfries in 1306.
The Declaration made a number of rhetorical points: that Scotland had always been independent, indeed for longer than England; that Edward I of England had unjustly attacked Scotland and perpetrated atrocities; that Robert the Bruce had delivered the Scottish nation from this peril; and, most controversially, that the independence of Scotland was the prerogative of the Scottish people, rather than the King of Scots. In fact it stated that the nobility would choose someone else to be king if Bruce proved to be unfit in maintaining Scotland's independence. Some have interpreted this last point as an early expression of 'popular sovereignty' – that government is contractual and that kings can be chosen by the community rather than by God alone.
It has also been argued that the Declaration was not a statement of popular sovereignty (and that its signatories would have had no such concept)
but a statement of royal propaganda supporting Bruce's faction. 
 A justification had to be given for the rejection of King John in whose name William Wallace and Andrew de Moray had rebelled in 1297. The reason given in the Declaration is that Bruce was able to defend Scotland from English aggression whereas, by implication, King John could not.
To this man, in as much as he saved our people, and for upholding our freedom, we are bound by right as much as by his merits, and choose to follow him in all that he does.
Whatever the true motive, the idea of a contract between King and people was advanced to the Pope as an excuse for Bruce's coronation whilst John de Balliol still lived in Papal custody.
There are 39 names (eight earls and thirty one barons) at the start of the document, all of whom may have had their seals appended, probably over the space of some weeks and months, with nobles sending in their seals to be used. (On the extant copy of the Declaration there are only 19 seals, and of those 19 people only 12 are named within the document.) It is thought likely that at least 11 more seals than the original 39 might have been appended.) The Declaration was then taken to the papal court at Avignon by Bishop Kininmund, Sir Adam Gordon and Sir Odard de Maubuisson.
The Pope heeded the arguments contained in the Declaration, influenced by the offer of support from the Scots for his long-desired crusade if they no longer had to fear English invasion. He exhorted Edward II in a letter to make peace with the Scots, but the following year was again persuaded by the English to take their side and issued six bulls to that effect. It was only in October 1328, after a short-lived peace treaty between Scotland and England, the Treaty of Edinburgh-Northampton (which renounced all English claims to Scotland and was signed by the new English king, Edward III, on 1 March 1328), that the interdict on Scotland and the excommunication of its king were finally removed.
The original copy of the Declaration that was sent to Avignon is lost. A copy of the Declaration survives among Scotland's state papers, held by the National Archives of Scotland in Edinburgh. The most widely known English language translation was made by Sir James Fergusson, formerly Keeper of the Records of Scotland, from text that he reconstructed using this extant copy and early copies of the original draft. One passage in particular is often quoted from the Fergusson translation:
...for, as long as but a hundred of us remain alive, never will we on any conditions be brought under English rule. It is in truth not for glory, nor riches, nor honours that we are fighting, but for freedom – for that alone, which no honest man gives up but with life itself.

</doc>
<doc id="8276" url="http://en.wikipedia.org/wiki?curid=8276" title="Digital data">
Digital data

Digital data, in information theory and information systems, are discrete, discontinuous representations of information or works, as contrasted with continuous, or analog signals which behave in a continuous manner, or represent information using a continuous function.
Although digital representations are the subject matter of discrete mathematics, the information represented can be either discrete, such as numbers and letters, or it can be continuous, such as sounds, images, and other measurements.
The word "digital" comes from the same source as the words digit and "digitus" (the Latin word for "finger"), as fingers are often used for discrete counting. Mathematician George Stibitz of Bell Telephone Laboratories used the word "digital" in reference to the fast electric pulses emitted by a device designed to aim and fire anti-aircraft guns in 1942. The term is most commonly used in computing and electronics, especially where real-world information is converted to binary numeric form as in digital audio and digital photography.
Symbol to digital conversion.
Since symbols (for example, alphanumeric characters) are not continuous, representing symbols digitally is rather simpler than conversion of continuous or analog information to digital. Instead of sampling and quantization as in analog-to-digital conversion, such techniques as polling and encoding are used.
A symbol input device usually consists of a group of switches that are polled at regular intervals to see which switches are switched. Data will be lost if, within a single polling interval, two switches are pressed, or a switch is pressed, released, and pressed again. This polling can be done by a specialized processor in the device to prevent burdening the main CPU. When a new symbol has been entered, the device typically sends an interrupt, in a specialized format, so that the CPU can read it.
For devices with only a few switches (such as the buttons on a joystick), the status of each can be encoded as bits (usually 0 for released and 1 for pressed) in a single word. This is useful when combinations of key presses are meaningful, and is sometimes used for passing the status of modifier keys on a keyboard (such as shift and control). But it does not scale to support more keys than the number of bits in a single byte or word.
Devices with many switches (such as a computer keyboard) usually arrange these switches in a scan matrix, with the individual switches on the intersections of x and y lines. When a switch is pressed, it connects the corresponding x and y lines together. Polling (often called scanning in this case) is done by activating each x line in sequence and detecting which y lines then have a signal, thus which keys are pressed. When the keyboard processor detects that a key has changed state, it sends a signal to the CPU indicating the scan code of the key and its new state. The symbol is then encoded, or converted into a number, based on the status of modifier keys and the desired character encoding.
A custom encoding can be used for a specific application with no loss of data. However, using a standard encoding such as ASCII is problematic if a symbol such as 'ß' needs to be converted but is not in the standard.
It is estimated that in the year 1986 less than 1% of the world's technological capacity to store information was digital and in 2007 it was already 94%. The year 2002 is assumed to be the year when human kind was able to store more information in digital than in analog format (the "beginning of the digital age").
Properties of digital information.
All digital information possesses common properties that distinguish it from analog communications methods:
Historical digital systems.
Even though digital signals are generally associated with the binary electronic digital systems used in modern electronics and computing, digital systems are actually ancient, and need not be binary or electronic.

</doc>
<doc id="8278" url="http://en.wikipedia.org/wiki?curid=8278" title="Deduction">
Deduction

Deduction may refer to:

</doc>
<doc id="8280" url="http://en.wikipedia.org/wiki?curid=8280" title="Demon">
Demon

A demon, daemon (from Koine Greek δαιμόνιον "daimonion"), or fiend is a supernatural, often malevolent being prevalent in religion, occultism, literature, fiction, mythology and folklore. The original Greek word "daimon" does not carry the negative connotation initially understood by implementation of the Koine δαιμόνιον (daimonion), and later ascribed to any cognate words sharing the root.
In Ancient Near Eastern religions as well as in the Abrahamic traditions, including ancient and medieval Christian demonology, a demon is considered an unclean spirit, a fallen angel, or a spirit of unknown type which may cause demonic possession, calling for an exorcism. In Western occultism and Renaissance magic, which grew out of an amalgamation of Greco-Roman magic, Jewish Aggadah and Christian demonology, a demon is believed to be a spiritual entity that may be conjured and controlled.
Etymology.
The Ancient Greek word δαίμων "daimōn" denotes a spirit or divine power, much like the Latin "genius" or "numen". "Daimōn" most likely came from the Greek verb "daiesthai" (to divide, distribute). The Greek conception of a "daimōns" notably appears in the works of Plato, where it describes the divine inspiration of Socrates. To distinguish the classical Greek concept from its later Christian interpretation, the former is anglicized as either "daemon" or "daimon" rather than "demon".
The Greek terms do not have any connotations of evil or malevolence. In fact, εὐδαιμονία "eudaimonia", (literally good-spiritedness) means happiness. By the early Roman Empire, cult statues were seen, by pagans and their Christian neighbors alike, as inhabited by the numinous presence of the gods: "Like pagans, Christians still sensed and saw the gods and their power, and as something, they had to assume, lay behind it, by an easy traditional shift of opinion they turned these pagan "daimones" into malevolent 'demons', the troupe of Satan... Far into the Byzantine period Christians eyed their cities' old pagan statuary as a seat of the demons' presence. It was no longer beautiful, it was infested." The term had first acquired its negative connotations in the Septuagint translation of the Hebrew Bible into Greek, which drew on the mythology of ancient Semitic religions. This was then inherited by the Koine text of the New Testament. The Western medieval and neo-medieval conception of a "demon" derives seamlessly from the ambient popular culture of Late (Roman) Antiquity. The Hellenistic "daemon" eventually came to include many Semitic and Near Eastern gods as evaluated by Christianity.
The supposed existence of demons remains an important concept in many modern religions and occultist traditions. Demons are still feared largely due to their alleged power to possess living creatures. In the contemporary Western occultist tradition (perhaps epitomized by the work of Aleister Crowley), a demon (such as Choronzon, the Demon of the Abyss) is a useful metaphor for certain inner psychological processes (inner demons), though some may also regard it as an objectively real phenomenon. Some scholars believe that large portions of the demonology (see Asmodai) of Judaism, a key influence on Christianity and Islam, originated from a later form of Zoroastrianism, and were transferred to Judaism during the Persian era.
Psychological archetype.
Psychologist Wilhelm Wundt remarked that "among the activities attributed by myths all over the world to demons, the harmful predominate, so that in popular belief bad demons are clearly older than good ones." Sigmund Freud developed this idea and claimed that the concept of demons was derived from the important relation of the living to the dead: "The fact that demons are always regarded as the spirits of those who have died "recently" shows better than anything the influence of mourning on the origin of the belief in demons."
M. Scott Peck, an American psychiatrist, wrote two books on the subject, "People of the Lie: The Hope For Healing Human Evil" and "Glimpses of the Devil: A Psychiatrist's Personal Accounts of Possession, Exorcism, and Redemption". Peck describes in some detail several cases involving his patients. In "People of the Lie" he provides identifying characteristics of an evil person, whom he classified as having a character disorder. In "Glimpses of the Devil" Peck goes into significant detail describing how he became interested in exorcism in order to debunk the "myth" of possession by evil spirits – only to be convinced otherwise after encountering two cases which did not fit into any category known to psychology or psychiatry. Peck came to the conclusion that possession was a rare phenomenon related to evil, and that possessed people are not actually evil; rather, they are doing battle with the forces of evil.
Although Peck's earlier work was met with widespread popular acceptance, his work on the topics of evil and possession has generated significant debate and derision. Much was made of his association with (and admiration for) the controversial Malachi Martin, a Roman Catholic priest and a former Jesuit, despite the fact that Peck consistently called Martin a liar and manipulator. Richard Woods, a Roman Catholic priest and theologian, has claimed that Dr. Peck misdiagnosed patients based upon a lack of knowledge regarding dissociative identity disorder (formerly known as multiple personality disorder), and had apparently transgressed the boundaries of professional ethics by attempting to persuade his patients into accepting Christianity. Father Woods admitted that he has never witnessed a genuine case of demonic possession in all his years.
Ancient Near East.
Mesopotamia.
According to the Jewish Encyclopedia, "In Chaldean mythology the seven evil deities were known as "shedu", storm-demons, represented in ox-like form." They were represented as winged bulls, derived from the colossal bulls used as protective jinn of royal palaces.
From Chaldea, the term "shedu" traveled to the Israelites. The writers of the Tanach applied the word as a dialogism to Canaanite deities.
There are indications that demons in popular Hebrew mythology were believed to come from the nether world. Various diseases and ailments were ascribed to them, particularly those affecting the brain and those of internal nature. Examples include the catalepsy, headache, epilepsy and nightmares. There also existed a demon of blindness, "Shabriri" (lit. "dazzling glare") who rested on uncovered water at night and blinded those who drank from it.
Demons supposedly entered the body and caused the disease while overwhelming or "seizing" the victim. To cure such diseases, it was necessary to draw out the evil demons by certain incantations and talismanic performances, which the Essenes excelled at. Josephus, who spoke of demons as "spirits of the wicked which enter into men that are alive and kill them", but which could be driven out by a certain root, witnessed such a performance in the presence of the Emperor Vespasian and ascribed its origin to King Solomon. In mythology, there were few defences against Babylonian demons. The mythical mace Sharur had the power to slay demons such as Asag, a legendary gallu or edimmu of hideous strength.
Ancient Arabia.
Pre-Islamic mythology did not differentiate between gods and demons. Jinn were considered divinities of inferior rank and had many human abilities, such as eating, drinking and procreating. While most jinn were considered peaceful and well-disposed towards humans, there also existed evil jinn who contrived to injure people.
Judaism.
As referring to the existence or non-existence of "shedim" (Hebr. for "demons", "spirits") there are converse opinions in Judaism. There are "practically nil" roles assigned to demons in the Jewish Bible. In Judaism today, beliefs in "shedim" ("demons" or "evil spirits") are either midot hasidut (Hebr. for "customs of the pious"), and therefore not halachah, or notions based on a superstition that are non-essential, non-binding parts of Judaism, and therefore not normative Jewish practice. In conclusion, Jews are not obligated to belief in the existence of "shedim", as posek rabbi David Bar-Hayim points out.
Tanach.
The word "shedim" (Hebr. for "demons" or "spirits") appears only in two places in the Tanakh (, ). In both places, the term appears in a scriptural context of animal or child sacrifice to non-existent false gods that are called "shedim".
Talmudic tradition.
In the Jerusalem Talmud notions of "shedim" ("demons" or "evil spirits") are almost unknown or occur only very rarely. As opposed to this, in the Babylon Talmud there are many references to "shedim" ("demons" or "evil spirits") and magical incantations. The existence of "shedim" ("demons") in general was not questioned by most of the Babylonian Talmudists. As a consequence of the rise of influence of the Babylonian Talmud over that of the Talmud Yerushalmi, late rabbis in general took as fact the existence of "shedim", nor did most of the medieval thinkers question their reality. However, rationalists like Maimonides, Saadia Gaon and Abraham ibn Ezra and others—ahead of their time—explicitly denied their existence, and completely rejected concepts of demons, evil spirits, negative spiritual influences, attaching and possessing spirits. Their point of view eventually became mainstream Jewish understanding.
Kabbalah.
Some benevolent "shedim" were used in kabbalistic ceremonies (as with the "golem" of Rabbi Yehuda Loevy) and malevolent "shedim" ("mazikin", from the root meaning "to damage") were often credited with possession ("see: Dybbuk").
Folklore and Aggadah.
Aggadic tales from the Persian tradition describe the "shedim", the "mazziḳim" ("harmers"), and the "ruḥin" ("spirits"). There were also "lilin" ("night spirits"), "ṭelane" ("shade", or "evening spirits"), "ṭiharire" ("midday spirits"), and "ẓafrire" ("morning spirits"), as well as the "demons that bring famine" and "such as cause storm and earthquake". According to some aggadic stories about demons is told that they were under the dominion of a king or chief, either Asmodai or, in the older Aggadah, Samael ("the angel of death"), who killed via poison. Stories in the fashion of this kind of folklore never became an essential feature of Jewish theology. Although occasionally an angel is called "satan" in the Babylon Talmud, this does not refer to a demon: "Stand not in the way of an ox when coming from the pasture, for Satan dances between his horns".
Hinduism.
Hinduism includes numerous varieties of spirits that might be classified as demons, including Vetalas, Bhutas and Pishachas. Rakshasas and Asuras are often also taken as demons.
Asuras.
Originally, "Asura", in the earliest hymns of the Rig Veda, meant any supernatural spirit, either good or bad. Since the /s/ of the Indic linguistic branch is cognate with the /h/ of the Early Iranian languages, the word "Asura", representing a category of celestial beings, became the word "Ahura" (Mazda), the Supreme God of the monotheistic Zoroastrians. Ancient Hinduism tells that Devas (also called "suras") and Asuras are half-brothers, sons of the same father Kasyapa; although some of the Devas, such as Varuna, are also called Asuras. Later, during Puranic age, Asura and Rakshasa came to exclusively mean any of a race of anthropomorphic, powerful, possibly evil beings. Daitya (lit. sons of the mother "Diti"), Rakshasa (lit. from "harm to be guarded against"), and Asura are incorrectly translated into English as "demon".
In Hindu scriptures, pious, highly enlightened Asuras, such as Prahlada and Vibheeshana, are not uncommon. The Asura are not fundamentally against the gods, nor do they tempt humans to fall. This is markedly different from the traditional Western notions of demons as a rival army of God but comparable with the concept of the jinns in Islam. Many people metaphorically interpret the Asura as manifestations of the ignoble passions in the human mind and as a symbolic devices. There were also cases of power-hungry Asuras challenging various aspects of the Gods, but only to be defeated eventually and seek forgiveness—see Surapadman and Narakasura.
Evil spirits.
Hinduism advocates the reincarnation and transmigration of souls according to one's karma. Souls (Atman) of the dead are adjudged by the Yama and are accorded various purging punishments before being reborn. Humans that have committed extraordinary wrongs are condemned to roam as lonely, often evil, spirits for a length of time before being reborn. Many kinds of such spirits (Vetalas, Pishachas, Bhūta) are recognized in the later Hindu texts. These beings, in a limited sense, can be called demons.
Second Temple-period texts.
To the Qumran community during the Second Temple Period this apotropaic prayer was assigned, stating: "And, I the Sage, declare the grandeur of his radiance in order to frighten and terri[fy] all the spirits of the ravaging angels and the bastard spirits, demons, Liliths, owls" ("Dead Sea Scrolls", "Songs of the Sage," Lines 4–5).
In the Dead Sea Scrolls, there exists a fragment entitled “Curses of Belial” ("Curses of Belial (Dead Sea Scrolls, 394, 4Q286(4Q287, fr. 6)=4QBerakhot)"). This fragment holds much rich language that reflects the sentiment shared between the Qumran towards Belial. In many ways this text shows how these people thought Belial influenced sin through the way they address him and speak of him. By addressing “Belial and all his guilty lot,” (4Q286:2) they make it clear that he is not only impious, but also guilty of sins. Informing this state of uncleanliness are both his “hostile” and “wicked design” (4Q286:3,4). Through this design, Belial poisons the thoughts of those who are not necessarily sinners. Thus a dualism is born from those inclined to be wicked and those who aren’t. It is clear that Belial directly influences sin by the mention of “abominable plots” and “guilty inclination” (4Q286:8,9). These are both mechanisms by which Belial advances his evil agenda that the Qumran have exposed and are calling upon God to protect them from. There is a deep sense of fear that Belial will “establish in their heart their evil devices” (4Q286:11,12). This sense of fear is the stimulus for this prayer in the first place. Without the worry and potential of falling victim to Belial’s demonic sway, the Qumran people would never feel impelled to craft a curse. This very fact illuminates the power Belial was believed to hold over mortals, and the fact that sin proved to be a temptation that must stem from an impure origin.
In Jubilees 1:20, Belial’s appearance continues to support the notion that sin is a direct product of his influence. Moreover, Belial’s presence acts as a placeholder for all negative influences or those that would potentially interfere with God’s will and a pious existence. Similarly to the “gentiles…[who] cause them to sin against you” (Jubilees 1:19), Belial is associated with a force that drives one away from God. Coupled in this plea for protection against foreign rule, in this case the Egyptians, is a plea for protection from “the spirit of Belial” (Jubilees 1:19). Belial’s tendency is to “ensnare [you] from every path of righteousness” (Jubilees 1:19). This phrase is intentionally vague, allowing room for interpretation. Everyone, in one way or another, finds themselves straying from the path of righteousness and by pawning this transgression off on Belial, he becomes a scapegoat for all misguidance, no matter what the cause. By associating Belial with all sorts of misfortune and negative external influence, the Qumran people are henceforth allowed to be let off for the sins they commit.
Belial’s presence is found throughout the War Scrolls, located in the Dead Sea Scrolls, and is established as the force occupying the opposite end of the spectrum of God. In Col. I, verse 1, the very first line of the document, it is stated that “the first attack of the Sons of Light shall be undertaken against the forces of the Sons of Darkness, the army of Belial” (1Q33;1:1). This dichotomy sheds light on the negative connotations that Belial held at the time. Where God and his Sons of Light are forces that protect and promote piety, Belial and his Sons of Darkness cater to the opposite, instilling the desire to sin and encouraging destruction. This opposition is only reinforced later in the document; it continues to read that the “holy ones” will “strike a blow at wickedness,” ultimately resulting in the “annihilation of the Sons of Darkness” (1Q33:1:13). This epic battle between good and evil described in such abstract terms, however it is also applicable to everyday life and serves as a lens through which the Qumran see the world. Every day is the Sons of Light battle evil and call upon God to help them overcome evil in ways small and large.
Belial’s influence is not taken lightly. In Col. XI, verse 8, the text depicts God conquering the “hordes of Belial” (1Q33;11:8). This defeat is indicative of God’s power over Belial and his forces of temptation. However the fact that Belial is the leader of hordes is a testament to how persuasive he can be. If Belial was obviously an arbiter of wrongdoing and was blatantly in the wrong, he wouldn’t be able to amass an army. This fact serves as a warning message, reasserting God’s strength, while also making it extremely clear the breadth of Belial’s prowess. Belial’s “council is to condemn and convict,” so the Qumran feel strongly that their people are not only aware of his purpose, but also equipped to combat his influence (1Q33;13:11).
In the Damascus Document, Belial also makes a prominent appearance, being established as a source of evil and an origin of several types of sin. In Column 4, the first mention of Belial reads: “Belial shall be unleashed against Israel” (4Q266). This phrase is able to be interpreted myriad different ways. Belial is characterized in a wild and uncontrollable fashion, making him seem more dangerous and unpredictable. The notion of being unleashed is such that once he is free to roam; he is unstoppable and able to carry out his agenda uninhibited. The passage then goes to enumerate the “three nets” (4Q266;4:16) by which Belial captures his prey and forces them to sin. “Fornication…, riches…, [and] the profanation of the temple” (4Q266;4:17,18) make up the three nets. These three temptations were three agents by which people were driven to sin, so subsequently, the Qumran people crafted the nets of Belial to rationalize why these specific temptations were so toxic. Later in Column 5, Belial is mentioned again as one of “the removers of bound who led Israel astray” (4Q266;5:20). This statement is a clear display of Belial’s influence over man regarding sin. The passage goes on to state: “they preached rebellion against…God” (4Q266;5:21,22). Belial’s purpose is to undermine the teachings of God, and he achieves this by imparting his nets on humans, or the incentive to sin.
In the "War Scrolls", Belial controls scores of demons, which are specifically allotted to him by God for the purpose of performing evil. Belial, despite his malevolent disposition, is considered an angel.
Christianity.
Christian Bible.
Old Testament.
Demons in the Old Testament of the Christian Bible are of two classes: the "satyrs" or "shaggy goats" (from Hebr. "se'irim" "hairy beings" and Greek Old Testament σάτυρος "satyros", "satyr"; , ) and the "demons" (from Hebr. "shedim", and Koine Greek δαιμόνιον "daimonion"; , ).
New Testament.
The term "demon" (from the Greek New Testament δαιμόνιον "daimonion") appears 63 times in the New Testament of the Christian Bible.
Pseudepigrapha and Deuterocanonical books.
Demons are sometimes included into biblical interpretation. In the story of Passover, the Bible tells the story as "the Lord struck down all the firstborn in Egypt" (Exodus 12:21–29). In Jubilees, which is considered canonical only by the Ethiopian Orthodox Church, this same event is told slightly differently: "All the powers of [the demon] Mastema had been let loose to slay all the first-born in the land of Egypt...And the powers of the Lord did everything according as the Lord commanded them" (Jubilees 49:2–4).
In Genesis in the story of the flood, the author explains how God was noticing "how corrupt the earth had become, for all the people on earth had corrupted their ways" (Genesis 6:12). In Jubilees the sins of man are attributed to "the unclean demons [who] began to lead astray the children of the sons of Noah, and to make to err and destroy them" (Jubilees 10:1). In Jubilees Mastema questions the loyalty of Abraham and tells God to "bid him offer him as a burnt offering on the altar, and Thou wilt see if he will do this command" (Jubilees 17:16). The discrepancy between the story in Jubilees and the story in Genesis 22 exists with the presence of Mastema. In Genesis, God tests the will of Abraham merely to determine whether he is a true follower, however; in Jubilees Mastema has an agenda behind promoting the sacrifice of Abraham’s son, "an even more demonic act than that of the Satan in Job." In Jubilees, where Mastema, an angel tasked with the tempting of mortals into sin and iniquity, requests that God give him a tenth of the spirits of the children of the watchers, demons, in order to aid the process. These demons are passed into Mastema’s authority, where once again, an angel is in charge of demonic spirits.
The sources of demonic influence were thought to originate from the Watchers or Nephilim, who are first mentioned in Genesis 6 and are the focus of 1 Enoch Chapters 1–16, and also in Jubilees 10. The Nephilim were seen as the source of the sin and evil on earth because they are referenced in Genesis 6:4 before the story of the Flood. In Genesis 6:5, God sees evil in the hearts of men. The passage states, “the wickedness of humankind on earth was great”, and that “Every inclination of the thoughts of their hearts was only continually evil” (Genesis 5). The mention of the Nephilim in the preceding sentence connects the spread of evil to the Nephilim. Enoch is a very similar story to Genesis 6:4–5, and provides further description of the story connecting the Nephilim to the corruption of humans. In Enoch, sin originates when angels descend from heaven and fornicate women, birthing giants as tall as 300 cubits. The giants and the angels’ departure of Heaven and mating with human women are also seen as the source of sorrow and sadness on Earth. The book of Enoch shows that these fallen angels can lead humans to sin through direct interaction or through providing forbidden knowledge. In Enoch, Semyaz leads the angels to mate with women. Angels mating with humans is against God’s commands and is a cursed action, resulting in the wrath of God coming upon Earth. Azazel indirectly influences humans to sin by teaching them divine knowledge not meant for humans. Asael brings down the “stolen mysteries” (Enoch 16:3). Asael gives the humans weapons, which they use to kill each other. Humans are also taught other sinful actions such as beautification techniques, alchemy, astrology and how to make medicine (considered forbidden knowledge at the time). Demons originate from the evil spirits of the giants that are cursed by God to wander the earth. These spirits are stated in Enoch to “corrupt, fall, be excited, and fall upon the earth, and cause sorrow” (Enoch 15:11).
The Book of Jubilees conveys that sin occurs when Cainan accidentally transcribes astrological knowledge used by the Watchers (Jubilees 8). This differs from Enoch in that it does not place blame on the Angels. However in Jubilees 10:4 the evil spirits of the Watchers are discussed as evil and still remain on earth to corrupt the humans. God binds only 90 percent of the Watchers and destroys them, leaving 10 percent to be ruled by Mastema. Because the evil in humans is great, only 10 percent would be needed to corrupt and lead humans astray. These spirits of the giants also referred to as “the bastards” in the Apotropaic prayer Songs of the Sage, which lists the names of demons the narrator hopes to expel.
Christian demonology.
In the Christian Bible, the deities of other religions are sometimes interpreted or created as "demons" (from the Greek Old Testament δαιμόνιον "daimonion"). The evolution of the Christian Devil and pentagram are examples of early rituals and images that showcase evil qualities, as seen by the Christian churches.
Since Early Christianity, demonology has developed from a simple acceptance of demons to a complex study that has grown from the original ideas taken from Jewish demonology and Christian scriptures. Christian demonology is studied in depth within the Roman Catholic Church, although many other Christian churches affirm and discuss the existence of demons.
Building upon the few references to "daemons" in the New Testament, especially the poetry of the Book of Revelation, Christian writers of apocrypha from the 2nd century onwards created a more complicated tapestry of beliefs about "demons" that was largely independent of Christian scripture.
The contemporary Roman Catholic Church unequivocally teaches that angels and demons are real beings rather than just symbolic devices. The Catholic Church has a cadre of officially sanctioned exorcists which perform many exorcisms each year. The exorcists of the Catholic Church teach that demons attack humans continually but that afflicted persons can be effectively healed and protected either by the formal rite of exorcism, authorized to be performed only by bishops and those they designate, or by prayers of deliverance, which any Christian can offer for themselves or others.
At various times in Christian history, attempts have been made to classify demons according to various proposed demonic hierarchies.
In the Gospels, particularly the Gospel of Mark, Jesus cast out many demons from those afflicted with various ailments. He also lent this power to some of his disciples ().
Apuleius, by Augustine of Hippo, is ambiguous as to whether "daemons" had become 'demonized' by the early 5th century:
Islam.
Islam recognizes the existence of jinn, which are sentient beings with free will that can co-exist with humans (though not the genies of modern lore). In Islam, evil jinn are referred to as the "shayātīn" or demons/devils, with Iblis (Satan) as their chief. Iblis was one of the first jinn; he disobeyed Allah and did not bow down before Adam refusing to acknowledge a creature made of "clay". Thus, Iblis was condemned to jahannam (hell). He asked for respite until the Last Day (Judgement Day), when he vowed to make mankind fall and deny the existence of their creator, to which Allah replied that Iblis would only be able to mislead those who were not righteous believers, warning that Iblis and all who followed him in evil would be punished in Hell.
Bahá'í Faith.
In the Bahá'í Faith, demons are not regarded as independent evil spirits as they are in some faiths. Rather, evil spirits described in various faiths' traditions, such as Satan, fallen angels, demons and jinns, are metaphors for the base character traits a human being may acquire and manifest when he turns away from God and follows his lower nature. Belief in the existence of ghosts and earthbound spirits is rejected and considered to be the product of superstition.
Modern interpretations.
According to the 'Hong Kong Journal of Psychiatry' God is shown sending a demon against Saul in 1 Samuel 16 and 18 in order to punish him for the failure to follow God’s instructions, showing God as having the power to use demons for his own purposes, putting the demon under his divine authority. According to the 'Britannica Concise Encyclopedia', demons, despite being typically associated with evil, are often shown to be under divine control, and not acting of their own devices.
Ceremonial magic.
While some people fear demons, or attempt to exorcise them, others willfully attempt to summon them for knowledge, assistance, or power. The ceremonial magician usually consults a grimoire, which gives the names and abilities of demons as well as detailed instructions for conjuring and controlling them. Grimoires aren't limited to demons - some give the names of angels or spirits which can be called, a process called theurgy. The use of ceremonial magic to call demons is also known as goetia, the name taken from a section within the famous grimoire "The Lesser Key of Solomon".
Wicca.
According to Rosemary Ellen Guiley, "Demons are not courted or worshipped in contemporary Wicca and Paganism. The existence of negative energies is acknowledged."

</doc>
<doc id="8286" url="http://en.wikipedia.org/wiki?curid=8286" title="Domino effect">
Domino effect

A domino effect or chain reaction is the cumulative effect produced when one event sets off a chain of similar events. The term is best known as a mechanical effect, and is used as an analogy to a falling row of dominoes. It typically refers to a linked sequence of events where the time between successive events is relatively small. It can be used literally (an observed series of actual collisions) or metaphorically (causal linkages within systems such as global finance or politics).
See also.
Relevant physical theory:
Mathematical theory
Political theory

</doc>
<doc id="8293" url="http://en.wikipedia.org/wiki?curid=8293" title="Diffusion pump">
Diffusion pump

Diffusion pumps use a high speed jet of vapor to direct gas molecules in the pump throat down into the bottom of the pump and out the exhaust. Invented in 1915 by Wolfgang Gaede using mercury vapor, and improved by Irving Langmuir and W. Crawford, they were the first type of high vacuum pumps operating in the regime of free molecular flow, where the movement of the gas molecules can be better understood as diffusion than by conventional fluid dynamics. Gaede used the name diffusion pump since his design was based on the finding that gas cannot diffuse against the vapor stream, but will be carried with it to the exhaust. However, the principle of operation might be more precisely described as gas-jet pump, since diffusion plays a role also in other high vacuum pumps. In modern text books, the diffusion pump is categorized as a momentum transfer pump. The diffusion pump is widely used in both industrial and research applications. Most modern diffusion pumps use silicone oil or polyphenyl ethers as the working fluid. Cecil Reginald Burch discovered the possibility of using silicone oil in 1928.
Oil diffusion pumps.
The oil diffusion pump is operated with an oil of low vapor pressure. Its purpose is to achieve higher vacuum (lower pressure) than is possible by use of positive displacement pumps alone. Although its use has been mainly associated within the high-vacuum range (down to 10−9 mbar), diffusion pumps today can produce pressures approaching 10−10 mbar when properly used with modern fluids and accessories. The features that make the diffusion pump attractive for high and ultra-high vacuum use are its high pumping speed for all gases and low cost per unit pumping speed when compared with other types of pump used in the same vacuum range. Diffusion pumps cannot discharge directly into the atmosphere, so a mechanical forepump is typically used to maintain an outlet pressure around 0.1 mbar.
The high speed jet is generated by boiling the fluid and directing the vapor through a jet assembly. Note that the oil is gaseous when entering the nozzles. Within the nozzles, the flow changes from laminar, to supersonic and molecular. Often several jets are used in series to enhance the pumping action. The outside of the diffusion pump is cooled using either air flow or a water line. As the vapor jet hits the outer cooled shell of the diffusion pump, the working fluid condenses and is recovered and directed back to the boiler. The pumped gases continue flowing to the base of the pump at increased pressure, flowing out through the diffusion pump outlet, where they are compressed to ambient pressure by the secondary mechanical forepump and exhausted. 
Unlike turbomolecular pumps and cryopumps, diffusion pumps have no moving parts and as a result are quite durable and reliable. They can function over pressure ranges of 10−10 to 10−2 mbar. They are driven only by convection and thus have a very low energy efficiency.
One major disadvantage of diffusion pumps is the tendency to backstream oil into the vacuum chamber. This oil can contaminate surfaces inside the chamber or upon contact with hot filaments or electrical discharges may result in carbonaceous or siliceous deposits. Due to backstreaming, oil diffusion pumps are not suitable for use with highly sensitive analytical equipment or other applications which require an extremely clean vacuum environment, but mercury diffusion pumps may be in the case of ultra high vacuum chambers used for metal deposition. Often cold traps and baffles are used to minimize backstreaming, although this results in some loss of pumping ability.
The oil of a diffusion pump cannot be exposed to the atmosphere when hot. If this occurs, the oil will burn and has to be replaced.
Steam ejectors.
 
The steam ejector is a popular form of diffusion pump for vacuum distillation and freeze-drying. A jet of steam entrains the vapour that must be removed from the vacuum chamber. Steam ejectors can have a single or multiple stages, with and without condensers in between the stages.
Compressed-air ejectors.
One class of diffusion vacuum pumps is the multistage compressed-air driven ejector. It is very popular in applications where objects are moved around using suction cups and vacuum lines.

</doc>
<doc id="8295" url="http://en.wikipedia.org/wiki?curid=8295" title="Declarative memory">
Declarative memory

Declarative memory (sometimes referred to as explicit memory) is one of two types of long-term human memory. Declarative memory refers to memories that can be consciously recalled such as facts and knowledge. While declarative memory is similar to explicit memory, declarative memory is memory that a patient can state in words, while explicit memory is the deliberate recall of information that the patient recognizes as a memory. Declarative memory's counterpart is known as non-declarative or procedural memory, which refers to unconscious memories such as skills (e.g. learning to ride a bicycle). Declarative memory can be divided into two categories: "episodic memory", which stores specific personal experiences, and "semantic memory", which stores factual information.
Types.
Semantic memories are those memories that store general factual knowledge that is independent of personal experience. This includes world knowledge, object knowledge, language knowledge, and conceptual priming. Some examples of semantic memory include types of food, capital cities of a geographic region, or the lexicon of a common language, such as a person's vocabulary.
Episodic memories are those memories that store chunks of observational information attached to a specific event. Some examples of episodic memory include the memory of entering a specific classroom for the first time, the memory of storing your carry-on baggage while boarding a plane headed to a specific destination on a specific day and time, the memory of being notified that you are being terminated from your job, or the memory of notifying a subordinate that they are being terminated from their job. The retrieval of these episodic memories can be thought of as the action of mentally reliving in detail the past events that they concern. Episodic memory is believed to be the system that provides the basic support for semantic memory.
History.
The study of human memory stretches back over the last 2000 years. An early attempt to understand memory can be found in Aristotle’s major treatise, On the Soul, in which he compares the human mind to a blank slate. He theorized that all humans are born free of any knowledge and are the sum of their experiences. It wasn’t until the late 1800s, however, that a young German philosopher by the name of Herman Ebbinghaus developed the first scientific approach to studying memory. While some of his findings have endured and remain relevant to this day (Learning Curve), his greatest contribution to the field of memory research was demonstrating that memory can be studied scientifically. In 1972, Endel Tulving proposed the distinction between episodic and semantic memory. This was quickly adopted and is now widely accepted. Following this, in 1985, Daniel Schacter proposed a more general distinction between explicit (declarative) and implicit (procedural) memory With the recent advances in neuroimaging technology, there have been a multitude of findings linking specific brain areas to declarative memory. Despite these advances in Cognitive psychology, there is still much to be discovered in terms of the operating mechanisms of declarative memory. It is unclear whether declarative memory is mediated by a particular “memory system” or if it is more accurately classified as a “type of knowledge” and it is not known how or why declarative memory evolved to begin with.
Neuropsychology.
Normal brain function.
Hippocampus.
Although many psychologists believe that the entire brain is involved with memory, the "hippocampus" and surrounding structures appear to be most important in declarative memory specifically. The ability to retain and recall episodic memories is highly dependent on the hippocampus, whereas the formation of new declarative memories relies on both the hippocampus and "parahippocampus" Other studies have found that the parahippocampal cortices were related to superior "Recognition Memory".
The Three Stage Model was developed by Eichenbaum, et. Al (2001), and proposes that the hippocampus does three things with episodic memory:
To support this model, a version of Piaget’s Transitive Inference Task was used to show that the hippocampus is in fact used as the memory space.
When experiencing an event for the first time, a link is formed in the hippocampus allowing us to recall that event in the future. Separate links are also made for features related to that event. For example, when you meet someone new, a unique link is created for them. More links are then connected to that person’s link so you can remember what colour their shirt was, what the weather was like when you met them, etc. Specific episodes are made easier to remember and recall by repeatedly exposing oneself to them (which strengthens the links in the memory space) allowing for faster retrieval when remembering.
Hippocampal cells ("neurons") are activated depending on what information one is exposed to at that moment. Some cells are specific to spatial information, certain stimuli (smells, etc.), or behaviours as has been shown in a "Radial Maze Task". It is therefore the hippocampus that allows us to recognize certain situations, environments, etc. as being either distinct or similar to others. However, the Three Stage Model does not incorporate the importance of other cortical structures in memory.
The anatomy of the hippocampus is largely conserved across mammals, and the role of these areas in declarative memory are conserved across species as well. The organization and neural pathways of the hippocampus are very similar in humans and other mammal species. In humans and other mammals, a cross-section of the hippocampus shows the dentate gyrus as well as the dense cell layers of the CA fields. The intrinsic connectivity of these areas are also conserved.
Results from an experiment by Davachi, Mitchell, and Wagner (2003) and numerous subsequent studies (Davachi, 2006) show that activation in the hippocampus during encoding is related to a subject's ability to recall prior events or later relational memories. These tests did not differentiate between individual test items later seen and those forgotten.
Prefrontal cortex.
The lateral Prefrontal cortex (PFC) is essential for remembering contextual details of an experience rather than for memory formation. The PFC is also more involved with episodic memory than semantic memory, although it does play a small role in semantics.
Using PET studies and word stimuli, Endel Tulving found that remembering is an automatic process. It is also well documented that a hemispheric asymmetry occurs in the PFC: When encoding memories, the Left Dorsolateral PFC (LPFC) is activated, and when retrieving memories, activation is seen in the Right Dorsolateral PFC (RPFC).
Studies have also shown that the PFC is extremely involved with autonoetic consciousness (See Tulving's theory). This is responsible for humans’ recollective experiences and ‘mental time travelling’ abilities (characteristics of episodic memory).
Amygdala.
The amygdala is believed to be involved in the encoding and retrieval of emotionally charged memories. Much of the evidence for this has come from research on a phenomenon known as flashbulb memories. These are instances in which memories of powerful emotional events are more highly detailed and enduring than regular memories (e.g. September 11 attacks, assassination of JFK). These memories have been linked to increased activation in the amygdala. Recent studies of patients with damage to the amygdala suggest that it is involved in memory for general knowledge, and not for specific information.
Other structures involved.
The regions of the Diencephalon have shown brain activation when a remote memory is being recovered and the Occipital lobe, Ventral Temporal lobe, and Fusiform gyrus all play a role in memory formation.
Lesion studies.
Lesion studies are commonly used in cognitive neuroscience research. Lesions can occur naturally through trauma or disease, or they can be surgically induced by researchers. In the study of declarative memory, the hippocampus and the amygdala are two structures frequently examined using this technique.
Hippocampal lesion studies.
The "Morris water navigation task" tests spatial learning in rats. In this test rats learn to escape from a pool by swimming toward a platform submerged just below the surface of the water. Visual cues that surround the pool (i.e. Chair or window) help the rat to locate the platform on subsequent trials. The rats' use of specific events, cues and places are all forms of declarative memory. Two groups of rats are observed: a control group with no lesions and an experimental group with hippocampal lesions. In this task created by Morris, "et al.", rats are placed in the pool at the same position for 12 trials. Each trial is timed and the path taken by the rats is recorded. Rats with hippocampal lesions successfully learn to find the platform. If the starting point is moved, the rats with hippocampal lesions typically fail to locate the platform. The control rats, however, are able to find the platform using the cues acquired during the learning trials. This demonstrates the involvement of the hippocampus in declarative memory.
The "Odor-odor Recognition Task", devised by Bunsey and Eichenbaum, involves a social encounter between two rats (a "subject" and a "demonstrator"). The demonstrator, after eating a specific type of food, interacts with the subject rat, who then smells the food odor on the other's breath. The experimenters then present the subject rat with a decision between two food options; the food previously eaten by the demonstrator, and a novel food. The researchers found that when there was no time delay, both control rats and rats with lesions chose the familiar food. After 24 hours, however, the rats with hippocampal lesions were just as likely to eat both types of food, while control rats chose the familiar food. This can be attributed to the inability to form episodic memories due to lesions in the hippocampus. The effects of this study can be observed in humans with amnesia, indicating the role of the hippocampus in developing episodic memories that can be generalized to similar situations.
Henry Molaison, previously known as H.M., had parts of both his left and right medial temporal lobes(hippocampi) removed which resulted in the loss of the ability to form new memories. The long-term declarative memory was crucially affected when the structures from the medial temporal lobe were removed, including the ability to form new semantic knowledge and memories. The dissociation in Molaison between the acquisition of declarative memory and other kinds of learning was seen initially in motor learning. Molaison's declarative memory was not functioning, as was seen when Molaison completed the task of repetition priming. His performance does improve over trials, however, his scores were inferior to those of control participants. In the condition of Molaison the same results from this priming task are reflected when looking at the other basic memory functions like remembering, recall and recognizing. Lesions should not be interpreted as an all-or-nothing condition, in the case of Molaison not all memory and recognition is lost, although the declarative memory is severely damaged he still has a sense of self and memories that were developed before the lesion occurred.
Patient R.B. was another clinical case reinforcing the role of the hippocampus in declarative memory. After suffering an ischemic episode during a cardiac bypass operation, Patient R.B. awoke with a severe anterograde amnesic disorder. IQ and cognition were unaffected, but declarative memory deficits were observed (although not to the extent of that seen in Molaison). Upon death, an autopsy revealed that Patient R.B. had bilateral lesions of the CA1 cell region along the whole length of the hippocampus.
Amygdala lesion studies.
Adolph, Cahill and Schul completed a study showing that emotional arousal facilitates the encoding of material into long term declarative memory. They selected two subjects with bilateral damage to the amygdala, as well as six control subjects and six subjects with brain damage. All subjects were shown a series of twelve slides accompanied by a narrative. The slides varied in the degree to which they evoked emotion - slides 1 through 4 and slides 9 through 12 contain non-emotional content. Slides 5 through 8 contain emotional material, and the seventh slide contained the most emotionally arousing image and description (a picture of surgically repaired legs of a car crash victim).
The emotionally arousing slide (slide 7) was remembered no better by the bilateral damage participants than any of the other slides. All other participants notably remembered the seventh slide the best and in most detail out of all the other slides. This shows that the amygdala is necessary to facilitate encoding of declarative knowledge regarding emotionally arousing stimuli, but is not required for encoding knowledge of emotionally neutral stimuli.
Factors that affect declarative memory.
Stress.
Stress may have an effect on the recall of declarative memories. Lupien, et al. completed a study that had 3 phases for participants to take part in. Phase 1 involved memorizing a series of words, phase 2 entailed either a stressful (public speaking) or non-stressful situation (an attention task), and phase 3 required participants to recall the words they learned in phase 1. There were signs of decreased declarative memory performance in the participants that had to complete the stressful situation after learning the words. Recall performance after the stressful situation was found to be worse overall than after the non-stressful situation, where performance differed based on whether the participant responded to the stressful situation with an increase in measured levels of salivary cortisol.
Posttraumatic stress disorder (PTSD) emerges after exposure to a traumatic event eliciting fear, horror or helplessness that involves bodily injury, the threat of injury, or death to one’s self or another person The chronic stress in PTSD contributes to an observed decrease in hippocampal volume and declarative memory deficits.
Neurochemical factors of stress on the brain.
In the brain, Glucocorticoids (GC's) modulate the ability of the hippocampus and PFC to process memories. Cortisol is one of the most common GC’s in the human body, and hydrocortisone (a derivative of cortisol) decreases brain activity in the above areas during declarative memory retrieval.
Elevations in cortisol occur during stress, and long-term stress impairs declarative memory this way. A study done by Damoiseaux et al. (2007) evaluated the effect of glucocorticoids on MTL and PFC activation in young men. They found that GC’s given to participants 1 hour before retrieval of information impairs free recall of words, yet when administered before or after learning they had no effect. Although it is not known exactly how GC’s influence memory, there are Glucocorticoid receptors in the hippocampus and PFC that tell us these structures are targets for the circulating hormone. However, it is known that cortisone impairs memory function by reducing the blood flow in the right parahippocampal gyrus, left visual cortex, and the Cerebellum.
Note: This study involved only male subjects, which may be significant as sex steroids may have different effects in the responses to cortisol administration. Men and women also respond to emotional stimuli differently, and this may affect cortisol levels. Also, this study was the first Functional magnetic resonance imaging(fMRI) study to be done involving GC's and more research is necessary to support these findings.
Declarative memory consolidation during sleep.
It is believed that sleep plays an active role in consolidation of declarative memory. Specifically, sleep’s unique properties enhance "memory consolidation", such as the reactivation of newly learned memories during sleep. For example, it has been suggested that the central mechanism for consolidation of declarative memory during sleep is the reactivation of hippocampal memory representations. This reactivation transfers information to neocortical networks where it is integrated into long-term representations. Studies on rats involving maze learning found that hippocampal neuronal assemblies that are used in the encoding of spatial information are reactivated in the same temporal order. Similarly, positron emission tomography (PET) has shown reactivation of the "hippocampus" in slow-wave sleep (SWS) after spatial learning. Together these studies show that newly learned memories are reactivated during sleep and through this process new memory traces are consolidated. In addition, researchers have identified three types of sleep (SWS, sleep spindle and REM) in which declarative memory is consolidated.
Slow-Wave Sleep, often referred to as deep sleep, plays the most important role in consolidation of declarative memory and there is a large amount of evidence to support this claim. One study found that the first 3.5 hours of sleep offer the greatest performance enhancement on memory recall tasks because the first couple of hours are dominated by SWS. Additional hours of sleep do not add to the initial level of performance. Thus this study suggests that full sleep may not be important for optimal performance of memory. Another study shows that people who experience SWS during the first half of their sleep cycle compared to subjects who did not, showed better recall of information. However this is not the case for subjects who were tested for the second half of their sleep cycle, as they experience less SWS.
Another key piece of evidence regarding SWS’s involvement in declarative memory consolidation is a finding that people with pathological conditions of sleep, such as insomnia, exhibit both reduction in "Slow-Wave Sleep" and also have impaired consolidation of declarative memory during sleep. Another study found that middle aged people compared to young group had a worse retrieval of memories. This in turn indicated that SWS is associated with poor declarative memory consolidation but not with age itself.
Some researchers suggest that "sleep spindle", a burst of brain activity occurring during stage 2 sleep, plays a role in boosting consolidation of declarative memories. Critics point out that spindle activity is positively correlated with intelligence. In contrast, Schabus and Gruber point out that sleep spindle activity only relates to performance on newly learned memories and not to absolute performance. This supports the hypothesis that sleep spindle helps to consolidate recent memory traces but not memory performance in general. The relationship between sleep spindles and declarative memory consolidation is not yet fully understood.
There is a relatively small body of evidence that supports the idea that "REM sleep" helps consolidate highly emotional declarative memories. For instance Wagner, et al. compared memory retention for emotional versus neutral text over two instances; early sleep that is dominated by SWS and late sleep that is dominated by REM phase. This study found that sleep improved memory retention of emotional text only during late sleep phase, which was primarily REM. Similarly, Hu & Stylos-Allen, et al. performed a study with emotional versus neutral pictures and concluded that REM sleep facilitates consolidation of emotional declarative memories.
The view that sleep plays an "active" role in declarative memory consolidation is not shared by all researchers. For instance Ellenbogen, et al. argue that sleep actively protects declarative memory from associative interference. Furthermore, Wixted believes that the sole role of sleep in declarative memory consolidation is nothing more but creating ideal conditions for memory consolidation. For example, when awake, people are bombarded with mental activity which interferes with effective consolidation. However, during sleep, when interference is minimal, memories can be consolidated without associative interference. More research is needed to make a definite statement whether sleep creates favourable conditions for consolidation or it actively enhances declarative memory consolidation.
In popular culture.
Amnesiacs are frequently portrayed in television and movies. Some of the better-known examples include:
In the romantic comedy "50 First Dates" (2004), Adam Sandler plays veterinarian Henry Roth, who falls for Lucy Whitmore, played by Drew Barrymore. Having lost her short term memory in a car crash, Lucy can only remember the current day's events until she falls asleep. When she wakes up the next morning, she has no recollection of the previous day's experiences. These experiences would normally be transferred into declarative knowledge, allowing them to be recalled in the future. Although this movie is not the most accurate representation of a true amnesic patient, it is useful for informing viewers of the detrimental effects of amnesia.
"Memento" (2000) a film inspired by the case of Henry Molaison (H.M.). Guy Pearce plays an ex-insurance investigator suffering from severe anterograde amnesia caused by a head injury. Unlike most amnesiacs, Leonard retains his identity and the memories of events that occurred before the injury, but loses all ability to form new memories. This loss of ability to form new memories indicates that the head injury affected the medial temporal lobe of the brain resulting in the inability for Leonard to form declarative memory.
"Finding Nemo" features a reef fish named Dory with an inability to develop declarative memory. This prevents her from learning or retaining any new information such as names or directions. The exact origin of Dory's impairment is not mentioned in the film, but her memory loss accurately portrays the difficulties facing amnesiacs.

</doc>
<doc id="8299" url="http://en.wikipedia.org/wiki?curid=8299" title="Domenico Alberti">
Domenico Alberti

Domenico Alberti (c. 1710 – 14 October 1740) was an Italian singer, harpsichordist, and composer.
Alberti was born in Venice and studied music with Antonio Lotti. He wrote operas, songs, and sonatas for keyboard instruments, for which he is best known today. These sonatas frequently employ a particular kind of arpeggiated accompaniment in the left hand that is now known as the "Alberti bass". It consists of regular broken chords, with the lowest note sounding first, then the highest, then the middle and then the highest again. This pattern is repeated. Today, Alberti is regarded as a minor composer, and his works are played or recorded only irregularly. The Alberti bass was used by many later composers, and it became an important element in much keyboard music of the Classical music era.
An example of Alberti bass (Mozart's "Piano Sonata, K 545"):
In his own lifetime, Alberti was known as a singer. He often used to accompany himself on the harpsichord. In 1736, he served as a page for Pietro Andrea Cappello, the Venetian ambassador to Spain. While at the Spanish court, the famous castrato singer Farinelli heard him sing. Farinelli was said to have been impressed, although Alberti was an amateur.
Alberti's best known pieces are his keyboard sonatas, although even they are very rarely performed. It is thought he wrote around 36 sonatas, of which 14 have survived. They all have two movements, each in binary form.
It is probable that Mozart's first violin sonatas, written at the age of seven, were modeled on Alberti's work.
Alberti died in 1740 in Rome.

</doc>
<doc id="8300" url="http://en.wikipedia.org/wiki?curid=8300" title="Doris Day">
Doris Day

Doris Day (born Doris Mary Ann Kappelhoff; April 3, 1922 or 1924) is an American actress, singer, and animal rights activist.
Day began her career as a big band singer in 1939. Her popularity began to rise after her first hit recording "Sentimental Journey", in 1945. After leaving Les Brown & His Band of Renown to embark on a solo career, Day started her long-lasting partnership with Columbia Records, which remained her only recording label. The contract lasted from 1947 to 1967 and included more than 650 recordings, making Day one of the most popular and acclaimed singers of the 20th century. In 1948, after being persuaded by songwriters Sammy Cahn and Jule Styne and by Al Levy, her agent at the time, she auditioned for film director Michael Curtiz, which led to her being cast as the female lead in "Romance on the High Seas".
Over the course of her career, Day appeared in 39 films. She was ranked the biggest box-office star, the only woman appearing on that list in the era, for four years (1960, 1962, 1963 and 1964), ranking in the top 10 for ten years (1951–1952 and 1959–1966). She became the top-ranking female box-office star of all time and is currently ranked sixth among the top 10 box office performers (male and female), as of 2012. Day received an Academy Award nomination for her performance in "Pillow Talk", won three Henrietta Awards (World Film Favorite), received the Los Angeles Film Critics Association's Career Achievement Award and in 1989, she received the Cecil B. DeMille Award for lifetime achievement in motion pictures. She made her last film in 1968.
She received the Grammy Lifetime Achievement Award and a Legend Award from the Society of Singers. In 2011, she released her 29th studio album, "My Heart", which debuted at No. 9 on the UK Top 40 charts. As of January 2014, Day is the oldest living artist to score a UK Top 10 with an album featuring new material.
Her strong commitment to animal welfare began in 1971, when she co-founded "Actors and Others for Animals". She started her own non-profit organization in the late 1970s, the Doris Day Animal Foundation and, later, the Doris Day Animal League (DDAL). Establishing the annual observance Spay Day USA in 1994, the Doris Day Animal League now partners with The Humane Society of the United States and continues to be a leading advocacy organization. In 2004, she received the Presidential Medal of Freedom from President George W. Bush in recognition of her distinguished service to the country. Day is retired from acting and performing, but has continued her work in animal rights causes and animal welfare.
Early life.
Doris Mary Ann Kappelhoff was born in Cincinnati, Ohio, to Alma Sophia (née Welz, a housewife) and Frederick Wilhelm von Kappelhoff (a music teacher and choir master) on April 3, 1922, although she has given the year as 1924. All of her grandparents were German immigrants.
The youngest of three siblings, she had two older brothers: Richard (who died before her birth) and Paul, several years older. Due to her father's alleged infidelity, her parents separated. She developed an early interest in dance, and in the mid-1930s formed a dance duo with Jerry Doherty that performed locally in Cincinnati. A car accident on October 13, 1937, injured her legs and curtailed her prospects as a professional dancer.
Career.
Early career (1938–1947).
While recovering, Day started to sing along with the radio and discovered a talent that she didn't know she had. Day said: "During this long, boring period, I used to while away a lot of time listening to the radio, sometimes singing along with the likes of Benny Goodman, Duke Ellington, Tommy Dorsey and Glenn Miller [...]. But the one radio voice I listened to above others belonged to Ella Fitzgerald. There was a quality to her voice that fascinated me, and I'd sing along with her, trying to catch the subtle ways she shaded her voice, the casual yet clean way she sang the words." 
Observing her daughter rekindled Alma's interest in show business, and she decided to give Doris singing lessons. She engaged a teacher, Grace Raine. After three lessons, Raine told Alma that Doris had "tremendous potential", which led Alma to give her daughter three lessons a week for the price of one. Years later, Day said that Raine had the biggest effect on her singing style and career. 
During the eight months she was taking singing lessons, Day had her first professional jobs as a vocalist, on the WLW radio program "Carlin's Carnival", and in a local restaurant, Charlie Yee's Shanghai Inn. It was during her radio performances that Day first caught the attention of Barney Rapp, who was looking for a girl vocalist and asked if Day would like to audition for the job. According to Rapp, he had auditioned about 200 singers when Day got the job.
While working for Rapp in 1939, she adopted the stage surname "Day", at Rapp's suggestion. Rapp felt that "Kappelhoff" was too long for marquees, and he admired her rendition of the song "Day After Day". After working with Rapp, Day worked with bandleaders Jimmy James, Bob Crosby, and Les Brown.
While working with Brown, Day scored her first hit recording, "Sentimental Journey", released in early 1945. It soon became an anthem of the desire of World War II demobilizing troops to return home. This song is still associated with Day, and she re-recorded it on several occasions, including a version in her 1971 television special. At one point in 1945–46, Day (as vocalist with the Les Brown Band) had six other Top Ten hits on the Billboard chart: "My Dreams Are Getting Better All the Time", "'Tain't Me", "Till The End of Time", "You Won't Be Satisfied (Until You Break My Heart)", "The Whole World is Singing My Song", and "I Got the Sun in the Mornin'". By the time she left Brown's band in August 1946, she was the highest paid female band vocalist in the world. 
Early film career (1948–1954).
While singing with the Les Brown band and for nearly two years on Bob Hope's weekly radio program, she toured extensively across the United States. Her popularity as a radio performer and vocalist, which included a second hit record "My Dreams Are Getting Better All the Time", led directly to a career in films. In 1941, Day appeared as a singer with the Les Brown band in a soundie (a Cinemasters production). During her separation from her second husband, George Weidler, in 1947, Day reportedly intended to leave Los Angeles and return to Cincinnati. Her agent Al Levy convinced her to attend a party at the home of composer Jule Styne. Her performance of the song "Embraceable You" impressed Styne and his partner, Sammy Cahn, and they recommended her for a role in "Romance on the High Seas", which they were working on for Warner Brothers. The withdrawal of Betty Hutton due to pregnancy left the main role to be re-cast, and Day got the part after auditioning for Michael Curtiz. The film provided her with her first #1 hit recording as a soloist, "It's Magic," which followed by two months her first #1 hit ("Love Somebody" in 1948) recorded as a duet with Buddy Clark. 
In 1950, U.S. servicemen in Korea voted her their favorite star. She continued to make minor and frequently nostalgic period musicals such as "Starlift", "The West Point Story", "On Moonlight Bay", "By the Light of the Silvery Moon", and "Tea For Two" for Warner Brothers. Her most commercially successful film for Warners was "I'll See You in My Dreams," which broke box-office records of 20 years' standing during its premiere engagement at Radio City Music Hall in 1951. In 1953, Day appeared as the title character in the comedic western-themed musical, "Calamity Jane", winning the Academy Award for Best Original Song for "Secret Love" (her recording of which became her fourth #1 hit single in the U.S.). Between 1950 and 1953, the albums from six of her movie musicals charted in the Top 10, three of them at #1. After filming "Lucky Me" with Phil Silvers and "Young at Heart" (both 1954) with Frank Sinatra, Day chose not to renew her contract with Warner Brothers. She elected to work under the advice and management of her third husband, Marty Melcher, whom she married in Burbank on April 3, 1951.
Motion picture breakthrough (1955–1958).
Day appeared as a mystery guest on "What's My Line?" on January 23, 1955. Day subsequently took on more dramatic roles, including her 1955 portrayal of singer Ruth Etting in the biographical film of Etting's life, "Love Me or Leave Me", in which she co-starred with James Cagney. 
Day would later call it, in her autobiography, her best film. The film garnered critical and commercial success, becoming Day's biggest hit so far. Producer Joe Pasternak said, "I was stunned that Doris didn't get an Oscar nomination." The sound track album from that movie was a #1 hit that stayed charted for 28 weeks and became the recording industry's third biggest selling album of the entire decade. Day starred in Alfred Hitchcock's suspense film, "The Man Who Knew Too Much" (1956) with James Stewart. She sang only two songs in the film, "Que Sera, Sera (Whatever Will Be, Will Be)", which won an Academy Award for Best Original Song, and "We'll Love Again". During the filming, Day became concerned about Hitchcock's lack of direction. She recalled being worried if she was pleasing him and confronted him on her performance. He told her, "If you weren't doing what I liked, you'd know." At the premiere, Hitchcock was asked how he got such a great performance from Day. He replied, "It wasn't me; it was Doris." The film was Day's 10th movie to be in the Top 10 at the box office. Day played the title role in the thriller/noir "Julie" (1956) with Louis Jourdan. The film received poor press acclaim and was unpopular with audiences.
After three successive dramatic films, Day returned to her musical/comedic roots in 1957's "The Pajama Game" with John Raitt. The film was based on the Broadway play of the same name. She worked with Paramount Pictures for the comedy "Teacher's Pet" (1958), alongside Clark Gable and Gig Young. She co-starred with Richard Widmark and Gig Young in the romantic comedy film, "The Tunnel of Love" (1958), but found scant success opposite Jack Lemmon in "It Happened to Jane" (1959). Billboard's annual nationwide poll of disc jockeys had ranked Day as the #1 female vocalist nine times in ten years (1949 through 1958), but her success and popularity as a singer was now being overshadowed by her box office appeal.
Box office success (1959–1968).
In 1959, Day entered her most successful phase as a film actress with a series of romantic comedies. This success began with "Pillow Talk" (1959), co-starring Rock Hudson, who became a lifelong friend, and Tony Randall. Day received a nomination for an Academy Award for Best Actress. Day, Hudson, and Randall made two more films together, "Lover Come Back" (1961) and "Send Me No Flowers" (1964). In 1962, Day appeared with Cary Grant in "That Touch of Mink", the first film in history ever to gross $1 million in one theatre (Radio City Music Hall). Day was in the Top 10 at the box office 10 times. During 1960 and the 1962 to 1964 period, she ranked No. 1 at the box office, the only woman to be #1 four times. She set an unprecedented record that has yet to be equaled, receiving seven consecutive Laurel Awards as the top female box office star.
Day teamed up with James Garner, starting with "The Thrill of It All", followed by "Move Over, Darling" (both 1963). "Move Over, Darling" was originally titled "Something's Got to Give", a 1962 comeback vehicle for Marilyn Monroe. Filming was suspended following Monroe's dismissal and her subsequent death. A year later, filming resumed with Day recast as the leading lady. This was the 21st and final of Day's 39 movies to be in the Top 10 at the box office. The film's theme song, "Move Over, Darling", was co-written by her son specifically for her and charted at #8 in the U.K. In between these comedic roles, Day co-starred with Rex Harrison in the movie thriller "Midnight Lace", an updating of the classic stage thriller, "Gaslight".
By the late 1960s, the sexual revolution of the baby boomer generation had refocused public attitudes about sex. Times changed, but Day's films did not. Day's 1965 film, "Do Not Disturb", was a box office failure and was unpopular with critics as well. Critics and comics dubbed Day "The World's Oldest Virgin", and audiences began to shy away from her films. As a result, she slipped from the list of top box office stars, last appearing in the top ten in 1966 with the hit film "The Glass Bottom Boat". One of the roles she turned down was that of "Mrs. Robinson" in "The Graduate", a role that eventually went to Anne Bancroft. In her published memoirs, Day said she had rejected the part on moral grounds: she found the script "vulgar and offensive".
She starred in the western film "The Ballad of Josie" (1967). That same year, Day recorded "The Love Album", although it was not released until 1994. The following year (1968), she starred in the comedy film "Where Were You When the Lights Went Out?" which centers on the Northeast blackout of November 9, 1965. Her final feature, the comedy "With Six You Get Eggroll", was released in 1968. From 1959 through 1970, Day received nine Laurel Award nominations (and won four times) for best female performance in eight comedies and one drama. From 1959 through 1969, she received six Golden Globe nominations for best female performance in three comedies, one drama ("Midnight Lace"), one musical ("Jumbo"), and her television series.
Bankruptcy and television career.
When her third husband Martin Melcher died on April 20, 1968, a shocked Day discovered that Melcher and his business partner Jerome Bernard Rosenthal had squandered her earnings, leaving her deeply in debt. Rosenthal had been her attorney since 1949, when he represented her in her uncontested divorce action against her second husband, saxophonist George W. Weidler. In February 1969, Day filed suit against Rosenthal and won the then-largest civil judgment (over $20 million) in the state of California. (She later settled for about one-quarter of the amount originally awarded.)
Day also learned that Melcher had committed her to a television series, which became "The Doris Day Show"."It was awful", Day told "OK! Magazine" in 1996. "I was really, really not very well when Marty [Melcher] passed away, and the thought of going into TV was overpowering. But he'd signed me up for a series. And then my son Terry [Melcher] took me walking in Beverly Hills and explained that it wasn't nearly the end of it. I had also been signed up for a bunch of TV specials, all without anyone ever asking me."
Day hated the idea of doing television, but felt obliged to it. "There was a contract. I didn't know about it. I never wanted to do TV, but I gave it 100 percent anyway. That's the only way I know how to do it." The first episode of "The Doris Day Show" aired on September 24, 1968, and, from 1968 to 1973, employed "Que Sera, Sera" as its theme song. Day grudgingly persevered (she needed the work to help pay off her debts), but only after CBS ceded creative control to her and her son. The successful show enjoyed a five-year run (its second season finished in the Top 10 of the Nielsen ratings), and functioned as a curtain-raiser for "The Carol Burnett Show". It is remembered today for its abrupt season-to-season changes in casting and premise. It was not widely syndicated as many of its contemporaries were, and was re-broadcast very little outside the United States, Australia and the UK. By the end of its run in 1973, public tastes had changed and her firmly established persona was regarded as passé. She largely retired from acting after "The Doris Day Show", but did complete two television specials, "The Doris Mary Anne Kappelhoff Special" (1971) and "Doris Day to Day" (1975). She appeared in a John Denver TV special in 1974.
In the 1985–86 season, Day hosted her own television talk show, "Doris Day's Best Friends", on CBN. The network canceled the show after 26 episodes, despite the worldwide publicity it received.
1970s.
On September 18, 1974, courts awarded Day $22,835,646 for fraud and malpractice in an hour-long oral decision by Superior Judge Lester E. Olson, ending a 99-day trial that involved 18 consolidated lawsuits and countersuits filed by Day and Rosenthal that involved Rosenthal's handling of her finances after she terminated him in July 1968. The civil trial included 14,451 pages of transcript from 67 witnesses. Represented by attorney Robert Winslow and the law firm of Mitchell, Silberberg & Knupp LLP, Day was awarded $1 million punitive damages, $5.6 million plus $2 million interest for losses incurred in a sham oil venture; $3.4 million plus $1.2 million interest over a hotel venture; $2.2 million plus $793,800 interest for duplicate or unnecessary fees paid to Rosenthal; more than $2 million to recoup loans to Rosenthal; $3.9 million plus $1 million interest for fraud, and $850,000 attorney fees for Day. Olson enjoined Rosenthal from filing any further lawsuits against Day or her business operations. (Rosenthal had filed more than 20 suits from 1969 to 1974). Olson, an expert in complex financial marital settlements, read every page of 3,275 individual exhibits and 68 boxes of miscellaneous financial records. In October 1979, Rosenthal's liability insurer settled with Day for about $6 million payable in 23 annual installments.
Rosenthal filed an appeal in the 2nd District Court of Appeal. He filed six additional lawsuits related to the case. Two were libel suits, one against Day and her publishers over comments she made about Rosenthal in her book in which he sought damages. The others sought court determinations that insurance companies and individual lawyers failed to defend Rosenthal properly before Olson and in appellate stages. In April 1979, he filed an unsuccessful suit to set aside the $6 million settlement with Day and recover damages from everyone involved in agreeing, supposedly without his permission, to the payment. 
1980s and 1990s.
In October 1985, the California Supreme Court rejected Rosenthal's appeal of the multimillion-dollar judgment against him for legal malpractice, and upheld conclusions of a trial court and a Court of Appeal that Rosenthal acted improperly. In April 1986, the U.S. Supreme Court refused to review the lower court's judgment. In June 1987, Rosenthal filed a $30 million lawsuit against lawyers he claimed cheated him out of millions of dollars in real estate investments. He named Day as a co-defendant, describing her as an "unwilling, involuntary plaintiff whose consent cannot be obtained". Rosenthal claimed that millions of dollars Day lost were in real estate sold after Melcher died in 1968, in which Rosenthal asserted that the attorneys gave Day bad advice, telling her to sell, at a loss, three hotels, in Palo Alto, California, Dallas, Texas and Atlanta, Georgia and some oil leases in Kentucky and Ohio. Rosenthal claimed he had made the investments under a long-term plan, and did not intend to sell them until they appreciated in value. Two of the hotels sold in 1970 for about $7 million, and their estimated worth in 1986 was $50 million. In July 1984, after a hearing panel of the State Bar Court, after 80 days of testimony and consideration of documentary evidence, the panel accused Rosenthal of 13 separate acts of misconduct and urged his disbarment in a 34-page unsigned opinion. The State Bar Court's review department upheld the panel's findings, which asked the justices to order Rosenthal's disbarment. He continued representing clients in federal courts until the U.S. Supreme Court ruled against him on March 21, 1988. Disbarment by the Ninth U.S. Circuit Court of Appeals followed on August 19, 1988. The Supreme Court of California, in affirming the disbarment, held that Rosenthal had engaged in transactions involving undisclosed conflicts of interest, took positions adverse to his former clients, overstated expenses, double-billed for legal fees, failed to return client files, failed to provide access to records, failed to give adequate legal advice, failed to provide clients with an opportunity to obtain independent counsel, filed fraudulent claims, gave false testimony, engaged in conduct designed to harass his clients, delayed court proceedings, obstructed justice and abused legal process. Rosenthal died August 15, 2007, at the age of 96.
Terry Melcher stated that Melcher's premature death saved Day from financial ruin. It remains unresolved whether Marty Melcher had himself also been duped. Day stated publicly that she believed her husband innocent of any deliberate wrongdoing, stating that he "simply trusted the wrong person". According to Day's autobiography, as told to A. E. Hotchner, the usually athletic and healthy Martin Melcher had an enlarged heart. Most of the interviews on the subject given to Hotchner (and included in Day's autobiography) paint an unflattering portrait of Melcher. Author David Kaufman asserts that one of Day's costars, actor Louis Jourdan, maintained that Day herself disliked her husband, but Day's public statements regarding Melcher appear to contradict that assertion.
Day was scheduled to present, along with Patrick Swayze and Marvin Hamlisch, the Best Original Score Oscar at the 61st Annual Academy Awards (March 1989) but she suffered a deep leg cut and was unable to attend. She had been walking through the gardens of her hotel when she cut her leg on a sprinkler. The cut required stitches.
Day was inducted into the Ohio Women's Hall of Fame in 1981 and received the Cecil B. DeMille Award for career achievement in 1989. In 1994, Day's Greatest Hits album became another entry into the British charts. The song "Perhaps, Perhaps, Perhaps" was included in the soundtrack of the Australian film "Strictly Ballroom", the theme song for the British TV show "Coupling", with Mari Wilson performing the song for the title sequence and in the promo for the fourth season of the FX series "Louie".
2000s.
In 2000, Day received the Ohio Medal of Honor, that state's highest civilian award. In 2006, Day recorded a commentary for the DVD release of the fifth (and final) season of her television show. Day has participated in telephone interviews with a radio station that celebrates her birthday with an annual Doris Day music marathon. In July 2008, she appeared on the Southern California radio show of longtime friend, newscaster George Putnam, reported in the "Los Angeles Times". 
Day turned down a tribute offer from the American Film Institute and from the Kennedy Center Honors because they require attendance in person. In 2004, she was awarded the Presidential Medal of Freedom by President George W. Bush for her achievements in the entertainment industry and for her work on behalf of animals. President Bush stated, "It was a good day for our fellow creatures when she gave her good heart to the cause of animal welfare." Day declined to attend the ceremony due to her fear of flying.
Both columnist Liz Smith and film critic Rex Reed have mounted vigorous campaigns to gather support for an honorary Academy Award for Day to herald her film career and her status as the top female box-office star of all time. Day received a Grammy for Lifetime Achievement in Music in 2008, albeit again in absentia. She received three Grammy Hall of Fame Awards, in 1998, 1999 and 2012 for her recordings of "Sentimental Journey", "Secret Love", and "Que Sera, Sera", respectively. Day was inducted into the Hit Parade Hall of Fame in 2007, and in 2010 received the first Legend Award ever presented by the Society of Singers.
2010s.
Day, aged 89, released "My Heart" in the United Kingdom on September 5, 2011, her first new album in nearly two decades, since the release of "The Love Album", which, although recorded in 1967, was not released until 1994. The album is a compilation of previously unreleased recordings produced by Day's son, Terry Melcher, before his death in 2004. Tracks include the 1970s Joe Cocker hit "You Are So Beautiful", The Beach Boys' "Disney Girls" and jazz standards such as "My Buddy", which Day originally sang in her 1951 film "I'll See You in My Dreams". Day dedicates this song to her son. The disc was released in the US via City Hall Records on December 6, and within two weeks had climbed to No. 12 on Amazon's bestseller list in spite of being priced over 25% higher than most CDs to raise funds for the Doris Day Animal League. It debuted at 135 on the Billboard 200, Day's first entry on that chart since "Love Him" (1963). She became the oldest artist to score a UK Top 10 with an album featuring new material, according to the Official Charts Company, entering at Number 9. (British singer Vera Lynn reached the top of the chart in August 2009 at age 92, but with a greatest hits album, "We'll Meet Again – The Very Best of Vera Lynn".)
In January 2012, the Los Angeles Film Critics Association presented Day with a Lifetime Achievement Award.
Personal life.
Since her retirement from films, Day has lived in Carmel-by-the-Sea, California. She has many pets and adopts stray animals.
Day is a Republican. Her only child, music producer and songwriter Terry Melcher, who had a hit in the 1960s with "Hey Little Cobra" under the name The Rip Chords, died of melanoma in 2004, about five months after Day had received the Presidential Medal of Freedom. She owns a hotel in Carmel-by-the-Sea, the Cypress Inn, which Melcher had co-owned with Day.
Day became an ordained minister with the Universal Life Church in order to officiate weddings. 
Marriages.
In 1975, Day published her autobiography, "Doris Day: Her Own Story", an "as-told-to" work with A. E. Hotchner. The book detailed her first three marriages:
Animal welfare activism.
Day's interest in animal welfare and related issues apparently dates to her teen years. While recovering from an automobile accident, she took her dog Tiny for a walk without a leash. Tiny ran into the street and was killed by a passing car. Day later confessed guilt and loneliness about Tiny's untimely death. In 1971, she co-founded Actors and Others for Animals, and appeared in a series of newspaper advertisements denouncing the wearing of fur, alongside Mary Tyler Moore, Angie Dickinson, and Jayne Meadows. Day's friend, Cleveland Amory, wrote about these events in .
In 1978, Day founded her own Doris Day Pet Foundation, now the Doris Day Animal Foundation (DDAF). A non-profit 501(c)(3) grant-giving public charity, DDAF funds other non-profit causes throughout the US that share DDAF's mission of helping animals and the people who love them. The DDAF continues to operate independently under Day's personal supervision.
To complement the Doris Day Animal Foundation, Day formed the Doris Day Animal League (DDAL) in 1987, a national non-profit citizen's lobbying organization whose mission is to reduce pain and suffering and protect animals through legislative initiatives. Day actively lobbied the United States Congress in support of legislation designed to safeguard animal welfare on a number of occasions and in 1995 she originated the annual Spay Day USA. The DDAL merged into The Humane Society of the United States in 2006. Staff members of DDAL took positions within The HSUS, and Day recorded public service announcements for the organization. The HSUS now manages World Spay Day, the annual one-day spay/neuter event that Day originated.
A facility to help abused and neglected horses opened in 2011 and bears her name—the Doris Day Horse Rescue and Adoption Center, located in Murchison, Texas, on the grounds of an animal sanctuary started by her late friend, author Cleveland Amory. Day contributed $250,000 toward the founding of the center.

</doc>
<doc id="8301" url="http://en.wikipedia.org/wiki?curid=8301" title="Distillation">
Distillation

Distillation is a process of separating the component substances from a liquid mixture by selective evaporation and condensation. Distillation may result in essentially complete separation (nearly pure components), or it may be a partial separation that increases the concentration of selected components of the mixture. In either case the process exploits differences in the volatility of mixture's components. In industrial chemistry, distillation is a unit operation of practically universal importance, but it is a physical separation process and not a chemical reaction.
Commercially, distillation has many applications. For example:
An installation for distillation, especially of alcohol, is a distillery. The distillation equipment is a still.
History.
Aristotle wrote about the process in his "Meteorologica" and even that "ordinary wine possesses a kind of exhalation, and that is why it gives out a flame". Later evidence of distillation comes from Greek alchemists working in Alexandria in the 1st century AD. Distilled water has been known since at least c. 200, when Alexander of Aphrodisias described the process. Distillation in China could have begun during the Eastern Han Dynasty (1st–2nd centuries), but archaeological evidence indicates that actual distillation of beverages began in the Jin and Southern Song dynasties. A still was found in an archaeological site in Qinglong, Hebei province dating to the 12th century. Distilled beverages were more common during the Yuan dynasty. Arabs learned the process from the Alexandrians and used it extensively in their chemical experiments.
Clear evidence of the distillation of alcohol comes from the School of Salerno in the 12th century. Fractional distillation was developed by Tadeo Alderotti in the 13th century.
In 1500, German alchemist Hieronymus Braunschweig published "Liber de arte destillandi" (The Book of the Art of Distillation) the first book solely dedicated to the subject of distillation, followed in 1512 by a much expanded version. In 1651, John French published the first major English compendium of practice, though it has been claimed that much of it derives from Braunschweig's work. This includes diagrams with people in them showing the industrial rather than bench scale of the operation.
As alchemy evolved into the science of chemistry, vessels called retorts became used for distillations. Both alembics and retorts are forms of glassware with long necks pointing to the side at a downward angle which acted as air-cooled condensers to condense the distillate and let it drip downward for collection. Later, copper alembics were invented. Riveted joints were often kept tight by using various mixtures, for instance a dough made of rye flour. These alembics often featured a cooling system around the beak, using cold water for instance, which made the condensation of alcohol more efficient. These were called pot stills. Today, the retorts and pot stills have been largely supplanted by more efficient distillation methods in most industrial processes. However, the pot still is still widely used for the elaboration of some fine alcohols such as cognac, Scotch whisky, tequila and some vodkas. Pot stills made of various materials (wood, clay, stainless steel) are also used by bootleggers in various countries. Small pot stills are also sold for the domestic production of flower water or essential oils.
Early forms of distillation were batch processes using one vaporization and one condensation. Purity was improved by further distillation of the condensate. Greater volumes were processed by simply repeating the distillation. Chemists were reported to carry out as many as 500 to 600 distillations in order to obtain a pure compound.
In the early 19th century the basics of modern techniques including pre-heating and reflux were developed, particularly by the French, then in 1830 a British Patent was issued to Aeneas Coffey for a whiskey distillation column, which worked continuously and may be regarded as the archetype of modern petrochemical units. In 1877, Ernest Solvay was granted a U.S. Patent for a tray column for ammonia distillation and the same and subsequent years saw developments of this theme for oil and spirits.
With the emergence of chemical engineering as a discipline at the end of the 19th century, scientific rather than empirical methods could be applied. The developing petroleum industry in the early 20th century provided the impetus for the development of accurate design methods such as the McCabe–Thiele method and the Fenske equation. The availability of powerful computers has also allowed direct computer simulation of distillation columns.
Applications of distillation.
The application of distillation can roughly be divided in four groups: laboratory scale, industrial distillation, distillation of herbs for perfumery and medicinals (herbal distillate), and food processing. The latter two are distinctively different from the former two in that in the processing of beverages, the distillation is not used as a true purification method but more to transfer all volatiles from the source materials to the distillate.
The main difference between laboratory scale distillation and industrial distillation is that laboratory scale distillation is often performed batch-wise, whereas industrial distillation often occurs continuously. In batch distillation, the composition of the source material, the vapors of the distilling compounds and the distillate change during the distillation. In batch distillation, a still is charged (supplied) with a batch of feed mixture, which is then separated into its component fractions which are collected sequentially from most volatile to less volatile, with the bottoms (remaining least or non-volatile fraction) removed at the end. The still can then be recharged and the process repeated.
In continuous distillation, the source materials, vapors, and distillate are kept at a constant composition by carefully replenishing the source material and removing fractions from both vapor and liquid in the system. This results in a better control of the separation process.
Idealized distillation model.
The boiling point of a liquid is the temperature at which the vapor pressure of the liquid equals the pressure around the liquid, enabling bubbles to form without being crushed. A special case is the normal boiling point, where the vapor pressure of the liquid equals the ambient atmospheric pressure.
It is a common misconception that in a liquid mixture at a given pressure, each component boils at the boiling point corresponding to the given pressure and the vapors of each component will collect separately and purely. This, however, does not occur even in an idealized system. Idealized models of distillation are essentially governed by Raoult's law and Dalton's law, and assume that vapor–liquid equilibria are attained.
Raoult's law states that the vapor pressure of a solution is dependent on 1) the vapor pressure of each chemical component in the solution and 2) the fraction of solution each component makes up a.k.a. the mole fraction. This law applies to ideal solutions, or solutions that have different components but whose molecular interactions are the same as or very similar to pure solutions.
Dalton's law states that the total vapor pressure is the sum of the vapor pressures of each individual component in the mixture. When a multi-component liquid is heated, the vapor pressure of each component will rise, thus causing the total vapor pressure to rise. When the total vapor pressure reaches the pressure surrounding the liquid, boiling occurs and liquid turns to gas throughout the bulk of the liquid. Note that a mixture with a given composition has one boiling point at a given pressure, when the components are mutually soluble.
An implication of one boiling point is that lighter components never cleanly "boil first". At boiling point, all volatile components boil, but for a component, its percentage in the vapor is the same as its percentage of the total vapor pressure. Lighter components have a higher partial pressure and thus are concentrated in the vapor, but heavier volatile components also have a (smaller) partial pressure and necessarily evaporate also, albeit being less concentrated in the vapor. Indeed, batch distillation and fractionation succeed by varying the composition of the mixture. In batch distillation, the batch evaporates, which changes its composition; in fractionation, liquid higher in the fractionation column contains more lights and boils at lower temperatures.
The idealized model is accurate in the case of chemically similar liquids, such as benzene and toluene. In other cases, severe deviations from Raoult's law and Dalton's law are observed, most famously in the mixture of ethanol and water. These compounds, when heated together, form an azeotrope, which is a composition with a boiling point higher or lower than the boiling point of each separate liquid. Virtually all liquids, when mixed and heated, will display azeotropic behaviour. Although there are computational methods that can be used to estimate the behavior of a mixture of arbitrary components, the only way to obtain accurate vapor–liquid equilibrium data is by measurement.
It is not possible to "completely" purify a mixture of components by distillation, as this would require each component in the mixture to have a zero partial pressure. If ultra-pure products are the goal, then further chemical separation must be applied. When a binary mixture is evaporated and the other component, e.g. a salt, has zero partial pressure for practical purposes, the process is simpler and is called evaporation in engineering.
Batch distillation.
Heating an ideal mixture of two volatile substances A and B (with A having the higher volatility, or lower boiling point) in a batch distillation setup (such as in an apparatus depicted in the opening figure) until the mixture is boiling results in a vapor above the liquid which contains a mixture of A and B. The ratio between A and B in the vapor will be different from the ratio in the liquid: the ratio in the liquid will be determined by how the original mixture was prepared, while the ratio in the vapor will be enriched in the more volatile compound, A (due to Raoult's Law, see above). The vapor goes through the condenser and is removed from the system. This in turn means that the ratio of compounds in the remaining liquid is now different from the initial ratio (i.e., more enriched in B than the starting liquid).
The result is that the ratio in the liquid mixture is changing, becoming richer in component B. This causes the boiling point of the mixture to rise, which in turn results in a rise in the temperature in the vapor, which results in a changing ratio of A : B in the gas phase (as distillation continues, there is an increasing proportion of B in the gas phase). This results in a slowly changing ratio A : B in the distillate.
If the difference in vapor pressure between the two components A and B is large (generally expressed as the difference in boiling points), the mixture in the beginning of the distillation is highly enriched in component A, and when component A has distilled off, the boiling liquid is enriched in component B.
Continuous distillation.
Continuous distillation is an ongoing distillation in which a liquid mixture is continuously (without interruption) fed into the process and separated fractions are removed continuously as output streams occur over time during the operation. Continuous distillation produces a minimum of two output fractions, including at least one volatile distillate fraction, which has boiled and been separately captured as a vapor, and then condensed to a liquid. There is always a bottoms (or residue) fraction, which is the least volatile residue that has not been separately captured as a condensed vapor.
Continuous distillation differs from batch distillation in the respect that concentrations should not change over time. Continuous distillation can be run at a steady state for an arbitrary amount of time. For any source material of specific composition, the main variables that affect the purity of products in continuous distillation are the reflux ratio and the number of theoretical equilibrium stages, in practice determined by the number of trays or the height of packing. Reflux is a flow from the condenser back to the column, which generates a recycle that allows a better separation with a given number of trays. Equilibrium stages are ideal steps where compositions achieve vapor–liquid equilibrium, repeating the separation process and allowing better separation given a reflux ratio. A column with a high reflux ratio may have fewer stages, but it refluxes a large amount of liquid, giving a wide column with a large holdup. Conversely, a column with a low reflux ratio must have a large number of stages, thus requiring a taller column.
General improvements.
Both batch and continuous distillations can be improved by making use of a fractionating column on top of the distillation flask. The column improves separation by providing a larger surface area for the vapor and condensate to come into contact. This helps it remain at equilibrium for as long as possible. The column can even consist of small subsystems ('trays' or 'dishes') which all contain an enriched, boiling liquid mixture, all with their own vapor–liquid equilibrium.
There are differences between laboratory-scale and industrial-scale fractionating columns, but the principles are the same. Examples of laboratory-scale fractionating columns (in increasing efficiency) include
Laboratory scale distillation.
Laboratory scale distillations are almost exclusively run as batch distillations. The device used in distillation, sometimes referred to as a "still", consists at a minimum of a reboiler or "pot" in which the source material is heated, a condenser in which the heated vapour is cooled back to the liquid state, and a receiver in which the concentrated or purified liquid, called the distillate, is collected. Several laboratory scale techniques for distillation exist (see also ).
Simple distillation.
In simple distillation, the vapor is immediately channeled into a condenser. Consequently, the distillate is not pure but rather its composition is identical to the composition of the vapors at the given temperature and pressure. That concentration follows Raoult's law.
As a result, simple distillation is effective only when the liquid boiling points differ greatly (rule of thumb is 25 °C) or when separating liquids from non-volatile solids or oils. For these cases, the vapor pressures of the components are usually different enough that the distillate may be sufficiently pure for its intended purpose.
Fractional distillation.
For many cases, the boiling points of the components in the mixture will be sufficiently close that Raoult's law must be taken into consideration. Therefore, fractional distillation must be used in order to separate the components by repeated vaporization-condensation cycles within a packed fractionating column. This separation, by successive distillations, is also referred to as rectification.
As the solution to be purified is heated, its vapors rise to the fractionating column. As it rises, it cools, condensing on the condenser walls and the surfaces of the packing material. Here, the condensate continues to be heated by the rising hot vapors; it vaporizes once more. However, the composition of the fresh vapors are determined once again by Raoult's law. Each vaporization-condensation cycle (called a "theoretical plate") will yield a purer solution of the more volatile component. In reality, each cycle at a given temperature does not occur at exactly the same position in the fractionating column; "theoretical plate" is thus a concept rather than an accurate description.
More theoretical plates lead to better separations. A spinning band distillation system uses a spinning band of Teflon or metal to force the rising vapors into close contact with the descending condensate, increasing the number of theoretical plates.
Steam distillation.
Like vacuum distillation, steam distillation is a method for distilling compounds which are heat-sensitive. The temperature of the steam is easier to control than the surface of a heating element, and allows a high rate of heat transfer without heating at a very high temperature. This process involves bubbling steam through a heated mixture of the raw material. By Raoult's law, some of the target compound will vaporize (in accordance with its partial pressure). The vapor mixture is cooled and condensed, usually yielding a layer of oil and a layer of water.
Steam distillation of various aromatic herbs and flowers can result in two products; an essential oil as well as a watery herbal distillate. The essential oils are often used in perfumery and aromatherapy while the watery distillates have many applications in aromatherapy, food processing and skin care.
Vacuum distillation.
Some compounds have very high boiling points. To boil such compounds, it is often better to lower the pressure at which such compounds are boiled instead of increasing the temperature. Once the pressure is lowered to the vapor pressure of the compound (at the given temperature), boiling and the rest of the distillation process can commence. This technique is referred to as vacuum distillation and it is commonly found in the laboratory in the form of the rotary evaporator.
This technique is also very useful for compounds which boil beyond their decomposition temperature at atmospheric pressure and which would therefore be decomposed by any attempt to boil them under atmospheric pressure.
Molecular distillation is vacuum distillation below the pressure of 0.01 torr. 0.01 torr is one order of magnitude above high vacuum, where fluids are in the free molecular flow regime, i.e. the mean free path of molecules is comparable to the size of the equipment. The gaseous phase no longer exerts significant pressure on the substance to be evaporated, and consequently, rate of evaporation no longer depends on pressure. That is, because the continuum assumptions of fluid dynamics no longer apply, mass transport is governed by molecular dynamics rather than fluid dynamics. Thus, a short path between the hot surface and the cold surface is necessary, typically by suspending a hot plate covered with a film of feed next to a cold plate with a line of sight in between. Molecular distillation is used industrially for purification of oils.
Air-sensitive vacuum distillation.
Some compounds have high boiling points as well as being air sensitive. A simple vacuum distillation system as exemplified above can be used, whereby the vacuum is replaced with an inert gas after the distillation is complete. However, this is a less satisfactory system if one desires to collect fractions under a reduced pressure. To do this a "cow" or "pig" adaptor can be added to the end of the condenser, or for better results or for very air sensitive compounds a Perkin triangle apparatus can be used.
The Perkin triangle, has means via a series of glass or Teflon taps to allows fractions to be isolated from the rest of the still, without the main body of the distillation being removed from either the vacuum or heat source, and thus can remain in a state of reflux. To do this, the sample is first isolated from the vacuum by means of the taps, the vacuum over the sample is then replaced with an inert gas (such as nitrogen or argon) and can then be stoppered and removed. A fresh collection vessel can then be added to the system, evacuated and linked back into the distillation system via the taps to collect a second fraction, and so on, until all fractions have been collected.
Short path distillation.
Short path distillation is a distillation technique that involves the distillate travelling a short distance, often only a few centimeters, and is normally done at reduced pressure. A classic example would be a distillation involving the distillate travelling from one glass bulb to another, without the need for a condenser separating the two chambers. This technique is often used for compounds which are unstable at high temperatures or to purify small amounts of compound. The advantage is that the heating temperature can be considerably lower (at reduced pressure) than the boiling point of the liquid at standard pressure, and the distillate only has to travel a short distance before condensing. A short path ensures that little compound is lost on the sides of the apparatus. The Kugelrohr is a kind of a short path distillation apparatus which often contain multiple chambers to collect distillate fractions.
Zone distillation.
Zone distillation is a distillation process in long container with partial melting of refined matter in moving liquid zone and condensation of vapor in the solid phase at condensate pulling in cold area. The process is worked in theory. When zone heater is moving from the top to the bottom of the container then solid condensate with irregular impurity distribution is forming. Then most pure part of the condensate may be extracted as product. The process may be iterated many times by moving (without turnover) the received condensate to the bottom part of the container on the place of refined matter. The irregular impurity distribution in the condensate (that is efficiency of purification) increases with number of repetitions of the process.
Zone distillation is a distillation analog of zone recrystallization. Impurity distribution in the condensate is described by known equations of zone recrystallization with various numbers of iteration of process – with replacement distribution efficient k of crystallization on separation factor α of distillation.
Other types.
The unit process of evaporation may also be called "distillation":
Other uses:
Azeotropic distillation.
Interactions between the components of the solution create properties unique to the solution, as most processes entail nonideal mixtures, where Raoult's law does not hold. Such interactions can result in a constant-boiling azeotrope which behaves as if it were a pure compound (i.e., boils at a single temperature instead of a range). At an azeotrope, the solution contains the given component in the same proportion as the vapor, so that evaporation does not change the purity, and distillation does not effect separation. For example, ethyl alcohol and water form an azeotrope of 95.6% at 78.1 °C.
If the azeotrope is not considered sufficiently pure for use, there exist some techniques to break the azeotrope to give a pure distillate. This set of techniques are known as azeotropic distillation. Some techniques achieve this by "jumping" over the azeotropic composition (by adding another component to create a new azeotrope, or by varying the pressure). Others work by chemically or physically removing or sequestering the impurity. For example, to purify ethanol beyond 95%, a drying agent or a (desiccant such as potassium carbonate) can be added to convert the soluble water into insoluble water of crystallization. Molecular sieves are often used for this purpose as well.
Immiscible liquids, such as water and toluene, easily form azeotropes. Commonly, these azeotropes are referred to as a low boiling azeotrope because the boiling point of the azeotrope is lower than the boiling point of either pure component. The temperature and composition of the azeotrope is easily predicted from the vapor pressure of the pure components, without use of Raoult's law. The azeotrope is easily broken in a distillation set-up by using a liquid–liquid separator (a decanter) to separate the two liquid layers that are condensed overhead. Only one of the two liquid layers is refluxed to the distillation set-up.
High boiling azeotropes, such as a 20 weight percent mixture of hydrochloric acid in water, also exist. As implied by the name, the boiling point of the azeotrope is greater than the boiling point of either pure component.
To break azeotropic distillations and cross distillation boundaries, such as in the DeRosier Problem, it is necessary to increase the composition of the light key in the distillate.
Breaking an azeotrope with unidirectional pressure manipulation.
The boiling points of components in an azeotrope overlap to form a band. By exposing an azeotrope to a vacuum or positive pressure, it's possible to bias the boiling point of one component away from the other by exploiting the differing vapour pressure curves of each; the curves may overlap at the azeotropic point, but are unlikely to be remain identical further along the pressure axis either side of the azeotropic point. When the bias is great enough, the two boiling points no longer overlap and so the azeotropic band disappears.
This method can remove the need to add other chemicals to a distillation, but it has two potential drawbacks.
Under negative pressure, power for a vacuum source is needed and the reduced boiling points of the distillates requires that the condenser be run cooler to prevent distillate vapours being lost to the vacuum source. Increased cooling demands will often require additional energy and possibly new equipment or a change of coolant.
Alternatively, if positive pressures are required, standard glassware can not be used, energy must be used for pressurization and there is a higher chance of side reactions occurring in the distillation, such as decomposition, due to the higher temperatures required to effect boiling.
A unidirectional distillation will rely on a pressure change in one direction, either positive or negative.
Pressure-swing distillation.
Pressure-swing distillation is essentially the same as the unidirectional distillation used to break azeotropic mixtures, but here both positive and negative pressures may be employed.
This improves the selectivity of the distillation and allows a chemist to optimize distillation by avoiding extremes of pressure and temperature that waste energy. This is particularly important in commercial applications.
One example of the application of pressure-swing distillation is during the industrial purification of ethyl acetate after its catalytic synthesis from ethanol.
Industrial distillation.
Large scale industrial distillation applications include both batch and continuous fractional, vacuum, azeotropic, extractive, and steam distillation. The most widely used industrial applications of continuous, steady-state fractional distillation are in petroleum refineries, petrochemical and chemical plants and natural gas processing plants.
To control and optimize such industrial distillation, a standardized laboratory method, ASTM D86, is established. This test method extends to the atmospheric distillation of petroleum products using a laboratory batch distillation unit to quantitatively determine the boiling range characteristics of petroleum products.
Industrial distillation is typically performed in large, vertical cylindrical columns known as distillation towers or distillation columns with diameters ranging from about 65 centimeters to 16 meters and heights ranging from about 6 meters to 90 meters or more. When the process feed has a diverse composition, as in distilling crude oil, liquid outlets at intervals up the column allow for the withdrawal of different "fractions" or products having different boiling points or boiling ranges. The "lightest" products (those with the lowest boiling point) exit from the top of the columns and the "heaviest" products (those with the highest boiling point) exit from the bottom of the column and are often called the bottoms.
Industrial towers use reflux to achieve a more complete separation of products. Reflux refers to the portion of the condensed overhead liquid product from a distillation or fractionation tower that is returned to the upper part of the tower as shown in the schematic diagram of a typical, large-scale industrial distillation tower. Inside the tower, the downflowing reflux liquid provides cooling and condensation of the upflowing vapors thereby increasing the efficiency of the distillation tower. The more reflux that is provided for a given number of theoretical plates, the better the tower's separation of lower boiling materials from higher boiling materials. Alternatively, the more reflux that is provided for a given desired separation, the fewer the number of theoretical plates required. Chemical engineers must choose what combination of reflux rate and number of plates is both economically and physically feasible for the products purified in the distillation column.
Such industrial fractionating towers are also used in cryogenic air separation, producing liquid oxygen, liquid nitrogen, and high purity argon. Distillation of chlorosilanes also enables the production of high-purity silicon for use as a semiconductor.
Design and operation of a distillation tower depends on the feed and desired products. Given a simple, binary component feed, analytical methods such as the McCabe–Thiele method or the Fenske equation can be used. For a multi-component feed, simulation models are used both for design and operation. Moreover, the efficiencies of the vapor–liquid contact devices (referred to as "plates" or "trays") used in distillation towers are typically lower than that of a theoretical 100% efficient equilibrium stage. Hence, a distillation tower needs more trays than the number of theoretical vapor–liquid equilibrium stages. A variety of models have been postulated to estimate tray efficiencies.
In modern industrial uses, a packing material is used in the column instead of trays when low pressure drops across the column are required. Other factors that favor packing are: vacuum systems, smaller diameter columns, corrosive systems, systems prone to foaming, systems requiring low liquid holdup, and batch distillation. Conversely, factors that favor plate columns are: presence of solids in feed, high liquid rates, large column diameters, complex columns, columns with wide feed composition variation, columns with a chemical reaction, absorption columns, columns limited by foundation weight tolerance, low liquid rate, large turn-down ratio and those processes subject to process surges.
This packing material can either be random dumped packing (1–3" wide) such as Raschig rings or structured sheet metal. Liquids tend to wet the surface of the packing and the vapors pass across this wetted surface, where mass transfer takes place. Unlike conventional tray distillation in which every tray represents a separate point of vapor–liquid equilibrium, the vapor–liquid equilibrium curve in a packed column is continuous. However, when modeling packed columns, it is useful to compute a number of "theoretical stages" to denote the separation efficiency of the packed column with respect to more traditional trays. Differently shaped packings have different surface areas and void space between packings. Both of these factors affect packing performance.
Another factor in addition to the packing shape and surface area that affects the performance of random or structured packing is the liquid and vapor distribution entering the packed bed. The number of theoretical stages required to make a given separation is calculated using a specific vapor to liquid ratio. If the liquid and vapor are not evenly distributed across the superficial tower area as it enters the packed bed, the liquid to vapor ratio will not be correct in the packed bed and the required separation will not be achieved. The packing will appear to not be working properly. The height equivalent to a theoretical plate (HETP) will be greater than expected. The problem is not the packing itself but the mal-distribution of the fluids entering the packed bed. Liquid mal-distribution is more frequently the problem than vapor. The design of the liquid distributors used to introduce the feed and reflux to a packed bed is critical to making the packing perform to it maximum efficiency. Methods of evaluating the effectiveness of a liquid distributor to evenly distribute the liquid entering a packed bed can be found in references. Considerable work as been done on this topic by Fractionation Research, Inc. (commonly known as FRI).
Multi-effect distillation.
The goal of multi-effect distillation is to increase the energy efficiency of the process, for use in desalination, or in some cases one stage in the production of ultrapure water. The number of effects is inversely proportional to the kW·h/m3 of water recovered figure, and refers to the volume of water recovered per unit of energy compared with single-effect distillation. One effect is roughly 636 kW·h/m3.
There are many other types of multi-effect distillation processes, including one referred to as simply multi-effect distillation (MED), in which multiple chambers, with intervening heat exchangers, are employed.
Distillation in food processing.
Distilled beverages.
Carbohydrate-containing plant materials are allowed to ferment, producing a dilute solution of ethanol in the process. Spirits such as whiskey and rum are prepared by distilling these dilute solutions of ethanol. Components other than ethanol, including water, esters, and other alcohols, are collected in the condensate, which account for the flavor of the beverage. Some of these beverages are then stored in barrels or other containers to acquire more flavor compounds and characteristic flavors.

</doc>
<doc id="8302" url="http://en.wikipedia.org/wiki?curid=8302" title="David Hilbert">
David Hilbert

David Hilbert (]; 23 January 1862 –
14 February 1943) was a German mathematician.
He is recognized as one of the most influential and universal mathematicians of the 19th and early 20th centuries. Hilbert discovered and developed a broad range of fundamental ideas in many areas, including invariant theory and the axiomatization of geometry. He also formulated the theory of Hilbert spaces, one of the foundations of functional analysis.
Hilbert adopted and warmly defended Georg Cantor's set theory and transfinite numbers. A famous example of his leadership in mathematics is his 1900 presentation of a collection of problems that set the course for much of the mathematical research of the 20th century.
Hilbert and his students contributed significantly to establishing rigor and developed important tools used in modern mathematical physics. Hilbert is known as one of the founders of proof theory and mathematical logic, as well as for being among the first to distinguish between mathematics and metamathematics.
Life.
Early life and education.
Hilbert, the first of two children of Otto and Maria Therese (Erdtmann) Hilbert, was born in the Province of Prussia, either in Königsberg (according to Hilbert's own statement) or in Wehlau (known since 1946 as Znamensk) near Königsberg where his father worked at the time of his birth.
In the fall of 1872, Hilbert entered the Friedrichskolleg Gymnasium ("Collegium fridericianum", the same school that Immanuel Kant had attended 140 years before); but, after an unhappy period, he transferred to (fall 1879) and graduated from (spring 1880) the more science-oriented Wilhelm Gymnasium. Upon graduation, in autumn 1880, Hilbert enrolled at the University of Königsberg, the "Albertina". In the spring of 1882, Hermann Minkowski (two years younger than Hilbert and also a native of Königsberg but so talented he had graduated early from his gymnasium and gone to Berlin for three semesters), returned to Königsberg and entered the university. "Hilbert knew his luck when he saw it. In spite of his father's disapproval, he soon became friends with the shy, gifted Minkowski".
Career.
In 1884, Adolf Hurwitz arrived from Göttingen as an Extraordinarius (i.e., an associate professor). An intense and fruitful scientific exchange among the three began, and Minkowski and Hilbert especially would exercise a reciprocal influence over each other at various times in their scientific careers. Hilbert obtained his doctorate in 1885, with a dissertation, written under Ferdinand von Lindemann, titled "Über invariante Eigenschaften spezieller binärer Formen, insbesondere der Kugelfunktionen" ("On the invariant properties of special binary forms, in particular the spherical harmonic functions").
Hilbert remained at the University of Königsberg as a "Privatdozent" (senior lecturer) from 1886 to 1895.
The Göttingen school.
Among Hibert's students were Hermann Weyl, chess champion Emanuel Lasker, Ernst Zermelo, and Carl Gustav Hempel. John von Neumann was his assistant. At the University of Göttingen, Hilbert was surrounded by a social circle of some of the most important mathematicians of the 20th century, such as Emmy Noether and Alonzo Church.
Among his 69 Ph.D. students in Göttingen were many who later became famous mathematicians, including (with date of thesis): Otto Blumenthal (1898), Felix Bernstein (1901), Hermann Weyl (1908), Richard Courant (1910), Erich Hecke (1910), Hugo Steinhaus (1911), and Wilhelm Ackermann (1925). Between 1902 and 1939 Hilbert was editor of the "Mathematische Annalen", the leading mathematical journal of the time.
"Good, he did not have enough imagination to become a mathematician".—Hilbert's response upon hearing that one of his students had dropped out to study poetry.
Later years.
Hilbert lived to see the Nazis' purge many of the prominent faculty members at University of Göttingen in 1933. Those forced out included Hermann Weyl (who had taken Hilbert's chair when he retired in 1930), Emmy Noether and Edmund Landau. One who had to leave Germany, Paul Bernays, had collaborated with Hilbert in mathematical logic, and co-authored with him the important book "Grundlagen der Mathematik" (which eventually appeared in two volumes, in 1934 and 1939). This was a sequel to the Hilbert-Ackermann book "Principles of Mathematical Logic" from 1928.
About a year later, Hilbert attended a banquet and was seated next to the new Minister of Education, Bernhard Rust. Rust asked, "How is mathematics in Göttingen now that it has been freed of the Jewish influence?" Hilbert replied, "Mathematics in Göttingen? There is really none any more."
By the time Hilbert died in 1943, the Nazis had nearly completely restaffed the university, inasmuch as many of the former faculty had either been Jewish or married to Jews. Hilbert's funeral was attended by fewer than a dozen people, only two of whom were fellow academics, among them Arnold Sommerfeld, a theoretical physicist and also a native of Königsberg. News of his death only became known to the wider world six months after he had died.
Hilbert was baptized and raised in the Reformed Protestant Church. He later on left the Church and became an agnostic. He also argued that mathematical truth was independent of the existence of God or other "a priori" assumptions.
The epitaph on his tombstone in Göttingen consists of the famous lines he spoke at the conclusion of his retirement address to the Society of German Scientists and Physicians on 8 September 1930. The words were given in response to the Latin maxim: "Ignoramus et ignorabimus" or "We do not know, we shall not know":
In English:
The day before Hilbert pronounced these phrases at the 1930 annual meeting of the Society of German Scientists and Physicians, Kurt Gödel—in a roundtable discussion during the Conference on Epistemology held jointly with the Society meetings—tentatively announced the first expression of his incompleteness theorem.
Personal life.
In 1892, Hilbert married Käthe Jerosch (1864–1945), "the daughter of a Königsberg merchant, an outspoken young lady with an independence of mind that matched his own". While at Königsberg they had their one child, Franz Hilbert (1893–1969). In 1895, as a result of intervention on his behalf by Felix Klein, he obtained the position of Professor of Mathematics at the University of Göttingen, at that time the best research center for mathematics in the world. He remained there for the rest of his life.
Hilbert's son Franz suffered throughout his life from an undiagnosed mental illness: his inferior intellect was a terrible disappointment to his father and this misfortune was a matter of distress to the mathematicians and students at Göttingen. Minkowski — Hilbert's "best and truest friend" — died prematurely of a ruptured appendix in 1909.
Hilbert solves Gordan's Problem.
Hilbert's first work on invariant functions led him to the demonstration in 1888 of his famous "finiteness theorem". Twenty years earlier, Paul Gordan had demonstrated the theorem of the finiteness of generators for binary forms using a complex computational approach. Attempts to generalize his method to functions with more than two variables failed because of the enormous difficulty of the calculations involved. In order to solve what had become known in some circles as "Gordan's Problem", Hilbert realized that it was necessary to take a completely different path. As a result, he demonstrated "Hilbert's basis theorem", showing the existence of a finite set of generators, for the invariants of quantics in any number of variables, but in an abstract form. That is, while demonstrating the existence of such a set, it was not a constructive proof — it did not display "an object" — but rather, it was an existence proof and relied on use of the Law of Excluded Middle in an infinite extension.
Hilbert sent his results to the "Mathematische Annalen". Gordan, the house expert on the theory of invariants for the "Mathematische Annalen", could not appreciate the revolutionary nature of Hilbert's theorem and rejected the article, criticizing the exposition because it was insufficiently comprehensive. His comment was:
Klein, on the other hand, recognized the importance of the work, and guaranteed that it would be published without any alterations. Encouraged by Klein, Hilbert extended his method in a second article, providing estimations on the maximum degree of the minimum set of generators, and he sent it once more to the "Annalen". After having read the manuscript, Klein wrote to him, saying:
Later, after the usefulness of Hilbert's method was universally recognized, Gordan himself would say:
For all his successes, the nature of his proof stirred up more trouble than Hilbert could have imagined at the time. Although Kronecker had conceded, Hilbert would later respond to others' similar criticisms that "many different constructions are subsumed under one fundamental idea" — in other words (to quote Reid): "Through a proof of existence, Hilbert had been able to obtain a construction"; "the proof" (i.e. the symbols on the page) "was" "the object". Not all were convinced. While Kronecker would die soon afterwards, his constructivist philosophy would continue with the young Brouwer and his developing intuitionist "school", much to Hilbert's torment in his later years. Indeed Hilbert would lose his "gifted pupil" Weyl to intuitionism — "Hilbert was disturbed by his former student's fascination with the ideas of Brouwer, which aroused in Hilbert the memory of Kronecker". Brouwer the intuitionist in particular opposed the use of the Law of Excluded Middle over infinite sets (as Hilbert had used it). Hilbert would respond:
Axiomatization of geometry.
The text "Grundlagen der Geometrie" (tr.: "Foundations of Geometry") published by Hilbert in 1899 proposes a formal set, the Hilbert's axioms, substituting the traditional axioms of Euclid. They avoid weaknesses identified in those of Euclid, whose works at the time were still used textbook-fashion. It is difficult to specify the axioms used by Hilbert without referring to the publication history of the "Grundlagen" since Hilbert changed and modified them several times. The original monograph was quickly followed by a French translation, in which Hilbert added V.2, the Completeness Axiom. An English translation, authorized by Hilbert, was made by E.J. Townsend and copyrighted in 1902. This translation incorporated the changes made in the French translation and so is considered to be a translation of the 2nd edition. Hilbert continued to make changes in the text and several editions appeared in German. The 7th edition was the last to appear in Hilbert's lifetime. New editions followed the 7th, but the main text was essentially not revised.
Hilbert's approach signaled the shift to the modern axiomatic method. In this, Hilbert was anticipated by Moritz Pasch's work from 1882. Axioms are not taken as self-evident truths. Geometry may treat "things", about which we have powerful intuitions, but it is not necessary to assign any explicit meaning to the undefined concepts. The elements, such as point, line, plane, and others, could be substituted, as Hilbert is reported to have said to Schoenflies and Kötter, by tables, chairs, glasses of beer and other such objects. It is their defined relationships that are discussed.
Hilbert first enumerates the undefined concepts: point, line, plane, lying on (a relation between points and lines, points and planes, and lines and planes), betweenness, congruence of pairs of points (line segments), and congruence of angles. The axioms unify both the plane geometry and solid geometry of Euclid in a single system.
The 23 Problems.
Hilbert put forth a most influential list of 23 unsolved problems at the International Congress of Mathematicians in Paris in 1900. This is generally reckoned the most successful and deeply considered compilation of open problems ever to be produced by an individual mathematician.
After re-working the foundations of classical geometry, Hilbert could have extrapolated to the rest of mathematics. His approach differed, however, from the later 'foundationalist' Russell-Whitehead or 'encyclopedist' Nicolas Bourbaki, and from his contemporary Giuseppe Peano. The mathematical community as a whole could enlist in problems, which he had identified as crucial aspects of the areas of mathematics he took to be key.
The problem set was launched as a talk "The Problems of Mathematics" presented during the course of the Second International Congress of Mathematicians held in Paris. Here is the introduction of the speech that Hilbert gave:
He presented fewer than half the problems at the Congress, which were published in the acts of the Congress. In a subsequent publication, he extended the panorama, and arrived at the formulation of the now-canonical 23 Problems of Hilbert. The full text is important, since the exegesis of the questions still can be a matter of inevitable debate, whenever it is asked how many have been solved.
Some of these were solved within a short time. Others have been discussed throughout the 20th century, with a few now taken to be unsuitably open-ended to come to closure. Some even continue to this day to remain a challenge for mathematicians.
Formalism.
In an account that had become standard by the mid-century, Hilbert's problem set was also a kind of manifesto, that opened the way for the development of the formalist school, one of three major schools of mathematics of the 20th century. According to the formalist, mathematics is manipulation of symbols according to agreed upon formal rules. It is therefore an autonomous activity of thought. There is, however, room to doubt whether Hilbert's own views were simplistically formalist in this sense.
Hilbert's program.
In 1920 he proposed explicitly a research project (in "metamathematics", as it was then termed) that became known as Hilbert's program. He wanted mathematics to be formulated on a solid and complete logical foundation. He believed that in principle this could be done, by showing that:
He seems to have had both technical and philosophical reasons for formulating this proposal. It affirmed his dislike of what had become known as the "ignorabimus", still an active issue in his time in German thought, and traced back in that formulation to Emil du Bois-Reymond.
This program is still recognizable in the most popular philosophy of mathematics, where it is usually called "formalism". For example, the Bourbaki group adopted a watered-down and selective version of it as adequate to the requirements of their twin projects of (a) writing encyclopedic foundational works, and (b) supporting the axiomatic method as a research tool. This approach has been successful and influential in relation with Hilbert's work in algebra and functional analysis, but has failed to engage in the same way with his interests in physics and logic.
Hilbert wrote in 1919:
Hilbert published his views on the foundations of mathematics in the 2-volume work Grundlagen der Mathematik.
Gödel's work.
Hilbert and the mathematicians who worked with him in his enterprise were committed to the project. His attempt to support axiomatized mathematics with definitive principles, which could banish theoretical uncertainties, was however to end in failure.
Gödel demonstrated that any non-contradictory formal system, which was comprehensive enough to include at least arithmetic, cannot demonstrate its completeness by way of its own axioms. In 1931 his incompleteness theorem showed that Hilbert's grand plan was impossible as stated. The second point cannot in any reasonable way be combined with the first point, as long as the axiom system is genuinely finitary.
Nevertheless, the subsequent achievements of proof theory at the very least "clarified" consistency as it relates to theories of central concern to mathematicians. Hilbert's work had started logic on this course of clarification; the need to understand Gödel's work then led to the development of recursion theory and then mathematical logic as an autonomous discipline in the 1930s. The basis for later theoretical computer science, in Alonzo Church and Alan Turing, also grew directly out of this 'debate'.
Functional analysis.
Around 1909, Hilbert dedicated himself to the study of differential and integral equations; his work had direct consequences for important parts of modern functional analysis. In order to carry out these studies, Hilbert introduced the concept of an infinite dimensional Euclidean space, later called Hilbert space. His work in this part of analysis provided the basis for important contributions to the mathematics of physics in the next two decades, though from an unanticipated direction.
Later on, Stefan Banach amplified the concept, defining Banach spaces. Hilbert spaces are an important class of objects in the area of functional analysis, particularly of the spectral theory of self-adjoint linear operators, that grew up around it during the 20th century.
Physics.
Until 1912, Hilbert was almost exclusively a "pure" mathematician. When planning a visit from Bonn, where he was immersed in studying physics, his fellow mathematician and friend Hermann Minkowski joked he had to spend 10 days in quarantine before being able to visit Hilbert. In fact, Minkowski seems responsible for most of Hilbert's physics investigations prior to 1912, including their joint seminar in the subject in 1905.
In 1912, three years after his friend's death, Hilbert turned his focus to the subject almost exclusively. He arranged to have a "physics tutor" for himself. He started studying kinetic gas theory and moved on to elementary radiation theory and the molecular theory of matter. Even after the war started in 1914, he continued seminars and classes where the works of Albert Einstein and others were followed closely.
By 1907 Einstein had framed the fundamentals of the theory of gravity, but then struggled for nearly 8 years with a confounding problem of putting the theory into final form. By early summer 1915, Hilbert's interest in physics had focused on general relativity, and he invited Einstein to Göttingen to deliver a week of lectures on the subject. Einstein received an enthusiastic reception at Göttingen. Over the summer Einstein learned that Hilbert was also working on the field equations and redoubled his own efforts. During November 1915 Einstein published several papers culminating in "The Field Equations of Gravitation" (see Einstein field equations). Nearly simultaneously David Hilbert published "The Foundations of Physics", an axiomatic derivation of the field equations (see Einstein–Hilbert action). Hilbert fully credited Einstein as the originator of the theory, and no public priority dispute concerning the field equations ever arose between the two men during their lives. See more at priority.
Additionally, Hilbert's work anticipated and assisted several advances in the mathematical formulation of quantum mechanics. His work was a key aspect of Hermann Weyl and John von Neumann's work on the mathematical equivalence of Werner Heisenberg's matrix mechanics and Erwin Schrödinger's wave equation and his namesake Hilbert space plays an important part in quantum theory. In 1926 von Neumann showed that if atomic states were understood as vectors in Hilbert space, then they would correspond with both Schrödinger's wave function theory and Heisenberg's matrices.
Throughout this immersion in physics, Hilbert worked on putting rigor into the mathematics of physics. While highly dependent on higher math, physicists tended to be "sloppy" with it. To a "pure" mathematician like Hilbert, this was both "ugly" and difficult to understand. As he began to understand physics and how physicists were using mathematics, he developed a coherent mathematical theory for what he found, most importantly in the area of integral equations. When his colleague Richard Courant wrote the now classic "Methoden der mathematischen Physik" (Methods of Mathematical Physics) including some of Hilbert's ideas, he added Hilbert's name as author even though Hilbert had not directly contributed to the writing. Hilbert said "Physics is too hard for physicists", implying that the necessary mathematics was generally beyond them; the Courant-Hilbert book made it easier for them.
Number theory.
Hilbert unified the field of algebraic number theory with his 1897 treatise "Zahlbericht" (literally "report on numbers"). He also resolved a significant number-theory problem formulated by Waring in 1770. As with the finiteness theorem, he used an existence proof that shows there must be solutions for the problem rather than providing a mechanism to produce the answers. He then had little more to publish on the subject; but the emergence of Hilbert modular forms in the dissertation of a student means his name is further attached to a major area.
He made a series of conjectures on class field theory. The concepts were highly influential, and his own contribution lives on in the names of the Hilbert class field and of the Hilbert symbol of local class field theory. Results were mostly proved by 1930, after work by Teiji Takagi.
Hilbert did not work in the central areas of analytic number theory, but his name has become known for the Hilbert–Pólya conjecture, for reasons that are anecdotal.

</doc>
<doc id="8303" url="http://en.wikipedia.org/wiki?curid=8303" title="Down syndrome">
Down syndrome

Down syndrome (DS or DNS) or Down's syndrome, also known as trisomy 21, is a genetic disorder caused by the presence of all or part of a third copy of chromosome 21. It is typically associated with physical growth delays, characteristic facial features, and mild to moderate intellectual disability. The average IQ of a young adult with Down syndrome is 50, equivalent to the mental age of an 8- or 9-year-old child, but this varies widely.
Down syndrome can be identified during pregnancy by prenatal screening followed by diagnostic testing, or after birth by direct observation and genetic testing. Since the introduction of screening, pregnancies with the diagnosis are often terminated. Regular screening for health problems common in Down syndrome is recommended throughout the person's life.
Education and proper care have been shown to improve quality of life. Some children with Down syndrome are educated in typical school classes, while others require more specialized education. Some individuals with Down syndrome graduate from high school and a few attend post-secondary education. In adulthood, about 20% in the United States do paid work in some capacity with many requiring a sheltered work environment. Support in financial and legal matters is often needed. Life expectancy is around 50 to 60 years in the developed world with proper health care.
Down syndrome is the most common chromosome abnormality in humans, occurring in about one per 1000 babies born each year. In 2013 it resulted in 36,000 deaths down from 43,000 deaths in 1990. It is named after John Langdon Down, the British doctor who fully described the syndrome in 1866. Some aspects of the condition were described earlier by Jean-Étienne Dominique Esquirol in 1838 and Édouard Séguin in 1844. The genetic cause of Down syndrome—an extra copy of chromosome 21—was identified by French researchers in 1959.
Signs and symptoms.
Those with Down syndrome nearly always have physical and intellectual disabilities. As adults, their mental abilities are typically similar to those of an 8- or 9-year-old. They also typically have poor immune function and generally reach developmental milestones at a later age. They have an increased risk of a number of other health problems, including congenital heart disease, leukemia, thyroid disorders, and mental illness, among others.
Physical.
People with Down syndrome may have some or all of these physical characteristics: a small chin, slanted eyes, poor muscle tone, a flat nasal bridge, a single crease of the palm, and a protruding tongue due to a small mouth and large tongue. These airway changes lead to obstructive sleep apnea in around half of those with Down syndrome. Other common features include: a flat and wide face, a short neck, excessive joint flexibility, extra space between big toe and second toe, abnormal patterns on the fingertips and short fingers. Instability of the atlantoaxial joint occurs in about 20% and may lead to spinal cord injury in 1–2%. Hip dislocations may occur without trauma in up to a third of people with Down syndrome.
Growth in height is slower, resulting in adults who tend to have short stature—the average height for men is 154 cm (5 ft 1 in) and for women is 142 cm (4 ft 8 in). Individuals with Down syndrome are at increased risk for obesity as they age. Growth charts have been developed specifically for children with Down syndrome.
Neurological.
Most individuals with Down syndrome have mild (IQ: 50–70) or moderate (IQ: 35–50) intellectual disability with some cases having severe (IQ: 20–35) difficulties. Those with mosaic Down syndrome typically have IQ scores 10–30 points higher. As they age, people with Down syndrome typically perform less well compared to their same-age peers. Some after 30 years of age may lose their ability to speak. This syndrome causes about a third of cases of intellectual disability. Many developmental milestones are delayed with the ability to crawl typically occurring around 8 months rather than 5 months and the ability to walk independently typically occurring around 21 months rather than 14 months.
Commonly, individuals with Down syndrome have better language understanding than ability to speak. Between 10 and 45% have either a stutter or rapid and irregular speech, making it difficult to understand them. They typically do fairly well with social skills. Behavior problems are not generally as great an issue as in other syndromes associated with intellectual disability. In children with Down syndrome, mental illness occurs in nearly 30% with autism occurring in 5–10%. People with Down syndrome experience a wide range of emotions. While generally happy, symptoms of depression and anxiety may develop in early adulthood.
Children and adults with Down syndrome are at increased risk of epileptic seizures which occur in 5–10% of children and up to 50% of adults. This includes an increased risk of a specific type of seizure called infantile spasms. Many (15%) who live 40 years or longer develop dementia of the Alzheimer's type. In those who reach 60 years of age, 50–70% have the disease.
Senses.
Hearing and vision disorders occur in more than half of people with Down syndrome. 
Vision problems occur in 38 to 80%. Between 20 and 50% have strabismus, in which the two eyes do not move together. Cataracts (cloudiness of the lens of the eye) occur in 15%, and may be present at birth. Keratoconus (a thin, cone-shaped cornea) and glaucoma (increased eye pressure) are also more common, as are refractive errors requiring glasses or contacts. Brushfield spots (small white or grayish/brown spots on the outer part of the iris) are present in 38 to 85% of individuals.
Hearing problems are found in 50–90% of children with Down syndrome. This is often the result of otitis media with effusion which occurs in 50–70% and chronic ear infections which occur in 40 to 60%. Ear infections often begin in the first year of life and are partly due to poor eustachian tube function. Excessive ear wax can also cause hearing loss due to obstruction of the outer ear canal. Even a mild degree of hearing loss can have negative consequences for speech, language understanding, and academics. Additionally, it is important to rule out hearing loss as a factor in social and cognitive deterioration. Age-related hearing loss of the sensorineural type occurs at a much earlier age and affects 10–70% of people with Down syndrome.
Heart.
The rate of congenital heart disease in newborns with Down syndrome is around 40%. Of those with heart disease, about 80% have an atrioventricular septal defect or ventricular septal defect. Mitral valve problems become common as people age, even in those without heart problems at birth. Other problems that may occur include tetralogy of Fallot and patent ductus arteriosus. People with Down syndrome have a lower risk of hardening of the arteries.
Cancer.
Although the overall risk of cancer is not changed; the risk of leukemia and testicular cancer is increased and risk of solid cancers is reduced. Solid cancers are believed to be less common due to increased expression of tumor suppressor genes present on chromosome 21.
Cancers of the blood are 10 to 15 times more common in children with Down syndrome. In particular, acute lymphoblastic leukemia is 20 times more common and the megakaryoblastic form of acute myelogenous leukemia is 500 times more common. Transient myeloproliferative disease, a disorder of blood cell production that does not occur outside of Down syndrome, affects 3–10% of infants. The disorder is typically not serious but occasionally can be. It resolves most times without treatment; however, in those who have had it, a 20 to 30% risk of developing acute lymphoblastic leukemia at a later time exists.
Endocrine.
Problems of the thyroid gland occur in 20–50% of individuals with Down syndrome. Low thyroid is the most common form, occurring in almost half of all individuals. Thyroid problems can be due to a poorly or nonfunctioning thyroid at birth (known as congenital hypothyroidism) which occurs in 1% or can develop later due to an attack on the thyroid by the immune system resulting in Graves disease or autoimmune hypothyroidism. Type 1 diabetes mellitus is also more common.
Gastrointestinal.
Constipation occurs in nearly half of people with Down syndrome and may result in changes in behavior. One potential cause is Hirschsprung's disease, occurring in 2–15%, which is due to a lack of nerve cells controlling the colon. Other frequent congenital problems include duodenal atresia, pyloric stenosis, Meckel diverticulum, and imperforate anus. Celiac disease affects about 7–20% and gastroesophageal reflux disease is also more common.
Fertility.
Males with Down syndrome usually do not father children, while females have lower rates of fertility relative to those who are unaffected. Fertility is estimated to be present in 30–50% of women. Menopause typically occurs at an earlier age. The poor fertility in men is thought to be due to problems with sperm development; however, it may also be related to not being sexually active. As of 2006, three instances of males with Down syndrome fathering children and 26 cases of women having children have been reported. Without assisted reproductive technologies, around half of the children of someone with Down syndrome will also have the syndrome.
Genetics.
Down syndrome is caused by having three copies of the genes on chromosome 21, rather than the usual two. The parents of the affected individual are typically genetically normal. Those who have one child with Down syndrome have about a 1% risk of having a second child with the syndrome, if both parents are found to have normal karyotypes.
The extra chromosome content can arise through several different ways. The most common cause (about 92–95% of cases) is a complete extra copy of chromosome 21, resulting in trisomy 21. In 1.0 to 2.5% of cases, some of the cells in the body are normal and others have trisomy 21, known as mosaic Down syndrome. The other common mechanisms that can give rise to Down syndrome include: a Robertsonian translocation, isochromosome, or ring chromosome. These contain additional material from chromosome 21 and occur in about 2.5% of cases. An isochromosome results when the two long arms of a chromosome separate together rather than the long and short arm separating together during egg or sperm development.
Trisomy 21.
Trisomy 21 (also known by the karyotype 47,XX,+21 for females and 47,XY,+21 for males) is caused by a failure of the 21st chromosome to separate during egg or sperm development. As a result, a sperm or egg cell is produced with an extra copy of chromosome 21; this cell thus has 24 chromosomes. When combined with a normal cell from the other parent, the baby has 47 chromosomes, with three copies of chromosome 21. About 88% of cases of trisomy 21 result from nonseparation of the chromosomes in the mother, 8% from nonseparation in the father, and 3% after the egg and sperm have merged.
Translocation.
The extra chromosome 21 material may also occur due to a Robertsonian translocation in 2–4% cases. In this situation, the long arm of chromosome 21 is attached to another chromosome, often chromosome 14. In a male affected with Down syndrome, it results in a karyotype of 46XY,t(14q21q). This may be a new mutation or previously present in one of the parents. The parent with such a translocation is usually normal physically and mentally; however, during production of egg or sperm cells, a higher chance of creating reproductive cells with extra chromosome 21 material exists. This results in a 15% chance of having a child with Down syndrome when the mother is affected and a less than 5% risk if the father is affected. The risk of this type of Down syndrome is not related to the mother's age. Some children without Down syndrome may inherit the translocation and have a higher risk of having children of their own with Down syndrome. In this case it is sometimes known as familial Down syndrome.
Mechanism.
The extra genetic material present in DS results in overexpression of a portion of the 310 genes located on chromosome 21. This overexpression has been estimated at around 50%. Some research has suggested the Down syndrome critical region is located at bands 21q22.1–q22.3, with this area including genes for amyloid, superoxide dismutase, and likely the ETS-2 proto oncogene. Other research, however, has not confirmed these findings.
The dementia which occurs in Down syndrome is due to an excess of amyloid beta peptide produced in the brain and is similar to Alzheimer's disease. This peptide is processed from amyloid precursor protein, the gene for which is located on chromosome 21. Senile plaques and neurofibrillary tangles are present in nearly all by 35 years of age, though dementia may not be present. Those with DS also lack a normal number of lymphocytes and produce less antibodies which contributes to their increased risk of infection.
Screening.
Guidelines recommend screening for Down syndrome to be offered to all pregnant women, regardless of age. A number of tests can be used, with varying levels of accuracy. They are usually used in combination to increase the detection rate, while maintaining a low false-positive rate. None can be definitive, thus if screening is positive, either amniocentesis or chorionic villous sampling is required to confirm the diagnosis. Screening in both the first and second trimesters is better than just screening in the first trimester. The different screening techniques in use are able to pick up 90 to 95% of cases with a false-positive rate of 2 to 5%.
Ultrasound.
Ultrasound imaging can be used to screen for Down syndrome. Findings that indicate increased risk when seen at 14 to 24 weeks of gestation include a small or no nasal bone, large ventricles, nuchal fold thickness, and an abnormal right subclavian artery, among others. The presence or absence of many markers is more accurate. Increased fetal nuchal translucency (NT) indicates an increased risk of Down syndrome picking up 75–80% of cases and being falsely positive in 6%.
Blood tests.
Several blood markers can be measured to predict the risk of Down syndrome during the first or second trimester. Testing in both trimesters is sometimes recommended and test results are often combined with ultrasound results. In the second trimester, often two or three tests are used in combination with two or three of: α-fetoprotein, unconjugated estriol, total hCG, and free βhCG detecting about 60–70% of cases.
Testing of the mother's blood for fetal DNA is being studied and appears promising in the first trimester. The International Society for Prenatal Diagnosis considers it a reasonable screening option for those women whose pregnancies are at a high risk for trisomy 21. Accuracy has been reported at 98.6% in the first trimester of pregnancy. Confirmatory testing by invasive techniques (amniocentesis, CVS) is still required to confirm the screening result.
Diagnosis.
Before birth.
When screening tests predict a high risk of Down syndrome, a more invasive diagnostic test (amniocentesis or chorionic villus sampling) is needed to confirm the diagnosis. If Down syndrome occurs in one in 500 pregnancies and the test used has a 5% false-positive rate, this means, of 28 women who test positive on screening, only one will have Down syndrome confirmed. If the screening test has a 2% false-positive rate, this means one of 10 who test positive on screening have a fetus with DS. Amniocentesis and chorionic villus sampling are more reliable tests, but they increase the risk of miscarriage between 0.5 and 1%. The risk of limb problems is increased in the offspring due to the procedure. The risk from the procedure is greater the earlier it is performed, thus amniocentesis is not recommended before 15 weeks gestational age and chorionic villus sampling before 10 weeks gestational age.
Abortion rates.
About 92% of pregnancies in Europe with a diagnosis of Down syndrome are terminated. In the United States, termination rates are around 67%, but this varies significantly depending upon the population evaluated. When nonpregnant people are asked if they would have a termination if their fetus tested positive, 23–33% said yes, when high-risk pregnant women were asked, 46–86% said yes, and when women who screened positive are asked, 89–97% say yes.
After birth.
The diagnosis can often be suspected based on the child's physical appearance at birth. An analysis of the child's chromosomes is needed to confirm the diagnosis, and to determine if a translocation is present, as this may help determine the risk of the child's parents having further children with Down syndrome. Parents generally wish to know the possible diagnosis once it is suspected and do not wish pity.
Management.
Efforts such as early childhood intervention, screening for common problems, medical treatment where indicated, a good family environment, and work-related training can improve the development of children with Down syndrome. Education and proper care can improve quality of life. Raising a child with Down syndrome is more work for parents than raising an unaffected child. Typical childhood vaccinations are recommended.
Health screening.
A number of health organizations have issued recommendations for screening those with Down syndrome for particular diseases. This is recommended to be done systematically.
At birth, all children should get an electrocardiogram and ultrasound of the heart. Surgical repair of heart problems may be required as early as three months of age. Heart valve problems may occur in young adults, and further ultrasound evaluation may be needed in adolescents and in early adulthood. Due to the elevated risk of testicular cancer, some recommend checking the person's testicles yearly.
Cognitive development.
Hearing aids or other amplification devices can be useful for language learning in those with hearing loss. Speech therapy may be useful and is recommended to be started around 9 months of age. As those with Down's typically have good hand-eye coordination, learning sign language may be possible. Augmentative and alternative communication methods, such as pointing, body language, objects, or pictures, are often used to help with communication. Behavioral issues and mental illness are typically managed with counseling and/or medications.
Education programs before reaching school age may be useful. School-age children with Down syndrome may benefit from inclusive education (whereby students of differing abilities are placed in classes with their peers of the same age), provided some adjustments are made to the curriculum. Evidence to support this, however, is not very strong. In the United States, the Individuals with Disabilities Education Act of 1975 requires public schools generally to allow attendance by students with Down's.
Other.
Tympanostomy tubes are often needed and often more than one set during the person's childhood. Tonsillectomy is also often done to help with sleep apnea and throat infections. Surgery, however, does not always address the sleep apnea and a continuous positive airway pressure (CPAP) machine may be useful. Physical therapy and participation in physical education may improve motor skills. Evidence to support this in adults, however, is not very good.
Efforts to prevent respiratory syncytial virus (RSV) infection with human monoclonal antibodies should be considered, especially in those with heart problems. In those who develop dementia there is no evidence for memantine, donepezil, rivastigmine, or galantamine.
Plastic surgery has been suggested as a method of improving the appearance and thus the acceptance of people with Down's. It has also been proposed as a way to improve speech. Evidence, however, does not support a meaningful difference in either of these outcomes. Plastic surgery on children with Down syndrome is uncommon, and continues to be controversial. The U.S. National Down Syndrome Society views the goal as one of mutual respect and acceptance, not appearance.
Many alternative medical techniques are used in Down syndrome; however, they are poorly supported by evidence. These include: dietary changes, massage, animal therapy, chiropractics and naturopathy, among others. Some proposed treatments may also be harmful.
Prognosis.
Between 5 and 15% of children with Down syndrome in Europe attend regular school. Some graduate from high school; however, most do not. Of those with intellectual disabilities in the United States who attended high school about 40% graduated. Many learn to read and write and some are able to do paid work. In adulthood about 20% in the United States do paid work in some capacity. In Europe, however, less than 1% have regular jobs. Many are able to live semi-independently, but they often require help with financial, medical, and legal matters. Those with mosaic Down syndrome usually have better outcomes.
Individuals with Down syndrome have a higher risk of early death than the general population. This is most often from heart problems or infections. Following improved medical care, particularly for heart and gastrointestinal problems, the life expectancy has increased. This increase has been from 12 years in 1912, to 25 years in the 1980s, to 50 to 60 years in the developed world in the 2000s. Currently between 4 and 12% die in the first year of life. The probability of long-term survival is partly determined by the presence of heart problems. In those with congenital heart problems 60% survive to 10 years and 50% survive to 30 years of age. In those without heart problems 85% survive to 10 years and 80% survive to 30 years of age. About 10% live to 70 years of age.
Epidemiology.
Globally, as of 2010, Down syndrome occurs in about 1 per 1000 births and results in about 17,000 deaths. More children are born with Down syndrome in countries where abortion is not allowed and in countries where pregnancy more commonly occurs at a later age. About 1.4 per 1000 live births in the United States and 1.1 per 1000 live births in Norway are affected. In the 1950s, in the United States, it occurred in 2 per 1000 live births with the decrease since then due to prenatal screening and abortions. The number of pregnancies with Down syndrome is more than two times greater with many spontaneously aborting. It is the cause of 8% of all congenital disorders.
Maternal age affects the chances of having a pregnancy with Down syndrome. At age 20, the chance is one in 1441; at age 30, it is one in 959; at age 40, it is one in 84; and at age 50 it is one in 44. Although the probability increases with maternal age, 70% of children with Down syndrome are born to women 35 years of age and younger, because younger people have more children. The father's older age is also a risk factor in women older than 35, but not in women younger than 35, and may partly explain the increase in risk as women age.
History.
English physician John Langdon Down first characterized Down syndrome as a separate form of mental disability in 1862, and in a more widely published report in 1866. Édouard Séguin described it as separate from cretinism in 1944. By the 20th century, Down syndrome had become the most recognizable form of mental disability.
In ancient times, many infants with disabilities were either killed or abandoned. A number of historical pieces of art are believed to portray Down syndrome, including pottery from AD 500 from South America and the 16th-century painting "The Adoration of the Christ Child".
In the 20th century, many individuals with Down syndrome were institutionalized, few of the associated medical problems were treated, and most died in infancy or early adult life. With the rise of the eugenics movement, 33 of the then 48 U.S. states and several countries began programs of forced sterilization of individuals with Down syndrome and comparable degrees of disability. Action T4 in Nazi Germany made public policy of a program of systematic involuntary euthanization.
With the discovery of karyotype techniques in the 1950s, it became possible to identify abnormalities of chromosomal number or shape. In 1959, Jérôme Lejeune reported the discovery that Down syndrome resulted from an extra chromosome. However, Lejeune's claim to the discovery has been disputed, and in 2014, the Scientific Council of the French Federation of Human Genetics unanimously awarded its Grand Prize to his colleague Marthe Gautier for this discovery. As a result of this discovery, the condition became known as trisomy 21. Even before the discovery of its cause, the presence of the syndrome in all races, its association with older maternal age, and its rarity of recurrence had been noticed. Medical texts had assumed it was caused by a combination of inheritable factors that had not been identified. Other theories had focused on injuries sustained during birth.
Society and culture.
Name.
Due to his perception that children with Down syndrome shared facial similarities with those of Blumenbach's Mongolian race, Down used the term 'mongoloid'. While the term mongoloid (also mongolism or Mongolian imbecility) continued to be used until the early 1970s, it is now considered unacceptable and is no longer in common use. In 1961, 19 scientists suggested that "mongolism" had "misleading connotations" and had become "an embarrassing term". The World Health Organization (WHO) dropped the term in 1965 after a request by the Mongolian delegate.
In 1975, the United States National Institutes of Health (NIH) convened a conference to standardize the naming and recommended eliminating the possessive form, Down's syndrome, although both the possessive and nonpossessive forms are used by the general population. The term "trisomy 21" is also used frequently.
Ethics.
Some argue not to offer screening for Down syndrome is unethical As it is a medically reasonable procedure, per informed consent, people should at least be given information about it. It will then be the woman's choice, based on her personal beliefs, how much or how little screening she wishes. When results from testing become available, it is also considered unethical not to give the results to the person in question.
Some deem it reasonable for parents to select a child who would have the highest well-being. One criticism of this is it often values those with disabilities less. The disability rights movement does not have a position on screening, although some members consider testing and abortion discriminatory. Some in the United States who are pro-life support abortion if the fetus is disabled, while others do not. Of a group of 40 mothers in the United States who have had one child with Down syndrome, half agreed to screening in the next pregnancy.
Within Christianity, some Protestants denominations see abortion as acceptable when a fetus has Down syndrome, while Orthodox Christians and Roman Catholics often do not. Some of those against screening refer to it as a form of "eugenics". Disagreement exists within Islam regarding the acceptability of abortion in those carrying a fetus with Down syndrome. Some Islamic countries allow abortion, while others do not. Women may face stigmatization whichever decision they make.
Advocacy groups.
Advocacy groups for Down syndrome formed after the Second World War. These were organizations advocating for the inclusion of people with Down syndrome into the general school system and for a greater understanding of the condition among the general population, as well as groups providing support for families with children with Down syndrome. Organizations included the Royal Society for Handicapped Children and Adults founded in the UK in 1946 by Judy Fryd, Kobato Kai founded in Japan in 1964, the National Down Syndrome Congress founded in the United States in 1973 by Kathryn McGee and others, and the National Down Syndrome Society founded in 1979 in the United States.
The first World Down Syndrome Day was held on 21 March 2006. The day and month were chosen to correspond with 21 and trisomy, respectively. It was recognized by the United Nations General Assembly in 2011.
Research.
Efforts are underway to determine how the extra chromosome 21 material causes Down syndrome, as currently this is unknown, and to develop treatments to improve intelligence in those with the syndrome. One hope is to use stem cells. Other methods being studied include the use of antioxidants, gamma secretase inhibition, adrenergic agonists, and memantine. Research is often carried out on an animal model, the Ts65Dn mouse.

</doc>
<doc id="8305" url="http://en.wikipedia.org/wiki?curid=8305" title="Dyslexia">
Dyslexia

Dyslexia, also known as reading disorder or alexia, is a learning disability characterised by trouble reading despite a normal intelligence. Different people are affected to different degrees. Problems may include sounding out words, spelling words, reading quickly, writing words, pronouncing words when reading aloud, and understanding what was read. Often these difficulties are first noticed at school. The difficulties are not voluntary and people with this disorder have a normal desire to learn.
The cause of dyslexia is believed to involve both genetic and environmental factors. Some cases run in families. It occurs more often in people with attention deficit hyperactivity disorder (ADHD) and is associated with problems with mathematics. When the condition begins in adults it may be the result of a traumatic brain injury, stroke, or dementia. The underlying mechanism involves problems with the brain's processing of language. Diagnosis is by a series of tests of a person's memory, spelling, ability to see, and reading skills. It is separate from reading difficulties due to poor teaching, or hearing or vision problems.
Treatment usually involves adjusting teaching methods to meet the persons needs. While this does not cure the underlying problem, difficulties can be lessened. Treatments aimed at vision are not effective. Dyslexia is the most common learning disability. It affects about 3 to 7 percent of people. While it is diagnosed more often in males, some believed it affects males and females equally. Up to 20 percent of the population may have some degree of symptoms. Dyslexia occurs in all areas of the world.
Classification.
Dyslexia is thought to have two kinds of causes, one related to language processing, and the other, to visual processing. It is considered a cognitive disorder, not a problem with intelligence; there are often emotional problems that arise because of it. There are many published definitions which are purely descriptive or involve proposed causes that encompass a variety of reading skills, deficits, and difficulties with distinct causes rather than a single condition. The National Institute of Neurological Disorders and Stroke definition describes dyslexia as "difficulty with spelling, phonological processing (the manipulation of sounds), or rapid visual-verbal responding." The British Dyslexia Association definition describes dyslexia as "a learning difficulty that primarily affects the skills involved in accurate and fluent word reading and spelling." Characterized by "difficulties in phonological awareness, verbal memory and verbal processing speed."
Acquired dyslexia or alexia may be caused by brain damage due to a stroke or atrophy. Forms of alexia include pure alexia, surface dyslexia, semantic dyslexia, phonological dyslexia, and deep dyslexia.
Signs and symptoms.
In early childhood, symptoms that correlate with a later diagnosis of dyslexia include delayed onset of speech, difficulty distinguishing left from right, difficulty with direction, as well as being easily distracted by background noise. While they do occur in people with dyslexia, reversal of letters or words, or mirror writing, is not included in the definition of dyslexia, and its relationship with dyslexia is controversial. 
Dyslexia and attention deficit hyperactivity disorder (ADHD) commonly occur together; about 15 percent of people with dyslexia also have ADHD and 35 percent of those with ADHD have dyslexia.
School-age dyslexic children may exhibit signs of difficulty identifying or generating rhyming words, or when counting syllables in words – both of which depend on phonological awareness. They may also show difficulty when segmenting words into individual sounds or may blending sounds when producing words – indicating reduced phonemic awareness. Difficulties with word retrieval or when naming things also feature in dyslexia.:647 Dyslexics are commonly poor spellers, a feature sometimes called dysorthographia or dysgraphia, which depends on orthographic coding.
Problems persist into adolescence and adulthood and may accompany difficulties with summarizing stories, memorization, reading aloud, or learning foreign languages. Adult dyslexics can often read with good comprehension, although they tend to read more slowly than non-dyslexics and perform worse in tests of spelling or when reading nonsense words – a measure of phonological awareness.
A common myth about dyslexia is that its defining feature is reading or writing letters or words backwards; however, this is true of many children as they learn to read and write.
Language.
The orthographic complexity of a language directly impacts how difficult learning to read the language is.:266 English and French have comparatively "deep orthographies" within the Latin alphabet writing system with complex structures employing spelling patterns of several levels: letter-sound correspondence, syllables, and morphemes.:421 Other languages, such as Spanish, Italian and Finnish have mostly alphabetic orthographies which employ primarily letter-sound correspondence, so-called "shallow orthographies", making them easier to learn for people with dyslexia.:266 Logographic writing systems such as Chinese characters have graphemes that are in part unlinked to pronunciation, pose problems to dyslexic learners.
Associated conditions.
Several learning disabilities often occur together with dyslexia, but it is unclear whether they share underlying neurological causes. These disabilities may include:
Causes.
Researchers have been trying to find the neurobiological basis of dyslexia since it was first identified in 1881. An example of one of the problems dyslexics experience would be seeing letters clearly, which may be due to abnormal development of their visual nerve cells.
Neuroanatomy.
In the area of neurological research into dyslexia, modern neuroimaging techniques such as functional magnetic resonance imaging (fMRI) and positron emission tomography (PET) have shown a correlation between functional and structural differences in the brains of children with reading difficulties. Some individuals with dyslexia show less electrical activation in parts of the left hemisphere of the brain involved in reading, which includes the inferior frontal gyrus, inferior parietal lobule, and middle and ventral temporal cortex. Brain activation studies using PET to study language have produced a breakthrough in understanding of the neural basis of language over the past decade. A neural basis for the visual lexicon and for auditory verbal short-term memory components have been proposed, with some implication that the observed neural manifestation of developmental dyslexia is task-specific (i.e., functional rather than structural). fMRIs in dyslexics have provided important data supporting the interactive role, of the cerebellum and cerebral cortex as well as other brain structures.
The cerebellar theory of dyslexia is based on the association of dyslexia with balance, coordination, and time estimation. Research has identified impairment on both sides of the cerebellum, displaying lower blood flow in the areas in question when active. Furthermore, in the early stages of human development such as crawling then walking may be delayed. It should be noted that, there is no current consensus in regards to the cerebellar/Dyslexia connection.
Genetics.
Genetic research into dyslexia and its inheritance has its roots in the examination of post-autopsy brains of people with dyslexia. When they observed anatomical differences in the language center in a dyslexic brain, they showed microscopic cortical malformations known as ectopias and more rarely vascular micro-malformations, and in some instances these cortical malformations appeared as a microgyrus. These studies and others suggested abnormal cortical development which was presumed to occur before or during the sixth month of fetal brain development. Abnormal cell formations in dyslexics found on autopsy have also been reported in non-language cerebral and subcortical brain structures. Several genes have been associated with dyslexia, including DCDC2 and KIAA0319 on chromosome 6, and DYX1C1 on chromosome 15.
Gene–environment interaction.
Research has examined gene–environment interactions in reading disability through twin studies, which estimate the proportion of variance associated with environment and the proportion associated with heritability. Studies examining the influence of environmental factors such as parental education and teacher quality have determined that genetics has greater influence in supportive, rather than less optimal, environments. Instead, it may just allow those genetic risk factors to account for more of the variance in outcome, because environmental risk factors that affect that outcome have been minimized. As the environment plays a large role in learning and memory, it is likely that epigenetic modifications play an important role in reading ability. Animal experiments and measures of gene expression and methylation in the human periphery are used to study epigenetic processes, both of which have many limitations in extrapolating results for application to the human brain.
Mechanisms.
The dual-route theory of reading aloud was first described in the early 1970s. This theory suggests that two separate mental mechanisms, or cognitive routes, are involved in reading aloud, with output of both mechanisms contributing to the pronunciation of a written stimulus. One mechanism is the "lexical route", which is the process whereby skilled readers can recognize known words by sight alone, through a “dictionary” lookup procedure. The other mechanism is the "nonlexical" or "sublexical route", which is the process whereby the reader can “sound out” a written word. This is done by identifying the word's constituent parts (letters, phonemes, graphemes) and applying knowledge of how these parts are associated with each other, for example how a string of neighboring letters sound together. The dual-route system can provide an explanation for the differences in dyslexia rates between different languages (e.g. the Spanish language dependence on phonological rules account for the fact that Spanish-speaking children show a higher level of performance in non-word reading,when compared to English-speakers).
Diagnosis.
There are tests that can indicate with high probability if the person is dyslexic. This is often follow up with of a full diagnostic assessment to determine the extent and nature of the disorder, tests can be administered by a teacher or computer.
Central dyslexias.
Central dyslexias include surface dyslexia, semantic dyslexia, phonological dyslexia, and deep dyslexia. ICD-10 reclassified the previous distinction between dyslexia (315.02 in ICD-9) and alexia (315.01 in ICD-9) into a single classification as R48.0. The terms are applied for developmental dyslexia and inherited dyslexia along with developmental aphasia and inherited alexia, which are now read as cognates in meaning and synonymous.
Surface dyslexia.
In surface dyslexia, words whose pronunciations are 'regular' (highly consistent with their spelling e.g., "mint") are read more accurately than words with irregular pronunciation, such as "colonel". Difficulty distinguishing homophones is diagnostic of some forms of surface dyslexia. This disorder is usually accompanied by (surface) agraphia and fluent aphasia. Acquired surface dyslexia arises after brain damage in a previously literate person and results in pronunciation errors that indicate impairment of the lexical route.
Phonological dyslexia.
In phonological dyslexia, patients can read familiar words but have difficulty reading unfamiliar words (such as invented pseudo-words). It is thought that they can recognize words by accessing lexical memory orthographically but cannot 'sound out' novel words. Phonological dyslexia is associated with lesions in varied locations within the territory of the middle cerebral artery. The superior temporal lobe is often also involved. Furthermore, dyslexics compensate by overusing a front-brain section, called Broca's area, associated with aspects of language and speech. Research has pointed towards the theory that phonological dyslexia is a development of deep dyslexia. A treatment for phonological dyslexia is the Lindamood Phoneme Sequencing Program (LiPS). This program is based on a three-way sensory feedback process. The subject uses their auditory, visual, and oral skills to learn to recognize words and word patterns. This is considered letter-by-letter reading using a bottom-up processing technique. Case studies with a total of three patients found a significant improvement in spelling and reading ability after using LiPS.
Deep dyslexia.
Patients with deep dyslexia experience semantic paralexia (para-dyslexia) and phonological dyslexia, which happens when the patient reads a word, and says a related meaning instead of the denoted meaning. Deep alexia is associated with clear phonological processing impairments. Deep dyslexia is caused by lesions that are often widespread and include much of the left frontal lobe; specifically, research suggests that damage to the left perisylvian region of the frontal lobe causes deep dyslexia.
Peripheral dyslexias.
Peripheral dyslexias have been described as a type of acquired dyslexia which is marked by problems in processing the visual factors of terms and, not like other dyslexias, originates from an injury to the system of visual analysis. Hemianopsia is associated with this condition and is a visual field loss on the left/right side of the vertical midline.
Pure dyslexia.
Pure dyslexia (phonologically based), also known as agnosic dyslexia, dyslexia without agraphia, and pure word blindness, is dyslexia due to difficulty recognizing written sequences of letters (such as words), or sometimes even letters. It is 'pure' because it is not accompanied by other (significant) language-related impairments. Pure dyslexia does not include speech, handwriting style, language, or comprehension impairments. Pure dyslexia is caused by lesions on the visual word form area (VWFA). The VWFA is composed of the left lateral occipital sulcus and is activated during reading. A lesion in the VWFA stops transmission between the visual cortex and the left angular gyrus. It can also be caused by a lesion involving the left occipital lobe and the splenium of the corpus callosum. It is usually accompanied by a homonymous hemianopsia in the right side of the visual field. Multiple oral re-reading (MOR) is a treatment for pure dyslexia. It is considered a top-down processing technique in which patients read and re-read texts a predetermined number of times or until reading speed or accuracy improves a predetermined amount.
Hemianopic dyslexia.
Commonly considered to derive from visual field loss due to damage to the primary visual cortex. Sufferers may complain of slow reading but are able to read individual words normally. This is the most common form of peripheral alexia, and the form with the best evidence of effective treatments.
Neglect dyslexia.
In neglect dyslexia, some letters are neglected (skipped or misread) during reading – most commonly the letters at the beginning or left side of words. This alexia is associated with right parietal lesions. Use of prism glasses in treatment has been demonstrated to produce substantial benefit.
Attentional dyslexia.
People with attentional dyslexia complain of letter crowding or migration, sometimes blending elements of two words into one. Sufferers perform better when word stimuli are presented in isolation rather than flanked by other words and letters. Using a large magnifying glass may help as this may reduce the effects of flanking from nearby words; however, no trials of this or indeed any other therapy for left parietal syndromes have been published as of 2014.
Management.
Through use of compensation strategies, therapy, and educational support dyslexic individuals can learn to read and write. There are techniques and technical aids that help manage or conceal symptoms of the disorder. Removing stress and anxiety alone can sometimes improve written comprehension. For dyslexia intervention with alphabet writing systems, the fundamental aim is to increase a child's awareness of correspondences between graphemes (letters) and phonemes (sounds), and to relate these to reading and spelling by teaching blending of the sounds into words. It has been found that reinforced collateral training focused towards visual language (reading) and orthography (spelling) yields longer-lasting gains than only oral phonological training. Early intervention – when language areas of the brain are developing – is most successful in reducing long-term impacts of dyslexia. There is some evidence that use of specially tailored fonts may be helpful. Among these fonts are Dyslexie, OpenDyslexic and Lexia Readable which were created with the notion that many of the letters in the Latin alphabet are visually similar and may therefore be confusing for people with dyslexia. Dyslexie, along with OpenDyslexic, puts emphasis on making each letter more unique in order to be identified easier. Font design can have an effect on reading, reading time and the perception of legibility, this is true for all readers, including those who have dyslexia and those who do not.
There have been many studies done regarding intervention, as a result one such meta-analysis found that there was functional activation as a result. Some test results indicate how to carry out teaching strategies.
Prognosis.
From an early age, children as they age into adults with dyslexia, require instruction for word analysis and spelling, however, there are fonts which can help the reader better comprehend the material before them.
The prognosis, generally speaking is positive for individuals who are identified in childhood, but who also have support from friends and family.
Epidemiology.
The percentage of people with dyslexia is unknown but it is estimated to vary between 1 to 33 percent of the population. An average estimate is 3 to 4 percent of a given population. Internationally there are different definitions of dyslexia, but despite significant differences in writing systems different populations suffer similarly from dyslexia. Dyslexia is not limited to difficulty in converting letters to sounds, and Chinese dyslexics have difficulty in extracting shapes of Chinese characters into meanings.
History.
Dyslexia was identified by Oswald Berkhan in 1881, but the term "dyslexia" was coined in 1887 by Rudolf Berlin, an ophthalmologist in Stuttgart. He used the term to refer to a case of a young boy who had a severe impairment in learning to read and write despite showing typical intelligence and physical abilities in all other respects. In 1896, W. Pringle Morgan, a British physician from Seaford, East Sussex published a description of a reading-specific learning disorder in a report to the "British Medical Journal" titled "Congenital Word Blindness". The distinction between phonological and surface types of Dyslexia is only descriptive, and devoid of any etiological assumption as to the underlying brain mechanisms. However, studies have alluded to potential differences due to variation in performance.
Research and society.
The majority of currently available dyslexia research relates to alphabetic writing systems, and especially to European languages. However, substantial research is also available regarding dyslexia in speakers of Arabic, Chinese, Hebrew, as well as other languages.
As is the case with any disorder, society often makes an assessment based on incomplete information. At the end of the 20th century (1980's) dyslexia was thought a consequence of education, rather than a basic disability. As a result, society often misjudges those afflicted with the disorder. Sometimes there is a lack of positive attitude in the work environment towards certain people with dyslexia.
Further reading.
</dl>

</doc>
<doc id="8308" url="http://en.wikipedia.org/wiki?curid=8308" title="Delft">
Delft

Delft (]) is a city and a municipality in the Central West of the European country Netherlands. It is located in the province of South Holland, where it's situated north of Rotterdam and south of the The Hague. 
Delft is known for its historic town centre with canals, Delft Blue pottery (Delftware), the Delft University of Technology, painter Johannes Vermeer and scientist Antony van Leeuwenhoek, and its association with the Dutch royal family, the House of Orange-Nassau.
History.
Early history.
The city of Delft came into being aside a canal, the 'Delf', which comes from the word "delven", meaning digging, and led to the name Delft. It presumably started around the 11th century as a landlord court.
From a rural village in the early Middle Ages Delft developed to a city, that in the 13th century (1246) received its charter. (For some more information about the early development, see the article "Gracht", section "Delft as an example")."
The town's association with the House of Orange started when William of Orange (Willem van Oranje), nicknamed William the Silent (Willem de Zwijger), took up residence in 1572. At the time he was the leader of growing national Dutch resistance against Spanish occupation of the country, which struggle is known as the Eighty Years' War. By then Delft was one of the leading cities of Holland and it was equipped with the necessary city walls to serve as a headquarters.
After the Act of Abjuration was proclaimed in 1581 Delft became the "de facto" capital of the newly independent Netherlands, as the seat of the Prince of Orange.
When William was shot dead in 1584, by Balthazar Gerards in the hall of the Prinsenhof, the family's traditional burial place in Breda was still in the hands of the Spanish. Therefore, he was buried in the Delft Nieuwe Kerk (New Church), starting a tradition for the House of Orange that has continued to the present day.
Delft Explosion.
The Delft Explosion, also known in history as the Delft Thunderclap, occurred on 12 October 1654 when a gunpowder store exploded, destroying much of the city. Over a hundred people were killed and thousands wounded.
About 30 t of gunpowder were stored in barrels in a magazine in a former Clarissen convent in the Doelenkwartier district. Cornelis Soetens, the keeper of the magazine, opened the store to check a sample of the powder and a huge explosion followed. Luckily, many citizens were away, visiting a market in Schiedam or a fair in The Hague. Artist Carel Fabritius was wounded in the explosion and died of his injuries. Later on, Egbert van der Poel painted several pictures of Delft showing the devastation. 
Sights.
The city centre retains a large number of monumental buildings, whereas in many streets there are canals of which the borders are connected by typical bridges, altogether making this city a notable tourist destination.
Historical buildings and other sights of interest include:
Culture.
Delft is well known for the Delft pottery ceramic products which were styled on the imported Chinese porcelain of the 17th century. The city had an early start in this area since it was a home port of the Dutch East India Company. It can still be seen at the pottery factories De Koninklijke Porceleyne Fles (or Royal Delft) and De Delftse Pauw.
The painter Johannes Vermeer (1632–1675) was born in Delft. Vermeer used Delft streets and home interiors as the subject or background of his paintings.
Several other famous painters lived and worked in Delft at that time, such as Pieter de Hoogh, Carel Fabritius, Nicolaes Maes, Gerard Houckgeest and Hendrick Cornelisz. van Vliet. They all were members of the Delft School. The Delft School is known for its images of domestic life, views of households, church interiors, courtyards, squares and the streets of Delft. The painters also produced pictures showing historic events, flower paintings, portraits for patrons and the court, and decorative pieces of art.
Education.
Delft University of Technology (TU Delft) is one of three universities of technology in the Netherlands. It was founded as an academy for civil engineering in 1842 by King William II. Today well over 16,000 students are enrolled.
The UNESCO-IHE Institute for Water Education, providing postgraduate education for people from developing countries, draws on the strong tradition in water management and hydraulic engineering of the Delft university.
Economy.
In the local economic field essential elements are:
Nature and recreation.
East of Delft a relatively vast nature and recreation area called the "Delftse Hout" ("Delft Wood") is situated. Apart from a forest, through which bike-, horseride- and footpaths are leading, it also comprises a vast lake (suitable for swimming and windsurfing), narrow beaches, a restaurant, community gardens, plus campground and other recreational and sports facilities. (There is a possibility to rent bikes at the station).
Inside the city apart from a central park there are also several smaller town parks, like "Nieuwe Plantage", "Agnetapark", "Kalverbos" and others.
Furthermore there's a Botanical Garden of the TU and an arboretum in Delftse Hout.
Delft was the birthplace of:
Before 1900
After 1900
Otherwise related
International relations.
Twin towns — Sister cities.
Delft is twinned with:
Transport.
 
Trains stopping at these stations connect Delft with, among others, nearby cities of Rotterdam and The Hague, up to every five minutes, for most of the day.
There are several bus routes from Delft to similar destinations. Trams frequently travel between Delft and The Hague via special double tracks crossing the city. One of those two lines (19) is still under construction inside Delft and is meant to connect The Hague with a science park, which being developed on the southern (Rotterdam) side of Delft and is a joint project by the Delft and Rotterdam municipalities.

</doc>
<doc id="8309" url="http://en.wikipedia.org/wiki?curid=8309" title="Duesberg hypothesis">
Duesberg hypothesis

The Duesberg hypothesis is the claim, associated with University of California, Berkeley, professor Peter Duesberg, that various noninfectious factors such as recreational and pharmaceutical drug use are the cause of AIDS, and that HIV (human immunodeficiency virus) is merely a harmless passenger virus. The most prominent supporters of this hypothesis are Duesberg himself, biochemist and vitamin proponent David Rasnick, and journalist Celia Farber. The scientific community contends that Duesberg's arguments are the result of cherry-picking predominantly outdated scientific data and selectively ignoring evidence in favor of HIV's role in AIDS. The scientific consensus is that the Duesberg hypothesis is incorrect and that HIV is the cause of AIDS.
Role of legal and illegal drug use.
Duesberg argues that there is a statistical correlation between trends in recreational drug use and trends in AIDS cases. He argues that the epidemic of AIDS cases in the 1980s corresponds to a supposed epidemic of recreational drug use in the United States and Europe during the same time frame.
These claims are not supported by epidemiologic data. The average yearly increase in opioid-related deaths from 1990 to 2002 was nearly three times the yearly increase from 1979–90, with the greatest increase in 2000–02, yet AIDS cases and deaths fell dramatically during the mid-to-late-1990s. Duesberg's claim that recreational drug use, rather than HIV, was the cause of AIDS has been specifically examined and found to be false. Cohort studies have found that only HIV-positive drug users develop opportunistic infections; HIV-negative drug users do not develop such infections, indicating that HIV rather than drug use is the cause of AIDS.
Duesberg has also argued that nitrite inhalants were the cause of the epidemic of Kaposi sarcoma (KS) in gay men. However, this argument has been described as an example of the fallacy of a statistical confounding effect; it is now known that a herpesvirus, potentiated by HIV, is responsible for AIDS-associated KS.
Moreover, in addition to recreational drugs, Duesberg argues that anti-HIV drugs such as zidovudine (AZT) can cause AIDS. Duesberg's claim that antiviral medication causes AIDS is regarded as disproven by the scientific community. Placebo-controlled studies have found that AZT as a single agent produces modest and short-lived improvements in survival and delays the development of opportunistic infections; it certainly did not cause AIDS, which develops in both treated and untreated study patients. With the subsequent development of protease inhibitors and highly active antiretroviral therapy, numerous studies have documented the fact that anti-HIV drugs prevent the development of AIDS and substantially prolong survival, further disproving the claim that these drugs "cause" AIDS.
Scientific study and rejection of Duesberg's risk-AIDS hypothesis.
Several studies have specifically addressed Duesberg's claim that recreational drug abuse or sexual promiscuity were responsible for the manifestations of AIDS. An early study of his claims, published in "Nature" in 1993, found Duesberg's drug abuse-AIDS hypothesis to have "no basis in fact."
A large prospective study followed a group of 715 homosexual men in the Vancouver, Canada, area; approximately half were HIV-seropositive or became so during the follow-up period, and the remainder were HIV-seronegative. After more than 8 years of follow-up, despite similar rates of drug use, sexual contact, and other supposed risk factors in both groups, only the HIV-positive group suffered from opportunistic infections. Similarly, CD4 counts dropped in the patients who were HIV-infected, but remained stable in the HIV-negative patients, despite similar rates of risk behavior. The authors concluded that "the risk-AIDS hypothesis ... is clearly rejected by our data," and that "the evidence supports the hypothesis that HIV-1 has an integral role in the CD4 depletion and progressive immune dysfunction that characterise AIDS."
Similarly, the Multicenter AIDS Cohort Study (MACS) and the Women's Interagency HIV Study (WIHS)—which between them observed more than 8,000 Americans—demonstrated that "the presence of HIV infection is the only factor that is strongly and consistently associated with the conditions that define AIDS." A 2008 study found that recreational drug use (including cannabis, cocaine, poppers, and amphetamines) had no effect on CD4 or CD8 T-cell counts, providing further evidence against a role of recreational drugs as a cause of AIDS.
Current AIDS definitions.
Duesberg argued in 1989 that a significant number of AIDS victims had died without proof of HIV infection. However, with the use of modern culture techniques and polymerase chain reaction testing, HIV can be demonstrated in virtually all patients with AIDS. Since AIDS is now defined partially by the presence of HIV, Duesberg claims it is impossible by definition to offer evidence that AIDS doesn't require HIV. However, the first definitions of AIDS mentioned no cause and the first AIDS diagnoses were made before HIV was discovered. The addition of HIV positivity to surveillance criteria as an absolutely necessary condition for case reporting occurred only in 1993, after a scientific consensus was established that HIV caused AIDS.
AIDS in Africa.
According to the Duesberg hypothesis, AIDS is not found in Africa. What Duesberg calls "the myth of an African AIDS epidemic," among people" exists for several reasons, including:
Duesberg states that African AIDS cases are "a collection of long-established, indigenous diseases, such as chronic fevers, weight loss, alias "slim disease," diarrhea, and tuberculosis" that result from malnutrition and poor sanitation. African AIDS cases, though, have increased in the last three decades as HIV's prevalence has increased but as malnutrition percentages and poor sanitation have declined in many African regions. In addition, while HIV and AIDS are more prevalent in urban than in rural settings in Africa, malnutrition and poor sanitation are found more commonly in rural than in urban settings.
According to Duesberg, common diseases are easily misdiagnosed as AIDS in Africa because "the diagnosis of African AIDS is arbitrary" and does not include HIV testing. A definition of AIDS agreed upon in 1985 by the World Health Organization in Bangui did not require a positive HIV test, but since 1985, many African countries have added positive HIV tests to the Bangui criteria for AIDS or changed their definitions to match those of the U.S. Centers for Disease Control. One of the reasons for using more HIV tests despite their expense is that, rather than overestimating AIDS as Duesberg suggests, the Bangui definition alone excluded nearly half of African AIDS patients."
Duesberg notes that diseases associated with AIDS differ between African and Western populations, concluding that the causes of immunodeficiency must be different. Tuberculosis is much more commonly diagnosed among AIDS patients in Africa than in Western countries, while PCP conforms to the opposite pattern. Tuberculosis, though, had higher prevalence in Africa than in the West before the spread of HIV. In Africa and the United States, HIV has spurred a similar percentage increase in tuberculosis cases. PCP may be underestimated in Africa: since machinery "required for accurate testing is relatively rare in many resource-poor areas, including large parts of Africa, PCP is likely to be underdiagnosed in Africa. Consistent with this hypothesis, studies that report the highest rates of PCP in Africa are those that use the most advanced diagnostic methods" Duesberg also claims that Kaposi's Sarcoma is "exclusively diagnosed in male homosexual risk groups using nitrite inhalants and other psychoactive drugs as aphrodisiacs", but the cancer is fairly common among heterosexuals in some parts of Africa, and is found in heterosexuals in the United States as well.
Because reported AIDS cases in Africa and other parts of the developing world include a larger proportion of people who do not belong to Duesberg's preferred risk groups of drug addicts and male homosexuals, Duesberg writes on his website that "There are no risk groups in Africa, like drug addicts and homosexuals." However, many studies have addressed the issue of risk groups in Africa and concluded that the risk of AIDS is not equally distributed. In addition, AIDS in Africa largely kills sexually active working-age adults.
Duesberg claims that retroviruses like HIV must be harmless to survive.
Duesberg argues that retroviruses like HIV must be harmless to survive: they do not kill cells and they do not cause cancer, he maintains. Duesberg writes, "retroviruses do not kill cells because they depend on viable cells for the replication of their RNA from viral DNA integrated into cellular DNA." Duesberg elsewhere states that "the typical virus reproduces by entering a living cell and commandeering the cell's resources in order to make new virus particles, a process that ends with the disintegration of the dead cell."
Duesberg also rejects the involvement of retroviruses and other viruses in cancer. To him, virus-associated cancers are "freak accidents of nature" that do not warrant research programs such as the War on Cancer. Duesberg rejects a role in cancer for numerous viruses, including leukemia viruses, Epstein-Barr Virus, Human Papilloma Virus, Hepatitis B, Feline Leukemia Virus, and Human T-lymphotropic virus.
Duesberg claims that the supposedly innocuous nature of all retroviruses is supported by what he considers to be their normal mode of proliferation: infection from mother to child "in utero". Duesberg does not suggest that HIV is an endogenous retrovirus, a virus integrated into the germ line and genetically heritable:
 ...[a mother] provides her child with a nine-month continuous exposure to her blood and therefore has at least a 50 percent chance of passing HIV to the baby.
Scientific response to the Duesberg hypothesis.
The consensus in the scientific community is that the Duesberg hypothesis has been refuted by a large and growing mass of evidence showing that HIV causes AIDS, that the amount of virus in the blood correlates with disease progression, that a plausible mechanism for HIV's action has been proposed, and that anti-HIV medication decreases mortality and opportunistic infection in people with AIDS.
In the 9 December 1994 issue of "Science" (Vol. 266, No. 5191), Duesberg's methods and claims were evaluated in a group of articles. The authors concluded that
Effectiveness of antiretroviral medication.
The vast majority of people with AIDS have never received antiretroviral drugs, including those in developed countries prior to the licensure of AZT (zidovudine) in 1987, and people in developing countries today where very few individuals have access to these medications.
The NIAID reports that "in the mid-1980s, clinical trials enrolling patients with AIDS found that AZT given as single-drug therapy conferred a modest survival advantage compared [with] placebo. Among HIV-infected patients who had not yet developed AIDS, placebo-controlled trials found that AZT given as single-drug therapy delayed, for a year or two, the onset of AIDS-related illnesses. Significantly, long-term follow-up of these trials did not show a prolonged benefit of AZT, but also did not indicate that the drug increased disease progression or mortality. The lack of excess AIDS cases and death in the AZT arms of these placebo-controlled trials in effect counters the argument that AZT causes AIDS. Subsequent clinical trials found that patients receiving two-drug combinations had up to 50 percent improvements in time to progression to AIDS and in survival when compared with people receiving single-drug therapy. In more recent years, three-drug combination therapies have produced another 50 to 80 percent improvement in progression to AIDS and in survival when compared with two-drug regimens in clinical trials." "Use of potent anti-HIV combination therapies has contributed to dramatic reductions in the incidence of AIDS and AIDS-related deaths in populations where these drugs are widely available, an effect which clearly would not be seen if antiretroviral drugs caused AIDS."
Opponents claim that nearly all HIV-positive people will develop AIDS.
Duesberg claims as support for his idea that many drug-free HIV-positive people have not yet developed AIDS; HIV/AIDS scientists note that many drug-free HIV-positive people have developed AIDS, and that, in the absence of medical treatment or rare genetic factors postulated to delay disease progression, it is very likely that nearly all HIV-positive people will eventually develop AIDS. Scientists also note that HIV-negative drug users do not suffer from immune system collapse.

</doc>
<doc id="8310" url="http://en.wikipedia.org/wiki?curid=8310" title="DSL (disambiguation)">
DSL (disambiguation)

DSL or digital subscriber line is a family of technologies that provide digital data transmission over the wires of a local telephone network.
DSL may also refer to:

</doc>
<doc id="8311" url="http://en.wikipedia.org/wiki?curid=8311" title="Dinosaur">
Dinosaur

Dinosaurs are a diverse group of animals of the clade Dinosauria. They first appeared during the Triassic period, 231.4 million years ago, and were the dominant terrestrial vertebrates for 135 million years, from the beginning of the Jurassic (about 201 million years ago) until the end of the Cretaceous (66 million years ago), when the Cretaceous–Paleogene extinction event led to the extinction of most dinosaur groups at the close of the Mesozoic Era. The fossil record indicates that birds evolved from theropod dinosaurs during the Jurassic Period and, consequently, they are considered a subgroup of dinosaurs by many paleontologists. Some birds survived the extinction event that occurred 66 million years ago, and their descendants continue the dinosaur lineage to the present day.
Dinosaurs are a varied group of animals from taxonomic, morphological and ecological standpoints. Birds, at over 10,000 living species, are the most diverse group of vertebrates besides perciform fish. Using fossil evidence, paleontologists have identified over 500 distinct genera and more than 1,000 different species of non-avian dinosaurs. Dinosaurs are represented on every continent by both extant species and fossil remains. Some are herbivorous, others carnivorous. While dinosaurs were ancestrally bipedal, many extinct groups included quadrupedal species, and some were able to shift between these stances. Elaborate display structures such as horns or crests are common to all dinosaur groups, and some extinct groups developed skeletal modifications such as bony armor and spines. Evidence suggests that egg laying and nest building are additional traits shared by all dinosaurs. While modern dinosaurs (birds) are generally small due to the constraints of flight, many prehistoric dinosaurs were large-bodied—the largest sauropod dinosaurs may have achieved lengths of 58 meters (190 feet) and heights of 9.25 meters (30 feet 4 inches). Still, the idea that non-avian dinosaurs were uniformly gigantic is a misconception based on preservation bias, as large, sturdy bones are more likely to last until they are fossilized. Many dinosaurs were quite small: "Xixianykus", for example, was only about 50 cm (20 in) long.
Although the word "dinosaur" means "terrible lizard", the name is somewhat misleading, as dinosaurs are not lizards. Instead, they represent a separate group of reptiles that, like many extinct forms, did not exhibit characteristics traditionally seen as reptilian, such as a sprawling limb posture or ectothermy. Additionally, many prehistoric animals, including mosasaurs, ichthyosaurs, pterosaurs, plesiosaurs, and "Dimetrodon", are popularly conceived of as dinosaurs, but are not taxonomically classified as dinosaurs. Through the first half of the 20th century, before birds were recognized to be dinosaurs, most of the scientific community believed dinosaurs to have been sluggish and cold-blooded. Most research conducted since the 1970s, however, has indicated that all dinosaurs were active animals with elevated metabolisms and numerous adaptations for social interaction.
Since the first dinosaur fossils were recognized in the early 19th century, mounted fossil dinosaur skeletons have been major attractions at museums around the world, and dinosaurs have become an enduring part of world culture. The large sizes of some groups, as well as their seemingly monstrous and fantastic nature, have ensured dinosaurs' regular appearance in best-selling books and films, such as "Jurassic Park". Persistent public enthusiasm for the animals has resulted in significant funding for dinosaur science, and new discoveries are regularly covered by the media.
Etymology.
The taxon Dinosauria was formally named in 1842 by paleontologist Sir Richard Owen, who used it to refer to the "distinct tribe or sub-order of Saurian Reptiles" that were then being recognized in England and around the world. The term is derived from the Greek words δεινός ("deinos", meaning "terrible," "potent," or "fearfully great") and σαῦρος ("sauros", meaning "lizard" or "reptile"). Though the taxonomic name has often been interpreted as a reference to dinosaurs' teeth, claws, and other fearsome characteristics, Owen intended it merely to evoke their size and majesty.
Definition.
Under phylogenetic taxonomy, dinosaurs are usually defined as the group consisting of "Triceratops", Neornithes [modern birds], their most recent common ancestor (MRCA), and all descendants. It has also been suggested that Dinosauria be defined with respect to the MRCA of "Megalosaurus" and "Iguanodon", because these were two of the three genera cited by Richard Owen when he recognized the Dinosauria. Both definitions result in the same set of animals being defined as dinosaurs: "Dinosauria = Ornithischia + Saurischia", encompassing theropods (mostly bipedal carnivores and birds), ankylosaurians (armored herbivorous quadrupeds), stegosaurians (plated herbivorous quadrupeds), ceratopsians (herbivorous quadrupeds with horns and frills), ornithopods (bipedal or quadrupedal herbivores including "duck-bills"), and sauropodomorphs (mostly large herbivorous quadrupeds with long necks and tails).
Birds are now recognized as being the sole surviving lineage of theropod dinosaurs. In traditional taxonomy, birds were considered a separate class that had evolved from dinosaurs, a distinct superorder. However, a majority of contemporary paleontologists concerned with dinosaurs reject the traditional style of classification in favor of phylogenetic nomenclature; this approach requires that, for a group to be natural, all descendants of members of the group must be included in the group as well. Birds are thus considered to be dinosaurs and dinosaurs are, therefore, not extinct. Birds are classified as belonging to the subgroup Maniraptora, which are coelurosaurs, which are theropods, which are saurischians, which are dinosaurs.
General description.
Using one of the above definitions, dinosaurs can be generally described as archosaurs with hind limbs held erect beneath the body. Many prehistoric animal groups are popularly conceived of as dinosaurs, such as ichthyosaurs, mosasaurs, plesiosaurs, pterosaurs, and "Dimetrodon", but are not classified scientifically as dinosaurs, and none had the erect hind limb posture characteristic of true dinosaurs. Dinosaurs were the dominant terrestrial vertebrates of the Mesozoic, especially the Jurassic and Cretaceous periods. Other groups of animals were restricted in size and niches; mammals, for example, rarely exceeded the size of a cat, and were generally rodent-sized carnivores of small prey.
Dinosaurs have always been an extremely varied group of animals; according to a 2006 study, over 500 non-avialan dinosaur genera have been identified with certainty so far, and the total number of genera preserved in the fossil record has been estimated at around 1850, nearly 75% of which remain to be discovered. An earlier study predicted that about 3400 dinosaur genera existed, including many which would not have been preserved in the fossil record. By September 17, 2008, 1047 different species of dinosaurs had been named. Some are herbivorous, others carnivorous, including seed-eaters, fish-eaters, insectivores, and omnivores. While dinosaurs were ancestrally bipedal (as are all modern birds), some prehistoric species were quadrupeds, and others, such as "Ammosaurus" and "Iguanodon", could walk just as easily on two or four legs. Cranial modifications like horns and crests are common dinosaurian traits, and some extinct species had bony armor. Although known for large size, many Mesozoic dinosaurs were human-sized or smaller, and modern birds are generally small in size. Dinosaurs today inhabit every continent, and fossils show that they had achieved global distribution by at least the early Jurassic period. Modern birds inhabit most available habitats, from terrestrial to marine, and there is evidence that some non-avialan dinosaurs (such as "Microraptor") could fly or at least glide, and others, such as spinosaurids, had semi-aquatic habits.
Distinguishing anatomical features.
While recent discoveries have made it more difficult to present a universally agreed-upon list of dinosaurs' distinguishing features, nearly all dinosaurs discovered so far share certain modifications to the ancestral archosaurian skeleton, or are clear descendants of older dinosaurs showing these modifications. Although some later groups of dinosaurs featured further modified versions of these traits, they are considered typical for Dinosauria; the earliest dinosaurs had them and passed them on to their descendants. Such modifications, originating in the last common ancestor of a certain taxonomic group, are called the synapomorphies of such a group.
A detailed assessment of archosaur interrelations by S. Nesbitt confirmed or found the following twelve unambiguous synapomorphies, some previously known:
Nesbitt found a number of further potential synapomorphies, and discounted a number of synapomorphies previously suggested. Some of these are also present in silesaurids, which Nesbitt recovered as a sister group to Dinosauria, including a large anterior trochanter, metatarsals II and IV of subequal length, reduced contact between ischium and pubis, the presence of a cnemial crest on the tibia and of an ascending process on the astragalus, and many others.
A variety of other skeletal features are shared by dinosaurs. However, because they are either common to other groups of archosaurs or were not present in all early dinosaurs, these features are not considered to be synapomorphies. For example, as diapsids, dinosaurs ancestrally had two pairs of temporal fenestrae (openings in the skull behind the eyes), and as members of the diapsid group Archosauria, had additional openings in the snout and lower jaw. Additionally, several characteristics once thought to be synapomorphies are now known to have appeared before dinosaurs, or were absent in the earliest dinosaurs and independently evolved by different dinosaur groups. These include an elongated scapula, or shoulder blade; a sacrum composed of three or more fused vertebrae (three are found in some other archosaurs, but only two are found in "Herrerasaurus"); and a perforate acetabulum, or hip socket, with a hole at the center of its inside surface (closed in "Saturnalia", for example). Another difficulty of determining distinctly dinosaurian features is that early dinosaurs and other archosaurs from the Late Triassic are often poorly known and were similar in many ways; these animals have sometimes been misidentified in the literature.
Dinosaurs stand with their hind limbs erect in a manner similar to most modern mammals, but distinct from most other reptiles, whose limbs sprawl out to either side. This posture is due to the development of a laterally facing recess in the pelvis (usually an open socket) and a corresponding inwardly facing distinct head on the femur. Their erect posture enabled early dinosaurs to breathe easily while moving, which likely permitted stamina and activity levels that surpassed those of "sprawling" reptiles. Erect limbs probably also helped support the evolution of large size by reducing bending stresses on limbs. Some non-dinosaurian archosaurs, including rauisuchians, also had erect limbs but achieved this by a "pillar erect" configuration of the hip joint, where instead of having a projection from the femur insert on a socket on the hip, the upper pelvic bone was rotated to form an overhanging shelf.
Evolutionary history.
Origins and early evolution.
Dinosaurs diverged from their archosaur ancestors during the Middle to Late Triassic period, roughly 20 million years after the Permian–Triassic extinction event wiped out an estimated 95% of all life on Earth. Radiometric dating of the rock formation that contained fossils from the early dinosaur genus "Eoraptor" at 231.4 million years old establishes its presence in the fossil record at this time. Paleontologists think that "Eoraptor" resembles the common ancestor of all dinosaurs; if this is true, its traits suggest that the first dinosaurs were small, bipedal predators. The discovery of primitive, dinosaur-like ornithodirans such as "Marasuchus" and "Lagerpeton" in Argentinian Middle Triassic strata supports this view; analysis of recovered fossils suggests that these animals were indeed small, bipedal predators. Dinosaurs may have appeared as early as 243 million years ago, as evidenced by remains of the genus "Nyasasaurus" from that period, though known fossils of these animals are too fragmentary to tell if they are dinosaurs or very close dinosaurian relatives.
When dinosaurs appeared, they were not the dominant terrestrial animals. The terrestrial habitats were occupied by various types of archosauromorphs and therapsids, like cynodonts and rhynchosaurs. Their main competitors were the pseudosuchia, such as aetosaurs, ornithosuchids and rauisuchians, which were more successful than the dinosaurs. Most of these other animals became extinct in the Triassic, in one of two events. First, at about 215 million years ago, variety of basal archosauromorphs, including the protorosaurs, became extinct. This was followed by the Triassic–Jurassic extinction event (about 200 million years ago), that saw the end of most of the other groups of early archosaurs, like aetosaurs, ornithosuchids, phytosaurs, and rauisuchians. Rhynchosaurs and dicynodonts survived (at least in some areas) at least as late as early-mid Norian and early Rhaetian, respectively, and the exact date of their extinction is uncertain. These losses left behind a land fauna of crocodylomorphs, dinosaurs, mammals, pterosaurians, and turtles. The first few lines of early dinosaurs diversified through the Carnian and Norian stages of the Triassic, possibly by occupying the niches of the groups that became extinct.
Evolution and paleobiogeography.
Dinosaur evolution after the Triassic follows changes in vegetation and the location of continents. In the Late Triassic and Early Jurassic, the continents were connected as the single landmass Pangaea, and there was a worldwide dinosaur fauna mostly composed of coelophysoid carnivores and early sauropodomorph herbivores. Gymnosperm plants (particularly conifers), a potential food source, radiated in the Late Triassic. Early sauropodomorphs did not have sophisticated mechanisms for processing food in the mouth, and so must have employed other means of breaking down food farther along the digestive tract. The general homogeneity of dinosaurian faunas continued into the Middle and Late Jurassic, where most localities had predators consisting of ceratosaurians, spinosauroids, and carnosaurians, and herbivores consisting of stegosaurian ornithischians and large sauropods. Examples of this include the Morrison Formation of North America and Tendaguru Beds of Tanzania. Dinosaurs in China show some differences, with specialized sinraptorid theropods and unusual, long-necked sauropods like "Mamenchisaurus". Ankylosaurians and ornithopods were also becoming more common, but prosauropods had become extinct. Conifers and pteridophytes were the most common plants. Sauropods, like the earlier prosauropods, were not oral processors, but ornithischians were evolving various means of dealing with food in the mouth, including potential cheek-like organs to keep food in the mouth, and jaw motions to grind food. Another notable evolutionary event of the Jurassic was the appearance of true birds, descended from maniraptoran coelurosaurians.
By the Early Cretaceous and the ongoing breakup of Pangaea, dinosaurs were becoming strongly differentiated by landmass. The earliest part of this time saw the spread of ankylosaurians, iguanodontians, and brachiosaurids through Europe, North America, and northern Africa. These were later supplemented or replaced in Africa by large spinosaurid and carcharodontosaurid theropods, and rebbachisaurid and titanosaurian sauropods, also found in South America. In Asia, maniraptoran coelurosaurians like dromaeosaurids, troodontids, and oviraptorosaurians became the common theropods, and ankylosaurids and early ceratopsians like "Psittacosaurus" became important herbivores. Meanwhile, Australia was home to a fauna of basal ankylosaurians, hypsilophodonts, and iguanodontians. The stegosaurians appear to have gone extinct at some point in the late Early Cretaceous or early Late Cretaceous. A major change in the Early Cretaceous, which would be amplified in the Late Cretaceous, was the evolution of flowering plants. At the same time, several groups of dinosaurian herbivores evolved more sophisticated ways to orally process food. Ceratopsians developed a method of slicing with teeth stacked on each other in batteries, and iguanodontians refined a method of grinding with tooth batteries, taken to its extreme in hadrosaurids. Some sauropods also evolved tooth batteries, best exemplified by the rebbachisaurid "Nigersaurus".
There were three general dinosaur faunas in the Late Cretaceous. In the northern continents of North America and Asia, the major theropods were tyrannosaurids and various types of smaller maniraptoran theropods, with a predominantly ornithischian herbivore assemblage of hadrosaurids, ceratopsians, ankylosaurids, and pachycephalosaurians. In the southern continents that had made up the now-splitting Gondwana, abelisaurids were the common theropods, and titanosaurian sauropods the common herbivores. Finally, in Europe, dromaeosaurids, rhabdodontid iguanodontians, nodosaurid ankylosaurians, and titanosaurian sauropods were prevalent. Flowering plants were greatly radiating, with the first grasses appearing by the end of the Cretaceous. Grinding hadrosaurids and shearing ceratopsians became extremely diverse across North America and Asia. Theropods were also radiating as herbivores or omnivores, with therizinosaurians and ornithomimosaurians becoming common.
The Cretaceous–Paleogene extinction event, which occurred approximately 66 million years ago at the end of the Cretaceous period, caused the extinction of all dinosaur groups except for the neornithine birds. Some other diapsid groups, such as crocodilians, sebecosuchians, turtles, lizards, snakes, sphenodontians, and choristoderans, also survived the event.
The surviving lineages of neornithine birds, including the ancestors of modern ratites, ducks and chickens, and a variety of waterbirds, diversified rapidly at the beginning of the Paleogene period, entering ecological niches left vacant by the extinction of Mesozoic dinosaur groups such as the arboreal enantiornithines, aquatic hesperornithines, and even the larger terrestrial theropods (in the form of "Gastornis", mihirungs, and "terror birds"). However, mammals were also rapidly diversifying during this time, and out-competed the neornithines for dominance of most terrestrial niches.
Classification.
Dinosaurs are archosaurs, like modern crocodilians. Within the archosaur group, dinosaurs are differentiated most noticeably by their gait. Dinosaur legs extend directly beneath the body, whereas the legs of lizards and crocodilians sprawl out to either side.
Collectively, dinosaurs as a clade are divided into two primary branches, Saurischia and Ornithischia. Saurischia includes those taxa sharing a more recent common ancestor with birds than with Ornithischia, while Ornithischia includes all taxa sharing a more recent common ancestor with "Triceratops" than with Saurischia. Anatomically, these two groups can be distinguished most noticeably by their pelvic structure. Early saurischians—"lizard-hipped", from the Greek "sauros" (σαῦρος) meaning "lizard" and "ischion" (ἰσχίον) meaning "hip joint—retained the hip structure of their ancestors, with a pubis bone directed cranially, or forward. This basic form was modified by rotating the pubis backward to varying degrees in several groups ("Herrerasaurus", therizinosauroids, dromaeosaurids, and birds). Saurischia includes the theropods (exclusively bipedal and with a wide variety of diets) and sauropodomorphs (long-necked herbivores which include advanced, quadrupedal groups).
By contrast, ornithischians—"bird-hipped", from the Greek "ornitheios" (ὀρνίθειος) meaning "of a bird" and "ischion" (ἰσχίον) meaning "hip joint"—had a pelvis that superficially resembled a bird's pelvis: the pubis bone was oriented caudally (rear-pointing). Unlike birds, the ornithischian pubis also usually had an additional forward-pointing process. Ornithischia includes a variety of species which were primarily herbivores. (NB: the terms "lizard hip" and "bird hip" are misnomers – birds evolved from dinosaurs with "lizard hips".)
Taxonomy.
The following is a simplified classification of dinosaur groups based on their evolutionary relationships, and organized based on the list of Mesozoic dinosaur species provided by Holtz (2008). A more detailed version can be found at Dinosaur classification.
The dagger (†) is used to signify groups with no living members.
Biology.
Knowledge about dinosaurs is derived from a variety of fossil and non-fossil records, including fossilized bones, feces, trackways, gastroliths, feathers, impressions of skin, internal organs and soft tissues. Many fields of study contribute to our understanding of dinosaurs, including physics (especially biomechanics; dinosaur mass, speed and blood flow), chemistry, biology, and the earth sciences (of which paleontology is a sub-discipline). Two topics of particular interest and study have been dinosaur size and behavior.
Size.
Current evidence suggests that dinosaur average size varied through the Triassic, early Jurassic, late Jurassic and Cretaceous periods. Predatory theropod dinosaurs, which occupied most terrestrial carnivore niches during the Mesozoic, most often fall into the 100 to 1000 kilogram (220 to 2200 lb) category when sorted by estimated weight into categories based on order of magnitude, whereas recent predatory carnivoran mammals peak in the 10 to 100 kilogram (22 to 220 lb) category. The mode of Mesozoic dinosaur body masses is between one and ten metric tonnes. This contrasts sharply with the size of Cenozoic mammals, estimated by the National Museum of Natural History as about 2 to 5 kilograms (5 to 10 lb).
The sauropods were the largest and heaviest dinosaurs. For much of the dinosaur era, the smallest sauropods were larger than anything else in their habitat, and the largest were an order of magnitude more massive than anything else that has since walked the Earth. Giant prehistoric mammals such as "Paraceratherium" (the largest land mammal ever) were dwarfed by the giant sauropods, and only modern whales approach or surpass them in size. There are several proposed advantages for the large size of sauropods, including protection from predation, reduction of energy use, and longevity, but it may be that the most important advantage was dietary. Large animals are more efficient at digestion than small animals, because food spends more time in their digestive systems. This also permits them to subsist on food with lower nutritive value than smaller animals. Sauropod remains are mostly found in rock formations interpreted as dry or seasonally dry, and the ability to eat large quantities of low-nutrient browse would have been advantageous in such environments.
Largest and smallest.
Scientists will probably never be certain of the largest and smallest dinosaurs to have ever existed. This is because only a tiny percentage of animals ever fossilize, and most of these remain buried in the earth. Few of the specimens that are recovered are complete skeletons, and impressions of skin and other soft tissues are rare. Rebuilding a complete skeleton by comparing the size and morphology of bones to those of similar, better-known species is an inexact art, and reconstructing the muscles and other organs of the living animal is, at best, a process of educated guesswork.
The tallest and heaviest dinosaur known from good skeletons is "Giraffatitan brancai" (previously classified as a species of "Brachiosaurus"). Its remains were discovered in Tanzania between 1907 and 1912. Bones from several similar-sized individuals were incorporated into the skeleton now mounted and on display at the Museum für Naturkunde Berlin; this mount is 12 m tall and 21.8 - long, and would have belonged to an animal that weighed between and  kilograms ( and  lb). The longest complete dinosaur is the 27-meter (89 ft) long "Diplodocus", which was discovered in Wyoming in the United States and displayed in Pittsburgh's Carnegie Natural History Museum in 1907.
There were larger dinosaurs, but knowledge of them is based entirely on a small number of fragmentary fossils. Most of the largest herbivorous specimens on record were all discovered in the 1970s or later, and include the massive "Argentinosaurus", which may have weighed to  kilograms (90 to 110 short tons); some of the longest were the 33.5 m long "Diplodocus hallorum" (formerly "Seismosaurus") and the 33 m long "Supersaurus"; and the tallest, the 18 m tall "Sauroposeidon", which could have reached a sixth-floor window. The heaviest and longest of them all may have been "Amphicoelias fragillimus", known only from a now lost partial vertebral neural arch described in 1878. Extrapolating from the illustration of this bone, the animal may have been 58 m long and weighed over kg ( lb). The largest known carnivorous dinosaur was "Spinosaurus", reaching a length of 16 to 18 meters (52 to 60 ft), and weighing in at 8150 kg ( lb). Other large carnivorous theropods included "Giganotosaurus", "Carcharodontosaurus" and "Tyrannosaurus". "Therizinosaurus" and "Deinocheirus" were among the tallest of the theropods.
Not including birds (Avialae), the smallest known dinosaurs were about the size of pigeons. The smallest non-avialan dinosaurs were those theropods most closely related to birds. "Anchiornis huxleyi", for example, had a total skeletal length of under 35 centimeters (1.1 ft). "A. huxleyi" is currently the smallest non-avialan dinosaur described from an adult specimen, with an estimated weight of 110 grams. The smallest herbivorous non-avialan dinosaurs included "Microceratus" and "Wannanosaurus", at about 60 cm long each.
The smallest dinosaur known is the bee hummingbird, with length of only 5 cm (2 in) and mass of around 1.8 g (0.06 oz).
Behavior.
Many modern birds are highly social, often found living in flocks. There is general agreement that some behaviors which are common in birds, as well as in crocodiles (birds' closest living relatives), were also common among extinct dinosaur groups. Interpretations of behavior in fossil species are generally based on the pose of skeletons and their habitat, computer simulations of their biomechanics, and comparisons with modern animals in similar ecological niches.
The first potential evidence for herding or flocking as a widespread behavior common to many dinosaur groups in addition to birds was the 1878 discovery of 31 "Iguanodon bernissartensis", ornithischians which were then thought to have perished together in Bernissart, Belgium, after they fell into a deep, flooded sinkhole and drowned. Other mass-death sites have been subsequently discovered. Those, along with multiple trackways, suggest that gregarious behavior was common in many early dinosaur species. Trackways of hundreds or even thousands of herbivores indicate that duck-bills (hadrosaurids) may have moved in great herds, like the American bison or the African springbok. Sauropod tracks document that these animals traveled in groups composed of several different species, at least in Oxfordshire, England, although there is not evidence for specific herd structures. Congregating into herds may have evolved for defense, for migratory purposes, or to provide protection for young. There is evidence that many types of slow-growing dinosaurs, including various theropods, sauropods, ankylosaurians, ornithopods, and ceratopsians, formed aggregations of immature individuals. One example is a site in Inner Mongolia that has yielded the remains of over 20 "Sinornithomimus", from one to seven years old. This assemblage is interpreted as a social group that was trapped in mud. The interpretation of dinosaurs as gregarious has also extended to depicting carnivorous theropods as pack hunters working together to bring down large prey. However, this lifestyle is uncommon among modern birds, crocodiles, and other reptiles, and the taphonomic evidence suggesting mammal-like pack hunting in such theropods as "Deinonychus" and "Allosaurus" can also be interpreted as the results of fatal disputes between feeding animals, as is seen in many modern diapsid predators.
The crests and frills of some dinosaurs, like the marginocephalians, theropods and lambeosaurines, may have been too fragile to be used for active defense, and so they were likely used for sexual or aggressive displays, though little is known about dinosaur mating and territorialism. Head wounds from bites suggest that theropods, at least, engaged in active aggressive confrontations.
From a behavioral standpoint, one of the most valuable dinosaur fossils was discovered in the Gobi Desert in 1971. It included a "Velociraptor" attacking a "Protoceratops", providing evidence that dinosaurs did indeed attack each other. Additional evidence for attacking live prey is the partially healed tail of an "Edmontosaurus", a hadrosaurid dinosaur; the tail is damaged in such a way that shows the animal was bitten by a tyrannosaur but survived. Cannibalism amongst some species of dinosaurs was confirmed by tooth marks found in Madagascar in 2003, involving the theropod "Majungasaurus".
Comparisons between the scleral rings of dinosaurs and modern birds and reptiles have been used to infer daily activity patterns of dinosaurs. Although it has been suggested that most dinosaurs were active during the day, these comparisons have shown that small predatory dinosaurs such as dromaeosaurids, "Juravenator", and "Megapnosaurus" were likely nocturnal. Large and medium-sized herbivorous and omnivorous dinosaurs such as ceratopsians, sauropodomorphs, hadrosaurids, ornithomimosaurs may have been cathemeral, active during short intervals throughout the day, although the small ornithischian "Agilisaurus" was inferred to be diurnal.
Based on current fossil evidence from dinosaurs such as "Oryctodromeus", some ornithischian species seem to have led a partially fossorial (burrowing) lifestyle. Many modern birds are arboreal (tree climbing), and this was also true of many Mesozoic birds, especially the enantiornithines. While some early bird-like species may have already been arboreal as well (including dromaeosaurids such as "Microraptor") most non-avialan dinosaurs seem to have relied on land-based locomotion. A good understanding of how dinosaurs moved on the ground is key to models of dinosaur behavior; the science of biomechanics, in particular, has provided significant insight in this area. For example, studies of the forces exerted by muscles and gravity on dinosaurs' skeletal structure have investigated how fast dinosaurs could run, whether diplodocids could create sonic booms via whip-like tail snapping, and whether sauropods could float.
Communication.
Modern birds are well known for communicating using primarily visual and auditory signals, and the wide diversity of visual display structures among fossil dinosaur groups suggests that visual communication has always been important to dinosaur biology. However, the evolution of dinosaur vocalization is less certain. In 2008, paleontologist Phil Senter examined the evidence for vocalization in Mesozoic animal life, including dinosaurs. Senter found that, contrary to popular depictions of roaring dinosaurs in motion pictures, it is likely that most Mesozoic dinosaurs were not capable of creating any vocalizations (though the hollow crests of the lambeosaurines could have functioned as resonance chambers used for a wide range of vocalizations). To draw this conclusion, Senter studied the distribution of vocal organs in modern reptiles and birds. He found that vocal cords in the larynx probably evolved multiple times among reptiles, including crocodilians, which are able to produce guttural roars. Birds, on the other hand, lack a larynx. Instead, bird calls are produced by the syrinx, a vocal organ found only in birds, and which is not related to the larynx, meaning it evolved independently from the vocal organs in reptiles. The syrinx depends on the air sac system in birds to function; specifically, it requires the presence of a "clavicular air sac" near the wishbone or collar bone. This air sac leaves distinctive marks or opening on the bones, including a distinct opening in the upper arm bone ("humerus"). While extensive air sac systems are a unique characteristic of saurischian dinosaurs, the clavicular air sac necessary to vocalize does not appear in the fossil record until the enantiornithines (one exception, "Aerosteon", probably evolved its clavicular air sac independently of birds for reasons other than vocalization).
The most primitive dinosaurs with evidence of a vocalizing syrinx are the enantironithine birds. Any bird-line archosaurs more primitive than this probably did not make vocal calls. Rather, several lines of evidence suggest that early dinosaurs used primarily visual communication, in the form of distinctive-looking (and possibly brightly colored) horns, frills, crests, sails and feathers. This is similar to some modern reptile groups such as lizards, in which many forms are largely silent (though like dinosaurs they possess well-developed senses of hearing) but use complex coloration and display behaviors to communicate.
In addition, dinosaurs use other methods of producing sound for communication. Other animals, including other reptiles, use a wide variety of non-vocal sound communication, including hissing, jaw grinding or clapping, use of environment (such as splashing), and wing beating (possible in winged maniraptoran dinosaurs).
Reproductive biology.
All dinosaurs lay amniotic eggs with hard shells made mostly of calcium carbonate. Eggs are usually laid in a nest. Most species create somewhat elaborate nests, which can be cups, domes, plates, beds scrapes, mounds, or burrows. Some species of modern bird have no nests; the cliff-nesting common guillemot lays its eggs on bare rock, and male emperor penguins keep eggs between their body and feet. Primitive birds and many non-avialan dinosaurs often lay eggs in communal nests, with males primarily incubating the eggs. While modern birds have only one functional oviduct and lay one egg at a time, more primitive birds and dinosaurs had two oviducts, like crocodiles. Some non-avialan dinosaurs, such as "Troodon", exhibited iterative laying, where the adult might lay a pair of eggs every one or two days, and then ensured simultaneous hatching by delaying brooding until all eggs were laid.
When laying eggs, females grow a special type of bone between the hard outer bone and the marrow of their limbs. This medullary bone, which is rich in calcium, is used to make eggshells. A discovery of features in a "Tyrannosaurus rex" skeleton provided evidence of medullary bone in extinct dinosaurs and, for the first time, allowed paleontologists to establish the sex of a fossil dinosaur specimen. Further research has found medullary bone in the carnosaur "Allosaurus" and the ornithopod "Tenontosaurus". Because the line of dinosaurs that includes "Allosaurus" and "Tyrannosaurus" diverged from the line that led to "Tenontosaurus" very early in the evolution of dinosaurs, this suggests that the production of medullary tissue is a general characteristic of all dinosaurs.
Another widespread trait among modern birds is parental care for young after hatching. Jack Horner's 1978 discovery of a "Maiasaura" ("good mother lizard") nesting ground in Montana demonstrated that parental care continued long after birth among ornithopods, suggesting this behavior might also have been common to all dinosaurs. There is evidence that other non-theropod dinosaurs, like Patagonian titanosaurian sauropods (1997 discovery), also nested in large groups. A specimen of the Mongolian oviraptorid "Citipati osmolskae" was discovered in a chicken-like brooding position in 1993, which indicates that they had begun using an insulating layer of feathers to keep the eggs warm. Parental care being a trait common to all dinosaurs is supported by other finds. For example, a dinosaur embryo (pertaining to the prosauropod "Massospondylus") was found without teeth, indicating that some parental care was required to feed the young dinosaurs. Trackways have also confirmed parental behavior among ornithopods from the Isle of Skye in northwestern Scotland. Nests and eggs have been found for most major groups of dinosaurs, and it appears likely that all dinosaurs cared for their young to some extent either before or shortly after hatching.
Physiology.
Because both modern crocodilians and birds have four-chambered hearts (albeit modified in crocodilians), it is likely that this is a trait shared by all archosaurs, including all dinosaurs. While all modern birds have high metabolisms and are "warm blooded" (endothermic), a vigorous debate has been ongoing since the 1960s regarding how far back in the dinosaur lineage this trait extends. Scientists disagree as to whether non-avian dinosaurs were endothermic, ectothermic, or some combination of both.
After non-avian dinosaurs were discovered, paleontologists first posited that they were ectothermic. This supposed "cold-bloodedness" was used to imply that the ancient dinosaurs were relatively slow, sluggish organisms, even though many modern reptiles are fast and light-footed despite relying on external sources of heat to regulate their body temperature. The idea of dinosaurs as ectothermic and sluggish remained a prevalent view until Robert T. "Bob" Bakker, an early proponent of dinosaur endothermy, published an influential paper on the topic in 1968.
Modern evidence indicates that even non-avian dinosaurs and birds thrived in cooler temperate climates, and that at least some early species must have regulated their body temperature by internal biological means (aided by the animals' bulk in large species and feathers or other body coverings in smaller species). Evidence of endothermy in Mesozoic dinosaurs includes the discovery of polar dinosaurs in Australia and Antarctica as well as analysis of blood-vessel structures within fossil bones that are typical of endotherms. Scientific debate continues regarding the specific ways in which dinosaur temperature regulation evolved.
In saurischian dinosaurs, higher metabolisms were supported by the evolution of the avian respiratory system, characterized by an extensive system of air sacs that extended the lungs and invaded many of the bones in the skeleton, making them hollow. Early avian-style respiratory systems with air sacs may have been capable of sustaining higher activity levels than mammals of similar size and build could sustain. In addition to providing a very efficient supply of oxygen, the rapid airflow would have been an effective cooling mechanism, which is essential for animals that are active but too large to get rid of all the excess heat through their skin.
Like other reptiles, dinosaurs are primarily uricotelic, that is, their kidneys extract nitrogenous wastes from their bloodstream and excrete it as uric acid instead of urea or ammonia via the ureters into the intestine. In most living species, uric acid is excreted along with feces as a semisolid waste. However, at least some modern birds (such as hummingbirds) can be facultatively ammonotelic, excreting most of the nitrogenous wastes as ammonia. They also excrete creatine, rather than creatinine like mammals. This material, as well as the output of the intestines, emerges from the cloaca. In addition, many species regurgitate pellets, and fossil pellets that may have come from dinosaurs are known from as long ago as the Cretaceous period.
Origin of birds.
The possibility that dinosaurs were the ancestors of birds was first suggested in 1868 by Thomas Henry Huxley. After the work of Gerhard Heilmann in the early 20th century, the theory of birds as dinosaur descendants was abandoned in favor of the idea of their being descendants of generalized thecodonts, with the key piece of evidence being the supposed lack of clavicles in dinosaurs. However, as later discoveries showed, clavicles (or a single fused wishbone, which derived from separate clavicles) were not actually absent; they had been found as early as 1924 in "Oviraptor", but misidentified as an interclavicle. In the 1970s, John Ostrom revived the dinosaur–bird theory, which gained momentum in the coming decades with the advent of cladistic analysis, and a great increase in the discovery of small theropods and early birds. Of particular note have been the fossils of the Yixian Formation, where a variety of theropods and early birds have been found, often with feathers of some type. Birds share over a hundred distinct anatomical features with theropod dinosaurs, which are now generally accepted to have been their closest ancient relatives.
They are most closely allied with maniraptoran coelurosaurs. A minority of scientists, most notably Alan Feduccia and Larry Martin, have proposed other evolutionary paths, including revised versions of Heilmann's basal archosaur proposal, or that maniraptoran theropods are the ancestors of birds but themselves are not dinosaurs, only convergent with dinosaurs.
Feathers.
Feathers are one of the most recognizable characteristics of modern birds, and a trait that was shared by all other dinosaur groups. Based on the current distribution of fossil evidence, it appears that feathers were an ancestral dinosaurian trait, though one that may have been selectively lost in some species. Direct fossil evidence of feathers or feather-like structures has been discovered in a diverse array of species in many non-avian dinosaur groups, both among saurischians and ornithischians. Simple, branched, feather-like structures are known from heterodontosaurids, primitive neornithischians and theropods, and primitive ceratopsians. Evidence for true, vaned feathers similar to the flight feathers of modern birds has been found only in the theropod subgroup Maniraptora, which includes oviraptorosaurs, troodontids, dromaeosaurids, and birds. Feather-like structures known as pycnofibres have also been found in pterosaurs, suggesting the possibility that feather-like filaments may have been common in the bird lineage and evolved before the appearance of dinosaurs themselves.
"Archaeopteryx" was the first fossil found which revealed a potential connection between dinosaurs and birds. It is considered a transitional fossil, in that it displays features of both groups. Brought to light just two years after Darwin's seminal "The Origin of Species", its discovery spurred the nascent debate between proponents of evolutionary biology and creationism. This early bird is so dinosaur-like that, without a clear impression of feathers in the surrounding rock, at least one specimen was mistaken for "Compsognathus". Since the 1990s, a number of additional feathered dinosaurs have been found, providing even stronger evidence of the close relationship between dinosaurs and modern birds. Most of these specimens were unearthed in the lagerstätte of the Yixian Formation, Liaoning, northeastern China, which was part of an island continent during the Cretaceous. Though feathers have been found in only a few locations, it is possible that non-avian dinosaurs elsewhere in the world were also feathered. The lack of widespread fossil evidence for feathered non-avian dinosaurs may be because delicate features like skin and feathers are not often preserved by fossilization and thus are absent from the fossil record.
The description of feathered dinosaurs has not been without controversy; perhaps the most vocal critics have been Alan Feduccia and Theagarten Lingham-Soliar, who have proposed that some purported feather-like fossils are the result of the decomposition of collagenous fiber that underlaid the dinosaurs' skin, and that maniraptoran dinosaurs with vaned feathers were not actually dinosaurs, but convergent with dinosaurs. However, their views have for the most part not been accepted by other researchers, to the point that the question of the scientific nature of Feduccia's proposals has been raised.
Skeleton.
Because feathers are often associated with birds, feathered dinosaurs are often touted as the missing link between birds and dinosaurs. However, the multiple skeletal features also shared by the two groups represent another important line of evidence for paleontologists. Areas of the skeleton with important similarities include the neck, pubis, wrist (semi-lunate carpal), arm and pectoral girdle, furcula (wishbone), and breast bone. Comparison of bird and dinosaur skeletons through cladistic analysis strengthens the case for the link.
Soft anatomy.
Large meat-eating dinosaurs had a complex system of air sacs similar to those found in modern birds, according to an investigation which was led by Patrick O'Connor of Ohio University. The lungs of theropod dinosaurs (carnivores that walked on two legs and had bird-like feet) likely pumped air into hollow sacs in their skeletons, as is the case in birds. "What was once formally considered unique to birds was present in some form in the ancestors of birds", O'Connor said. In a 2008 paper published in the online journal "PLoS ONE", scientists described "Aerosteon riocoloradensis", the skeleton of which supplies the strongest evidence to date of a dinosaur with a bird-like breathing system. CT-scanning of "Aerosteon"'s fossil bones revealed evidence for the existence of air sacs within the animal's body cavity.
Behavioral evidence.
Fossils of the troodonts "Mei" and "Sinornithoides" demonstrate that some dinosaurs slept with their heads tucked under their arms. This behavior, which may have helped to keep the head warm, is also characteristic of modern birds. Several deinonychosaur and oviraptorosaur specimens have also been found preserved on top of their nests, likely brooding in a bird-like manner. The ratio between egg volume and body mass of adults among these dinosaurs suggest that the eggs were primarily brooded by the male, and that the young were highly precocial, similar to many modern ground-dwelling birds.
Some dinosaurs are known to have used gizzard stones like modern birds. These stones are swallowed by animals to aid digestion and break down food and hard fibers once they enter the stomach. When found in association with fossils, gizzard stones are called gastroliths.
Extinction of major groups.
The discovery that birds are a type of dinosaur showed that dinosaurs in general are not, in fact, extinct as is commonly stated. However, all non-avian dinosaurs as well as many groups of birds did suddenly become extinct approximately 66 million years ago. It has been suggested that because small mammals, squamata and birds occupied the ecological niches suited for small body size, non-avian dinosaurs never evolved a diverse fauna of small-bodied species, which lead to their downfall when large bodied terrestrial tetrapods were hit by the mass extinction event. Many other groups of animals also became extinct at this time, including ammonites (nautilus-like mollusks), mosasaurs, plesiosaurs, pterosaurs, and many groups of mammals. Significantly, the insects suffered no discernible population loss, which left them available as food for other survivors. This mass extinction is known as the Cretaceous–Paleogene extinction event. The nature of the event that caused this mass extinction has been extensively studied since the 1970s; at present, several related theories are supported by paleontologists. Though the consensus is that an impact event was the primary cause of dinosaur extinction, some scientists cite other possible causes, or support the idea that a confluence of several factors was responsible for the sudden disappearance of dinosaurs from the fossil record.
At the peak of the Mesozoic, there were no polar ice caps, and sea levels are estimated to have been from 100 to 250 meters (300 to 800 ft) higher than they are today. The planet's temperature was also much more uniform, with only 25 C-change separating average polar temperatures from those at the equator. On average, atmospheric temperatures were also much higher; the poles, for example, were 50 C-change warmer than today.
The atmosphere's composition during the Mesozoic is a matter for debate. While some academics argue that oxygen levels were much higher than today, others argue that biological adaptations seen in birds and dinosaurs indicate that respiratory systems evolved beyond what would be necessary if oxygen levels were high. By the late Cretaceous, the environment was changing dramatically. Volcanic activity was decreasing, which led to a cooling trend as levels of atmospheric carbon dioxide dropped. Oxygen levels in the atmosphere also started to fluctuate and would ultimately fall considerably. Some scientists hypothesize that climate change, combined with lower oxygen levels, might have led directly to the demise of many species.
Impact event.
The asteroid collision theory, which was brought to wide attention in 1980 by Walter Alvarez and colleagues, links the extinction event at the end of the Cretaceous period to a bolide impact approximately 66 million years ago. Alvarez "et al." proposed that a sudden increase in iridium levels, recorded around the world in the period's rock stratum, was direct evidence of the impact. The bulk of the evidence now suggests that a bolide 5 to 15 kilometers (3 to 9 mi) wide hit in the vicinity of the Yucatán Peninsula (in southeastern Mexico), creating the approximately 180 km Chicxulub Crater and triggering the mass extinction. Scientists are not certain whether dinosaurs were thriving or declining before the impact event. Some scientists propose that the meteorite caused a long and unnatural drop in Earth's atmospheric temperature, while others claim that it would have instead created an unusual heat wave. The consensus among scientists who support this theory is that the impact caused extinctions both directly (by heat from the meteorite impact) and also indirectly (via a worldwide cooling brought about when matter ejected from the impact crater reflected thermal radiation from the sun). Although the speed of extinction cannot be deduced from the fossil record alone, various models suggest that the extinction was extremely rapid, being down to hours rather than years.
Deccan Traps.
Before 2000, arguments that the Deccan Traps flood basalts caused the extinction were usually linked to the view that the extinction was gradual, as the flood basalt events were thought to have started around 68 million years ago and lasted for over 2 million years. However, there is evidence that two thirds of the Deccan Traps were created in only 1 million years about 66 million years ago, and so these eruptions would have caused a fairly rapid extinction, possibly over a period of thousands of years, but still longer than would be expected from a single impact event.
The Deccan Traps could have caused extinction through several mechanisms, including the release into the air of dust and sulphuric aerosols, which might have blocked sunlight and thereby reduced photosynthesis in plants. In addition, Deccan Trap volcanism might have resulted in carbon dioxide emissions, which would have increased the greenhouse effect when the dust and aerosols cleared from the atmosphere. Before the mass extinction of the dinosaurs, the release of volcanic gases during the formation of the Deccan Traps "contributed to an apparently massive global warming. Some data point to an average rise in temperature of 8 C-change in the last half million years before the impact [at Chicxulub]."
In the years when the Deccan Traps theory was linked to a slower extinction, Luis Alvarez (who died in 1988) replied that paleontologists were being misled by sparse data. While his assertion was not initially well-received, later intensive field studies of fossil beds lent weight to his claim. Eventually, most paleontologists began to accept the idea that the mass extinctions at the end of the Cretaceous were largely or at least partly due to a massive Earth impact. However, even Walter Alvarez has acknowledged that there were other major changes on Earth even before the impact, such as a drop in sea level and massive volcanic eruptions that produced the Indian Deccan Traps, and these may have contributed to the extinctions.
Possible Paleocene survivors.
Non-avian dinosaur remains are occasionally found above the Cretaceous–Paleogene boundary. In 2001, paleontologists Zielinski and Budahn reported the discovery of a single hadrosaur leg-bone fossil in the San Juan Basin, New Mexico, and described it as evidence of Paleocene dinosaurs. The formation in which the bone was discovered has been dated to the early Paleocene epoch, approximately 64.5 million years ago. If the bone was not re-deposited into that stratum by weathering action, it would provide evidence that some dinosaur populations may have survived at least a half million years into the Cenozoic Era. Other evidence includes the finding of dinosaur remains in the Hell Creek Formation up to 1.3 meters (51 in) above ( years later than) the Cretaceous–Paleogene boundary. Similar reports have come from other parts of the world, including China. Many scientists, however, dismissed the supposed Paleocene dinosaurs as re-worked, that is, washed out of their original locations and then re-buried in much later sediments. However, direct dating of the bones themselves has supported the later date, with U–Pb dating methods resulting in a precise age of 64.8 ± 0.9 million years ago. If correct, the presence of a handful of dinosaurs in the early Paleocene would not change the underlying facts of the extinction.
History of study.
Dinosaur fossils have been known for millennia, although their true nature was not recognized. The Chinese, whose modern word for dinosaur is "kǒnglóng" (恐龍, or "terrible dragon"), considered them to be dragon bones and documented them as such. For example, "Hua Yang Guo Zhi", a book written by Chang Qu during the Western Jin Dynasty (265-316), reported the discovery of dragon bones at Wucheng in Sichuan Province. Villagers in central China have long unearthed fossilized "dragon bones" for use in traditional medicines, a practice that continues today. In Europe, dinosaur fossils were generally believed to be the remains of giants and other biblical creatures.
Scholarly descriptions of what would now be recognized as dinosaur bones first appeared in the late 17th century in England. Part of a bone, now known to have been the femur of a "Megalosaurus", was recovered from a limestone quarry at Cornwell near Chipping Norton, Oxfordshire, England, in 1676. The fragment was sent to Robert Plot, Professor of Chemistry at the University of Oxford and first curator of the Ashmolean Museum, who published a description in his "Natural History of Oxfordshire" in 1677. He correctly identified the bone as the lower extremity of the femur of a large animal, and recognized that it was too large to belong to any known species. He therefore concluded it to be the thigh bone of a giant human similar to those mentioned in the Bible. In 1699, Edward Lhuyd, a friend of Sir Isaac Newton, was responsible for the first published scientific treatment of what would now be recognized as a dinosaur when he described and named a sauropod tooth, "Rutellum implicatum", that had been found in Caswell, near Witney, Oxfordshire.
Between 1815 and 1824, the Rev William Buckland, a professor of geology at Oxford University, collected more fossilized bones of "Megalosaurus" and became the first person to describe a dinosaur in a scientific journal. The second dinosaur genus to be identified, "Iguanodon", was discovered in 1822 by Mary Ann Mantell – the wife of English geologist Gideon Mantell. Gideon Mantell recognized similarities between and the bones of modern iguanas. He published his findings in 1825.
The study of these "great fossil lizards" soon became of great interest to European and American scientists, and in 1842 the English paleontologist Richard Owen coined the term "dinosaur". He recognized that the remains that had been found so far, "Iguanodon", "Megalosaurus" and "Hylaeosaurus", shared a number of distinctive features, and so decided to present them as a distinct taxonomic group. With the backing of Prince Albert of Saxe-Coburg-Gotha, the husband of Queen Victoria, Owen established the Natural History Museum in South Kensington, London, to display the national collection of dinosaur fossils and other biological and geological exhibits.
In 1858, the first known American dinosaur was discovered, in marl pits in the small town of Haddonfield, New Jersey (although fossils had been found before, their nature had not been correctly discerned). The creature was named "Hadrosaurus foulkii". It was an extremely important find: "Hadrosaurus" was one of the first nearly complete dinosaur skeletons found (the first was in 1834, in Maidstone, Kent, England), and it was clearly a bipedal creature. This was a revolutionary discovery as, until that point, most scientists had believed dinosaurs walked on four feet, like other lizards. Foulke's discoveries sparked a wave of dinosaur mania in the United States.
Dinosaur mania was exemplified by the fierce rivalry between Edward Drinker Cope and Othniel Charles Marsh, both of whom raced to be the first to find new dinosaurs in what came to be known as the Bone Wars. The feud probably originated when Marsh publicly pointed out that Cope's reconstruction of an "Elasmosaurus" skeleton was flawed: Cope had inadvertently placed the plesiosaur's head at what should have been the animal's tail end. The fight between the two scientists lasted for over 30 years, ending in 1897 when Cope died after spending his entire fortune on the dinosaur hunt. Marsh 'won' the contest primarily because he was better funded through a relationship with the US Geological Survey. Unfortunately, many valuable dinosaur specimens were damaged or destroyed due to the pair's rough methods: for example, their diggers often used dynamite to unearth bones (a method modern paleontologists would find appalling). Despite their unrefined methods, the contributions of Cope and Marsh to paleontology were vast: Marsh unearthed 86 new species of dinosaur and Cope discovered 56, a total of 142 new species. Cope's collection is now at the American Museum of Natural History in New York, while Marsh's is on display at the Peabody Museum of Natural History at Yale University.
After 1897, the search for dinosaur fossils extended to every continent, including Antarctica. The first Antarctic dinosaur to be discovered, the ankylosaurid "Antarctopelta oliveroi", was found on James Ross Island in 1986, although it was 1994 before an Antarctic species, the theropod "Cryolophosaurus ellioti", was formally named and described in a scientific journal.
Current dinosaur "hot spots" include southern South America (especially Argentina) and China. China in particular has produced many exceptional feathered dinosaur specimens due to the unique geology of its dinosaur beds, as well as an ancient arid climate particularly conducive to fossilization.
"Dinosaur renaissance".
The field of dinosaur research has enjoyed a surge in activity that began in the 1970s and is ongoing. This was triggered, in part, by John Ostrom's discovery of "Deinonychus", an active predator that may have been warm-blooded, in marked contrast to the then-prevailing image of dinosaurs as sluggish and cold-blooded. Vertebrate paleontology has become a global science. Major new dinosaur discoveries have been made by paleontologists working in previously unexploited regions, including India, South America, Madagascar, Antarctica, and most significantly China (the amazingly well-preserved feathered dinosaurs in China have further consolidated the link between dinosaurs and their conjectured living descendants, modern birds). The widespread application of cladistics, which rigorously analyzes the relationships between biological organisms, has also proved tremendously useful in classifying dinosaurs. Cladistic analysis, among other modern techniques, helps to compensate for an often incomplete and fragmentary fossil record.
Soft tissue and DNA.
One of the best examples of soft-tissue impressions in a fossil dinosaur was discovered in Pietraroia, Italy. The discovery was reported in 1998, and described the specimen of a small, very young coelurosaur, "Scipionyx samniticus". The fossil includes portions of the intestines, colon, liver, muscles, and windpipe of this immature dinosaur.
In the March 2005 issue of "Science", the paleontologist Mary Higby Schweitzer and her team announced the discovery of flexible material resembling actual soft tissue inside a 68-million-year-old "Tyrannosaurus rex" leg bone from the Hell Creek Formation in Montana. After recovery, the tissue was rehydrated by the science team.
When the fossilized bone was treated over several weeks to remove mineral content from the fossilized bone-marrow cavity (a process called demineralization), Schweitzer found evidence of intact structures such as blood vessels, bone matrix, and connective tissue (bone fibers). Scrutiny under the microscope further revealed that the putative dinosaur soft tissue had retained fine structures (microstructures) even at the cellular level. The exact nature and composition of this material, and the implications of Schweitzer's discovery, are not yet clear; study and interpretation of the material is ongoing.
In 2009, a team including Schweitzer announced that, using even more careful methodology, they had duplicated their results by finding similar soft tissue in a duck-billed dinosaur, Brachylophosaurus canadensis, found in the Judith River Formation of Montana. This included even more detailed tissue, down to preserved bone cells that seem even to have visible remnants of nuclei and what seem to be red blood cells. Among other materials found in the bone was collagen, as in the "Tyrannosaurus" bone mentioned above. The type of collagen an animal has in its bones varies according to its DNA and, in both cases, this collagen was of the same type found in modern chickens and ostriches.
The successful extraction of ancient DNA from dinosaur fossils has been reported on two separate occasions; upon further inspection and peer review, however, neither of these reports could be confirmed. However, a functional peptide involved in the vision of a theoretical dinosaur has been inferred using analytical phylogenetic reconstruction methods on gene sequences of related modern species such as reptiles and birds. In addition, several proteins, including hemoglobin, have putatively been detected in dinosaur fossils.
Cultural depictions.
By human standards, dinosaurs were creatures of fantastic appearance and often enormous size. As such, they have captured the popular imagination and become an enduring part of human culture. Entry of the word "dinosaur" into the common vernacular reflects the animals' cultural importance: in English, "dinosaur" is commonly used to describe anything that is impractically large, obsolete, or bound for extinction.
Public enthusiasm for dinosaurs first developed in Victorian England, where in 1854, three decades after the first scientific descriptions of dinosaur remains, a menagerie of lifelike dinosaur sculptures were unveiled in London's Crystal Palace Park. The Crystal Palace dinosaurs proved so popular that a strong market in smaller replicas soon developed. In subsequent decades, dinosaur exhibits opened at parks and museums around the world, ensuring that successive generations would be introduced to the animals in an immersive and exciting way. Dinosaurs' enduring popularity, in its turn, has resulted in significant public funding for dinosaur science, and has frequently spurred new discoveries. In the United States, for example, the competition between museums for public attention led directly to the Bone Wars of the 1880s and 1890s, during which a pair of feuding paleontologists made enormous scientific contributions.
The popular preoccupation with dinosaurs has ensured their appearance in literature, film, and other media. Beginning in 1852 with a passing mention in Charles Dickens‍ '​ "Bleak House", dinosaurs have been featured in large numbers of fictional works. Jules Verne's 1864 novel "Journey to the Center of the Earth", Sir Arthur Conan Doyle's 1912 book "The Lost World", the iconic 1933 film "King Kong", the 1954 "Godzilla" and its many sequels, the best-selling 1990 novel "Jurassic Park" by Michael Crichton and its 1993 film adaptation are just a few notable examples of dinosaur appearances in fiction. Authors of general-interest non-fiction works about dinosaurs, including some prominent paleontologists, have often sought to use the animals as a way to educate readers about science in general. Dinosaurs are ubiquitous in advertising; numerous companies have referenced dinosaurs in printed or televised advertisements, either in order to sell their own products or in order to characterize their rivals as slow-moving, dim-witted, or obsolete.
External links.
Listen to this article ()
This audio file was created from a revision of the "Dinosaur" article dated 2005-12-30, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="8315" url="http://en.wikipedia.org/wiki?curid=8315" title="Diamagnetism">
Diamagnetism

Diamagnetic materials create an induced magnetic field in a direction opposite to an externally applied magnetic field, and are repelled by the applied magnetic field. In contrast, the opposite behavior is exhibited by paramagnetic materials. Diamagnetism is a quantum mechanical effect that occurs in all materials; when it is the only contribution to the magnetism the material is called a "diamagnet". Unlike a ferromagnet, a diamagnet is not a permanent magnet. Its magnetic permeability is less than μ0 (the permeability of vacuum). In most materials diamagnetism is a weak effect, but a superconductor repels the magnetic field entirely, apart from a thin layer at the surface.
Diamagnets were first discovered when Sebald Justinus Brugmans observed in 1778 that bismuth and antimony were repelled by magnetic fields. In 1845, Michael Faraday demonstrated that it was a property of matter and concluded that every material responded (in either a diamagnetic or paramagnetic way) to an applied magnetic field. He adopted the term "diamagnetism" after it was suggested to him by William Whewell.
Materials.
Diamagnetism, to a greater or lesser degree, is a property of all materials and always makes a weak contribution to the material's response to a magnetic field. For materials that show some other form of magnetism (such as ferromagnetism or paramagnetism), the diamagnetic contribution becomes negligible. Substances that mostly display diamagnetic behaviour are termed diamagnetic materials, or diamagnets. Materials called diamagnetic are those that laymen generally think of as "non-magnetic", and include water, wood, most organic compounds such as petroleum and some plastics, and many metals including copper, particularly the heavy ones with many core electrons, such as mercury, gold and bismuth. The magnetic susceptibility values of various molecular fragments are called Pascal's constants.
Diamagnetic materials, like water, or water based materials, have a relative magnetic permeability that is less than or equal to 1, and therefore a magnetic susceptibility less than or equal to 0, since susceptibility is defined as . This means that diamagnetic materials are repelled by magnetic fields. However, since diamagnetism is such a weak property its effects are not observable in everyday life. For example, the magnetic susceptibility of diamagnets such as water is . The most strongly diamagnetic material is bismuth, , although pyrolytic carbon may have a susceptibility of in one plane. Nevertheless, these values are orders of magnitude smaller than the magnetism exhibited by paramagnets and ferromagnets. Note that because χv is derived from the ratio of the internal magnetic field to the applied field, it is a dimensionless value.
All conductors exhibit an effective diamagnetism when they experience a changing magnetic field. The Lorentz force on electrons causes them to circulate around forming eddy currents. The eddy currents then produce an induced magnetic field opposite the applied field, resisting the conductor's motion.
Superconductors.
Superconductors may be considered perfect diamagnets (), since they expel all fields (except in a thin surface layer) due to the Meissner effect. However this effect is not due to eddy currents, as in ordinary diamagnetic materials (see the article on superconductivity).
Demonstrations.
Curving water surfaces.
If a powerful magnet (such as a supermagnet) is covered with a layer of water (that is thin compared to the diameter of the magnet) then the field of the magnet significantly repels the water. This causes a slight dimple in the water's surface that may be seen by its reflection.
Levitation.
Diamagnets may be levitated in stable equilibrium in a magnetic field, with no power consumption. Earnshaw's theorem seems to preclude the possibility of static magnetic levitation. However, Earnshaw's theorem only applies to objects with positive susceptibilities, such as ferromagnets (which have a permanent positive moment) and paramagnets (which induce a positive moment). These are attracted to field maxima, which do not exist in free space. Diamagnets (which induce a negative moment) are attracted to field minima, and there can be a field minimum in free space.
A thin slice of pyrolytic graphite, which is an unusually strong diamagnetic material, can be stably floated in a magnetic field, such as that from rare earth permanent magnets. This can be done with all components at room temperature, making a visually effective demonstration of diamagnetism.
The Radboud University Nijmegen, the Netherlands, has conducted experiments where water and other substances were successfully levitated. Most spectacularly, a live frog (see figure) was levitated.
In September 2009, NASA's Jet Propulsion Laboratory in Pasadena, California announced they had successfully levitated mice using a superconducting magnet, an important step forward since mice are closer biologically to humans than frogs. They hope to perform experiments regarding the effects of microgravity on bone and muscle mass.
Recent experiments studying the growth of protein crystals has led to a technique using powerful magnets to allow growth in ways that counteract Earth's gravity.
A simple homemade device for demonstration can be constructed out of bismuth plates and a few permanent magnets that levitate a permanent magnet.
Theory.
The electrons in a material generally circulate in orbitals, with effectively zero resistance and act like current loops. Thus it might be imagined that diamagnetism effects in general would be very, very common, since any applied magnetic field would generate currents in these loops that would oppose the change, in a similar way to superconductors, which are essentially perfect diamagnets. However, since the electrons are rigidly held in orbitals by the charge of the protons and are further constrained by the Pauli exclusion principle, many materials exhibit diamagnetism, but typically respond very little to the applied field.
The Bohr–van Leeuwen theorem proves that there cannot be any diamagnetism or paramagnetism in a purely classical system. However, the classical theory for Langevin diamagnetism gives the same prediction as the quantum theory. The classical theory is given below.
Langevin diamagnetism.
The Langevin theory of diamagnetism applies to materials containing atoms with closed shells (see dielectrics). A field with intensity B, applied to an electron with charge e and mass m, gives rise to Larmor precession with frequency . The number of revolutions per unit time is ω / 2π, so the current for an atom with Z electrons is (in SI units)
The magnetic moment of a current loop is equal to the current times the area of the loop. Suppose the field is aligned with the z axis. The average loop area can be given as formula_2, where formula_3 is the mean square distance of the electrons perpendicular to the z axis. The magnetic moment is therefore
If the distribution of charge is spherically symmetric, we can suppose that the distribution of x,y,z coordinates are independent and identically distributed. Then formula_5, where formula_6 is the mean square distance of the electrons from the nucleus. Therefore formula_7. If formula_8 is the number of atoms per unit volume, the diamagnetic susceptibility in SI units is
In metals.
The Langevin theory does not apply to metals because they have non-localized electrons. The theory for the diamagnetism of a free electron gas is called Landau diamagnetism, and instead considers the weak counter-acting field that forms when their trajectories are curved due to the Lorentz force. Landau diamagnetism, however, should be contrasted with Pauli paramagnetism, an effect associated with the polarization of delocalized electrons' spins.

</doc>
<doc id="8317" url="http://en.wikipedia.org/wiki?curid=8317" title="Duke of Marlborough (title)">
Duke of Marlborough (title)

Duke of Marlborough ( ) is a title in the Peerage of England. It was created by Queen Anne in 1702 for John Churchill, 1st Earl of Marlborough (1650–1722), the noted military leader. The name of the dukedom refers to Marlborough in Wiltshire. It is one of the few titles in the peerage which allows for "suo jure" female inheritance, and the only current dukedom to do so.
History of the Dukedom.
Churchill had been made "Lord Churchill of Eyemouth" (1682) in the Scottish peerage, "Baron Churchill" of Sandridge (1685), and "Earl of Marlborough" (1689) in the Peerage of England. Shortly after her accession to the throne in 1702, Queen Anne made Churchill the first "Duke of Marlborough" and granted him the subsidiary title "Marquess of Blandford".
In 1678, Churchill married Sarah Jennings (1660–1744), a courtier and influential favourite of the queen. They had seven children, of whom four daughters married into some of the most important families in Great Britain; one daughter and one son died in infancy. He was pre-deceased by his son, John Churchill, Marquess of Blandford, in 1703; so, to prevent the extinction of the titles, a special Act of Parliament was passed. When the 1st Duke of Marlborough died in 1722 his title as "Lord Churchill of Eyemouth" in the Scottish peerage became extinct and the Marlborough titles passed, according to the Act, to his eldest daughter Henrietta (1681-1733), the 2nd Duchess of Marlborough. She was married to the 2nd Earl of Godolphin and had a son who predeceased her.
When Henrietta died in 1733, the Marlborough titles passed to her nephew Charles Spencer (1706–1758), the third son of her late sister Anne (1683-1716), who had married the 3rd Earl of Sunderland in 1699. After his older brother's death in 1729, Charles Spencer had already inherited the Spencer family estates and the titles of "Earl of Sunderland" (1643) and "Baron Spencer" of Wormleighton (1603), all in the Peerage of England. Upon his maternal aunt Henrietta's death in 1733, Charles Spencer succeeded to the Marlborough family estates and titles and became the 3rd Duke. When he died in 1758, his titles passed to his eldest son George (1739–1817), who was succeeded by his eldest son George, the 5th Duke (1766–1840). In 1815, Francis Spencer (the younger son of the 4th Duke) was created "Baron Churchill" in the Peerage of the United Kingdom. In 1902, his grandson, the 3rd Baron Churchill, was created "Viscount Churchill".
In 1817, the 5th Duke obtained permission to assume and bear the surname of Churchill in addition to his surname of Spencer, to perpetuate the name of his illustrious great-great-grandfather. At the same time he received Royal Licence to quarter the coat of arms of Churchill with his paternal arms of Spencer. The modern Dukes thus originally bore the surname "Spencer": the double-barrelled surname of "Spencer-Churchill" as used since 1817 remains in the family, though some members have preferred to style themselves "Churchill".
The 7th Duke was the paternal grandfather of the British Prime Minister Sir Winston Churchill, born at Blenheim Palace on 30 November 1874.
The 11th duke, John Spencer-Churchill died in 2014, having assumed the title in 1972. The 12th and present duke is Charles James Spencer-Churchill.
Family seat.
The family seat is Blenheim Palace in Woodstock, Oxfordshire.
After his leadership in the victory against the French in the Battle of Blenheim on 13 August 1704, the 1st Duke was honoured by Queen Anne granting him the royal manor of Woodstock, and building him a house at her expense to be called Blenheim. Construction started in 1705 and the house was completed in 1722, the year of the 1st Duke's death. Blenheim Palace has since remained in the Churchill and Spencer-Churchill family.
With the exception of the 10th Duke and his first wife, the Dukes and Duchesses of Marlborough are buried in Blenheim Palace's chapel. Most other members of the Spencer-Churchill family are interred in St. Martin's parish churchyard at Bladon, a short distance from the palace.
Succession to the title.
The dukedom is the only one in the United Kingdom that can still pass through a female line. However, unlike the remainder to heirs general found in most other peerages that allow male-preference primogeniture, the grant does not allow for abeyance and follows a more restrictive Semi-Salic formula designed to keep succession wherever possible in the male line. The succession is as follows:
Succession to the title under the first and second contingencies have lapsed; holders of the title from the 3rd Duke trace their status from the third contingency.
It is now very unlikely that the Dukedom will be passed to a woman or through a woman, since all the male-line descendants of Anne Spencer, Countess of Sunderland - including the line of the Earls Spencer and the Spencer-Churchill family - would have to become extinct. If that were to happen, the Churchill titles would pass to the Earl of Jersey, the heir-male of Anne Villiers, Countess of Jersey, daughter of Elizabeth Egerton, Duchess of Bridgewater, a younger daughter of the first Duke.
Other titles of the Dukes.
Subsidiary titles.
The Duke holds subsidiary titles: "Marquess of Blandford" (created in 1702 for John Churchill), "Earl of Sunderland" (created in 1643 for the Spencer family), "Earl of Marlborough" (created in 1689 for John Churchill), "Baron Spencer" of Wormleighton (created in 1603 for the Spencer family), and "Baron Churchill" of Sandridge (created in 1685 for John Churchill), all in the Peerage of England.
The title "Marquess of Blandford" is used as the courtesy title for the Duke's eldest son and heir. The Duke's eldest son's eldest son can use the courtesy title "Earl of Sunderland", and the duke's eldest son's eldest son's eldest son (eldest great-grandson) the title "Lord Spencer of Wormleighton" (not to be confused with Earl Spencer).
The title of "Earl of Marlborough", created for John Churchill in 1689, had previously been created for James Ley, in 1626, becoming extinct in 1679.
Foreign titles.
The 1st Duke was honoured with titles in the Holy Roman Empire: Emperor Joseph I created him a Prince in 1704, and in 1705 he was given the principality of Mindelheim (once the lordship of the noted soldier Georg von Frundsberg). He was obliged to surrender Mindelheim in 1714 by the Treaty of Utrecht, which returned it to Bavaria. According to some sources, the 1st Duke received the principality of Mellenburg in exchange. The 1st Duke's principality titles of Mindelheim and Mellenburg did not pass to his daughters (the Empire operated Salic Law, which prevented female succession), so became extinct on his death in 1722.
Coats of arms.
Original arms of the Churchill family.
The original arms of Sir Winston Churchill (1620–1688), father of the 1st Duke of Marlborough, were simple and in use by his own father in 1619. The shield was Sable a lion rampant Argent, debruised by a bendlet Gules. The addition of a canton of Saint George (see below) rendered the distinguishing mark of the bendlet unnecessary.
The Churchill crest is blazoned as a lion couchant guardant Argent, supporting with its dexter forepaw a banner Gules, charged with a dexter hand appaumée of the first, staff Or.
In recognition of Sir Winston's services to King Charles I as Captain of the Horse, and his loyalty to King Charles II as a Member of Parliament, he was awarded an augmentation of honour to his arms around 1662. This rare mark of royal favour took the form of a canton of Saint George. At the same time, he was authorised to omit the bendlet, which had served the purpose of distinguishing this branch of the Churchill family from others which bore an undifferenced lion.
Arms of the 1st Duke of Marlborough.
Sir Winston's shield and crest were inherited by his son John Churchill, 1st Duke of Marlborough. Minor modifications reflected the bearer's social rise: the helm was now shown in profile and had a closed grille to signify the bearer's rank as a peer, and there were now supporters placed on either side of the shield. They were the mythical Griffin (part lion, part eagle) and Wyvern (a dragon without hind legs). The supporters were derived from the arms of the family of the 1st Duke's mother, Drake of Ash (Argent, a wyvern gules; these arms can be seen on the monument in Musbury Church to Sir Bernard Drake, d.1586).
The motto was "Fiel pero desdichado" (Spanish for "Faithful but unfortunate"). The 1st Duke was also entitled to a coronet indicating his rank.
When the 1st Duke was made a Prince of the Holy Roman Empire in 1705, two unusual features were added: the Imperial Eagle and a Princely Coronet. His estates in Germany, such as Mindelheim, were represented in his arms by additional quarterings.
Arms of the Spencer-Churchill family.
In 1817, the 5th Duke received Royal Licence to place the quarter of Churchill ahead of his paternal arms of Spencer. The shield of the Spencer family arms is: quarterly Argent and Gules, in the second and third quarters a fret Or, over all on a bend Sable three escallops of the first. The Spencer crest is: out of a ducal coronet Or, a griffin's head between two wings expanded Argent, gorged with a collar gemel and armed Gules. Paul Courtenay observes that "It would be normal in these circumstances for the paternal arms (Spencer) to take precedence over the maternal (Churchill), but because the Marlborough dukedom was senior to the Sunderland earldom, the procedure was reversed in this case."
Also in 1817, a further augmentation of honour was added to his armorial achievement. This incorporated the bearings from the standard of the Manor of Woodstock and was borne on an escutcheon, displayed over all in the centre chief point, as follows: Argent a cross of Saint George surmounted by an inescutcheon Azure, charged with three fleurs-de-lys Or, two over one. This inescutcheon represents the royal arms of France.
The resulting heraldic achievement is:
These quartered arms, incorporating the two augmentations of honour, have been the arms of all subsequent Dukes of Marlborough.
Motto.
The motto "Fiel pero desdichado" is Spanish for "Faithful though unhappy". "Desdichado" means without happiness or without joy, alluding to the first Duke's father, Winston, who was a royalist and faithful supporter of the king during the English Civil War but was not compensated for his losses after the restoration. Charles II created Winston Churchill and other Civil War royalists knights but did not compensate them for their wartime losses, thereby inducing Winston to adopt the motto. It is unusual for the motto of an Englishman of the era to be in Spanish rather than Latin, and it is not known why this is the case.
List of title holders.
Dukes of Marlborough (1702).
The heir apparent to the Dukedom is George John Godolphin Spencer-Churchill, Marquess of Blandford (b. 1992), eldest son of the 12th Duke.
Family tree.
Spencer-Churchill Family Tree: Dukes of Marlborough 

</doc>
