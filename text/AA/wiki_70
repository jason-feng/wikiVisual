<doc id="9679" url="http://en.wikipedia.org/wiki?curid=9679" title="Prince Eugene of Savoy">
Prince Eugene of Savoy

Prince Eugene of Savoy (French: "François-Eugène de Savoie", Italian: "Principe Eugenio di Savoia-Carignano", German: "Prinz Eugen von Savoyen"; 18 October 1663 – 21 April 1736) was a general of the Imperial Army and statesman of the Holy Roman Empire and the Archduchy of Austria and one of the most successful military commanders in modern European history, rising to the highest offices of state at the Imperial court in Vienna. Born in Paris, Eugene grew up around the French court of King Louis XIV. Based on his poor physique and bearing, the Prince was initially prepared for a career in the church, but by the age of 19 he had determined on a military career. Rejected by Louis XIV for service in the French army, Eugene moved to Austria and transferred his loyalty to the Habsburg Monarchy.
Spanning six decades, Eugene served three Holy Roman Emperors: Leopold I, Joseph I, and Charles VI. He first saw action against the Ottoman Turks at the Siege of Vienna in 1683 and the subsequent War of the Holy League, before serving in the Nine Years' War, fighting alongside his cousin, the Duke of Savoy. However, the Prince's fame was secured with his decisive victory against the Ottomans at the Battle of Zenta in 1697, earning him Europe-wide fame. Eugene enhanced his standing during the War of the Spanish Succession, where his partnership with the Duke of Marlborough secured victories against the French on the fields of Blenheim (1704), Oudenarde (1708), and Malplaquet (1709); he gained further success in the war as Imperial commander in northern Italy, most notably at the Battle of Turin (1706). Renewed hostilities against the Ottomans in the Austro-Turkish War consolidated his reputation, with victories at the battles of Petrovaradin (1716), and the decisive encounter at Belgrade (1717).
Throughout the late 1720s, Eugene's influence and skilful diplomacy managed to secure the Emperor powerful allies in his dynastic struggles with the Bourbon powers, but physically and mentally fragile in his later years, Eugene enjoyed less success as commander-in-chief of the army during his final conflict, the War of the Polish Succession. Nevertheless, in Austria, Eugene's reputation remains unrivalled. Although opinions differ as to his character, there is no dispute over his great achievements: he helped to save the Habsburg Empire from French conquest; he broke the westward thrust of the Ottomans, liberating central Europe after a century and a half of Turkish occupation; and he was one of the great patrons of the arts whose building legacy can still be seen in Vienna today. Eugene died in his sleep at his home on 21 April 1736, aged 72.
Early life (1663–99).
Hôtel de Soissons.
Prince Eugene was born in the Hôtel de Soissons in Paris on 18 October 1663. His mother, Olympia Mancini, was one of Cardinal Mazarin's nieces whom he had brought to Paris from Rome in 1647 to further his, and, to a lesser extent, their ambitions. The Mancinis were raised at the Palais-Royal along with the young Louis XIV, with whom Olympia formed an intimate relationship. Yet to her great disappointment, her chance to become queen passed by, and in 1657, Olympia married Eugene Maurice, Count of Soissons, Count of Dreux and Prince of Savoy. Together they had had five sons (Eugene being the youngest) and three daughters, but neither parent spent much time with the children: his father, a brave, unglamorous French soldier, spent much of his time away campaigning, while Olympia's passion for court intrigue meant the children received little attention from her.
The King remained strongly attached to Olympia, so much so that many believed them to be lovers; but her scheming eventually led to her downfall. After falling out of favour at court, Olympia turned to Catherine Deshayes (known as "La Voisin"), and the arts of black magic and astrology. It was a fatal relationship. Embroiled in the "affaire des poisons", suspicions now abounded of her involvement in her husband's premature death in 1673, and even implicated her in a plot to kill the King himself. Whatever the truth, Olympia, rather than face trial, subsequently fled France for Brussels in January 1680, leaving Eugene in the care of his father's mother, Marie de Bourbon, and her daughter, Hereditary Princess of Baden, mother of Prince Louis of Baden.
From the age of ten, Eugene had been brought up for a career in the church; a personal choice of the King, basing the decision on the young Prince's poor physique and bearing. Certainly Eugene's appearance was not impressive — "He was never good-looking …" wrote the Duchess of Orléans, "It is true that his eyes are not ugly, but his nose ruins his face; he has two large teeth which are visible at all times."
In February 1683, to the surprise of his family, Eugene declared his intention of joining the army. Now 19 years old, Eugene applied directly to Louis XIV for command of a company in French service, but the King – who had shown no compassion for Olympia's children since her disgrace – refused him out of hand. "The request was modest, not so the petitioner," he remarked. "No one else ever presumed to stare me out so insolently."
Denied a military career in France, Eugene decided to seek service abroad. One of Eugene's brothers, Louis Julius, had entered Imperial service the previous year, but he had been immediately killed fighting the Ottoman Turks in 1683. When news of his death reached Paris, Eugene decided to travel to Austria in the hope of taking over his brother's command. It was not an unnatural decision: his cousin, Louis of Baden, was already a leading general in the Imperial army, as was a more distant cousin, Maximilian II Emanuel, Elector of Bavaria. On the night of July 26, 1683, Eugene left Paris and headed east.
Great Turkish War.
By May 1683, the Ottoman threat to Emperor Leopold I's capital, Vienna, was very real. The Grand Vizier, Kara Mustafa Pasha – encouraged by Imre Thököly's Magyar rebellion – had invaded Hungary with between 100,000–200,000 men; within two months approximately 90,000 were beneath Vienna's walls. With the 'Turks at the gates', the Emperor fled for the safe refuge of Passau up the Danube, a more distant and secure part of his dominion. It was at Leopold I's camp that Eugene arrived in mid-August.
Although Eugene was not of Austrian extraction, he did have Habsburg antecedents. His grandfather, Thomas Francis, founder of the Carignano line of the House of Savoy, was the son of Catherine Michelle – a daughter of Philip II of Spain – and the great-grandson of the Emperor Charles V. But of more immediate consequence to Leopold I was the fact that Eugene was the second cousin of Victor Amadeus, the Duke of Savoy, a connection that the Emperor hoped might prove useful in any future confrontation with France. These ties, together with his ascetic manner and appearance (a positive advantage to him at the sombre court of Leopold I), ensured the refugee from the hated French king a warm welcome at Passau, and a position in Imperial service. Though French was his favored language, he communicated with Leopold in Italian, as the Emperor (though he knew it perfectly) disliked French. But Eugene also had a reasonable command of German, which he understood very easily, something that helped him much in the military.
Eugene was in no doubt where his new allegiance lay – "I will devote all my strength, all my courage, and if need be, my last drop of blood, to the service of your Imperial Majesty." This loyalty was immediately put to the test. By September, the Imperial forces under the Duke of Lorraine, together with a powerful Polish army under King John III Sobieski, were poised to strike the Sultan's army. On the morning of 12 September, the Christian forces drew up in line of battle on the south-eastern slopes of the Vienna Woods, looking down on the massed enemy camp. The day-long Battle of Vienna resulted in the lifting of the 60-day siege, and the Sultan's forces were routed and in retreat. Serving under Baden, Eugene distinguished himself in the battle, earning commendation from Lorraine and the Emperor; he later received the nomination for the colonelcy of the Dragoon Regiment Kufstein.
Holy League.
In March 1684, Leopold I formed the Holy League with Poland and Venice to counter the Ottoman threat. For the next two years, Eugene continued to perform with distinction on campaign and establish himself as a dedicated, professional soldier; by the end of 1685, still only 22 years old, he was made a Major-General. However, little is known of Eugene's life during these early campaigns. Contemporary observers make only passing comments of his actions, and his own surviving correspondence, largely to his cousin Victor Amadeus, are typically reticent about his own feelings and experiences. Nevertheless, it is clear that Baden was impressed with Eugene's qualities – "This young man will, with time, occupy the place of those whom the world regards as great leaders of armies."
In June 1686, the Duke of Lorraine besieged Buda (Budapest), the centre of the Ottoman occupation in Hungary. After resisting for 78 days, the city fell on 2 September, and Turkish resistance collapsed throughout the region as far away as Transylvania and Serbia. Further success followed in 1687, where, commanding a cavalry brigade, Eugene made an important contribution to the victory at the Battle of Mohács on 12 August. Such was the scale of their defeat that the Ottoman army mutinied – a revolt which spread to Constantinople. The Grand Vizier, Suluieman Pasha, was executed and Sultan Mehmed IV, deposed. Once again, Eugene's courage earned him recognition from his superiors, who granted him the honour of personally conveying the news of victory to the Emperor in Vienna. For his services, Eugene was promoted to Lieutenant-General in November 1687. He was also gaining wider recognition. King Charles II of Spain bestowed upon him the Order of the Golden Fleece, while his cousin, Victor Amadeus, provided him with money and two profitable abbeys in Piedmont. However, Eugene's military career suffered a temporary setback in 1688 when, on 6 September, the Prince suffered a severe wound to his knee by a musket ball during the Siege of Belgrade. It was not until January 1689 that he could return to active service.
Interlude in the west: Nine Years' War.
Just as Belgrade was falling to Imperial forces under Max Emmanuel in the east, French troops in the west were crossing the Rhine into the Holy Roman Empire. Louis XIV had hoped that a show of force would lead to a quick resolution to his dynastic and territorial disputes with the princes of the Empire along his eastern border, but his intimidatory moves only strengthened German resolve, and in May 1689, Leopold I and the Dutch signed an offensive compact aimed at repelling French aggression.
The Nine Years' War was professionally and personally frustrating for the Prince. Initially fighting on the Rhine with Max Emmanuel – receiving a slight head wound at the Siege of Mainz in 1689 – Eugene subsequently transferred himself to Piedmont after Victor Amadeus joined the Alliance against France in 1690. Promoted to general of cavalry, he arrived in Turin with his friend the Prince of Commercy; but it proved an inauspicious start. Against Eugene's advice, Amadeus insisted on engaging the French at Staffarda and suffered a serious defeat – only Eugene's handling of the Savoyard cavalry in retreat saved his cousin from disaster. Eugene remained unimpressed with the men and their commanders throughout the war in Italy. "The enemy would long ago have been beaten," he wrote to Vienna, "if everyone had done their duty." So contemptuous was he of the Imperial commander, Count Caraffa, he threatened to leave Imperial service.
In Vienna, Eugene's attitude was dismissed as the arrogance of a young upstart, but so impressed was the Emperor by his passion for the Imperial cause, he promoted him to Field-Marshal in 1693. When Caraffa's replacement, Count Caprara, was himself transferred in 1694, it seemed that Eugene's chance for command and decisive action had finally arrived. But Amadeus, doubtful of victory and now more fearful of Habsburg influence in Italy than he was of French, had begun secret dealings with Louis XIV aimed at extricating himself from the war. By 1696, the deal was done, and Amadeus transferred his troops and his loyalty to the enemy. Eugene was never to fully trust his cousin again; although he continued to pay due reverence to the Duke as head of his family, their relationship would forever after remain strained.
Military honours in Italy undoubtedly belonged to the French commander Marshal Catinat, but Eugene, the one Allied general determined on action and decisive results, did well to emerge from the Nine Years' War with an enhanced reputation. With the signing of the Treaty of Ryswick in September/October 1697, the desultory war in the west was finally brought to an inconclusive end, and Leopold I could once again devote all his martial energies into defeating the Ottoman Turks in the east.
Zenta.
The distractions of the war against Louis XIV had enabled the Turks to recapture Belgrade in 1690. In August 1691, the Austrians, under Louis of Baden, regained the advantage by heavily defeating the Turks at the Battle of Slankamen on the Danube, securing Habsburg possession of Hungary and Transylvania. However, when Baden was transferred west to fight the French in 1692, his successors, first Caprara, then from 1696, Frederick Augustus, the Elector of Saxony, proved incapable of delivering the final blow. On the advice of the President of the Imperial War Council, Rüdiger Starhemberg, Eugene was offered supreme command of Imperial forces in April 1697. This was Eugene's first truly independent command – no longer need he suffer under the excessively cautious generalship of Caprara and Caraffa, or be thwarted by the deviations of Victor Amadeus. But on joining his army, he found it in a state of 'indescribable misery'. Confident and self-assured, the Prince of Savoy (ably assisted by Commercy and Guido Starhemberg) set about restoring order and discipline.
Leopold I had warned Eugene to act cautiously, but when the Imperial commander learnt of Sultan Mustafa II's march on Transylvania, Eugene abandoned all ideas of a defensive campaign and moved to intercept the Turks as they crossed the River Tisza at Zenta on 11 September 1697. It was late in the day before the Imperial army struck. The Turkish cavalry had already crossed the river so Eugene decided to attack immediately, arranging his men in a half-moon formation. The vigour of the assault wrought terror and confusion amongst the Turks, and by nightfall, the battle was won. For the loss of some 2,000 dead and wounded, Eugene had inflicted approximately 25,000 casualties on his enemy – including the Grand Vizier, Elmas Mehmed Pasha – annihilating the Turkish army. Although the Ottomans lacked western organisation and training, the Savoyard prince had revealed his tactical skill, his capacity for bold decision, and his ability to inspire his men to excel in battle against a dangerous foe.
After a brief terror-raid into Ottoman-held Bosnia, culminating in the sack of Sarajevo, Eugene returned to Vienna in November to a triumphal reception. His victory at Zenta had turned him into a European hero, and with victory came reward. Land in Hungary, given him by the Emperor, yielded a good income, enabling the Prince to cultivate his newly acquired tastes in art and architecture (see below); but for all his new-found wealth and property, he was, nevertheless, without personal ties or family commitments. Of his four brothers, only one was still alive at this time. His fourth brother, Emmanuel, had died aged 14 in 1676; his third, Louis Julius (already mentioned) had died on active service in 1683, and his second brother, Philippe, died of smallpox in 1693. Eugene's remaining brother, Louis Thomas – ostracised for incurring the displeasure of Louis XIV – travelled Europe in search of a career, before arriving in Vienna in 1699. With Eugene's help, Louis found employment in the Imperial army, only to be killed in action against the French in 1702. Of Eugene's sisters, the youngest had died in childhood. The other two, Marie Jeanne-Baptiste and Louise Philiberte, led dissolute lives. Expelled from France, Marie joined her mother in Brussels, before eloping with a renegade priest to Geneva, living with him unhappily until her premature death in 1705. Of Louise, little is known after her early salacious life in Paris, but in due course, she lived for a time in a convent in Savoy before her death in 1726.
The Battle of Zenta proved to be the decisive victory in the long war against the Turks. With Leopold I's interests now focused on Spain and the imminent death of Charles II, the Emperor terminated the conflict with the Sultan, and signed the Treaty of Karlowitz on 26 January 1699.
Mid life (1700–20).
War of the Spanish Succession.
With the death of the infirm and childless Charles II of Spain on 1 November 1700, the succession of the Spanish throne and subsequent control over her empire once again embroiled Europe in war – the War of the Spanish Succession. On his deathbed Charles II had bequeathed the entire Spanish inheritance to Louis XIV's grandson, Philip, Duke of Anjou. This threatened to unite the Spanish and French kingdoms under the House of Bourbon – something unacceptable to England, the Dutch Republic, and Leopold I, who had himself a claim to the Spanish throne. From the beginning, the Emperor had refused to accept the will of Charles II, and he did not wait for England and the Dutch Republic to begin hostilities. Before a new Grand Alliance could be concluded Leopold I prepared to send an expedition to seize the Spanish lands in Italy.
Eugene crossed the Alps with some 30,000 men in May/June 1701. After a series of brilliant manoeuvres the Imperial commander defeated Catinat at the Battle of Carpi on 9 July. "I have warned you that you are dealing with an enterprising young prince," wrote Louis XIV to his commander, "he does not tie himself down to the rules of war." On 1 September Eugene defeated Catinat's successor, Marshal Villeroi, at the Battle of Chiari, in a clash as destructive as any in the Italian theatre. But as so often throughout his career the Prince faced war on two fronts – the enemy in the field and the government in Vienna. Starved of supplies, money and men, Eugene was forced into unconventional means against the vastly superior enemy. During a daring raid on Cremona on the night of 31 January/1 February 1702 Eugene captured the French commander-in-chief. Yet the coup was less successful than hoped: Cremona remained in French hands, and the Duke of Vendôme, whose talents far exceeded Villeroi's, became the theatre's new commander. Villeroi's capture caused a sensation in Europe, and had a galvanising effect on English public opinion. "The surprise at Cremona," wrote the diarist John Evelyn, "… was the greate discourse of this weeke"; but appeals for succour from Vienna remained unheeded, forcing Eugene to seek battle and gain a 'lucky hitt'. The resulting Battle of Luzzara on 15 August proved inconclusive. Although Eugene's forces inflicted double the number of casualties on the French the battle settled little except to deter Vendôme trying an all-out assault on Imperial forces that year, enabling Eugene to hold on south of the Alps. With his army rotting away, and personally grieving for his long standing friend Prince Commercy who had died at Luzzara, Eugene returned to Vienna in January 1703.
President of the Imperial War Council.
Eugene's European reputation was growing (Cremona and Luzzara had been celebrated as victories throughout the Allied capitals), yet because of the condition and morale of his troops the 1702 campaign had not been a success. Austria itself was now facing the direct threat of invasion from across the border in Bavaria where the state's Elector, Maximilian Emanuel, had declared for the Bourbons in August the previous year. Meanwhile in Hungary a small-scale revolt had broken out in May and was fast gaining momentum. With the monarchy at the point of complete financial breakdown Leopold I was at last persuaded to change the government. At the end of June 1703 Gundaker Starhemberg replaced Gotthard Salaburg as President of the Treasury, and Prince Eugene succeeded Henry Mansfeld as the new President of the Imperial War Council ("Hofkriegsratspräsident").
As head of the war council Eugene was now part of the Emperor's inner circle, and the first president since Montecuccoli to remain an active commander. Immediate steps were taken to improve efficiency within the army: encouragement and, where possible, money, was sent to the commanders in the field; promotion and honours were distributed according to service rather than influence; and discipline improved. But the Austrian monarchy faced severe peril on several fronts in 1703: by June the Duke of Villars had reinforced the Elector of Bavaria on the Danube thus posing a direct threat to Vienna, while Vendôme remained at the head of a large army in northern Italy opposing Guido Starhemberg's weak Imperial force. Of equal alarm was Francis II Rákóczi's revolt which, by the end of the year, had reached as far as Moravia and Lower Austria.
Blenheim.
Dissension between Villars and the Elector of Bavaria had prevented an assault on Vienna in 1703, but in the Courts of Versailles and Madrid, ministers confidently anticipated the city's fall. The Imperial ambassador in London, Count Wratislaw, had pressed for Anglo-Dutch assistance on the Danube as early as February 1703, but the crisis in southern Europe seemed remote from the Court of St. James's where colonial and commercial considerations were more to the fore of men's minds. Only a handful of statesmen in England or the Dutch Republic realised the true implications of Austria's peril; foremost amongst these was the English Captain-General, the Duke of Marlborough.
By early 1704 Marlborough had resolved to march south and rescue the situation in southern Germany and on the Danube, personally requesting the presence of Eugene on campaign so as to have "a supporter of his zeal and experience". The Allied commanders met for the first time at the small village of Mundelsheim on 10 June, and immediately formed a close rapport – the two men becoming, in the words of Thomas Lediard, 'Twin constellations in glory'. This professional and personal bond ensured mutual support on the battlefield, enabling many successes during the Spanish Succession war. The first of these victories, and the most celebrated, came on 13 August 1704 at the Battle of Blenheim. Eugene commanded the right wing of the Allied army, holding the Elector of Bavaria's and Marshal Marsin's superior forces, while Marlborough broke through the Marshal Tallard's center, inflicting over 30,000 casualties. The battle proved decisive: Vienna was saved and Bavaria was knocked out of the war. Both Allied commanders were full of praise for each other's performance. Eugene's holding operation, and his pressure for action leading up to the battle, proved crucial for the Allied success.
In Europe Blenheim is regarded as much a victory for Eugene as it is for Marlborough, a sentiment echoed by Sir Winston Churchill (Marlborough's descendant and biographer), who pays tribute to "the glory of Prince Eugene, whose fire and spirit had exhorted the wonderful exertions of his troops." France now faced the real danger of invasion, but Leopold I in Vienna was still under severe strain: Rákóczi's revolt was a major threat; and Guido Starhemberg and Victor Amadeus (who had once again switched loyalties and rejoined the Grand Alliance in 1703) had been unable to halt the French under Vendôme in northern Italy. Only Amadeus' capital, Turin, held on.
Turin and Toulon.
Eugene returned to Italy in April 1705, but his attempts to move west towards Turin were thwarted by Vendôme's skilful manoeuvres. Lacking boats and bridging materials, and with desertion and sickness rife within his army, the outnumbered Imperial commander was helpless. Leopold I's assurances of money and men had proved illusory, but desperate appeals from Amadeus and criticism from Vienna goaded the Prince into action, resulting in the Imperialists' bloody defeat at the Battle of Cassano on 16 August. However, following Leopold I's death and the accession of Joseph I to the Imperial throne in May 1705, Eugene at last began to receive the personal backing he desired. Joseph I proved to be a strong supporter of Eugene's supremacy in military affairs; he was the most effective emperor the Prince served and the one he was happiest under. Promising support, Joseph I persuaded Eugene to return to Italy and restore Habsburg honour.
The Imperial commander arrived in theatre in mid-April 1706, just in time to organise an orderly retreat of what was left of Count Reventlow's inferior army following his defeat by Vendôme at the Battle of Calcinato on 19 April. Vendôme now prepared to defend the lines along the river Adige, determined to keep Eugene cooped to the east while the Marquis of La Feuillade threatened Turin. However, feigning attacks along the Adige, Eugene descended south across the river Po in mid-July, outmanoeuvring the French commander and gaining a favourable position from which he could at last move west towards Piedmont and relieve Savoy's capital.
Events elsewhere were now to have major consequences for the war in Italy. With Villeroi's crushing defeat by Marlborough at the Battle of Ramillies on 23 May, Louis XIV recalled Vendôme north to take command of French forces in Flanders. It was a transfer that Saint-Simon considered something of a deliverance for the French commander who was "now beginning to feel the unlikelihood of success [in Italy] … for Prince Eugene, with the reinforcements that had joined him after the Battle of Calcinato, had entirely changed the outlook in that theatre of the war." The Duke of Orléans, under the direction of Marsin, replaced Vendôme, but indecision and disorder in the French camp led to their undoing. After uniting his forces with Victor Amadeus at Villastellone in early September, Eugene attacked, overwhelmed, and decisively defeated the French forces besieging Turin on 7 September. Eugene's success broke the French hold on northern Italy, and the whole Po valley fell under Allied control. Eugene had gained a victory as signal as his colleague had at Ramillies – "It is impossible for me to express the joy it has given me;" wrote Marlborough, "for I not only esteem but I really love the prince. This glorious action must bring France so low, that if our friends could but be persuaded to carry on the war with vigour one year longer, we cannot fail, with the blessing of God, to have such a peace as will give us quiet for all our days."
The Imperial victory in Italy marked the beginning of Austrian rule in Lombardy, and earned Eugene the Governorship of Milan. But the following year was to prove a disappointment for the Prince and the Grand Alliance as a whole. The Emperor and Eugene (whose main goal after Turin was to take Naples and Sicily from Philip duc d'Anjou's supporters), reluctantly agreed to Marlborough's plan for an attack on Toulon – the seat of French naval power in the Mediterranean. However, disunion between the Allied commanders – Victor Amadeus, Eugene, and the English Admiral Shovell – doomed the Toulon enterprise to failure. Although Eugene favoured some sort of attack on France's south-eastern border it was clear he felt the expedition impractical, and had shown none of the "alacrity which he had displayed on other occasions." Substantial French reinforcements finally brought an end to the venture, and on 22 August 1707 the Imperial army began its retirement. The subsequent capture of Susa could not compensate for the total collapse of the Toulon expedition and with it any hope of an Allied war-winning blow that year.
Oudenarde and Malplaquet.
At the beginning of 1708 Eugene successfully evaded calls for him to take charge in Spain (in the end Guido Starhemberg was sent), thus enabling him to take command of the Imperial army on the Moselle and once again unite with Marlborough in the Spanish Netherlands. Eugene (without his army) arrived at the Allied camp at Assche, west of Brussels, in early July, providing a welcome boost to morale after the early defection of Bruges and Ghent to the French. " … our affairs improved through God's support and Eugene's aid," wrote the Prussian General Natzmer, "whose timely arrival raised the spirits of the army again and consoled us." Heartened by the Prince's confidence the Allied commanders devised a bold plan to engage the French army under Vendôme and the Duke of Burgundy. On 10 July the Anglo-Dutch army made a forced march to surprise the French, reaching the river Scheldt just as the enemy were crossing to the north. The ensuing battle on 11 July – more a contact action rather than a set-piece engagement – ended in a resounding success for the Allies, aided by the dissension of the two French commanders. While Marlborough remained in overall command, Eugene had led the crucial right flank and centre. Once again the Allied commanders had co-operated remarkably well. "Prince Eugene and I," wrote the Duke, "shall never differ about our share of the laurels."
Marlborough now favoured a bold advance along the coast to bypass the major French fortresses, followed by a march on Paris. But fearful of unprotected supply-lines, the Dutch and Eugene favoured a more cautious approach. Marlborough acquiesced and resolved upon the siege of Vauban's great fortress, Lille. While the Duke commanded the covering force, Eugene oversaw the siege of the town which surrendered on 22 October; however, it was not until 10 December that the resolute Marshal Boufflers yielded the citadel. Yet for all the difficulties of the siege (Eugene was badly wounded above his left eye by a musket ball, and even survived an attempt to poison him), the campaign of 1708 had been a remarkable success. The French were driven out of almost all the Spanish Netherlands. "He who has not seen this," wrote Eugene, "has seen nothing."
The recent defeats, together with the severe winter of 1708–09, had caused extreme famine and privation in France. Louis XIV was close to accepting Allied terms, but the conditions demanded by the leading Allied negotiators, Anthonie Heinsius, Charles Townshend, Marlborough, and Eugene – principally that Louis XIV should use his own troops to force Philip V off the Spanish throne – proved unacceptable to the French. Neither Eugene nor Marlborough had objected to the Allied demands at the time, but neither wanted the war with France to continue, and would have preferred further talks to deal with the Spanish issue. But the French King offered no further proposals. Lamenting the collapse of the negotiations, and aware of the vagaries of war, Eugene wrote to the Emperor in mid-June 1709. "There can be no doubt that the next battle will be the biggest and bloodiest that has yet been fought."
After the fall of Tournai on 3 September (itself a major undertaking), the Allied generals turned their attention towards Mons. Marshal Villars, recently joined by Boufflers, moved his army south-west of the town and began to fortify his position. Marlborough and Eugene favoured an engagement before Villars could render his position impregnable; but they also agreed to wait for reinforcements from Tournai which did not arrive until the following night, thus giving the French further opportunity to prepare their defences. Notwithstanding the difficulties of the attack, however, the Allied generals did not shrink from their original determination. The subsequent Battle of Malplaquet, fought on 11 September 1709, was the bloodiest engagement of the war. On the left flank, the Prince of Orange led his Dutch infantry in desperate charges only to have it cut to pieces; on the other flank, Eugene attacked and suffered almost as severely. But sustained pressure on his extremities forced Villars to weaken his centre, thus enabling Marlborough to breakthrough and claim victory. Villars was unable to save Mons, which subsequently capitulated on 21 October, but his resolute defence at Malplaquet – inflicting up to 25% casualties on the Allies – may have saved France from destruction.
Final campaigning: Eugene alone.
In August 1709 Eugene's chief political opponent and critic in Vienna, Prince Salm, retired as court chamberlain. Eugene and Wratislaw were now the undisputed leaders of the Austrian government: all major departments of state were in their hands or those of their political allies. However, another attempt at a negotiated settlement at Geertruidenberg in April 1710 failed, largely because the English Whigs still felt strong enough to refuse concessions, while Louis XIV saw little reason to accept what he had refused the previous year. Eugene and Marlborough could not be accused of wrecking the negotiations, but neither showed regret at the breakdown of the talks. There was no alternative but to continue the war, and in June the Allied commanders captured Douai. This success was followed by a series of minor sieges, and by the close of 1710 the Allies had cleared much of France's protective ring of fortresses. Yet there had been no final, decisive breakthrough, and this was to be the last year that Eugene and Marlborough would work together.
Following the death of Joseph I on 17 April 1711 his brother, Charles, the pretender to the Spanish throne, became emperor. In England the new Tory government (the 'peace party' who had deposed the Whigs in October 1710) declared their unwillingness to see Charles VI become Emperor as well as King of Spain, and had already begun secret negotiations with the French. In January 1712 Eugene arrived in England hoping to divert the government away from its peace policy, but despite the social success the visit was a political failure: Queen Anne and her ministers remained determined to end the war regardless of the Allies. Eugene had also arrived too late to save Marlborough who, seen by the Tories as the main obstacle to peace, had already been dismissed on charges of embezzlement. Elsewhere, however, the Austrians had made some progress – the Hungarian revolt had finally came to end. Although Eugene would have preferred to crush the rebels the Emperor had offered lenient conditions, leading to the signing of the Treaty of Szatmár on 30 April 1711.
Hoping to influence public opinion in England and force the French into making substantial concessions, Eugene prepared for a major campaign. However, on 21 May 1712 – when the Tories felt they had secured favourable terms with their unilateral talks with the French – the Duke of Ormonde (Marlborough's successor) received the so-called 'restraining orders', forbidding him to take part in any military action. Eugene took the fortress of Le Quesnoy in early July, before besieging Landrecies, but Villars, taking advantage of Allied disunity, outmanoeuvred Eugene and defeated the Earl of Albermarle's Dutch garrison at Denain on 24 July. The French followed the victory by seizing the Allies' main supply magazine at Marchiennes, before reversing their earlier losses at Douai, Le Quesnoy and Bouchain. In one summer the whole forward Allied position laboriously built up over the years to act as the springboard into France had been precipitously abandoned.
With the death in December of his friend and close political ally, Count Wratislaw, Eugene became undisputed 'first minister' in Vienna. His position was built on his military successes, but his actual power was expressed through his role as president of the war council, and as "de facto" president of the conference which dealt with foreign policy. In this position of influence Eugene took the lead in pressing Charles VI towards peace. The government had come to accept that further war in the Netherlands or Spain was impossible without the aid of the Maritime Powers; yet the Emperor, still hoping that somehow he could place himself on the throne in Spain, refused to make peace at the Utrecht conference along with the other Allies. Reluctantly, Eugene prepared for another campaign, but lacking troops, finance, and supplies his prospects in 1713 were poor. Villars, with superior numbers, was able to keep Eugene guessing as to his true intent. Through successful feints and stratagems Landau fell to the French commander in August, followed in November by Freiburg. Eugene was reluctant to carry on the war, and wrote to the Emperor in June that a bad peace would be better than being 'ruined equally by friend and foe'. With Austrian finances exhausted and the German states reluctant to continue the war, Charles VI was compelled to enter into negotiations. Eugene and Villars (who had been old friends since the Turkish campaigns of the 1680s) initiated talks on 26 November. Eugene proved an astute and determined negotiator, and gained favourable terms by the Treaty of Rastatt signed on 7 March 1714 and the Treaty of Baden signed on 7 September 1714. Despite the failed campaign in 1713 the Prince was able to declare that, "in spite of the military superiority of our enemies and the defection of our Allies, the conditions of peace will be more advantageous and more glorious than those we would have obtained at Utrecht."
Austro-Turkish War.
Eugene's main reason for desiring peace in the west was the growing danger posed by the Turks in the east. Turkish military ambitions had revived after 1711 when they had mauled Peter the Great's army on the river Pruth: in December 1714 Sultan Ahmed III's forces attacked the Venetians in the Morea. To Vienna it was clear that the Turks intended to attack Hungary and undo the whole Karlowitz settlement of 1699. After the Porte rejected an offer of mediation in April 1716, Charles VI despatched Eugene to Hungary to lead his relatively small but professional army. Of all Eugene's wars this was the one in which he exercised most direct control; it was also a war which, for the most part, Austria fought and won on her own.
Eugene left Vienna in early June 1716 with a field army of between 80,000–90,000 men. By early August 1716 the Ottoman Turks, some 200,000 men under the sultan's son-in-law, the Grand Vizier Damat Ali Pasha, were marching from Belgrade towards Eugene's position west of the fortress of Petrovaradin on the north bank of the Danube. The Grand Vizier had intended to seize the fortress; but Eugene gave him no chance to do so. After resisting calls for caution and forgoing a council of war, the Prince decided to attack immediately on the morning of 5 August with approximately 70,000 men. The Turkish janissaries had some initial success, but after an Imperial cavalry attack on their flank, Ali Pasha's forces fell into confusion. Although the Imperials lost almost 5,000 dead or wounded, the Turks, who retreated in disorder to Belgrade, seem to have lost double that amount, including the Grand Vizier himself who had entered the mêlée and subsequently died of his wounds.
Eugene proceeded to take the Banat fortress of Temeswar in mid-October 1716 (thus ending 164 years of Turkish rule), before turning his attention to the next campaign and to what he considered the main goal of the war – Belgrade. Situated at the confluence of the Rivers Danube and Sava, Belgrade held a garrison of 30,000 men under Mustapha Pasha. Imperial troops besieged the place in mid-June 1717, and by the end of July large parts of the city had been destroyed by artillery fire. By the first days of August, however, a huge Turkish field army (150,000–200,000 strong), under the new Grand Vizier, Halil Pasha, had arrived on the plateau east of the city to relieve the garrison. News spread through Europe of Eugene's imminent destruction; but he had no intention of lifting the siege. With his men suffering from dysentery, and continuous bombardment from the plateau, Eugene, aware that a decisive victory alone could extricate his army, decided to attack the relief force. On the morning of 16 August 40,000 Imperial troops marched through the fog, caught the Turks unawares, and routed Halil Pasha's army; a week later Belgrade surrendered, effectively bringing an end to the war. The victory was the crowning point of Eugene's military career and had confirmed him as the leading European general. His ability to snatch victory at the moment of defeat had shown the Prince at his best.
The principal objectives of the war had been achieved: the task Eugene had begun at Zenta was complete, and the Karlowitz settlement secured. By the terms of the Treaty of Passarowitz, signed on 21 July 1718, the Turks surrendered the Banat of Temeswar, along with Belgrade and most of Serbia, although they regained the Morea from the Venetians. The war had dispelled the immediate Turkish threat to Hungary, and was a triumph for the Empire and for Eugene personally.
Quadruple Alliance.
While Eugene fought the Turks in the east, unresolved issues following the Utrecht/Rastatt settlements led to hostilities between the Emperor and Philip V of Spain in the west. Charles VI had refused to recognise Philip V as King of Spain, a title which he himself claimed; in return, Philip V had refused to renounce his claims to Naples, Milan, and the Netherlands, all of which had transferred to the House of Austria following the Spanish Succession war. Philip V was roused by his influential wife, Elisabeth Farnese, daughter of the Hereditary Prince of Parma, who personally held dynastic claims in the name of her son, Don Charles, to the duchies of Tuscany, Parma and Piacenza. Representatives from a newly formed Anglo-French alliance – who were desirous of European peace for their own dynastic securities and trade opportunities – called on both parties to recognise each other's sovereignty. Yet Philip V remained intractable, and on 22 August 1717 his chief minister, Alberoni, effected the invasion of Austrian Sardinia in what seemed like the beginning of the reconquest of Spain's former Italian empire.
Eugene returned to Vienna from his recent victory at Belgrade (before the conclusion of the Turkish war) determined to prevent an escalation of the conflict, complaining that, "two wars cannot be waged with one army"; only reluctantly did the Prince release some troops from the Balkans for the Italian campaign. Rejecting all diplomatic overtures Philip V unleashed another assault in June 1718, this time against Savoyard Sicily as a preliminary to attacking the Italian mainland. Realising that only the British fleet could prevent further Spanish landings, and that pro-Spanish groups in France might push the regent, Duke of Orléans, into war against Austria, Charles VI had no option but to sign the Quadruple Alliance on 2 August 1718, and formally renounce his claim to Spain. Despite the Spanish fleet's destruction off Cape Passaro, Philip V and Elisabeth remained resolute, and rejected the treaty.
Although Eugene could have gone south after the conclusion of the Turkish war, he chose instead to conduct operations from Vienna; but Austria's military effort in Sicily proved derisory, and Eugene's chosen commanders, Zum Jungen, and later Count Mercy, performed poorly. It was only from pressure exerted by the French army advancing into the Basque provinces of northern Spain in April 1719, and the British Navy's attacks on the Spanish fleet and shipping, that compelled Philip V and Elisabeth to dismiss Alberoni and join the Quadruple Alliance on 25 January 1720. Nevertheless, the Spanish attacks had strained Charles VI's government, causing tension between the Emperor and his Spanish Council on the one hand, and the conference, headed by Eugene, on the other. Despite Charles VI's own personal ambitions in the Mediterranean it was clear to the Emperor that Eugene had put the safeguarding of his conquests in Hungary before everything else, and that military failure in Sicily also had to rest on Eugene. Consequently the Prince's influence over the Emperor declined considerably.
Later life (1721–36).
Governor-General of the Southern Netherlands.
Eugene had become governor of the Southern Netherlands – now the Austrian Netherlands – in June 1716, but he was an absent ruler, directing policy from Vienna through his chosen representative the Marquis of Prié. De Prié proved unpopular with the local population and the guilds who, following the Barrier Treaty of 1715, were obliged to meet the financial demands of the administration and the Dutch barrier garrisons; with Eugene's backing and encouragement civil disturbances in Antwerp and Brussels were forcibly suppressed. After displeasing the Emperor over his initial opposition to the formation of the Ostend Company, de Prié also lost the support of the native nobility from within his own council of state in Brussels, particularly from the Marquis de Mérode-Westerloo. One of Eugene's former favourites, General Bonneval, also joined the nobles in opposition to de Prié, further undermining the Prince. When de Prié's position became untenable Eugene felt compelled to resign his post as governor on 16 November 1724. As compensation Charles VI conferred on him the honorary position as vicar-general of Italy, worth 140,000 gulden a year, and an estate at Siebenbrunn in Lower Austria said to be worth double that amount. But his resignation distressed him, and to compound his concerns Eugene caught a severe bout of influenza that Christmas, marking the beginning of permanent bronchitis and acute infections every winter for the remaining twelve years of his life.
'Cold war'.
The 1720s saw rapidly changing alliances between the European powers and almost constant diplomatic confrontation, largely over unsolved issues regarding the Quadruple Alliance. The Emperor and the Spanish King continued to use each other's titles, and Charles VI still refused to remove the remaining legal obstacles to Don Charles' eventual succession to the duchies of Parma and Tuscany. Yet in a surprise move Spain and Austria moved closer with the signing of the Treaty of Vienna in April/May 1725. In response Britain, France, and Prussia joined together in the Alliance of Hanover to counter the danger to Europe of an Austro-Spanish hegemony. For the next three years there was the continual threat of war between the Hanover Treaty powers and the Austro-Spanish bloc.
From 1726 Eugene gradually began to regain his political influence. With his many contacts throughout Europe Eugene, backed by Gundaker Starhemberg and Count Schönborn, the Imperial vice-chancellor, managed to secure powerful allies and strengthen the Emperor's position – his skill in managing the vast secret diplomatic network over the coming years was the main reason why Charles VI once again came to depend upon him. In August 1726 Russia acceded to the Austro-Spanish alliance, and in October Frederick William of Prussia followed suit by defecting from the Allies with the signing of a mutual defensive treaty with the Emperor. Despite the conclusion of the brief Anglo-Spanish conflict, war between the European powers persisted throughout 1727–28. However, in 1729 Elisabeth Farnese abandoned the Austro-Spanish alliance. Realizing that Charles VI could not be drawn into the marriage pact she wanted, Elisabeth concluded that the best way to secure her son's succession to Parma and Tuscany now lay with Britain and France. To Eugene it was 'an event that which is seldom to be found in history'. Following the Prince's determined lead to resist all pressure, Charles VI sent troops into Italy to prevent the entry of Spanish garrisons into the contested duchies. By the beginning of 1730 Eugene, who had remained bellicose throughout the whole period, was again in control of Austrian policy.
In Britain there now emerged a new political re-alignment as the Anglo-French "entente" became increasingly defunct. Believing that a resurgent France now posed the greatest danger to their security British ministers, headed by Robert Walpole, moved to reform the Anglo-Austrian alliance, leading to the signing of the Second Treaty of Vienna on 16 March 1731. Eugene had been the Austrian minister most responsible for the alliance, believing once again it would provide security against France and Spain. The treaty compelled Charles VI to sacrifice the Ostend Company (a rival to the English and Dutch trading companies) and accept, unequivocally, the accession of Don Charles to Parma and Tuscany. In return King George II as King of Great Britain and Elector of Hanover guaranteed the Pragmatic Sanction, the device to secure the rights of the Emperor's daughter, Maria Theresa, to the entire Habsburg inheritance. It was largely through Eugene's diplomacy that in January 1732 the Imperial diet also guaranteed the Pragmatic Sanction which, together with the Treaties with Britain, Russia, and Prussia, marked the culmination of the Prince's diplomacy. But the Treaty of Vienna had infuriated the court of King Louis XV: the French had been ignored and the Pragmatic Sanction guaranteed, thus increasing Habsburg influence and confirming Austria's vast territorial size. The Emperor also intended Maria Theresa to marry Francis Stephen of Lorraine which would present an unacceptable threat on France's border. By the beginning of 1733 the French army was ready for war: all that was needed was the excuse.
War of the Polish Succession.
In 1733 the Polish King and Elector of Saxony, Augustus the Strong, died. There were two candidates for his successor: first, Stanisław Leszczyński, the father-in-law of Louis XV; second, the Elector of Saxony's son, Augustus, supported by Russia, Austria, and Prussia. The Polish succession had afforded Louis XV's chief minister, Fleury, the opportunity to attack Austria and take Lorraine from Francis Stephen. In order to gain Spanish support France backed the succession of Elisabeth Farnese's sons to further Italian lands.
Eugene entered the War of the Polish Succession as President of the Imperial War Council and commander-in-chief of the army, but he was severely handicapped by the quality of his troops and the shortage of funds; now in his seventies, the Prince was also burdened by rapidly declining physical and mental powers. France declared war on Austria on 10 October 1733, but without the funds from the Maritime Powers – who, despite the Vienna treaty, remained neutral throughout the war – Austria could not hire the necessary troops to wage an offensive campaign. "The danger to the monarchy," wrote Eugene to the Emperor in October, "cannot be exaggerated". By the end of the year Franco-Spanish forces had seized Lorraine and Milan; by early 1734 Spanish troops had taken Sicily.
Eugene took command on the Rhine in April 1734, but vastly outnumbered he was forced onto the defensive. In June Eugene set out to relieve Philippsburg, yet his former drive and energy was now gone. Accompanying Eugene was a young Frederick the Great, sent by his father to learn the art of war. Frederick gained considerable knowledge from Eugene, recalling in later life his great debt to his Austrian mentor, but the Prussian prince was aghast at Eugene's condition, writing later, "his body was still there but his soul had gone." Eugene conducted another cautious campaign in 1735, once again pursuing a sensible defensive strategy on limited resources; but his short-term memory was by now practically non-existent, and his political influence disappeared completely – Gundaker Starhemberg and Johann Christoph von Bartenstein now dominated the conference in his place. However, fortunately for Charles VI Fleury was determined to limit the scope of the war, and in October 1735 he granted generous peace preliminaries to the Emperor.
Private life and death.
Despite being one of the richest and most celebrated men of his age, Eugene never married and the suggestion is that he was predominantly homosexual. History knows little of his life before 1683. In his early boyhood in Paris "he belonged to a small, effeminate set that included such unabashed perverts as the young abbé de Choisy who was invariably dressed as a girl" wrote the English historian Nicholas Henderson. The Duchess of Orléans, who had known Eugene from those days, would later write to her aunt, Princess Sophia of Hanover, describing Eugene's antics with lackeys and pages. He was "a vulgar whore" along with the Prince of Turenne, and "often played the woman with young people" with the nickname of 'Madame Simone' or 'Madam l'Ancienne'. He preferred a "couple of fine page boys" to any woman, and was refused an ecclesiastical benefice due to his "depravity" Eugene's behaviour may have been a result of his mother's lax household and her own failure to show any affection towards him.
Of related interest is a popular soldier's song which parodied an imaginary voyage by Eugene and the marquis de la Moussaye on the Rhine. A storm breaks and the general fears the worst, but the Marquis consoles him: "Our lives are safe/ For we are sodomites/ Destined to perish only by fire/ We shall land." A comment made by Johann Matthias von der Schulenburg in 1709, who had served under Eugene, could be read that the prince enjoyed "la petite débauche et la p[ine] au-delà de tout," or that he derived his sexual gratification from the virile member of others.
During the last 20 years of his life Eugene was particularly close to Countess Eleonora Batthyány, daughter of Count Theodor von Strattman.
Much about their acquaintance remains speculative (Eugene never mentions her in any of his surviving letters), and there is certainly no suggestion of a sexual relationship, but although they lived apart most foreign diplomats regarded Eleonora as his "official lover". Eugene and Eleonora were constant companions, meeting for dinner, receptions and card games almost every day till his death. But their surviving correspondence does not indicate any real intimacy in the relationship. Eugene's other friends such as the papal nuncio, Passionei, made up for the family he still lacked.
For his only surviving nephew, Emmanuel, the son of his brother Louis Thomas, Eugene arranged a marriage with one of the daughters of Prince Liechenstein, but Emmanuel died of smallpox in 1729. With the death of Emmanuel's son in 1734, no close male relatives remained to succeed the Prince. His closest relative, therefore, was Louis Thomas's unmarried daughter, Princess Maria Anna Victoria of Savoy, whom Eugene had never met and, as he had heard nothing but bad of her, made no effort to do so.
Eugene returned to Vienna from the War of the Polish Succession in October 1735, weak and feeble; when Maria Theresa and Francis Stephen married in February 1736 Eugene was too ill to attend. After playing cards at Countess Batthyány's on the evening of 20 April he returned to his bed at the Stadtpalais. When his servants arrived to wake him the next morning, 21 April 1736, they found Prince Eugene dead after choking from phlegm in his throat, presumably after suffering from pneumonia. Eugene's heart was buried with those of others of his family in Turin. His remains were carried in a long procession to St. Stephen's Cathedral, where the body was interred in the "Kreuzkapelle".
Countess Batthyány expressed in a letter dated 23 December 1720, that at the "Kreuzkapelle" a solemn requiem would be held annually. She dedicated for this purpose two thousand guilders.
Patron of the arts.
Eugene's rewards for his victories, his share of booty, his revenues from his abbeys in Savoy, and a steady income from his Imperial offices and governorships, enabled him to contribute to the landscape of baroque architecture. Eugene spent most of his life in Vienna at his Winter Palace, the Stadtpalais, built by Fischer von Erlach. The palace acted as his official residence and home, but for reasons that remain speculative the Prince's association with Fischer ended before the building was complete, favouring instead Johann Lukas von Hildebrandt as his chief architect. Eugene first employed Hildebrandt to finish the Stadtpalais before commissioning him to prepare plans for a palace (Savoy Castle) on his Danubian island at Ráckeve. Began in 1701 the single-story building took twenty years to complete; yet, probably because of the Rákóczi revolt, the Prince seems to have visited it only once – after the siege of Belgrade in 1717.
Of more importance was the grandiose complex of the two Belvedere palaces in Vienna. The single-storey Lower Belvedere, with its exotic gardens and zoo, was completed in 1716. The Upper Belvedere, completed between 1720 and 1722, is a more substantial building; with sparkling white stucco walls and copper roof it became a wonder of Europe. Eugene and Hildebrandt also converted an existing structure on his Marchfeld estate into a country seat, the Schlosshof, situated between the Rivers Danube and Morava. The building, completed in 1729, was far less elaborate than his other projects but it was strong enough to serve as a fortress in case of need. Eugene spent much of his spare time there in his last years accommodating large hunting parties.
In the years following the Peace of Rastatt Eugene became acquainted with a large number of scholarly men. Given his position and responsiveness they were keen to meet him: few could exist without patronage and this was probably the main reason for Gottfried Leibniz's association with him in 1714. Eugene also befriended the French writer Jean-Baptiste Rousseau who, by 1716, was receiving financial support from Eugene. Rousseau stayed on attached to the Prince's household, probably helping in the library, until he left for the Netherlands in 1722. Another acquaintance, Montesquieu, already famous for his "Persian Letters" when he arrived in Vienna in 1728, favourably recalled his time spent at the Prince's table. Nevertheless, Eugene had no literary pretensions of his own, and was not tempted like Maurice de Saxe or Marshal Villars to write his memoirs or books on the art of war. He did, however, become a collector on the grandest scale: his picture galleries were filled with 16th and 17th century Italian, Dutch and Flemish art; his library at the Stadtpalais crammed with over 15,000 books, 237 manuscripts as well as a huge collection of prints (of particular interest were books on natural history and geography). "It is hardly believable," wrote Rousseau, "that a man who carries on his shoulders the burden of almost all the affairs of Europe … should find as much time to read as though he had nothing else to do." At Eugene's death his possessions and estates, except those in Hungary which the crown reclaimed, went to his niece, Princess Maria Anna Victoria, who at once decided to sell everything. The artwork was bought by Charles Emmanuel III of Sardinia. Eugene's library, prints and drawings were purchased by the Emperor in 1737 and have since passed into Austrian national collections.
Assessment.
Napoleon considered Eugene one of the seven greatest commanders of history. Although later military critics have disagreed with that assessment, Eugene was undoubtedly the greatest Austrian general. He was no military innovator, but he had the ability to make an inadequate system work. He was equally adept as an organizer, strategist and tactician, believing in the primacy of battle and his ability to seize the opportune moment to launch a successful attack. "The important thing," wrote Maurice de Saxe in his "Reveries", "is to see the opportunity and to know how to use it. Prince Eugene possessed this quality which is the greatest in the art of war and which is the test of the most elevated genius." This fluidity was key to his battlefield successes in Italy and in his wars against the Turks. Nevertheless, in the Low Countries, particularly after the battle of Oudenarde in 1708, Eugene, like his cousin Louis of Baden, tended to play safe and become bogged down in a conservative strategy of sieges and defending supply lines. After the attempt on Toulon in 1707, he also became very wary of combined land/sea operations. To historian Derek McKay, however, the main criticism of him as a general is his legacy – he left no school of officers nor an army able to function without him.
Eugene was a disciplinarian – when ordinary soldiers disobeyed orders he was prepared to shoot them himself – but he rejected blind brutality, writing "you should only be harsh when, as often happens, kindness proves useless". On the battlefield Eugene demanded courage in his subordinates, and expected his men to fight where and when he wanted; his criteria for promotion were based primarily on obedience to orders and courage on the battlefield rather than social position. On the whole his men responded because he was willing to push himself as hard as them. However, his position as President of the Imperial War Council proved less successful. Following the long period of peace after the Austro-Turkish War, the idea of creating a separate field army or providing garrison troops with effective training for them to be turned into such an army quickly was never considered by Eugene. By the time of the War of the Polish Succession, therefore, the Austrians were outclassed by a better prepared French force. For this Eugene was largely to blame – in his view (unlike the drilling and manoeuvres carried out by the Prussians which to Eugene seemed irrelevant to real warfare) the time to create actual fighting men was when war came. But although Frederick the Great had been struck by the muddle of the Austrian army and its poor organisation during the Polish Succession war, he later amended his initial harsh judgements. "If I understand anything of my trade," commented Frederick in 1758, "especially in the more difficult aspects, I owe that advantage to Prince Eugene. From him I learnt to hold grand objectives constantly in view, and direct all my resources to those ends." To historian Christopher Duffy it was this awareness of the 'grand strategy' that was Eugene's legacy to Frederick.
To his responsibilities Eugene attached his own personal values – physical courage, loyalty to his sovereign, honesty, self-control in all things – and he expected these qualities from his commanders. Eugene's approach was dictatorial, but he was willing to co-operate with someone he regarded as his equal, such as Baden or Marlborough. Yet the contrast to his co-commander of the Spanish Succession war were stark. 'Marlborough,' writes Churchill was 'the model husband and father, concerned with building up a home, founding a family, and gathering a fortune to sustain it'; whereas Eugene, the bachelor, was 'disdainful of money, content with his bright sword and his lifelong animosities against Louis XIV.' The result was an austere figure, inspiring respect and admiration rather than affection. The huge equestrian statue in the centre of Vienna commemorates Eugene's achievements. Inscribed on one side, 'To the wise counsellor of three Emperors', and on the other, 'To the glorious conqueror of Austria's enemies'.
References.
Further reading.
</dl>

</doc>
<doc id="9683" url="http://en.wikipedia.org/wiki?curid=9683" title="Emanuel Leutze">
Emanuel Leutze

Emanuel Gottlieb Leutze (May 24, 1816 – July 18, 1868) was a German American history painter best known for his painting "Washington Crossing the Delaware". He is associated with the Düsseldorf school of painting.
Biography.
Philadelphia.
Leutze was born in Schwäbisch Gmünd, Württemberg, Germany, and was brought to the United States as a child. His parents settled first in Fredericksburg, Virginia, and then at Philadelphia. His early education was good, though not especially in the direction of art. The first development of his artistic talent occurred while he was attending the sickbed of his father, when he attempted drawing to occupy the long hours of waiting. His father died in 1831. At 14, he was painting portraits for $5 apiece. Through such work, he supported himself after the death of his father. In 1834, he received his first instruction in art in classes of John Rubens Smith, a portrait painter in Philadelphia. He soon became skilled, and promoted a plan for publishing, in Washington, portraits of eminent American statesmen; however, he met with but slight encouragement.
Europe.
In 1840, one of his paintings attracted attention and procured him several orders, which enabled him to go to the Kunstakademie Düsseldorf, where he studied with Lessing. In 1842 he went to Munich, studying the works of Cornelius and Kaulbach, and, while there, finished his "Columbus before the Queen". The following year he visited Venice and Rome, making studies from Titian and Michelangelo. His first work, "Columbus before the Council of Salamanca" was purchased by the Düsseldorf Art Union. A companion picture, "Columbus in Chains", procured him the gold medal of the Brussels Art Exhibition, and was subsequently purchased by the Art Union in New York; it was the basis of the 1893 $2 Columbian stamp. In 1845, after a tour in Italy, he returned to Düsseldorf, marrying Juliane Lottner and making his home there for 14 years.
During his years in Düsseldorf, he was a resource for visiting Americans: he found them places to live and work, provided introductions, and emotional and even financial support. For many years, he was the president of the Düsseldorf Artists' Association; in 1848, he was an early promoter of the “Malkasten” art association; and in 1857, he led the call for a gathering of artists which led to the founding of the Allgemeine deutsche Kunstgenossenschaft.
A strong supporter of Europe's Revolutions of 1848, Leutze decided to paint an image that would encourage Europe's liberal reformers with the example of the American Revolution. Using American tourists and art students as models and assistants, Leutze finished "Washington Crossing the Delaware" in 1850. It is owned by the Metropolitan Museum of Art in New York. In 1854, Leutze finished his depiction of the Battle of Monmouth, "Washington rallying the troops at Monmouth," commissioned by an important Leutze patron, banker David Leavitt of New York City and Great Barrington, Massachusetts.
New York City and Washington, D.C..
In 1859, Leutze returned to the United States and opened a studio in New York City. He divided his time between New York City and Washington, D.C. In 1859, he painted a portrait of Chief Justice Roger Brooke Taney which hangs in the Harvard Law School. In a 1992 opinion, Justice Antonin Scalia described the portrait of Taney, made two years after Taney's infamous decision in Dred Scott v. Sandford, as showing Taney "in black, sitting in a shadowed red armchair, left hand resting upon a pad of paper in his lap, right hand hanging limply, almost lifelessly, beside the inner arm of the chair. He sits facing the viewer and staring straight out. There seems to be on his face, and in his deep-set eyes, an expression of profound sadness and disillusionment."
Leutze also executed other portraits, including one of fellow painter William Morris Hunt. That portrait was owned by Hunt's brother Leavitt Hunt, a New York attorney and sometime Vermont resident, and was shown at an exhibition devoted to William Morris Hunt's work at the Museum of Fine Arts, Boston in 1878.
In 1860 Leutze was commissioned by the U.S. Congress to decorate a stairway in the Capitol Building in Washington, DC, for which he painted a large composition, "Westward the Course of Empire Takes Its Way", which is also commonly known as "Westward Ho!".
Late in life, he became a member of the National Academy of Design. He was also a member of the Union League Club of New York, which has a number of his paintings. He died in Washington, D.C., in his 52nd year, of heatstroke. He was interred at Glenwood Cemetery. At the time of his death, a painting, "The Emancipation of the Slaves", was in preparation.
Leutze's portraits are known less for their artistic quality than for their patriotic emotionalism. "Washington Crossing the Delaware" firmly ranks among the American national iconography, and is thus often caricatured.

</doc>
<doc id="9684" url="http://en.wikipedia.org/wiki?curid=9684" title="Erasmus Alberus">
Erasmus Alberus

Erasmus Alberus (c. 1500–1553), German humanist, reformer, and poet.
Life.
He was born in the village of Bruchenbrücken (now part of Friedberg, Hesse) about the year 1500. Although his father Tilemann Alber was a schoolmaster, his early education was neglected. 
Ultimately in 1518 he found his way to the University of Wittenberg, where he studied theology. He had the good fortune to attract the attention of Martin Luther and Philipp Melanchthon, and subsequently became one of Luther's most active helpers in the Protestant Reformation.
Not only did he fight for the Protestant cause as a preacher and theologian, but he was almost the only member of Luther's party who was able to confront the Roman Catholics with the weapon of literary satire. In 1542 he published a prose satire to which Luther wrote the preface, "Der Barfusser Monche Eulenspiegel und Alkoran," a parodic adaptation of the "Liber conformitatum" of the Franciscan Bartolommeo Rinonico of Pisa, in which the Franciscan order is held up to ridicule.
Of higher literary value is the didactic and satirical "Buch von der Tugend und Weisheit" (1550), a collection of forty-nine fables in which Alberus embodies his views on the relations of Church and State. His satire is incisive, but in a scholarly and humanistic way; it does not appeal to popular passions with the fierce directness which enabled the master of Catholic satire, Thomas Murner, to inflict such telling blows.
Several of Alberus's hymns, all of which show the influence of his master Luther, have been retained in the German Protestant hymnal.
After Luther's death, Alberus was for a time a deacon in Wittenberg; he became involved, however, in the political conflicts of the time, and was in Magdeburg in 1550-1551, while that town was besieged by Maurice, Elector of Saxony. In 1552 he was appointed Generalsuperintendent at Neubrandenburg in Mecklenburg, where he died on the 5 May 1553.

</doc>
<doc id="9685" url="http://en.wikipedia.org/wiki?curid=9685" title="Earley parser">
Earley parser

In computer science, the Earley parser is an algorithm for parsing strings that belong to a given context-free language, though (depending on the variant) it may suffer problems with certain nullable grammars. The algorithm, named after its inventor, Jay Earley, is a chart parser that uses dynamic programming; it is mainly used for parsing in computational linguistics. It was first introduced in his dissertation in 1968 (and later appeared in abbreviated, more legible form in a journal).
Earley parsers are appealing because they can parse all context-free languages["discuss"], unlike LR parsers and LL parsers, which are more typically used in compilers but which can only handle restricted classes of languages. The Earley parser executes in cubic time in the general case formula_1, where "n" is the length of the parsed string, quadratic time for unambiguous grammars formula_2, and linear time for almost all LR(k) grammars. It performs particularly well when the rules are written left-recursively.
Earley Recogniser.
The following algorithm describes the Earley recogniser. The recogniser can be easily modified to create a parse tree as it recognises, and in that way can be turned into a parser.
The algorithm.
In the following descriptions, α, β, and γ represent any string of terminals/nonterminals (including the empty string), X and Y represent single nonterminals, and "a" represents a terminal symbol.
Earley's algorithm is a top-down dynamic programming algorithm. In the following, we use Earley's dot notation: given a production X → αβ, the notation X → α • β represents a condition in which α has already been parsed and β is expected.
Input position 0 is the position prior to input. Input position "n" is the position after accepting the "n"th token. (Informally, input positions can be thought of as locations at token boundaries.) For every input position, the parser generates a "state set". Each state is a tuple (X → α • β, "i"), consisting of
The state set at input position "k" is called S("k"). The parser is seeded with S(0) consisting of only the top-level rule. The parser then repeatedly executes three operations: "prediction", "scanning", and "completion".
It is important to note that duplicate states are not added to the state set, only new ones. These three operations are repeated until no new states can be added to the set. The set is generally implemented as a queue of states to process, with the operation to be performed depending on what kind of state it is.
Pseudocode.
Adapted from by Daniel Jurafsky and James H. Martin
function EARLEY-PARSE(words, grammar)
 ENQUEUE((γ → •S, 0), chart[0])
 for i ← from 0 to LENGTH(words) do
 for each state in chart[i] do
 if INCOMPLETE?(state) then
 if NEXT-CAT(state) is a nonterminal then
 PREDICTOR(state, i, grammar) // non-terminal
 else do
 SCANNER(state, i) // terminal
 else do
 COMPLETER(state, i)
 end
 end
 return chart
procedure PREDICTOR((A → α•B, i), j, grammar)
 for each (B → γ) in GRAMMAR-RULES-FOR(B, grammar) do
 ADD-TO-SET((B → •γ, j), chart[j])
 end
procedure SCANNER((A → α•B, i), j)
 if B ⊂ PARTS-OF-SPEECH(word[j]) then
 ADD-TO-SET((B → word[j], j), chart[j + 1])
 end
procedure COMPLETER((B → γ•, j), k)
 for each (A → α•Bβ, i) in chart[j] do
 ADD-TO-SET((A → αB•β, i), chart[k])
 end
Example.
Consider the following simple grammar for arithmetic expressions:
<P> ::= <S> # the start rule
<S> ::= <S> "+" <M> | <M>
<M> ::= <M> "*" <T> | <T>
<T> ::= "1" | "2" | "3" | "4"
With the input:
 2 + 3 * 4
This is the sequence of state sets:
 (state no.) Production (Origin) # Comment
S(0): • 2 + 3 * 4.
 (1) P → • S (0) # start rule
 (2) S → • S + M (0) # predict from (1)
 (3) S → • M (0) # predict from (1)
 (4) M → • M * T (0) # predict from (3)
 (5) M → • T (0) # predict from (3)
 (6) T → • number (0) # predict from (5)
S(1): 2 • + 3 * 4.
 (1) T → number • (0) # scan from S(0)(6)
 (2) M → T • (0) # complete from (1) and S(0)(5)
 (3) M → M • * T (0) # complete from (2) and S(0)(4)
 (4) S → M • (0) # complete from (2) and S(0)(3)
 (5) S → S • + M (0) # complete from (4) and S(0)(2)
 (6) P → S • (0) # complete from (4) and S(0)(1)
S(2): 2 + • 3 * 4.
 (1) S → S + • M (0) # scan from S(1)(5)
 (2) M → • M * T (2) # predict from (1)
 (3) M → • T (2) # predict from (1)
 (4) T → • number (2) # predict from (3)
S(3): 2 + 3 • * 4.
 (1) T → number • (2) # scan from S(2)(4)
 (2) M → T • (2) # complete from (1) and S(2)(3)
 (3) M → M • * T (2) # complete from (2) and S(2)(2)
 (4) S → S + M • (0) # complete from (2) and S(2)(1)
 (5) S → S • + M (0) # complete from (4) and S(0)(2)
 (6) P → S • (0) # complete from (4) and S(0)(1)
S(4): 2 + 3 * • 4.
 (1) M → M * • T (2) # scan from S(3)(3)
 (2) T → • number (4) # predict from (1)
S(5): 2 + 3 * 4 •.
 (1) T → number • (4) # scan from S(4)(2)
 (2) M → M * T • (2) # complete from (1) and S(4)(1)
 (3) M → M • * T (2) # complete from (2) and S(2)(2)
 (4) S → S + M • (0) # complete from (2) and S(2)(1)
 (5) S → S • + M (0) # complete from (4) and S(0)(2)
 (6) P → S • (0) # complete from (4) and S(0)(1)
The state (P → S •, 0) represents a completed parse. This state also appears in S(3) and S(1), which are complete sentences.

</doc>
<doc id="9686" url="http://en.wikipedia.org/wiki?curid=9686" title="Ethiopian cuisine">
Ethiopian cuisine

Ethiopian cuisine (Amharic: የኢትዮጵያ ምግብ) characteristically consists of vegetable and often very spicy meat dishes. This is usually in the form of "wat" (also "w'et" or "wot"), a thick stew, served atop "injera", a large sourdough flatbread, which is about 50 cm in diameter and made out of fermented teff flour. Ethiopians eat exclusively with their right hands, using pieces of "injera" to pick up bites of entrées and side dishes. Utensils are optional.
The Ethiopian Orthodox Church prescribes a number of fasting ("tsom", Ge'ez: ጾም "ṣōm") periods, including Wednesdays, Fridays, and the entire Lenten season, so Ethiopian cuisine contains many dishes that are vegan.
Overview.
A typical dish consists of injera accompanied by a spicy stew, which frequently includes beef, lamb, vegetables and various types of legumes, such as lentils. Gurage cuisine also makes use of the false banana plant ("enset", Ge'ez: እንሰት "inset"), a type of ensete. The plant is pulverized and fermented to make a bread-like food called "qocho" or "kocho" (Ge'ez: ቆጮ "ḳōč̣ō"), which is eaten with kitfo. The root of this plant may be powdered and prepared as a hot drink called "bulla" (Ge'ez: ቡላ "būlā"), which is often given to those who are tired or ill. Another typical Gurage preparation is coffee with butter ("kebbeh"). "Kita" herb bread is also baked.
Pasta is frequently available throughout Ethiopia, including rural areas. Coffee is also a large part of Ethiopian culture and cuisine. After every meal, a coffee ceremony is enacted and espresso coffee is served.
Traditional ingredients.
"Berbere", a combination of powdered chili pepper and other spices (somewhat analogous to Southwestern American chili powder), is an important ingredient used in many dishes. Also essential is "niter kibbeh", a clarified butter infused with ginger, garlic, and several spices.
"Mitmita" (Amharic: ሚጥሚጣ, ]) is a powdered seasoning mix used in Ethiopian and Eritrean cuisine. It is orange-red in color and contains ground birdseye chili peppers (piri piri), cardamom seed, cloves and salt. It occasionally has other spices including cinnamon, cumin and ginger.
In their adherence to strict fasting, Ethiopian cooks have developed a rich array of cooking oil sources—besides sesame and safflower—for use as a substitute for animal fats which is forbidden during fasting periods. Ethiopian cuisine also uses "nug" (also spelled "noog", also known as "niger seed").
Dishes.
Wat.
"Wat" begins with a large amount of chopped red onion, which is simmered or sauteed in a pot. Once the onions have softened, "niter kebbeh" (or, in the case of vegan dishes, vegetable oil) is added. Following this, "berbere" is added to make a spicy "keiy wat" or "keyyih tsebhi". Turmeric is used instead of "berbere" for a milder "alicha wat" or both spices are omitted when making vegetable stews, such as "atkilt wat". Meat such as beef (Amharic: "ሥጋ", "səga"), chicken (Amharic: "ዶሮ", "doro" or Tigrinya: "derho"), fish (Amharic: "ዓሣ", "asa"), goat or lamb (Amharic: "በግ", "beg" or Tigrinya: "beggi") is also added. Legumes such as split peas (Amharic: "ክክ", "kək" or Tigrinya: "kikki"') and lentils (Amharic: "ምስር", "məsər" or "birsin"); or vegetables such as potatoes (Amharic: "ድንች", "Dənəch"), carrots and chard (Amharic: ቆስጣ) are also used instead in vegan dishes. 
Each variation is named by appending the main ingredient to the type of wat (e.g. "kek alicha wat"). However, the word "keiy" is usually not necessary, as the spicy variety is assumed when it is omitted (e.g. "doro wat"). The term "atkilt wat" is sometimes used to refer to all vegetable dishes, but a more specific name can also be used (as in "dinich'na caroht wat", which translates to "potatoes and carrots stew"; but notice the word "atkilt" is usually omitted when using the more specific term).
Tibs.
Meat along with vegetables are sautéed to make "tibs" (also "tebs", "t'ibs", "tibbs", etc., Ge'ez: ጥብስ "ṭibs"). Tibs is served in a variety of manners, and can range from hot to mild or contain little to no vegetables. There are many variations of the delicacy, depending on type, size or shape of the cuts of meat used. 
The mid-18th century European visitor to Ethiopia, Remedius Prutky, describes "tibs" as a portion of grilled meat served "to pay a particular compliment or show especial respect to someone." This is perhaps still true as the dish is still prepared today to commemorate special events and holidays.
Gurage dishes.
Kitfo.
Another distinctively Ethiopian dish is "kitfo" (frequently spelled "ketfo"). It consists of raw (or rare) beef mince marinated in "mitmita" (Ge'ez: ሚጥሚጣ "mīṭmīṭā" a very spicy chili powder similar to the "berbere") and "niter kibbeh". "Gored gored" is very similar to "kitfo", but uses cubed rather than ground beef.
Ayibe.
"Ayibe" is a cottage cheese that is mild and crumbly. It is much closer in texture to crumbled feta. Although not quite pressed, the whey has been drained and squeezed out. It is often served as a side dish to soften the effect of very spicy food. It has little to no distinct taste of its own. However, when served separately, ayibe is often mixed with a variety of mild or hot spices typical of Gurage cuisine.
Gomen Kitfo.
"Gomen kitfo" is another typical Gurage dish. Collard greens (ጎመን "gōmen") are boiled, dried and then finely chopped and served with butter, chili and spices. It is a dish specially prepared for the occasion of Meskel, a very popular holiday marking the discovery of the True Cross. It is served along with "ayibe" or sometimes even "kitfo" in this tradition called "dengesa".
Breakfast.
"Fit-fit" or "fir-fir" is a common breakfast dish. It is made from shredded "injera" or "kitcha" stir-fried with spices or wat. Another popular breakfast food is "fatira". The delicacy consists of a large fried pancake made with flour, often with a layer of egg. It is eaten with honey. "Chechebsa" (or "kita firfir") resembles a pancake covered with "berbere" and "niter kibbeh", or other spices, and may be eaten with a spoon. "Genfo" is a kind of porridge, which is another common breakfast dish. It is usually served in a large bowl with a dug-out made in the middle of the genfo and filled with spiced "niter kibbeh". A variation of "ful", a fava bean stew with condiments, served with baked rolls instead of "injera", is also common for breakfast.
Snacks.
Typical Ethiopian snacks are "dabo kolo" (small pieces of baked bread that are similar to pretzels) or "kolo" (roasted barley sometimes mixed with other local grains). "Kolo" is often sold by kiosks and street venders wrapped in a paper cone. Snacking on popcorn is also common.
Beverages.
Coffee.
According to some sources, coffee ("buna") holds a legitimate claim as originating from Ethiopia. A key national beverage, it is an important part of local commerce.
The coffee ceremony is the traditional serving of coffee, usually after a big meal. It often involves the use of a "jebena" (ጀበና), a clay coffee pot in which the coffee is boiled. The preparer roasts the coffee beans right in front of guests, then walks around wafting the smoke throughout the room so participants may sample the scent of coffee. Then the preparer grinds the coffee beans in a traditional tool called a "mokecha". The coffee is put into the "jebena", boiled with water, and then served with small cups called "si'ni". Coffee is usually served with sugar, but is also served with salt in many parts of Ethiopia. In some parts of the country, "niter kibbeh" is added instead of sugar or salt.
Snacks, such as popcorn or toasted barley (or "kollo"), are often served with the coffee. In most homes, a dedicated coffee area is surrounded by fresh grass, with special furniture for the coffee maker. A complete ceremony has three rounds of coffee (Abol, Tona and Bereka) and is accompanied by the burning of frankincense.
Tea ("shahee") will most likely be served if coffee is declined.
Non-alcoholic brews.
"Atmet" is a barley and oat-flour based drink that is cooked with water, sugar and "kibe" (Ethiopian clarified butter) until the ingredients have married and become a consistency slightly thicker than egg-nog. Though this drink is often given to women who are nursing, the sweetness and smooth texture make it a comfort drink for anyone who enjoys its flavor.
Manufactured drinks.
Ambo Mineral Water or "Ambo wuha" is a bottled carbonated mineral water, sourced from the springs in Ambo Senkele near the town of Ambo.
Spirits.
Tej is a potent honey wine. It is similar to mead, which is frequently served in bars (in particular, in a "tej bet" or "tej house"). "Katikala" and "araqe" are inexpensive local spirits that are very strong.
Tella is a home-brewed beer served in "tella bet" (""tella" houses") which specialize in serving "tella" only. "Tella" is the most common beverage made and served in households during holidays.
Gursha.
A "gursha" (var. "gorsha", "goorsha") is an act of friendship and love. When eating injera, a person uses his or her right hand to strip off a piece, wraps it around some "wat" or "kitfo", and then puts it into his or her mouth. During a meal with friends or family, it is a common custom to feed others in the group with one's hand by putting the rolled injera or a spoon full of other dishes into another's mouth. This is called a "gursha", and the larger the gursha, the stronger the friendship or bond (only surpassed by the brewing of Tej together). This tradition was popularized and celebrated in "The Food Wife," an episode of "The Simpsons" that uses Ethiopian cuisine as a plot point.

</doc>
<doc id="9688" url="http://en.wikipedia.org/wiki?curid=9688" title="Epistle of James">
Epistle of James

In Christianity, the Epistle of James (Ancient Greek: Ἰάκωβος "Iakōbos"), usually referred to simply as James, is a letter (epistle) in the New Testament. The earliest extant manuscripts of James usually date to the mid-to-late third century.
The author identifies himself as "James, a servant of God and of the Lord Jesus Christ" and the epistle is traditionally attributed to James the Just.
There are four views concerning the Epistle of James:
Framed within an overall theme of patient perseverance during trials and temptations, the text condemns various sins. The epistle was addressed to "the twelve tribes scattered abroad" (James 1:1), which is generally taken to mean a Jewish Christian audience outside of Palestine.
Composition.
The epistle may not be a true piece of correspondence between specific parties, but rather an example of wisdom literature formulated as a letter for circulation. The work is considered New Testament wisdom literature because, "like Proverbs and Sirach, it consists largely of moral exhortations and precepts of a traditional and eclectic nature." Similarly, the "Catholic Encyclopedia" says, "the subjects treated of in the Epistle are many and various; moreover, St. James not infrequently, whilst elucidating a certain point, passes abruptly to another, and presently resumes once more his former argument."
Authorship.
The writer calls himself simply "James, a servant of God and of the Lord Jesus Christ." Jesus had two apostles named James, but it is unlikely that either of these wrote the letter. One apostle, James, the son of Zebedee, was martyred about 44 AD. This would be very early for him to have been the writer. The other apostle James, the son of Alphaeus, is not prominent in the Scriptural record, and very little is known about him.
Rather, evidence points to James the brother of Jesus, to whom Jesus evidently had made a special appearance after his resurrection described in the New Testament. This James was prominent among the disciples. The writer of the letter of James identifies himself as "a slave of God and of the Lord Jesus Christ", in much the same way as did Jude, who introduced the Epistle of Jude by calling himself "a slave of Jesus Christ, but a brother of James". (Jas 1:1; Jude 1) Furthermore, the salutation of James’ letter includes the term “Greetings!” in the same way as did the letter concerning circumcision that was sent to the congregations. In this latter instance it was apparently Jesus’ brother James who spoke prominently in the assembly of "the apostles and the older men" at Jerusalem.
From the middle of the 3rd century, patristic authors cited the "Epistle" as written by James the Just, a relation of Jesus and first Bishop of Jerusalem. Not numbered among the Twelve Apostles, unless he is identified as James the Less, James was nonetheless a very important figure: Paul described him as "the brother of the Lord" in Galatians 1:19 and as one of the three "pillars of the Church" in 2:9. He is traditionally considered the first of the Seventy Disciples. John Calvin and others suggested that the author was the Apostle James, son of Alphaeus, who was often identified with James the Just. If written by James the Just, the place and time of the writing of the epistle would be Jerusalem, where James resided before his martyrdom in 62.
The Protestant reformer Martin Luther denied it was the work of an apostle and termed it an "epistle of straw" as compared to some other books in the New Testament, not least because of the conflict he thought it raised with Paul on the doctrine of justification (see below).
Many scholars consider the epistle to be written in the late 1st or early 2nd centuries. Among the reasons for this are:
Content.
The United Bible Societies' "Greek New Testament" divides the letter into the following sections:
Alternative view.
There are other approaches to understanding, and reading, the epistle of James. A historical approach begins on different assumptions, and is not content leaving the book as just "New Testament wisdom literature, like a small book of Proverbs", or like a loose collection of random pearls dropped in no particular order onto a piece of string. A 2013 journal article explores a violent historical background behind the epistle and offers the suggestion that it was indeed written by James the brother of Jesus, and therefore written before AD 62, the year of James' murder. The decade of the 50's saw the growth of turmoil and violence in Palestine as Jews became more and more frustrated with corruption, injustice and poverty. It continued into the 60's and four years after the murder of James, war broke out with Rome - a war that would lead to the destruction of Jerusalem and the scattering of the people. The epistle of James is renowned for exhortations on fighting poverty and caring for the poor in practical ways (1:26–27; 2:1-4; 2:14-19; 5:1-6), standing up for the oppressed (2:1-4; 5:1-6) and not being "like the world" in the way one responds to evil in the world (1:26-27; 2:11; 3:13-18; 4:1-10). Worldly wisdom is rejected and people are exhorted to embrace heavenly wisdom, which includes peacemaking and pursuing righteousness and justice (3:13-18). 
This approach sees the epistle as a real letter with a real immediate purpose: to encourage Christian Jews not to revert to violence in their response to injustice and poverty, but rather to stay focused on doing good, staying holy, and embracing the wisdom of heaven not the wisdom of the world.
Doctrine.
Justification.
The letter contains the following famous passage concerning salvation and justification:
14What good is it, my brothers, if someone says he has faith but does not have works? Can that faith save him?15If a brother or sister is poorly clothed and lacking in daily food, 16and one of you says to them, “Go in peace, be warmed and filled,” without giving them the things needed for the body, what good is that? 17So also faith by itself, if it does not have works, is dead.<br>
<br>
18But someone will say, “You have faith and I have works.” Show me your faith apart from your works, and I will show you my faith by my works. 19You believe that God is one; you do well. Even the demons believe—and shudder! 20Do you want to be shown, you foolish person, that faith apart from works is useless? 21Was not Abraham our father justified by works when he offered up his son Isaac on the altar? 22You see that faith was active along with his works, and faith was completed by his works; 23and the Scripture was fulfilled that says, “Abraham believed God, and it was counted to him as righteousness”—and he was called a friend of God. 24You see that a person is justified by works and not by faith alone. 25And in the same way was not also Rahab the prostitute justified by works when she received the messengers and sent them out by another way?26For as the body apart from the spirit is dead, so also faith apart from works is dead.
This passage has been cited in Christian theological debates, especially regarding the doctrine of justification. Gaius Marius Victorinus (4th century) associated James's teaching on works with the heretical Symmachian sect, followers of Symmachus the Ebionite, and openly questioned whether James's teachings were heretical. This passage has also been contrasted with the teachings of Paul the Apostle on justification; indeed, some scholars believe that this passage is a response to Paul. One issue in the debate is the meaning of the Greek word δικαιόω (dikaiόο) ‘render righteous or such as he ought to be’, with some among the participants taking the view that James is responding to a misunderstanding of Paul.
Roman Catholicism and Eastern Orthodoxy argue that this passage disproves the doctrine of justification by faith alone (or "sola fide"), whereas the early and many modern Protestants continue to believe that Catholic and Orthodox interpretations do not fully understand the meaning of the term "justification" and resolve James' and Paul's apparent conflict regarding faith and works in alternate ways from the Catholics and Orthodox:
Paul was dealing with one kind of error while James was dealing with a different error. The errorists Paul was dealing with were people who said that works of the law were needed to be added to faith in order to help earn God's favor. Paul countered this error by pointing out that salvation was by faith alone apart from deeds of the law (Galatians 2:16; Romans 3:21-22). Paul also taught that saving faith is not dead but alive, showing thanks to God in deeds of love (Galatians 5:6 ['...since in Christ Jesus it is not being circumcised or being uncircumcised that can effect anything - only faith working through love.']). James was dealing with errorists who said that if they had faith they didn't need to show love by a life of faith (James 2:14-17). James countered this error by teaching that faith is alive, showing itself to be so by deeds of love (James 2:18,26). James and Paul both teach that salvation is by faith alone and also that faith is never alone but shows itself to be alive by deeds of love that express a believer's thanks to God for the free gift of salvation by faith in Jesus. 
Anointing of the Sick.
James's epistle is also the chief Biblical text for the Anointing of the Sick. James wrote:
14Is anyone among you sick? Let him call for the elders of the church, and let them pray over him, anointing him with oil in the name of the Lord. 15And the prayer of faith will save the one who is sick, and the Lord will raise him up. And if he has committed sins, he will be forgiven.
G. A. Wells suggested this passage was evidence of late authorship of the epistle, on the grounds that the healing of the sick being done through an official body of presbyters (elders) indicated a considerable development of ecclesiastical organisation, "whereas in Paul's day to heal and work miracles pertained to believers indiscriminately (I Corinthians, XII:9)."
Canonicity.
The Epistle was first explicitly referred to and quoted by Origen of Alexandria, and possibly a bit earlier by Irenaeus of Lyons as well as Clement of Alexandria in a lost work according to Eusebius, although it was not mentioned by Tertullian, who was writing at the end of the Second Century. It is also absent from the Muratorian fragment, the earliest known list of New Testament books.
The Epistle of James was included among the twenty-seven New Testament books first listed by Athanasius of Alexandria in his "Thirty-Ninth Festal Epistle" (AD 367) and was confirmed as a canonical epistle of the New Testament by a series of councils in the Fourth Century. Today, virtually all denominations of Christianity consider this book to be a canonical epistle of the New Testament.
In the first centuries of the Church the authenticity of the Epistle was doubted by some, including Theodore, Bishop of Mopsuestia in Cilicia. Because of the silence of several of the western churches regarding it, Eusebius classes it among the Antilegomena or contested writings ("Historia ecclesiae", 3.25; 2.23). St. Jerome gives a similar appraisal but adds that with time it had been universally admitted. Gaius Marius Victorinus, in his commentary on the Epistle to the Galatians, openly questioned whether the teachings of James were heretical.
Its late recognition in the Church, especially in the West, may be explained by the fact that it was written for or by Jewish Christians, and therefore not widely circulated among the Gentile Churches. There is some indication that a few groups distrusted the book because of its doctrine. In Reformation times a few theologians, most notably Martin Luther in his early career, argued that this epistle should not be part of the canonical New Testament.
Martin Luther's description of the Epistle of James changes. In some cases, Luther argues that it was not written by an apostle; but in other cases, he describes James as the work of an apostle. He even cites it as authoritative teaching from God and describes James as "a good book, because it sets up no doctrines of men but "vigorously promulgates the law of God"." Lutherans hold that the Epistle is rightly part of the New Testament, citing its authority in the Book of Concord, however it remains part of the Lutheran antilegomena.

</doc>
<doc id="9689" url="http://en.wikipedia.org/wiki?curid=9689" title="Epistle of Jude">
Epistle of Jude

The Epistle of Jude, often shortened to Jude, is the penultimate book of the New Testament and is attributed to Jude, the brother of James the Just. 
Composition.
The letter of Jude was one of the disputed books of the Canon. Although its canonical status was contested, its authenticity was never doubted by the Early Church. The links between the Epistle and 2 Peter, its use of the Apocryphal Books, and its brevity raised concern. It is one of the shortest books in the Bible, being only 25 verses long.
Content.
Jude urges his readers to defend the deposit of Christ's doctrine that had been closed by the time he wrote his epistle, and to remember the words of the apostles spoken somewhat before. He uses language similar to the second epistle of Peter to answer concerns that the Lord seemed to tarry, "How that they told you there should be mockers in the last time, who should walk after their own ungodly lusts..."
Jude then asks the reader to recall how even after the Lord saved his people out of the land of Egypt, he did not hesitate to destroy those who fell into unbelief, much as he punished the angels who fell from their original exalted status.
Jude quotes directly from the Book of Enoch, part of the scripture of the Ethiopian and Eritrean churches but rejected by other churches. He cites Enoch's prophecy that the Lord would come with many thousands of his saints to render judgement on the whole world. He also paraphrases an incident in a text that has been lost about Satan and Michael quarrelling over the body of Moses.
Outline.
I. Salutation (1-2)
II. Occasion for the Letter (3-4)
 A. The change of Subject (3)
 B. The Reason for the Change: The Presence of Godless Apostates (4)
III. Warning against the False Teachers (5-16)
 A. Historical Examples of the Judgement of Apostates (5-7)
 1. Unbelieving Israel (5)
 2. Angels who fell (6)
 3. Sodom and Gomorrah (7)
 B. Description of the Apostates of Jude's Day (8-16)
 1. Their slanderous speech deplored (8-10)
 2. Their character graphically portrayed (11-13)
 3. Their destruction prophesied (14-16)
IV. Exhortation to Believers (17-23)
V. Concluding Doxology (24-25)
Canonical status.
The Epistle of Jude is held as canonical in the Christian Church. Although some scholars consider the letter a pseudonymous work written between the end of the 1st century and the first quarter of the 2nd century, arguing from the references to the apostles, tradition; and the book's competent Greek style, conservative scholars date it between 66 to 90.
"More remarkable is the evidence that by the end of the second century Jude was widely accepted as canonical." Clement of Alexandria, Tertullian and the Muratorian canon considered the letter canonical. The first historical record of doubts as to authorship are found in the writings of Origen of Alexandria, who spoke of the doubts held by some—albeit not him. Eusebius classified it with the "disputed writings, the "antilegomena."" The letter was eventually accepted as part of the Canon by the Church Fathers such as Athanasius and the Synods of Laodicea (c. 363) and Carthage (397).
Authorship.
The Epistle title is written as follows: "Jude, a servant of Jesus Christ and brother of James" (NRSV). There is a dispute as to whether "brother" means someone who has the same father and mother, or a half-brother or cousin or more distant familial relationship. This dispute over the true meaning of "brother" grew as the doctrine of the Virgin Birth evolved.
The debate has continued over the author's identity as the apostle, the brother of Jesus, both, or neither. Some scholars have argued that since the author of that letter has not identified himself as an apostle and actually refers to the apostles as a third party, he cannot be identified with the Jude who is listed as one of the Twelve. Others have drawn the opposite conclusion, i.e., that as an apostle, he would not have made such a claim on his own behalf. The many Judes, named in the gospels and among the relatives of Jesus, and his relationship to James the Just called the brother of Jesus has caused much confusion. Not a lot is known of Jude, which would explain the apparent need to identify him by reference to his better-known brother. It is agreed that he is not the Jude who betrayed Jesus, Judas Iscariot.
Style.
The "Epistle of Jude" is a brief book of only a single chapter with 25 verses. It was composed as an "encyclical letter"—that is, one not directed to the members of one church in particular, but intended rather to be circulated and read in all churches. The form, as opposed to the earlier letters of Paul, suggests that the author knew Paul's "Epistle to the Ephesians" or even that the Pauline epistles had already been collected and were circulating when the text was written.
The wording and syntax of this epistle in its original Greek demonstrates that the author was capable and fluent. The epistle is addressed to Christians in general, and it warns them about the doctrine of certain errant teachers to whom they were exposed. Examples of heterodox opinions that were circulating in the early 2nd century include Docetism, Marcionism, and Gnosticism.
The epistle's style is combative, impassioned, and rushed. Many examples of evildoers and warnings about their fates are given in rapid succession. The epithets contained in this writing are considered to be some of the strongest found in the New Testament.
The epistle concludes with a doxology, which is considered to be one of the highest in quality contained in the Bible. 
Jude and 2 Peter.
Part of Jude is very similar to 2 Peter (mainly 2 Peter chapter 2), so much so that most scholars agree that there is a dependence between the two; that either one letter used the other directly, or they both drew on a common source.
Because this epistle is much shorter than 2 Peter, and due to various stylistic details, some writers consider that Jude was the source for the similar passages of 2 Peter. However other writers, noting that Jude 18 quotes 2 Peter 3:3 as past tense, consider that Jude came after 2 Peter.
Some scholars who consider Jude to predate 2 Peter note that the latter appears to quote the former but excises the reference to the non-canonical Enoch.
References to other books.
The Epistle of Jude references at least two other books, with one being non-canonical in all churches and the other non-canonical in most churches.
The Book of Enoch is not considered canonical by most churches, although it is by the Ethiopian Orthodox church. According to Western scholars the older sections of the Book of Enoch (mainly in the "Book of the Watchers") date from about 300 BC and the latest part ("Book of Parables") probably was composed at the end of 1st century BC. It is generally accepted by scholars that the author of the Epistle of Jude was familiar with the Book of Enoch and was influenced by it in thought and diction. Jude 1:14–15 quotes 1Enoch 1:9 which is part of the pseudepigrapha and is among the Dead Sea Scrolls [4Q Enoch (4Q204[4QENAR]) COL I 16–18].
External links.
Online translations of the Epistle of Jude:
Additional information:

</doc>
<doc id="9692" url="http://en.wikipedia.org/wiki?curid=9692" title="Eusebius Amort">
Eusebius Amort

Eusebius Amort (November 15, 1692 – February 5, 1775) was a German Roman Catholic theologian.
Life.
Amort was born at Bibermuhle, near Tolz, in Upper Bavaria. He studied at Munich, and at an early age joined the Canons Regular at Polling, where, shortly after his ordination in 1717, he taught theology and philosophy.
The Parnassus Boicus learned society was based on a plan started in 1720 by three Augustian fathers. Eusebius Amort, Gelasius Hieber (1671-1731), a famous preacher in the German language and Agnellus Kandler (1692-1745), a genealogist and librarian. The initial plans fell through, but in 1722 they issued the first number of the "Parnassus Boicus" journal, communicating interesting information from the arts and sciences.
In 1733 Amort went to Rome as theologian to Cardinal Niccolo Maria Lercari (d. 1757).
He returned to Polling in 1735 and devoted the rest of his life to the revival of learning in Bavaria. He died at Polling in 1775.
Works.
Amort, who had the reputation of being the most learned man of his age, was a voluminous writer on every conceivable subject, from poetry to astronomy, from dogmatic theology to mysticism. His best known works are:
The list of his other works, including his three erudite contributions to the question of authorship of the "Imitatio Christi", will be found in C. Toussaint's scholarly article in Alfred Vacant's "Dictionnaire de theologie" (1900, cols 1115-1117).
References.
Citations
Sources
</dl>

</doc>
<doc id="9693" url="http://en.wikipedia.org/wiki?curid=9693" title="Episcopi vagantes">
Episcopi vagantes

Episcopi vagantes (singular: episcopus vagans, Latin for wandering bishops or stray bishops) are those persons consecrated, in a "clandestine or irregular way," as Christian bishops outside the structures and canon law of the established churches; those regularly consecrated but later excommunicated, and not in communion with any generally recognized diocese; and those who have in communion with them small groups that appear to exist solely for the bishop's sake.(pp1–2) David V. Barrett, in "Encyclopedia of new religious movements", specifies that now "episcopi vagantes" are "those independent bishops who collect several different lines of transmission of apostolic succession, and who will happily (and sometimes for a fee) consecrate anyone who requests it." Those described as wandering bishops often see the term as pejorative. The general term for "wandering" clerics, as were common in the Middle Ages, is "clerici vagantes"; the general term for those recognising no leader is "acephali".
The "Oxford Dictionary of the Christian Church" mentions as the main lines of succession deriving from "episcopi vagantes" in the 20th century those founded by Arnold Mathew, Joseph René Vilatte, and Leon Chechemian. Others that could be added are those derived from Aftimios Ofiesh, Carlos Duarte Costa, Emmanuel Milingo, and Pierre Martin Ngô Đình Thục.
Theological issues.
In Western Christianity it has traditionally been taught, since as far back as the time of the Donatist controversy of the fourth and fifth centuries, that any bishop can consecrate any other baptised man as a bishop provided that the bishop observes the minimum requirements for the sacramental validity of the ceremony. This means that the consecration is considered valid even if it flouts certain ecclesiastical laws, and even if the participants are schismatics or heretics.
According to a theological view affirmed, for instance, by the International Bishops' Conference of the Old Catholic Church with regard to ordinations by Arnold Mathew, an episcopal ordination is for service within a specific Christian church, and an ordination ceremony that concerns only the individual himself does not make him truly a bishop. The Holy See has not commented on the validity of this theory, but has declared with regard to ordinations of this kind carried out, for example, by Emmanuel Milingo, that the Church "does not recognize and does not intend to recognize in the future those ordinations or any of the ordinations derived from them and therefore the canonical state of the alleged bishops remains that in which they were before the ordination conferred by Mr Milingo". Other theologians also, notably those of the Eastern Orthodox Church, dispute the notion that such ordinations have effect, a notion that opens up the possibility of valid but irregular consecrations proliferating outside the structures of the "official" denominations.
A Catholic ordained to the episcopacy without a mandate from the Pope is automatically excommunicated and is thereby forbidden to celebrate the sacraments.
Eastern Orthodox.
Vlassios Pheidas, on an official Church of Greece site, uses the canonical language of the Orthodox tradition, to describe the conditions in ecclesial praxis when sacraments, including Holy Orders, are real, valid, and efficacious. He notes language is itself part of the ecclesiological problem.(ch. 1)
If [...] divine grace is granted to all, [...] then it stands to reason that it is bestowed also in those believers outside the Roman-Catholic Church, even if such persons belong to a heresy of schism. Thus, the sacraments performed outside the Church are not only real (υποστατά), but also valid (έγκυρα), because they only lack the efficacy (ενέργεια) of the bestowed divine grace, which is operative through the Holy Spirit only within the Roman-Catholic Church.
Through such a teaching [...] one finds himself face to face with the [...] principle of "extra Ecclesia nulla salus", which strictly determines the canonical limits of the Church. Thus, the Orthodox Church, while accepting the canonical possibility of recognising the existence (υποστατόν) of sacraments performed outside herself, it questions their validity (έγκυρον) and certainly rejects their efficacy (ενεργόν). It is already well-known that in the ecclesial praxis, the Orthodox Church moves, according to the specific circumstances, between canonical "acribeia" and ecclesial economy, recognising by economy the validity (κύρος) of the sacraments of those ecclesiastical bodies. Yet, such a practice of economy does not overthrow the canonical "acribeia", which also remains in force and expresses the exclusive character of orthodox ecclesiology.
This observation is really important, because it reveals that the canonical recognition (αναγνώρισις) of the validity of sacraments performed outside the Orthodox Church: (a) is done by economy, (b) covers only specific cases in certain given instances, and (c) refers to the validity of the sacraments only of those who join the Orthodox Church, and not of the ecclesiastical bodies to which belong those who join the Orthodox Church. There is, [...] a variety of opinions or reservations concerning this question. No one, [...] could propose or support the view that the mutual recognition of the validity of sacraments among the Churches is an ecclesiastical act consistent with orthodox ecclesiology, or an act which is not rejected by the orthodox canonical tradition. [...]
[...] the mutual recognition of the validity of certain sacraments, [...] is for an Orthodox an act of inconsistency, when it is assessed with orthodox ecclesiological principles. These ecclesiological principles manifest in a strict fashion the organic unity of the orthodox ecclesial body and differentiate those who do not belong to its body as either schismatics or heretics.
The relation of schismatics or heretics to the body of the Orthodox Church is strictly defined by the canonical tradition. However, orthodox canonical tradition and praxis appraises and classifies these ecclesiastical bodies into various categories, [...] in which some form of ecclesiality is recognised. This type of ecclesiality is not easily determined, because the orthodox tradition [...] does not recognise the efficacy of the divine grace outside the canonical boundaries of the Orthodox Church.(ch. 2)
This applies to the validity and efficacy of the ordination of bishops and the other sacraments, not only of the Independent Catholic Churches, but also of all other Christian churches, including the Roman Catholic Church, Oriental Orthodoxy and the Assyrian Church of the East.
Anglican.
Anglican bishop Colin Buchanan, in "Historical Dictionary of Anglicanism", says that the Anglican Communion has held an Augustinian view of orders, by which "the validity of Episcopal ordinations (to whichever order) is based solely upon the historic succession in which the ordaining bishop stands, irrespective of their contemporary ecclesial context."
He describes the circumstances of Archbishop Matthew Parker's consecration as one of the reasons why this theory is "generally held". Parker was chosen by Queen Elizabeth I of England to be the first Church of England Archbishop of Canterbury after the death of the previous office holder, Cardinal Reginald Pole, the last Roman Catholic Archbishop of Canterbury. Buchanan notes the Roman Catholic Church also focuses on issues of intention and not just breaks in historical succession. He does not explain whether intention has an ecclesiological role, for Anglicans, in conferring or receiving sacraments.
History.
According to Buchanan, "the real rise of the problem" happened on the 19th century, in the "wake of the Anglo-Catholic movement", "through mischievous activities of a tiny number of independently acting bishops". They exist worldwide, he writes, "mostly without congregations", and "many in different stages of delusion and fantasy, not least in the Episcopal titles they confer on themselves"; "the distinguishing mark" to "specifically identif[y]" an "episcopus vagans" is "the lack of a true see or the lack of a real church life to oversee". Paul Halsall, on the Internet History Sourcebooks Project, did not list a single church edifice of independent bishops, in a 1996–1998 New York City building architecture survey of religious communities, which maintain bishops claiming apostolic succession and claim cathedral status but noted there "are now literally hundreds of these 'episcopi vagantes', of lesser or greater spiritual probity. They seem to have a tendency to call living room sanctuaries 'cathedrals';" those buildings were not perceived as cultural symbols and did not meet the survey criteria. David V. Barrett wrote, in "A brief guide to secret religions", that "one hallmark of such bishops is that they often collect as many lineages as they can to strengthen their Episcopal legitimacy—at least in their own eyes" and their groups have more clergy than members.
Many "episcopi vagantes" claim succession from the Old Catholic See of Utrecht, or from Eastern Orthodox, Oriental Orthodox, or Eastern Catholic Churches. A few others derive their orders from Roman Catholic bishops who have consecrated their own bishops after disputes with the Holy See.
Barrett wrote that leaders "of some esoteric movements, are also priests or bishops in small non-mainstream Christian Churches"; he explains, this type of "independent or autocephalous" group has "little in common with the Church it developed from, the Old Catholic Church, and even less in common with the Roman Catholic Church" but still claims its authority from Apostolic succession.(p56)
Many, if not most, "episcopi vagantes" are associated with Independent Catholic Churches. They may be very liberal or very conservative. "Episcopi vagantes" may also include some conservative "Continuing Anglicans" who have broken with the Anglican Communion over various issues such as Prayer Book revision, the ordination of women and the ordination of unmarried, non-celibate individuals (including homosexuals).
Buchanan writes that based the criteria of having "a true see" or having "a real church life to oversee", the bishops of most forms of Continuing Anglican movement are not necessarily classified as vagantes, but "are always in danger of becoming such".
Particular consecrations.
Mathew, according to Buchanan, "lapsed into the vagaries of an "episcopus vagans""(p335) Stephen Edmonds, in "Oxford Dictionary of National Biography", wrote that in 1910 Mathew's wife separated from him; that same year, he declared himself and his church seceded from the Union of Utrecht. Within a few months, on none }}, he was excommunicated by the Roman Catholic Church; sued "The Times" for libel based on the words "pseudo-bishop" used to describe him in the newspaper's translation from the Latin text ""pseudo-episcopus""; and, lost his case in 1913. Henry R.T. Brandreth wrote, in "Episcopi Vagantes and the Anglican Church", "[o]ne of the most regrettable features of Mathew's episcopate was the founding of the Order of Corporate Reunion in 1908. This claimed to be a revival of Frederick George Lee's movement, but was in fact unconnected with it." Brandreth thought it "seems still to exist in a shadowy underground way" in 1947, but disconnected.(p18) Colin Holden, in "Ritualist on a Tricycle", places Mathew and his into perspective, he wrote Mathew was an "episcopus vagans", lived in a cottage provided for him, and performed his conditional acts, sometimes called according to Holden "bedroom ordinations", in his cottage. Mathew questioned the validity of Anglican ordinations and became involved with the , in 1911 according to Edmonds, and he openly advertised his offer to reordain Anglican clergy who requested it. This angered the Church of England. In 1912, D. J. Scannell O'Neill wrote in "The Fortnightly Review" that London "seems to have more than her due share of bishops" and enumerates what he refers to as "these hireling shepherds". He also announces that one of them, Mathew, revived the and published "The Torch", a monthly review, advocating the reconstruction of Western Christianity and reunion with Eastern Christianity. "The Torch" stated "that the ordinations of the Church of England are not recognized by any church claiming to be Catholic" so the promoters involved Mathew to conditionally ordain group members who are "clergy of the Established Church" and "sign a profession of the Catholic Faith". It stipulated Mathew's services were not a system of simony and given without simoniac expectations. The group sought to enroll "earnest-minded Catholics who sincerely desire to help forward the work of [c]orporate [r]eunion with the Holy See". Nigel Yates, in "Anglican Ritualism in Victorian Britain, 1830-1910", described it as "an even more bizarre scheme to promote a Catholic Uniate Church in Britain" than Lee and Ambrose Lisle March Phillipps de Lisle's "Association for the Promotion of the Unity of Christendom". It was editorialized by O'Neill as the "most charitable construction to be placed on this latest move of Mathew is that he is not mentally sound. Being an Irishman, it is strange that he has not sufficient humor to see the absurdity of falling away from the Catholic Church in order to assist others to unite with the Holy See." Edmonds reports that "anything between 4 and 265 was suggested" as to how many took up his offer of reordination.
When it declared devoid of canonical effect the consecration ceremony conducted by Archbishop Pierre Martin Ngô Đình Thục for the Carmelite Order of the Holy Face group at midnight of 31 December 1975, the Holy See refrained from pronouncing on its validity. It made the same statement with regard also to later ordinations by those bishops, saying that, "as for those who have already thus unlawfully received ordination or any who may yet accept ordination from these, whatever may be the validity of the orders ("quidquid sit de ordinum validitate"), the Church does not and will not recognise their ordination ("ipsorum ordinationem"), and will consider them, for all legal effects, as still in the state in which they were before, except that the ... penalties remain until they repent".
A similar declaration was issued with regard to Archbishop Emmanuel Milingo's conferring of episcopal ordination on four men - all of whom, by virtue of previous Independent Catholic consecrations, claimed already to be bishops - on 24 September 2006: the Holy See, as well as stating that, in accordance with Canon 1382 of the Code of Canon Law, all five men involved incurred automatic ("latae sententiae") excommunication through their actions, declared that "the Church does not recognise and does not intend in the future to recognise these ordinations or any ordinations derived from them, and she holds that the canonical state of the four alleged bishops is the same as it was prior to the ordination."
In contrast, the Holy See has not questioned the validity of the consecrations that the late Archbishop Marcel Lefebvre performed in 1988 for the service of the relatively numerous followers of the Traditionalist Catholic Society of St. Pius X that he had founded, and of the bishops who, under pressure from the Chinese Catholic Patriotic Association, "have been ordained without the Pontifical mandate and who have not asked for, or have not yet obtained, the necessary legitimation", and who consequently, Pope Benedict XVI declared, "are to be considered illegitimate, but validly ordained".
Use as cultural reference.
Victor LaValle, in the novel "Big Machine" (2009), included three "episcopi vagantes" as part of his character's childhood involvement with an independent church:
Your church is broken! The Washerwomen are here to rebuild!
The Washerwomen didn't proselytize to people of other religions, or those without beliefs. [...]
Gina, Karen, and Rose called themselves "priests without a parish." Local clergy discussed the Washerwomen the way you discuss a calf born with two heads. [...] The cult on Colden Street. Once, a visiting Anglican priest even gave the sisters a nickname: "episcopi vagantes", the "wandering bishops."
It was a title the sisters liked (the English, not the Latin). They embraced it, [...] Even had fun with it. [...] They seemed to shine like beasts of prophecy, their vitality more persuasive than any words.
That's why we believed.
Calvin Baker, in his novel "Dominion" (2006), includes an "episcopus vagans" as one of his characters:
The tent was silent as they listened, for he spoke with intense care for his words, but also with a strange accent.
"I am what is known as an Episcopi Vagantes, which means I have been fully invested with the sacraments of the one original church. I received my ordination first as a priest, while still in my youth, and raised still young to bishop—I was twenty-six at the time—by no less a vassal of God than the Pope of Antioch."
"None can undo [...] so I remain now a high bishop but have had an argument with the other bishops on your behalf."
The residents of the town were baffled by much that the preacher was saying, but he put up such a show with that great purple robe [...], that they decided to let him finish his sermon before making up their minds.
"Today I wish to read to you from one of the Hidden Books of Christ, which the popes and high bishops have all conspired [...] to keep you from knowing, [...]"
Intertextuality of "episcopi vagantes" language. Jim Higgins saw, in "More Years for the Locust", "similarities between Marxist obscurantism and an addiction to Christian arcana" and used "episcopi vagantes" pejoratively as his example of "the ever-growing proliferation of sects, sectlets and insects claiming direct descent from the master" with "fissiparous tendencies". He saw humor in the ludicrous characters and farce in their titles.No sooner had the apostolic hands graced [Arnold] Mathew's head than he was off forming his own church. There is nothing like a Bishop's mitre and crozier to make a chap look posh and become the object of envious glances from other would-be Bishops. Where one man has ventured others will surely follow, if not always by the same route. JR Vilatte and [Ulric] Vernon Herford were made Bishops by the Nestorian Church of the Malabar Coast. Thus it was that the good work continued: the new bishops built their churches and, in time, felt the need for additional bishops. Need being father to the deed, they laid their hands on suitable candidates, who often, in their turn, developed doctrinal differences which necessitated them breaking away to form their own church. With each split there was a new accretion of theological exotica. One vagrant bishop blended Catholicism with theosophy and built his cathedral around a massive brass funnel through which God sent down beneficent rays to the faithful, who stood underneath the blessed metal conduit to receive them. Another, perhaps unsure of the effectiveness of one ceremony, was consecrated on numerous occasions in various vagrant churches and when last heard of was styled Mar Georgius, Patriarch of Glastonbury, the Episcopate of the West, and his subsidiary titles covered ten full lines of 12 point type. Among this small but sparky firmament, one with real star quality was the French "Bishop" who combined catholicism with druidism. He conducted baptism, weather permitting, in the sea off the Normandy coast. This splendid chap styled himself, "His Whiteness the Humble Tugdual the Second". May his God preserve him from pneumonia. The most recent count, in 1961, of the number of such "Bishops" was over 200 and I sincerely hope that Tugdual II, who was one of them, is still with us.

</doc>
<doc id="9695" url="http://en.wikipedia.org/wiki?curid=9695" title="Elizabeth Garrett Anderson">
Elizabeth Garrett Anderson

Elizabeth Garrett Anderson, LSA, MD (9 June 1836 – 17 December 1917), was an English physician and feminist, the first Englishwoman to qualify as a physician and surgeon in Britain, the co-founder of the first hospital staffed by women, the first dean of a British medical school, the first female doctor of medicine in France, the first woman in Britain to be elected to a school board and, as Mayor of Aldeburgh, the first female mayor and magistrate in Britain.
Early life.
Elizabeth Garrett was born on 9 June 1836 in Whitechapel, London, the second of eleven children of Newson Garrett (1812–1893), from Leiston, Suffolk, and his wife, Louisa née Dunnell (1813–1903), from London.
The Garrett ancestors had been ironworkers in East Suffolk since the early seventeenth century. Newson was the youngest of three sons and not academically inclined, although he possessed the family’s entrepreneurial spirit. When he finished school, the town of Leiston offered little to Newson, so he left for London to make his fortune. There, he fell in love with his brother's sister-in-law, Louisa Dunnell, the daughter of an innkeeper of Suffolk origin. After their wedding, the couple went to live in a pawnbroker's shop at 1 Commercial Road, Whitechapel. The Garretts had their first three children in quick succession: Louie, Elizabeth and their brother (Newson Dunnell) who died at the age of six months. While Louisa grieved the loss of her third child, it was not easy to raise their two daughters in the London of that time. When Garrett was 3 years old, the family moved to 142 Long Acre, where they were to live for 2 years, whilst two more children were born and her father moved up in the world, becoming not only the manager of a larger pawnbroker's shop, but also a silversmith. Garrett's grandfather, owner of the family engineering works, Richard Garrett & Sons, had died in 1837, leaving the business to his eldest son, Garrett's uncle. Despite his lack of capital, Newson was determined to be successful and in 1841, at the age of 29, he moved his family to Suffolk, where he bought a barley and coal merchants business in Snape, constructing Snape Maltings, a fine range of buildings for malting barley.
The Garretts lived in a square Georgian house opposite the church in Aldeburgh until 1852. Newson's malting business expanded and five more children were born, Alice (1842), Millicent (1847), who was to become a leader in the constitutional campaign for women's suffrage, Sam (1850), Josephine (1853) and George (1854). By 1850, Newson was a prosperous businessman and was able to build Alde House, a mansion on a hill behind Aldeburgh. A “by-product of the industrial revolution”, Garrett grew up in an atmosphere of “triumphant economic pioneering” and the Garrett children were to grow up to become achievers in the professional classes of late-Victorian England. Garrett was encouraged to take an interest in local politics and, contrary to practices at the time, was allowed the freedom to explore the town with its nearby salt-marshes, beach and the small port of Slaughden with its boatbuilders' yards and sailmakers' lofts. 
Early education.
There was no school in Aldeburgh so Garrett learned the three Rs from her mother. When she was 10 years old, a governess, Miss Edgeworth, a poor gentlewoman, was employed to educate Garrett and her sister. Mornings were spent in the schoolroom; there were regimental afternoon walks; educating the young ladies continued at mealtimes when Edgeworth ate with the family; at night, the governess slept in a curtained off area in the girls' bedroom. Garrett despised her governess and sought to outwit the teacher in the classroom. When Garrett was 13 and her sister 15, they were sent to a private school, the Boarding School for Ladies in Blackheath, London, which was run by the step aunts of the poet Robert Browning. There, English literature, French, Italian and German as well as deportment, were taught. 
Later in life, Garrett recalled the stupidity of her teachers there, though her schooling there did help establish a love of reading. Her main complaint about the school was the lack of science and mathematics instruction. Her reading matter included Tennyson, Wordsworth, Milton, Coleridge, Trollope, Thackeray and George Eliot. Elizabeth and Louie were known as “the bathing Garretts”, as their father had insisted they be allowed a hot bath once a week. However, they made what were to be lifelong friends there. When they finished in 1851, they were sent on a short tour abroad, ending with a memorable visit to the Great Exhibition in Hyde Park, London.
After this formal education, Garrett spent the next nine years tending to domestic duties, but she continued to study Latin and arithmetic in the mornings and also read widely. Her sister Millicent recalled Garrett's weekly lectures, “Talks on Things in General”, when her younger siblings would gather her while she discussed politics and current affairs from Garibaldi to Macaulay's "History of England". In 1854, when she was eighteen, Garrett and her sister went on a long visit to their school friends, Jane and Anne Crow, in Gateshead where she met Emily Davies, the early feminist and future co-founder of Girton College, Cambridge. Davies was to be a lifelong friend and confidante, always ready to give sound advice during the important decisions of Garrett’s career. It may have been in the "English Woman's Journal", first issued in 1858, that Garrett first read of Elizabeth Blackwell, who had become the first female doctor in the United States in 1849. When Blackwell visited London in 1859, Garrett travelled to the capital. By then, her sister Louie was married and living in London. Garrett joined the Society for Promoting the Employment of Women, which organised Blackwell's lectures on "Medicine as a Profession for Ladies" and set up a private meeting between Garrett and the doctor. It is said that during a visit to Alde House around 1860, one evening while sitting by the fireside, Garrett and Davies selected careers for advancing the frontiers of women's rights; Garrett was to open the medical profession to women, Davies the doors to a university education for women, while 13-year-old Millicent was allocated politics and votes for women. At first Newson was opposed to the radical idea of his daughter becoming a physician but came round and agreed to do all in his power, both financially and otherwise, to support Garrett.
Medical education.
After an initial unsuccessful visit to leading doctors in Harley Street, Garrett decided to first spend six months as a surgery nurse at Middlesex Hospital, London in August 1860. On proving to be a good nurse, she was allowed to attend an outpatients' clinic, then her first operation. She unsuccessfully attempted to enroll in the hospital's Medical School but was allowed to attend private tuition in Latin, Greek and "materia medica" with the hospital's apothecary, while continuing her work as a nurse. She also employed a tutor to study anatomy and physiology three evenings a week. Eventually she was allowed into the dissecting room and the chemistry lectures. Gradually, Garrett became an unwelcome presence among the male students, who in 1861 presented a memorial to the school against her admittance as a fellow student, despite the support she enjoyed from the administration. She was obliged to leave the Middlesex Hospital but she did so with an honours certificate in chemistry and "materia medica". Garrett then applied to several medical schools, including Oxford, Cambridge, Glasgow, Edinburgh, St Andrews and the Royal College of Surgeons, all of which refused her admittance. 
A companion to her in this struggle was the lesser known Dr. Sophia Jex-Blake. Whilst both are considered "outstanding" medical figures of the late 19th century, Anderson was able to obtain her credentials by way of a "side door" through a loophole in admissions at the Society of Apothecaries. 
Having privately obtained a certificate in anatomy and physiology and in 1862, she was finally admitted by the Society of Apothecaries who, as a condition of their charter, could not legally exclude her on account of her sex. She continued her battle to qualify by studying privately with various professors, including some at the University of St Andrews, the Edinburgh Royal Maternity and the London Hospital Medical School. 
In 1865, she finally took her exam and obtained a licence (LSA) from the Society of Apothecaries to practise medicine, the first woman qualified in Britain to do so (previously there was Dr James Barry who was assigned a female gender at birth but lived his adult life as a man). On the day, three out of seven candidates passed the exam, Garrett with the highest marks. The Society of Apothecaries immediately amended its regulations to prevent other women obtaining a licence meaning that Jex-Blake however could not follow this same path; the new rule disallowed privately educated women to be eligible for examination.
Career.
Though she was now a licentiate of the Society of Apothecaries, as a woman, Garrett could not take up a medical post in any hospital. So in late 1865, Garrett opened her own practice at 20 Upper Berkeley Street, London. At first, patients were scarce but the practice gradually grew. After six months in practice, she wished to open an outpatients dispensary, to enable poor women to obtain medical help from a qualified practitioner of their own gender. In 1865, there was outbreak of cholera in Britain, affecting both rich and poor, and in their panic, some people forgot any prejudices they had in relation to a female physician. The first death due to cholera occurred in 1866, but by then Garrett had already opened St Mary's Dispensary for Women and Children, at 69 Seymour Place. In the first year, she tended to 3,000 new patients, who made 9,300 outpatient visits to the dispensary. On hearing that the Dean of the faculty of medicine at the University of Sorbonne, Paris was in favour of admitting women as medical students, Garrett studied French so that she could apply for a medical degree, which she obtained in 1870 after some difficulty.
The same year she was elected to the first London School Board, an office newly opened to women; Garrett's was the highest vote among all the candidates. Also in that year, she was made one of the visiting physicians of the East London Hospital for Children, becoming the first woman in Britain to be appointed to a medical post, but she found the duties of these two positions to be incompatible with her principal work in her private practice and the dispensary, as well as her role as a new mother, so she resigned from these posts by 1873. In 1872, the dispensary became the New Hospital for Women and Children, treating women from all over London for gynaecological conditions; the hospital moved to new premises in Marylebone Street in 1874. Around this time, Garrett also entered into discussion with male medical views regarding women. In 1874, Henry Maudsley’s article on Sex and Mind in Education appeared, which argued that education for women caused over-exertion and thus reduced their reproductive capacity, sometimes causing “nervous and even mental disorders”. Garrett’s counter-argument was that the real danger for women was not education but boredom and that fresh air and exercise were preferable to sitting by the fire with a novel. In the same year, she co-founded London School of Medicine for Women with Sophia Jex-Blake and became a lecturer in what was the only teaching hospital in Britain to offer courses for women. She continued to work there for the rest of her career and was dean of the school from 1883 to 1902. This school was later called the Royal Free Hospital of Medicine, which later became part of what is now the medical school of University College London.
BMA membership.
In 1873 she gained membership of the British Medical Association and remained the only female member for 19 years, due to the Association's vote against the admission of further women – "one of several instances where Garrett, uniquely, was able to enter a hitherto all male medical institution which subsequently moved formally to exclude any women who might seek to follow her." In 1897, Garrett Anderson was elected president of the East Anglian branch of the British Medical Association.
Garrett Anderson worked steadily at the development of the New Hospital for Women, and (from 1874) at the creation of the London School of Medicine for Women, where she served as its dean. Both institutions have since been handsomely and suitably housed and equipped, the New Hospital for Women (in the Euston Road) for many years being worked entirely by medical women, and the schools (in Hunter Street, WC1) having over 200 students, most of them preparing for the medical degree of London University (the present-day University College London), which was opened to women in 1877.
On 9 November 1908, she was elected mayor of Aldeburgh, the first female mayor in England. Her father was mayor in 1889. 
She died in 1917 and is buried in the churchyard of St Peter and St Paul's Church, Aldeburgh.
Women’s Suffrage Movement.
Garrett Anderson was also active in the women's suffrage movement. In 1866, Garrett Anderson and Davies presented petitions signed by more than 1,500 asking that female heads of household be given the vote. That year, Garrett Anderson joined the first British Women's Suffrage Committee. She was not as active as her sister, Millicent Garrett Fawcett, though Garrett Anderson became a member of the Central Committee of the National Society for Women's Suffrage in 1889. After her husband's death in 1907, she became more active. As mayor of Aldeburgh, she gave speeches for suffrage, before the increasing militant activity in the movement led to her withdrawal in 1911. Her daughter Louisa, also a physician, was more active and more militant, spending time in prison in 1912 for her suffrage activities.
Personal life.
Elizabeth Garrett Anderson once remarked that “a doctor leads two lives, the professional and the private, and the boundaries between the two are never traversed”. In 1871, she married James George Skelton Anderson (d. 1907) of the Orient Steamship Company co-owned by his uncle Arthur Anderson, but she did not give up her medical practice. She had three children, Louisa (1873–1943), Margaret (1874–1875), who died of meningitis, and Alan (1877–1952). Louisa also became a pioneering doctor of medicine and feminist activist. 
They retired to Aldeburgh in 1902, moving to Alde House in 1903, after the death of Elizabeth’s mother. Skelton died of stroke in 1907. She enjoyed a happy marriage and in later life, devoted time to Alde House, gardening, and travelling with younger members of the extended family. 
Legacy.
The New Hospital for Women was renamed the Elizabeth Garrett Anderson Hospital in 1918 and amalgamated with the Obstetric Hospital in 2001 to form the Elizabeth Garrett Anderson and Obstetric Hospital before relocating to become the University College Hospital Elizabeth Garrett Anderson Wing at UCH.
The former Elizabeth Garrett Anderson Hospital buildings are incorporated into the new National Headquarters for the public service trade union UNISON. The Elizabeth Garrett Anderson Gallery, a permanent installation set within the restored hospital building, uses a variety of media to set the story of Garrett Anderson, her hospital, and women's struggle to achieve equality in the field of medicine within the wider framework of 19th and 20th century social history.
There is a secondary school for girls in Islington, London which is named after her; Elizabeth Garrett Anderson School.
The archives of Elizabeth Garrett Anderson are held at The Women's Library at the , ref 
The archives of the Elizabeth Garrett Anderson Hospital (formerly the New Hospital for Women) are held at the London Metropolitan Archives.

</doc>
<doc id="9696" url="http://en.wikipedia.org/wiki?curid=9696" title="Erosion">
Erosion

In geomorphology and geology, erosion is the action of exogenic processes (such as water flow or wind) which remove soil and rock from one location on the Earth's crust, then transport it to another location where it is deposited. Eroded sediment may be transported just a few millimetres, or for thousands of kilometres.
While erosion is a natural process, human activities have increased by 10-40 times the rate at which erosion is occurring globally. Excessive (or accelerated) erosion causes both 'on-site' and 'off-site' problems. On-site impacts include decreases in agricultural productivity and (on natural landscapes) ecological collapse, both because of loss of the nutrient-rich upper soil layers. In some cases, the eventual end result is desertification. Off-site effects include sedimentation of waterways and eutrophication of water bodies, as well as sediment-related damage to roads and houses. Water and wind erosion are the two primary causes of land degradation; combined, they are responsible for about 84% of the global extent of degraded land, making excessive erosion one of the most significant environmental problems world-wide.
Intensive agriculture, deforestation, roads, anthropogenic climate change and urban sprawl are amongst the most significant human activities in regard to their effect on stimulating erosion. However, there are many prevention and remediation practices that can curtail or limit erosion of vulnerable soils.
Physical processes.
Rainfall and surface runoff.
Rainfall, and the surface runoff which may result from rainfall, produces four main types of soil erosion: "splash erosion", "sheet erosion", "rill erosion", and "gully erosion". Splash erosion is generally seen as the first and least severe stage in the soil erosion process, which is followed by sheet erosion, then rill erosion and finally gully erosion (the most severe of the four).
In "splash erosion", the impact of a falling raindrop creates a small crater in the soil, ejecting soil particles. The distance these soil particles travel can be as much as 0.6 m (two feet) vertically and 1.5 m (five feet) horizontally on level ground.
If the soil is saturated, or if the rainfall rate is greater than the rate at which water can infiltrate into the soil, surface runoff occurs. If the runoff has sufficient flow energy, it will transport loosened soil particles (sediment) down the slope. "Sheet erosion" is the transport of loosened soil particles by overland flow.
"Rill erosion" refers to the development of small, ephemeral concentrated flow paths which function as both sediment source and sediment delivery systems for erosion on hillslopes. Generally, where water erosion rates on disturbed upland areas are greatest, rills are active. Flow depths in rills are typically of the order of a few centimeters (about an inch) or less and along-channel slopes may be quite steep. This means that rills exhibit hydraulic physics very different from water flowing through the deeper, wider channels of streams and rivers.
"Gully erosion" occurs when runoff water accumulates and rapidly flows in narrow channels during or immediately after heavy rains or melting snow, removing soil to a considerable depth.
Rivers and streams.
"Valley" or "stream erosion" occurs with continued water flow along a linear feature. The erosion is both downward, deepening the valley, and headward, extending the valley into the hillside, creating head cuts and steep banks. In the earliest stage of stream erosion, the erosive activity is dominantly vertical, the valleys have a typical V cross-section and the stream gradient is relatively steep. When some base level is reached, the erosive activity switches to lateral erosion, which widens the valley floor and creates a narrow floodplain. The stream gradient becomes nearly flat, and lateral deposition of sediments becomes important as the stream meanders across the valley floor. In all stages of stream erosion, by far the most erosion occurs during times of flood, when more and faster-moving water is available to carry a larger sediment load. In such processes, it is not the water alone that erodes: suspended abrasive particles, pebbles and boulders can also act erosively as they traverse a surface, in a process known as "traction".
"Bank erosion" is the wearing away of the banks of a stream or river. This is distinguished from changes on the bed of the watercourse, which is referred to as "scour". Erosion and changes in the form of river banks may be measured by inserting metal rods into the bank and marking the position of the bank surface along the rods at different times.
"Thermal erosion" is the result of melting and weakening permafrost due to moving water. It can occur both along rivers and at the coast. Rapid river channel migration observed in the Lena River of Siberia is due to thermal erosion, as these portions of the banks are composed of permafrost-cemented non-cohesive materials. Much of this erosion occurs as the weakened banks fail in large slumps. Thermal erosion also affects the Arctic coast, where wave action and near-shore temperatures combine to undercut permafrost bluffs along the shoreline and cause them to fail. Annual erosion rates along a 100 km segment of the Beaufort Sea shoreline averaged 5.6 m per year from 1955 to 2002.
Coastal erosion.
Shoreline erosion, which occurs on both exposed and sheltered coasts, primarily occurs through the action of currents and waves but sea level (tidal) change can also play a role.
"Hydraulic action" takes place when air in a joint is suddenly compressed by a wave closing the entrance of the joint. This then cracks it. "Wave pounding" is when the sheer energy of the wave hitting the cliff or rock breaks pieces off. "Abrasion" or "corrasion" is caused by waves launching seaload at the cliff. It is the most effective and rapid form of shoreline erosion (not to be confused with "corrosion"). "Corrosion" is the dissolving of rock by carbonic acid in sea water. Limestone cliffs are particularly vulnerable to this kind of erosion. "Attrition" is where particles/seaload carried by the waves are worn down as they hit each other and the cliffs. This then makes the material easier to wash away. The material ends up as shingle and sand. Another significant source of erosion, particularly on carbonate coastlines, is the boring, scraping and grinding of organisms, a process termed "bioerosion".
Sediment is transported along the coast in the direction of the prevailing current (longshore drift). When the upcurrent amount of sediment is less than the amount being carried away, erosion occurs. When the upcurrent amount of sediment is greater, sand or gravel banks will tend to form as a result of deposition. These banks may slowly migrate along the coast in the direction of the longshore drift, alternately protecting and exposing parts of the coastline. Where there is a bend in the coastline, quite often a buildup of eroded material occurs forming a long narrow bank (a spit). Armoured beaches and submerged offshore sandbanks may also protect parts of a coastline from erosion. Over the years, as the shoals gradually shift, the erosion may be redirected to attack different parts of the shore.
Glaciers.
Glaciers erode predominantly by three different processes: abrasion/scouring, plucking, and ice thrusting. In an abrasion process, debris in the basal ice scrapes along the bed, polishing and gouging the underlying rocks, similar to sandpaper on wood. Glaciers can also cause pieces of bedrock to crack off in the process of plucking. In ice thrusting, the glacier freezes to its bed, then as it surges forward, it moves large sheets of frozen sediment at the base along with the glacier. This method produced some of the many thousands of lake basins that dot the edge of the Canadian Shield. The erosion caused by glaciers worldwide erodes mountains so effectively that the term "glacial buzz-saw" has become widely used, which describes the limiting effect of glaciers on the height of mountain ranges. As mountains grow higher, they generally allow for more glacial activity (especially in the accumulation zone above the glacial equilibrium line altitude), which causes increased rates of erosion of the mountain, decreasing mass faster than isostatic rebound can add to the mountain. This provides a good example of a negative feedback loop. Ongoing research is showing that while glaciers tend to decrease mountain size, in some areas, glaciers can actually reduce the rate of erosion, acting as a "glacial armor".
These processes, combined with erosion and transport by the water network beneath the glacier, leave moraines, drumlins, ground moraine (till), kames, kame deltas, moulins, and glacial erratics in their wake, typically at the terminus or during glacier retreat.
Floods.
At extremely high flows, kolks, or vortices are formed by large volumes of rapidly rushing water. Kolks cause extreme local erosion, plucking bedrock and creating pothole-type geographical features called Rock-cut basins. Examples can be seen in the flood regions result from glacial Lake Missoula, which created the channeled scablands in the Columbia Basin region of eastern Washington.
Freezing and thawing.
Cold weather causes water trapped in tiny rock cracks to freeze and expand, breaking the rock into several pieces. This can lead to gravity erosion on steep slopes. The scree which forms at the bottom of a steep mountainside is mostly formed from pieces of rock (soil) broken away by this means. It is a common engineering problem wherever rock cliffs are alongside roads, because morning thaws can drop hazardous rock pieces onto the road.
Wind erosion.
Wind erosion is a major geomorphological force, especially in arid and semi-arid regions. It is also a major source of land degradation, evaporation, desertification, harmful airborne dust, and crop damage—especially after being increased far above natural rates by human activities such as deforestation, urbanization, and agriculture.
Wind erosion is of two primary varieties: "deflation", where the wind picks up and carries away loose particles; and "abrasion", where surfaces are worn down as they are struck by airborne particles carried by wind. Deflation is divided into three categories: (1) "surface creep", where larger, heavier particles slide or roll along the ground; (2) "saltation", where particles are lifted a short height into the air, and bounce and saltate across the surface of the soil; and (3) "suspension", where very small and light particles are lifted into the air by the wind, and are often carried for long distances. Saltation is responsible for the majority (50-70%) of wind erosion, followed by suspension (30-40%), and then surface creep (5-25%).
Wind erosion is much more severe in arid areas and during times of drought. For example, in the Great Plains, it is estimated that soil loss due to wind erosion can be as much as 6100 times greater in drought years than in wet years.
Mass movement.
"Mass movement" is the downward and outward movement of rock and sediments on a sloped surface, mainly due to the force of gravity.
Mass movement is an important part of the erosional process, and is often the first stage in the breakdown and transport of weathered materials in mountainous areas. It moves material from higher elevations to lower elevations where other eroding agents such as streams and glaciers can then pick up the material and move it to even lower elevations. Mass-movement processes are always occurring continuously on all slopes; some mass-movement processes act very slowly; others occur very suddenly, often with disastrous results. Any perceptible down-slope movement of rock or sediment is often referred to in general terms as a landslide. However, landslides can be classified in a much more detailed way that reflects the mechanisms responsible for the movement and the velocity at which the movement occurs. One of the visible topographical manifestations of a very slow form of such activity is a scree slope.
"Slumping" happens on steep hillsides, occurring along distinct fracture zones, often within materials like clay that, once released, may move quite rapidly downhill. They will often show a spoon-shaped isostatic depression, in which the material has begun to slide downhill. In some cases, the slump is caused by water beneath the slope weakening it. In many cases it is simply the result of poor engineering along highways where it is a regular occurrence.
"Surface creep" is the slow movement of soil and rock debris by gravity which is usually not perceptible except through extended observation. However, the term can also describe the rolling of dislodged soil particles 0.5 to in diameter by wind along the soil surface.
Exfoliation.
"Exfoliation" is a type of erosion that occurs when a rock is rapidly heated up by the sun. This results in the expansion of the rock. When the temperature decreases again, the rock contracts, causing pieces of the rock to break off. Exfoliation occurs mainly in deserts due to the high temperatures during the day and cold temperatures at night.
Lightning strikes.
When water in cracked rock is rapidly heated by a lightning strike, the resulting steam explosion can cause rock disintegration and shift boulders. It may be a significant factor in erosion of tropical and subtropical mountains that have never been glaciated. Evidence of lightning strikes include craters, partially melted rock and erratic magnetic fields.
Factors affecting erosion rates.
Climate.
The amount and intensity of precipitation is the main climatic factor governing soil erosion by water. The relationship is particularly strong if heavy rainfall occurs at times when, or in locations where, the soil's surface is not well protected by vegetation. This might be during periods when agricultural activities leave the soil bare, or in semi-arid regions where vegetation is naturally sparse. Wind erosion requires strong winds, particularly during times of drought when vegetation is sparse and soil is dry (and so is more erodible). Other climatic factors such as average temperature and temperature range may also affect erosion, via their effects on vegetation and soil properties. In general, given similar vegetation and ecosystems, areas with more precipitation (especially high-intensity rainfall), more wind, or more storms are expected to have more erosion.
In some areas of the world (e.g. the mid-western USA), rainfall intensity is the primary determinant of erosivity, with higher intensity rainfall generally resulting in more soil erosion by water. The size and velocity of rain drops is also an important factor. Larger and higher-velocity rain drops have greater kinetic energy, and thus their impact will displace soil particles by larger distances than smaller, slower-moving rain drops.
In other regions of the world (e.g. western Europe), runoff and erosion result from relatively low intensities of stratiform rainfall falling onto previously saturated soil. In such situations, rainfall amount rather than intensity is the main factor determining the severity of soil erosion by water.
Soil structure and composition.
The composition, moisture, and compaction of soil are all major factors in determining the erosivity of rainfall. Sediments containing more clay tend to be more resistant to erosion than those with sand or silt, because the clay helps bind soil particles together. Soil containing high levels of organic materials are often more resistant to erosion, because the organic materials coagulate soil colloids and create a stronger, more stable soil structure. The amount of water present in the soil before the precipitation also plays an important role, because it sets limits on the amount of water that can be absorbed by the soil (and hence prevented from flowing on the surface as erosive runoff). Wet, saturated soils will not be able to absorb as much rain water, leading to higher levels of surface runoff and thus higher erosivity for a given volume of rainfall. Soil compaction also affects the permeability of the soil to water, and hence the amount of water that flows away as runoff. More compacted soils will have a larger amount of surface runoff than less compacted soils.
Vegetative cover.
Vegetation acts as an interface between the atmosphere and the soil. It increases the permeability of the soil to rainwater, thus decreasing runoff. It shelters the soil from winds, which results in decreased wind erosion, as well as advantageous changes in microclimate. The roots of the plants bind the soil together, and interweave with other roots, forming a more solid mass that is less susceptible to both water and wind erosion. The removal of vegetation increases the rate of surface erosion.
Topography.
The topography of the land determines the velocity at which surface runoff will flow, which in turn determines the erosivity of the runoff. Longer, steeper slopes (especially those without adequate vegetative cover) are more susceptible to very high rates of erosion during heavy rains than shorter, less steep slopes. Steeper terrain is also more prone to mudslides, landslides, and other forms of gravitational erosion processes.
Human activities that increase erosion rates.
Agricultural practices.
Unsustainable agricultural practices are the single greatest contributor to the global increase in erosion rates.
The tillage of agricultural lands, which breaks up soil into finer particles, is one of the primary factors. The problem has been exacerbated in modern times, due to mechanized agricultural equipment that allows for deep plowing, which severely increases the amount of soil that is available for transport by water erosion. Others include mono-cropping, farming on steep slopes, pesticide and chemical fertilizer usage (which kill organisms that bind soil together), row-cropping, and the use of surface irrigation. A complex overall situation with respect to defining nutrient losses from soils, could arise as a result of the size selective nature of soil erosion events. Loss of total phosphorus, for instance, in the finer eroded fraction is greater relative to the whole soil. Extrapolating this evidence to predict subsequent behaviour within receiving aquatic systems, the reason is that this more easily transported material may support a lower solution P concentration compared to coarser sized fractions. Tillage also increases wind erosion rates, by dehydrating the soil and breaking it up into smaller particles that can be picked up by the wind. Exacerbating this is the fact that most of the trees are generally removed from agricultural fields, allowing winds to have long, open runs to travel over at higher speeds. Heavy grazing reduces vegetative cover and causes severe soil compaction, both of which increase erosion rates.
Deforestation.
In an undisturbed forest, the mineral soil is protected by a layer of "leaf litter" and an "humus" that cover the forest floor. These two layers form a protective mat over the soil that absorbs the impact of rain drops. They are porous and highly permeable to rainfall, and allow rainwater to slow percolate into the soil below, instead of flowing over the surface as runoff. The roots of the trees and plants hold together soil particles, preventing them from being washed away. The vegetative cover acts to reduce the velocity of the raindrops that strike the foliage and stems before hitting the ground, reducing their kinetic energy. However it is the forest floor, more than the canopy, that prevents surface erosion. The terminal velocity of rain drops is reached in about 8 m. Because forest canopies are usually higher than this, rain drops can often regain terminal velocity even after striking the canopy. However, the intact forest floor, with its layers of leaf litter and organic matter, is still able to absorb the impact of the rainfall.
Deforestation causes increased erosion rates due to exposure of mineral soil by removing the humus and litter layers from the soil surface, removing the vegetative cover that binds soil together, and causing heavy soil compaction from logging equipment. Once trees have been removed by fire or logging, infiltration rates become high and erosion low to the degree the forest floor remains intact. Severe fires can lead to significant further erosion if followed by heavy rainfall.
Globally one of the largest contributors to erosive soil loss in the year 2006 is the slash and burn treatment of tropical forests. In a number of regions of the earth, entire sectors of a country have been rendered unproductive. For example, on the Madagascar high central plateau, comprising approximately ten percent of that country's land area, virtually the entire landscape is sterile of vegetation, with gully erosive furrows typically in excess of 50 m deep and 1 km wide. Shifting cultivation is a farming system which sometimes incorporates the slash and burn method in some regions of the world. This degrades the soil and causes the soil to become less and less fertile.
Roads and urbanization.
Urbanization has major effects on erosion processes—first by denuding the land of vegetative cover, altering drainage patterns, and compacting the soil during construction; and next by covering the land in an impermeable layer of asphalt or concrete that increases the amount of surface runoff and increases surface wind speeds. Much of the sediment carried in runoff from urban areas (especially roads) is highly contaminated with fuel, oil, and other chemicals. This increased runoff, in addition to eroding and degrading the land that it flows over, also causes major disruption to surrounding watersheds by altering the volume and rate of water that flows through them, and filling them with chemically polluted sedimentation. The increased flow of water through local waterways also causes a large increase in the rate of bank erosion.
Climate change.
The warmer atmospheric temperatures observed over the past decades are expected to lead to a more vigorous hydrological cycle, including more extreme rainfall events. The rise in sea levels that has occurred as a result of climate change has also greatly increased coastal erosion rates.
Studies on soil erosion suggest that increased rainfall amounts and intensities will lead to greater rates of erosion. Thus, if rainfall amounts and intensities increase in many parts of the world as expected, erosion will also increase, unless amelioration measures are taken. Soil erosion rates are expected to change in response to changes in climate for a variety of reasons. The most direct is the change in the erosive power of rainfall. Other reasons include: a) changes in plant canopy caused by shifts in plant biomass production associated with moisture regime; b) changes in litter cover on the ground caused by changes in both plant residue decomposition rates driven by temperature and moisture dependent soil microbial activity as well as plant biomass production rates; c) changes in soil moisture due to shifting precipitation regimes and evapo-transpiration rates, which changes infiltration and runoff ratios; d) soil erodibility changes due to decrease in soil organic matter concentrations in soils that lead to a soil structure that is more susceptible to erosion and increased runoff due to increased soil surface sealing and crusting; e) a shift of winter precipitation from non-erosive snow to erosive rainfall due to increasing winter temperatures; f) melting of permafrost, which induces an erodible soil state from a previously non-erodible one; and g) shifts in land use made necessary to accommodate new climatic regimes.
Studies by Pruski and Nearing indicated that, other factors such as land use not considered, it is reasonable to expect approximately a 1.7% change in soil erosion for each 1% change in total precipitation under climate change.
Global environmental effects.
Due to the severity of its ecological effects, and the scale on which it is occurring, erosion constitutes one of the most significant global environmental problems we face today.
Land degradation.
Water and wind erosion are now the two primary causes of land degradation; combined, they are responsible for 84% of degraded acreage.
Each year, about 75 billion tons of soil is eroded from the land—a rate that is about 13-40 times as fast as the natural rate of erosion. Approximately 40% of the world's agricultural land is seriously degraded. According to the United Nations, an area of fertile soil the size of Ukraine is lost every year because of drought, deforestation and climate change. In Africa, if current trends of soil degradation continue, the continent might be able to feed just 25% of its population by 2025, according to UNU's Ghana-based Institute for Natural Resources in Africa.
The loss of soil fertility due to erosion is further problematic because the response is often to apply chemical fertilizers, which leads to further water and soil pollution, rather than to allow the land to regenerate.
Sedimentation of aquatic ecosystems.
Soil erosion (especially from agricultural activity) is considered to be the leading global cause of diffuse water pollution, due to the effects of the excess sediments flowing into the world's waterways. The sediments themselves act as pollutants, as well as being carriers for other pollutants, such as attached pesticide molecules or heavy metals.
The effect of increased sediments loads on aquatic ecosystems can be catastrophic. Silt can smother the spawning beds of fish, by filling in the space between gravel on the stream bed. It also reduces their food supply, and causes major respiratory issues for them as sediment enters their gills. The biodiversity of aquatic plant and algal life is reduced, and invertebrates are also unable to survive and reproduce. While the sedimentation event itself might be relatively short-lived, the ecological disruption caused by the mass die off often persists long into the future.
One of the most serious and long-running water erosion problems worldwide is in the People's Republic of China, on the middle reaches of the Yellow River and the upper reaches of the Yangtze River. From the Yellow River, over 1.6 billion tons of sediment flows into the ocean each year. The sediment originates primarily from water erosion in the Loess Plateau region of the northwest.
Airborne dust pollution.
Soil particles picked up during wind erosion are a major source of air pollution, in the form of airborne particulates—"dust". These airborne soil particles are often contaminated with toxic chemicals such as pesticides or petroleum fuels, posing ecological and public health hazards when they later land, or are inhaled/ingested.
Dust from erosion acts to suppress rainfall and changes the sky color from blue to white, which leads to an increase in red sunsets. Dust events have been linked to a decline in the health of coral reefs across the Caribbean and Florida, primarily since the 1970s. Similar dust plumes originate in the Gobi desert, which combined with pollutants, spread large distances downwind, or eastward, into North America.
Tectonic effects.
The removal by erosion of large amounts of rock from a particular region, and its deposition elsewhere, can result in a lightening of the load on the lower crust and mantle. This can cause tectonic or isostatic uplift in the region.
Monitoring, measuring and modeling erosion.
Monitoring and modeling of erosion processes can help us better understand the causes, make predictions, and plan how to implement preventative and restorative strategies. However, the complexity of erosion processes and the number of areas that must be studied to understand and model them (e.g. climatology, hydrology, geology, chemistry, physics, etc.) makes accurate modelling quite challenging. Erosion models are also non-linear, which makes them difficult to work with numerically, and makes it difficult or impossible to scale up to making predictions about large areas from data collected by sampling smaller plots.
The most commonly used model for predicting soil loss from water erosion is the "Universal Soil Loss Equation (USLE)", which estimates the average annual soil loss "A" as:
where "R" is the , "K" is the , "L" and "S" are representing length and slope, "C" is the cover and management factor and "P" is the support practices factor.
A new soil erosion model named monitors soil erosion by a spatio-temporal index. is a dynamic model, as it takes account of contemporary changes of rainfall erosivity and vegetation retention. Based on the empirical USLE-family models, it needs calibration for rainstorm erosivity, while vegetation retention is based on biophysical parameters derived with remote sensing.
Erosion is measured and further understood using tools such as the micro-erosion meter (MEM) and the traversing micro-erosion meter (TMEM). The MEM has proved helpful in measuring bedrock erosion in various ecosystems around the world. It can measure both terrestrial and oceanic erosion. On the other hand, the TMEM can be used to track the expanding and contracting of volatile rock formations and can give a reading of how quickly a rock formation is deteriorating.
Prevention and remediation.
The most effective known method for erosion prevention is to increase vegetative cover on the land, which helps prevent both wind and water erosion. Terracing is an extremely effective means of erosion control, which has been practiced for thousands of years by people all over the world. Windbreaks (also called shelterbelts) are rows of trees and shrubs that are planted along the edges of agricultural fields, to shield the fields against winds. In addition to significantly reducing wind erosion, windbreaks provide many other benefits such as improved microclimates for crops (which are sheltered from the dehydrating and otherwise damaging effects of wind), habitat for beneficial bird species, carbon sequestration, and aesthetic improvements to the agricultural landscape. Traditional planting methods, such as mixed-cropping (instead of monocropping) and crop rotation have also been shown to significantly reduce erosion rates.
Further reading.
</dl>

</doc>
<doc id="9697" url="http://en.wikipedia.org/wiki?curid=9697" title="Euclidean space">
Euclidean space

In geometry, Euclidean space encompasses the two-dimensional Euclidean plane, the three-dimensional space of Euclidean geometry, and certain other spaces. It is named after the Ancient Greek mathematician Euclid of Alexandria. The term "Euclidean" distinguishes these spaces from other types of spaces considered in modern geometry. Euclidean spaces also generalize to higher dimensions.
Classical Greek geometry defined the Euclidean plane and Euclidean three-dimensional space using certain postulates, while the other properties of these spaces were deduced as theorems. Geometric constructions are also used to define rational numbers. When algebra and mathematical analysis became developed enough, this relation reversed and now it is more common to define Euclidean space using Cartesian coordinates and the ideas of analytic geometry. It means that points of the space are specified with collections of real numbers, and geometric shapes are defined as equations and inequalities. This approach brings the tools of algebra and calculus to bear on questions of geometry and has the advantage that it generalizes easily to Euclidean spaces of more than three dimensions.
From the modern viewpoint, there is essentially only one Euclidean space of each dimension. With Cartesian coordinates it is modelled by the real coordinate space (R"n") of the same dimension. In one dimension, this is the real line; in two dimensions, it is the Cartesian plane; and in higher dimensions it is a coordinate space with three or more real number coordinates. Mathematicians denote the n-dimensional Euclidean space by E"n" if they wish to emphasize its Euclidean nature, but R"n" is used as well since the latter is assumed to have the standard Euclidean structure, and these two structures are not always distinguished. Euclidean spaces have finite dimension.
Intuitive overview.
One way to think of the Euclidean plane is as a set of points satisfying certain relationships, expressible in terms of distance and angle. For example, there are two fundamental operations (referred to as motions) on the plane. One is translation, which means a shifting of the plane so that every point is shifted in the same direction and by the same distance. The other is rotation about a fixed point in the plane, in which every point in the plane turns about that fixed point through the same angle. One of the basic tenets of Euclidean geometry is that two figures (usually considered as subsets) of the plane should be considered equivalent (congruent) if one can be transformed into the other by some sequence of translations, rotations and reflections (see below).
In order to make all of this mathematically precise, the theory must clearly define the notions of distance, angle, translation, and rotation for a mathematically described space. Even when used in physical theories, Euclidean space is an abstraction detached from actual physical locations, specific reference frames, measurement instruments, and so on. A purely mathematical definition of Euclidean space also ignores questions of units of length and other physical dimensions: the distance in a "mathematical" space is a number, not something expressed in inches or metres. The standard way to define such space, as carried out in the remainder of this article, is to define the Euclidean plane as a two-dimensional real vector space equipped with an inner product. The reason for working with arbitrary vector spaces instead of R"n" is that it is often preferable to work in a "coordinate-free" manner (that is, without choosing a preferred basis). For then:
Once the Euclidean plane has been described in this language, it is actually a simple matter to extend its concept to arbitrary dimensions. For the most part, the vocabulary, formulae, and calculations are not made any more difficult by the presence of more dimensions. (However, rotations are more subtle in high dimensions, and visualizing high-dimensional spaces remains difficult, even for experienced mathematicians.)
A Euclidean space is not technically a vector space but rather an affine space, on which a vector space acts by translations, or, conversely, a Euclidean vector is the difference (displacement) in an ordered pair of points, not a single point. Intuitively, the distinction says merely that there is no canonical choice of where the origin should go in the space, because it can be translated anywhere. When a certain point is chosen, it can be declared the origin and subsequent calculations may ignore the difference between a point and its coordinate vector, as said above. See point–vector distinction for details.
Euclidean structure.
These are distances between points and the angles between lines or vectors, which satisfy certain conditions (see below), which makes a set of points a Euclidean space. The natural way to obtain these quantities is by introducing and using the standard inner product (also known as the dot product) on R"n". The inner product of any two real n-vectors x and y is defined by
where xi and yi are ith coordinates of vectors x and y respectively.
The result is always a real number.
Distance.
The inner product of x with itself is always non-negative. This product allows us to define the "length" of a vector x through square root:
This length function satisfies the required properties of a norm and is called the Euclidean norm on R"n".
Finally, one can use the norm to define a metric (or distance function) on R"n" by
This distance function is called the Euclidean metric. This formula expresses a special case of the Pythagorean theorem.
This distance function (which makes a metric space) is sufficient to define all Euclidean geometry, including the dot product. Thus, a real coordinate space together with this Euclidean structure is called Euclidean space. Its vectors form an inner product space (in fact a Hilbert space), and a normed vector space.
The metric space structure is the main reason behind the use of real numbers R, not some other ordered field, as the mathematical foundation of Euclidean (and many other) spaces. Euclidean space is a complete metric space, a property which is impossible to achieve operating over rational numbers, for example.
Angle.
The (non-reflex) angle θ (0° ≤ "θ" ≤ 180°) between vectors x and y is then given by
where arccos is the arccosine function. It is useful only for "n" > 1, and the case "n" = 2 is somewhat special. Namely, on an oriented Euclidean plane one can define an angle between two vectors as a number defined modulo 1 turn (usually denoted as either 2π or 360°), such that ∠y x = −∠x y. This oriented angle is equal either to the angle θ from the formula above or to −"θ". If one non-zero vector is fixed (such as the first basis vector), then each non-zero vector is uniquely defined by its magnitude and angle.
The angle does not change if vectors x and y are multiplied by positive numbers.
Unlike the aforementioned situation with distance, the scale of angles is the same in pure mathematics, physics, and computing. It does not depend on the scale of distances; all distances may be multiplied by some fixed factor, and all angles will be preserved. Usually, the angle is considered a dimensionless quantity, but there are different units of measurement, such as radian (preferred in pure mathematics and theoretical physics) and degree (°) (preferred in most applications).
Rotations and reflections.
Symmetries of a Euclidean space are transformations which preserve the Euclidean metric (called "isometries"). Although aforementioned translations are most obvious of them, they have the same structure for any affine space and do not show a distinctive character of Euclidean geometry. Another family of symmetries leave one point fixed, which may be seen as the origin without loss of generality. All transformations, which preserves the origin and the Euclidean metric, are linear maps. Such transformations Q must, for any x and y, satisfy:
Such transforms constitute a group called the "orthogonal group" O("n"). Its elements Q are exactly solutions of a matrix equation
where QT is the transpose of Q and "I" is the identity matrix.
But a Euclidean space is orientable. Each of these transformations either preserves or reverses orientation depending on whether its determinant is +1 or −1 respectively. Only transformations which preserve orientation, which form the "special orthogonal" group SO("n"), are considered (proper) rotations. This group has, as a Lie group, the same dimension "n"("n" − 1) /2 and is the identity component of O("n").
Groups SO("n") are well-studied for "n" ≤ 4. There are no non-trivial rotations in 0- and 1-spaces. Rotations of a Euclidean plane ("n" = 2) are parametrized by the angle (modulo 1 turn). Rotations of a 3-space are parametrized with axis and angle, whereas a rotation of a 4-space is a superposition of two 2-dimensional rotations around perpendicular planes.
Among linear transforms in O("n") which reverse the orientation are hyperplane reflections. This is the only possible case for "n" ≤ 2, but starting from three dimensions, such isometry in the general position is a rotoreflection.
Euclidean group.
The Euclidean group "E"("n"), also referred to as the group of all isometries ISO("n"), treats translations, rotations, and reflections in a uniform way, considering them as group actions in the context of group theory, and especially in Lie group theory. These group actions preserve the Euclidean structure.
As the group of all isometries, ISO("n"), the Euclidean group is important because it makes Euclidean geometry a case of Klein geometry, a theoretical framework including many alternative geometries.
The structure of Euclidean spaces – distances, lines, vectors, angles (up to sign), and so on – is invariant under the transformations of their associated Euclidean group. For instance, translations form a commutative subgroup that acts freely and transitively on E"n", while the stabilizer of any point there is the aforementioned O("n").
Along with translations, rotations, reflections, as well as the identity transformation, Euclidean motions comprise also glide reflections (for "n" ≥ 2), screw operations and rotoreflections (for "n" ≥ 3), and even more complex combinations of primitive transformations for "n" ≥ 4.
The group structure determines which conditions a metric space needs to satisfy to be a Euclidean space:
Non-Cartesian coordinates.
Cartesian coordinates are arguably the standard, but not the only possible option for a Euclidean space.
Skew coordinates are compatible with the affine structure of E"n", but make formulae for angles and distances more complicated.
Another approach, which goes in line with ideas of differential geometry and conformal geometry, is orthogonal coordinates, where coordinate hypersurfaces of different coordinates are orthogonal, although curved. Examples include the polar coordinate system on Euclidean plane, the second important plane coordinate system.
See below about expression of the Euclidean structure in curvilinear coordinates.
Geometric shapes.
Lines, planes, and other subspaces.
The simplest (after points) objects in Euclidean space are flats, or Euclidean "subspaces" of lesser dimension. Points are 0-dimensional flats, 1-dimensional flats are called "(straight) lines", and 2-dimensional flats are "planes". ("n" − 1)-dimensional flats are called "hyperplanes".
Any two distinct points lie on exactly one line. Any line and a point outside it lie on exactly one plane. More generally, the properties of flats and their incidence of Euclidean space are shared with affine geometry, whereas the affine geometry is devoid of distances and angles.
Line segments and triangles.
This is not only a line which a pair ("A", "B") of distinct points defines. Points on the line which lie between A and B, together with A and B themselves, constitute a line segment "A" "B". Any line segment has the length, which equals to distance between A and B. If "A" = "B", then the segment is degenerate and its length equals to 0, otherwise the length is positive.
A (non-degenerate) triangle is defined by three points not lying on the same line. Any triangle lies on one plane. The concept of triangle is not specific to Euclidean spaces, but Euclidean triangles have numerous special properties and define many derived objects.
A triangle can be thought of as a 3-gon on a plane, a special (and the first meaningful in Euclidean geometry) case of a polygon.
Polytopes and root systems.
Polytope is a concept that generalizes polygons on a plane and polyhedra in 3-dimensional space (which are among the earliest studied geometrical objects). A simplex is a generalization of a line segment (1-simplex) and a triangle (2-simplex). A tetrahedron is a 3-simplex.
The concept of a polytope belongs to affine geometry, which is more general than Euclidean. But Euclidean geometry distinguish "regular polytopes". For example, affine geometry does not see the difference between an equilateral triangle and a right triangle, but in Euclidean space the former is regular and the latter is not.
Root systems are special sets of Euclidean vectors. A root system is often identical to the set of vertices of a regular polytope.
Topology.
Since Euclidean space is a metric space, it is also a topological space with the natural topology induced by the metric. The metric topology on E"n" is called the Euclidean topology, and it is identical to the standard topology on R"n". A set is open if and only if it contains an open ball around each of its points; in other words, open balls form a base of the topology. The topological dimension of the Euclidean n-space equals n, which implies that spaces of different dimension are not homeomorphic. A finer result is the invariance of domain, which proves that any subset of n-space, that is (with its subspace topology) homeomorphic to an open subset of n-space, is itself open.
Applications.
Aside from countless uses in fundamental mathematics, a Euclidean model of the physical space can be used to solve many practical problems with sufficient precision. Two usual approaches are a fixed, or "stationary" reference frame (i.e. the description of a motion of objects as their positions that change continuously with time), and the use of Galilean space-time symmetry (such as in Newtonian mechanics). To both of them the modern Euclidean geometry provides a convenient formalism; for example, the space of Galilean velocities is itself a Euclidean space (see relative velocity for details).
Topographical maps and technical drawings are planar Euclidean. An idea behind them is the scale invariance of Euclidean geometry, that permits to represent large objects in a small sheet of paper, or a screen.
Alternatives and generalizations.
Although Euclidean spaces are not considered as the only possible setting for a geometry any more, they form the prototypes for other geometric objects. Ideas and terminology from Euclidean geometry (both traditional and analytic) are pervasive in modern mathematics, where other geometric objects share many similarities with Euclidean spaces, have a portion of their structure, or include Euclidean spaces as a partial case.
Curved spaces.
A smooth manifold is a Hausdorff topological space that is locally diffeomorphic to Euclidean space. Diffeomorphism does not respect distance and angle, but if one additionally prescribes a smoothly varying inner product on the manifold's tangent spaces, then the result is what is called a Riemannian manifold. Put differently, a Riemannian manifold is a space constructed by deforming and patching together Euclidean spaces. Such a space enjoys notions of distance and angle, but they behave in a curved, non-Euclidean manner. The simplest Riemannian manifold, consisting of R"n" with a constant inner product, is essentially identical to Euclidean n-space itself. Less trivial examples are n-sphere and hyperbolic spaces. Discovery of the latter in the 19th century was branded as the non-Euclidean geometry.
Also, the concept of a Riemannian manifold permits an expression of the Euclidean structure in any smooth coordinate system, via metric tensor. From this tensor one can compute the Riemann curvature tensor. Where the latter equals to zero, the metric structure is locally Euclidean (it means that at least some open set in the coordinate space is isometric to a piece of Euclidean space), no matter whether coordinates are affine or curvilinear.
Indefinite quadratic form.
If one replaces the inner product of a Euclidean space with an indefinite quadratic form, the result is a pseudo-Euclidean space. Smooth manifolds built from such spaces are called pseudo-Riemannian manifolds. Perhaps their most famous application is the theory of relativity, where flat spacetime is a pseudo-Euclidean space called Minkowski space, where rotations correspond to motions of hyperbolic spaces mentioned above. Further generalization to curved spacetimes form pseudo-Riemannian manifolds, such as in general relativity.
Other number fields.
Another line of generalization is to consider other number fields than one of real numbers. Over complex numbers, a Hilbert space can be seen as a generalization of Euclidean dot product structure, although the definition of the inner product becomes a sesquilinear form for compatibility with metric structure.

</doc>
<doc id="9700" url="http://en.wikipedia.org/wiki?curid=9700" title="Edwin Austin Abbey">
Edwin Austin Abbey

Edwin Austin Abbey (April 1, 1852 – August 1, 1911) was an American muralist, illustrator, and painter. He flourished at the beginning of what is now referred to as the "golden age" of illustration, and is best known for his drawings and paintings of Shakespearean and Victorian subjects, as well as for his painting of Edward VII's coronation. His most famous set of murals, "The Quest of the Holy Grail", adorns the Boston Public Library.
Biography.
Abbey was born in Philadelphia in 1852. He studied art at the Pennsylvania Academy of the Fine Arts under Christian Schuessele. Abbey began as an illustrator, producing numerous illustrations and sketches for such magazines as Harper's Weekly (1871–1874) and Scribner's Magazine. His illustrations began appearing in Harper's Weekly at an early age: before Abbey was twenty years old. He moved to New York City in 1871. His illustrations were strongly influenced by French and German black and white art. He also illustrated several best-selling books, including "Christmas Stories" by Charles Dickens (1875), "Selections from the Poetry of Robert Herrick" (1882), and "She Stoops to Conquer" by Oliver Goldsmith (1887). Abbey also illustrated a four-volume set of "The Comedies of Shakespeare" for Harper & Brothers in 1896.
He moved to England in 1878, at the request of his employers, to gather material concerning Robert Herrick, and he settled permanently there in 1883. In 1883, he was elected to the Royal Institute of Painters in Water-Colours. About this time, he was appraised critically by the American writer, S.G.W. Benjamin:
It must be taken into consideration that he is still very young; that he now for the first time visits the studios and galleries of Europe; that his advantages for a regular art education have been very moderate, and that he is practically self-educated. And then compare with these disadvantages the amount and the quality of the illustrations he has turned out, and we see represented in him genius of a high order, combining almost inexhaustible creativeness, clearness and vividness of conception, a versatile fancy, a poetic perception of beauty, a quaint, delicate humor, a wonderful grasp of whatever is weird and mysterious, and admirable chiaro-oscuro, drawing, and composition. When we note such a rare combination of qualities, we cease to be surprised at the cordial recognition awarded his genius by the best judges, both in London and Paris, even before he had left this country.
He was made a full member of the Royal Academy in 1898. In 1902 he was chosen to paint the coronation of King Edward VII. It was the official painting of the occasion and, hence, resides at Buckingham Palace. He did receive a knighthood, although some say he refused it in 1907. Friendly with other expatriate American artists, he summered at Broadway, Worcestershire, England, where he painted and vacationed alongside John Singer Sargent at the home of Francis Davis Millet.
He completed murals for the Boston Public Library in the 1890s. The frieze for the Library was titled "The Quest for the Holy Grail." It took Abbey eleven years to complete this series of murals in his England studio.
Pennsylvania State Capitol.
In 1908–09, Abbey began an ambitious program of murals and other artworks for the newly completed Pennsylvania State Capitol in Harrisburg, Pennsylvania. These included allegorical medallion murals representing "Science", "Art", "Justice", and "Religion" for the dome of the Rotunda, four large lunette murals beneath the dome, and multiple works for the House and Senate Chambers. He was working on the "Reading of the Declaration of Independence" mural in early 1911, when his health began to fail. He was diagnosed with cancer. Studio assistant Ernest Board continued work on the mural with little supervision from Abbey, and with contributions by John Singer Sergeant.
Abbey died in August 1911, leaving two rooms of the commission unfinished. The remainder of the work was given to Violet Oakley, who completed the commission from start to finish using her own designs.
Legacy.
Abbey was elected to the National Academy of Design, in 1902, and The American Academy of Arts and Letters. He was a prolific illustrator, and attention to detail, including historical accuracy, influenced successive generations of illustrators.
In 1890, Edwin married Gertrude Mead, the daughter of a wealthy New York merchant. Mrs Abbey encouraged her husband to secure more ambitious commissions, although with their marriage commencing when both were in their forties, the couple remained childless. After her husband’s death, Gertrude was active in preserving her husband’s legacy, writing about his work and giving her substantial collection and archive to Yale. Edwin had been a keen supporter of the newly founded British School at Rome (BSR), so, in his memory, she donated £6000 to assist in building the artists’ studio block and, in 1926, founded the Incorporated Edwin Austin Abbey Memorial Scholarships. The scholarships were established to enable British and American painters to pursue their practice. Recipients of Abbey funding – Scholars and, more recently, Fellows – devote their scholarship to working in the studios at the BSR, where there has, ever since, been at least one Abbey-funded artist in residence. Previous award holders include Stephen Farthing, Chantal Joffe and Spartacus Chetwynd. The Abbey Fellowships (formerly ‘Awards’) were established in their present form in 1990, and the Abbey studios also host the BSR’s other fine art residencies, such as the Derek Hill Foundation Scholarship and the Sainsbury Scholarship in Painting and Drawing. A bust of Edwin Abbey, by Sir Thomas Brock, stands in the courtyard of the BSR.
Works by Abbey.
<gallery>
File:Edwin Austin Abbey Vanity Fair 29 December 1898.jpg|"Edwin Austin Abbey" (1898), by Leslie Ward, "Vanity Fair", 29 December 1898.
File:Edwin Austin Abbey - Bob Acres and His Servant.jpg|"Bob Acres and His Servant" (c. 1895), Yale University Art Gallery.
File:Edwin Austin Abbey richard duke of gloucester and the lady anne 1896.jpg|"Richard III of England, and the Lady Anne" (1896), Yale University Art Gallery.
File:Edwin Austin Abbey 01.jpg|"Sir Galahad and the Holy Grail" (1896-1901), Boston Public Library.
File:The Play Scene in Hamlet.jpg|"The Play Scene in Hamlet" (1897), Yale University Art Gallery.
File:Abbey - The Queen in Hamlet.jpg|"The Queen in Hamlet" (c. 1897), private collection.
File:Abbey, Edwin Austin - Potpourri - 1899.jpg|"Potpourri" (1899), private collection. 
File:Pennsylvania State Capitol House Chamber.jpg|"Apotheosis of Pennsylvania" (1908–11), House Chamber, Pennsylvania State Capitol.
File:Abbey, Edwin Austin - Fairies.jpg|Fairies
</Gallery>

</doc>
<doc id="9703" url="http://en.wikipedia.org/wiki?curid=9703" title="Evolutionary psychology">
Evolutionary psychology

Evolutionary psychology (EP) is an approach in the social and natural sciences that examines psychological structure from a modern evolutionary perspective. It seeks to identify which human psychological traits are evolved adaptations – that is, the functional products of natural selection or sexual selection. Adaptationist thinking about physiological mechanisms, such as the heart, lungs, and immune system, is common in evolutionary biology. Some evolutionary psychologists apply the same thinking to psychology, arguing that the mind has a modular structure similar to that of the body, with different modular adaptations serving different functions. Evolutionary psychologists argue that much of human behavior is the output of psychological adaptations that evolved to solve recurrent problems in human ancestral environments.
Evolutionary psychologists suggest that EP is not simply a subdiscipline of psychology but that evolutionary theory can provide a foundational, metatheoretical framework that integrates the entire field of psychology, in the same way it has for biology.
Evolutionary psychologists hold that behaviors or traits that occur universally in all cultures are good candidates for evolutionary adaptations including the abilities to infer others' emotions, discern kin from non-kin, identify and prefer healthier mates, and cooperate with others. They report successful tests of theoretical predictions related to such topics as infanticide, intelligence, marriage patterns, promiscuity, perception of beauty, bride price, and parental investment.
The theories and findings of EP have applications in many fields, including economics, environment, health, law, management, psychiatry, politics, and literature.
Controversies concerning EP involve questions of testability, cognitive and evolutionary assumptions (such as modular functioning of the brain, and large uncertainty about the ancestral environment), importance of non-genetic and non-adaptive explanations, as well as political and ethical issues due to interpretations of research results.
Scope.
Principles.
Evolutionary psychology is an approach that views human nature as the product of a universal set of evolved psychological adaptations to recurring problems in the ancestral environment. Proponents of EP suggest that it seeks to integrate psychology into the other natural sciences, rooting it in the organizing theory of biology (evolutionary theory), and thus understanding psychology as a branch of biology. Anthropologist John Tooby and psychologist Leda Cosmides note:
Evolutionary psychology is the long-forestalled scientific attempt to assemble out of the disjointed, fragmentary, and mutually contradictory human disciplines a single, logically integrated research framework for the psychological, social, and behavioral sciences—a framework
that not only incorporates the evolutionary sciences on a full and equal basis, but that systematically works out all of the revisions in existing belief and research practice that such a synthesis requires.
Just as human physiology and evolutionary physiology have worked to identify physical adaptations of the body that represent "human physiological nature," the purpose of evolutionary psychology is to identify evolved emotional and cognitive adaptations that represent "human psychological nature." According to Steven Pinker, EP is "not a single theory but a large set of hypotheses" and a term that "has also come to refer to a particular way of applying evolutionary theory to the mind, with an emphasis on adaptation, gene-level selection, and modularity." Evolutionary psychology adopts an understanding of the mind that is based on the computational theory of mind. It describes mental processes as computational operations, so that, for example, a fear response is described as arising from a neurological computation that inputs the perceptional data, e.g. a visual image of a spider, and outputs the appropriate reaction, e.g. fear of possibly dangerous animals.
While philosophers have generally considered the human mind to include broad faculties, such as reason and lust, evolutionary psychologists describe evolved psychological mechanisms as narrowly focused to deal with specific issues, such as catching cheaters or choosing mates. EP views the human brain as comprising many functional mechanisms, called "psychological adaptations" or evolved cognitive mechanisms or "cognitive modules", designed by the process of natural selection. Examples include language-acquisition modules, incest-avoidance mechanisms, cheater-detection mechanisms, intelligence and sex-specific mating preferences, foraging mechanisms, alliance-tracking mechanisms, agent-detection mechanisms, and others. Some mechanisms, termed "domain-specific", deal with recurrent adaptive problems over the course of human evolutionary history. "Domain-general" mechanisms, on the other hand, are proposed to deal with evolutionary novelty.
EP has roots in cognitive psychology and evolutionary biology but also draws on behavioral ecology, artificial intelligence, genetics, ethology, anthropology, archaeology, biology, and zoology. EP is closely linked to sociobiology, but there are key differences between them including the emphasis on "domain-specific" rather than "domain-general" mechanisms, the relevance of measures of current fitness, the importance of mismatch theory, and psychology rather than behavior. Most of what is now labeled as sociobiological research is now confined to the field of behavioral ecology.
Nikolaas Tinbergen's four categories of questions can help to clarify the distinctions between several different, but complementary, types of explanations. Evolutionary psychology focuses primarily on the "why?" questions, while traditional psychology focuses on the "how?" questions.
Premises.
Evolutionary psychology is founded on several core premises.
History.
Evolutionary psychology has its historical roots in Charles Darwin's theory of natural selection. In "The Origin of Species", Darwin predicted that psychology would develop an evolutionary basis:
In the distant future I see open fields for far more important researches. Psychology will be based on a new foundation, that of the necessary acquirement of each mental power and capacity by gradation.{{#if:Darwin, Charles (1859){{
 #if: 
 #if:  
 |, 
 #if: 
 }}{{
 #if: 
 #if: 
 | ()
 |{{
 #if: 
 #if: 
 | {{#if:||}}{{
 #if: 
 #if: 
 #if: {{
 #if: Page%3AOrigin_of_Species_1859_facsimile.djvu/500The Origin of Species
 #if: 
 #if: 
 #switch: 
 #if: 488
 #if: 
 |{{
 #switch: 
 #if: 488
 #if: 
 |: {{
 #if: Page%3AOrigin_of_Species_1859_facsimile.djvu/500The Origin of Species
 #if: 
 #if: 
 #switch: 
 #if: 488
 #if: 
 |{{
 #switch: 
 #if: 488
 #if: 
 
 #if:  
 |{{
 #if: Darwin
 }} {{Citation/make link
 | 1={{
 #if: 
 #if: 
 #if: 
 |{{
 #if: 
 | 2="  
 #if:| []
 #if: 
 }}{{
 #if: 
 }}{{
 #if: 
 }}{{
 #if: 
 }}{{
 #if: 
 | ( ed.)
 #if: 
 }}{{
 #if: 
 #if: 
 |,
 #if: Darwin
 |{{
 #if: 1859
 |, 1859{{
 #if:
}}{{
 #if: 
 #ifeq: | 1859
 |{{
 #if: 
 #if: Darwin
 | (published )
 |{{
 #if: 
 | (published )
}}{{
 #if: 
 |{{
 #if: {{
 #if: Page%3AOrigin_of_Species_1859_facsimile.djvu/500The Origin of Species
 #if: 
 #if: 
 #switch: 
 #if: 488
 #if: 
 |{{
 #switch: 
 #if: 488
 #if: 
 |, {{
 #if: Page%3AOrigin_of_Species_1859_facsimile.djvu/500The Origin of Species
 #if: 
 #if: 
 #switch: 
 #if: 488
 #if: 
 |{{
 #switch: 
 #if: 488
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
}}{{
 #if:
 | , {{#ifeq: | no
 | {{#if:
 |{{Citation/make link||{{#ifeq:|.|A|a}}rchived}} from the original
 |{{#ifeq:|.|A|a}}rchived
 | {{#ifeq:|.|A|a}}rchived{{#if:
 }}{{#if:| on }}{{
 |. {{citation error|nocat=
 #if:  
 |, {{
 #if: 
 |
 |, {{
 #if: 
 |
 }}{{
 #if: 
 | {{#ifeq:|,|, r|. R}}etrieved 
}}{{#if:
}}{{#if:
}}{{#if:
}}<span
 class="Z3988"
 title="ctx_ver=Z39.88-2004&rft_val_fmt={{urlencode:info:ofi/fmt:kev:mtx:}}{{
 #if: 
 |journal&rft.genre=article&rft.atitle={{urlencode:  
 |book{{
 #if: 
 |&rft.genre=bookitem&rft.btitle={{urlencode:}}&rft.atitle={{urlencode:  
 |&rft.genre=book&rft.btitle={{urlencode:  
 #if: Darwin |&rft.aulast={{urlencode:Darwin}}{{
 }}{{
 #if: Darwin |&rft.au={{urlencode:Darwin}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: {{
 #if: Page%3AOrigin_of_Species_1859_facsimile.djvu/500The Origin of Species
 #if: 
 #if: 
 #switch: 
 #if: 488
 #if: 
 |{{
 #switch: 
 #if: 488
 #if: 
 |&rft.pages={{urlencode: {{
 #if: Page%3AOrigin_of_Species_1859_facsimile.djvu/500The Origin of Species
 #if: 
 #if: 
 #switch: 
 #if: 488
 #if: 
 |{{
 #switch: 
 #if: 488
 #if: 
 }}{{
 }}&rfr_id=info:sid/en.wikipedia.org:{{FULLPAGENAMEE}}"> 
 |IncludedWorkTitle = 
 |IncludedWorkURL = 
 |Other = 
 |Edition = 
 |Place = 
 |PublicationPlace = 
 |Publisher = Wikisource
 |PublicationDate = 
 |EditorSurname1 = 
 |EditorSurname2 = 
 |EditorSurname3 = 
 |EditorSurname4 = 
 |EditorGiven1 = 
 |EditorGiven2=
 |EditorGiven3=
 |EditorGiven4=
 |Editorlink1=
 |Editorlink2=
 |Editorlink3=
 |Editorlink4=
 |language = 
 |format = 
 |ARXIV=
 |ASIN=
 |BIBCODE=
 |DOI=
 |DoiBroken=
 |ISBN=
 |ISSN=
 |JFM=
 |JSTOR=
 |LCCN=
 |MR=
 |OCLC=
 |OL=
 |OSTI=
 |PMC=
 |Embargo=1010-10-10
 |PMID=
 |RFC=
 |SSRN=
 |ZBL=
 |ID=
 |AccessDate=
 |DateFormat=none
 |quote = 
 |laysummary = 
 |laydate = 
 |Ref=
 |Sep = .
 |PS = .
 |AuthorSep = ; 
 |NameSep = , 
 |Trunc = 8
 |amp = 
 |—Darwin, Charles (1859){{
 #if: 
 #if:  
 |, 
 #if: 
 }}{{
 #if: 
 #if: 
 | ()
 |{{
 #if: 
 #if: 
 | {{#if:||}}{{
 #if: 
 #if: 
 #if: {{
 #if: Page%3AOrigin_of_Species_1859_facsimile.djvu/500The Origin of Species
 #if: 
 #if: 
 #switch: 
 #if: 488
 #if: 
 |{{
 #switch: 
 #if: 488
 #if: 
 |: {{
 #if: Page%3AOrigin_of_Species_1859_facsimile.djvu/500The Origin of Species
 #if: 
 #if: 
 #switch: 
 #if: 488
 #if: 
 |{{
 #switch: 
 #if: 488
 #if: 
 
 #if:  
 |{{
 #if: Darwin
 }} {{Citation/make link
 | 1={{
 #if: 
 #if: 
 #if: 
 |{{
 #if: 
 | 2="  
 #if:| []
 #if: 
 }}{{
 #if: 
 }}{{
 #if: 
 }}{{
 #if: 
 }}{{
 #if: 
 | ( ed.)
 #if: 
 }}{{
 #if: 
 #if: 
 |,
 #if: Darwin
 |{{
 #if: 1859
 |, 1859{{
 #if:
}}{{
 #if: 
 #ifeq: | 1859
 |{{
 #if: 
 #if: Darwin
 | (published )
 |{{
 #if: 
 | (published )
}}{{
 #if: 
 |{{
 #if: {{
 #if: Page%3AOrigin_of_Species_1859_facsimile.djvu/500The Origin of Species
 #if: 
 #if: 
 #switch: 
 #if: 488
 #if: 
 |{{
 #switch: 
 #if: 488
 #if: 
 |, {{
 #if: Page%3AOrigin_of_Species_1859_facsimile.djvu/500The Origin of Species
 #if: 
 #if: 
 #switch: 
 #if: 488
 #if: 
 |{{
 #switch: 
 #if: 488
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
}}{{
 #if:
 | , {{#ifeq: | no
 | {{#if:
 |{{Citation/make link||{{#ifeq:|.|A|a}}rchived}} from the original
 |{{#ifeq:|.|A|a}}rchived
 | {{#ifeq:|.|A|a}}rchived{{#if:
 }}{{#if:| on }}{{
 |. {{citation error|nocat=
 #if:  
 |, {{
 #if: 
 |
 |, {{
 #if: 
 |
 }}{{
 #if: 
 | {{#ifeq:|,|, r|. R}}etrieved 
}}{{#if:
}}{{#if:
}}{{#if:
}}<span
 class="Z3988"
 title="ctx_ver=Z39.88-2004&rft_val_fmt={{urlencode:info:ofi/fmt:kev:mtx:}}{{
 #if: 
 |journal&rft.genre=article&rft.atitle={{urlencode:  
 |book{{
 #if: 
 |&rft.genre=bookitem&rft.btitle={{urlencode:}}&rft.atitle={{urlencode:  
 |&rft.genre=book&rft.btitle={{urlencode:  
 #if: Darwin |&rft.aulast={{urlencode:Darwin}}{{
 }}{{
 #if: Darwin |&rft.au={{urlencode:Darwin}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: {{
 #if: Page%3AOrigin_of_Species_1859_facsimile.djvu/500The Origin of Species
 #if: 
 #if: 
 #switch: 
 #if: 488
 #if: 
 |{{
 #switch: 
 #if: 488
 #if: 
 |&rft.pages={{urlencode: {{
 #if: Page%3AOrigin_of_Species_1859_facsimile.djvu/500The Origin of Species
 #if: 
 #if: 
 #switch: 
 #if: 488
 #if: 
 |{{
 #switch: 
 #if: 488
 #if: 
 }}{{
 }}&rfr_id=info:sid/en.wikipedia.org:{{FULLPAGENAMEE}}"> 
 |IncludedWorkTitle = 
 |IncludedWorkURL = 
 |Other = 
 |Edition = 
 |Place = 
 |PublicationPlace = 
 |Publisher = Wikisource
 |PublicationDate = 
 |EditorSurname1 = 
 |EditorSurname2 = 
 |EditorSurname3 = 
 |EditorSurname4 = 
 |EditorGiven1 = 
 |EditorGiven2=
 |EditorGiven3=
 |EditorGiven4=
 |Editorlink1=
 |Editorlink2=
 |Editorlink3=
 |Editorlink4=
 |language = 
 |format = 
 |ARXIV=
 |ASIN=
 |BIBCODE=
 |DOI=
 |DoiBroken=
 |ISBN=
 |ISSN=
 |JFM=
 |JSTOR=
 |LCCN=
 |MR=
 |OCLC=
 |OL=
 |OSTI=
 |PMC=
 |Embargo=1010-10-10
 |PMID=
 |RFC=
 |SSRN=
 |ZBL=
 |ID=
 |AccessDate=
 |DateFormat=none
 |quote = 
 |laysummary = 
 |laydate = 
 |Ref=
 |Sep = .
 |PS = .
 |AuthorSep = ; 
 |NameSep = , 
 |Trunc = 8
 |amp = 
}}{{#if:
Two of his later books were devoted to the study of animal emotions and psychology; "The Descent of Man, and Selection in Relation to Sex" in 1871 and "The Expression of the Emotions in Man and Animals" in 1872. Darwin's work inspired William James's functionalist approach to psychology. Darwin's theories of evolution, adaptation, and natural selection have provided insight into why brains function the way they do.
The content of EP has derived from, on one hand, the biological sciences (especially evolutionary theory as it relates to ancient human environments, the study of paleoanthropology and animal behavior) and, on the other, the human sciences, especially psychology.
Evolutionary biology as an academic discipline emerged with the modern evolutionary synthesis in the 1930s and 1940s. In the 1930s the study of animal behavior (ethology) emerged with the work of Dutch biologist Nikolaas Tinbergen and Austrian biologists Konrad Lorenz and Karl von Frisch.
W.D. Hamilton's (1964) papers on inclusive fitness and Robert Trivers's (1972) theories on reciprocity and parental investment helped to establish evolutionary thinking in psychology and the other social sciences. In 1975, Edward O. Wilson combined evolutionary theory with studies of animal and social behavior, building on the works of Lorenz and Tinbergen, in his book "".
In the 1970s, two major branches developed from ethology. Firstly, the study of animal "social" behavior (including humans) generated sociobiology, defined by its pre-eminent proponent Edward O. Wilson in 1975 as "the systematic study of the biological basis of all social behavior" and in 1978 as "the extension of population biology and evolutionary theory to social organization." Secondly, there was behavioral ecology which placed less emphasis on "social" behavior by focusing on the ecological and evolutionary basis of both animal and human behavior.
In the 1970s and 1980s university departments began to include the term "evolutionary biology" in their titles. The modern era of evolutionary psychology was ushered in, in particular, by Donald Symons' 1979 book "The Evolution of Human Sexuality" and Leda Cosmides and John Tooby's 1992 book "The Adapted Mind".
From psychology there are the primary streams of developmental, social and cognitive psychology. Establishing some measure of the relative influence of genetics and environment on behavior has been at the core of behavioral genetics and its variants, notably studies at the molecular level that examine the relationship between genes, neurotransmitters and behavior. Dual inheritance theory (DIT), developed in the late 1970s and early 1980s, has a slightly different perspective by trying to explain how human behavior is a product of two different and interacting evolutionary processes: genetic evolution and cultural evolution. DIT is seen by some as a "middle-ground" between views that emphasize human universals versus those that emphasize cultural variation.
Theoretical foundations.
The theories on which evolutionary psychology is based originated with Charles Darwin's work, including his speculations about the evolutionary origins of social instincts in humans. Modern evolutionary psychology, however, is possible only because of advances in evolutionary theory in the 20th century.
Evolutionary psychologists say that natural selection has provided humans with many psychological adaptations, in much the same way that it generated humans' anatomical and physiological adaptations. As with adaptations in general, psychological adaptations are said to be specialized for the environment in which an organism evolved, the environment of evolutionary adaptedness, or EEA. Sexual selection provides organisms with adaptations related to mating. For male mammals, which have a relatively high maximal potential reproduction rate, sexual selection leads to adaptations that help them compete for females. For female mammals, with a relatively low maximal potential reproduction rate, sexual selection leads to choosiness, which helps females select higher quality mates. Charles Darwin described both natural selection and sexual selection, and he relied on group selection to explain the evolution of altruistic (self-sacrificing) behavior. But group selection was considered a weak explanation, because in any group the less altruistic individuals will be more likely to survive, and the group will become less self-sacrificing as a whole.
In 1964, William D. Hamilton proposed inclusive fitness theory, emphasizing a "gene's-eye" view of evolution. Hamilton noted that genes can increase the replication of copies of themselves into the next generation by influencing the organism's social traits in such a way that (statistically) results in helping the survival and reproduction of other copies of the same genes (most simply, identical copies in the organism's close relatives). According to "Hamilton's rule", self-sacrificing behaviors (and the genes influencing them) can evolve if they typically help the organism's close relatives so much that it more than compensates for the individual animal's sacrifice. Inclusive fitness theory resolved the issue of how "altruism" can evolve. Other theories also help explain the evolution of altruistic behavior, including evolutionary game theory, tit-for-tat reciprocity, and generalized reciprocity. These theories not only help explain the development of altruistic behavior, but also account for hostility toward cheaters (individuals that take advantage of others' altruism).
Several mid-level evolutionary theories inform evolutionary psychology. The r/K selection theory proposes that some species prosper by having many offspring, while others follow the strategy of having fewer offspring but investing much more in each one. Humans follow the second strategy. Parental investment theory explains how parents invest more or less in individual offspring based on how successful those offspring are likely to be, and thus how much they might improve the parents' inclusive fitness. According to the Trivers-Willard hypothesis, parents in good conditions tend to invest more in sons (who are best able to take advantage of good conditions), while parents in poor conditions tend to invest more in daughters (who are best able to have successful offspring even in poor conditions). According to life history theory, animals evolve life histories to match their environments, determining details such as age at first reproduction and number of offspring. Dual inheritance theory posits that genes and human culture have interacted, with genes affecting the development of culture, and culture, in turn, affecting human evolution on a genetic level (see also the Baldwin effect).
Evolved psychological mechanisms.
Evolutionary psychology is based on the hypothesis that, just like hearts, lungs, livers, kidneys, and immune systems, cognition has functional structure that has a genetic basis, and therefore has evolved by natural selection. Like other organs and tissues, this functional structure should be universally shared amongst a species, and should solve important problems of survival and reproduction.
Evolutionary psychologists seek to understand psychological mechanisms by understanding the survival and reproductive functions they might have served over the course of evolutionary history. These might include abilities to infer others' emotions, discern kin from non-kin, identify and prefer healthier mates, cooperate with others and follow leaders. Consistent with the theory of natural selection, evolutionary psychology sees humans as often in conflict with others, including mates and relatives. For instance, a mother may wish to wean her offspring from breastfeeding earlier than does her infant, which frees up the mother to invest in additional offspring. Evolutionary psychology also recognizes the role of kin selection and reciprocity in evolving prosocial traits such as altruism. Like chimps and bonobos, humans have subtle and flexible social instincts, allowing them to form extended families, lifelong friendships, and political alliances. In studies testing theoretical predictions, evolutionary psychologists have made modest findings on topics such as infanticide, intelligence, marriage patterns, promiscuity, perception of beauty, bride price and parental investment.
Products of evolution: adaptations, exaptations, byproducts, and random variation.
Not all traits of organisms are adaptations. As noted in the table below, traits may also be exaptations, byproducts of adaptations (sometimes called "spandrels"), or random variation between individuals.
Psychological adaptations are hypothesized to be innate or relatively easy to learn, and to manifest in cultures worldwide. For example, the ability of toddlers to learn a language with virtually no training is likely to be a psychological adaptation. On the other hand, ancestral humans did not read or write, thus today, learning to read and write require extensive training, and presumably represent byproducts of cognitive processing that use psychological adaptations designed for other functions. However, variations in manifest behavior can result from universal mechanisms interacting with different local environments. For example, Caucasians who move from a northern climate to the equator will have darker skin. The mechanisms regulating their pigmentation do not change; rather the input to the those mechanisms change, resulting in different output.
One of the tasks of evolutionary psychology is to identify which psychological traits are likely to be adaptations, byproducts or random variation. George C Williams suggested that an "adaptation is a special and onerous concept that should only be used where it is really necessary." As noted by Williams and others, adaptations can be identified by their improbable complexity, species universality, and adaptive functionality.
Obligate and facultative adaptations.
A question that may be asked about an adaptation is whether it is generally obligate (relatively robust in the face of typical environmental variation) or facultative (sensitive to typical environmental variation). The sweet taste of sugar and the pain of hitting one's knee against concrete are the result of fairly obligate psychological adaptations; typical environmental variability during development does not much affect their operation. By contrast, facultative adaptations are somewhat like "if-then" statements. For example, adult attachment style seems particularly sensitive to early childhood experiences. As adults, the propensity to develop close, trusting bonds with others is dependent on whether early childhood caregivers could be trusted to provide reliable assistance and attention. The adaptation for skin to tan is conditional to exposure to sunlight; this is an example of another facultative adaptation. When a psychological adaptation is facultative, evolutionary psychologists concern themselves with how developmental and environmental inputs influence the expression of the adaptation.
Cultural universals.
Evolutionary psychologists hold that behaviors or traits that occur universally in all cultures are good candidates for evolutionary adaptations. Cultural universals include behaviors related to language, cognition, social roles, gender roles, and technology. Evolved psychological adaptations (such as the ability to learn a language) interact with cultural inputs to produce specific behaviors (e.g., the specific language learned). Basic gender differences, such as greater eagerness for sex among men and greater coyness among women, are explained as sexually dimorphic psychological adaptations that reflect the different reproductive strategies of males and females. Evolutionary psychologists contrast their approach to what they term the "standard social science model," according to which the mind is a general-purpose cognition device shaped almost entirely by culture.
Environment of evolutionary adaptedness.
EP argues that to properly understand the functions of the brain, one must understand the properties of the environment in which the brain evolved. That environment is often referred to as the "environment of evolutionary adaptedness" (EEA).
The idea of an "environment of evolutionary adaptedness" was first explored as a part of attachment theory by John Bowlby. This is the environment to which a particular evolved mechanism is adapted. More specifically, the EEA is defined as the set of historically recurring selection pressures that formed a given adaptation, as well as those aspects of the environment that were necessary for the proper development and functioning of the adaptation.
Humans, comprising the genus "Homo", appeared between 1.5 and 2.5 million years ago, a time that roughly coincides with the start of the Pleistocene 2.6 million years ago. Because the Pleistocene ended a mere 12,000 years ago, most human adaptations either newly evolved during the Pleistocene, or were maintained by stabilizing selection during the Pleistocene. Evolutionary psychology therefore proposes that the majority of human psychological mechanisms are adapted to reproductive problems frequently encountered in Pleistocene environments. In broad terms, these problems include those of growth, development, differentiation, maintenance, mating, parenting, and social relationships.
The EEA is significantly different from modern society. The ancestors of modern humans lived in smaller groups, had more cohesive cultures, and had more stable and rich contexts for identity and meaning. Researchers look to existing hunter-gatherer societies for clues as to how hunter-gatherers lived in the EEA. Unfortunately, the few surviving hunter-gatherer societies are different from each other, and they have been pushed out of the best land and into harsh environments, so it is not clear how closely they reflect ancestral culture.
Evolutionary psychologists sometimes look to chimpanzees, bonobos, and other great apes for insight into human ancestral behavior. Christopher Ryan and Cacilda Jetha argue that evolutionary psychologists have overemphasized the similarity of humans and chimps, which are more violent, while underestimating the similarity of humans and bonobos, which are more peaceful.
Mismatches.
Since an organism's adaptations were suited to its ancestral environment, a new and different environment can create a mismatch. Because humans are mostly adapted to Pleistocene environments, psychological mechanisms sometimes exhibit "mismatches" to the modern environment. One example is the fact that although about 10,000 people are killed with guns in the US annually, whereas spiders and snakes kill only a handful, people nonetheless learn to fear spiders and snakes about as easily as they do a pointed gun, and more easily than an unpointed gun, rabbits or flowers. A potential explanation is that spiders and snakes were a threat to human ancestors throughout the Pleistocene, whereas guns (and rabbits and flowers) were not. There is thus a mismatch between humans' evolved fear-learning psychology and the modern environment.
This mismatch also shows up in the phenomena of the supernormal stimulus, a stimulus that elicits a response more strongly than the stimulus for which the response evolved. The term was coined by Niko Tinbergen to refer to non-human animal behavior, but psychologist Deirdre Barrett said that supernormal stimulation governs the behavior of humans as powerfully as that of other animals. She explained junk food as an exaggerated stimulus to cravings for salt, sugar, and fats, and she says that television is an exaggeration of social cues of laughter, smiling faces and attention-grabbing action. Magazine centerfolds and double cheeseburgers pull instincts intended for an EEA where breast development was a sign of health, youth and fertility in a prospective mate, and fat was a rare and vital nutrient.
Psychologist Mark van Vugt recently argued that modern organizational leadership is a mismatch. His argument is that humans are not adapted to work in large, anonymous bureaucratic structures with formal hierarchies. The human mind still responds to personalized, charismatic leadership primarily in the context of informal, egalitarian settings. Hence the dissatisfaction and alienation that many employees experience. Salaries, bonuses and other privileges exploit instincts for relative status, which attract particularly males to senior executive positions.
Research methods.
Evolutionary theory is heuristic in that it may generate hypotheses that might not be developed from other theoretical approaches. One of the major goals of adaptationist research is to identify which organismic traits are likely to be adaptations, and which are byproducts or random variations. As noted earlier, adaptations are expected to show evidence of complexity, functionality, and species universality, while byproducts or random variation will not. In addition, adaptations are expected to manifest as proximate mechanisms that interact with the environment in either a generally obligate or facultative fashion (see above). Evolutionary psychologists are also interested in identifying these proximate mechanisms (sometimes termed "mental mechanisms" or "psychological adaptations") and what type of information they take as input, how they process that information, and their outputs. Evolutionary developmental psychology, or "evo-devo," focuses on how adaptations may be activated at certain developmental times (e.g., losing baby teeth, adolescence, etc.) or how events during the development of an individual may alter life history trajectories.
Evolutionary psychologists use several strategies to develop and test hypotheses about whether a psychological trait is likely to be an evolved adaptation. Buss (2011) notes that these methods include:
Evolutionary psychologists also use various sources of data for testing, including experiments, archaeological records, data from hunter-gatherer societies, observational studies, neuroscience data, self-reports and surveys, public records, and human products.
Recently, additional methods and tools have been introduced based on fictional scenarios, mathematical models, and multi-agent computer simulations.
Major areas of research.
Foundational areas of research in evolutionary psychology can be divided into broad categories of adaptive problems that arise from the theory of evolution itself: survival, mating, parenting, family and kinship, interactions with non-kin, and cultural evolution.
Survival and individual level psychological adaptations.
Problems of survival are thus clear targets for the evolution of physical and psychological adaptations. Major problems the ancestors of present day humans faced included food selection and acquisition; territory selection and physical shelter; and avoiding predators and other environmental threats.
Consciousness.
Consciousness meets George Williams' criteria of species universality, complexity, and functionality, and it is a trait that apparently increases fitness.
In his paper "Evolution of consciousness," John Eccles argues that special anatomical and physical adaptations of the mammalian cerebral cortex gave rise to consciousness. In contrast, others have argued that the recursive circuitry underwriting consciousness is much more primitive, having evolved initially in pre-mammalian species because it improves the capacity for interaction with both social "and" natural environments by providing an energy-saving "neutral" gear in an otherwise energy-expensive motor output machine. Once in place, this recursive circuitry may well have provided a basis for the subsequent development of many of the functions that consciousness facilitates in higher organisms, as outlined by Bernard J. Baars. Richard Dawkins suggested that humans evolved consciousness in order to make themselves the subjects of thought. Daniel Povinelli suggests that large, tree-climbing apes evolved consciousness to take into account one's own mass when moving safely among tree branches. Consistent with this hypothesis, Gordon Gallup found that chimps and orangutans, but not little monkeys or terrestrial gorillas, demonstrated self-awareness in mirror tests.
The concept of consciousness can refer to voluntary action, awareness, or wakefulness. However, even voluntary behavior involves unconscious mechanisms. Many cognitive processes take place in the cognitive unconscious, unavailable to conscious awareness. Some behaviors are conscious when learned but then become unconscious, seemingly automatic. Learning, especially implicitly learning a skill, can take place outside of consciousness. For example, plenty of people know how to turn right when they ride a bike, but very few can accurately explain how they actually do so. Evolutionary psychology approaches self-deception as an adaptation that can improve one's results in social exchanges.
Sleep may have evolved to conserve energy when activity would be less fruitful or more dangerous, such as at night, especially in winter.
Sensation and perception.
Many experts, such as Jerry Fodor, write that the purpose of perception is knowledge, but evolutionary psychologists hold that its primary purpose is to guide action. For example, they say, depth perception seems to have evolved not to help us know the distances to other objects but rather to help us move around in space. Evolutionary psychologists say that animals from fiddler crabs to humans use eyesight for collision avoidance, suggesting that vision is basically for directing action, not providing knowledge.
Building and maintaining sense organs is metabolically expensive, so these organs evolve only when they improve an organism's fitness. More than half the brain is devoted to processing sensory information, and the brain itself consumes roughly one-fourth of one's metabolic resources, so the senses must provide exceptional benefits to fitness. Perception accurately mirrors the world; animals get useful, accurate information through their senses.
Scientists who study perception and sensation have long understood the human senses as adaptations. Depth perception consists of processing over half a dozen visual cues, each of which is based on a regularity of the physical world. Vision evolved to respond to the narrow range of electromagnetic energy that is plentiful and that does not pass through objects. Sound waves go around corners and interact with obstacles, creating a complex pattern that includes useful information about the sources of and distances to objects. Larger animals naturally make lower-pitched sounds as a consequence of their size. The range over which an animal hears, on the other hand, is determined by adaptation. Homing pigeons, for example, can hear very low-pitched sound (infrasound) that carries great distances, even though most smaller animals detect higher-pitched sounds. Taste and smell respond to chemicals in the environment that are thought to have been significant for fitness in the EEA. For example, salt and sugar were apparently both valuable to the human or pre-human inhabitants of the EEA, so present day humans have an intrinsic hunger for salty and sweet tastes. The sense of touch is actually many senses, including pressure, heat, cold, tickle, and pain. Pain, while unpleasant, is adaptive. An important adaptation for senses is range shifting, by which the organism becomes temporarily more or less sensitive to sensation. For example, one's eyes automatically adjust to dim or bright ambient light. Sensory abilities of different organisms often coevolve, as is the case with the hearing of echolocating bats and that of the moths that have evolved to respond to the sounds that the bats make.
Evolutionary psychologists contend that perception demonstrates the principle of modularity, with specialized mechanisms handling particular perception tasks. For example, people with damage to a particular part of the brain suffer from the specific defect of not being able to recognize faces (prosopagnosia). EP suggests that this indicates a so-called face-reading module.
Learning and facultative adaptations.
In evolutionary psychology, learning is said to be accomplished through evolved capacities, specifically facultative adaptations. Facultative adaptations express themselves differently depending on input from the environment. Sometimes the input comes during development and helps shape that development. For example, migrating birds learn to orient themselves by the stars during a critical period in their maturation. Evolutionary psychologists believe that humans also learn language along an evolved program, also with critical periods. The input can also come during daily tasks, helping the organism cope with changing environmental conditions. For example, animals evolved Pavlovian conditioning in order to solve problems about causal relationships. Animals accomplish learning tasks most easily when those tasks resemble problems that they faced in their evolutionary past, such as a rat learning where to find food or water. Learning capacities sometimes demonstrate differences between the sexes. In many animal species, for example, males can solve spatial problem faster and more accurately than females, due to the effects of male hormones during development. The same might be true of humans.
Emotion and motivation.
Motivations direct and energize behavior, while emotions provide the affective component to motivation, positive or negative. In the early 1970s, Paul Ekman and colleagues began a line of research which suggests that many emotions are universal. He found evidence that humans share at least five basic emotions: fear, sadness, happiness, anger, and disgust. Social emotions evidently evolved to motivate social behaviors that were adaptive in the EEA. For example, spite seems to work against the individual but it can establish an individual's reputation as someone to be feared. Shame and pride can motivate behaviors that help one maintain one's standing in a community, and self-esteem is one's estimate of one's status.
Motivation has a neurobiologial basis in the reward system of the brain. Recently, it has been suggested that reward systems may evolve in such a way that there may be an inherent or unavoidable trade-off in the motivational system for activities of short versus long duration.
Cognition.
Cognition refers to internal representations of the world and internal information processing. From an EP perspective, cognition is not "general purpose," but uses heuristics, or strategies, that generally increase the likelihood of solving problems that the ancestors of present day humans routinely faced. For example, present day humans are far more likely to solve logic problems that involve detecting cheating (a common problem given humans' social nature) than the same logic problem put in purely abstract terms. Since the ancestors of present day humans did not encounter truly random events, present day humans may be cognitively predisposed to incorrectly identify patterns in random sequences. "Gamblers' Fallacy" is one example of this. Gamblers may falsely believe that they have hit a "lucky streak" even when each outcome is actually random and independent of previous trials. Most people believe that if a fair coin has been flipped 9 times and Heads appears each time, that on the tenth flip, there is a greater than 50% chance of getting Tails. Humans find it far easier to make diagnoses or predictions using frequency data than when the same information is presented as probabilities or percentages, presumably because the ancestors of present day humans lived in relatively small tribes (usually with fewer than 150 people) where frequency information was more readily available.
Personality.
Evolutionary psychology is primarily interested in finding commonalities between people, or basic human psychological nature. From an evolutionary perspective, the fact that people have fundamental differences in personality traits initially presents something of a puzzle. (Note: The field of behavioral genetics is concerned with statistically partitioning differences between people into genetic and environmental sources of variance. However, understanding the concept of heritability can be tricky—heritability refers only to the differences between people, never the degree to which the traits of an individual are due to environmental or genetic factors, since traits are always a complex interweaving of both.)
Personality traits are conceptualized by evolutionary psychologists as due to normal variation around an optimum, due to frequency-dependent selection (behavioral polymorphisms), or as facultative adaptations. Like variability in height, some personality traits may simply reflect inter-individual variability around a general optimum. Or, personality traits may represent different genetically predisposed "behavioral morphs" – alternate behavioral strategies that depend on the frequency of competing behavioral strategies in the population. For example, if most of the population is generally trusting and gullible, the behavioral morph of being a "cheater" (or, in the extreme case, a sociopath) may be advantageous. Finally, like many other psychological adaptations, personality traits may be facultative—sensitive to typical variations in the social environment, especially during early development. For example, later born children are more likely than first borns to be rebellious, less conscientious and more open to new experiences, which may be advantageous to them given their particular niche in family structure. It is important to note that shared environmental influences do play a role in personality and are not always of less importance than genetic factors. However, shared environmental influences often decrease to near zero after adolescence but do not completely disappear.
Language.
According to Steven Pinker, who builds on the work by Noam Chomsky, the universal human ability to learn to talk between the ages of 1 – 4, basically without training, suggests that language acquisition is a distinctly human psychological adaptation (see, in particular, Pinker's "The Language Instinct"). Pinker and Bloom (1990) argue that language as a mental faculty shares many likenesses with the complex organs of the body which suggests that, like these organs, language has evolved as an adaptation, since this is the only known mechanism by which such complex organs can develop.
Pinker follows Chomsky in arguing that the fact that children can learn any human language with no explicit instruction suggests that language, including most of grammar, is basically innate and that it only needs to be activated by interaction. Chomsky himself does not believe language to have evolved as an adaptation, but suggests that it likely evolved as a byproduct of some other adaptation, a so-called spandrel. But Pinker and Bloom argue that the organic nature of language strongly suggests that it has an adaptational origin.
Evolutionary psychologists hold that the FOXP2 gene may well be associated with the evolution of human language. In the 1980s, psycholinguist Myrna Gropnik identified a dominant gene that causes language impairment in the KE family of Britain. This gene turned out to be a mutation of the FOXP2 gene. Humans have a unique allele of this gene, which has otherwise been closely conserved through most of mammalian evolutionary history. This unique allele seems to have first appeared between 100 and 200 thousand years ago, and it is now all but universal in humans. However, the once-popular idea that FOXP2 is a 'grammar gene' or that it triggered the emergence of language in "Homo sapiens" is now widely discredited.
Currently several competing theories about the evolutionary origin of language coexist, none of them having achieved a general consensus. Researchers of language acquisition in primates and humans such as Michael Tomasello and Talmy Givón, argue that the innatist framework has understated the role of imitation in learning and that it is not at all necessary to posit the existence of an innate grammar module to explain human language acquisition. Tomasello argues that studies of how children and primates actually acquire communicative skills suggests that humans learn complex behavior through experience, so that instead of a module specifically dedicated to language acquisition, language is acquired by the same cognitive mechanisms that are used to acquire all other kinds of socially transmitted behavior.
On the issue of whether language is best seen as having evolved as an adaptation or as a spandrel, evolutionary biologist W. Tecumseh Fitch, following Stephen J. Gould, argues that it is unwarranted to assume that every aspect of language is an adaptation, or that language as a whole is an adaptation. He criticizes some strands of evolutionary psychology for suggesting a pan-adaptionist view of evolution, and dismisses Pinker and Bloom's question of whether "Language has evolved as an adaptation" as being misleading. He argues instead that from a biological viewpoint the evolutionary origins of language is best conceptualized as being the probable result of a convergence of many separate adaptations into a complex system. A similar argument is made by Terrence Deacon who in "The Symbolic Species" argues that the different features of language have co-evolved with the evolution of the mind and that the ability to use symbolic communication is integrated in all other cognitive processes.
If the theory that language could have evolved as a single adaptation is accepted, the question becomes which of its many functions has been the basis of adaptation, several evolutionary hypotheses have been posited: that it evolved for the purpose of social grooming, that it evolved to as a way to show mating potential or that it evolved to form social contracts. Evolutionary psychologists recognize that these theories are all speculative and that much more evidence is required to understand how language might have been selectively adapted.
Mating.
Given that sexual reproduction is the means by which genes are propagated into future generations, sexual selection plays a large role in human evolution. Human mating, then, is of interest to evolutionary psychologists who aim to investigate evolved mechanisms to attract and secure mates. Several lines of research have stemmed from this interest, such as studies of mate selection mate poaching, mate retention, mating preferences and conflict between the sexes.
In 1972 Robert Trivers published an influential paper on sex differences that is now referred to as parental investment theory. The size differences of gametes (anisogamy) is the fundamental, defining difference between males (small gametes—sperm) and females (large gametes—ova). Trivers noted that anisogamy typically results in different levels of parental investment between the sexes, with females initially investing more. Trivers proposed that this difference in parental investment leads to the sexual selection of different reproductive strategies between the sexes and to sexual conflict. For example, he suggested that the sex that invests less in offspring will generally compete for access to the higher-investing sex to increase their inclusive fitness (also see Bateman's principle<ref name=doi10.1038/hdy.1948.21>Error: Bad DOI specified: 10.1038/hdy.1948.21</ref>). Trivers posited that differential parental investment led to the evolution sexual dimorphisms in mate choice, intra- and inter- sexual reproductive competition, and courtship displays. In mammals, including humans, females make a much larger parental investment than males (i.e. gestation followed by childbirth and lactation). Parental investment theory is a branch of life history theory.
Buss and Schmitt's (1993) "Sexual Strategies Theory" proposed that, due to differential parental investment, humans have evolved sexually dimorphic adaptations related to "sexual accessibility, fertility assessment, commitment seeking and avoidance, immediate and enduring resource procurement, paternity certainty, assessment of mate value, and parental investment." Their "Strategic Interference Theory" suggested that conflict between the sexes occurs when the preferred reproductive strategies of one sex interfere with those of the other sex, resulting in the activation of emotional responses such as anger or jealousy.
Women are generally more selective when choosing mates, especially under short-term mating conditions. However, under some circumstances, short term mating can provide benefits to women as well, such as fertility insurance, trading up to better genes, reducing risk of inbreeding, and insurance protection of her offspring.
Due to male paternity insecurity, sex differences have been found in such domains as sexual jealousy. Females generally react more adversely to emotional infidelity and males will react more to sexual infidelity. This particular pattern is predicted because the costs involved in mating for each sex are distinct. Women, on average, should prefer a mate who can offer resources (e.g., financial, commitment), thus, a woman risks losing such resources with a mate who commits emotional infidelity. Men, on the other hand, are never certain of the genetic paternity of their children because they do not bear the offspring themselves ("paternity insecurity"). This suggests that for men sexual infidelity would generally be more aversive than emotional infidelity because investing resources in another man's offspring does not lead to propagation of their own genes.
Another interesting line of research is that which examines women's mate preferences across the ovulatory cycle. The theoretical underpinning of this research is that ancestral women would have evolved mechanisms to select mates with certain traits depending on their hormonal status. For example, the theory hypothesizes that, during the ovulatory phase of a woman's cycle (approximately days 10–15 of a woman's cycle), a woman who mated with a male with high genetic quality would have been more likely, on average, to produce and rear a healthy offspring than a woman who mated with a male with low genetic quality. These putative preferences are predicted to be especially apparent for short-term mating domains because a potential male mate would only be offering genes to a potential offspring. This hypothesis allows researchers to examine whether women select mates who have characteristics that indicate high genetic quality during the high fertility phase of their ovulatory cycles. Indeed, studies have shown that women's preferences vary across the ovulatory cycle. In particular, Haselton and Miller (2006) showed that highly fertile women prefer creative but poor men as short-term mates. Creativity may be a proxy for good genes. Research by Gangestad et al. (2004) indicates that highly fertile women prefer men who display social presence and intrasexual competition; these traits may act as cues that would help women predict which men may have, or would be able to acquire, resources.
Parenting.
Reproduction is always costly for women, and can also be for men. Individuals are limited in the degree to which they can devote time and resources to producing and raising their young, and such expenditure may also be detrimental to their future condition, survival and further reproductive output.
Parental investment is any parental expenditure (time, energy etc.) that benefits one offspring at a cost to parents' ability to invest in other components of fitness (Clutton-Brock 1991: 9; Trivers 1972). Components of fitness (Beatty 1992) include the well being of existing offspring, parents' future reproduction, and inclusive fitness through aid to kin (Hamilton, 1964). Parental investment theory is a branch of life history theory.
Robert Trivers' theory of parental investment predicts that the sex making the largest investment in lactation, nurturing and protecting offspring will be more discriminating in mating and that the sex that invests less in offspring will compete for access to the higher investing sex (see Bateman's principle). Sex differences in parental effort are important in determining the strength of sexual selection.
The benefits of parental investment to the offspring are large and are associated with the effects on condition, growth, survival and ultimately, on reproductive success of the offspring. However, these benefits can come at the cost of parent's ability to reproduce in the future e.g. through the increased risk of injury when defending offspring against predators, the loss of mating opportunities whilst rearing offspring and an increase in the time to the next reproduction. Overall, parents are selected to maximize the difference between the benefits and the costs, and parental care will be likely to evolve when the benefits exceed the costs.
The Cinderella effect is an alleged high incidence of stepchildren being physically, emotionally or sexually abused, neglected, murdered, or otherwise mistreated at the hands of their stepparents at significantly higher rates than their genetic counterparts. It takes its name from the fairy tale character Cinderella, who in the story was cruelly mistreated by her stepmother and stepsisters. Daly and Wilson (1996) noted: "Evolutionary thinking led to the discovery of the most important risk factor for child homicide – the presence of a stepparent. Parental efforts and investments are valuable resources, and selection favors those parental psyches that allocate effort effectively to promote fitness. The adaptive problems that challenge parental decision making include both the accurate identification of one's offspring and the allocation of one's resources among them with sensitivity to their needs and abilities to convert parental investment into fitness increments…. Stepchildren were seldom or never so valuable to one's expected fitness as one's own offspring would be, and those parental psyches that were easily parasitized by just any appealing youngster must always have incurred a selective disadvantage"(Daly & Wilson, 1996, pp. 64–65). However, they note that not all stepparents will "want" to abuse their partner's children, or that genetic parenthood is any insurance against abuse. They see step parental care as primarily "mating effort" towards the genetic parent.
Family and kin.
Inclusive fitness is the sum of an organism's classical fitness (how many of its own offspring it produces and supports) and the number of equivalents of its own offspring it can add to the population by supporting others. The first component is called classical fitness by Hamilton (1964).
From the gene's point of view, evolutionary success ultimately depends on leaving behind the maximum number of copies of itself in the population. Until 1964, it was generally believed that genes only achieved this by causing the individual to leave the maximum number of viable offspring. However, in 1964 W. D. Hamilton proved mathematically that, because close relatives of an organism share some identical genes, a gene can also increase its evolutionary success by promoting the reproduction and survival of these related or otherwise similar individuals. Hamilton concluded that this leads natural selection to favor organisms that would behave in ways that maximize their inclusive fitness. It is also true that natural selection favors behavior that maximizes personal fitness.
Hamilton's rule describes mathematically whether or not a gene for altruistic behavior will spread in a population:
where
The concept serves to explain how natural selection can perpetuate altruism. If there is an '"altruism gene"' (or complex of genes) that influences an organism's behavior to be helpful and protective of relatives and their offspring, this behavior also increases the proportion of the altruism gene in the population, because relatives are likely to share genes with the altruist due to common descent. Altruists may also have some way to recognize altruistic behavior in unrelated individuals and be inclined to support them. As Dawkins points out in "The Selfish Gene" (Chapter 6) and "The Extended Phenotype", this must be distinguished from the green-beard effect.
Although it is generally true that humans tend to be more altruistic toward their kin than toward non-kin, the relevant proximate mechanisms that mediate this cooperation have been debated (see kin recognition), with some arguing that kin status is determined primarily via social and cultural factors (such as co-residence, maternal association of sibs, etc.), while others have argued that kin recognition can also mediated by biological factors such as facial resemblance and immunogenetic similarity of the major histocompatibility complex (MHC). For a discussion of the interaction of these social and biological kin recognition factors see Lieberman, Tooby, and Cosmides (2007) ().
Whatever the proximate mechanisms of kin recognition there is substantial evidence that humans act generally more altruistically to close genetic kin compared to genetic non-kin.
Interactions with non-kin / reciprocity.
Although interactions with non-kin are generally less altruistic compared to those with kin, cooperation can be maintained with non-kin via mutually beneficial reciprocity as was proposed by Robert Trivers. If there are repeated encounters between the same two players in an evolutionary game in which each of them can choose either to "cooperate" or "defect," then a strategy of mutual cooperation may be favored even if it pays each player, in the short term, to defect when the other cooperates. Direct reciprocity can lead to the evolution of cooperation only if the probability, w, of another encounter between the same two individuals exceeds the cost-to-benefit ratio of the altruistic act:
Reciprocity can also be indirect if information about previous interactions is shared. Reputation allows evolution of cooperation by indirect reciprocity. Natural selection favors strategies that base the decision to help on the reputation of the recipient: studies show that people who are more helpful are more likely to receive help. The calculations of indirect reciprocity are complicated and only a tiny fraction of this universe has been uncovered, but again a simple rule has emerged. Indirect reciprocity can only promote cooperation if the probability, q, of knowing someone’s reputation exceeds the cost-to-benefit ratio of the altruistic act:
One important problem with this explanation is that individuals may be able to evolve the capacity to obscure their reputation, reducing the probability, q, that it will be known.
Trivers argues that friendship and various social emotions evolved in order to manage reciprocity. Liking and disliking, he says, evolved to help present day humans' ancestors form coalitions with others who reciprocated and to exclude those who did not reciprocate. Moral indignation may have evolved to prevent one's altruism from being exploited by cheaters, and gratitude may have motivated present day humans' ancestors to reciprocate appropriately after benefiting from others' altruism. Likewise, present day humans feel guilty when they fail to reciprocate. These social motivations match what evolutionary psychologists expect to see in adaptations that evolved to maximize the benefits and minimize the drawbacks of reciprocity.
Evolutionary psychologists say that humans have psychological adaptations that evolved specifically to help us identify nonreciprocators, commonly referred to as "cheaters." In 1993, Robert Frank and his associates found that participants in a prisoner's dilemma scenario were often able to predict whether their partners would "cheat," based on a half hour of unstructured social interaction. In a 1996 experiment, for example, Linda Mealey and her colleagues found that people were better at remembering the faces of people when those faces were associated with stories about those individuals cheating (such as embezzling money from a church).
Strong reciprocity (or "tribal reciprocity").
Humans may have an evolved set of psychological adaptations that predispose them to be more cooperative than otherwise would be expected with members of their tribal in-group, and, more nasty to members of tribal out groups. These adaptations may have be a consequent of tribal warfare. Humans may also have predispositions for "altruistic punishment"—to punish in-group members who violate in-group rules, even when this altruistic behavior cannot be justified in terms of helping those you are related to (kin selection), cooperating with those who you will interact with again (direct reciprocity), or cooperating to better your reputation with others (indirect reciprocity).
Evolution and culture.
Memetics is a theory of mental content based on an analogy with evolution, originating from Richard Dawkins' 1976 book "The Selfish Gene." It purports to be an approach to evolutionary models of cultural information transfer. A meme, analogous to a gene, is essentially a "unit of culture"—an idea, belief, pattern of behavior, etc. which is "hosted" in one or more individual minds, and which can reproduce itself from mind to mind. Thus what would otherwise be regarded as one individual influencing another to adopt a belief is seen memetically as a meme reproducing itself. As with genetics, particularly under Dawkins's interpretation, a meme's success may be due to its contribution to the effectiveness of its host. Memetics is notable for sidestepping the traditional concern with the "truth" of ideas and beliefs.
Susan Blackmore (2002) re-stated the definition of meme as: whatever is copied from one person to another person, whether habits, skills, songs, stories, or any other kind of information. Further she said that memes, like genes, are replicators in the sense as defined by Dawkins. That is, they are information that is copied. Memes are copied by imitation, teaching and other methods. The copies are not perfect: memes are copied with variation; moreover, memes compete for humans' limited memory capacity and for the chance to be copied again. Only some of the variants can survive. The combination of these three elements (copies; variation; competition for survival) forms precisely the condition for Darwinian evolution, and so memes (and hence human cultures) evolve. Large groups of memes that are copied and passed on together are called co-adapted meme complexes, or "memeplexes". In her definition, the way that a meme replicates is through imitation.
Dual inheritance theory (DIT), also known as gene-culture coevolution, suggests that cultural information and genes co-evolve. Marcus Feldman and Luigi Luca Cavalli-Sforza (1976) published perhaps the first dynamic models of gene-culture coevolution. These models were to form the basis for subsequent work on DIT, heralded by the publication of three seminal books in 1980 and 1981. Charles Lumsden and E.O. Wilson's "Genes, Mind and Culture" (1981). also outlined a series of mathematical models of how genetic evolution might favor the selection of cultural traits and how cultural traits might, in turn, affect the speed of genetic evolution. Another 1981 book relevant to this topic was Cavalli-Sforza and Feldman's "Cultural Transmission and Evolution: A Quantitative Approach". Borrowing heavily from population genetics and epidemiology, this book built a mathematical theory concerning the spread of cultural traits. It describes the evolutionary implications of vertical transmission, passing cultural traits from parents to offspring; oblique transmission, passing cultural traits from any member of an older generation to a younger generation; and horizontal transmission, passing traits between members of the same population.
Robert Boyd and Peter Richerson's (1985) "Culture and the Evolutionary Process" presents models of the evolution of social learning under different environmental conditions, the population effects of social learning, various forces of selection on cultural learning rules, different forms of biased transmission and their population-level effects, and conflicts between cultural and genetic evolution.
Along with game theory, Herbert Gintis suggested that Dual inheritance theory has potential for unifying the behavioral sciences, including economics, biology, anthropology, sociology, psychology and political science because it addresses both the genetic and cultural components of human inheritance. Laland and Brown hold a similar view.
In psychology sub-fields.
Developmental psychology.
According to Paul Baltes, the benefits granted by evolutionary selection decrease with age. Natural selection has not eliminated many harmful conditions and nonadaptive characteristics that appear among older adults, such as Alzheimer disease. If it were a disease that killed 20 year-olds instead of 70 year-olds this may have been a disease that natural selection could have eliminated ages ago. Thus, unaided by evolutionary pressures against nonadaptive conditions, modern humans suffer the aches, pains, and infirmities of aging and as the benefits of evolutionary selection decrease with age, the need for culture increases.
Social psychology.
As humans are a highly social species, there are many adaptive problems associated with navigating the social world (e.g., maintaining allies, managing status hierarchies, interacting with outgroup members, coordinating social activities, collective decision-making). Researchers in the emerging field of evolutionary social psychology have made many discoveries pertaining to topics traditionally studied by social psychologists, including person perception, social cognition, attitudes, altruism, emotions, group dynamics, leadership, motivation, prejudice, intergroup relations, and cross-cultural differences.
When endeavouring to solve a problem humans at an early age show determination while chimpanzees have no comparable facial expression. Researchers suspect the human determined expression evolved because when a human is determinedly working on a problem other people will frequently help.
Abnormal psychology.
Adaptationist hypotheses regarding the etiology of psychological disorders are often based on analogies between physiological and psychological dysfunctions, as noted in the table below. Prominent theorists and evolutionary psychiatrists include Michael T. McGuire and Randolph M. Nesse. They, and others, suggest that mental disorders are due to the interactive effects of both nature and nurture, and often have multiple contributing causes.
Evolutionary psychologists have suggested that schizophrenia and bipolar disorder may reflect a side-effect of genes with fitness benefits, such as increased creativity. (Some individuals with bipolar disorder are especially creative during their manic phases and the close relatives of schizophrenics have been found to be more likely to have creative professions.) A 1994 report by the American Psychiatry Association found that people suffered from schizophrenia at roughly the same rate in Western and non-Western cultures, and in industrialized and pastoral societies, suggesting that schizophrenia is not a disease of civilization nor an arbitrary social invention. Sociopathy may represent an evolutionarily stable strategy, by which a small number of people who cheat on social contracts benefit in a society consisting mostly of non-sociopaths. Mild depression may be an adaptive response to withdraw from, and re-evaluate, situations that have led to disadvantageous outcomes (the "analytical rumination hypothesis") (see Evolutionary approaches to depression).
Some of these speculations have yet to be developed into fully testable hypotheses, and a great deal of research is required to confirm their validity.
Psychology of religion.
Adaptationist perspectives on religious belief suggest that, like all behavior, religious behaviors are a product of the human brain. As with all other organ functions, cognition's functional structure has been argued to have a genetic foundation, and is therefore subject to the effects of natural selection and sexual selection. Like other organs and tissues, this functional structure should be universally shared amongst humans and should have solved important problems of survival and reproduction in ancestral environments. However, evolutionary psychologists remain divided on whether religious belief is more likely a consequence of evolved psychological adaptations, or is a byproduct of other cognitive adaptations.
Reception.
Critics of evolutionary psychology accuse it of promoting genetic determinism, panadaptionism (the idea that all behaviors and anatomical features are adaptations), unfalsifiable hypotheses, distal or ultimate explanations of behavior when proximate explanations are superior, and malevolent political or moral ideas.
Ethical implications.
Critics have argued that evolutionary psychology might be used to justify existing social hierarchies and reactionary policies. It has also been suggested by critics that evolutionary psychologists' theories and interpretations of empirical data rely heavily on ideological assumptions about race and gender.
In response to such criticism, evolutionary psychologists often caution against committing the naturalistic fallacy – the assumption that "what is natural" is necessarily a moral good. However, their caution against committing the naturalistic fallacy has been criticized as means to stifle legitimate ethical discussions.
Standard social science model.
Evolutionary psychology has been entangled in the larger philosophical and social science controversies related to the debate on nature and nurture. Evolutionary psychologists typically contrast evolutionary psychology with what they call the standard social science model (SSSM). They characterize the SSSM as the "blank slate", social constructionist, or "cultural determinist" perspective that they say dominated the social sciences throughout the 20th century and assumed that the mind was shaped almost entirely by culture.
Critics have argued that evolutionary psychologists created a false dichotomy between their own view and the caricature of the SSSM. Other critics regard the SSSM as a rhetorical device or a straw man and suggest that the scientists whom evolutionary psychologists associate with the SSSM did not believe that the mind was a blank state devoid of any natural predispositions.
Reductionism and determinism.
Some critics view evolutionary psychology as a form of genetic reductionism and genetic determinism, a common critique being that evolutionary psychology does not address the complexity of individual development and experience and fails to explain the influence of genes on behavior in individual cases. Evolutionary psychologists respond that EP works within a nature-nurture interactionist framework that acknowledges that many psychological adaptations are facultative (sensitive to environmental variations during individual development). EP is generally not focused on proximate analyses of behavior but rather its focus is on the study of distal/ultimate causality (the evolution of psychological adaptations). The field of behavioral genetics is focused on the study of the proximate influence of genes on behavior.
Testability of hypotheses.
A frequent critique of the discipline is that the hypotheses of evolutionary psychology are frequently arbitrary and difficult or impossible to adequately test, thus questioning its status as an actual scientific discipline, for example because many current traits probably evolved to serve different functions than they do now. While evolutionary psychology hypotheses are difficult to test, evolutionary psychologists assert that it is not impossible. Part of the critique of the scientific base of evolutionary psychology includes a critique of the concept of the Environments of Evolutionary Adaptation (EEA). Some critics have argued that researchers know so little about the environment in which "Homo sapiens" evolved that explaining specific traits as an adaption to that environment becomes highly speculative. Evolutionary psychologists respond that they do know many things about this environment, including the facts that present day humans' ancestors were hunter-gatherers, that they generally lived in small tribes, etc.
Modularity of mind.
Evolutionary psychologists generally presume that, like the body, the mind is made up of many evolved modular adaptations, although there is some disagreement within the discipline regarding the degree of general plasticity, or "generality," of some modules. It has been suggested that modularity evolves because, compared to non-modular networks, it would have conferred an advantage in terms of fitness and because connection costs are lower.
In contrast, some academics argue that it is unnecessary to posit the existence of highly domain specific modules, and, suggest that the neural anatomy of the brain supports a model based on more domain general faculties and processes. Moreover, empirical support for the domain-specific theory stems almost entirely from performance on variations of the Wason selection task which is extremely limited in scope as it only tests one subtype of deductive reasoning.
Evolutionary psychology defense.
Evolutionary psychologists have addressed many of their critics (see, for example, books by Segerstråle (2000), "Defenders of the Truth: The Battle for Science in the Sociobiology Debate and Beyond," Barkow (2005), "Missing the Revolution: Darwinism for Social Scientists," and Alcock (2001), "The Triumph of Sociobiology".). Among their rebuttals are that some criticisms are straw men, are based on an incorrect nature versus nurture dichotomy, are based on misunderstandings of the discipline, etc. Robert Kurzban suggested that "...critics of the field, when they err, are not slightly missing the mark. Their confusion is deep and profound. It’s not like they are marksmen who can’t quite hit the center of the target; they’re holding the gun backwards."

</doc>
<doc id="9705" url="http://en.wikipedia.org/wiki?curid=9705" title="Languages of Europe">
Languages of Europe

"Not to be confused with Indo-European languages."
Most of the languages of Europe belong to the Indo-European language family. This family is divided into a number of branches, including Romance, Germanic, Baltic, Slavic, Albanian, Celtic, Armenian and Hellenic (Greek). The Uralic languages, which include Hungarian, Finnish, and Estonian, also have a significant presence in Europe. The Turkic and Mongolic families also have several European members, while the North Caucasian and Kartvelian families are important in the southeastern extremity of geographical Europe. The Basque language of the western Pyrenees is an isolate unrelated to any other group, while Maltese is the only Semitic language in Europe with national language status.
Indo-European languages.
The Indo-European language family descended from Proto-Indo-European, believed to have been spoken thousands of years ago. Indo-European languages are spoken throughout Europe, but particularly dominate Western Europe.
Albanian.
Albanian has two major dialects, Gheg and Tosk. It is spoken in Albania, Kosovo (Kosovar Albanians) and parts of Montenegro (Albanians in Montenegro), Serbia (mainly in Preševo Valley), Turkey, southern Italy (Arbëresh), western parts of Macedonia, Greece (Arvanitika and Cham Albanians) and Albanian diaspora.
Armenian.
Armenian has two major dialects, Western Armenian and Eastern Armenian. It is spoken in Armenia, where it has sole official status, and is also spoken in neighboring Georgia, Iran, and Azerbaijan (mainly in Nagorno-Karabakh Republic). It is also spoken in Turkey by a very small minority (Western Armenian and Homshetsi), and by small minorities in many other countries where members of the widely dispersed Armenian diaspora reside.
Baltic languages.
The Baltic languages are spoken in Lithuania (Lithuanian, Samogitian) and Latvia (Latvian, Latgalian). Samogitian and Latgalian are usually considered to be dialects of Lithuanian and Latvian respectively.
New Curonian is nearly extinct: it was spoken in the Curonian Spit which is now divided between Lithuania and the Kaliningrad Oblast. There are also several extinct Baltic languages, including Old Prussian and Sudovian.
Celtic.
There are about six living Celtic languages, spoken in areas of northwestern Europe dubbed the "Celtic nations". All six are members of the Insular Celtic family, which in turn is divided into:
Continental Celtic languages had previously been spoken across Europe from Iberia and Gaul to Asia Minor, but became extinct in the first millennium AD.
Germanic.
The Germanic languages make up the predominant language family in northwestern Europe, reaching from Iceland to Sweden and from parts of the United Kingdom and Ireland to Austria. There are two extant major sub-divisions: West Germanic and North Germanic. A third group, East Germanic, is now extinct; the only known surviving East Germanic texts are written in the Gothic language.
West Germanic.
There are three major groupings of West Germanic languages: Anglo-Frisian, Low Franconian (now primarily modern Dutch) and High German.
Anglo-Frisian.
The Anglo-Frisian language family has two major groups:
High German.
German is spoken throughout Germany, Austria, Liechtenstein, Luxembourg, the East Cantons of Belgium, much of Switzerland (including the northeast areas bordering on Germany and Austria) and northern Italy (South Tyrol).
There are several groups of German dialects:
Low German.
Low German is a separate language group from High German, but is still considered a dialect. It is spoken in various regions throughout Northern Germany, but has no official status, as the official language is Standard German.
North Germanic.
The North Germanic languages are spoken in Scandinavian countries and include Danish (Denmark, Greenland and the Faroe Islands), Norwegian (Norway), Swedish (Sweden and parts of Finland), Elfdalian or Övdalian (in a small part of central Sweden), Faroese (Faroe Islands), and Icelandic (Iceland).
Indo-Iranian languages.
The Indo-Iranian languages have two major groupings, Indo-Aryan languages including Romani, and Iranian languages, which include Kurdish, Persian, and Ossetian.
Romance languages.
The Romance languages descended from the Vulgar Latin spoken across most of the lands of the Roman Empire. Some of the Romance languages are official in the European Union and the Latin Union and the more prominent ones are studied in many educational institutions worldwide. Three of the Romance languages (Spanish, French, and Portuguese) are spoken by one billion speakers worldwide. Many other Romance languages and their local varieties are spoken throughout Europe, and some are recognized as regional languages.
The list below is a summary of Romance languages commonly encountered in Europe:
Slavic.
Slavic languages are spoken in large areas of Central Europe, Southern Europe and Eastern Europe including Russia.
Languages not from the Indo-European family.
Basque.
The Basque language (or "Euskara") is a language isolate and the ancestral language of the Basque people who inhabit the Basque Country, a region in the western Pyrenees mountains mostly in northeastern Spain and partly in southwestern France of about 3 million inhabitants, where it is spoken fluently by about 750,000 and understood by more than 1.5 million people.
Basque is directly related to ancient Aquitanian, and it is likely that an early form of the Basque language was present in Western Europe before the arrival of the Indo-European languages in the area. The language may have been spoken since Paleolithic times.
Basque is also spoken by immigrants in Australia, Costa Rica, Mexico, the Philippines and the United States, especially in the states of Nevada, Idaho, and California.
Kartvelian languages.
The Kartvelian language family consists of Georgian and the related languages of Svan, Mingrelian, and Laz. Proto-Kartvelian is believed to be a common ancestor language of all Kartvelian languages, with the earliest split occurring in the second millennium BC or earlier when Svan was separated. Megrelian and Laz split from Georgian roughly a thousand years later, roughly at the beginning of the first millennium BC (e.g., Klimov, T. Gamkrelidze, G. Machavariani).
The group is considered as isolated, and although for simplicity it is at times grouped with North Caucasian languages, no linguistic relationship exists between the two language families.
North Caucasian.
North Caucasian languages (sometimes called simply "Caucasic", as opposed to Kartvelian, and to avoid confusion with the concept of the "Caucasian race") is a blanket term for two language families spoken chiefly in the north Caucasus and Turkey—the Northwest Caucasian family (including Abkhaz, spoken in Abkhazia, and Circassian) and the Northeast Caucasian family, spoken mainly in the border area of the southern Russian Federation (including Dagestan, Chechnya, and Ingushetia).
Many linguists, notably Sergei Starostin and Sergei Nikolayev, believe that the two groups sprang from a common ancestor about 5,000 years ago. However this view is difficult to evaluate, and remains controversial.
Uralic.
Europe has a number of Uralic languages and language families, including Estonian, Finnish, and Hungarian.
Mongolic.
The Mongolic languages originated in Asia, and most did not proliferate west to Europe. Kalmyk is spoken in the Republic of Kalmykia, part of the Russian Federation, and is thus the only native Mongolic language spoken in Europe.
Semitic.
Cypriot Maronite Arabic.
Cypriot Maronite Arabic (also known as Cypriot Arabic) is a variety of Arabic spoken by Maronites in Cyprus. Most speakers live in Nicosia, but others are in the communities of Kormakiti and Lemesos. Brought to the island by Maronites fleeing Lebanon over 700 years ago, this variety of Arabic has been influenced by Greek in both phonology and vocabulary, while retaining certain unusually archaic features in other respects.
Hebrew.
Hebrew has been written and spoken by the Jewish communities of all of Europe in liturgical, educational, and often conversational contexts since the entry of the Jews into Europe some time during the late antiquity. Its restoration as the official language of Israel has accelerated its secular use. It also has been used in educational and liturgical contexts by some segments of the Christian population. Hebrew has its own consonantal alphabet, in which the vowels may be marked by diacritical marks termed "pointing" in English and "Niqqud" in Hebrew. The Hebrew alphabet was also used to write Yiddish, a West Germanic language, and Ladino, a Romance language, formerly spoken by Jews in northern and southern Europe respectively, but now nearly extinct in Europe itself.
Maltese.
Maltese is a Semitic language with Romance and Germanic influences, spoken in Malta. It is based on Sicilian Arabic, with influences from Italian (particularly Sicilian), French, and, more recently, English.
It is unique in that it is the only Semitic language whose standard form is written in the Latin alphabet. It is also the smallest official language of the EU in terms of speakers, and the only official Semitic language within the EU.
General issues.
Lingua Franca—past and present.
Europe has had a number of languages that were considered linguae francae over some ranges for some periods according to some historians. Typically in the rise of a national language the new language becomes a lingua franca to peoples in the range of the future nation until the consolidation and unification phases. If the nation becomes internationally influential, its language may become a lingua franca among nations that speak their own national languages. Europe has had no lingua franca ranging over its entire territory spoken by all or most of its populations during any historical period. Some linguae francae of past and present over some of its regions for some of its populations are:
First dictionaries and grammars.
The earliest dictionaries were glossaries, i.e., more or less structured lists of lexical pairs (in alphabetical order or according to conceptual fields). The Latin-German (Latin-Bavarian) Abrogans was among the first. A new wave of lexicography can be seen from the late 15th century onwards (after the introduction of the printing press, with the growing interest in standardizing languages).
Language and identity, standardization processes.
In the Middle Ages the two most important defining elements of Europe were "Christianitas" and "Latinitas". Thus language—at least the supranational language—played an elementary role. The concept of the nation state became increasingly important. Nations adopted particular dialects as their national language. This, together with improved communications, led to official efforts to standardise the national language, and a number of language academies were established (e.g., 1582 Accademia della Crusca in Florence, 1617 Fruchtbringende Gesellschaft in Weimar, 1635 Académie française in Paris, 1713 Real Academia Española in Madrid). Language became increasingly linked to nation as opposed to culture, and was also used to promote religious and ethnic identity (e.g., different Bible translations in the same language for Catholics and Protestants).
The first languages for which standardisation was promoted included Italian ("questione della lingua": Modern Tuscan/Florentine vs. Old Tuscan/Florentine vs. Venetian → Modern Florentine + archaic Tuscan + Upper Italian), French (the standard is based on Parisian), English (the standard is based on the London dialect) and (High) German (based on the dialects of the chancellery of Meissen in Saxony, Middle German, and the chancellery of Prague in Bohemia ("Common German")). But several other nations also began to develop a standard variety in the 16th century.
Scripts.
The main scripts used in Europe today are the Latin and Cyrillic; Greek also has its own script. All of the aforementioned are alphabets.
History.
The Greek alphabet was derived from the Phoenician and Latin was derived from the Greek via the Old Italic alphabet.
In the Early Middle Ages, Ogham was used in Ireland and runes (derived the Old Italic script) in Scandinavia. Both were replaced in general use by the Latin alphabet by the Late Middle Ages. The Cyrillic script was derived from the Greek with the first texts appearing around 940 AD.
Around 1900 there were mainly two typeface variants of the Latin alphabet used in Europe: Antiqua and Fraktur. Fraktur was used most for German, Estonian, Latvian, Norwegian and Danish whereas Antiqua was used for Italian, Spanish, French, Portuguese, English, Romanian, Swedish and Finnish. The Fraktur variant was banned by Hitler in 1941, having been described as "Schwabacher Jewish letters". Other scripts have historically been in use in Europe, including Arabic during the era of the Ottoman Empire, Phoenician, from which modern Latin letters descend, Ancient Egyptian hieroglyphs on Egyptian artefacts traded during Antiquity, and various runic systems used in Northern Europe preceding Christianisation.
Hungarian rovás was used by the Hungarian people in the early Middle Ages, but it was gradually replaced with the Latin-based Hungarian alphabet when Hungary became a kingdom, though it was revived in the 20th century and has certain marginal, but growing area of usage since then.
Linguistic diversity and conflict.
The most ancient historical social structure of Europe is that of politically independent tribes, each with its own ethnic identity, based among other cultural factors on its language: for example, the Latini speaking Latin in Latium. Linguistic conflict has been important in European history. Historical attitudes towards linguistic diversity are illustrated by two French laws: the Ordonnance de Villers-Cotterêts (1539), which said that every document in France should be written in French (neither in Latin nor in Occitan) and the Loi Toubon (1994), which aimed to eliminate Anglicisms from official documents. States and populations within a state have often resorted to war to settle their differences. There have been attempts to prevent such hostilities: one such initiative was promoted by the Council of Europe, founded in 1949, which affirms the right of minority language speakers to use their language fully and freely. The Council of Europe is committed to protecting linguistic diversity.
Currently all European countries except France, Andorra and Turkey have signed the Framework Convention for the Protection of National Minorities, while Greece, Iceland and Luxembourg have signed it, but have not ratified it. This framework entered into force in 1998.
Language and the European Union.
Official status.
The European Union designates one or more languages as "official and working" with regard to any member state if they are the official languages of that state. The decision as to whether they are and their use by the EU as such is entirely up to the laws and policies of the member states. In the case of multiple official languages the member state must designate which one is to be the working language.
As the EU is an entirely voluntary association established by treaty — a member state may withdraw at any time — each member retains its sovereignty in deciding what use to make of its own languages; it must agree to legislate any EU acceptance criteria before membership. The EU designation as official and working is only an agreement concerning the languages to be used in transacting official business between the member state and the EU, especially in the translation of documents passed between the EU and the member state. The EU does not attempt in any way to govern language use in a member state.
Currently the EU has designated by agreement with the member states 24 languages as "official and working:" Bulgarian, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, German, Greek, Hungarian, Irish, Italian, Latvian, Lithuanian, Maltese, Polish, Portuguese, Romanian, Slovak, Slovenian, Spanish and Swedish. This designation provides member states with two "entitlements:" the member state may communicate with the EU in the designated one of those languages and view "EU regulations and other legislative documents" in that language.
Proficiency.
The European Union and the Council of Europe have been collaborating in a number of tasks, among which is the education of member populations in languages for "the promotion of plurilingualism" among EU member states, The joint document, "Common European Framework of Reference for Languages: Learning, Teaching, Assessment (CEFR)", is an educational standard defining "the competencies necessary for communication" and related knowledge for the benefit of educators in setting up educational programs. That document defines three general levels of knowledge: A Basic User, B Independent User and C Proficient User. The ability to speak the language falls under competencies B and C ranging from "can keep going comprehensibly" to "can express him/herself at length with a natural, effortless, unhesitating flow."
These distinctions were simplified in a 2005 independent survey requested by the EU's Directorate-General for Education and Culture regarding the extent to which major European languages were spoken in member states. The results were published in a 2006 document, "Europeans and Their Languages", or "Eurobarometer 243", which is disavowed as official by the European Commission, but does supply some scientific data concerning language use in the EU. In this study, statistically relevant samples of the population in each country were asked to fill out a survey form concerning the languages that they spoke with sufficient competency "to be able to have a conversation". Some of the results showing the distribution of major languages are shown in the maps below. The darkest colors report the highest proportion of speakers. Only EU members were studied. Thus data on Russian speakers were gathered, but Russia is not an EU member and so Russian does not appear in Russia on the maps. It does appear as spoken to the greatest extent in the Baltic countries, which are EU members that were formerly under Soviet rule; followed by former Eastern bloc countries such as Poland, the Czech Republic, and the northeastern part of Germany (former socialist East Germany).
Number of speakers.
The following is a table displaying the number of speakers of a given European language in Europe only. There is a relatively high level of language endangerment in Europe; only 42 languages have more than 1 million speakers.

</doc>
<doc id="9706" url="http://en.wikipedia.org/wiki?curid=9706" title="Eindhoven University of Technology">
Eindhoven University of Technology

The Eindhoven University of Technology is a university of technology located in Eindhoven, Netherlands. Its motto is "Mens agitat molem" (The mind brings matter into motion). The university was the second of its kind in the Netherlands, only Delft University of Technology existed previously. Until mid-1980 it was known as the "Technische Hogeschool Eindhoven" (abbr. "THE"). In 2011 QS World University Rankings placed Eindhoven at 146th internationally, but 61st globally for Engineering & IT. Furthermore, in 2011 Academic Ranking of World Universities (ARWU) rankings, TU/e was placed at the 52-75 bucket internationally in Engineering/Technology and Computer Science (ENG) category and at 34th place internationally in the field of Computer Science. In 2003 a European Commission report ranked TU/e at third place among all European research universities (after Cambridge and Oxford and at equal rank with TU Munich), thus making it the highest ranked Technical University in Europe.
Overview.
The Eindhoven University of Technology was founded as the "Technische Hogeschool Eindhoven" (THE) on 23 June 1956 by the Dutch government. The University was acknowledged for its research in Automobile sector. It was the second institute of its kind in the Netherlands, preceded only by the Delft University of Technology. It is located on its own campus in the center of Eindhoven, just north of the central station. It is currently home to about 240 professors, 7200 students, 250 PDEng-students, 600 Ph.D. students, 200 post-doc students and 3000 regular employees. It supports about 100 student associations and 15 alumni associations. Yearly, the Eindhoven University of Technology produces almost 3000 scientific publications, 140 PhD-awards, and 40 patents.
The Eindhoven University of Technology is main participant in the technological top institutes DPI and M2i. One of its former students is Gerard Kleisterlee, a former CEO of Philips.
The university is in an area where several companies active in technology are doing their research, like Philips, ASML and DAF. The university maintains close contacts with most of these companies.
As of 29 April 2005, Prof.dr.ir. C.J. van Duijn has the position of rector magnificus.
In 2006, the university celebrated its 50th birthday.
In a 2003 European Commission report, TU/e was ranked as 3rd among European research universities (after Cambridge and Oxford, at equality with TU Munich and thus making it the highest ranked Technical University in Europe), based on the impact of its scientific research. In 'The Times Higher Education Supplement World University Ranking 2005'. it was ranked 74th among world universities, and 67th in 2006.
The university operates several international cooperations with other universities all over the world; the Brain Bridge with Zhejiang University, People's Republic of China, is an example of such a cooperation. Also, the university maintains partnerships with several Dutch universities and announced a "preferred partnership" with the Universiteit Utrecht on 3 January 2011.
Strategic Vision 2020.
On 3 January 2011, ir. Arno Peels presented the university's strategic vision document for the period up to 2020, the "Strategic Plan 2020". Despite the economic crisis and the budget cutbacks announced by the Dutch government for the period up to 2014, the university has set itself an ambitious strategic vision for the period up to 2020. This vision includes establishing a University College to foster both in-depth and wide-interest, society-interest driven education for upcoming engineers; establishing a combined Graduate School to manage the graduate programs; an increase of the student body by 50 percent; a 50 percent increase in the number of annual Ph.D graduations; an increase of knowledge valorisation to a campus-wide score of 4.2; increasing the international position of the university to within the top-100 universities; and increasing the embedding of the university within the city and the Brainport region by transforming the campus into a high-grade science park with laboratories, housing facilities for 700 students and researchers and supporting facilities. Particularly the science park of the vision is costly, with an expected 700 million euro investment in the campus needed for realization of the plan.
Organization.
The Eindhoven University of Technology is a public university of the Netherlands. As such its general structure and management is determined by the "Wet op het Hoger Onderwijs en Wetenschappelijk Onderzoek" (English: "Law on Higher Education and Scientific Research"). Between that law and the statutes of the university itself, the management of the university is organized according to the following chart:
Executive college.
The day-to-day running of the university is in the hands of the Executive College (Dutch: "College van Bestuur"). The College provides oversight for the departments, the service organizations and the Innovation Lab, plus the local activities of the Stan Ackermans Institute. The College consists of three people, plus a secretary:
Oversight of the executive college.
There are two bodies that provide oversight over the Executive College:
Departments and service organizations.
Most of the work at the university is done in the departments and the service organizations.
Both for the departments and the service organizations, the staff (and students) are involved with the running of the body. For that reason both types of bodies have advisory councils which have advisory and co-decision authorities.
TU/e Holding B.V..
Over the past two decades, the TU/e has increasingly developed commercial interests and off-campus ties. These include commercial agreements and contracts directly between the university and external companies, but also interests in spinoff companies. In order to manage these kinds of contractual obligations the university started the TU/e Holding B.V. in 1997. The Holding is a limited company, dedicated to the commercial exploitation of scientific knowledge.
Education.
Departments.
The scientific departments (or faculties; Dutch: "faculteiten") are the primary vehicles for teaching and research in the university. They employ the majority of the academic staff, are responsible for teaching and sponsor the research schools and institutions.
The vast majority of education is undergraduate education provided by the departments to students, who are adults with no other academic qualifications than a secondary education diploma. Some education is also provided to members of the postgraduate designer programs, but they are employed by the university and do not count as part of the student body.
Undergraduate education was given in four- or five-year programs until 2002, styled along the lines of the German system of education; graduates of these programs were granted an engineering title and allowed to prefix their name with the title "ir." (an abbreviation of ingenieur; not to be confused with graduates of technical "hogescholen", who were engineers abbreviated "ing."). Starting in 2002, following the entry into force of the Bologna Accords, the university switched to the bachelor/master structure (students graduating in 2002 were given both an old-style engineering title and a new master's title). The undergraduate programs are now split into two programs each, a three-year bachelor program and a two-year master program. These programs are completely independent, in the sense that a bachelor can leave the university with his title and go to work, can enter a master's program at another university or continue on to the master's program of his department at the university. Of course bachelors from other universities can also enroll in the new master's programs.
The departments also offer Ph.D programs (Dutch: "promotiefase") whereby a qualified master may earn a Ph.D. Unlike in anglo-saxon countries these are not educational programs, however; rather, a person working towards promotion is a research assistant, employed by the department, with teaching responsibilities in addition to his research work.
The TU/e has nine departments:
Honors programs.
The university offers two honors programs for "top students": students who have proven to have a knack for studying, have the capacity to handle a higher academic load and have an interest in more depth in their programs. There are two honors programs, both aimed at the bachelor students:
Qualifying students can choose between the Honors programs, or follow both at the same time.
Shared minors.
As of September 2010 the university offers bachelors in their third year a minor on sustainable energy. This minor will focus on providing students with skills needed to work in the sustainable energy industry, or to start their own company in this industry. For that reason the minor is driven entirely by problems supplied by industry and knowledge institutes, which minor students must solve in multidisciplinary teams. In addition, students must come up with an innovative plan and develop a start-up company.
Postgraduate designer programs.
The university started a number of postgraduate designer programs together with the other Dutch technical universities in 1986. These programs are currently managed by the Stan Ackermans Institute on behalf of the 3TU Federation. Each program is two years in length and graduates earn a Professional Doctorate in Engineering and may call themselves "technical designers". There are a total of eleven program active, of which eight are available at the TU/e:
The post-MSc program as a whole graduated its 3.000th technical designer (Dipl.-Eng. Sissy Papatheologou, PDEng) on 16 September 2010.
Other educational programs.
The university hosts a number of other educational programs that are in some way related to the main educational programs. These include the teacher's program and an MBA program.
Research.
The TU/e does not only host research in its departments. The TU/e participates in a large number of research institutes which balance in different ways between pure science and applied science research. Some of these institutes are bound strictly to the university, others combine research across different universities. Some have even been designated to be of national importance.
Top in research partnerships with industry.
The TU/e is among the world’s ten best-performing research universities in terms of research cooperation with industry in 2011 (Number 1 in 2009). Ten to 20 percent of the scientific publications of these ten universities in the period 2006–2008 were the result of partnerships with researchers in industry. As well as TU/e and Delft University of Technology, the top 10 also includes two universities in Japan (Tokyo Institute of Technology and Keio University in Tokyo), two in Sweden (CTH Chalmers University of Technology and KTH Royal Institute of Technology in Stockholm), and one each in Denmark (DTU Technical University of Denmark in Lyngby), Finland (University of Helsinki), Norway (Norwegian University of Science and Technology in Trondheim) and the USA (Rensselaer Polytechnic Institute in Troy, New York).
Technological Topinstitutes.
A Technological Topinstitute is a research institute that is a combined effort of different universities, commercial entities and the government. The Dutch government has identified a number of areas of research as "key areas" of vital, national interest and has commissioned a Top Institute for each of them. The TU/e hosts and manages two of them:
Research schools.
The TU/e is commissioner and participant of a number of research schools:
Off-campus activities.
The TU/e plays a central role in the academic, economic and social life of Eindhoven and the surrounding region. In addition the university maintains relations with institutions far beyond that region as well and participates in national and international events (sometimes through the student body).
Economic and research motor.
The TU/e is enormously important to the economy of the Eindhoven region, as well as the wider areas of BrabantStad and the Samenwerkingsverband Regio Eindhoven. It provides highly skilled labor for the local knowledge economy and is a knowledge and research partner for technology companies in the area.
The historic basis for the university's role as an economy and research motor was the interaction with Philips. The university was founded primarily to address the need of Philips for local personnel with academic levels of education in electronics, physics, chemistry and later computer science. Later that interest spread to DAF and Royal Dutch Shell (which became the primary employer for graduates of the chemistry department). There was also a synergy with these companies in that senior personnel were hired from them to form the academic staff of the university (which led to the Eindhoven joke that the university trains the engineers and Philips trains the professors).
Changing economic times and business strategies changed the relationship during the 1980s and 1990s. As Philips started moving away from the region, its importance to the region and the university decreased. A struggle for economic survival forced the university to seek closer ties with the city and region of Eindhoven in the 1989–1995 period, resulting in the creation of the Brainport initiative to draw high tech business and industry to the region. The university started expending more effort in knowledge valorisation, in incubating technology startups, in providing direct knowledge support for local technology companies. Also the academic interests of the research shifted with the times, with more effort going into energy efficiency research, green technologies, and other areas of interest driven by social relevance (the call for better technology in the medical field, for example, led to cooperation with the Catharina Hospital and the University of Maastricht medical department and finally the creation of the Biomedical Technology department).
The TU/e is host (and in some cases also commissioner) of a number of highly successful research schools, including the ESI and the DPI. These research institutes are a source of high-tech knowledge for high-tech companies in the area, such as ASML, NXP and FEI. The university also plays a large role as knowledge and personnel supplier to other companies in the High Tech Campus Eindhoven and helps incubate startups through the Eindhoven Twinning Center. It is also a knowledge supporter of the automotive industry in the Helmond region.
In the extended region, the TU/e is part of the backbone of the Eindhoven-Leuven-Aachen triangle. This economic cooperation agreement between three cities in three countries has created one of the most innovative regions in the European Union (measured in terms of money invested in technology and knowledge economy); the agreement is based on the cooperative triangle that connects the three technical universities in those cities.
Eindhoven Energy Institute.
As of the summer of 2010, the TU/e is host to the Eindhoven Energy Institute (EEI). The EEI is a virtual research institute (meaning that it doesn't have any actual offices or facilities), which manages and coordinates the activities of a large number of groups and subinstitutes in the general area of sustainable and alternative energy technologies.
The scientific director of the institute is prof.dr.ir. David Smeulders. He is pro forma head of the research department, which is split into four key areas: "Built Environment" (energy usage and patterns in building, headed by prof.dr.ir. Jan Hensen from the Department of the Built Environment), "Future Fuels" (headed by prof.dr. Philip de Goey of Mechanical Engineering), "Energy Conversion" (headed by prof.dr.ir. René Janssen from Chemical Engineering) and "Fusion and Plasma" (headed by prof.dr. Niek Lopes Cardozo from Physics). The EEI also incorporates the Graduate School on Sustainable Energy, which the TU/e had already established together with the TU Munich and DTU Lyngby. Secretarial services will be provided by the Center Technology for Sustainable Development (TDO) which also already existed at the TU/e (since 1994).
Energy research at the TU/e is among the best in academic Europe (a February 2010 study by Reed Elsevier puts it second only to Imperial College London). This fact, as well as the unique attention to energy in the built-up environment, drew the attention of the European Institute of Innovation and Technology. The EEI is now a full co-location of EIT's KIC on Sustainable Energy (InnoEnergy).
International cooperation and appeal.
The TU/e sets a lot of store by international contacts and cooperation. The university maintains active, academic cooperation with sister institutions in several different countries, for example:
The TU/e also provides education to an increasing number of foreign students and graduates. According to the 2009 annual report in the academic year 2008–2009 there were 490 exchange students, 103 foreign nationals registered in a bachelor program, 430 in a master program, 158 in a professional doctorate program (79% of the total). In 2009 the university employed 37 foreign professors (15.9% of the total) and 16 foreign associate professors (12.8%). Overall, 29.5% of the university staff was non-Dutch.
In 2011/2012, the TU/e has Erasmus bilateral agreements with many universities in 30 countries across Europe in a diverse range of subjects for student exchange.
Technological sports.
In addition to the "regular" types of sports practiced among the student body and by the staff, the TU/e collaborates with the student body in a number of "technology sporting efforts". These usually take the form of cross-department projects, which makes them multidisciplinary efforts. Some examples include:
Student organizations and facilities.
The university offers many different facilities for its student body and hosts many different student organizations<ref name="TU Eindhoven Student/Study/Sport/Culture/International associations"></ref> on campus as well.
Student and study associations.
There are two main types of student clubs at Dutch universities: student associations (Dutch: "studentenvereniging") and study associations (Dutch: "studievereninging"). The first are somewhat analogous to fraternities and sororities in the United States, except that they tend to be coed. The second are linked to the departments and educational programs.
Student associations.
There are three main student associations associated with the TU/e, plus a number of independent clubs:
Culture, international association and spirituality.
There are several associations, clubs and circles associated with the TU/e, which are meant to help students and staff develop themselves in non-academic areas. Such areas include cultural development, building international contacts and investigating spiritual beliefs.
Cultural activities.
The following associations organize activities with cultural or social/societal relevance:
Sport associations.
There are many sports associations within the university. They are overseen by the general sports council ESSF.
Service organizations.
There university is more than just the departments, research bodies and the students. There are several ancillary activities necessary to the running of the university, activities that cross the boundaries and interests of the different departments. These activities are carried out by the universities' service organizations.
The university has the following service organizations:
Spinoffs.
Over the years several spin off companies have been started by TU/e graduates, based on some research done at the university. Examples include:
International acclaim.
On the 2009 THE–QS World University Rankings (From 2010 two separate rankings will be produced by the Times Higher Education World University Rankings and the QS World University Rankings)
list, the Eindhoven University of Technology was ranked inside the top 200 for the fifth consecutive year. An overview of the 2005–2009 rankings can be seen below. In 2010 the QS World University Rankings ranked the university 126th in the world. On the Times Higher Education ranking of 2010 Eindhoven University of Technology is the highest ranked (#114) University of the Netherlands. They are followed by Leiden University (#124).
In a 2003 European Commission report, TU/e was ranked as third among European research universities (after Cambridge and Oxford, at equality with TU Munich and thus making it the highest ranked Technical University in Europe), based on the impact of its scientific researches.
In 2011 Academic Ranking of World Universities (ARWU) rankings, TU/e was placed at the 52-75 bucket internationally in Engineering/Technology and Computer Science ( ENG ) category and at 34th place internationally in the Computer Science subject field.

</doc>
<doc id="9707" url="http://en.wikipedia.org/wiki?curid=9707" title="Electronegativity">
Electronegativity

Electronegativity, symbol χ, is a chemical property that describes the tendency of an atom or a functional group to attract electrons (or electron density) towards itself. An atom's electronegativity is affected by both its atomic number and the distance at which its valence electrons reside from the charged nucleus. The higher the associated electronegativity number, the more an element or compound attracts electrons towards it. First proposed by Linus Pauling in 1932 as a development of valence bond theory, it has been shown to correlate with a number of other chemical properties. Electronegativity cannot be directly measured and must be calculated from other atomic or molecular properties. Several methods of calculation have been proposed, and although there may be small differences in the numerical values of the electronegativity, all methods show the same periodic trends between elements. 
The most commonly used method of calculation is that originally proposed by Linus Pauling. This gives a dimensionless quantity, commonly referred to as the Pauling scale, on a relative scale running from around 0.7 to 3.98 (hydrogen = 2.20). When other methods of calculation are used, it is conventional (although not obligatory) to quote the results on a scale that covers the same range of numerical values: this is known as an electronegativity in Pauling units. 
Electronegativity, as it is usually calculated, is not strictly a property of an atom, but rather a property of an atom in a molecule. Properties of a free atom include ionization energy and electron affinity. It is to be expected that the electronegativity of an element will vary with its chemical environment, but it is usually considered to be a transferable property, that is to say that similar values will be valid in a variety of situations.
On the most basic level, electronegativity is determined by factors like the nuclear charge (the more protons an atom has, the more "pull" it will have on electrons) and the number/location of other electrons present in the atomic shells (the more electrons an atom has, the farther from the nucleus the valence electrons will be, and as a result the less positive charge they will experience—both because of their increased distance from the nucleus, and because the other electrons in the lower energy core orbitals will act to shield the valence electrons from the positively charged nucleus).
The opposite of electronegativity is electropositivity: a measure of an element's ability to donate electrons.
Francium is the least electronegative element in the periodic table (=0.7), while fluorine is the greatest one (=3.98). 
Methods of calculation.
Pauling electronegativity.
Pauling first proposed the concept of electronegativity in 1932 as an explanation of the fact that the covalent bond between two different atoms (A–B) is stronger than would be expected by taking the average of the strengths of the A–A and B–B bonds. According to valence bond theory, of which Pauling was a notable proponent, this "additional stabilization" of the heteronuclear bond is due to the contribution of ionic canonical forms to the bonding.
The difference in electronegativity between atoms A and B is given by:
where the dissociation energies, "E"d, of the A–B, A–A and B–B bonds are expressed in electronvolts, the factor (eV)−½ being included to ensure a dimensionless result. Hence, the difference in Pauling electronegativity between hydrogen and bromine is 0.73 (dissociation energies: H–Br, 3.79 eV; H–H, 4.52 eV; Br–Br 2.00 eV)
As only differences in electronegativity are defined, it is necessary to choose an arbitrary reference point in order to construct a scale. Hydrogen was chosen as the reference, as it forms covalent bonds with a large variety of elements: its electronegativity was fixed first at 2.1, later revised to 2.20. It is also necessary to decide which of the two elements is the more electronegative (equivalent to choosing one of the two possible signs for the square root). This is usually done using "chemical intuition": in the above example, hydrogen bromide dissolves in water to form H+ and Br− ions, so it may be assumed that bromine is more electronegative than hydrogen. However, in principle, since the same electronegativities should be obtained for any two bonding compounds, the data is in fact overdetermined, and the signs are unique once a reference point is fixed (usually, for H or F).
To calculate Pauling electronegativity for an element, it is necessary to have data on the dissociation energies of at least two types of covalent bond formed by that element. A. L. Allred updated Pauling's original values in 1961 to take account of the greater availability of thermodynamic data, and it is these "revised Pauling" values of the electronegativity that are most often used.
The essential point of Pauling electronegativity is that there is an underlying, quite accurate, semi-empirical formula for dissociation energies, namely:
or sometimes, a more accurate fit
This is an approximate equation, but holds with good accuracy. Pauling obtained it by noting that a bond can be approximately represented as a quantum mechanical superposition of a covalent bond and two ionic bond-states. The covalent energy of a bond is approximately, by quantum mechanical calculations, the geometric mean of the two energies of covalent bonds of the same molecules, and there is an additional energy that comes from ionic factors, i.e. polar character of the bond.
The geometric mean is approximately equal to the arithmetic mean - which is applied in the first formula above - when the energies are of the similar value, e.g., except for the highly electropositive elements, where there is a larger difference of two dissociation energies; the geometric mean is more accurate and almost always gives a positive excess energy, due to ionic bonding. The square root of this excess energy, Pauling notes, is approximately additive, and hence one can introduce the electronegativity. Thus, it is this semi-empirical formula for bond energy that underlies Pauling electronegativity concept.
The formulas are approximate, but this rough approximation is in fact relatively good and gives the right intuition, with the notion of polarity of the bond and some theoretical grounding in quantum mechanics. The electronegativities are then determined to best fit the data.
In more complex compounds, there is additional error since electronegativity depends on the molecular environment of an atom. Also, the energy estimate can be only used for single, not for multiple bonds. The energy of formation of a molecule containing only single bonds then can be approximated from an electronegativity table, and depends on the constituents and sum of squares of differences of electronegativities of all pairs of bonded atoms. Such a formula for estimating energy typically has relative error of order of 10%, but can be used to get a rough qualitative idea and understanding of a molecule.
Mulliken electronegativity.
Robert S. Mulliken proposed that the arithmetic mean of the first ionization energy (Ei) and the electron affinity (Eea) should be a measure of the tendency of an atom to attract electrons. As this definition is not dependent on an arbitrary relative scale, it has also been termed absolute electronegativity, with the units of kilojoules per mole or electronvolts.
However, it is more usual to use a linear transformation to transform these absolute values into values that resemble the more familiar Pauling values. For ionization energies and electron affinities in electronvolts,
and for energies in kilojoules per mole,
The Mulliken electronegativity can only be calculated for an element for which the electron affinity is known, fifty-seven elements as of 2006.
The Mulliken electronegativity of an atom is sometimes said to be the negative of the chemical potential. By inserting the energetic definitions of the ionization potential and electron affinity into the Mulliken electronegativity, it is possible to show that the Mulliken chemical potential is a finite difference approximation of the electronic energy with respect to the number of electrons., i.e.,
Allred–Rochow electronegativity.
A. Louis Allred and Eugene G. Rochow considered that electronegativity should be related to the charge experienced by an electron on the "surface" of an atom: The higher the charge per unit area of atomic surface the greater the tendency of that atom to attract electrons. The effective nuclear charge, "Z"eff, experienced by valence electrons can be estimated using Slater's rules, while the surface area of an atom in a molecule can be taken to be proportional to the square of the covalent radius, "r"cov. When "r"cov is expressed in picometres,
Sanderson electronegativity equalization.
R.T. Sanderson has also noted the relationship between Mulliken electronegativity and atomic size, and has proposed a method of calculation based on the reciprocal of the atomic volume. With a knowledge of bond lengths, Sanderson's model allows the estimation of bond energies in a wide range of compounds. Sanderson's model has also been used to calculate molecular geometry, "s"-electrons energy, NMR spin-spin constants and other parameters for organic compounds. This work underlies the concept of electronegativity equalization, which suggests that electrons distribute themselves around a molecule to minimize or to equalize the Mulliken electronegativity. This behavior is analogous to the equalization of chemical potential in macroscopic thermodynamics.
Allen electronegativity.
Perhaps the simplest definition of electronegativity is that of Leland C. Allen, who has proposed that it is related to the average energy of the valence electrons in a free atom,
where εs,p are the one-electron energies of s- and p-electrons in the free atom and "n"s,p are the number of s- and p-electrons in the valence shell. It is usual to apply a scaling factor, 1.75×10−3 for energies expressed in kilojoules per mole or 0.169 for energies measured in electronvolts, to give values that are numerically similar to Pauling electronegativities.
The one-electron energies can be determined directly from spectroscopic data, and so electronegativities calculated by this method are sometimes referred to as spectroscopic electronegativities. The necessary data are available for almost all elements, and this method allows the estimation of electronegativities for elements that cannot be treated by the other methods, e.g. francium, which has an Allen electronegativity of 0.67. However, it is not clear what should be considered to be valence electrons for the d- and f-block elements, which leads to an ambiguity for their electronegativities calculated by the Allen method.
In this scale neon has the highest electronegativity of all elements, followed by fluorine, helium, and oxygen.
Correlation of electronegativity with other properties.
The wide variety of methods of calculation of electronegativities, which all give results that correlate well with one another, is one indication of the number of chemical properties which might be affected by electronegativity. The most obvious application of electronegativities is in the discussion of bond polarity, for which the concept was introduced by Pauling. In general, the greater the difference in electronegativity between two atoms the more polar the bond that will be formed between them, with the atom having the higher electronegativity being at the negative end of the dipole. Pauling proposed an equation to relate "ionic character" of a bond to the difference in electronegativity of the two atoms, although this has fallen somewhat into disuse.
Several correlations have been shown between infrared stretching frequencies of certain bonds and the electronegativities of the atoms involved: however, this is not surprising as such stretching frequencies depend in part on bond strength, which enters into the calculation of Pauling electronegativities. More convincing are the correlations between electronegativity and chemical shifts in NMR spectroscopy or isomer shifts in Mössbauer spectroscopy (see figure). Both these measurements depend on the s-electron density at the nucleus, and so are a good indication that the different measures of electronegativity really are describing "the ability of an atom in a molecule to attract electrons to itself".
Trends in electronegativity.
Periodic trends.
In general, electronegativity increases on passing from left to right along a period, and decreases on descending a group. Hence, fluorine is the most electronegative of the elements (not counting noble gases), whereas caesium is the least electronegative, at least of those elements for which substantial data is available. This would lead one to believe that cesium fluoride is the compound with the strongest ionic bond, which is correct.
There are some exceptions to this general rule. Gallium and germanium have higher electronegativities than aluminium and silicon, respectively, because of the d-block contraction. Elements of the fourth period immediately after the first row of the transition metals have unusually small atomic radii because the 3d-electrons are not effective at shielding the increased nuclear charge, and smaller atomic size correlates with higher electronegativity (see Allred-Rochow electronegativity, Sanderson electronegativity above). The anomalously high electronegativity of lead, in particular when compared to thallium and bismuth, appears to be an artifact of data selection (and data availability)—methods of calculation other than the Pauling method show the normal periodic trends for these elements.
Variation of electronegativity with oxidation number.
In inorganic chemistry it is common to consider a single value of the electronegativity to be valid for most "normal" situations. While this approach has the advantage of simplicity, it is clear that the electronegativity of an element is "not" an invariable atomic property and, in particular, increases with the oxidation state of the element.
Allred used the Pauling method to calculate separate electronegativities for different oxidation states of the handful of elements (including tin and lead) for which sufficient data was available. However, for most elements, there are not enough different covalent compounds for which bond dissociation energies are known to make this approach feasible. This is particularly true of the transition elements, where quoted electronegativity values are usually, of necessity, averages over several different oxidation states and where trends in electronegativity are harder to see as a result.
The chemical effects of this increase in electronegativity can be seen both in the structures of oxides and halides and in the acidity of oxides and oxoacids. Hence CrO3 and Mn2O7 are acidic oxides with low melting points, while Cr2O3 is amphoteric and Mn2O3 is a completely basic oxide.
The effect can also be clearly seen in the dissociation constants of the oxoacids of chlorine. The effect is much larger than could be explained by the negative charge being shared among a larger number of oxygen atoms, which would lead to a difference in p"K"a of log10(¼) = –0.6 between hypochlorous acid and perchloric acid. As the oxidation state of the central chlorine atom increases, more electron density is drawn from the oxygen atoms onto the chlorine, reducing the partial negative charge on the oxygen atoms and increasing the acidity.
Group electronegativity.
In organic chemistry, electronegativity is associated more with different functional groups than with individual atoms. The terms group electronegativity and substituent electronegativity are used synonymously. However, it is common to distinguish between the inductive effect and the resonance effect, which might be described as σ- and π-electronegativities, respectively. There are a number of linear free-energy relationships that have been used to quantify these effects, of which the Hammett equation is the best known. Kabachnik parameters are group electronegativities for use in organophosphorus chemistry.
Electropositivity.
Electropositivity is a measure of an element's ability to donate electrons, and therefore form positive ions; thus, it is opposed to electronegativity. Mainly, this is an attribute of metals, meaning that, in general, the greater the metallic character of an element the greater the electropositivity. Therefore the alkali metals are most electropositive of all. This is because they have a single electron in their outer shell and, as this is relatively far from the nucleus of the atom, it is easily lost; in other words, these metals have low ionization energies.
While electronegativity increases along periods in the periodic table, and decreases down groups, electropositivity "decreases" along periods (from left to right) and "increases" down groups.
Electropositive shark repellent utilizes electropositive metals as shark repellents, since they generate measurable voltages in a seawater electrolyte relative to a shark.

</doc>
<doc id="9708" url="http://en.wikipedia.org/wiki?curid=9708" title="European Charter for Regional or Minority Languages">
European Charter for Regional or Minority Languages

The European Charter for Regional or Minority Languages (ECRML) is a European treaty (CETS 148) adopted in 1992 under the auspices of the Council of Europe to protect and promote historical regional and minority languages in Europe. The preparation for the charter was undertaken by the predecessor to the current Congress of Local and Regional Authorities, the Standing Conference of Local and Regional Authorities of Europe because involvement of local and regional government was essential. The actual charter was written in the Parliamentary Assembly based on the Congress' Recommendations. It only applies to languages traditionally used by the nationals of the State Parties (thus excluding languages used by recent immigrants from other states, see immigrant languages), which significantly differ from the majority or official language (thus excluding what the state party wishes to consider as mere local dialects of the official or majority language) and that either have a territorial basis (and are therefore traditionally spoken by populations of regions or areas within the State) or are used by linguistic minorities within the State as a whole (thereby including such languages as Yiddish and Romani, which are used over a wide geographic area).
Languages that are official within regions, provinces or federal units within a State (for example Catalan in Spain) are not classified as official languages of the State and may therefore benefit from the Charter. On the other hand, Ireland has not been able to sign the Charter on behalf of the Irish language (although a minority language) as it is defined as the first official language of the state. The United Kingdom has ratified the Charter in respect to (among other languages) Welsh in Wales and Irish in Northern Ireland. France, although a signatory, has been constitutionally blocked from ratifying the Charter in respect to the languages of France.
The charter provides a large number of different actions state parties can take to protect and promote historical regional and minority languages. There are two levels of protection—all signatories must apply the lower level of protection to qualifying languages. Signatories may further declare that a qualifying language or languages will benefit from the higher level of protection, which lists a range of actions from which states must agree to undertake at least 35.
Protections.
Countries can ratify the charter in respect of its minority languages based on Part II or Part III of the charter, which contain varying principles. Countries can treat languages differently under the charter, for example, in the United Kingdom, the Welsh language is ratified under the general Part II principles as well as the more specific Part III commitments, while the Cornish language is ratified only under Part II.
Part II.
Part II of the Charter details eight main principles and objectives upon which States must base their policies and legislation. They are seen as a framework for the preservation of the languages concerned.
Part III.
Part III details comprehensive rules across a number of sectors, that states agree to abide by. Each language to which Part III of the Charter is applied must be specifically named by the government. States must select at least thirty-five of the undertakings in respect of each language. Many provisions contain several options, of varying degrees of stringency, one of which has to be chosen “according to the situation of each language”. The areas from which these specific undertakings must be chosen are as follows:
Languages protected under the Charter.
Countries that have ratified the Charter, and languages for which the ratification was made:

</doc>
<doc id="9709" url="http://en.wikipedia.org/wiki?curid=9709" title="English Civil War">
English Civil War

The English Civil War (1642–1651) was a series of armed conflicts and political disagreements between Parliamentarians ("Roundheads") and Royalists ("Cavaliers") in the Kingdom of England over, principally, the manner of its government. The first (1642–46) and second (1648–49) wars pitted the supporters of King Charles I against the supporters of the Long Parliament, while the third (1649–51) saw fighting between supporters of King Charles II and supporters of the Rump Parliament. The war ended with the Parliamentarian victory at the Battle of Worcester on 3 September 1651.
The overall outcome of the war was threefold: the trial and execution of Charles I; the exile of his son, Charles II; and the replacement of English monarchy with, at first, the Commonwealth of England (1649–53) and then the Protectorate (1653–59) under Oliver Cromwell's personal rule. The monopoly of the Church of England on Christian worship in England ended with the victors consolidating the established Protestant Ascendancy in Ireland. Constitutionally, the wars established the precedent that an English monarch cannot govern without Parliament's consent, although this concept was legally established only as part of the Glorious Revolution in 1688.
Terminology.
As here, the term "English Civil War" appears most often in the singular form, although historians often divide the conflict into two or three separate wars. Although the term describes events as impinging on England, from the outset the conflicts involved wars with and civil wars within both Scotland and Ireland; see Wars of the Three Kingdoms for an overview.
Unlike other civil wars in England, which focused on who should rule rather than how the nation should be ruled, this war was more concerned with the manner in which the kingdoms of England, Scotland and Ireland were governed. The 1911 Encyclopædia Britannica called the series of conflicts the "Great Rebellion", while some historians – especially Marxists such as Christopher Hill (1912–2003) – have long favoured the term "English Revolution".
Strategy and Tactics.
Many of the officers and veteran soldiers of the English Civil war studied and implemented war strategies that had been learned and perfected in other wars across Europe, namely by the Spanish and the Dutch during the Dutch war for independence which began in 1568.
The main battle tactic came to be known as pike-and-shot-infantry, in which both sides would line up facing each other with infantry brigades of musketeers in the center, carrying matchlock muskets; these muskets were inaccurate, but could be lethal from up to 300 yards. The brigades would arrange themselves in lines of musketeers, three deep, where the first row would kneel, the second would crouch, and the third would stand, allowing all three to fire a volley simultaneously. At times there would be two groups of three lines allowing one group to reload while the other group arranged themselves and fired. Mixed in among the musketeers were pike men carrying pikes that were between 12 and 18 feet long, whose primary purpose was to protect the musketeers from cavalry charges. Positioned each side of the infantry were the cavalry, with a right-wing led by the lieutenant-general, and a left-wing by the commissary general; the main goal of the cavalry was to route the opponent’s cavalry and then turn and overpower their infantry.
The Royalist cavaliers' skill and speed on horseback led to many early victories. Prince Rupert, the leader of the king’s cavalry, learned a tactic while fighting in the Dutch army where the cavalry would charge at full speed into the opponent’s infantry firing their pistols just before impact.
However, with Oliver Cromwell and the introduction of the more disciplined new model army, a group of disciplined pike man who would stand their ground in the face of charging cavalry and could have a devastating effect. While the Parliamentarian cavalry were slower than the cavaliers, they were also better disciplined. The Royalists had a tendency to chase down individual targets after the initial charge leaving their forces scattered and tired; Cromwell’s cavalry, on the other hand, trained to operate as a single unit, which led to many decisive victories.
Background.
The King's rule.
The English Civil War broke out less than forty years after the death of Queen Elizabeth I in 1603. Elizabeth's death had resulted in the accession of her first cousin twice-removed, King James VI of Scotland, to the English throne as James I of England, creating the first personal union of the Scottish and English kingdoms. As King of Scots, James had become accustomed to Scotland's weak parliamentary tradition since assuming control of the Scottish government in 1583, so that upon assuming power south of the border, the new King of England was genuinely affronted by the constraints the English Parliament attempted to place on him in exchange for money. In spite of this, James' personal extravagance meant he was perennially short of money and had to resort to extra-Parliamentary sources of income.
James' personal extravagance was tempered by his peaceful disposition, so that by the succession of his son Charles I to the English and Scottish thrones in 1625 the two kingdoms had both experienced relative peace, both internally and in their relations with each other, for as long as anyone could remember. Charles hoped to unite the kingdoms of England, Scotland and Ireland into a new single kingdom, fulfilling the dream of his father. Many English Parliamentarians had suspicions regarding such a move, because they feared that setting up a new kingdom might destroy the old English traditions which had bound the English monarchy. As Charles shared his father's position on the power of the crown (James had described kings as "little gods on Earth", chosen by God to rule in accordance with the doctrine of the "Divine Right of Kings"), the suspicions of the Parliamentarians had some justification.
Parliament in the English constitutional framework.
At the time, the Parliament of England did not have a large permanent role in the English system of government. Instead, Parliament functioned as a temporary advisory committee and was only summoned if, and when, the monarch saw fit to summon it. Once summoned, a parliament's continued existence was at the king's pleasure, since it was subject to dissolution by him at any time.
Yet in spite of this limited role, Parliament had, over the preceding centuries, acquired "de facto" powers of enough significance that monarchs could not simply ignore them indefinitely. Without question, for a monarch, Parliament's most indispensable power was its ability to raise tax revenues far in excess of all other sources of revenue at the Crown's disposal. By the seventeenth century, Parliament's tax-raising powers had come to be derived from the fact that the gentry was the only stratum of society with the ability and authority to actually collect and remit the most meaningful forms of taxation then available at the local level. This meant that if the king wanted to ensure a smooth collection of revenue, he needed the co-operation of the gentry. For all of the Crown's legal authority, by any modern standard, its resources were limited to the extent that, if and when the gentry refused to collect the king's taxes on a national scale, the Crown lacked any practical means with which to compel them.
Therefore, in order to secure their co-operation, monarchs permitted the gentry (and only the gentry) to elect representatives to sit in the House of Commons. When assembled along with the House of Lords, these elected representatives formed a Parliament. Parliaments therefore allowed representatives of the gentry to meet, primarily (at least in the opinion of the monarch) so that they could give their sanction to whatever taxes the monarch expected their electorate to collect. In the process, the representatives could also confer and send policy proposals to the king in the form of bills. However, Parliament lacked any legal means of forcing its will upon the monarch; its only leverage with the king was the threat of its withholding the financial means required to execute his plans.
Parliamentary concerns and the Petition of Right.
Many concerns were raised over Charles's marriage to a Roman Catholic, French princess Henrietta Maria, in 1625. The Parliament refused to assign him the traditional right to collect customs duties for his entire reign, deciding instead to grant it only on a provisional basis and negotiate with him.
Charles, meanwhile, decided to send an expeditionary force to relieve the French Huguenots whom French royal troops held besieged in La Rochelle. Military support for Protestants on the Continent was, in itself, popular both in Parliament and with the Protestant majority in general, and it had the potential to alleviate concerns brought about by the King's marriage to a Catholic. However, Charles's insistence on having his unpopular royal favourite George Villiers, the Duke of Buckingham, assume command of the English force undermined that support. Unfortunately for Charles and Buckingham, the relief expedition proved a fiasco (1627), and Parliament, already hostile to Buckingham for his monopoly on royal patronage, opened impeachment proceedings against him. Charles responded by dissolving Parliament. This move, while saving Buckingham, reinforced the impression that Charles wanted to avoid Parliamentary scrutiny of his ministers.
Having dissolved Parliament and unable to raise money without it, the king assembled a new one in 1628. (The elected members included Oliver Cromwell and Edward Coke.) The new Parliament drew up the Petition of Right, and Charles accepted it as a concession in order to obtain his subsidy. Amongst other things, the Petition referred to the Magna Carta.
Personal rule.
Charles I avoided calling a Parliament for the next decade, a period known as the "personal rule of Charles I", or the "Eleven Years' Tyranny". During this period, Charles's lack of money determined policies. First and foremost, to avoid Parliament, the King needed to avoid war. Charles made peace with France and Spain, effectively ending England's involvement in the Thirty Years' War. However, that in itself was far from enough to balance the Crown's finances.
Unable to raise revenue through Parliament, and unwilling to convene it, Charles resorted to other means. One method was reviving certain conventions, often long-outdated. For example, a failure to attend and to receive knighthood at Charles's coronation was a finable offence with the fine paid to the Crown. The King also tried to raise revenue through the ship money tax, by exploiting a naval war-scare in 1635, demanding that the inland English counties pay the tax for the Royal Navy. Established law supported this policy, but authorities had ignored it for centuries, and many regarded it as yet another extra-Parliamentary (and therefore illegal) tax. Some prominent men refused to pay ship money, arguing that the tax was illegal, but they lost in court, and the fines imposed on them for refusing to pay ship money (and for standing against the tax's legality) aroused widespread indignation.
During the "Personal Rule," Charles aroused most antagonism through his religious measures: he believed in High Anglicanism, a sacramental version of the Church of England, theologically based upon Arminianism, a creed shared with his main political advisor, Archbishop William Laud. In 1633, Charles appointed Laud as Archbishop of Canterbury and started making the Church more ceremonial, replacing the wooden communion tables with stone altars. Puritans accused Laud of reintroducing Catholicism; when they complained, he had them arrested. In 1637 John Bastwick, Henry Burton, and William Prynne had their ears cut off for writing pamphlets attacking Laud's views—a rare penalty for gentlemen, and one that aroused anger. Moreover, the Church authorities revived the statutes passed in the time of Elizabeth I about church attendance, and fined Puritans for not attending Anglican church services.
Rebellion in Scotland.
The end of Charles's independent governance came when he attempted to apply the same religious policies in Scotland. The Church of Scotland, reluctantly episcopal in structure, had independent traditions. Charles, however, wanted one uniform Church throughout Britain and introduced a new, High Anglican version of the English Book of Common Prayer to Scotland in the middle of 1637. This was violently resisted; a riot broke out in Edinburgh, which may have been started in St Giles' Cathedral, according to legend, by Jenny Geddes. In February 1638, the Scots formulated their objections to royal policy in the National Covenant. This document took the form of a "loyal protest," rejecting all innovations not first having been tested by free parliaments and General Assemblies of the Church.
In the spring of 1639, King Charles I accompanied his forces to the Scottish border to end the rebellion known as the Bishops' War. But, after an inconclusive military campaign, he accepted the offered Scottish truce: the Pacification of Berwick. The truce proved temporary, and a second war followed in the middle of 1640. This time, a Scots army defeated Charles's forces in the north, then captured Newcastle. Charles eventually agreed not to interfere with Scotland's religion and paid the Scots' war-expenses.
Recall of the English Parliament.
Charles needed to suppress the rebellion in Scotland. He had insufficient funds, however, and needed to seek money from a newly elected English Parliament in 1640. The majority faction in the new Parliament, led by John Pym, took this appeal for money as an opportunity to discuss grievances against the Crown and opposed the idea of an English invasion of Scotland. Charles took exception to this "lèse-majesté" (offence against the ruler) and dissolved the Parliament after only a few weeks; hence the name "the Short Parliament".
Without Parliament's support, Charles attacked Scotland again, breaking the truce at Berwick, and suffered a comprehensive defeat. The Scots went on to invade England, occupying Northumberland and Durham. Meanwhile, another of Charles' chief advisors, Thomas Wentworth, 1st Viscount Wentworth, had risen to the role of Lord Deputy of Ireland in 1632 and brought in much-needed revenue for Charles by persuading the Irish Catholic gentry to pay new taxes in return for promised religious concessions.
In 1639, Charles had recalled Wentworth to England and in 1640 made him Earl of Strafford, attempting to have him achieve similar results in Scotland. This time he proved less successful and the English forces fled the field in their second encounter with the Scots in 1640. Almost the entirety of Northern England was occupied and Charles was forced to pay £850 per day to keep the Scots from advancing. If he did not, they would "take" the money by pillaging and burning the cities and towns of Northern England.
All this put Charles in a desperate financial position. As King of Scots, he had to find money to pay the Scottish army in England; as King of England, he had to find money to pay and equip an English army to defend England. His means of raising English revenue without an English Parliament fell critically short of achieving this. Against this backdrop, and according to advice from the Magnum Concilium (the House of Lords, but without the Commons, so not a Parliament), Charles finally bowed to pressure and summoned another English Parliament in November 1640.
The Long Parliament.
The new Parliament proved even more hostile to Charles than its predecessor. It immediately began to discuss grievances against Charles and his government and with Pym and Hampden (of ship money fame) in the lead, took the opportunity presented by the King's troubles to force various reforming measures—including many with strong 'anti-Papist' themes—upon him. The legislators passed a law which stated that a new Parliament should convene at least once every three years—without the King's summons, if necessary. Other laws passed by the Parliament made it illegal for the king to impose taxes without Parliamentary consent and later gave Parliament control over the king's ministers. Finally, the Parliament passed a law forbidding the King to dissolve it without its consent, even if the three years were up. Ever since, this Parliament has been known as the "Long Parliament". However, Parliament did attempt to avert conflict by requiring all adults to sign The Protestation, an oath of allegiance to Charles.
Early in the Long Parliament's proceedings the house overwhelmingly accused Thomas Wentworth, Earl of Strafford of high treason and other crimes and misdemeanors.
Henry Vane the Younger supplied evidence in relation to Strafford's claimed improper use of the army in Ireland, alleging that Strafford was encouraging the King to use his army raised in Ireland to threaten England into compliance. This evidence was obtained from Vane's father, Henry Vane the Elder, a member of the King's Privy council, who refused to confirm it in Parliament out of loyalty to Charles. On 10 April, Pym's case collapsed, but Pym made a direct appeal to Henry Vane the Younger to produce a copy of the notes from the King's Privy council, discovered by the younger Vane and secretly turned over to Pym, to the great anguish of the Elder Vane. These notes from the King's Privy Council contained evidence Strafford had told the King, "Sir, you have done your duty, and your subjects have failed in theirs; and therefore you are absolved from the rules of government, and may supply yourself by extraordinary ways; you have an army in Ireland, with which you may reduce the kingdom."
Pym immediately launched a Bill of Attainder, stating Strafford's guilt and demanding that the Earl be put to death. Unlike a guilty finding in a court case, attainder did not require a legal burden of proof, but it did require the king's approval. Charles, however, guaranteed Strafford that he would not sign the attainder, without which the bill could not be passed. Furthermore, the Lords were opposed to the severity of the sentence of death imposed upon Strafford. Yet, increased tensions and a plot in the army to support Strafford began to sway the issue. On 21 April, the Commons passed the Bill (204 in favour, 59 opposed, and 250 abstained), and the Lords acquiesced. Charles, still incensed over the Commons' handling of Buckingham, refused. Strafford himself, hoping to head off the war he saw looming, wrote to the king and asked him to reconsider. Charles, fearing for the safety of his family, signed on 10 May. Strafford was beheaded two days later. In the meantime both Parliament and the King agreed to an independent investigation into the king's involvement in Strafford's plot.
The Long Parliament then passed the Triennial Act, also known as the Dissolution Act in May 1641, to which the Royal Assent was readily granted. The Triennial Act required that Parliament be summoned at least once every three years, and that when the King failed to issue proper summons, the members could assemble on their own. This act also forbade ship money without Parliament's consent, fines in destraint of knighthood and forced loans. Monopolies were cut back severely, and the Courts of Star Chamber and High Commission were abolished by the Habeas Corpus Act 1640 and the Triennial Act respectively. All remaining forms of taxation were legalised and regulated by the Tonnage and Poundage Act. On 3 May, Parliament decreed The Protestation, attacking the 'wicked counsels' of Charles's government, whereby those who signed the petition undertook to defend 'the true reformed religion', parliament, and the king's person, honour and estate. Throughout May, the House of Commons launched several bills attacking bishops and episcopalianism in general, each time defeated in the Lords.
It was hoped by both Charles and Parliament that the execution of Strafford and the Protestation would end the drift towards war; in fact, they encouraged it. Charles and his supporters continued to resent Parliament's demands, while Parliamentarians continued to suspect Charles of wanting to impose episcopalianism and unfettered royal rule by military force. Within months, the Irish Catholics, fearing a resurgence of Protestant power, struck first, and all Ireland soon descended into chaos. Rumours circulated that the King supported the Irish, and Puritan members of the Commons soon started murmuring that this exemplified the fate that Charles had in store for them all.
In early January 1642, accompanied by 400 soldiers, Charles attempted to arrest five members of the House of Commons on a charge of treason. This attempt failed. When the troops marched into Parliament, Charles enquired of William Lenthall, the Speaker, as to the whereabouts of the five. Lenthall replied, "May it please your Majesty, I have neither eyes to see nor tongue to speak in this place but as the House is pleased to direct me, whose servant I am here." In other words, the Speaker proclaimed himself a servant of Parliament, rather than of the King.
Local grievances.
In the summer of 1642 these national troubles helped to polarise opinion, ending indecision about which side to support or what action to take. Opposition to Charles also arose owing to many local grievances. For example, the imposition of drainage-schemes in The Fens negatively affected the livelihood of thousands of people after the King awarded a number of drainage-contracts. Many regarded the King as indifferent to the welfare of the people, and this played a role in bringing a large part of eastern England into Parliament’s camp. This sentiment brought with it people such as the Earl of Manchester and Oliver Cromwell, each a notable wartime adversary of the King. Conversely, one of the leading drainage contractors, the Earl of Lindsey, was to die fighting for the King at the Battle of Edgehill.
First English Civil War (1642-1646).
In early January 1642, a few days after his failure to capture five members of the House of Commons, fearing for the safety of his family and retinue, Charles left the London area for the north of the country. Further negotiations by frequent correspondence between the King and the Long Parliament through to early summer proved fruitless. As the summer progressed, cities and towns declared their sympathies for one faction or the other: for example, the garrison of Portsmouth under the command of Sir George Goring declared for the King, but when Charles tried to acquire arms for his cause from Kingston upon Hull, the depository for the weapons used in the previous Scottish campaigns, Sir John Hotham, the military governor appointed by Parliament in January, refused to let Charles enter Hull, and when Charles returned with more men later, Hotham drove them off. Charles issued a warrant for Hotham to be arrested as a traitor but was powerless to enforce it. Throughout the summer months, tensions rose and there was brawling in a number of places, with the first death from the conflict taking place in Manchester.
At the outset of the conflict, much of the country remained neutral, though the Royal Navy and most English cities favoured Parliament, while the King found considerable support in rural communities. Historians estimate that between them, both sides had only about 15,000 men. However, the war quickly spread and eventually involved every level of society. Many areas attempted to remain neutral. Some formed bands of Clubmen to protect their localities against the worst excesses of the armies of both sides, but most found it impossible to withstand both the King and Parliament. On one side, the King and his supporters fought for traditional government in Church and state. On the other, most supporters of the Parliamentary cause initially took up arms to defend what they thought of as the traditional balance of government in Church and state, which the bad advice the King had received from his advisers had undermined before and during the "Eleven Years' Tyranny." The views of the Members of Parliament ranged from unquestioning support of the King – at one point during the First Civil War, more members of the Commons and Lords gathered in the King's Oxford Parliament than at Westminster – through to radicals, who wanted major reforms in favour of religious independence and the redistribution of power at the national level. However, even the most radical supporters of the Parliamentarian cause still favoured the retention of Charles on the throne.
After the debacle at Hull, Charles moved on to Nottingham, where on 22 August 1642, he raised the royal standard. When he raised his standard, Charles had with him about 2,000 cavalry and a small number of Yorkshire infantry-men, and using the archaic system of a Commission of Array, Charles's supporters started to build a larger army around the standard. Charles moved in a south-westerly direction, first to Stafford, and then on to Shrewsbury, because the support for his cause seemed particularly strong in the Severn valley area and in North Wales. While passing through Wellington, in what became known as the "Wellington Declaration," he declared that he would uphold the "Protestant religion, the laws of England, and the liberty of Parliament."
The Parliamentarians who opposed the King had not remained passive during this pre-war period. As in the case of Kingston upon Hull, they had taken measures to secure strategic towns and cities by appointing to office men sympathetic to their cause, and on 9 June they had voted to raise an army of 10,000 volunteers and appointed Robert Devereux, 3rd Earl of Essex commander three days later. He received orders "to rescue His Majesty's person, and the persons of the Prince [of Wales] and the Duke of York out of the hands of those desperate persons who were about them." The Lords Lieutenant, whom Parliament appointed, used the Militia Ordinance to order the militia to join Essex's army.
Two weeks after the King had raised his standard at Nottingham, Essex led his army north towards Northampton, picking up support along the way (including a detachment of Cambridgeshire cavalry raised and commanded by Oliver Cromwell). By the middle of September Essex's forces had grown to 21,000 infantry and 4,200 cavalry and dragoons. On 14 September he moved his army to Coventry and then to the north of the Cotswolds, a strategy which placed his army between the Royalists and London. With the size of both armies now in the tens of thousands, and only Worcestershire between them, it was inevitable that cavalry reconnaissance units would sooner or later meet. This happened in the first major skirmish of the Civil War, when a cavalry troop of about 1,000 Royalists commanded by Prince Rupert, a German nephew of the King and one of the outstanding cavalry commanders of the war, defeated a Parliamentary cavalry detachment under the command of Colonel John Brown in the Battle of Powick Bridge, at a bridge across the River Teme close to Worcester.
Rupert withdrew to Shrewsbury, where a council-of-war discussed two courses of action: whether to advance towards Essex's new position near Worcester, or to march along the now opened road towards London. The Council decided to take the London route, but not to avoid a battle, for the Royalist generals wanted to fight Essex before he grew too strong, and the temper of both sides made it impossible to postpone the decision. In the Earl of Clarendon's words: "it was considered more counsellable to march towards London, it being morally sure that Essex would put himself in their way". Accordingly, the army left Shrewsbury on 12 October, gaining two days' start on the enemy, and moved south-east. This had the desired effect, as it forced Essex to move to intercept them.
The first pitched battle of the war, fought at Edgehill on 23 October 1642, proved inconclusive, and both the Royalists and Parliamentarians claimed it as a victory. The second field action of the war, the stand-off at Turnham Green, saw Charles forced to withdraw to Oxford. This city would serve as his base for the remainder of the war.
In 1643 the Royalist forces won at Adwalton Moor, and gained control of most of Yorkshire. In the Midlands, a Parliamentary force under Sir John Gell besieged and captured the cathedral city of Lichfield, after the death of the original commander, Lord Brooke. This group subsequently joined forces with Sir John Brereton to fight the inconclusive Battle of Hopton Heath (19 March 1643), where the Royalist commander, the Earl of Northampton, was killed. Subsequent battles in the west of England at Lansdowne and at Roundway Down also went to the Royalists. Prince Rupert could then take Bristol. In the same year, Oliver Cromwell formed his troop of "Ironsides", a disciplined unit that demonstrated his military leadership ability. With their assistance, he won a victory at the Battle of Gainsborough in July.
At this stage, from 7th to 9 August 1643, there were some popular demonstrations in London—both pro and against war. They were protesting at Westminster. A peace demonstration by London women, which turned violent, was suppressed by William Waller's regiment of horse. Some women were beaten and even killed, and many arrested.
Following these events of August, the representative of Venice in England reported to the doge that the London government took considerable measures to stifle dissent.
In general, the early part of the war went well for the Royalists. The turning point came in the late summer and early autumn of 1643, when the Earl of Essex's army forced the king to raise the siege of Gloucester and then brushed the Royalist army aside at the First Battle of Newbury (20 September 1643), in order to return triumphantly to London. Other Parliamentarian forces won the Battle of Winceby, giving them control of Lincoln. Political manoeuvering to gain an advantage in numbers led Charles to negotiate a ceasefire in Ireland, freeing up English troops to fight on the Royalist side in England, while Parliament offered concessions to the Scots in return for aid and assistance.
With the help of the Scots, Parliament won at Marston Moor (2 July 1644), gaining York and the north of England. Cromwell's conduct in this battle proved decisive, and demonstrated his potential as both a political and an important military leader. The defeat at the Battle of Lostwithiel in Cornwall, however, marked a serious reverse for Parliament in the south-west of England. Subsequent fighting around Newbury (27 October 1644), though tactically indecisive, strategically gave another check to Parliament.
In 1645 Parliament reaffirmed its determination to fight the war to a finish. It passed the Self-denying Ordinance, by which all members of either House of Parliament laid down their commands, and re-organized its main forces into the New Model Army ("Army"), under the command of Sir Thomas Fairfax, with Cromwell as his second-in-command and Lieutenant-General of Horse. In two decisive engagements—the Battle of Naseby on 14 June and the Battle of Langport on 10 July—the Parliamentarians effectively destroyed Charles' armies.
In the remains of his English realm Charles attempted to recover a stable base of support by consolidating the Midlands. He began to form an axis between Oxford and Newark on Trent in Nottinghamshire. Those towns had become fortresses and showed more reliable loyalty to him than to others. He took Leicester, which lies between them, but found his resources exhausted. Having little opportunity to replenish them, in May 1646 he sought shelter with a Presbyterian Scottish army at Southwell in Nottinghamshire. Charles was eventually handed over to the English Parliament by the Scots and was imprisoned. This marked the end of the First English Civil War.
Second English Civil War (1648-1649).
Charles I took advantage of the deflection of attention away from himself to negotiate a secret treaty with the Scots, again promising church reform, on 28 December 1647. Under the agreement, called the "Engagement", the Scots undertook to invade England on Charles' behalf and restore him to the throne on condition of the establishment of Presbyterianism for three years.
A series of Royalist uprisings throughout England and a Scottish invasion occurred in the summer of 1648. Forces loyal to Parliament put down most of the uprisings in England after little more than skirmishes, but uprisings in Kent, Essex and Cumberland, the rebellion in Wales, and the Scottish invasion involved the fighting of pitched battles and prolonged sieges.
In the spring of 1648 unpaid Parliamentarian troops in Wales changed sides. Colonel Thomas Horton defeated the Royalist rebels at the Battle of St Fagans (8 May) and the rebel leaders surrendered to Cromwell on 11 July after the protracted two-month siege of Pembroke. Sir Thomas Fairfax defeated a Royalist uprising in Kent at the Battle of Maidstone on 1 June. Fairfax, after his success at Maidstone and the pacification of Kent, turned northward to reduce Essex, where, under their ardent, experienced and popular leader Sir Charles Lucas, the Royalists had taken up arms in great numbers. Fairfax soon drove the enemy into Colchester, but his first attack on the town met with a repulse and he had to settle down to a long siege.
In the North of England, Major-General John Lambert fought a very successful campaign against a number of Royalist uprisings—the largest that of Sir Marmaduke Langdale in Cumberland. Thanks to Lambert's successes, the Scottish commander, the Duke of Hamilton, had perforce to take the western route through Carlisle in his pro-Royalist Scottish invasion of England. The Parliamentarians under Cromwell engaged the Scots at the Battle of Preston (17–19 August). The battle took place largely at Walton-le-Dale near Preston in Lancashire, and resulted in a victory by the troops of Cromwell over the Royalists and Scots commanded by Hamilton. This Parliamentarian victory marked the end of the Second English Civil War.
Nearly all the Royalists who had fought in the First Civil War had given their parole not to bear arms against the Parliament, and many of these, like Lord Astley, refused to break their word by taking any part in the second war. So the victors in the Second Civil War showed little mercy to those who had brought war into the land again. On the evening of the surrender of Colchester, Parliamentarians had Sir Charles Lucas and Sir George Lisle shot. Parliamentary authorities sentenced the leaders of the Welsh rebels, Major-General Rowland Laugharne, Colonel John Poyer and Colonel Rice Powel to death, but executed Poyer alone (25 April 1649), having selected him by lot. Of five prominent Royalist peers who had fallen into the hands of Parliament, three, the Duke of Hamilton, the Earl of Holland, and Lord Capel, one of the Colchester prisoners and a man of high character, were beheaded at Westminster on 9 March.
Trial of Charles I for treason.
Charles' secret pacts and encouragement of his supporters to break their parole caused Parliament to debate whether to return the King to power at all. Those who still supported Charles' place on the throne, such as the army leader and moderate Fairfax, tried once more to negotiate with him. Furious that Parliament continued to countenance Charles as a ruler, the Army marched on Parliament and conducted "Pride's Purge" (named after the commanding officer of the operation, Thomas Pride) in December 1648. Troops arrested 45 Members of Parliament and kept 146 out of the chamber. They allowed only 75 Members in, and then only at the Army's bidding. This Rump Parliament received orders to set up, in the name of the people of England, a High Court of Justice for the trial of Charles I for treason. Fairfax, a constitutional monarchist and moderate, refused to participate whatsoever in the trial and resigned as head of the army, allowing Oliver Cromwell to ascend in power.
At the end of the trial the 59 Commissioners (judges) found Charles I guilty of high treason, as a "tyrant, traitor, murderer and public enemy". His beheading took place on a scaffold in front of the Banqueting House of the Palace of Whitehall on 30 January 1649. After the Restoration in 1660, of the surviving regicides not living in exile, nine were executed and most of the rest sentenced to life imprisonment.
Third English Civil War (1649-1651).
Ireland.
Ireland had known continuous war since the rebellion of 1641, with most of the island controlled by the Irish Confederates. Increasingly threatened by the armies of the English Parliament after Charles I's arrest in 1648, the Confederates signed a treaty of alliance with the English Royalists. The joint Royalist and Confederate forces under the Duke of Ormonde attempted to eliminate the Parliamentary army holding Dublin, but their opponents routed them at the Battle of Rathmines (2 August 1649). As the former Member of Parliament Admiral Robert Blake blockaded Prince Rupert's fleet in Kinsale, Oliver Cromwell could land at Dublin on 15 August 1649 with an army to quell the Royalist alliance in Ireland.
Cromwell's suppression of the Royalists in Ireland during 1649 still has a strong resonance for many Irish people. After the siege of Drogheda, the massacre of nearly 3,500 people — comprising around 2,700 Royalist soldiers and 700 others, including civilians, prisoners, and Catholic priests (Cromwell claimed all the men were carrying arms) — became one of the historical memories that has driven Irish-English and Catholic-Protestant strife during the last three centuries. The Parliamentarian conquest of Ireland ground on for another four years until 1653, when the last Irish Confederate and Royalist troops surrendered. The victors confiscated almost all Irish Catholic-owned land in the wake of the conquest and distributed it to the Parliament's creditors, to the Parliamentary soldiers who served in Ireland, and to English people who had settled there before the war.
Scotland.
The execution of Charles I altered the dynamics of the Civil War in Scotland, which had raged between Royalists and Covenanters since 1644. By 1649, the struggle had left the Royalists there in disarray and their erstwhile leader, the Marquess of Montrose, had gone into exile. At first, Charles II encouraged Montrose to raise a Highland army to fight on the Royalist side. However, when the Scottish Covenanters (who did not agree with the execution of Charles I and who feared for the future of Presbyterianism under the new Commonwealth) offered him the crown of Scotland, Charles abandoned Montrose to his enemies. However, Montrose, who had raised a mercenary force in Norway, had already landed and could not abandon the fight. He did not succeed in raising many Highland clans and the Covenanters defeated his army at the Battle of Carbisdale in Ross-shire on 27 April 1650. The victors captured Montrose shortly afterwards and took him to Edinburgh. On 20 May the Scottish Parliament sentenced him to death and had him hanged the next day.
Charles II landed in Scotland at Garmouth in Morayshire on 23 June 1650 and signed the 1638 National Covenant and the 1643 Solemn League and Covenant shortly after coming ashore. With his original Scottish Royalist followers and his new Covenanter allies, King Charles II became the greatest threat facing the new English republic. In response to the threat, Cromwell left some of his lieutenants in Ireland to continue the suppression of the Irish Royalists and returned to England.
He arrived in Scotland on 22 July 1650 and proceeded to lay siege to Edinburgh. By the end of August disease and a shortage of supplies had reduced his army, and he had to order a retreat towards his base at Dunbar. A Scottish army, assembled under the command of David Leslie, tried to block the retreat, but Cromwell defeated them at the Battle of Dunbar on 3 September. Cromwell's army then took Edinburgh, and by the end of the year his army had occupied much of southern Scotland.
In July 1651, Cromwell's forces crossed the Firth of Forth into Fife and defeated the Scots at the Battle of Inverkeithing (20 July 1651). The New Model Army advanced towards Perth, which allowed Charles, at the head of the Scottish army, to move south into England. Cromwell followed Charles into England, leaving George Monck to finish the campaign in Scotland. Monck took Stirling on 14 August and Dundee on 1 September. The next year, 1652, saw the mopping up of the remnants of Royalist resistance, and under the terms of the "Tender of Union", the Scots received 30 seats in a united Parliament in London, with General Monck appointed as the military governor of Scotland.
England.
Although Cromwell's New Model Army had defeated a Scottish army at Dunbar, Cromwell could not prevent Charles II from marching from Scotland deep into England at the head of another Royalist army. The Royalists marched to the west of England because English Royalist sympathies were strongest in that area, but although some English Royalists joined the army, they came in far fewer numbers than Charles and his Scottish supporters had hoped. Cromwell finally engaged and defeated the new king at Worcester on 3 September 1651. Charles II escaped, via safe houses and a famous oak tree, to France, ending the civil wars.
Political control.
During the Wars, the Parliamentarians established a number of successive committees to oversee the war-effort. The first of these, the Committee of Safety, set up in July 1642, comprised 15 Members of Parliament. Following the Anglo-Scottish alliance against the Royalists, the Committee of Both Kingdoms replaced the Committee of Safety between 1644 and 1648. Parliament dissolved the Committee of Both Kingdoms when the alliance ended, but its English members continued to meet and became known as the Derby House Committee. A second Committee of Safety then replaced that committee.
Episcopacy during the English Civil War.
During the period of the English Civil War, the role of bishops as wielders of political power and as upholders of the established church became a matter of heated political controversy. John Calvin formulated a doctrine of Presbyterianism, which held that in the New Testament the offices of "presbyter" and "episkopos" were identical; he rejected the doctrine of apostolic succession. Calvin's follower John Knox brought Presbyterianism to Scotland when the Scottish church was reformed in 1560. In practice, Presbyterianism meant that committees of lay elders had a substantial voice in church government, as opposed to merely being subjects to a ruling hierarchy.
This vision of at least partial democracy in ecclesiology paralleled the struggles between Parliament and the King. A body within the Puritan movement in the Church of England sought to abolish the office of bishop and remake the Church of England along Presbyterian lines. The Martin Marprelate tracts (1588–89), applying the pejorative name of "prelacy" to the church hierarchy, attacked the office of bishop with satire that deeply offended Elizabeth I and her Archbishop of Canterbury John Whitgift. The vestments controversy also related to this movement, seeking further reductions in church ceremony, and labelling the use of elaborate vestments as "unedifying" and even idolatrous.
King James I, reacting against the perceived contumacy of his Presbyterian Scottish subjects, adopted "No Bishop, no King" as a slogan; he tied the hierarchical authority of the bishop to the absolute authority he sought as king, and viewed attacks on the authority of the bishops as attacks on his own authority. Matters came to a head when King Charles I appointed William Laud as the Archbishop of Canterbury; Laud aggressively attacked the Presbyterian movement and sought to impose the full Anglican liturgy. The controversy eventually led to Laud's impeachment for treason by a bill of attainder in 1645, and subsequent execution. Charles also attempted to impose episcopacy on Scotland; the Scots' violent rejection of bishops and liturgical worship sparked the Bishops' Wars in 1639–1640.
During the height of Puritan power in the Commonwealth and the Protectorate, episcopacy was formally abolished in the Church of England on 9 October 1646. The Church of England remained Presbyterian until the Restoration of the monarchy with Charles II in 1660.
English overseas possessions during the English Civil War.
During the period of the English Civil War, the English overseas possessions were highly involved.
Casualties.
As usual in wars of this era, disease caused more deaths than combat. There are no accurate figures for these periods, and it is not possible to give a precise overall figure for those killed in battle, as opposed to those who died from disease, or even from a natural decline in population.
Figures for casualties during this period are unreliable, but some attempt has been made to provide rough estimates.
In England, a conservative estimate is that roughly 100,000 people died from war-related disease during the three civil wars. Historical records count 84,830 dead from the wars themselves. Counting in accidents and the two Bishops' wars, an estimate of 190,000 dead is achieved, out of a total population of about five million.
Figures for Scotland are more unreliable and should be treated with greater caution. Casualties include the deaths of prisoners-of-war in conditions that accelerated their deaths, with estimates of 10,000 prisoners not surviving or not returning home (8,000 captured during and immediately after the Battle of Worcester were deported to New England, Bermuda and the West Indies to work for landowners as indentured labourers). There are no figures to calculate how many died from war-related diseases, but if the same ratio of disease to battle deaths from English figures is applied to the Scottish figures, a not unreasonable estimate of 60,000 people is achieved, from a population of about one million.
Figures for Ireland are described as "miracles of conjecture". Certainly the devastation inflicted on Ireland was massive, with the best estimate provided by Sir William Petty, the father of English demography. Petty estimates that 112,000 Protestants and 504,000 Catholics were killed through plague, war and famine, giving an estimated total of 616,000 dead, from a pre-war population of about one and a half million. Although Petty's figures are the best available, they are still acknowledged as being tentative; they do not include the estimate of 40,000 driven into exile, some of whom served as soldiers in European continental armies, while others were sold as indentured servants to New England and the West Indies. Many of those sold to landowners in New England eventually prospered, but many of those sold to landowners in the West Indies were worked to death.
These estimates indicate that England suffered a 3.7% loss of population, Scotland a loss of 6%, while Ireland suffered a loss of 41% of its population. Putting these numbers into the context of other catastrophes helps to understand the devastation to Ireland in particular. The Great Hunger of 1845–1852 resulted in a loss of 16% of the population, while during the Second World War the population of the Soviet Union fell by 16%.
Popular gains.
Ordinary people took advantage of the dislocation of civil society during the 1640s to derive advantages for themselves. The contemporary guild democracy movement won its greatest successes among London's transport workers, notably the Thames watermen. Rural communities seized timber and other resources on the sequestrated estates of royalists and Catholics, and on the estates of the royal family and the church hierarchy. Some communities improved their conditions of tenure on such estates. The old "status quo" began a retrenchment after the end of the First Civil War in 1646, and more especially after the restoration of monarchy in 1660. But some gains were long-term. The democratic element introduced in the watermen's company in 1642, for example, survived, with vicissitudes, until 1827.
Aftermath.
The wars left England, Scotland, and Ireland among the few countries in Europe without a monarch. In the wake of victory, many of the ideals (and many of the idealists) became sidelined. The republican government of the Commonwealth of England ruled England (and later all of Scotland and Ireland) from 1649 to 1653 and from 1659 to 1660. Between the two periods, and due to in-fighting amongst various factions in Parliament, Oliver Cromwell ruled over the Protectorate as Lord Protector (effectively a military dictator) until his death in 1658.
Upon his death, Oliver Cromwell's son Richard became Lord Protector, but the Army had little confidence in him. After seven months the Army removed Richard, and in May 1659 it re-installed the Rump. However, since the Rump Parliament acted as though nothing had changed since 1653 and as though it could treat the Army as it liked, military force shortly afterwards dissolved this, as well. After the second dissolution of the Rump, in October 1659, the prospect of a total descent into anarchy loomed as the Army's pretence of unity finally dissolved into factions.
Into this atmosphere General George Monck, Governor of Scotland under the Cromwells, marched south with his army from Scotland. On 4 April 1660, in the Declaration of Breda, Charles II made known the conditions of his acceptance of the Crown of England. Monck organised the Convention Parliament, which met for the first time on 25 April 1660. On 8 May 1660, it declared that King Charles II had reigned as the lawful monarch since the execution of Charles I in January 1649. Charles returned from exile on 23 May 1660. On 29 May 1660, the populace in London acclaimed him as king. His coronation took place at Westminster Abbey on 23 April 1661. These events became known as the "Restoration".
Although the monarchy was restored, it was still only with the consent of Parliament; therefore, the civil wars effectively set England and Scotland on course to adopt a parliamentary monarchy form of government. This system would result in the outcome that the future Kingdom of Great Britain, formed in 1707 under the Acts of Union, would manage to forestall the kind of often-bloody revolution, typical of European republican movements that followed the Jacobin revolution in 18th century France and the later success of Napoleon, which generally resulted in the total abolition of monarchy. It was no coincidence that the United Kingdom was spared the wave of revolutions that occurred in Europe in the 1840s. Specifically, future monarchs became wary of pushing Parliament too hard, and Parliament effectively chose the line of royal succession in 1688 with the Glorious Revolution and in the 1701 Act of Settlement. After the Restoration, Parliament's factions became political parties (later becoming the Tories and Whigs) with competing views and varying abilities to influence the decisions of their monarchs.
Historiography and explanations for the English Civil War.
In the early decades of the 20th century the Whig school was the dominant theoretical view. They explained the Civil War as resulting from a centuries-long struggle between Parliament (especially the House of Commons) and the Monarchy, with Parliament defending the traditional rights of Englishmen, while the Stuart monarchy continually attempted to expand its right to arbitrarily dictate law. The most important Whig historian, S.R. Gardiner, popularised the English Civil War as a 'Puritan Revolution': challenging the repressive Stuart Church, and preparing the way for religious toleration in the Restoration. Thus, Puritanism was the natural ally of a people preserving their traditional rights against arbitrary monarchical power.
The Whig view was challenged and largely superseded by the Marxist school, which became popular in the 1940s, and which interpreted the English Civil War as a bourgeois revolution. According to Marxist historian Christopher Hill:
The Civil War was a class war, in which the despotism of Charles I was defended by the reactionary forces of the established Church and conservative landlords, Parliament beat the King because it could appeal to the enthusiastic support of the trading and industrial classes in town and countryside, to the yeomen and progressive gentry, and to wider masses of the population whenever they were able by free discussion to understand what the struggle was really about.—Christopher Hill
In the 1970s, revisionist historians challenged both the Whig and the Marxist theories, notably in the 1973 anthology "The Origins of the English Civil War" (Conrad Russell ed.). These historians produced work focused on the minutiae of the years immediately preceding the civil war, thereby returning to the contingency-based historiography of Clarendon's famous contemporary history "History of the Rebellion and Civil Wars in England". This, it was claimed, demonstrated that factional war-allegiance patterns did not fit either Whig or Marxist history. Parliament was not inherently progressive, with the events of 1640 a precursor for the Glorious Revolution, nor did Puritans necessarily ally themselves with Parliamentarians. Many members of the bourgeoisie fought for the King, while many landed aristocrats supported Parliament. Thus, revisionist historians claim to have discredited some Whig and Marxist interpretations of the English Civil War.
From the 1990s, a number of historians discarded and replaced the historical title "English Civil War" with the titles the Wars of the Three Kingdoms and the "British Civil Wars", positing that the civil war in England cannot be understood isolated from events in other parts of Great Britain and Ireland; King Charles I remains crucial, not just as King of England, but also because of his relationship with the peoples of his other realms. For example, the wars began when King Charles I tried imposing an Anglican Prayer Book upon Scotland, and when this was met with resistance from the Covenanters, he needed an army to impose his will. However, this forced him to call an English Parliament to raise new taxes to pay for the army. The English Parliaments were not willing to grant Charles the revenue he needed to pay for the Scottish expeditionary army unless he addressed their grievances. By the early 1640s, Charles was left in a state of near permanent crisis management; often he was not willing to concede enough ground to any one faction to neutralise the threat, and in some circumstances to do so would only antagonise another faction. For example, Charles finally agreed upon terms with the Covenanters in August 1641, but although this might have weakened the position of the English Parliament, the Irish Rebellion of 1641 broke out in October 1641, largely negating the political advantage he had obtained by relieving himself of the cost of the Scottish invasion.
Thomas Hobbes gives a much earlier historical account of the English Civil War in his essay Behemoth, written in 1668 and published in 1681. He reports that the causes of the war were the doctrines of politics and conflicts that arose from science that disputed those political doctrines. 
Behemoth offered a uniquely historical and philosophical approach to naming the catalysts for the war. It also served as a political statement to explain why King Charles I was incapable of holding his place of power and maintaining peace in his kingdom.
Specifically, Hobbes analyses the following aspects of English thought during the war (listed in order of his discussions in Behemoth): the opinions of divinity and politics that spurred rebellion; rhetoric and doctrine used by the rebels against the king; and how opinions about “taxation, the conscription of soldiers, and military strategy” affected the outcomes of battles and shifts of sovereignty.
Hobbes offered a unique contribution to historical interpretation of the civil war through his Behemoth by connecting the civil war to the motivations of intellectuals who Hobbes reports caused it by trying to spread certain ideas throughout the nation, largely for the sake of displaying their own wisdom and learning. 
Hobbes held the belief that clerical pretensions had contributed significantly to the trouble during the civil war—“whether those of puritan fundamentalists, papal supremacists or divine right Episcopalians” (Sommerville). Hobbes wanted to revoke all of independent power of the clergy and to change the civil system such that they were controlled by the state.
Some scholars suggest that Behemoth has not received its due respect as an academic work, being comparatively overlooked and underrated in the shadow of Leviathan. One factor that may have contributed to its lack of reception as a historical work is that it takes the form of a dialogue. While philosophical dialogues are common, historical ones are not. Other factors that hindered its success include King Charles II refusing it to be published and Hobbes’ chiefly interpretive approach to the historical narrative.
Much can be gleaned about Hobbes as a person from looking at the difficulties he faced while seeking an audience for Behemoth. The essay illuminates a flaw shared by most of Hobbes's political philosophy as well, which is his lack of ability or willingness to empathize with perspectives that largely differed from his own. As his perspective was so unique, Hobbes struggled to understand the thinking of most of his potential audience and people in general. For instance, he accredits the Presbyterians and Parliamentarians with “improbably long-planned and wicked ambitions”. What’s more, “he hardly understands the orthodox Royalists (he was himself a highly unorthodox Royalist) any better, and he makes only limited concessions of sincerity to the religious feelings of the various parties”.
Re-enactments.
Two large historical societies exist, The Sealed Knot and The English Civil War Society, which regularly re-enact events and battles of the Civil War in full period costume.
References.
</dl>

</doc>
<doc id="9710" url="http://en.wikipedia.org/wiki?curid=9710" title="Elementary algebra">
Elementary algebra

Elementary algebra encompasses some of the basic concepts of algebra, one of the main branches of mathematics. It is typically taught to secondary school students and builds on their understanding of arithmetic. Whereas arithmetic deals with specified numbers, algebra introduces quantities without fixed values, known as variables. This use of variables entails a use of algebraic notation and an understanding of the general rules of the operators introduced in arithmetic. Unlike abstract algebra, elementary algebra is not concerned with algebraic structures outside the realm of real and complex numbers.
The use of variables to denote quantities allows general relationships between quantities to be formally and concisely expressed, and thus enables solving a broader scope of problems. Most quantitative results in science and mathematics are expressed as algebraic equations.
Algebraic notation.
Algebraic notation describes how algebra is written. It follows certain rules and conventions, and has its own terminology. For example, the expression formula_1 has the following components:
<br>
1 : Exponent (power), 2 : Coefficient, 3 : term, 4 : operator, 5 : constant, formula_2 : variables
A "coefficient" is a numerical value which multiplies a variable (the operator is omitted). A "term" is an addend or a summand, a group of coefficients, variables, constants and exponents that may be separated from the other terms by the plus and minus operators. Letters represent variables and constants. By convention, letters at the beginning of the alphabet (e.g. formula_3) are typically used to represent constants, and those toward the end of the alphabet (e.g. formula_2 and formula_5) are used to represent variables. They are usually written in italics.
Algebraic operations work in the same way as arithmetic operations, such as addition, subtraction, multiplication, division and exponentiation. and are applied to algebraic variables and terms. Multiplication symbols are usually omitted, and implied when there is no space between two variables or terms, or when a coefficient is used. For example, formula_6 is written as formula_7, and formula_8 may be written formula_9.
Usually terms with the highest power (exponent), are written on the left, for example, formula_10 is written to the left of formula_11. When a coefficient is one, it is usually omitted (e.g. formula_12 is written formula_10). Likewise when the exponent (power) is one, (e.g. formula_14 is written formula_15). When the exponent is zero, the result is always 1 (e.g. formula_16 is always rewritten to formula_17). However formula_18, being undefined, should not appear in an expression, and care should be taken in simplifying expressions in which variables may appear in exponents.
Alternative notation.
Other types of notation are used in algebraic expressions when the required formatting is not available, or can not be implied, such as where only letters and symbols are available. For example, exponents are usually formatted using superscripts, e.g. formula_10. In plain text, and in the TeX mark-up language, the caret symbol "^" represents exponents, so formula_10 is written as "x^2". In programming languages such as Ada, Fortran, Perl, Python and Ruby, a double asterisk is used, so formula_10 is written as "x**2". Many programming languages and calculators use a single asterisk to represent the multiplication symbol, and it must be explicitly used, for example, formula_15 is written "3*x".
Concepts.
Variables.
Elementary algebra builds on and extends arithmetic by introducing letters called variables to represent general (non-specified) numbers. This is useful for several reasons.
Evaluating expressions.
Algebraic expressions may be evaluated and simplified, based on the basic properties of arithmetic operations (addition, subtraction, multiplication, division and exponentiation). For example,
Equations.
An equation states that two expressions are equal using the symbol for equality, formula_42 (the equals sign). One of the most well-known equations describes Pythagoras' law relating the length of the sides of a right angle triangle:
This equation states that formula_44, representing the square of the length of the side that is the hypotenuse (the side opposite the right angle), is equal to the sum (addition) of the squares of the other two sides whose lengths are represented by formula_45 and formula_46.
An equation is the claim that two expressions have the same value and are equal. Some equations are true for all values of the involved variables (such as formula_47); such equations are called identities. Conditional equations are true for only some values of the involved variables, e.g. formula_48 is true only for formula_49 and formula_50. The values of the variables which make the equation true are the solutions of the equation and can be found through equation solving.
Another type of equation is an inequality. Inequalities are used to show that one side of the equation is greater, or less, than the other. The symbols used for this are: formula_51 where formula_52 represents 'greater than', and formula_53 where formula_54 represents 'less than'. Just like standard equality equations, numbers can be added, subtracted, multiplied or divided. The only exception is that when multiplying or dividing by a negative number, the inequality symbol must be flipped.
Properties of equality.
By definition, equality is an equivalence relation, meaning it has the properties (a) reflexive (i.e. formula_55), (b) symmetric (i.e. if formula_56 then formula_57) (c) transitive (i.e. if formula_56 and formula_59 then formula_60). It also satisfies the important property that if two symbols are used for equal things, then one symbol can be substituted for the other in any true statement about the first and the statement will remain true. This implies the following properties:
Properties of inequality.
The relations "less than" formula_54 and greater than formula_52 have the property of transitivity:
By reversing the inequation, formula_54 and formula_52 can be swapped, for example:
Substitution.
Substitution is replacing the terms in an expression to create a new expression. Substituting 3 for a in the expression a*5 makes a new expression 3*5 with meaning 15. Substituting the terms of a statement makes a new statement. When the original statement is true independent of the values of the terms, the statement created by substitutions is also true. Hence definitions can be made in symbolic terms and interpreted through substitution: if formula_88, where := means "is defined to equal", substituting 3 for formula_45 informs the reader of this statement that formula_90 means 3*3=9. Often it's not known whether the statement is true independent of the values of the terms, and substitution allows one to derive restrictions on the possible values, or show what conditions the statement holds under. For example, taking the statement x+1=0, if x is substituted with 1, this imples 1+1=2=0, which is false, which implies that if x+1=0 then x can't be 1.
If "x" and "y" are integers, rationals, or real numbers, then "xy"=0 implies "x"=0 or "y"=0. Suppose "abc"=0. Then, substituting "a" for "x" and "bc" for "y", we learn "a"=0 or "bc"=0. Then we can substitute again, letting "x"="b" and "y"="c", to show that if "bc"=0 then "b"=0 or "c"=0. Therefore if "abc"=0, then "a"=0 or ("b"=0 or "c"=0), so "abc"=0 implies "a"=0 or "b"=0 or "c"=0.
Consider if the original fact were stated as ""ab"=0 implies "a"=0 or "b"=0." Then when we say "suppose "abc"=0," we have a conflict of terms when we substitute. Yet the above logic is still valid to show that if "abc"=0 then "a"=0 or "b"=0 or "c"=0 if instead of letting "a"="a" and "b"="bc" we substitute "a" for "a" and "b" for "bc" (and with "bc"=0, substituting "b" for "a" and "c" for "b"). This shows that substituting for the terms in a statement isn't always the same as letting the terms from the statement equal the substituted terms. In this situation it's clear that if we substitute an expression "a" into the "a" term of the original equation, the "a" substituted does not refer to the "a" in the statement ""ab"=0 implies "a"=0 or "b"=0."
Solving algebraic equations.
The following sections lay out examples of some of the types of algebraic equations that may be encountered.
Linear equations with one variable.
Linear equations are so-called, because when they are plotted, they describe a straight line. The simplest equations to solve are linear equations that have only one variable. They contain only constant numbers and a single variable without an exponent. As an example, consider:
To solve this kind of equation, the technique is add, subtract, multiply, or divide both sides of the equation by the same number in order to isolate the variable on one side of the equation. Once the variable is isolated, the other side of the equation is the value of the variable. This problem and its solution are as follows:
In words: my son's age is 4.
The general form of a linear equation with one variable, can be written as: formula_93
Following the same procedure (i.e. subtract formula_46 from both sides, and then divide by formula_45), the general solution is given by formula_96
Linear equations with two variables.
A linear equation with two variables has many (i.e. an infinite number of) solutions. For example:
This can not be worked out by itself. If I told you my son's age, then there would no longer be two unknowns (variables), and the problem becomes a linear equation with just one variable, that can be solved as described above.
To solve a linear equation with two variables (unknowns), requires two related equations. For example, if I also revealed that:
Now there are two related linear equations, each with two unknowns, which lets us produce a linear equation with just one variable, by subtracting one from the other (called the elimination method):
In other words, my son is aged 12, and as I am 22 years older, I must be 34. In 10 years time, my son will 22, and I will be twice his age, 44. This problem is illustrated on the associated plot of the equations.
For other ways to solve this kind of equations, see below, System of linear equations.
Quadratic equations.
A quadratic equation is one which includes a term with an exponent of 2, for example, formula_34, and no term with higher exponent. The name derives from the Latin "quadrus", meaning square. In general, a quadratic equation can be expressed in the form formula_101, where formula_45 is not zero (if it were zero, then the equation would not be quadratic but linear). Because of this a quadratic equation must contain the term formula_103, which is known as the quadratic term. Hence formula_104, and so we may divide by formula_45 and rearrange the equation into the standard form
where formula_107 and formula_108. Solving this, by a process known as completing the square, leads to the quadratic formula
where the symbol "±" indicates that both
are solutions of the quadratic equation.
Quadratic equations can also be solved using factorization (the reverse process of which is expansion, but for two linear terms is sometimes denoted foiling). As an example of factoring:
which is the same thing as
It follows from the zero-product property that either formula_113 or formula_114 are the solutions, since precisely one of the factors must be equal to zero. All quadratic equations will have two solutions in the complex number system, but need not have any in the real number system. For example,
has no real number solution since no real number squared equals −1.
Sometimes a quadratic equation has a root of multiplicity 2, such as:
For this equation, −1 is a root of multiplicity 2. This means −1 appears two times, since the equation can be rewritten in factored form as
Complex numbers.
All quadratic equations have two solutions in complex numbers, a category that includes real numbers, imaginary numbers, and sums of real and imaginary numbers. Complex numbers first arise in the teaching of quadratic equations and the quadratic formula. For example, the quadratic equation
has solutions
Since formula_120 is not any real number, both of these solutions for "x" are complex numbers.
Exponential and logarithmic equations.
An exponential equation is one which has the form formula_121 for formula_122, which has solution
when formula_124. Elementary algebraic techniques are used to rewrite a given equation in the above way before arriving at the solution. For example, if
then, by subtracting 1 from both sides of the equation, and then dividing both sides by 3 we obtain
whence
or
A logarithmic equation is an equation of the form formula_129 for formula_122, which has solution
For example, if
then, by adding 2 to both sides of the equation, followed by dividing both sides by 4, we get
whence
from which we obtain
Radical equations.
A radical equation is one that includes a radical sign, which includes square roots, formula_136, cube roots, formula_137, and "n"th roots, formula_138. Recall that an "n"th root can be rewritten in exponential format, so that formula_138 is equivalent to formula_140. Combined with regular exponents (powers), then formula_141 (the square root of formula_11 cubed), can be rewritten as formula_143. So a common form of a radical equation is formula_144 (equivalent to formula_145) where formula_146 and formula_147 are integers. It has real solution(s):
For example, if:
then
System of linear equations.
There are different methods to solve a system of linear equations with two variables.
Elimination method.
An example of solving a system of linear equations is by using the elimination method:
Multiplying the terms in the second equation by 2:
Adding the two equations together to get:
which simplifies to
Since the fact that formula_113 is known, it is then possible to deduce that formula_156 by either of the original two equations (by using "2" instead of formula_11 ) The full solution to this problem is then
Note that this is not the only way to solve this specific system; formula_98 could have been solved before formula_11.
Substitution method.
Another way of solving the same system of linear equations is by substitution.
An equivalent for formula_98 can be deduced by using one of the two equations. Using the second equation:
Subtracting formula_164 from each side of the equation:
and multiplying by −1:
Using this formula_98 value in the first equation in the original system:
Adding "2" on each side of the equation:
which simplifies to
Using this value in one of the equations, the same solution as in the previous method is obtained.
Note that this is not the only way to solve this specific system; in this case as well, formula_98 could have been solved before formula_11.
Other types of systems of linear equations.
Inconsistent systems.
In the above example, a solution exists. However, there are also systems of equations which do not have any solution. Such a system is called inconsistent. An obvious example is
As 0≠2, the second equation in the system has no solution. Therefore, the system has no solution.
However, not all inconsistent systems are recognized at first sight. As an example, let us consider the system 
Multiplying by 2 both sides of the second equation, and adding it to the first one results in
which has clearly no solution.
Undetermined systems.
There are also systems which have infinitely many solutions, in contrast to a system with a unique solution (meaning, a unique pair of values for formula_11 and formula_98) For example:
Isolating formula_98 in the second equation:
And using this value in the first equation in the system:
The equality is true, but it does not provide a value for formula_11. Indeed, one can easily verify (by just filling in some values of formula_11) that for any formula_11 there is a solution as long as formula_186. There is an infinite number of solutions for this system.
Over- and underdetermined systems.
Systems with more variables than the number of linear equations are called underdetermined. Such a system, if it has any solutions, does not have a unique one but rather an infinitude of them. An example of such a system is
When trying to solve it, one is led to express some variables as functions of the other ones if any solutions exist, but cannot express "all" solutions numerically because there are an infinite number of them if there are any.
A system with a greater number of equations than variables is called overdetermined. If an overdetermined system has any solutions, necessarily some equations are linear combinations of the others.

</doc>
<doc id="9712" url="http://en.wikipedia.org/wiki?curid=9712" title="ERP">
ERP

ERP or Erp may refer to:

</doc>
<doc id="9713" url="http://en.wikipedia.org/wiki?curid=9713" title="Ernest Thayer">
Ernest Thayer

Ernest Lawrence Thayer (August 14, 1863 – August 21, 1940) was an American writer and poet who wrote "Casey at the Bat," "the single most famous baseball poem ever written" according to the Baseball Almanac, and "the nation’s best-known piece of comic verse—a ballad that began a native legend as colorful and permanent as that of Johnny Appleseed or Paul Bunyan."
Biography.
Thayer was born in Lawrence, Massachusetts and raised in Worcester. He graduated "magna cum laude" in philosophy from Harvard in 1885, where he was editor of the "Harvard Lampoon" and member of the Hasty Pudding theatrical club. William Randolph Hearst, a friend from both the Pudding and Lampoon, hired Thayer as humor columnist for the "San Francisco Examiner" 1886–88.
Thayer's last piece, dated June 3, 1888, was a ballad entitled "Casey" ("Casey at the Bat") which made him "a prize specimen of the one-poem poet" according to American Heritage.
It took several months after its publication for the poem to make Thayer famous, since he was hardly the boastful type and had signed the June 24 poem with the nickname "Phin" which he had used since his time on the "Lampoon". Two mysteries remain about the poem: whether anyone or anyplace was the real-life Casey and Mudville, and, if so, their actual identities. On March 31, 2007, Katie Zezima of "The New York Times" penned an article called "In 'Casey' Rhubarb, 2 Cities Cry 'Foul!'" on the competing claims of two towns to such renown: Stockton, California, and Holliston, Massachusetts.
On the possible model for Casey, Thayer dismissed the notion that any single living baseball player was an influence. However, late 1880s Boston star Mike "King" Kelly is odds-on the most likely model for Casey's baseball situations. Besides being a native of a town close to Boston, Thayer, as a "San Francisco Examiner" baseball reporter in the offseason of 1887–88, covered exhibition games featuring Kelly. In November 1887, some of his reportage about a Kelly at-bat has the same ring as Casey's famous at-bat in the poem. A 2004 book by Howard W. Rosenberg, "Cap Anson 2: The Theatrical and Kingly Mike Kelly: U.S. Team Sport's First Media Sensation and Baseball's Original Casey at the Bat," reprints a 1905 Thayer letter to a Baltimore scribe who was asking about the poem's roots. In the letter, Thayer singled out Kelly (d. 1894), as having shown "impudence" in claiming to have written it. Rosenberg argues that if Thayer still felt offended, Thayer may have steered later comments away from connecting Kelly to it. Kelly had also performed in vaudeville, and recited the poem dozens of times, possibly, to Thayer's dismay, butchering it. Incidentally, the first public performance of the poem was on August 14, 1888, by actor De Wolf Hopper, on Thayer's 25th birthday.
Thayer's recitation of it at a Harvard class reunion in 1895 may seem trivial except that it helps solve the mystery, which lingered into the 20th century, of who had written it. In the mid-1890s, Thayer contributed several other comic poems for Hearst's "New York Journal" and then turned to overseeing his family's mills in Worcester full-time.
Thayer moved to Santa Barbara in 1912, where he married Rosalind Buel Hammett and retired. He died in 1940, at age 77.
The New York Times' obituary of Thayer on August 22, 1940, p. 19 quotes comedian DeWolf Hopper, who helped make the poem famous: 

</doc>
<doc id="9714" url="http://en.wikipedia.org/wiki?curid=9714" title="List of English-language poets">
List of English-language poets

This is a list of English-language poets, who wrote or write much of their poetry in English. 
 "This literature-related list is ; you can help by [ expanding it]".

</doc>
<doc id="9717" url="http://en.wikipedia.org/wiki?curid=9717" title="Excalibur">
Excalibur

Excalibur or Caliburn is the legendary sword of King Arthur, sometimes attributed with magical powers or associated with the rightful sovereignty of Great Britain. Sometimes Excalibur and the Sword in the Stone (the proof of Arthur's lineage) are said to be the same weapon, but in most versions they are considered separate. The sword was associated with the Arthurian legend very early. In Welsh, the sword is called Caledfwlch; in Cornish, the sword is called Calesvol; in Breton, the sword is called Kaledvoulc'h; in Latin, the sword is called Caliburnus.
Forms and etymologies.
The name "Excalibur" ultimately comes from the ancestor of Welsh "Caledfwlch" (and Breton "Kaledvoulc'h", Middle Cornish "Calesvol") which is a compound of "caled" "hard" and "bwlch" "breach, cleft". Caledfwlch appears in several early Welsh works, including the poem "Preiddeu Annwfn" (though it is not directly named - but only alluded to - here) and the prose tale "Culhwch and Olwen", a work associated with the "Mabinogion" and written perhaps around 1100. The name was later used in Welsh adaptations of foreign material such as the "Brut"s, which were based on Geoffrey of Monmouth. It is often considered to be related to the phonetically similar "Caladbolg", a sword borne by several figures from Irish mythology, although a borrowing of "Caledfwlch" from Irish "Caladbolg" has been considered unlikely by Rachel Bromwich and D. Simon Evans. They suggest instead that both names "may have similarly arisen at a very early date as generic names for a sword"; this sword then became exclusively the property of Arthur in the British tradition. 
Geoffrey of Monmouth, in his Historia Regum Britanniae (c. 1136), Latinised the name of Arthur's sword as "Caliburnus" (potentially influenced by the Medieval Latin spelling "calibs" of Classical Latin "chalybs", from Greek "chályps" ["χάλυψ"] "steel") and states that it was forged in the Isle of Avalon. Most Celticists consider Geoffrey's "Caliburnus" to be derivative of a lost Old Welsh text in which "bwlch" had not yet been lenited to "fwlch". In Old French sources this then became "Escalibor", "Excalibor" and finally the familiar "Excalibur".
Geoffrey Gaimar, in his Old French "L'Estoire des Engles" (1134-1140), mentions Arthur and his sword: "this Constantine was the nephew of Arthur, who had the sword Caliburc" ("Cil Costentin li niès Artur, Ki out l'espée Caliburc").
In Wace's "Roman de Brut" (c. 1150-1155), an Old French translation and versification of Geoffrey of Monmouth's "Historia Regum Britanniae", the sword is called "Calabrum", "Callibourc", "Chalabrun", and "Calabrun" (with alternate spellings such as "Chalabrum", "Calibore", "Callibor", "Caliborne", "Calliborc", and "Escaliborc", found in various manuscripts of the "Brut").
In Chrétien de Troyes' late 12th century Old French "Perceval", Gawain carries the sword "Escalibor" and it is stated, "for at his belt hung Excalibor, the finest sword that there was, which sliced through iron as through wood" ("Qu'il avoit cainte Escalibor, la meillor espee qui fust, qu'ele trenche fer come fust"). This statement was probably picked up by the author of the "Estoire Merlin", or Vulgate Merlin, where the author (who was fond of fanciful folk etymologies) asserts that Escalibor "is a Hebrew name which means in French 'cuts iron, steel, and wood'" ("c'est non Ebrieu qui dist en franchois trenche fer & achier et fust"; note that the word for "steel" here, achier, also means "blade" or "sword" and comes from medieval Latin "aciarium", a derivative of "acies" "sharp", so there is no direct connection with Latin "chalybs" in this etymology). It is from this fanciful etymological musing that Thomas Malory got the notion that Excalibur meant "cut steel" ("'the name of it,' said the lady, 'is Excalibur, that is as moche to say, as Cut stele'").
Excalibur and the Sword in the Stone.
In Arthurian romance, a number of explanations are given for Arthur's possession of Excalibur. In Robert de Boron's "Merlin", Arthur obtained the throne by pulling a sword from a stone. (The story of the Sword in the Stone has an analogue in some versions of the story of Sigurd, the Norse proto-Siegfried, whose father, Sigmund, draws the sword Gram out of the tree Barnstokkr where it is embedded by the Norse god Odin.) In this account, the act could not be performed except by "the true king," meaning the divinely appointed king or true heir of Uther Pendragon. This sword is thought by many to be the famous Excalibur, and its identity is made explicit in the later Prose "Merlin", part of the Lancelot-Grail cycle. The challenge of drawing a sword from a stone also appears in the Arthurian legends of Galahad, whose achievement of the task indicates that he is destined to find the Holy Grail. 
However, in what is called the Post-Vulgate Cycle, Excalibur was given to Arthur by the Lady of the Lake sometime after he began to reign. She calls the sword "Excalibur, that is as to say as Cut-steel." In the Vulgate "Mort Artu", Arthur orders Griflet to throw the sword into the enchanted lake. After two failed attempts (as he felt such a great sword should not be thrown away), he finally complies with the wounded king's request and a hand emerges from the lake to catch it, a tale which becomes attached to Bedivere instead in Malory and the English tradition. Malory records both versions of the legend in his "Le Morte d'Arthur", naming both swords as Excalibur. 
History.
In Welsh legend, Arthur's sword is known as "Caledfwlch". In "Culhwch and Olwen", it is one of Arthur's most valuable possessions and is used by Arthur's warrior Llenlleawg the Irishman to kill the Irish king Diwrnach while stealing his magical cauldron. Irish mythology mentions a weapon "Caladbolg", the sword of Fergus mac Róich. Caladbolg was also known for its incredible power and was carried by some of Ireland's greatest heroes. The name, which can also mean "hard cleft" in Irish, appears in the plural, "caladbuilc", as a generic term for "great swords" in "Togail Troi" ("The Destruction of Troy"), the 10th century Irish translation of the classical tale.
Though not named as Caledfwlch, Arthur's sword is described vividly in "The Dream of Rhonabwy" one of the tales associated with the "Mabinogion":
Then they heard Cadwr Earl of Cornwall being summoned, and saw him rise with Arthur's sword in his hand, with a design of two chimeras on the golden hilt; when the sword was unsheathed what was seen from the mouths of the two chimeras was like two flames of fire, so dreadful that it was not easy for anyone to look. At that the host settled and the commotion subsided, and the earl returned to his tent.—From "The Mabinogion", translated by Jeffrey Gantz.
In the late 15th/early 16th-century Middle Cornish play Beunans Ke, Arthur's sword is called "Calesvol", which is etymologically an exact Middle Cornish cognate of the Welsh "Caledfwlch". It is unclear if the name was borrowed from the Welsh (if so, it must have been an early loan, for phonological reasons), or represents an early, pan-Brittonic traditional name for Arthur's sword.
Geoffrey's "Historia" is the first non-Welsh source to speak of the sword. Geoffrey says the sword was forged in Avalon and Latinises the name "Caledfwlch" as "Caliburnus". When his influential pseudo-history made it to Continental Europe, writers altered the name further until it finally took on the popular form "Excalibur" (various spellings in the medieval Arthurian Romance and Chronicle tradition include: Calabrun, Calabrum, Calibourne, Callibourc, Calliborc, Calibourch, Escaliborc, and Escalibor). The legend was expanded upon in the Vulgate Cycle, also known as the Lancelot-Grail Cycle, and in the Post-Vulgate Cycle which emerged in its wake. Both included the work known as the "Prose Merlin", but the Post-Vulgate authors left out the "Merlin" continuation from the earlier cycle, choosing to add an original account of Arthur's early days including a new origin for Excalibur.
In several early French works, such as Chrétien de Troyes' "Perceval, the Story of the Grail" and the Vulgate "Lancelot Proper" section, Excalibur is used by Gawain, Arthur's nephew and one of his best knights. This is in contrast to later versions, where Excalibur belongs solely to the king.
Attributes.
In many versions, Excalibur's blade was engraved with phrases on opposite sides: "Take me up" and "Cast me away" (or similar). In addition, when Excalibur was first drawn, in the first battle testing Arthur's sovereignty, its blade blinded his enemies. Thomas Malory writes: "thenne he drewe his swerd Excalibur, but it was so breyght in his enemyes eyen that it gaf light lyke thirty torchys."
Excalibur's scabbard was said to have powers of its own. Loss of blood from injuries, for example, would not kill the bearer. In some tellings, wounds received by one wearing the scabbard did not bleed at all. The scabbard is stolen by Morgan le Fay in revenge for the death of her beloved Accolon and thrown into a lake, never to be found again.
Nineteenth century poet Alfred, Lord Tennyson, described the sword in full Romantic detail in his poem "Morte d'Arthur", later rewritten as "The Passing of Arthur", one of the "Idylls of the King":
Arthur's other weapons.
Excalibur is by no means the only weapon associated with Arthur, nor the only sword. Welsh tradition also knew of a dagger named Carnwennan and a spear named Rhongomyniad that belonged to him. Carnwennan ("Little White-Hilt") first appears in "Culhwch and Olwen", where it was used by Arthur to slice the Black Witch in half. Rhongomyniad ("spear" + "striker, slayer") is also first mentioned in "Culhwch", although only in passing; it appears as simply "Ron" ("spear") in Geoffrey's "Historia". In the "Alliterative Morte Arthure", a Middle English poem, there is mention of Clarent, a sword of peace meant for knighting and ceremonies as opposed to battle, which was stolen and then used to kill Arthur by Mordred. The "Prose Lancelot" of the "Vulgate Cycle" mentions a sword called Seure, which belonged to the king but was used by Lancelot in one battle.
Similar weapons.
There are other similar weapons described in other mythologies. In particular, Claíomh Solais, which is an Irish term meaning "Sword of Light", or "Shining Sword", which appears in a number of orally transmitted Irish folk-tales.
References.
</dl>

</doc>
<doc id="9719" url="http://en.wikipedia.org/wiki?curid=9719" title="Eight-bar blues">
Eight-bar blues

In music, an eight-bar blues is a typical blues chord progression, "the second most common blues form," "common to folk, rock, and jazz forms of the blues," taking eight 4/4 or 12/8 bars to the verse.
Examples include "Sitting on Top of the World" and "Key to the Highway", "Trouble in Mind" and "Stagolee". "Heartbreak Hotel", "How Long Blues", "Ain't Nobody's Business", "Cherry Red", and "Get a Haircut" are all eight-bar blues standards.
One variant using this progression is to couple one eight-bar blues melody with a different eight-bar blues bridge to create a blues variant of the standard 32-bar song. "Walking By Myself", "I Want a Little Girl" and "(Romancing) In The Dark" are examples of this form. See also blues ballad.
Eight bar blues progressions have more variations than the more rigidly defined twelve bar format. The move to the IV chord usually happens at bar 3 (as opposed to 5 in twelve bar). However, "the I chord moving to the V chord right away, in the second measure, is a characteristic of the eight-bar blues."
In the following examples each box represents a 'bar' of music (the specific time signature is not relevant). The chord in the box is played for the full bar. If two chords are in the box they are each played for half a bar, etc. The chords are represented as scale degrees in Roman numeral analysis. Roman numerals are used so the musician may understand the progression of the chords regardless of the key it is played in.
"Worried Life Blues" (probably the most common eight bar blues progression):
"Heartbreak Hotel" (variation with the I on the first half):
J. B. Lenoir's "Slow Down" and "Key to the Highway" (variation with the V at bar 2):
"Get a Haircut" by George Thorogood (simple progression):
Jimmy Rogers' "Walkin' By Myself" (somewhat unorthodox example of the form):
Howlin Wolf's version of "Sitting on Top of the World" uses movement between major and dominant 7th and major and minor fourth:
The first four bar progression used by Wolf is also used in Nina Simone's 1965 version of Trouble in Mind, but with a more uptempo beat than Sitting on Top of the World:
The progression may be created by dropping the first four bars from the twelve-bar blues, as in the solo section of Bonnie Raitt's "Love Me Like a Man" and Buddy Guy's "Mary Had a Little Lamb":
(The same chord progression can also be called a sixteen-bar blues, if each symbol above is taken to be a half note in 2/2 or 4/4 time—blues has not traditionally been associated with notation, so its form becomes a bit slippery when written down.) For example "Nine Pound Hammer". Ray Charles's original instrumental "Sweet Sixteen Bars" is another example.

</doc>
<doc id="9720" url="http://en.wikipedia.org/wiki?curid=9720" title="Echidna (disambiguation)">
Echidna (disambiguation)

Echidna is the common name for a family of Australian mammals.
Other.
Echidna may also refer to:

</doc>
<doc id="9723" url="http://en.wikipedia.org/wiki?curid=9723" title="Edward Waring">
Edward Waring

Edward Waring FRS (c. 1736 – 15 August 1798) was an English mathematician who was born in Old Heath (near Shrewsbury), Shropshire, England and died in Pontesbury, Shropshire, England. He entered Magdalene College, Cambridge as a sizar and became Senior wrangler in 1757. He was elected a Fellow of Magdalene and in 1760 Lucasian Professor of Mathematics, holding the chair until his death. He made the assertion known as Waring's problem without proof in his writings "Meditationes Algebraicae". Waring was elected a Fellow of the Royal Society in 1763 and awarded the Copley Medal in 1784.
Early years.
Waring was the eldest son of John and Elizabeth Waring, a prosperous farming couple. He received his early education in Shrewsbury School under a Mr Hotchkin and was admitted as a sizar at Magdalene College, Cambridge, on 24 March 1753, being also Millington exhibitioner. His extraordinary talent for mathematics was recognised from his early years in Cambridge. In 1757 he graduated BA as senior wrangler and on 24 April 1758 was elected to a fellowship at Magdalene. He belonged to the Hyson Club, whose members included William Paley.
Career.
At the end of 1759 Waring published the first chapter of "Miscellanea Analytica". On 28 January the next year he was appointed Lucasian professor of mathematics, one of the highest positions in Cambridge. William Samuel Powell, then tutor in St John's College, Cambridge opposed Waring's election and instead supported the candidacy of William Ludlam. In the polemic with Powell, Waring was backed by John Wilson. In fact Waring was very young and did not hold the MA, necessary for qualifying for the Lucasian chair, but this was granted him in 1760 by royal mandate. In 1762 he published the full "Miscellanea Analytica", mainly devoted to the theory of numbers and algebraic equations. In 1763 he was elected to the Royal Society. He was awarded its Copley Medal in 1784 but withdrew from the society in 1795, after he had reached sixty, 'on account of [his] age'. Waring was also a member of the academies of sciences of Göttingen and Bologna. In 1767 he took an MD degree, but his activity in medicine was quite limited. He carried out dissections with Richard Watson, professor of chemistry and later bishop of Llandaff. From about 1770 he was physician at Addenbrooke's Hospital at Cambridge, and he also practised at St Ives, Huntingdonshire, where he lived for some years after 1767. His career as a physician was not very successful since he was seriously short-sighted and a very shy man.
Personal life.
Waring had a younger brother, Humphrey, who obtained a fellowship at Magdalene in 1775. In 1776 Waring married Mary Oswell, sister of a draper in Shrewsbury; they moved to Shrewsbury and then retired to Plealey, 8 miles out of the town, where Waring owned an estate of 215 acres in 1797
Work.
Waring wrote a number of papers in the "Philosophical Transactions of the Royal Society", dealing with the resolution of algebraic equations, number theory, series, approximation of roots, interpolation, the geometry of conic sections, and dynamics. The "Meditationes Algebraicae" (1770), where many of the results published in "Miscellanea Analytica" were reworked and expanded, was described by Joseph-Louis Lagrange as 'a work full of excellent researches'. In this work Waring published many theorems concerning the solution of algebraic equations which attracted the attention of continental mathematicians, but his best results are in number theory. Included in this work was the so-called Goldbach conjecture (every even integer is the sum of two primes), and also the following conjecture: every odd integer is a prime or the sum of three primes. Lagrange had proved that every positive integer is the sum of not more than four squares; Waring suggested that every positive integer is either a cube or the sum of not more than nine cubes. He also advanced the hypothesis that every positive integer is either a biquadrate or the sum of not more than nineteen biquadrates. These hypotheses form what is known as Waring's problem. He also published a theorem, due to his friend John Wilson, concerning prime numbers; it was later proved rigorously by Lagrange.
In "Proprietates Algebraicarum Curvarum" (1772) Waring reissued in a much revised form the first four chapters of the second part of "Miscellanea Analytica". He devoted himself to the classification of higher plane curves, improving results obtained by Isaac Newton, James Stirling, Leonhard Euler, and Gabriel Cramer. In 1794 he published a few copies of a philosophical work entitled "An Essay on the Principles of Human Knowledge", which were circulated among his friends.
Waring's mathematical style is highly analytical. In fact he criticised those British mathematicians who adhered too strictly to geometry. It is indicative that he was one of the subscribers of John Landen's "Residual Analysis" (1764), one of the works in which the tradition of the Newtonian fluxional calculus was more severely criticised. In the preface of "Meditationes Analyticae" Waring showed a good knowledge of continental mathematicians such as Alexis Clairaut, Jean le Rond d'Alembert, and Euler. He lamented the fact that in Great Britain mathematics was cultivated with less interest than on the continent, and clearly desired to be considered as highly as the great names in continental mathematics—there is no doubt that he was reading their work at a level never reached by any other eighteenth-century British mathematician. Most notably, at the end of chapter three of "Meditationes Analyticae" Waring presents some partial fluxional equations (partial differential equations in Leibnizian terminology); such equations are a mathematical instrument of great importance in the study of continuous bodies which was almost completely neglected in Britain before Waring's researches. One of the most interesting results in "Meditationes Analyticae" is a test for the convergence of series generally attributed to d'Alembert (the 'ratio test'). The theory of convergence of series (the object of which is to establish when the summation of an infinite number of terms can be said to have a finite 'sum') was not much advanced in the eighteenth century.
Waring's work was known both in Britain and on the continent, but it is difficult to evaluate his impact on the development of mathematics. His work on algebraic equations contained in "Miscellanea Analytica" was translated into Italian by Vincenzo Riccati in 1770. Waring's style is not systematic and his exposition is often obscure. It seems that he never lectured and did not habitually correspond with other mathematicians. After Jérôme Lalande in 1796 observed, in "Notice sur la vie de Condorcet", that in 1764 there was not a single first-rate analyst in England, Waring's reply, published after his death as 'Original letter of Dr Waring' in the "Monthly Magazine", stated that he had given 'somewhere between three and four hundred new propositions of one kind or another'.
Death.
During his last years he sank into a deep religious melancholy, and a violent cold caused his death, in Plealey, on 15 August 1798. He was buried in the churchyard at Fitz, Shropshire.

</doc>
<doc id="9724" url="http://en.wikipedia.org/wiki?curid=9724" title="Eden Phillpotts">
Eden Phillpotts

Eden Phillpotts (4 November 1862 – 29 December 1960) was an English author, poet and dramatist. He was born in Mount Abu, British India, educated in Plymouth, Devon, and worked as an insurance officer for 10 years before studying for the stage and eventually becoming a writer. 
He co-wrote two plays with his daughter Adelaide Phillpotts, "The Farmer's Wife" (1924) and "Yellow Sands" (1926); but is best known as the author of many novels, plays and poems about Dartmoor. His Dartmoor cycle of 18 novels and two volumes of short stories still has many avid readers despite the fact that many titles are out of print.
Life and character.
Phillpotts was for many years the President of the Dartmoor Preservation Association and cared passionately about the conservation of Dartmoor. He was also an agnostic and a supporter of the Rationalist Press Association.
Phillpotts was a friend of Agatha Christie, who was an admirer of his work and a regular visitor to his home. Jorge Luis Borges was another admirer. Borges mentioned him numerous times, wrote at least two reviews of his novels, and included him in his "Personal Library", a collection of works selected to reflect his personal literary preferences.
Phillpotts died in Broadclyst.
Writings.
Phillpotts wrote a great many books with a Dartmoor setting. One of his novels, "Widecombe Fair", inspired by an annual fair at the village of Widecombe-in-the-Moor, provided the scenario for his comic play "The Farmer's Wife". It went on to become a silent movie of the same name, directed by Alfred Hitchcock and filmed in 1927. The cast included Jameson Thomas, Lillian Hall-Davis, Gordon Harker and Gibb McLaughlin.
He also wrote a series of novels each set against the background of a different trade or industry. Titles include: "Brunel's Tower" (a pottery) and "Storm in a Teacup" (hand-papermaking).
Among his other works is "The Grey Room", the plot of which is centered on a haunted room in an English manor house. He also wrote a number of other mystery novels, both under his own name and the pseudonym Harrington Hext. Titles include: "The Thing at Their Heels", "The Red Redmaynes", "The Monster", "The Clue from the Stars", and "The Captain's Curio".
"The Human Boy" was a collection of schoolboy stories in the same genre as Rudyard Kipling's Stalky & Co., though different in mood and style.
Although mainly a novelist, he also wrote several plays.
Late in his long writing career he wrote a few books of interest to science fiction and fantasy readers, the most noteworthy being "Saurus", which involves an alien reptilian observing human life.
Quality of writing.
Eric Partridge praised the immediacy and impact of his dialect writing.

</doc>
<doc id="9725" url="http://en.wikipedia.org/wiki?curid=9725" title="Ecuador–United States relations">
Ecuador–United States relations

The Republic of Ecuador and the United States of America maintained close ties based on mutual interests in maintaining democratic institutions; combating cannabis and cocaine; building trade, investment, and financial ties; cooperating in fostering Ecuador's economic development; and participating in inter-American organizations. Ties are further strengthened by the presence of an estimated 150,000-200,000 Ecuadorians living in the United States and by 24,000 U.S. citizens visiting Ecuador annually, and by approximately 15,000 U.S. citizens residing in Ecuador. The United States assists Ecuador's economic development directly through the Agency for International Development (USAID) program in Ecuador and through multilateral organizations such as the Inter-American Development Bank and the World Bank. In addition, the U.S. Peace Corps operates a sizable program in Ecuador. More than 100 U.S. companies are doing business in Ecuador.
Relations between the two nations have been strained following Julian Assange's bid to seek political asylum in the Ecuadorian embassy in London following repeated claims that the US government was pursuing his extradition due to his work with Wikileaks.
 Ecuador offered political asylum to Julian Assange in November 2012.
History.
Both nations are signatories of the Inter-American Treaty of Reciprocal Assistance (the "Rio Treaty") of 1947, the Western Hemisphere's regional mutual security treaty. Ecuador shares U.S. concern over increasing narcotrafficking and international terrorism and has energetically condemned terrorist actions, whether directed against government officials or private citizens. The government has maintained Ecuador virtually free of coca production since the mid-1980s and is working to combat money laundering and the transshipment of drugs and chemicals essential to the processing of cocaine.
Ecuador and the U.S. agreed in 1999 to a 10-year arrangement whereby U.S. military surveillance aircraft could use the airbase at Manta, Ecuador, as a "Forward Operating Location" to detect drug trafficking flights through the region. The arrangement expired in 2009; current president Rafael Correa vowed not to renew it, and since then the Ecuador has not had any foreign military facilities in the country.
In fisheries issues, the United States claims jurisdiction for the management of coastal fisheries up to 200 mile (370 km) from its coast, but excludes highly migratory species; Ecuador, on the other hand, claims a 200-mile (370-km) territorial sea, and imposes license fees and fines on foreign fishing vessels in the area, making no exceptions for catches of migratory species. In the early 1970s, Ecuador seized about 100 foreign-flag vessels (many of them U.S.) and collected fees and fines of more than $6 million. After a drop-off in such seizures for some years, several U.S. tuna boats were again detained and seized in 1980 and 1981.
The U.S. Magnuson Fishery Conservation and Management Act then triggered an automatic prohibition of U.S. imports of tuna products from Ecuador. The prohibition was lifted in 1983, and although fundamental differences between U.S. and Ecuadorian legislation still exist, there is no current conflict. During the period that has elapsed since seizures which triggered the tuna import ban, successive Ecuadorian governments have declared their willingness to explore possible solutions to this problem with mutual respect for longstanding positions and principles of both sides. The election of Rafael Correa in October 2006, has strained relations between the two countries and relations have since been fraught with tension. Rafael Correa is quite critical of U.S. foreign polcy. 
In April 2011, relations between Ecuador and the United States soured particularly after Ecuador expelled the U.S. ambassador after a leaked diplomatic cable was shown accusing president Correa of knowingly ignoring police corruption. In reciprocation, the Ecuadorian ambassador Luis Gallegos was expelled from the United States.
In 2013, when Ecuador unilaterally pulled out of a preferential trade pact with the United States over claiming the U.S. used it as blackmail in regards to the asylum request of Edward Snowden, relations between Ecuador and the United States reached an all time low. The pact offered Ecuador US$23 million, which it offered to the U.S. for human rights training.<Ref>http://www.aljazeera.com/news/americas/2013/06/201362845849856254.html</ref> Tariff free imports had been offered to Ecuador in exchange for drug elimination efforts.
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="9727" url="http://en.wikipedia.org/wiki?curid=9727" title="Eight-ball">
Eight-ball

Eight-ball (often spelled 8-ball or eightball, and sometimes called solids and stripes, spots and stripes in the UK or, more rarely, bigs and littles or highs and lows) is a pool (pocket billiards) game popular in much of the world, and the subject of international professional and amateur competition. Played on a pool table with six pockets, the game is so universally known in some countries that beginners are often unaware of other pool games and believe the word "pool" itself refers to eight-ball. The game has numerous variations, including Alabama eight-ball, crazy eight, last pocket, misery, Missouri, 1 and 15 in the sides, rotation eight ball, soft eight, and others. Standard eight-ball is the second most competitive professional pool game, after nine-ball and for the last several decades ahead of straight pool.
Eight-ball is played with cue sticks and 16 balls: a <dfn id="">cue ball</dfn>, and 15 <dfn id=">object balls</dfn> consisting of seven striped balls, seven solid-colored balls and the black 8 ball. After the balls are scattered with a <dfn id=">break shot</dfn>, the players are assigned either the group of solid balls or the stripes once a ball from a particular group is legally pocketed. The ultimate object of the game is to legally pocket the eight ball in a called pocket, which can only be done after all of the balls from a player's assigned group have been cleared from the table. 
History.
The game of eight-ball is derived from an earlier game invented around 1900 (first recorded in 1908) in the United States and initially popularized under the name "B.B.C. Co. Pool" (a name that was still in use as late as 1925) by the Brunswick-Balke-Collender Company. This forerunner game was played with seven <dfn id="">yellow</dfn> and seven <dfn id=">red balls</dfn>, a <dfn id=">black ball</dfn>, and the cue ball. Today, numbered <dfn id=">stripes</dfn> and <dfn id=">solids</dfn> are preferred in most of the world, though the British-style offshoot, blackball, uses the traditional colors (as did early televised "casino" tournaments in the U.S.). The game had relatively simple rules compared to today and was not added (under any name) to an official rule book (i.e., one published by a national or international sport governing body) until 1940.:24, 89–90
Standardized "Rules of Play".
American-style eight-ball rules are played around the world by professionals, and in many amateur leagues. Nevertheless, the rules for eight-ball may be the most contested of any billiard game. There are several competing sets of "official" rules. The non-profit World Pool-Billiard Association (WPA) – with national affiliates around the world, some of which long pre-date the WPA, such as the Billiard Congress of America (BCA) – promulgates standardized rules as "Pool Billiards – The Rules of Play" for amateur and professional play. Meanwhile, many amateur leagues, such as the American Poolplayers Association (APA) / Canadian Poolplayers Association (CPA), and the Valley National Eight-ball Association (VNEA, international in scope despite its historic name), use their own rulesets (most of them at least loosely based on the WPA/BCA version), while millions of individuals play informally using colloquial rules which vary not only from area to area but even from venue to venue.
A summary of the international rules follows "(see the WPA/BCA or other leagues' published rules, which conflict on minor points, for more details)".
Equipment.
The table's playing surface is approximately 9 by (regulation size), though some leagues and tournaments using the World Standardized Rules may allow smaller sizes, down to 7 by, and early-20th-century 10 by models are sometimes also used.
There are seven <dfn id="">solid-colored balls</dfn> numbered 1 through 7, seven <dfn id=">striped balls</dfn> numbered 9 through 15, an <dfn id=">8 ball</dfn>, and a <dfn id=">cue ball</dfn>. The balls are usually colored as follows:
Setup.
To start the game, the <dfn id=">object balls</dfn> are placed in a triangular rack. The base of the rack is parallel to the <dfn id=">end rail</dfn> (the short end of the pool table) and positioned so the apex ball of the rack is located on the <dfn id=">foot spot</dfn>. The balls in the rack are ideally placed so that they are all in contact with one another; this is accomplished by pressing the balls together from the back of the rack toward the apex ball. The order of the balls should be random, with the exceptions of the 8 ball, which must be placed in the center of the rack (i.e., the middle of the third row), and the two back corner balls one of which must be a stripe and the other a solid. The cue ball is placed anywhere the breaker desires inside the <dfn id=">kitchen</dfn>. 
Break.
One person is chosen (by a predetermined method, e.g., coin flip, win or loss of previous game, or <dfn id=">lag</dfn>) to shoot first, using the cue ball to <dfn id=">break</dfn> the object-ball rack apart. If the shooter who breaks fails to make a legal break (usually defined as at least four balls hitting cushions or an object ball being pocketed), then the opponent can call for a <dfn id=">re-rack</dfn> and become the breaker, or elect to play from the current position of the balls.
According to World Standardized Rules, if the 8 ball is pocketed on the break without <dfn id=">fouling</dfn>, the breaker may ask for a re-rack and break again, or have the 8 ball <dfn id=">spotted</dfn> and continue shooting with the balls as they lie. If the breaker scratches (pockets the cue ball) while pocketing the 8 ball on the break, the incoming player may call for a re-rack and break, or have the 8 ball spotted and begin shooting with <dfn id=">ball-in-hand</dfn> behind the <dfn id=">head string</dfn>, with the balls as they lie. "(For regional amateur variations, such as pocketing the 8 on the break being an instant win or loss, see "Informal rule variations", below.)"
Turns.
A player (or team) will continue to shoot until committing a <dfn id="">foul</dfn>, or failing to legally pocket an object ball on a non-foul shot (whether <dfn id=">intentionally</dfn> or not). Thereupon it is the turn of the opposing player(s). Play alternates in this manner for the remainder of the game. Following a foul, the incoming player has <dfn id=">ball-in-hand</dfn> anywhere on the table, unless the foul occurred on the break shot, as noted previously.
Selection of the target group.
At some point in the game, one of the players can select (or is assigned) balls 1–7 (the "solids") or balls 9-15 (the "stripes") as their group of object balls. The other player is assigned to the other group. Once the target groups have been assigned, they remain fixed throughout the remainder of the game.
Pocketing the 8 ball.
Once all of a player's or team's group of object balls are pocketed, they may attempt to sink the 8 ball. To win, the player (or team) must first designate which pocket they plan to sink the 8 ball into and then successfully pot the 8 ball in that called pocket. If the 8 ball falls into any pocket other than the one designated or is knocked off the table, or a foul (see below) occurs and the 8 ball is pocketed, this results in loss of game. Otherwise, the shooter's turn is simply over, including when a foul such as a scratch occurs on an "unsuccessful" attempt to pocket the 8 ball. In short, a World Standardized Rules game of eight-ball, like a game of nine-ball, is "not" over until the "<dfn id="">money ball</dfn>" is no longer on the table. This rule is unusual to some bar and league players, because in American, Canadian and many other varieties of <dfn id="">bar pool</dfn>, and in some leagues, such as APA, such a foul is a loss of game. This is not the case in World Standardized Rules, nor in some other leagues that use those rules or a variant of them, e.g. VNEA beginning with the 2008/2009 season, and BCAPL), and USAPL.
Winning.
Any of the following results in a game win:
Informal rule variations.
Canada.
In Canada there are a similar level and types of variation as in the US "(see below)". One particularly common feature of Canadian bar pool is the "hooked yourself on the 8" rule — failure to hit the 8 ball when one is shooting for the 8 is a loss of game, "unless" one was hooked (<dfn id="">snookered</dfn>) by one's opponent (even then, if <dfn id=">a pocket is called</dfn> for the 8, as opposed to "just a shot", i.e. a <dfn id="">safety</dfn>, failure to hit the 8 is an instant loss). Pocketing an opponent's <dfn id=">object ball</dfn> while shooting for the 8, even if the shot was otherwise legal, is also a game-loser, often even in local league play. <dfn id=">"Split" shots</dfn>, where the <dfn id="">cue ball</dfn> appears to simultaneously strike a legal and an opponent's object balls, are generally considered legal shots in informal games, as long as they are called as split shots, and the hit is in fact simultaneous to the human eye. A further Canadian bar-pool rule is that a shot is a <dfn id=">visit</dfn>-ending (but not <dfn id=">ball-in-hand</dfn>) foul if one pockets one's called shot but also pockets another ball incidentally, even if it is one's own (however, if that secondary pocketing was also called, the shot is legal, regardless of the order in which the balls were dropped).
Latin America.
The <dfn id=">racked</dfn> balls are often loose, crooked and/or not exactly on the <dfn id=">foot spot</dfn> (it is not considered to matter), and the rack itself may be made of rubber, and flexible, making a tight rack physically impossible to achieve. Other than the 8 ball, other balls may be placed far more randomly than players in other areas would tolerate, with large clusters of solids together, and stripes with each other.
In most of Latin America, including Mexico, shots are un-<dfn id=">called</dfn>, as in British pool (i.e. <dfn id=">slop shots</dfn> count, a concept foreign to most American players other than APA league members). In many if not most areas (Brazil being an exception), fouls result in <dfn id=">ball-in-hand</dfn> behind the <dfn id=">head string</dfn> only, as in American bar pool (allowing for intentional scratches that leave the opponent a very difficult shot if all opponent balls are "in the <dfn id="">kitchen</dfn>", behind the headstring).
A common Latin American variant of "<dfn id="">last-pocket</dfn>" is that each player is allowed either one (or even two) cue ball scratches when shooting for the 8, which "must" be pocketed in the same pocket as the shooter's final object ball. Such fouls simply end the shooter's turn at the table and give the opponent ball-in-hand behind the head string; only the second (or third, respectively) such scratch is a loss of game (though scratching the 8 ball itself off the table or into the wrong pocket is an instant loss). This version is common even in US pool bars that are dominated by recent Latino immigrants. This requirement has a profound effect upon game strategy – it is effectively 5 times harder to <dfn id="">run out</dfn> – and most North American (and British, etc.) players are completely unprepared for it, unless they are last-pocket players. Players must be very mindful what they do with their last few balls, and common failure to get <dfn id=">shape</dfn> that allows for the last object-ball shot to set the player up for an easy 8 ball shot into the same pocket leads to long games with many <dfn id=">bank</dfn>, <dfn id=">kick</dfn> and <dfn id=">kiss</dfn> shots on the 8.
In some parts of Latin America, especially South America, the 1 ball often must be pocketed in the right side pocket (relative to the end of the table one breaks from), and the 15 ball must be pocketed in the other side pocket (left). This rule probably developed to make it harder to run out after the first shot. Position play takes a larger role in this variation, and a player's strategy must necessarily initially revolve around getting the 1 or 15 in and preventing this opponent from doing likewise. When racking the balls for this variation, the 1 and 15 balls are placed behind the 8 ball at the center of the rack, the 1 ball on the left and the 15 ball on the right (from the "racker's" perspective). Latino last-pocket is virtually the only version of eight-ball played in Mexico, other than in the Mexico–United States border area.
In Mexico, a minority of players rack with the 8 ball rather than the apex ball on the foot spot, a trait in common with British blackball/8-ball pool. Pocketing the 8 ball on the <dfn id="">break shot</dfn> is an instant win, as it usually is in American bar pool, but is not in the international rules. The only ball-in-hand (behind the head string) foul in Mexican pool is <dfn id=">scratching</dfn> the cue ball into a pocket; other fouls are simply loss-of-turn. Because Mexican pool, except near the US border, is almost always played on open-pocket pool-hall-style tables, rather than coin-operated tables that trap object balls, any of one's own balls pocketed on a foul are <dfn id=">spotted</dfn> (but how they are spotted varies widely, with the balls often placed against the <dfn id=">foot cushion</dfn> on the <dfn id=">center string</dfn>, and adjacent to nearby <dfn id=">diamonds</dfn> if more than one must be spotted, instead of on the foot spot, but sometimes even to the "side" at <dfn id="">long rail</dfn> diamonds, due to the influence of coyote, a Mexican variant of Chicago; foot-spot spotting is neither common nor uncommon.) Pool itself is not considered a very serious game in the country other than in the northern states; in most of Mexico, three-cushion billiards is the serious game, while pool is mostly played by youths, by groups of friends (including many young women) as a bar game to pass the time, and by older working-class men as an after-work activity. In many recreation halls, dominoes is more popular than pool.
In many bars in Brazil (and not an official rule), a foul is generally punished by pocketing the lowest-numbered ball of the opponent. In that case, the cue ball remains where it stopped, as ball-in-hand is not commonly used. Additionally, in the case of scratching the cue ball, the opponent places the cue ball in <dfn id=">the kitchen</dfn>, on the <dfn id=">head spot</dfn>, or most commonly anywhere inside <dfn id=">the "D"</dfn>, indicating some British snooker and/or blackball influence.
New Zealand.
New Zealand eight-ball in many respects is closer to British blackball, but with numbered balls being used. <dfn id="">A "D"</dfn> is typically drawn on the table above the <dfn id="">baulk line</dfn> (as on a snooker table) and the shooting player is required to place the cue ball within it on the <dfn id=">break-off</dfn> and after an opponent <dfn id=">scratches</dfn>. The shooting player can shoot the ball in any direction from within the "D". If no "D" is drawn on the table then the "forward play" rule is followed: After a scratch, the player with ball-in-hand must shoot forward of the baulk line, i.e. towards the rack area, even if all legal balls are behind the baulk line. The "<dfn id="">two-shot rule</dfn>" of blackball may or may not be followed; this depends on individual players and/or pubs.
The "nomination" rule is unique to New Zealand: A player <dfn id="">snookered</dfn> on the 8 ball may nominate one of the opponent's balls (if any remain) to hit as an alternative, legal "<dfn id="">ball on</dfn>". However, the shooter is not permitted to pot (pocket) such a nominated ball – doing so results in a loss of game.
North Africa.
In North African countries (as in Latin America, but reversed), both the 1 and 15 balls must be pocketed in the sides, the 15 on the right and 1 on the left (relative to the end of the table one breaks from). The North African version of the informal game is always played "<dfn id="">last-pocket</dfn>". <dfn id="">Ball-in-hand</dfn> is not taken on fouls, and "<dfn id="">bank-the-8</dfn>" is a very common rule in addition to last-pocket.
United Kingdom & Ireland.
Pool is popularly played in two forms. Traditionally it is played with smaller balls than the internationally standardized version, on a 4.5 by 7 foot pub-sized table, with differently shaped, smaller pockets. The <dfn id="">cue ball</dfn> is also slightly smaller than the <dfn id=">object balls</dfn>. "American-style" pool tables are also common in the UK, especially for nine-ball competition; the tables themselves are often referred to as "nine-ball tables", with that game being played only rarely on the more common, smaller traditional British-style tables. The two most common competitive rule sets used on the traditional tables are WEPF world eightball pool rules (replacing old EPA rules) and WPA world-standardized blackball rules. Most amateurs play "pub rules", meaning the local rule variation established at that venue.
The two main rule sets have features about them which most amateurs find offensive. WEPF rules permit intentional fouls; despite opponents being awarded two visits for a standard foul causing an intentional foul, or not trying to play a legal shot, it is seen as unfair play and distasteful. In WPA rules, two shots following a foul do not carry, meaning the first shot is a "free shot" rather than two visits, a player takes their free shot and then play returns to normal. As most pub rules are based around old EPA rules, in which two visits are awarded (rather than a free shot) amateurs are often unhappy with this difference in blackball, although it is in no way as offensive as intentional fouls which are illegal in blackball and result in loss of frame. Both WEPF rules and WPA require a player to either pot on their visit, or drive any ball, including the white, into a cushion after hitting a legal object ball, or else they give a foul. Although this rule, and the precise specifics of it are somewhat a mouthful, amateur players usually find the rule acceptable and see it primarily as a way to prevent "tucking up", whereby a player does not attempt to pot and instead just rolls up to their object ball to use it to snooker their opponent; tucking up is seen as unsporting, so being forced to play harder shots is quite welcomed.
There are several sets of rules which use a combination of many others in an attempt to find a balance between WPA rules, which are seen as more aggressive, and WEPF rules which are often referred to, detrimentally, as "chess".
Pakistan.
During game play, if the player fails to hit a ball of his designated group or he hits the opponent's ball with the cue ball, then the opponent receives 2 shots unless the opponent has pocketed all his balls and only the 8 ball remains, then the opponent will only get one shot. In case of such a foul, the game continues with the player playing the cue ball at the place where it stopped. If a Scratch occurs, then the opponent plays Ball-in-Hand but he is only allowed to place it anywhere in the D however he can play the cue ball in any direction. Knocking a ball (apart from the cue ball) off the table carries no penalty. Instead the misplaced ball is returned to its original place and the game continues.
India.
If a Foul or a Scratch is occurred while playing the 8 ball, as long as opponent has at least 1 ball of his group present on the Table and the 8 ball is not pocketed the game continues and the opponent gets the chance, If the cue ball is scratched, the opponent player gets 2 chances, but the ball has to be placed behind the break line, but if a foul occurs, the cue ball continues to stay there and the opponent gets 2 chances before the opponent can play(irrespective of if the player pots any of his balls in the first chance). the opponent also gets 2 chances if a player scratches. But if a player has only 8 ball left and the opponent sink his last solid/stripe ball with cue ball and if there is no ball in hand rule the player loses the game. And if the 8 ball is the only ball on the table and if the player commits any kind of foul the game is over and the opponent wins.
United States.
Most commonly of all in American <dfn id="">bar pool</dfn>, it is sometimes required that all shots be <dfn id=">called</dfn> "in detail", as to what balls and bank/kick cushions will be involved in the shot, with the shot considered a turn-ending (but not ball-in-hand) foul if not executed precisely as planned (and a loss of game if the "foul" shot pocketed the 8 ball). Contrariwise, some Americans hold that nothing other than the 8 ball has to be called in any way — "<dfn id="">slop</dfn>" counts.
In informal amateur play in most areas, the table will only be considered open if no balls were pocketed, or an equal number of stripes and solids were pocketed, or the cue ball was <dfn id="">scratched</dfn> (into a pocket or off the table), on the <dfn id=">break</dfn>; if an odd number of balls were legally pocketed, such as one solid and two stripes, or no solids and one stripe, the breaker must shoot the balls that were pocketed in the greatest quantity (stripes in these examples). The table is almost never considered so <dfn id=">open</dfn> as for it to be legal to use a ball of the opposite <dfn id=">suit</dfn>, much less the 8 ball, as the first ball in a combination shot while the table is open (despite this being perfectly legal in WPA World Standardized and many US league rules). In non-<dfn id=">money games</dfn> it is fairly common for a foul break in which the rack was not struck at all (e.g., due to a <dfn id=">miscue</dfn>) to be re-shot by the original breaker.
Fouls, in common bar pool, that are not cue ball scratches generally only cause loss of turn, with cue ball left in place (even if it is <dfn id=">hooked</dfn>). Even in the case of a scratch, this only results in <dfn id=">ball-in-hand</dfn> "behind the <dfn id="">head string</dfn>". Regionally, there is a great deal of bar pool variation in the handling of fouls while shooting at and/or pocketing the 8 ball. In some cases any foul while shooting at but not pocketing the 8 is a loss of game, in others only a foul while otherwise successfully pocketing the 8, and in yet others only certain fouls, such as also sinking an opponent's ball or touching the 8 ball and scratching.
What is considered a foul further diverges from established, published rulesets. Scoop-under <dfn id="">jump shots</dfn> are usually considered valid (these are fouls in WPA and most league rules, as they are <dfn id=">double-hits</dfn>, though few players realize it). When a cue ball is frozen or near-frozen to an object ball, shooting it dead-on, in line with both balls, is a foul in formal rulesets (as another kind of double-hit), but is generally tolerated in bar pool.
Other US bar pool oddities varying from area to area include: Knocking the cue ball off the table on the break may be an instant loss; scratching on the break may be an instant loss; pocketing the 8 ball on the break (without scratching) may be either an instant win or instant loss (the latter being a rare variant); no safeties may be allowed at all – all shots may be required to be at least vaguely plausible attempts to pocket a legal ball; all jump shots may be banned; <dfn id=">massé shots</dfn> may be banned; it may be illegal to use the 8 ball in any way in combinations, caroms or kisses; the 8 ball may be required to be pocketed "cleanly" in the sense of no contact with other object balls (even if the <dfn id="">kiss shot</dfn> can be accurately called); failure to hit one of one's own object balls (or the 8 if shooting for the 8) may be considered a "table scratch" that gives the opponent a shot in-hand from behind the head string; failure to hit the 8 if shooting for the 8 may be a loss of game; and a "split" shot, where the cue ball appears to simultaneously strike a legal ball and an opponent's object ball, may be considered a legal shot, as long as it is called as a split shot, and the hit is in fact simultaneous to the naked eye.
"<dfn id="">Bank-the-8</dfn>" is a common American amateur variation, especially on coin-operated <dfn id="">bar tables</dfn> (because it usually makes the game last longer), in which the 8 ball must be <dfn id=">banked</dfn> off one or more <dfn id=">cushions</dfn> (<dfn id=">kick shots</dfn> may also qualify in some versions), into the <dfn id=">called pocket</dfn>; either player may suggest bank-the-eight at any time before or during the game, and the other may accept or refuse; all other rules apply as usual. Playing bank-the-eight may be considered rude if there is a long line of players waiting to use the table.
A similarly-motivated variant is "<dfn id="">last-pocket</dfn>", in which the 8 ball must be pocketed in the same pocket as the shooting player's last object ball (i.e., each player may be said to eventually "own" a pocket in which their 8 ball shot must be played if they have already run out their <dfn id="">suit</dfn>); all other rules apply as usual. This variant is popular in Mexico.
Due probably to the influence of nine-ball, in which the 1 ball "must" be the apex ball of the rack, most American bar players traditionally rack a game of eight-ball with the 1 ball in this position. Racking is also typically done solid-stripe-solid-stripe-solid along the two sides of the rack, resulting in solids being on all three corners. This is not a legal rack in World Standardized Rules, nor any other notable league ruleset other than APA.
Derivative games and variants.
British-style variant.
<span name="UK" />In the United Kingdom, eight-ball pool (and its internationally standardized variant blackball) as an overall rather different version of the game has evolved, influenced by English billiards and snooker, and has become popular in amateur competition in Britain, Ireland, Australia, and some other countries. As with American eight-ball, there are multiple competing standards bodies that have issued international rules. Aside from using unnumbered object balls (except for the 8), UK-style tables have pockets just larger than the balls, and more than one type of <dfn id="">rest</dfn> is typically used. The rules significantly differ in numerous ways, including the handling of fouls, which may give the opponent two shots, racking (the 8 ball, not the apex ball, goes on the foot spot), selection of which group of balls will be shot by which player, handling of <dfn id=">frozen</dfn> balls and <dfn id=">snookers</dfn>, and many other details.
The English Pool Association is recognized by the Sports Council as the governing body for pool including blackball in England.
Eight-ball rotation.
The hybrid game eight-ball rotation is a combination of eight-ball and rotation, in which the players must pocket their balls (other than the 8, which remains last) in numerical order.

</doc>
<doc id="9728" url="http://en.wikipedia.org/wiki?curid=9728" title="Earned value management">
Earned value management

Earned value management (EVM), or Earned value project/performance management (EVPM) is a project management technique for measuring project performance and progress in an objective manner.
Overview.
Earned value management is a project management technique for measuring project performance and progress. It has the ability to combine measurements of the project management triangle:
In a single integrated system, Earned Value Management is able to provide accurate forecasts of project performance problems, which is an important contribution for project management.
Early EVM research showed that the areas of planning and control are significantly impacted by its use; and similarly, using the methodology improves both scope definition as well as the analysis of overall project performance. More recent research studies have shown that the principles of EVM are positive predictors of project success. Popularity of EVM has grown in recent years beyond government contracting, in which sector its importance continues to rise (e.g., recent new DFARS rules), in part because EVM can also surface in and help substantiate contract disputes.
Essential features of any EVM implementation include
EVM implementations for large or complex projects include many more features, such as indicators and forecasts of cost performance (over budget or under budget) and schedule performance (behind schedule or ahead of schedule). However, the most basic requirement of an EVM system is that it quantifies progress using PV and EV.
Application example.
Project A has been approved for a duration of 1 year and with the budget of X. It was also planned, that the project spends 50% of the approved budget in the first 6 months. If now 6 months after the start of the project a Project Manager would report that he has spent 50% of the budget, one can initially think, that the project is perfectly on plan. However in reality the provided information is not sufficient to come to such a conclusion. The project can spend 50% of the budget, whilst finishing only 25% of the work, which would mean the project is not doing well; or the project can spend 50% of the budget, whilst completing 75% of the work, which would mean that project is doing better than planned. EVM is meant to address such and similar issues.
History.
EVM emerged as a financial analysis specialty in United States Government programs in the 1960s, but it has since become a significant branch of project management and cost engineering. Project management research investigating the contribution of EVM to project success suggests a moderately strong positive relationship.
Implementations of EVM can be scaled to fit projects of all sizes and complexities.
The genesis of EVM occurred in industrial manufacturing at the turn of the 20th century, based largely on the principle of "earned time" popularized by Frank and Lillian Gilbreth, but the concept took root in the United States Department of Defense in the 1960s. The original concept was called PERT/COST, but it was considered overly burdensome (not very adaptable) by contractors who were mandated to use it, and many variations of it began to proliferate among various procurement programs. In 1967, the DoD established a criterion-based approach, using a set of 35 criteria, called the Cost/Schedule Control Systems Criteria (C/SCSC). In the 1970s and early 1980s, a subculture of C/SCSC analysis grew, but the technique was often ignored or even actively resisted by project managers in both government and industry. C/SCSC was often considered a financial control tool that could be delegated to analytical specialists.
In 1979, EVM was introduced to the architecture and engineering industry in a "Public Works Magazine" article by David Burstein, a project manager with a national engineering firm. This technique has been taught ever since as part of the project management training program presented by PSMJ Resources, an international training and consulting firm that specializes in the engineering and architecture industry.
In the late 1980s and early 1990s, EVM emerged as a project management methodology to be understood and used by managers and executives, not just EVM specialists. In 1989, EVM leadership was elevated to the Undersecretary of Defense for Acquisition, thus making EVM an element of program management and procurement. In 1991, Secretary of Defense Dick Cheney canceled the Navy A-12 Avenger II Program because of performance problems detected by EVM. This demonstrated conclusively that EVM mattered to secretary-level leadership. In the 1990s, many U.S. Government regulations were eliminated or streamlined. However, EVM not only survived the acquisition reform movement, but became strongly associated with the acquisition reform movement itself. Most notably, from 1995 to 1998, ownership of EVM criteria (reduced to 32) was transferred to industry by adoption of ANSI EIA 748-A standard.
The use of EVM expanded beyond the U.S. Department of Defense. It was adopted by the National Aeronautics and Space Administration, United States Department of Energy and other technology-related agencies. Many industrialized nations also began to utilize EVM in their own procurement programs.
An overview of EVM was included in the Project Management Institute's first PMBOK Guide in 1987 and was expanded in subsequent editions. In the most recent edition of the PMBOK guide, EVM is listed among the general tools and techniques for processes to control project costs.
The construction industry was an early commercial adopter of EVM. Closer integration of EVM with the practice of project management accelerated in the 1990s. In 1999, the Performance Management Association merged with the Project Management Institute (PMI) to become PMI’s first college, the College of Performance Management. The United States Office of Management and Budget began to mandate the use of EVM across all government agencies, and, for the first time, for certain internally managed projects (not just for contractors). EVM also received greater attention by publicly traded companies in response to the Sarbanes-Oxley Act of 2002.
In Australia EVM has been codified as standards AS 4817-2003 and AS 4817-2006.
Earned value management topics.
Project tracking.
 It is helpful to see an example of project tracking that does not include earned value performance management. Consider a project that has been planned in detail, including a time-phased spend plan for all elements of work. Figure 1 shows the cumulative budget (cost) for this project as a function of time (the blue line, labeled PV). It also shows the cumulative actual cost of the project (red line) through week 8. To those unfamiliar with EVM, it might appear that this project was over budget through week 4 and then under budget from week 6 through week 8. However, what is missing from this chart is any understanding of how much work has been accomplished during the project. If the project was actually completed at week 8, then the project would actually be well under budget and well ahead of schedule. If, on the other hand, the project is only 10% complete at week 8, the project is significantly over budget and behind schedule. A method is needed to measure technical performance objectively and quantitatively, and that is what EVM accomplishes.
Project tracking with EVM.
Consider the same project, except this time the project plan includes pre-defined methods of quantifying the accomplishment of work. At the end of each week, the project manager identifies every detailed element of work that has been completed, and sums the EV for each of these completed elements. Earned value may be accumulated monthly, weekly, or as progress is made.
Earned value (EV).
formula_1
Figure 2 shows the EV curve (in green) along with the PV curve from Figure 1. The chart indicates that technical performance (i.e., progress) started more rapidly than planned, but slowed significantly and fell behind schedule at week 7 and 8. This chart illustrates the schedule performance aspect of EVM. It is complementary to critical path or critical chain schedule management.
Figure 3 shows the same EV curve (green) with the actual cost data from Figure 1 (in red). It can be seen that the project was actually under budget, relative to the amount of work accomplished, since the start of the project. This is a much better conclusion than might be derived from Figure 1.
Figure 4 shows all three curves together – which is a typical EVM line chart. The best way to read these three-line charts is to identify the EV curve first, then compare it to PV (for schedule performance) and AC (for cost performance). It can be seen from this illustration that a true understanding of cost performance and schedule performance "relies first on measuring technical performance objectively." This is the "foundational principle" of EVM.
Scaling EVM from simple to advanced implementations.
The "foundational principle" of EVM, mentioned above, does not depend on the size or complexity of the project. However, the "implementations" of EVM can vary significantly depending on the circumstances. In many cases, organizations establish an all-or-nothing threshold; projects above the threshold require a full-featured (complex) EVM system and projects below the threshold are exempted. Another approach that is gaining favor is to scale EVM implementation according to the project at hand and skill level of the project team.
Simple implementations (emphasizing only technical performance).
There are many more small and simple projects than there are large and complex ones, yet historically only the largest and most complex have enjoyed the benefits of EVM. Still, lightweight implementations of EVM are achievable by any person who has basic spreadsheet skills. In fact, spreadsheet implementations are an excellent way to learn basic EVM skills.
The "first step" is to define the work. This is typically done in a hierarchical arrangement called a work breakdown structure (WBS) although the simplest projects may use a simple list of tasks. In either case, it is important that the WBS or list be comprehensive. It is also important that the elements be mutually exclusive, so that work is easily categorized in one and only one element of work. The most detailed elements of a WBS hierarchy (or the items in a list) are called activities (or tasks).
The "second step" is to assign a value, called planned value (PV), to each activity. For large projects, PV is almost always an allocation of the total project budget, and may be in units of currency (e.g., dollars or euros) or in labor hours, or both. However, in very simple projects, each activity may be assigned a weighted “point value" which might not be a budget number. Assigning weighted values and achieving consensus on all PV quantities yields an important benefit of EVM, because it exposes misunderstandings and miscommunications about the scope of the project, and resolving these differences should always occur as early as possible. Some terminal elements can not be known (planned) in great detail in advance, and that is expected, because they can be further refined at a later time.
The "third step" is to define “earning rules” for each activity. The simplest method is to apply just one earning rule, such as the 0/100 rule, to all activities. Using the 0/100 rule, no credit is earned for an element of work until it is finished. A related rule is called the 50/50 rule, which means 50% credit is earned when an element of work is started, and the remaining 50% is earned upon completion. Other fixed earning rules such as a 25/75 rule or 20/80 rule are gaining favor, because they assign more weight to finishing work than for starting it, but they also motivate the project team to identify when an element of work is started, which can improve awareness of work-in-progress. These simple earning rules work well for small or simple projects because generally each activity tends to be fairly short in duration.
These initial three steps define the minimal amount of planning for simplified EVM. The "final step" is to execute the project according to the plan and measure progress. When activities are started or finished, EV is accumulated according to the earning rule. This is typically done at regular intervals (e.g., weekly or monthly), but there is no reason why EV cannot be accumulated in near real-time, when work elements are started/completed. In fact, waiting to update EV only once per month (simply because that is when cost data are available) only detracts from a primary benefit of using EVM, which is to create a technical performance scoreboard for the project team.
 In a lightweight implementation such as described here, the project manager has not accumulated cost nor defined a detailed project schedule network (i.e., using a critical path or critical chain methodology). While such omissions are inappropriate for managing large projects, they are a common and reasonable occurrence in many very small or simple projects. Any project can benefit from using EV alone as a real-time score of progress. One useful result of this very simple approach (without schedule models and actual cost accumulation) is to compare EV curves of similar projects, as illustrated in Figure 5. In this example, the progress of three residential construction projects are compared by aligning the starting dates. If these three home construction projects were measured with the same PV valuations, the "relative" schedule performance of the projects can be easily compared.
Intermediate implementations (integrating technical and schedule performance).
In many projects, schedule performance (completing the work on time) is equal in importance to technical performance. For example, some new product development projects place a high premium on finishing quickly. It is not that cost is unimportant, but finishing the work later than a competitor may cost a great deal more in lost market share. It is likely that these kinds of projects will not use the lightweight version of EVM described in the previous section, because there is no planned timescale for measuring schedule performance. A second layer of EVM skill can be very helpful in managing the schedule performance of these “intermediate” projects. The project manager may employ a critical path or critical chain to build a project schedule model. As in the lightweight implementation, the project manager must define the work comprehensively, typically in a WBS hierarchy. He/she will construct a project schedule model that describes the precedence links between elements of work. This schedule model can then be used to develop the PV curve (or baseline), as shown in Figure 2.
It should be noted that measuring schedule performance using EVM does not replace the need to understand schedule performance versus the project's schedule model (precedence network). However, EVM schedule performance, as illustrated in Figure 2 provides an additional indicator — one that can be communicated in a single chart. Although it is theoretically possible that detailed schedule analysis will yield different conclusions than broad schedule analysis, in practice there tends to be a high correlation between the two. Although EVM schedule measurements are not necessarily conclusive, they provide useful diagnostic information.
Although such intermediate implementations do not require units of currency (e.g., dollars), it is common practice to use budgeted dollars as the scale for PV and EV. It is also common practice to track labor hours in parallel with currency. The following EVM formulas are for schedule management, and do not require accumulation of actual cost (AC). This is important because it is common in small and intermediate size projects for true costs to be unknown or unavailable.
However, Schedule Variance (SV) measured through EVM method is indicative only. To know whether a project is really behind or ahead of schedule (on time completion), Project Manager has to perform critical path analysis based on precedence and inter-dependencies of the project activities.
Making earned value schedule metrics concordant with the CPM schedule.
The actual critical path is ultimately the determining factor of every project's duration. Because earned value schedule metrics take no account of critical path data, big budget activities that are not on the critical path have the potential to dwarf the impact of performing small budget critical path activities. This can lead to "gaming" the SV and SPI metrics by ignoring critical path activities in favor of big budget activities that may have lots of float. This can sometimes even lead to performing activities out-of-sequence just to improve the schedule tracking metrics, which can cause major problems with quality.
A simple two-step process has been suggested to fix this:
1. Create a second earned value baseline strictly for schedule, with the weighted activities/milestones on the as-late-as-possible dates of the backward pass of the critical path algorithm, where there is no float. 
2. Allow earned value credit for schedule metrics to be taken no earlier than the reporting period during which the activity is scheduled unless it is on the project's current critical path.
In this way, the distorting aspect of float would be eliminated. There would be no benefit to performing a non-critical activity with lots of float until it is due in proper sequence. Also, an activity would not generate a negative schedule variance until it had used up its float. Under this method, one way of gaming the schedule metrics would be eliminated. The only way of generating a positive schedule variance (or SPI over 1.0) would be by completing work on the current critical path ahead of schedule, which is in fact the only way for a project to get ahead of schedule.
Advanced implementations (integrating cost, schedule and technical performance).
In addition to managing technical and schedule performance, large and complex projects require that cost performance be monitored and reviewed at regular intervals. To measure cost performance, planned value (or BCWS - Budgeted Cost of Work Scheduled) and earned value (or BCWP - Budgeted Cost of Work Performed) must be in units of currency (the same units that actual costs are measured.) In large implementations, the planned value curve is commonly called a Performance Measurement Baseline (PMB) and may be arranged in control accounts, summary-level planning packages, planning packages and work packages. In large projects, establishing control accounts is the primary method of delegating responsibility and authority to various parts of the performing organization. Control accounts are cells of a responsibility assignment (RACI) matrix, which is the intersection of the project WBS and the organizational breakdown structure (OBS). Control accounts are assigned to Control Account Managers (CAMs). Large projects require more elaborate processes for controlling baseline revisions, more thorough integration with subcontractor EVM systems, and more elaborate management of procured materials.
In the United States, the primary standard for full-featured EVM systems is the ANSI/EIA-748A standard, published in May 1998 and reaffirmed in August 2002. The standard defines 32 criteria for full-featured EVM system compliance. As of the year 2007, a draft of ANSI/EIA-748B, a revision to the original is available from ANSI. Other countries have established similar standards.
In addition to using BCWS and BCWP, prior to 1998 implementations often use the term Actual Cost of Work Performed (ACWP) instead of AC. Additional acronyms and formulas include:
Agile EVM.
In complex environments like software development, an iterative and incremental or Agile approach is often used to deliver complex products more successfully. Agile EVM is used as trend burndown/burnup graphs to make forecasts of progress towards a completion date transparent. However, EVM techniques are always used for the underlying calculations.
Preparation.
Setting up Agile EVM is similar to a simple implementation of EVM with the following preparation steps:
Practices.
 Agile EVM is now all about executing the project and tracking the accumulated EV according to the simple earning rule. Because Agile EVM has been evolving for many years the following practices are well-established:
Calculations.
Agile EVM is based on transparency and therefore graphically used in various trend charts. However, all EVM formulas (CPI, SPI, EAC, etc.) can still be used in Agile EVM by expressing the input variables like EV, PV and AC as:
Agile embraces change and therefore scope is considered variable (i.e. not fixed). Instead of using the INITIAL estimate in total number of Story Points, in Agile EVM calculations always the LATEST estimate in total number of Story Points is used to calculate CPI, SPI, EAC, etc.
Schedule Performance.
The use of SPI in EVM is rather limited in forecasting schedule performance problems because it is dependent on the completion of earned value on the Critical Time Path(CTP).
Because Agile EVM is used in a complex environment, any earned value is more likely to be on the CTP. The latest estimate for the number of fixed time intervals can be calculated in Agile EVM as:
Limitations.
Proponents of EVM note a number of issues with implementing it
, and further limitations may be inherent to the concept itself.
Because EVM requires quantification of a project plan, it is often perceived to be inapplicable to discovery-driven or Agile software development projects. For example, it may be impossible to plan certain research projects far in advance, because research itself uncovers some opportunities (research paths) and actively eliminates others. However, another school of thought holds that all work can be planned, even if in weekly timeboxes or other short increments. Thus, the challenge is to create agile or discovery-driven "implementations" of the EVM principle, and not simply to reject the notion of measuring technical performance objectively. (See the lightweight implementation for small projects, described above). Applying EVM in fast-changing work environments is, in fact, an area of project management research.
Traditional EVM is not intended for non-discrete (continuous) effort. In traditional EVM standards, non-discrete effort is called “level of effort" (LOE). If a project plan contains a significant portion of LOE, and the LOE is intermixed with discrete effort, EVM results will be contaminated. This is another area of EVM research.
Traditional definitions of EVM typically assume that project accounting and project network schedule management are prerequisites to achieving any benefit from EVM. Many small projects don't satisfy either of these prerequisites, but they too can benefit from EVM, as described for simple implementations, above. Other projects can be planned with a project network, but do not have access to true and timely actual cost data. The systems that feed the data required by earned value management are usually in silos rather than interfaced and integrated. In practice, the collection of true and timely actual cost data can be the most difficult aspect of EVM. Such projects can benefit from EVM, as described for intermediate implementations, above, and Earned Schedule.
As a means of overcoming objections to EVM's lack of connection to qualitative performance issues, the Naval Air Systems Command (NAVAIR) PEO(A) organization initiated a project in the late 1990s to integrate true technical achievement into EVM projections by utilizing risk profiles. These risk profiles anticipate opportunities that may be revealed and possibly be exploited as development and testing proceeds. The published research resulted in a Technical Performance Management (TPM) methodology and software application that is still used by many DoD agencies in informing EVM estimates with technical achievement.
The research was peer-reviewed and was the recipient of the Defense Acquisition University Acquisition Research Symposium 1997 Acker Award for excellence in the exchange of information in the field of acquisition research.
There is the difficulty inherent for any periodic monitoring of synchronizing data timing: actual deliveries, actual invoicing, and the date the EVM analysis is done are all independent, so that some items have arrived but their invoicing has not and by the time analysis is delivered the data will likely be weeks behind events. This may limit EVM to a less tactical or less definitive role where use is combined with other forms to explain why or add recent news and manage future expectations.
There is a measurement limitation for how precisely EVM can be used, stemming from classic conflict between accuracy and precision, as the mathematics can calculate deceptively far beyond the precision of the measurements of data and the approximation that is the plan estimation. The limitation on estimation is commonly understood (such as the ninety-ninety rule in software) but is not visible in any margin of error. The limitations on measurement are largely a form of digitization error as EVM measurements ultimately can be no finer than by item, which may be the Work Breakdown Structure terminal element size, to the scale of reporting period, typically end summary of a month, and by the means of delivery measure. (The delivery measure may be actual deliveries, may include estimates of partial work done at the end of month subject to estimation limits, and typically does not include QC check or risk offsets.)

</doc>
<doc id="9730" url="http://en.wikipedia.org/wiki?curid=9730" title="Electron microscope">
Electron microscope

An electron microscope is a microscope that uses a beam of accelerated electrons as a source of illumination. Because the wavelength of an electron can be up to 100,000 times shorter than that of visible light photons, the electron microscope has a higher resolving power than a light microscope and can reveal the structure of smaller objects. A transmission electron microscope can achieve better than 50 pm resolution and magnifications of up to about 10,000,000x whereas most light microscopes are limited by diffraction to about 200 nm resolution and useful magnifications below 2000x.
The transmission electron microscope uses electrostatic and electromagnetic lenses to control the electron beam and focus it to form an image. These electron optical lenses are analogous to the glass lenses of an optical light microscope.
Electron microscopes are used to investigate the ultrastructure of a wide range of biological and inorganic specimens including microorganisms, cells, large molecules, biopsy samples, metals, and crystals. Industrially, the electron microscope is often used for quality control and failure analysis. Modern electron microscopes produce electron micrographs using specialized digital cameras and frame grabbers to capture the image.
History.
The first electromagnetic lens was developed in 1926 by Hans Busch.
According to Dennis Gabor, the physicist Leó Szilárd tried in 1928 to convince Busch to build an electron microscope, for which he had filed a patent.
German physicist Ernst Ruska and the electrical engineer Max Knoll constructed the prototype electron microscope in 1931, capable of four-hundred-power magnification; the apparatus was the first demonstration of the principles of electron microscopy. Two years later, in 1933, Ruska built an electron microscope that exceeded the resolution attainable with an optical (light) microscope. Moreover, Reinhold Rudenberg, the scientific director of Siemens-Schuckertwerke, obtained the patent for the electron microscope in May 1931.
In 1932, Ernst Lubcke of Siemens & Halske built and obtained images from a prototype electron microscope, applying concepts described in the Rudenberg patent applications. Five years later (1937), the firm financed the work of Ernst Ruska and Bodo von Borries, and employed Helmut Ruska (Ernst’s brother) to develop applications for the microscope, especially with biological specimens. Also in 1937, Manfred von Ardenne pioneered the scanning electron microscope. The first "practical" electron microscope was constructed in 1938, at the University of Toronto, by Eli Franklin Burton and students Cecil Hall, James Hillier, and Albert Prebus; and Siemens produced the first "commercial" transmission electron microscope (TEM) in 1939. Although contemporary electron microscopes are capable of two million-power magnification, as scientific instruments, they remain based upon Ruska’s prototype.
Types.
Transmission electron microscope (TEM).
The original form of electron microscope, the transmission electron microscope (TEM) uses a high voltage electron beam to create an image. The electron beam is produced by an electron gun, commonly fitted with a tungsten filament cathode as the electron source. The electron beam is accelerated by an anode typically at +100 keV (40 to 400 keV) with respect to the cathode, focused by electrostatic and electromagnetic lenses, and transmitted through the specimen that is in part transparent to electrons and in part scatters them out of the beam. When it emerges from the specimen, the electron beam carries information about the structure of the specimen that is magnified by the objective lens system of the microscope. The spatial variation in this information (the "image") may be viewed by projecting the magnified electron image onto a fluorescent viewing screen coated with a phosphor or scintillator material such as zinc sulfide. Alternatively, the image can be photographically recorded by exposing a photographic film or plate directly to the electron beam, or a high-resolution phosphor may be coupled by means of a lens optical system or a fibre optic light-guide to the sensor of a CCD (charge-coupled device) camera. The image detected by the CCD may be displayed on a monitor or computer.
Resolution of the TEM is limited primarily by spherical aberration, but a new generation of aberration correctors have been able to partially overcome spherical aberration to increase resolution. Hardware correction of spherical aberration for the high-resolution transmission electron microscopy (HRTEM) has allowed the production of images with resolution below 0.5 angstrom (50 picometres) and magnifications above 50 million times. The ability to determine the positions of atoms within materials has made the HRTEM an important tool for nano-technologies research and development.
An important mode of TEM utilization is electron diffraction. The advantages of electron diffraction over X-ray crystallography are that the specimen need not be a single crystal or even a polycrystalline powder, and also that the Fourier transform reconstruction of the object's magnified structure occurs physically and thus avoids the need for solving the phase problem faced by the X-ray crystallographers after obtaining their X-ray diffraction patterns of a single crystal or polycrystalline powder. The major disadvantage of the transmission electron microscope is the need for extremely thin sections of the specimens, typically about 100 nanometers. Biological specimens are typically required to be chemically fixed, dehydrated and embedded in a polymer resin to stabilize them sufficiently to allow ultrathin sectioning. Sections of biological specimens, organic polymers and similar materials may require special treatment with heavy atom labels in order to achieve the required image contrast.
Scanning electron microscope (SEM).
Unlike the TEM, where electrons of the high voltage beam carry the image of the specimen, the electron beam of the scanning electron microscope (SEM) does not at any time carry a complete image of the specimen. The SEM produces images by probing the specimen with a focused electron beam that is scanned across a rectangular area of the specimen (raster scanning). When the electron beam interacts with the specimen, it loses energy by a variety of mechanisms. The lost energy is converted into alternative forms such as heat, emission of low-energy secondary electrons and high-energy backscattered electrons, light emission (cathodoluminescence) or X-ray emission, all of which provide signals carrying information about the properties of the specimen surface, such as its topography and composition. The image displayed by an SEM maps the varying intensity of any of these signals into the image in a position corresponding to the position of the beam on the specimen when the signal was generated. In the SEM image of an ant shown at right, the image was constructed from signals produced by a secondary electron detector, the normal or conventional imaging mode in most SEMs.
Generally, the image resolution of an SEM is at least an order of magnitude poorer than that of a TEM. However, because the SEM image relies on surface processes rather than transmission, it is able to image bulk samples up to many centimetres in size and (depending on instrument design and settings) has a great depth of field, and so can produce images that are good representations of the three-dimensional shape of the sample. Another advantage of SEM is its variety called environmental scanning electron microscope (ESEM) can produce images of sufficient quality and resolution with the samples being wet or contained in low vacuum or gas. This greatly facilitates imaging biological samples that are unstable in the high vacuum of conventional electron microscopes.
Color.
In their most common configurations, electron microscopes produce images with a single brightness value per pixel, with the results usually rendered in grayscale. However, often these images are then colorized through the use of feature-detection software, or simply by hand-editing using a graphics editor. This is usually for aesthetic effect or for clarifying structure, and generally does not add information about the specimen.
In some configurations more information about specimen properties is gathered per pixel, usually by the use of multiple detectors. In SEM, the attributes of topography and material contrast can be obtained by a pair of backscattered electron detectors and such attributes can be superimposed in a single color image by assigning a different primary color to each attribute. Similarly, a combination of backscattered and secondary electron signals can be assigned to different colors and superimposed on a single color micrograph displaying simultaneously the properties of the specimen.
In a similar method, secondary electron and backscattered electron detectors are superimposed and a colour is assigned to each of the images captured by each detector, with an end result of a combined colour image where colours are related to the density of the components. This method is known as Density-dependent colour SEM (DDC-SEM). Micrographs produced by DDC-SEM retain topographical information, which is better captured by the secondary electrons detector and combine it to the information about density, obtained by the backscattered electron detector.
Some types of detectors used in SEM have analytical capabilities, and can provide several items of data at each pixel. Examples are the Energy-dispersive X-ray spectroscopy (EDS) detectors used in elemental analysis and Cathodoluminescence microscope (CL) systems that analyse the intensity and spectrum of electron-induced luminescence in (for example) geological specimens. In SEM systems using these detectors it is common to color code the signals and superimpose them in a single color image, so that differences in the distribution of the various components of the specimen can be seen clearly and compared. Optionally, the standard secondary electron image can be merged with the one or more compositional channels, so that the specimen's structure and composition can be compared. Such images can be made while maintaining the full integrity of the original signal, which is not modified in any way.
Reflection electron microscope (REM).
In the reflection electron microscope (REM) as in the TEM, an electron beam is incident on a surface but instead of using the transmission (TEM) or secondary electrons (SEM), the reflected beam of elastically scattered electrons is detected. This technique is typically coupled with reflection high energy electron diffraction (RHEED) and "reflection high-energy loss spectroscopy (RHELS)". Another variation is spin-polarized low-energy electron microscopy (SPLEEM), which is used for looking at the microstructure of magnetic domains.
Scanning transmission electron microscope (STEM).
The STEM rasters a focused incident probe across a specimen that (as with the TEM) has been thinned to facilitate detection of electrons scattered "through" the specimen. The high resolution of the TEM is thus possible in STEM. The focusing action (and aberrations) occur before the electrons hit the specimen in the STEM, but afterward in the TEM. The STEMs use of SEM-like beam rastering simplifies annular dark-field imaging, and other analytical techniques, but also means that image data is acquired in serial rather than in parallel fashion. Often TEM can be equipped with the scanning option and then it can function both as TEM and STEM.
Sample preparation.
Materials to be viewed under an electron microscope may require processing to produce a suitable sample. The technique required varies depending on the specimen and the analysis required:
Disadvantages.
Electron microscopes are expensive to build and maintain, but the capital and running costs of confocal light microscope systems now overlaps with those of basic electron microscopes. Microscopes designed to achieve high resolutions must be housed in stable buildings (sometimes underground) with special services such as magnetic field cancelling systems.
The samples largely have to be viewed in vacuum, as the molecules that make up air would scatter the electrons. One exception is the environmental scanning electron microscope, which allows hydrated samples to be viewed in a low-pressure (up to 20 Torr) and/or wet environment.
Scanning electron microscopes operating in conventional high-vacuum mode usually image conductive specimens; therefore non-conductive materials require conductive coating (gold/palladium alloy, carbon, osmium, etc.) Low-voltage mode of modern microscopes makes possible observation of non-conductive specimens without coating. Non-conductive materials can be imaged also by a variable pressure (or environmental) scanning electron microscope.
Small, stable specimens such as carbon nanotubes, diatom frustules and small mineral crystals (asbestos fibres, for example) require no special treatment before being examined in the electron microscope. Samples of hydrated materials, including almost all biological specimens have to be prepared in various ways to stabilize them, reduce their thickness (ultrathin sectioning) and increase their electron optical contrast (staining). These processes may result in "artifacts", but these can usually be identified by comparing the results obtained by using radically different specimen preparation methods. It is generally believed by scientists working in the field that as results from various preparation techniques have been compared and that there is no reason that they should all produce similar artifacts, it is reasonable to believe that electron microscopy features correspond with those of living cells. Since the 1980s, analysis of cryofixed, vitrified specimens has also become increasingly used by scientists, further confirming the validity of this technique.
External links.
History.
John H L Watson's recollections at the University of Toronto when he worked with Hillier and Prebus: 

</doc>
<doc id="9731" url="http://en.wikipedia.org/wiki?curid=9731" title="List of recently extinct birds">
List of recently extinct birds

Since 1500, over 190 species of birds have become extinct, and this rate of extinction seems to be increasing. The situation is exemplified by Hawaii, where 30% of all known recently extinct bird taxa originally lived. Other areas, such as Guam, have also been hit hard; Guam has lost over 60% of its native bird taxa in the last 30 years, many of them due to the introduced brown tree snake.
Currently there are approximately 10,000 species of birds, with an estimated 1,200 considered to be under threat of extinction.
Island species in general, and flightless island species in particular are most at risk. The disproportionate number of rails in the list reflects the tendency of that family to lose the ability to fly when geographically isolated. Even more rails became extinct before they could be described by scientists; these taxa are listed in Late Quaternary prehistoric birds.
The extinction dates given below are usually approximations of the actual date of extinction. In some cases, more exact dates are given as it is sometimes possible to pinpoint the date of extinction to a specific year or even day (the San Benedicto rock wren is possibly the most extreme example—its extinction could be timed with an accuracy of maybe half an hour). Extinction dates in the literature are usually the dates of the last verified record (credible observation or specimen taken); in many Pacific birds which became extinct shortly after European contact, however, this leaves an uncertainty period of over a century because the islands on which they used to occur were only rarely visited by scientists.
Extinct bird species.
Struthioniformes.
The ostrich and related ratites
Anseriformes.
Ducks, geese and swans
Galliformes.
Quails and relatives
See also Bokaak "bustard" under Gruiformes below
Charadriiformes.
Shorebirds, gulls and auks
Gruiformes.
Rails and allies - probably paraphyletic
Podicipediformes.
Grebes
Ciconiiformes.
Herons and related birds - possibly paraphyletic
Pelecaniformes.
Cormorants and related birds
Procellariiformes.
Petrels, shearwaters, albatrosses and storm petrels.
Sphenisciformes.
Penguins
Columbiformes.
Pigeons, doves and dodos
For the "Réunion solitaire", see Réunion sacred ibis.
Psittaciformes.
Parrots
Cuculiformes.
Cuckoos
Falconiformes.
Birds of prey
Strigiformes.
Typical owls and barn-owls.
Caprimulgiformes.
Caprimulgidae - nightjars and nighthawks
Reclusive ground-nesting birds that sally out at night to hunt for large insects and similar prey. They are easily located by the males' song, but this is not given all year. Habitat destruction represents currently the biggest threat, while island populations are threatened by introduced mammalian predators, notably dogs, cats, pigs and mongoose.
Apodiformes.
Swifts and hummingbirds
Coraciiformes.
Kingfishers and related birds
Piciformes.
Woodpeckers and related birds
Passeriformes.
Perching birds
Acanthisittidae– New Zealand "wrens"
Formicariidae – antpittas and antthrushes
Mohoidae – Hawaiian "honeyeaters". Family established in 2008, previously in Meliphagidae.
Meliphagidae – honeyeaters and Australian chats
Acanthizidae – scrubwrens, thornbills, and gerygones
Pachycephalidae – whistlers, shrike-thrushes, pitohuis and allies
Dicruridae – monarch flycatchers and allies
†Turnagridae – piopios
Callaeidae – New Zealand wattlebirds
Hirundinidae – swallows and martins
Acrocephalidae – marsh and tree warblers
Muscicapidae – Old World flycatchers and chats
Megaluridae – megalurid warblers or grass warblers
Cisticolidae – cisticolas and allies
Zosteropidae – white-eyes - probably belonging to Timaliidae
Timaliidae – Old World babblers
Pycnonotidae – bulbuls
Sylvioidea "incertae sedis"
Sturnidae – starlings
Turdidae – thrushes and allies
Mimidae – mockingbirds and thrashers
Estrildidae– estrildid finches (waxbills, munias, etc.)
Icteridae – grackles
Parulidae – New World warblers
Ploceidae – weavers
Fringillidae – true finches and Hawaiian honeycreepers
Emberizidae – buntings and American sparrow
(Probably) extinct subspecies of birds.
Extinction of subspecies is a subject very dependent on guesswork. National and international conservation projects and research publications such as redlists usually focus on species as a whole. Reliable information on the status of threatened subspecies usually has to be assembled piecemeal from published observations such as regional checklists. Therefore, the following listing contains a high proportion of taxa that may still exist, but are listed here due to any combination of absence of recent records, a known threat such as habitat destruction, or an observed decline.
Struthioniformes.
The ostrich and related ratites
Tinamiformes.
Tinamous
Anseriformes.
Ducks, geese and swans
Galliformes.
Quails and relatives
Charadriiformes.
Shorebirds, gulls and auks
Gruiformes.
Rails and allies - probably paraphyletic
Ciconiiformes.
Herons and related birds - possibly paraphyletic
Pteroclidiformes.
Sandgrouses
Columbiformes.
Pigeons, doves and dodos
Psittaciformes.
Parrots
Cuculiformes.
Cuckoos
Falconiformes.
Birds of prey
Strigiformes.
Typical owls and barn-owls
Caprimulgiformes.
Nightjars and allies
Apodiformes.
Swifts and hummingbirds
Coraciiformes.
Kingfishers and related birds
Piciformes.
Woodpeckers and related birds
Passeriformes.
Perching birds
Pittidae – pittas
Tyrannidae – tyrant flycatchers
Furnariidae – ovenbirds
Formicariidae – antpittas and antthrushes
Maluridae – Australasian "wrens"
Pardalotidae – pardalotes, scrubwrens, thornbills, and gerygones
Petroicidae – Australasian "robins"
Cinclosomatidae – whipbirds and allies
Artamidae – woodswallows, currawongs and allies
Monarchidae – monarch flycatchers
Rhipiduridae – fantails
Campephagidae – cuckoo-shrikes and trillers
Oriolidae – orioles and figbird
Corvidae – crows, ravens, magpies and jays
Callaeidae – New Zealand wattlebirds
Regulidae – kinglets
Hirundinidae – swallows and martins
Phylloscopidae – phylloscopid warblers or leaf-warblers
Cettiidae – cettiid warblers or typical bush-warblers
Acrocephalidae – acrocephalid warblers or marsh- and tree warblers
Pycnonotidae – bulbuls
Cisticolidae – cisticolas and allies
Sylviidae – sylviid ("true") warblers and parrotbills
Zosteropidae – white-eyes. Probably belong into Timaliidae
Timaliidae – Old World babblers
"African warblers"
Sylvioidea "incertae sedis"
Troglodytidae – wrens
Paridae – tits, chickadees and titmice
Cinclidae – dippers
Muscicapidae – Old World flycatchers and chats
Turdidae – thrushes and allies
Mimidae – mockingbirds and thrashers
Estrildidae – Estrildid finches (waxbills, munias, etc.)
Fringillidae – True finches and Hawaiian honeycreepers
Icteridae – grackles
Parulidae – New World warblers
Thraupidae – tanagers
Emberizidae – buntings and American sparrows

</doc>
<doc id="9732" url="http://en.wikipedia.org/wiki?curid=9732" title="Eli Whitney">
Eli Whitney

Eli Whitney (December 8, 1765 – January 8, 1825) was an American inventor best known for inventing the cotton gin. This was one of the key inventions of the Industrial Revolution and shaped the economy of the Antebellum South. Whitney's invention made upland short cotton into a profitable crop, which strengthened the economic foundation of slavery in the United States. Despite the social and economic impact of his invention, Whitney lost many profits in legal battles over patent infringement for the cotton gin. Thereafter, he turned his attention into securing contracts with the government in the manufacture of muskets for the newly formed United States Army. He continued making arms and inventing until his death in 1825.
Early life and education.
Whitney was born in Westborough, Massachusetts, on December 8, 1765, the eldest child of Eli Whitney Sr., a prosperous farmer, and his wife Elizabeth Fay, also of Westborough.
Although the younger Eli, born in 1765, could technically be called a "Junior", history has never known him as such. He was famous during his lifetime and afterward by the name "Eli Whitney". His son, born in 1820, also named Eli, was well known during his lifetime and afterward by the name "Eli Whitney, Jr."
Whitney's mother, Elizabeth Fay, died in 1777, when he was 11. At age 14 he operated a profitable nail manufacturing operation in his father's workshop during the Revolutionary War.
Because his stepmother opposed his wish to attend college, Whitney worked as a farm laborer and school teacher to save money. He prepared for Yale at Leicester Academy (now Becker College) and under the tutelage of Rev. Elizur Goodrich of Durham, Connecticut, he entered the class of 1789 and graduated Phi Beta Kappa in 1792. Whitney expected to study law but, finding himself short of funds, accepted an offer to go to South Carolina as a private tutor.
Instead of reaching his destination, he was convinced to visit Georgia. In the closing years of the 18th century, Georgia was a magnet for New Englanders seeking their fortunes (its Revolutionary-era governor had been Lyman Hall, a migrant from Connecticut). When he initially sailed for South Carolina, among his shipmates were the widow and family of the Revolutionary hero Gen. Nathanael Greene of Rhode Island. Mrs. Greene invited Whitney to visit her Georgia plantation, Mulberry Grove. Her plantation manager and husband-to-be was Phineas Miller, another Connecticut migrant and Yale graduate (class of 1785), who would become Whitney's business partner.
Whitney is most famous for two innovations which later divided the United States in the mid-19th century: the cotton gin (1793) and his advocacy of interchangeable parts. In the South, the cotton gin revolutionized the way cotton was harvested and reinvigorated slavery. In the North the adoption of interchangeable parts revolutionized the manufacturing industry, and contributed greatly to the U.S. victory in the Civil War.
Career.
Interchangeable parts.
Eli Whitney has often been incorrectly credited with inventing the idea of interchangeable parts, which he championed for years as a maker of muskets; however, the idea predated Whitney, and Whitney's role in it was one of promotion and popularizing, not invention. Successful implementation of the idea eluded Whitney until near the end of his life, occurring first in others' armories.
Attempts at interchangeability of parts can be traced back as far as the Punic Wars through both archaeological remains of boats now in Museo Archeologico Baglio Anselmi and contemporary written accounts. In modern times the idea developed over decades among many people. An early leader was Jean-Baptiste Vaquette de Gribeauval, an 18th-century French artillerist who created a fair amount of standardization of artillery pieces, although not true interchangeability of parts. He inspired others, including Honoré Blanc and Louis de Tousard, to work further on the idea, and on shoulder weapons as well as artillery. In the 19th century these efforts produced the "armory system," or American system of manufacturing. Certain other New Englanders, including Captain John H. Hall and Simeon North, arrived at successful interchangeability before Whitney's armory did. The Whitney armory finally succeeded not long after his death in 1825.
The motives behind Whitney's acceptance of a contract to manufacture muskets in 1798 were mostly monetary. By the late 1790s, Whitney was on the verge of bankruptcy and the cotton gin litigation had left him deeply in debt. His New Haven cotton gin factory had burned to the ground, and litigation sapped his remaining resources. The French Revolution had ignited new conflicts between Great Britain, France, and the United States. The new American government, realizing the need to prepare for war, began to rearm. The War Department issued contracts for the manufacture of 10,000 muskets. Whitney, who had never made a gun in his life, obtained a contract in January 1798 to deliver 10,000 to 15,000 muskets in 1800. He had not mentioned interchangeable parts at that time. Ten months later, the Treasury Secretary, Oliver Wolcott, Jr., sent him a "foreign pamphlet on arms manufacturing techniques," possibly one of Honoré Blanc's reports, after which Whitney first began to talk about interchangeability.
In May 1798, Congress voted for legislation that would use eight hundred thousand dollars in order to pay for small arms and cannons in case war with France erupted. They offered a 5,000 dollar incentive with an additional 5,000 dollars once that money was exhausted for the person that was able to accurately produce arms for the government. Because the cotton gin had not brought Whitney the rewards he believed he would get, he accepted the contract. Although the contract was for one year, Whitney did not deliver the arms until eight years later in 1809 using multiple excuses for the delay of such. Recently, historians have found that during 1801–1806, Whitney took the money and headed into South Carolina in order to profit from the cotton gin.
Although Whitney's demonstration of 1801 appeared to show the ingenuity of interchangeable parts, Merritt Roe Smith concludes that Whitney's demonstration was "staged" and "duped government authorities" into believing that he had created interchangeable parts. The charade was only useful in order to gain more time and resources for the project but not to create interchangeable parts.
When the government complained that Whitney's price per musket compared unfavorably with those produced in government armories, Whitney was able to calculate an actual price per musket by including fixed costs such as insurance and machinery, which the government had not included. He thus made early contributions to both the concept of cost accounting, and the concept of the efficiency of private industry.
Cotton gin.
The cotton gin is a mechanical device that removes the seeds from cotton, a process that had previously been extremely labor-intensive. The word "gin" is short for "engine." The cotton gin was a wooden drum stuck with hooks that pulled the cotton fibers through a mesh. The cotton seeds would not fit through the mesh and fell outside. Whitney occasionally told a story wherein he was pondering an improved method of seeding the cotton when he was inspired by observing a cat attempting to pull a chicken through a fence, and could only pull through some of the feathers.
A single cotton gin could generate up to 55 lb of cleaned cotton daily. This contributed to the economic development of the Southern states of the United States, a prime cotton growing area; some historians believe that this invention allowed for the African slavery system in the Southern United States to become more sustainable at a critical point in its development.
Whitney received a patent (later numbered as X72) for his cotton gin on March 14, 1794, but it was not validated until 1807. Whitney and his partner, Miller, did not intend to sell the gins. Rather, like the proprietors of grist and sawmills, they expected to charge farmers for cleaning their cotton – two-fifths of the value, paid in cotton. Resentment at this scheme, the mechanical simplicity of the device and the primitive state of patent law, made infringement inevitable. Whitney and Miller could not build enough gins to meet demand, so gins from other makers found ready sale. Ultimately, patent infringement lawsuits consumed the profits and their cotton gin company went out of business in 1797. One oft-overlooked point is that there were drawbacks to Whitney's first design. There is significant evidence that the design flaws were solved by a plantation owner, Catherine Littlefield Greene, wife of the American Revolutionary War general Nathanael Greene; Whitney gave her no public credit or recognition.
While the cotton gin did not earn Whitney the fortune he had hoped for, it did give him fame.
It has been argued by some historians that Whitney's cotton gin was an important if unintended cause of the American Civil War. Before the invention of the cotton gin, slavery had been on the decline; in fact many slaveholders had even given away their slaves. After Whitney's invention, the plantation slavery industry was rejuvenated, eventually culminating in the Civil War.
And the cotton gin transformed Southern agriculture and the national economy. Southern cotton found ready markets in Europe and in the burgeoning textile mills of New England. Cotton exports from the U.S. boomed after the cotton gin's appearance – from less than 500000 lb in 1793 to 93 e6lb by 1810. Cotton was a staple that could be stored for long periods and shipped long distances, unlike most agricultural products. It became the U.S.'s chief export, representing over half the value of U.S. exports from 1820 to 1860.
Paradoxically, the cotton gin, a labor-saving device, helped preserve slavery in the U.S. Before the 1790s, slave labor was primarily employed in growing rice, tobacco, and indigo, none of which were especially profitable any more. Neither was cotton, due to the difficulty of seed removal. But with the gin, growing cotton with slave labor became highly profitable – the chief source of wealth in the American South, and the basis of frontier settlement from Georgia to Texas. "King Cotton" became a dominant economic force, and slavery was sustained as a key institution of Southern society.
Milling machine.
Machine tool historian Joseph W. Roe credited Whitney with inventing the first milling machine circa 1818. Subsequent work by other historians (Woodbury; Smith; Muir; Battison [cited by Baida]) suggests that Whitney was among a group of contemporaries all developing milling machines at about the same time (1814 to 1818), and that the others were more important to the innovation than Whitney was. (The machine that excited Roe may not have been built until 1825, after Whitney's death.) Therefore, no one person can properly be described as the inventor of the milling machine.
Later life and legacy.
Despite his humble origins, Whitney was keenly aware of the value of social and political connections. In building his arms business, he took full advantage of the access that his status as a Yale alumnus gave him to other well-placed graduates, such as Oliver Wolcott, Jr., Secretary of the Treasury (class of 1778), and James Hillhouse, a New Haven developer and political leader.
His 1817 marriage to Henrietta Edwards, granddaughter of the famed evangelist Jonathan Edwards, daughter of Pierpont Edwards, head of the Democratic Party in Connecticut, and first cousin of Yale's president, Timothy Dwight, the state's leading Federalist, further tied him to Connecticut's ruling elite. In a business dependent on government contracts, such connections were essential to success.
Whitney died of prostate cancer on January 8, 1825, in New Haven, Connecticut, just a month after his 59th birthday. He left a widow and his four children behind. During the course of his illness, he invented and constructed several devices to mechanically ease his pain. These devices, drawings of which are in his collected papers, were effective but were never manufactured for use of others due to his heirs' reluctance to trade in "indelicate" items.
The Eli Whitney Students Program, Yale University's admissions program for non-traditional students, is named after Whitney who matriculated into Yale when he was 23.
Further reading.
</dl>

</doc>
<doc id="9734" url="http://en.wikipedia.org/wiki?curid=9734" title="The American Prisoner">
The American Prisoner

The American Prisoner is a British novel written by Eden Phillpotts and published in 1904 and adapted into a film by the same name in 1929. The story concerns an English woman who lives at Fox Tor farm, and an American captured during the American Revolutionary War and held at the prison at Princetown on Dartmoor.
The heroine's father, Maurice Malherb, is based on Thomas Windeatt.
In the novel "Malherb" is a miscreant who destroys Childe's tomb and beats his servant. He is depicted as a victim of his own bad temper rather than a sadist.
Malherb is introduced as the younger son of a noble family and he builds the Fox Tor house to be the impressive gentleman's residence suggested by William Crossing rather than the humble cottage which it actually is.

</doc>
<doc id="9735" url="http://en.wikipedia.org/wiki?curid=9735" title="Electromagnetic field">
Electromagnetic field

An electromagnetic field (also EMF or EM field) is a physical field produced by electrically charged objects. It affects the behavior of charged objects in the vicinity of the field. The electromagnetic field extends indefinitely throughout space and describes the electromagnetic interaction. It is one of the four fundamental forces of nature (the others are gravitation, weak interaction and strong interaction).
The field can be viewed as the combination of an electric field and a magnetic field. The electric field is produced by stationary charges, and the magnetic field by moving charges (currents); these two are often described as the sources of the field. The way in which charges and currents interact with the electromagnetic field is described by Maxwell's equations and the Lorentz force law.
From a classical perspective in the history of electromagnetism, the electromagnetic field can be regarded as a smooth, continuous field, propagated in a wavelike manner; whereas from the perspective of quantum field theory, the field is seen as quantized, being composed of individual particles.
Structure of the electromagnetic field.
The electromagnetic field may be viewed in two distinct ways: a continuous structure or a discrete structure.
Continuous structure.
Classically, electric and magnetic fields are thought of as being produced by smooth motions of charged objects. For example, oscillating charges produce electric and magnetic fields that may be viewed in a 'smooth', continuous, wavelike fashion. In this case, energy is viewed as being transferred continuously through the electromagnetic field between any two locations. For instance, the metal atoms in a radio transmitter appear to transfer energy continuously. This view is useful to a certain extent (radiation of low frequency), but problems are found at high frequencies (see ultraviolet catastrophe).
Discrete structure.
The electromagnetic field may be thought of in a more 'coarse' way. Experiments reveal that in some circumstances electromagnetic energy transfer is better described as being carried in the form of packets called quanta (in this case, photons) with a fixed frequency. Planck's relation links the energy "E" of a photon to its frequency ν through the equation:
where "h" is Planck's constant, named in honor of Max Planck, and ν is the frequency of the photon . Although modern quantum optics tells us that there also is a semi-classical explanation of the photoelectric effect—the emission of electrons from metallic surfaces subjected to electromagnetic radiation—the photon was historically (although not strictly necessarily) used to explain certain observations. It is found that increasing the intensity of the incident radiation (so long as one remains in the linear regime) increases only the number of electrons ejected, and has almost no effect on the energy distribution of their ejection. Only the frequency of the radiation is relevant to the energy of the ejected electrons.
This quantum picture of the electromagnetic field (which treats it as analogous to harmonic oscillators) has proved very successful, giving rise to quantum electrodynamics, a quantum field theory describing the interaction of electromagnetic radiation with charged matter. It also gives rise to quantum optics, which is different from quantum electrodynamics in that the matter itself is modelled using quantum mechanics rather than quantum field theory.
Dynamics of the electromagnetic field.
In the past, electrically charged objects were thought to produce two different, unrelated types of field associated with their charge property. An electric field is produced when the charge is stationary with respect to an observer measuring the properties of the charge, and a magnetic field (as well as an electric field) is produced when the charge moves (creating an electric current) with respect to this observer. Over time, it was realized that the electric and magnetic fields are better thought of as two parts of a greater whole — the electromagnetic field. Recall that until 1831 electricity and magnetism had been viewed as unrelated phenomena. In 1831, Michael Faraday, one of the great thinkers of his time, made the seminal observation that time-varying magnetic fields could induce electric currents and then, in 1864, James Clerk Maxwell published his famous paper on a dynamical theory of the electromagnetic field. See Maxwell 1864 5, page 499; also David J. Griffiths (1999), Introduction to electrodynamics, third Edition, ed. Prentice Hall, pp. 559-562"(as quoted in Gabriela, 2009).
Once this electromagnetic field has been produced from a given charge distribution, other charged objects in this field will experience a force (in a similar way that planets experience a force in the gravitational field of the Sun). If these other charges and currents are comparable in size to the sources producing the above electromagnetic field, then a new net electromagnetic field will be produced. Thus, the electromagnetic field may be viewed as a dynamic entity that causes other charges and currents to move, and which is also affected by them. These interactions are described by Maxwell's equations and the Lorentz force law. (This discussion ignores the radiation reaction force.)
Electromagnetic field as a feedback loop.
The behavior of the electromagnetic field can be resolved into four different parts of a loop:
A common misunderstanding is that (a) the quanta of the fields act in the same manner as (b) the charged particles that generate the fields. In our everyday world, charged particles, such as electrons, move slowly through matter with a drift velocity of a fraction of a centimeter (or inch) per second, but fields propagate at the speed of light - approximately 300 thousand kilometers (or 186 thousand miles) a second. The mundane speed difference between charged particles and field quanta is on the order of one to a million, more or less. Maxwell's equations relate (a) the presence and movement of charged particles with (b) the generation of fields. Those fields can then affect the force on, and can then move other slowly moving charged particles. Charged particles can move at relativistic speeds nearing field propagation speeds, but, as Einstein showed, this requires enormous field energies, which are not present in our everyday experiences with electricity, magnetism, matter, and time and space.
The feedback loop can be summarized in a list, including phenomena belonging to each part of the loop:
Mathematical description.
There are different mathematical ways of representing the electromagnetic field. The first one views the electric and magnetic fields as three-dimensional vector fields. These vector fields each have a value defined at every point of space and time and are thus often regarded as functions of the space and time coordinates. As such, they are often written as E(x, y, z, t) (electric field) and B(x, y, z, t) (magnetic field).
If only the electric field (E) is non-zero, and is constant in time, the field is said to be an electrostatic field. Similarly, if only the magnetic field (B) is non-zero and is constant in time, the field is said to be a magnetostatic field. However, if either the electric or magnetic field has a time-dependence, then both fields must be considered together as a coupled electromagnetic field using Maxwell's equations.
With the advent of special relativity, physical laws became susceptible to the formalism of tensors. Maxwell's equations can be written in tensor form, generally viewed by physicists as a more elegant means of expressing physical laws.
The behaviour of electric and magnetic fields, whether in cases of electrostatics, magnetostatics, or electrodynamics (electromagnetic fields), is governed by Maxwell's equations. In the vector field formalism, these are:
where formula_6 is the charge density, which can (and often does) depend on time and position, formula_7 is the permittivity of free space, formula_8 is the permeability of free space, and J is the current density vector, also a function of time and position. The units used above are the standard SI units. Inside a linear material, Maxwell's equations change by switching the permeability and permittivity of free space with the permeability and permittivity of the linear material in question. Inside other materials which possess more complex responses to electromagnetic fields, these terms are often represented by complex numbers, or tensors.
The Lorentz force law governs the interaction of the electromagnetic field with charged matter.
When a field travels across to different media, the properties of the field change according to the various boundary conditions. These equations are derived from Maxwell's equations.
The tangential components of the electric and magnetic fields as they relate on the boundary of two media are as follows:
The angle of refraction of an electric field between media is related to the permittivity formula_13 of each medium:
The angle of refraction of a magnetic field between media is related to the permeability formula_15 of each medium:
Properties of the field.
Reciprocal behavior of electric and magnetic fields.
The two Maxwell equations, Faraday's Law and the Ampère-Maxwell Law, illustrate a very practical feature of the electromagnetic field. Faraday's Law may be stated roughly as 'a changing magnetic field creates an electric field'. This is the principle behind the electric generator.
Ampere's Law roughly states that 'a changing electric field creates a magnetic field'. Thus, this law can be applied to generate a magnetic field and run an electric motor.
Light as an electromagnetic disturbance.
Maxwell's equations take the form of an electromagnetic wave in a volume of space not containing charges or currents (free space) – that is, where formula_6 and J are zero. Under these conditions, the electric and magnetic fields satisfy the electromagnetic wave equation:
James Clerk Maxwell was the first to obtain this relationship by his completion of Maxwell's equations with the addition of a displacement current term to Ampere's Circuital law.
Relation to and comparison with other physical fields.
Being one of the four fundamental forces of nature, it is useful to compare the electromagnetic field with the gravitational, strong and weak fields. The word 'force' is sometimes replaced by 'interaction' because modern particle physics models electromagnetism as an exchange of particles known as gauge bosons.
Electromagnetic and gravitational fields.
Sources of electromagnetic fields consist of two types of charge – positive and negative. This contrasts with the sources of the gravitational field, which are masses. Masses are sometimes described as "gravitational charges", the important feature of them being that there are only positive masses and no negative masses. Further, gravity differs from electromagnetism in that positive masses attract other positive masses whereas same charges in electromagnetism repel each other.
The relative strengths and ranges of the four interactions and other information are tabulated below:
Applications.
Static E and M fields and static EM fields.
When an EM field (see electromagnetic tensor) is not varying in time, it may be seen as a purely electrical field or a purely magnetic field, or a mixture of both. However the general case of a static EM field with both electric and magnetic components present, is the case that appears to most observers. Observers who see only an electric or magnetic field component of a static EM field, have the other (electric or magnetic) component suppressed, due to the special case of the immobile state of the charges that produce the EM field in that case. In such cases the other component becomes manifest in other observer frames.
A consequence of this, is that any case that seems to consist of a "pure" static electric or magnetic field, can be converted to an EM field, with both E and M components present, by simply moving the observer into a frame of reference which is moving with regard to the frame in which only the “pure” electric or magnetic field appears. That is, a pure static electric field will show the familiar magnetic field associated with a current, in any frame of reference where the charge moves. Likewise, any new motion of a charge in a region that seemed previously to contain only a magnetic field, will show that that the space now contains an electric field as well, which will be found to produces an additional Lorentz force upon the moving charge.
Thus, electrostatics, as well as magnetism and magnetostatics, are now seen as studies of the static EM field when a particular frame has been selected to suppress the other type of field, and since an EM field with both electric and magnetic will appear in any other frame, these "simpler" effects are merely the observer's. The "applications" of all such non-time varying (static) fields are discussed in the main articles linked in this section.
Time-varying EM fields in Maxwell’s equations.
An EM field that varies in time has two “causes” in Maxwell’s equations. One is charges and currents (so-called “sources”), and the other cause for an E or M field is a change in the other type of field (this last cause also appears in “free space” very far from currents and charges).
An electromagnetic field very far from currents and charges (sources) is called electromagnetic radiation (EMR) since it radiates from the charges and currents in the source, and has no "feedback" effect on them, and is also not affected directly by them in the present time (rather, it is indirectly produced by a sequences of changes in fields radiating out from them in the past). EMR consists of the radiations in the electromagnetic spectrum, including radio waves, microwave, infrared, visible light, ultraviolet light, X-rays, and gamma rays. The many commercial applications of these radiations are discussed in the named and linked articles.
A notable application of visible light is that this type of energy from the Sun powers all life on Earth that either makes or uses oxygen.
A changing electromagnetic field which is physically close to currents and charges (see near and far field for a definition of “close”) will have a dipole characteristic that is dominated by either a changing electric dipole, or a changing magnetic dipole. This type of dipole field near sources is called an electromagnetic "near-field".
Changing "electric" dipole fields, as such, are used commercially as near-fields mainly as a source of dielectric heating. Otherwise, they appear parasitically around conductors which absorb EMR, and around antennas which have the purpose of generating EMR at greater distances.
Changing "magnetic" dipole fields (i.e., magnetic near-fields) are used commercially for many types of magnetic induction devices. These include motors and electrical transformers at low frequencies, and devices such as metal detectors and MRI scanner coils at higher frequencies. Sometimes these high-frequency magnetic fields change at radio frequencies without being far-field waves and thus radio waves; see RFID tags.
See also near-field communication.
Further uses of near-field EM effects commercially, may be found in the article on virtual photons, since at the quantum level, these fields are represented by these particles. Far-field effects (EMR) in the quantum picture of radiation, are represented by ordinary photons.
Health and safety.
The potential health effects of the very low frequency EMFs surrounding power lines and electrical devices are the subject of on-going research and a significant amount of public debate. The US National Institute for Occupational Safety and Health (NIOSH) has issued some cautionary advisories but stresses that the data is currently too limited to draw good conclusions.
The potential effects of electromagnetic fields on human health vary widely depending on the frequency and intensity of the fields. For more information on the health effects due to specific parts of the electromagnetic spectrum, see the following articles:

</doc>
<doc id="9736" url="http://en.wikipedia.org/wiki?curid=9736" title="Empire State Building">
Empire State Building

The Empire State Building is a 102-story skyscraper located in Midtown Manhattan, New York City, on Fifth Avenue between West 33rd and 34th Streets. It has a roof height of 1250 ft, and with its antenna spire included, it stands a total of 1454 ft high. Its name is derived from the nickname for New York, the Empire State. It stood as the world's tallest building for nearly 40 years, from its completion in early 1931 until the topping out of the original World Trade Center's North Tower in late 1970. Following the September 11 attacks in 2001, the Empire State Building was again the tallest building in New York (although it was no longer the tallest in the US or the world), until One World Trade Center reached a greater height on April 30, 2012. The Empire State Building is currently the fourth-tallest completed skyscraper in the United States (after the One World Trade Center, the Willis Tower and Trump International Hotel and Tower, both in Chicago), and the 25th-tallest in the world (the tallest now is Burj Khalifa, located in Dubai). It is also the fifth-tallest freestanding structure in the Americas.
The Empire State Building is generally thought of as an American cultural icon. It is designed in the distinctive Art Deco style and has been named as one of the Seven Wonders of the Modern World by the American Society of Civil Engineers. The building and its street floor interior are designated landmarks of the New York City Landmarks Preservation Commission, and confirmed by the New York City Board of Estimate. It was designated as a National Historic Landmark in 1986. In 2007, it was ranked number one on the AIA's List of America's Favorite Architecture.
The building is owned by the Empire State Realty Trust, of which Anthony Malkin serves as Chairman, CEO and President. In 2010, the Empire State Building underwent a $550 million renovation, with $120 million spent to transform the building into a more energy efficient and eco-friendly structure. The Empire State Building is the tallest Leadership in Energy and Environmental Design (LEED)-certified building in the United States, having received a gold LEED rating in September 2011.
History.
The site of the Empire State Building was first developed as the John Thompson Farm in the late 18th century. At the time, a stream ran across the site, emptying into Sunfish Pond, located a block away. Beginning in the late 19th century, the block was occupied by the Waldorf-Astoria Hotel, frequented by The Four Hundred, the social elite of New York.
The limestone for the Empire State Building came from the Empire Mill in Sanders, Indiana which is an unincorporated town adjacent to Bloomington, Indiana. The Empire Mill Land office is near State Road 37 and Old State Road 37 just south of Bloomington. Bloomington, Bedford and Oolitic area are known locally as the limestone capital of the world.
Design and construction.
The Empire State Building was designed by William F. Lamb from the architectural firm Shreve, Lamb and Harmon, which produced the building drawings in just two weeks, using its earlier designs for the Reynolds Building in Winston-Salem, North Carolina, and the Carew Tower in Cincinnati, Ohio (designed by the architectural firm W. W. Ahlschlager & Associates) as a basis. Every year the staff of the Empire State Building sends a Father's Day card to the staff at the Reynolds Building in Winston-Salem to pay homage to its role as predecessor to the Empire State Building. The building was designed from the top down. The general contractors were The Starrett Brothers and Eken, and the project was financed primarily by John J. Raskob and Pierre S. du Pont. The construction company was chaired by Alfred E. Smith, a former Governor of New York and James Farley's General Builders Supply Corporation supplied the building materials. John W. Bowser was project construction superintendent.
Excavation of the site began on January 22, 1930, and construction on the building itself started symbolically on March 17—St. Patrick's Day—per Al Smith's influence as Empire State, Inc. president. The project involved 3,400 workers, mostly immigrants from Europe, along with hundreds of Mohawk iron workers, many from the Kahnawake reserve near Montreal. According to official accounts, five workers died during the construction. Governor Smith's grandchildren cut the ribbon on May 1, 1931. Lewis Wickes Hine's photography of the construction provides not only invaluable documentation of the construction, but also a glimpse into common day life of workers in that era.
The construction was part of an intense competition in New York for the title of "world's tallest building". Two other projects fighting for the title, 40 Wall Street and the Chrysler Building, were still under construction when work began on the Empire State Building. Each held the title for less than a year, as the Empire State Building surpassed them upon its completion, on April 11, 1931, 12 days ahead of schedule, just 410 days after construction commenced. The building was officially opened on May 1, 1931 in dramatic fashion, when United States President Herbert Hoover turned on the building's lights with the push of a button from Washington, D.C. Coincidentally, the first use of tower lights atop the Empire State Building, the following year, was for the purpose of signaling the victory of Franklin D. Roosevelt over Hoover in the presidential election of November 1932.
Opening.
The building's opening coincided with the Great Depression in the United States, and as a result much of its office space was initially unrented. The building's vacancy was exacerbated by its poor location on 34th Street, which placed it relatively far from public transportation, as Grand Central Terminal and Penn Station, built decades beforehand, are several blocks away, as is the more recently built Port Authority Bus Terminal. Other more successful skyscrapers, such as the Chrysler Building, did not have this problem. In its first year of operation, the observation deck took in approximately 2 million dollars, as much money as its owners made in rent that year. The lack of renters led New Yorkers to deride the building as the "Empty State Building". The building would not become profitable until 1950. The famous 1951 sale of the Empire State Building to Roger L. Stevens and his business partners was brokered by the prominent upper Manhattan real-estate firm Charles F. Noyes & Company for a record $51 million. At the time, that was the highest price paid for a single structure in real-estate history.
Incidents.
1945 plane crash.
At 9:40 am on Saturday, July 28, 1945, a B-25 Mitchell bomber, piloted in thick fog by Lieutenant Colonel William Franklin Smith, Jr., crashed into the north side of the Empire State Building, between the 79th and 80th floors, where the offices of the National Catholic Welfare Council were located. One engine shot through the side opposite the impact and flew as far as the next block, where it landed on the roof of a nearby building, starting a fire that destroyed a penthouse. The other engine and part of the landing gear plummeted down an elevator shaft. The resulting fire was extinguished in 40 minutes. Fourteen people were killed in the accident. Elevator operator Betty Lou Oliver survived a plunge of 75 stories inside an elevator, which still stands as the Guinness World Record for the longest survived elevator fall recorded. Despite the damage and loss of life, the building was open for business on many floors on the following Monday. The crash helped spur the passage of the long-pending Federal Tort Claims Act of 1946, as well as the insertion of retroactive provisions into the law, allowing people to sue the government for the accident.
A year later, another aircraft narrowly missed striking the building.
Suicide attempts.
Over the years, more than thirty people have attempted suicide, most successfully, by jumping from the upper parts of the building. The first suicide occurred even before its completion, by a worker who had been laid off. The fence around the observatory terrace was put up in 1947 after five people tried to jump during a three-week span.
On May 1, 1947, 23-year-old Evelyn McHale leapt to her death from the 86th floor observation deck and landed on a limousine parked at the curb. Photography student Robert Wiles took a photo of McHale's oddly intact corpse a few minutes after her death. The police found a suicide note among possessions she left on the observation deck: "He is much better off without me ... I wouldn’t make a good wife for anybody". The photo ran in the edition of May 12, 1947 of "Life" magazine, and is often referred to as "The Most Beautiful Suicide". It was later used by visual artist Andy Warhol in one of his prints entitled "Suicide (Fallen Body)".
In December 1943, ex-United States Navy gunner's mate William Lloyd Rambo jumped to his death, landing amidst Christmas shoppers on the street below.
Only one person has jumped from the upper observatory: on November 3, 1932, Frederick Eckert, of Astoria, ran past a guard in the enclosed 102nd floor gallery and jumped a gate leading to an outdoor catwalk intended for dirigible passengers. Eckert's body landed on the roof of the 86th floor observation promenade.
Two people have survived jumps, in both cases by not managing to fall more than a floor: On December 2, 1979, Elvita Adams jumped from the 86th floor, only to be blown back onto a ledge on the 85th floor by a gust of wind and left with a broken hip. On April 25, 2013, a man, who is presumed to have jumped, fell from the 86th floor observation deck but landed alive on an 85th floor ledge – where security guards managed to bring him inside; he suffered only minor injuries.
Shootings.
Two major shooting incidents have occurred at or in front of the Empire State Building.
On February 23, 1997, at about 5 p.m. EST, a gunman shot seven people on the 86th floor observation deck. Abu Kamal, a 69-year-old Palestinian teacher, killed one person and wounded six others, supposedly in response to events happening in Palestine and Israel, before committing suicide.
On August 24, 2012 at about 9 a.m. EDT, on the sidewalk at the Fifth Avenue side of the building, a gunman shot and killed a former co-worker from a workplace that had laid him off in 2011. When two police officers confronted the gunman, 58-year-old Jeffrey T. Johnson, he aimed his firearm at them. They responded by firing 16 shots at Johnson, killing him but also wounding nine bystanders, most of whom were hit by fragments, although three took direct hits from bullets.
Architecture.
Interior.
The Empire State Building rises to 1250 ft at the 102nd floor, and including the 203 ft pinnacle, its full height reaches 1453 ft. The building has 85 stories of commercial and office space representing 2158000 sqft. It has an indoor and outdoor observation deck on the 86th floor. The remaining 16 stories represent the Art Deco tower, which is capped by a 102nd-floor observatory. Atop the tower is the 203 ft pinnacle, much of which is covered by broadcast antennas, with a lightning rod at the very top.
The Empire State Building was the first building to have more than 100 floors. It has 6,500 windows and 73 elevators, and there are 1,860 steps from street level to the 102nd floor. It has a total floor area of 2768591 sqft; the base of the Empire State Building is about 2 acre. The building houses 1,000 businesses and has its own ZIP code, 10118. As of 2007, approximately 21,000 employees work in the building each day, making the Empire State Building the second-largest single office complex in America, after the Pentagon. The building was completed in one year and 45 days. Its original 64 elevators are located in a central core; today, the Empire State Building has 73 elevators in all, including service elevators. It takes less than one minute by elevator to get to the 80th floor, which contains a gift shop and an exhibit detailing the building's construction. From there, visitors can take another elevator or climb the stairs to the 86th floor, where an outdoor observation deck is located. The building has 70 mi of pipe, 2500000 ft of electrical wire, and about 9,000 faucets. It is heated by low-pressure steam; despite its height, the building only requires between 2 and of steam pressure for heating. It weighs approximately 370000 ST. The exterior of the building was built using Indiana limestone panels.
The Empire State Building cost $40,948,900 to build (equivalent to $<br>{Inflation} - Amount must not have "" prefix: 40,948,900.   in 2015). Long-term forecasting of the life cycle of the structure was implemented at the design phase to ensure that the building's future intended uses were not restricted by the requirements of previous generations. This is particularly evident in the over-design of the building's electrical system.
Unlike most of today's skyscrapers, the Empire State Building features an art deco design, typical of pre–World War II architecture in New York. The modernistic stainless steel canopies of the entrances on 33rd and 34th Streets lead to two story-high corridors around the elevator core, crossed by stainless steel and glass-enclosed bridges at the second-floor level. The elevator core contains 67 elevators.
The lobby is three stories high and features an aluminum relief of the skyscraper without the antenna, which was not added to the spire until 1952. The north corridor contained eight illuminated panels, created by Roy Sparkia and Renée Nemorov in 1963 in time for the 1964 World's Fair, which depicts the building as the Eighth Wonder of the World, alongside the traditional seven. These panels were eventually moved near a ticketing line for the observation deck.
Up until the 1960s, the ceilings in the lobby had a shiny art deco mural inspired by both the sky and the Machine Age, until it was covered with ceiling tiles and fluorescent lighting. Because the original murals, designed by an artist named Leif Neandross, were damaged, reproductions were installed. Over 50 artists and workers used 15,000 square feet of aluminum and 1,300 square feet of 23-carat gold leaf to re-create the mural. Renovations to the lobby alluded to original plans for the building; replacing the clock over the information desk in the Fifth Avenue lobby with an anemometer, as well as installing two chandeliers originally intended to be part of the building when it first opened. In 2000, the building's owners installed a series of paintings by the New York artist Kysa Johnson in the concourse level. In January 2014 the artist filed suit in federal court in New York under the Visual Artists Rights Act, alleging the negligent destruction of the paintings and damage to her reputation as an artist.
The building's lobbies and common areas received a $550 million renovation in 2009, which included new air conditioning, waterproofing, and renovating the observation deck; moving the gift shop to the 80th floor. Of this, $120 million was spent in an effort to transform the building into a more energy efficient and eco-friendly structure. For example, the 6,500 windows were remanufactured onsite into superwindows which block heat but pass light. Air conditioning operating costs on hot days were reduced and this saved $17 million of the project's capital cost immediately, partly funding other retrofitting. Receiving a gold Leadership in Energy and Environmental Design (LEED) rating in September 2011, the Empire State Building is the tallest LEED certified building in the United States.
Features.
Above the 102nd floor.
On the 102nd floor of the Empire State Building there is a door with stairs ascending up, which leads into the 103rd floor. This was originally built as a disembarkation floor for airships tethered to the building's spire, and features a circular balcony outside the room as well. It is now a hot spot for when celebrities visit, and an access point to reach the spire for maintenance purposes. The room currently contains electrical equipment, though this was edited out, by camera angle, during the "In the Wind" season-four finale of "White Collar". Above the 103rd floor, there is a set of stairs and a ladder to reach the spire for maintenance work only.
The building's distinctive Art Deco spire was originally designed to be a mooring mast and depot for dirigibles. The 103rd floor was originally a landing platform with a dirigible gangplank. A particular elevator, traveling between the 86th and 102nd floors, was supposed to transport passengers after they checked in at the observation deck on the 86th floor. However, the idea proved to be impractical and dangerous after a few attempts with airships, due to the powerful updrafts caused by the size of the building itself, as well as the lack of mooring lines tying the other end of the craft to the ground. A large broadcast tower was added to the top of the spire in the early 1950s, in order to support the transmission antennas of several television and FM stations. Up to that point, NBC had the exclusive rights to the site, and – beginning in 1931 – built various, smaller antenna structures dedicated to their television transmissions.
Broadcast stations.
New York City is the largest media market in the United States. Since the September 11 attacks, nearly all of the city's commercial broadcast stations (both television and FM radio) have transmitted from the top of the Empire State Building, although a few FM stations are located at the nearby Condé Nast Building. Most New York City AM stations broadcast from sites across the Hudson River in New Jersey or from other surrounding areas.
Broadcasting began at the Empire State Building on December 22, 1931, when RCA began transmitting experimental television broadcasts from a small antenna erected atop the spire. They leased the 85th floor and built a laboratory there, and—in 1934—RCA was joined by Edwin Howard Armstrong in a cooperative venture to test his FM system from the building's antenna. When Armstrong and RCA fell out in 1935 and his FM equipment was removed, the 85th floor became the home of RCA's New York television operations, first as experimental station W2XBS channel 1, which eventually became (on July 1, 1941) commercial station WNBT, channel 1 (now WNBC-TV channel 4). NBC's FM station (WEAF-FM, now WQHT) began transmitting from the antenna in 1940. NBC retained exclusive use of the top of the building until 1950, when the FCC ordered the exclusive deal broken, based on consumer complaints that a common location was necessary for the (now) seven New York-area television stations (five licensed to New York City, NY, one licensed to Newark, NJ, and one licensed to Secaucus, NJ) to transmit from so that receiving antennas would not have to be constantly adjusted. Construction on a giant tower began. Other television broadcasters then joined RCA at the building, on the 83rd, 82nd, and 81st floors, frequently bringing sister FM stations along for the ride. Multiple transmissions of TV and FM began from the new tower in 1951. In 1965, a separate set of FM antennas was constructed ringing the 103rd floor observation area. When the World Trade Center was being constructed, it caused serious reception problems for the television stations, most of which then moved to the World Trade Center as soon as it was completed. This made it possible to renovate the antenna structure and the transmitter facilities for the benefit of the FM stations remaining there, which were soon joined by other FMs and UHF TVs moving in from elsewhere in the metropolitan area. The destruction of the World Trade Center necessitated a great deal of shuffling of antennas and transmitter rooms to accommodate the stations moving back uptown.
As of 2012, the Empire State Building is home to the following stations:
Observation decks.
The Empire State Building has one of the most popular outdoor observatories in the world, having been visited by over 110 million people. The 86th-floor observation deck offers impressive 360-degree views of the city. There is a second observation deck on the 102nd floor that is open to the public. It was closed in 1999, but reopened in November 2005. It is completely enclosed and much smaller than the first one; it may be closed on high-traffic days. Tourists may pay to visit the observation deck on the 86th floor and an additional amount for the 102nd floor. The lines to enter the observation decks, according to Concierge.com, are "as legendary as the building itself:" there are five of them: the sidewalk line, the lobby elevator line, the ticket purchase line, the second elevator line, and the line to get off the elevator and onto the observation deck. For an extra fee tourists can skip to the front of the line. The Empire State Building makes more money from tickets sales for its observation decks than it does from renting office space.
The skyscraper's observation deck plays host to several cinematic, television, and literary classics including, "An Affair To Remember", "On the Town", "Love Affair" and "Sleepless in Seattle". In the Latin American literary work "Empire of Dreams" by Giannina Braschi the observation deck is the site of a pastoral revolution; shepherds take over the City of New York. The deck was also the site of a publicity-stunt Martian invasion in an episode of "I Love Lucy" ("Lucy Is Envious", season 3, episode 25).
A panoramic view of New York City from the 86th-floor observation deck of the Empire State Building, spring 2005
New York Skyride.
The Empire State Building also has a motion simulator attraction located on the 2nd floor. Opened in 1994 as a complement to the observation deck, the New York Sky ride (or NY Sky ride) is a simulated aerial tour over the city. The cinematic presentation lasts approximately 25 minutes. As of May 2013, tickets are Adults $57, Children $42, Seniors $49.
Since its opening, the ride has gone through two incarnations. The original version, which ran from 1994 until around 2002, featured James Doohan, "" Scotty, as the airplane's pilot, who humorously tried to keep the flight under control during a storm, with the tour taking an unexpected route through the subway, Coney Island, and FAO Schwartz, among other places. After the September 11 attacks in 2001, however, the ride was closed, and an updated version debuted in mid-2002 with actor Kevin Bacon as the pilot. The new version of the narration attempted to make the attraction more educational, and included some minor post-9/11 patriotic undertones with retrospective footage of the World Trade Center. The new flight also goes haywire, but this segment is much shorter than in the original.
Lights.
In 1964, floodlights were added to illuminate the top of the building at night. Since 1976 the spire has been lit in colors chosen to match seasonal and other events, such as St. Patrick's Day, Christmas, Independence Day and Bastille Day. After the eightieth birthday and subsequent death of Frank Sinatra, for example, the building was bathed in blue light to represent the singer's nickname "Ol' Blue Eyes". After the death of actress Fay Wray ("King Kong") in late 2004, the building stood in complete darkness for 15 minutes.
The floodlights bathed the building in red, white, and blue for several months after the destruction of the World Trade Center, then reverted to the standard schedule. On June 4, 2002, the Empire State Building donned purple and gold (the royal colors of Elizabeth II), in thanks for the United Kingdom playing the Star Spangled Banner during the Changing of the Guard at Buckingham Palace on September 12, 2001 (a show of support after the September 11 attacks). This would also be shown after the Westminster Dog Show. Traditionally, in addition to the standard schedule, the building will be lit in the colors of New York's sports teams on the nights they have home games (orange, blue and white for the New York Knicks, red, white and blue for the New York Rangers, and so on). The first weekend in June finds the building bathed in green light for the Belmont Stakes held in nearby Belmont Park. The building is illuminated in tennis-ball yellow during the US Open tennis tournament in late August and early September. It was twice lit in scarlet to support nearby Rutgers University: once for a football game against the University of Louisville on November 9, 2006, and again on April 3, 2007 when the women's basketball team played in the national championship game. From June 1 to 3, 2012, the building was lit in blue and white, the colors of the Israeli flag, in honor of the 49th annual Celebrate Israel Parade.
During 2012, the building's metal halide lamps and floodlights were replaced with LED fixtures, increasing the available colors from nine to over 16 million. The computer-controlled system allows the building to be illuminated in ways that were unable to be done previously with plastic gels. For instance, on November 6, 2012, CNN used the top of the Empire State Building as a scoreboard for the 2012 United States presidential election. When incumbent president Barack Obama had reached the 270 electoral votes necessary to win re-election, the lights turned blue. Had Republican challenger Mitt Romney won, the building would have been lit red. Also, on November 26, 2012, the building had its first ever synchronized light show, using music from recording artist Alicia Keys. Those wishing to hear the music could tune to certain radio stations in the New York area. A video of the performance was posted online the next day. In 2013 the lights were changed to "Financial Times" pink. In the run-up week to Super Bowl XLIII held at MetLife Stadium on February 2, 2014, the building was lit in a contest sponsored by the National Football League's wireless partner, Verizon Wireless to determine both the winner and fan support for the two teams via their team colors in the game through the #WhosGonnaWin Twitter hashtag, either the "action green" and navy blue of the Seattle Seahawks or orange and blue of the Denver Broncos, along with a light show during the game's halftime.
Height records and comparisons.
The Empire State Building remained the tallest man-made structure in the world for 23 years before it was surpassed by the Griffin Television Tower Oklahoma (KWTV Mast) in 1954. It was also the tallest free-standing structure in the world for 36 years before it was surpassed by the Ostankino Tower in 1967.
The longest world record held by the Empire State Building was for the tallest skyscraper (to structural height), which it held for 42 years until it was surpassed by the North Tower of the World Trade Center in 1972. An early-1970s proposal to dismantle the spire and replace it with an additional 11 floors, which would have brought the building's height to 1,494 feet (455 m) and made it once again the world's tallest at the time, was considered but ultimately rejected.
With the destruction of the World Trade Center in the September 11 attacks, the Empire State Building again became the tallest building in New York City, and the second-tallest building in the Americas, surpassed only by the Willis Tower in Chicago. It is currently the fifth-tallest, surpassed by the Willis Tower, the Trump International Hotel and Tower (Chicago), 432 Park Avenue and the new One World Trade Center. One World Trade Center surpassed the roof height of the Empire State Building on April 30, 2012, and became the tallest building in New York City—on the way toward becoming the tallest building in the Americas at a planned 1,776 feet (541 m).
When measured by pinnacle height, the Empire State Building is the fourth-tallest building in the USA, surpassed by One World Trade Center, Willis Tower and Chicago's John Hancock Center. On clear days, the building can be seen from much of the New York Metropolitan Area, and as far away as New Haven, Connecticut and Morristown, New Jersey.
Neighboring Midtown Manhattan landmarks.
The Empire State Building anchors an area of Midtown which features other major Manhattan landmarks as well, including Macy's Herald Square, Koreatown, Penn Station, Madison Square Garden, and the Flower District. Together, these sites contribute to a significant volume of commuter and tourist pedestrian traffic traversing the southern portion of Midtown Manhattan.
References.
Notes
Citations
Bibliography
</dl>

</doc>
<doc id="9737" url="http://en.wikipedia.org/wiki?curid=9737" title="Eugenics">
Eugenics

Eugenics (; from Greek εὐγενής "eugenes" "well-born" from εὖ "eu", "good, well" and γένος "genos", "race, stock, kin") is the belief and practice which aims at improving the genetic quality of the human population. It is a social philosophy advocating the improvement of human genetic traits through the promotion of higher reproduction of people with desired traits (positive eugenics), or reduced reproduction and or sterilization of people with less-desired or undesired traits (negative eugenics), or both.
While eugenic principles have been practiced as far back in world history as Ancient Greece, the modern history of eugenics began in the early 20th century when a popular eugenics movement emerged in Britain and spread to many countries, including the United States and most European countries. In this period eugenic ideas were espoused across the political spectrum. Consequently, many countries adopted eugenic policies meant to improve the genetic stock of their countries. Such programs often included both "positive" measures, such as encouraging individuals deemed particularly "fit" to reproduce, and "negative" measures such as marriage prohibitions and forced sterilization of people deemed unfit for reproduction. People deemed unfit to reproduce often included people with mental or physical disabilities, people who scored in the low ranges of different IQ tests, criminals and deviants, and members of disfavored minority groups. The eugenics movement reached a climax in Nazi Germany where a state policy of racial hygiene based on eugenic principles led to the Holocaust and the murder by the German state of at least 10 million people. In the decades following World War II, with the institution of human rights, many countries gradually abandoned eugenics policies, although some Western countries, among them Sweden and the US, continued to carry out forced sterilizations for several decades. 
The main critique towards eugenics policies is that regardless of whether "negative" or "positive" policies are used, they are vulnerable to political abuse because the criteria of selection are determined by whichever group is in power. Furthermore, negative eugenics in particular is considered by many to be a violation of basic human rights, which include the right to reproduction.
History.
The idea of eugenics to produce better human beings has existed at least since Plato suggested selective mating to produce a guardian class. The idea of eugenics to decrease the birth of inferior human beings has existed at least since William Goodell (1829-1894) advocated the castration and spaying of the insane.
However, the term "eugenics" to describe the modern concept of improving the quality of human beings born into the world was originally developed by Francis Galton. Galton had read his half-cousin Charles Darwin's theory of evolution, which sought to explain the development of plant and animal species, and desired to apply it to humans. Galton believed that desirable traits were hereditary based on biographical studies. In 1883, one year after Darwin's death, Galton gave his research a name: "eugenics". Throughout its recent history, eugenics has remained a controversial concept.
Eugenics became an academic discipline at many colleges and universities, and received funding from many sources. Three International Eugenics Conferences presented a global venue for eugenists with meetings in 1912 in London, and in 1921 and 1932 in New York. Eugenic policies were first implemented in the early 1900s in the United States. It has roots in France, Germany, Great Britain, and the United States. Later, in the 1920s and 30s, the eugenic policy of sterilizing certain mental patients was implemented in other countries, including Belgium, Brazil, Canada, Japan, and Sweden.
The scientific reputation of eugenics started to decline in the 1930s, a time when Ernst Rüdin used eugenics as a justification for the racial policies of Nazi Germany. Nevertheless, in Sweden the eugenics program continued until 1975. In addition to being practised in a number of countries, eugenics was internationally organized through the International Federation of Eugenics Organizations. Its scientific aspects were carried on through research bodies such as the Kaiser Wilhelm Institute of Anthropology, Human Heredity, and Eugenics, the Cold Spring Harbour Carnegie Institution for Experimental Evolution, and the Eugenics Record Office. Its political aspects involved advocating laws allowing the pursuit of eugenic objectives, such as sterilization laws. Its moral aspects included rejection of the doctrine that all human beings are born equal, and redefining morality purely in terms of genetic fitness. Its racist elements included pursuit of a pure "Nordic race" or "Aryan" genetic pool and the eventual elimination of "less fit" races.
As a social movement, eugenics reached its greatest popularity in the early decades of the 20th century. At this point in time, eugenics was practiced around the world and was promoted by governments and influential individuals and institutions. Many countries enacted various eugenics policies and programmes, including: genetic screening, birth control, promoting differential birth rates, marriage restrictions, segregation (both racial segregation and segregation of the mentally ill from the rest of the population), compulsory sterilization, forced abortions or forced pregnancies, and genocide. Most of these policies were later regarded as coercive or restrictive, and now few jurisdictions implement policies that are explicitly labelled as eugenic or unequivocally eugenic in substance. The methods of implementing eugenics varied by country; however, some early 20th century methods involved identifying and classifying individuals and their families, including the poor, mentally ill, blind, deaf, developmentally disabled, promiscuous women, homosexuals, and racial groups (such as the Roma and Jews in Nazi Germany) as "degenerate" or "unfit", the segregation or institutionalization of such individuals and groups, their sterilization, euthanasia, and their mass murder. The practice of euthanasia was carried out on hospital patients in the Aktion T4 centers such as Hartheim Castle.
By the end of World War II, many of the discriminatory eugenics laws were largely abandoned, having become associated with Nazi Germany. After World War II, the practice of "imposing measures intended to prevent births within [a population] group" fell within the definition of the new international crime of genocide, set out in the Convention on the Prevention and Punishment of the Crime of Genocide. The Charter of Fundamental Rights of the European Union also proclaims "the prohibition of eugenic practices, in particular those aiming at selection of persons". In spite of the decline in discriminatory eugenics laws, government practices of compulsive sterilization continued into the 21st century. During the ten years President Alberto Fujimori led Peru from 1990 to 2000, allegedly 2,000 persons were involuntarily sterilized. China maintains its forcible one-child policy as well as a suite of other eugenics based legislation in order to reduce population size and manage fertility rates of different populations. In 2007 the United Nations reported forcible sterilisations and hysterectomies in Uzbekistan. During the years 2005–06 to 2012–13, nearly one-third of the 144 California prison inmates who were sterilized did not give lawful consent to the operation.
Developments in genetic, genomic, and reproductive technologies at the end of the 20th century are raising numerous questions regarding the ethical status of eugenics, effectively creating a resurgence of interest in the subject.
Some, such as UC Berkeley sociologist Troy Duster, claim that modern genetics is a back door to eugenics. This view is shared by White House Assistant Director for Forensic Sciences, Tania Simoncelli, who stated in a 2003 publication by the Population and Development Program at Hampshire College that advances in pre-implantation genetic diagnosis (PGD) are moving society to a "new era of eugenics", and that, unlike the Nazi eugenics, modern eugenics is consumer driven and market based, "where children are increasingly regarded as made-to-order consumer products." In a 2006 newspaper article, Richard Dawkins said that discussion regarding eugenics was inhibited by the shadow of Nazi misuse, to the extent that some scientists would not admit that breeding humans for certain abilities is at all possible. He believes that it is not physically different from breeding domestic animals for traits such as speed or herding skill. Dawkins felt that enough time had elapsed to at least ask just what the ethical differences were between breeding for ability versus training athletes or forcing children to take music lessons, though he could think of persuasive reasons to draw the distinction.
Some, such as Nathaniel C. Comfort from Johns Hopkins University, claim that the change from state-led reproductive-genetic decision-making to individual choice has moderated the worst abuses of eugenics by transferring the decision-making from the state to the patient and their family. Comfort suggests that "[t]he eugenic impulse drives us to eliminate disease, live longer and healthier, with greater intelligence, and a better adjustment to the conditions of society; and the health benefits, the intellectual thrill and the profits of genetic bio-medicine are too great for us to do otherwise." Others, such as bioethicist Stephen Wilkinson of Keele University and Honorary Research Fellow Eve Garrard at the University of Manchester, claim that some aspects of modern genetics can be classified as eugenics, but that this classification does not inherently make modern genetics immoral. In a co-authored publication by Keele University, they stated that "[e]ugenics doesn't seem always to be immoral, and so the fact that PGD, and other forms of selective reproduction, might sometimes technically be eugenic, isn't sufficient to show that they're wrong."
Meanings and types.
The term eugenics and its modern field of study were first formulated by Francis Galton in 1883, drawing on the recent work of his half-cousin Charles Darwin. Galton published his observations and conclusions in his book "Inquiries into Human Faculty and Its Development".
The origins of the concept began with certain interpretations of Mendelian inheritance, and the theories of August Weismann. The word "eugenics" is derived from the Greek word "eu" ("good" or "well") and the suffix "-genēs" ("born"), and was coined by Galton in 1883 to replace the word "stirpiculture", which he had used previously but which had come to be mocked due to its perceived sexual overtones. Galton defined eugenics as "the study of all agencies under human control which can improve or impair the racial quality of future generations". Galton did not understand the mechanism of inheritance.
Eugenics has, from the very beginning, meant many different things. Historically, the term has referred to everything from prenatal care for mothers to forced sterilization and euthanasia. To population geneticists, the term has included the avoidance of inbreeding without altering allele frequencies; for example, J. B. S. Haldane wrote that "the motor bus, by breaking up inbred village communities, was a powerful eugenic agent." Debate as to what exactly counts as eugenics has continued to the present day.
Edwin Black, journalist and author of "War Against the Weak", claims eugenics is often deemed a pseudoscience because what is defined as a genetic improvement of a desired trait is often deemed a cultural choice rather than a matter that can be determined through objective scientific inquiry. The most disputed aspect of eugenics has been the definition of "improvement" of the human gene pool, such as what is a beneficial characteristic and what is a defect. This aspect of eugenics has historically been tainted with scientific racism.
Early eugenists were mostly concerned with perceived intelligence factors that often correlated strongly with social class. Some of these early eugenists include Karl Pearson and Walter Weldon, who worked on this at the University College London.
Eugenics also had a place in medicine. In his lecture "Darwinism, Medical Progress and Eugenics", Karl Pearson said that everything concerning eugenics fell into the field of medicine. He basically placed the two words as equivalents. He was supported in part by the fact that Francis Galton, the father of eugenics, also had medical training.
Eugenic policies have been conceptually divided into two categories. Positive eugenics is aimed at encouraging reproduction among the genetically advantaged; for example, the reproduction of the intelligent, the healthy, and the successful. Possible approaches include financial and political stimuli, targeted demographic analyses, "in vitro" fertilization, egg transplants, and cloning. The movie Gattaca provides a fictional example of positive eugenics done voluntarily. Negative eugenics aimed to eliminate, through sterilization or segregation, those deemed physically, mentally, or morally "undesirable". This includes abortions, sterilization, and other methods of family planning. Both positive and negative eugenics can be coercive; abortion for fit women, for example, was illegal in Nazi Germany.
Jon Entine claims that eugenics simply means "good genes" and using it as synonym for genocide is an "all-too-common distortion of the social history of genetics policy in the United States." According to Entine, eugenics developed out of the Progressive Era and not "Hitler's twisted Final Solution."
Implementation methods.
According to Richard Lynn, eugenics may be divided into two main categories based on the ways in which the methods of eugenics can be applied.
Arguments.
Doubts on traits triggered by inheritance.
The first major challenge to conventional eugenics based upon genetic inheritance was made in 1915 by Thomas Hunt Morgan, who demonstrated the event of genetic mutation occurring outside of inheritance involving the discovery of the hatching of a fruit fly ("Drosophila melanogaster") with white eyes from a family of red-eyes. Morgan claimed that this demonstrated that major genetic changes occurred outside of inheritance and that the concept of eugenics based upon genetic inheritance was not completely scientifically accurate. Additionally, Morgan criticized the view that subjective traits, such as intelligence and criminality, were caused by heredity because he believed that the definitions of these traits varied and that accurate work in genetics could only be done when the traits being studied were accurately defined. In spite of Morgan's public rejection of eugenics, much of his genetic research was absorbed by eugenics.
Ethics.
A common criticism of eugenics is that "it inevitably leads to measures that are unethical". Historically, this statement is evidenced by the obvious control of one group imposing its agenda on minority groups. This includes programs in England, Germany, and America targeting various groups, including Jews, homosexuals, Muslims, Romani, the homeless, and those with intellectual disabilities.
Many of the ethical concerns from eugenics arise from the controversial past, prompting a discussion on what place, if any, it should have in the future. Advances in science have changed eugenics. In the past, eugenics has had more to do with sterilization and enforced reproduction laws (i.e. no inter-racial marriage and marriage restrictions based on land ownership). Now, in the age of a progressively mapped genome, embryos can be tested for susceptibility to disease, gender, and genetic defects, and alternatives to reproduction are becoming increasingly common, such as in vitro fertilization. In short, eugenics is no longer ex post facto regulation of the living but instead preemptive action on the unborn.
With this change, however, there are ethical concerns which lack adequate attention, and which must be addressed before eugenic policies can be properly implemented in the future. Sterilized individuals, for example, could volunteer for the procedure, albeit under incentive or duress, or at least voice their opinion. The unborn fetus on which these new eugenic procedures are performed cannot speak out, as the fetus lacks the voice to consent or to express his or her opinion. The ability to manipulate a fetus and determine who the child will be is something questioned by many of the opponents or, and even proponents for, eugenic policies.
Societal and political consequences of eugenics call for a place in the discussion on the ethics behind the eugenics movement. Public policy often focuses on issues related to race and gender, both of which could be controlled by manipulation of embryonic genes; eugenics and political issues are interconnected and the political aspect of eugenics must be addressed. Laws controlling the subjects, the methods, and the extent of eugenics will need to be considered in order to prevent the repetition of the unethical events of the past.
Most of the ethical concerns about eugenics involve issues of morality and power. Decisions about the morality and the control of this new science (and the subsequent results of the science) will need to be made as eugenics continue to influence the development of the science and medical fields.
Losing genetic diversity by classifying traits as diseases.
Eugenic policies could also lead to loss of genetic diversity, in which case a culturally accepted "improvement" of the gene pool could very likely—as evidenced in numerous instances in isolated island populations (e.g., the dodo, "Raphus cucullatus", of Mauritius)—result in extinction due to increased vulnerability to disease, reduced ability to adapt to environmental change, and other factors both known and unknown. A long-term species-wide eugenics plan might lead to a scenario similar to this because the elimination of traits deemed undesirable would reduce genetic diversity by definition.
Edward M. Miller claims that, in any one generation, any realistic program should make only minor changes in a fraction of the gene pool, giving plenty of time to reverse direction if unintended consequences emerge, reducing the likelihood of the elimination of desirable genes. Miller also argues that any appreciable reduction in diversity is so far in the future that little concern is needed for now.
While the science of genetics has increasingly provided means by which certain characteristics and conditions can be identified and understood, given the complexity of human genetics, culture, and psychology there is at this point no agreed objective means of determining which traits might be ultimately desirable or undesirable. Some diseases such as sickle-cell disease and cystic fibrosis respectively confer immunity to malaria and resistance to cholera when a single copy of the recessive allele is contained within the genotype of the individual. Reducing the instance of sickle-cell disease in Africa where malaria is a common and deadly disease could indeed have extremely negative net consequences.
However, some genetic diseases such as haemochromatosis can increase susceptibility to illness, cause physical deformities, and other dysfunctions, which provides some incentive for people to re-consider some elements of eugenics.
Autistic people have advocated a shift in perception of autism spectrum disorders as complex syndromes rather than diseases that must be cured. Proponents of this view reject the notion that there is an "ideal" brain configuration and that any deviation from the norm is pathological; they promote tolerance for what they call neurodiversity. Baron-Cohen argues that the genes for Asperger's combination of abilities have operated throughout recent human evolution and have made remarkable contributions to human history. The possible reduction of autism rates through selection against the genetic predisposition to autism is a significant political issue in the autism rights movement, which claims that autism is a form of neurodiversity.
Heterozygous recessive traits.
The heterozygote test is used for the early detection of recessive hereditary diseases, allowing for couples to determine if they are at risk of passing genetic defects to a future child. The goal of the test is to estimate the likelihood of passing the hereditary disease to future descendants.
Recessive traits can be severely reduced, but never eliminated unless the complete genetic makeup of all members of the pool was known, as aforementioned. As only very few undesirable traits, such as Huntington's disease, are dominant, it could be argued from certain perspectives that the practicality of "eliminating" traits is quite low.
There are examples of eugenic acts that managed to lower the prevalence of recessive diseases, although not influencing the prevalence of heterozygote carriers of those diseases. The elevated prevalence of certain genetically transmitted diseases among the Ashkenazi Jewish population (Tay–Sachs, cystic fibrosis, Canavan's disease, and Gaucher's disease), has been decreased in current populations by the application of genetic screening.
Pleiotropic genes.
Pleiotropy occurs when one gene influences multiple, seemingly unrelated phenotypic traits, an example being phenylketonuria, which is a human disease that affects multiple systems but is caused by one gene defect. Andrzej Pękalski, from the University of Wrocław, argues that eugenics can cause harmful loss of genetic diversity if a eugenics program selects for a pleiotropic gene that is also associated with a positive trait. Pekalski uses the example of a coercive government eugenics program that prohibits people with myopia from breeding but has the unintended consequence of also selecting against high intelligence since the two go together.
Supporters and critics.
At its peak of popularity, eugenics was supported by a wide variety of prominent people, including Winston Churchill, Margaret Sanger, Marie Stopes, H. G. Wells, Norman Haire, Havelock Ellis, Theodore Roosevelt, Herbert Hoover, George Bernard Shaw, John Maynard Keynes, John Harvey Kellogg, Robert Andrews Millikan, Linus Pauling, Sidney Webb, and W. E. B. Du Bois. Adolf Hitler praised and incorporated eugenic ideas in "Mein Kampf" and emulated eugenic legislation for the sterilization of "defectives" that had been pioneered in the United States.
The American sociologist Lester Frank Ward, the English writer G. K. Chesterton, the German-American anthropologist Franz Boas, and Scottish tuberculosis pioneer and author Halliday Sutherland were all early critics of the philosophy of eugenics. Ward's 1913 article "Eugenics, Euthenics, and Eudemics", Chesterton's 1917 book "", and Boas' 1916 article "Eugenics" (published in "The Scientific Monthly") were all harshly critical of the rapidly growing movement. Sutherland identified eugenists as a major obstacle to the eradication and cure of tuberculosis in his 1917 address "Consumption: Its Cause and Cure", and criticism of eugenists and Neo-Malthusians in his 1921 book "Birth Control" led to a writ for libel from the eugenist Marie Stopes. Several biologists were also antagonistic to the eugenics movement, including Lancelot Hogben. Other biologists such as J. B. S. Haldane and R. A. Fisher expressed skepticism that sterilization of "defectives" would lead to the disappearance of undesirable genetic traits.
Some supporters of eugenics later reversed their positions on it. For example, H. G. Wells, who had called for "the sterilization of failures" in 1904, stated in his 1940 book "The Rights of Man: Or What are we fighting for?" that among the human rights he believed should be available to all people was "a prohibition on mutilation, sterilization, torture, and any bodily punishment".
Among institutions, the Catholic Church was an early opponent of state-enforced eugenics. In his 1930 encyclical "Casti connubii", Pope Pius XI explicitly condemned eugenics laws: "Public magistrates have no direct power over the bodies of their subjects; therefore, where no crime has taken place and there is no cause present for grave punishment, they can never directly harm, or tamper with the integrity of the body, either for the reasons of eugenics or for any other reason."

</doc>
<doc id="9738" url="http://en.wikipedia.org/wiki?curid=9738" title="Email">
Email

Electronic mail, most commonly referred to as email or e-mail since c 1993, is a method of exchanging digital messages from an author to one or more recipients. Modern email operates across the Internet or other computer networks. Some early email systems required the author and the recipient to both be online at the same time, in common with instant messaging. Today's email systems are based on a store-and-forward model. Email servers accept, forward, deliver, and store messages. Neither the users nor their computers are required to be online simultaneously; they need connect only briefly, typically to a mail server, for as long as it takes to send or receive messages.
Historically, the term "electronic mail" was used generically for any electronic document transmission. For example, several writers in the early 1970s used the term to describe fax document transmission. As a result, it is difficult to find the first citation for the use of the term with the more specific meaning it has today.
An Internet email message consists of three components, the message "envelope", the message "header", and the message "body". The message header contains control information, including, minimally, an originator's email address and one or more recipient addresses. Usually descriptive information is also added, such as a subject header field and a message submission date/time stamp.
Originally a text-only (ASCII) communications medium, Internet email was extended to carry, e.g. text in other character sets, multi-media content attachments, a process standardized in RFC 2045 through 2049. Collectively, these RFCs have come to be called Multipurpose Internet Mail Extensions (MIME). have proposed standards for internationalized email addresses using UTF-8.
Electronic mail predates the inception of the Internet and was in fact a crucial tool in creating it, but the history of modern, global Internet email services reaches back to the early ARPANET. Standards for encoding email messages were proposed as early as 1973 (RFC 561). Conversion from ARPANET to the Internet in the early 1980s produced the core of the current services. An email message sent in the early 1970s looks quite similar to a basic text message sent on the Internet today.
Email is an information and communications technology. It uses technology to communicate a digital message over the Internet. Users use email differently, based on how they think about it. There are many software platforms available to send and receive. Popular email platforms include Gmail, Hotmail, Yahoo! Mail, Outlook, and many others.
Network-based email was initially exchanged on the ARPANET in extensions to the File Transfer Protocol (FTP), but is now carried by the Simple Mail Transfer Protocol (SMTP), first published as Internet standard 10 (RFC 821) in 1982. In the process of transporting email messages between systems, SMTP communicates delivery parameters using a message "envelope" separate from the message (header and body) itself.
Spelling.
Electronic mail has several English spelling options that occasionally are the cause of vehement disagreement.
Origin.
The AUTODIN network, first operational in 1962, provided a message service between 1,350 terminals, handling 30 million messages per month, with an average message length of approximately 3,000 characters. Autodin was supported by 18 large computerized switches, and was connected to the United States General Services Administration Advanced Record System, which provided similar services to roughly 2,500 terminals.
Host-based mail systems.
With the introduction of MIT's Compatible Time-Sharing System (CTSS) in 1961 multiple users were able to log into a central system from remote dial-up terminals, and to store and share files on the central disk. Informal methods of using this to pass messages were developed and expanded :
Developers of other early systems developed similar email applications:
These original messaging systems had widely different features and ran on systems that were incompatible with each other. Most of them only allowed communication between users logged into the same host or "mainframe", although there might be hundreds or thousands of users within an organization.
LAN email systems.
In the early 1980s, networked personal computers on LANs became increasingly important. Server-based systems similar to the earlier mainframe systems were developed. Again, these systems initially allowed communication only between users logged into the same server infrastructure. Examples include:
Eventually these systems too could link different organizations as long as they ran the same email system and proprietary protocol.
Email networks.
To facilitate electronic mail exchange between remote sites and with other organizations, telecommunication links, such as dialup modems or leased lines, provided means to transport email globally, creating local and global networks.
Attempts at interoperability.
Early interoperability among independent systems included:
From SNDMSG to MSG.
In the early 1970s, Ray Tomlinson updated an existing utility called SNDMSG so that it could copy messages (as files) over the network. Lawrence Roberts The project manager for the ARPANET development, took the idea of READMAIL, which dumped all "recent" messages onto the user's terminal, and wrote a programme for TENEX in TECO macros called "RD", which permitted access to individual messages. Barry Wessler then updated RD and called it "NRD".
Marty Yonke rewrote NRD to include reading, access to SNDMSG for sending, and a help system, and called the utility "WRD", which was later known as "BANANARD". John Vittal then updated this version to include three important commands: "Move" (combined save/delete command), "Answer" (determined to whom a reply should be sent) and "Forward" (sent an email to a person who was not already a recipient). The system was called "MSG". With inclusion of these features, MSG is considered to be the first integrated modern email programme, from which many other applications have descended.
ARPANET mail.
Experimental email transfers between separate computer systems began shortly after the creation of the ARPANET in 1969. Ray Tomlinson is generally credited as having sent the first email across a network, initiating the use of the "@" sign to separate the names of the user and the user's machine in 1971, when he sent a message from one Digital Equipment Corporation DEC-10 computer to another DEC-10. The two machines were placed next to each other. Tomlinson's work was quickly adopted across the ARPANET, which significantly increased the popularity of email. For many years, email was the killer app of the ARPANET and then the Internet.
As the influence of the ARPANET and later the Internet grew, gateways were developed to pass mail between it and other networks such as JANET, BITNET, X.400, FidoNet, and UUCP. This often involved addresses such as:
Despite the complex format of some such addresses, (in this case an Internet email address to route mail to a user with a "bang path" address at a UUCP host), they did lead to the universal connectivity that is the key feature of modern email.
Operation overview.
The diagram to the right shows a typical sequence of events that takes place when Alice composes a message using her mail user agent (MUA). She enters the email address of her correspondent, and hits the "send" button.
This server may need to forward the message to other MTAs before the message reaches the final message delivery agent (MDA).
That sequence of events applies to the majority of email users. However, there are many alternative possibilities and complications to the email system:
Many MTAs used to accept messages for any recipient on the Internet and do their best to deliver them. Such MTAs are called "open mail relays". This was very important in the early days of the Internet when network connections were unreliable. If an MTA couldn't reach the destination, it could at least deliver it to a relay closer to the destination. The relay stood a better chance of delivering the message at a later time. However, this mechanism proved to be exploitable by people sending unsolicited bulk email and as a consequence very few modern MTAs are open mail relays, and many MTAs don't accept messages from open mail relays because such messages are very likely to be spam.
Message format.
The Internet email message format is now defined by RFC 5322, with multi-media content attachments being defined in RFC 2045 through RFC 2049, collectively called "Multipurpose Internet Mail Extensions" or "MIME". RFC 5322 replaced the earlier RFC 2822 in 2008, and in turn RFC 2822 in 2001 replaced RFC 822 – which had been the standard for Internet email for nearly 20 years. Published in 1982, RFC 822 was based on the earlier RFC 733 for the ARPANET.
Internet email messages consist of two major sections:
The header is separated from the body by a blank line.
Message header.
Each message has exactly one header, which is structured into fields. Each field has a name and a value. RFC 5322 specifies the precise syntax.
Informally, each line of text in the header that begins with a printable character begins a separate field. The field name starts in the first character of the line and ends before the separator character ":". The separator is then followed by the field value (the "body" of the field). The value is continued onto subsequent lines if those lines have a space or tab as their first character. Field names and values are restricted to 7-bit ASCII characters. Non-ASCII values may be represented using MIME encoded words.
Header fields.
Email header fields can be multi-line, and each line should be at most 78 characters long and in no event more than 998 characters long. Header fields defined by RFC 5322 can only contain US-ASCII characters; for encoding characters in other sets, a syntax specified in RFC 2047 can be used. Recently the IETF EAI working group has defined some standards track extensions, replacing previous experimental extensions, to allow UTF-8 encoded Unicode characters to be used within the header. In particular, this allows email addresses to use non-ASCII characters. Such characters must only be used by servers that support these extensions.
The message header must include at least the following fields:
The message header should include at least the following fields:
RFC 3864 describes registration procedures for message header fields at the IANA; it provides for and message header field names, including also fields defined for MIME, netnews, and http, and referencing relevant RFCs. Common header fields for email include:
Note that the "To:" field is not necessarily related to the addresses to which the message is delivered. The actual delivery list is supplied separately to the transport protocol, SMTP, which may or may not originally have been extracted from the header content. The "To:" field is similar to the addressing at the top of a conventional letter which is delivered according to the address on the outer envelope. In the same way, the "From:" field does not have to be the real sender of the email message. Some mail servers apply email authentication systems to messages being relayed. Data pertaining to server's activity is also part of the header, as defined below.
SMTP defines the "trace information" of a message, which is also saved in the header using the following two fields:
Other header fields that are added on top of the header by the receiving server may be called "trace fields", in a broader sense.
Message body.
Content encoding.
Email was originally designed for 7-bit ASCII. Most email software is 8-bit clean but must assume it will communicate with 7-bit servers and mail readers. The MIME standard introduced character set specifiers and two content transfer encodings to enable transmission of non-ASCII data: quoted printable for mostly 7 bit content with a few characters outside that range and base64 for arbitrary binary data. The 8BITMIME and BINARY extensions were introduced to allow transmission of mail without the need for these encodings, but many mail transport agents still do not support them fully. In some countries, several encoding schemes coexist; as the result, by default, the message in a non-Latin alphabet language appears in non-readable form (the only exception is coincidence, when the sender and receiver use the same encoding scheme). Therefore, for international character sets, Unicode is growing in popularity.
Plain text and HTML.
Most modern graphic email clients allow the use of either plain text or HTML for the message body at the option of the user. HTML email messages often include an automatically generated plain text copy as well, for compatibility reasons.
Advantages of HTML include the ability to include in-line links and images, set apart previous messages in block quotes, wrap naturally on any display, use emphasis such as underlines and italics, and change font styles. Disadvantages include the increased size of the email, privacy concerns about web bugs, abuse of HTML email as a vector for phishing attacks and the spread of malicious software.
Some web based Mailing lists recommend that all posts be made in plain-text, with 72 or 80 characters per line for all the above reasons, but also because they have a significant number of readers using text-based email clients such as Mutt.
Some Microsoft email clients allow rich formatting using RTF, but unless the recipient is guaranteed to have a compatible email client this should be avoided.
In order to ensure that HTML sent in an email is rendered properly by the recipient's client software, an additional header must be specified when sending: "Content-type: text/html". Most email programs send this header automatically.
Servers and client applications.
Messages are exchanged between hosts using the Simple Mail Transfer Protocol with software programs called mail transfer agents (MTAs); and delivered to a mail store by programs called mail delivery agents (MDAs, also sometimes called local delivery agents, LDAs). Users can retrieve their messages from servers using standard protocols such as POP or IMAP, or, as is more likely in a large corporate environment, with a proprietary protocol specific to Novell Groupwise, Lotus Notes or Microsoft Exchange Servers. Webmail interfaces allow users to access their mail with any standard web browser, from any computer, rather than relying on an email client. Programs used by users for retrieving, reading, and managing email are called mail user agents (MUAs).
Mail can be stored on the client, on the server side, or in both places. Standard formats for mailboxes include Maildir and mbox. Several prominent email clients use their own proprietary format and require conversion software to transfer email between them. Server-side storage is often in a proprietary format but since access is through a standard protocol such as IMAP, moving email from one server to another can be done with any MUA supporting the protocol.
Accepting a message obliges an MTA to deliver it, and when a message cannot be delivered, that MTA must send a bounce message back to the sender, indicating the problem.
Filename extensions.
Upon reception of email messages, email client applications save messages in operating system files in the file system. Some clients save individual messages as separate files, while others use various database formats, often proprietary, for collective storage. A historical standard of storage is the "mbox" format. The specific format used is often indicated by special filename extensions:
Some applications (like Apple Mail) leave attachments encoded in messages for searching while also saving separate copies of the attachments. Others separate attachments from messages and save them in a specific directory.
Mobile devices, such as cell phones and tablet computers, commonly have the ability to receive email. Since users may always have their mobile device with them, users may access email significantly faster on these devices than through other methods, such as desktop computers or laptops.
URI scheme mailto.
The URI scheme, as registered with the IANA, defines the mailto: scheme for SMTP email addresses. Though its use is not strictly defined, URLs of this form are intended to be used to open the new message window of the user's mail client when the URL is activated, with the address as defined by the URL in the "To:" field.
Types.
Web-based email (webmail).
Many email providers have a web-based email client (e.g. AOL Mail, Gmail, Outlook.com and Yahoo! Mail). This allows users to log into the email account by using any compatible web browser to send and receive their email. Mail is typically not downloaded to the client, so can't be read without a current Internet connection.
POP3 email services.
POP3 is the acronym for Post Office Protocol 3. In a POP3 email account, email messages are downloaded to the client device (i.e. a computer) and then they are deleted from the mail server unless specific instruction to save has been given . It is difficult to save and view messages on multiple devices. Also, the messages sent from the computer/one device are not copied to the Sent Items folder on the (other) devices. The messages are deleted from the server to make room for more incoming messages. POP supports simple download-and-delete requirements for access to remote mailboxes (termed maildrop in the POP RFC's). Although most POP clients have an option to leave messages on the server after downloading a copy of them, most email clients using POP3 simply connect, retrieve all messages, store them on the client device as new messages, delete them from the server, and then disconnect.
IMAP email servers.
IMAP refers to Internet Message Access Protocol. With an IMAP account, a user's account has access to mail folders on the mail server and can use any compatible device to read and reply to messages, as long as such a device can access the server. Small portable devices like smartphones are increasingly used to check email while travelling, and to make brief replies, larger devices with better keyboard access being used to reply at greater length. IMAP shows the headers of messages, the sender and the subject and the device needs to request to download specific messages. Usually mail is left in folders in the mail server.
MAPI email servers.
Messaging Application Programming Interface (MAPI) is a messaging architecture and a Component Object Model based API for Microsoft Windows.
Use.
Flaming.
Flaming occurs when a person sends a message with angry or antagonistic content. The term is derived from the use of the word Incendiary to describe particularly heated email discussions. Flaming is assumed to be more common today because of the ease and impersonality of email communications: confrontations in person or via telephone require direct interaction, where social norms encourage civility, whereas typing a message to another person is an indirect interaction, so civility may be forgotten.
Email bankruptcy.
Also known as "email fatigue", email bankruptcy is when a user ignores a large number of email messages after falling behind in reading and answering them. The reason for falling behind is often due to information overload and a general sense there is so much information that it is not possible to read it all. As a solution, people occasionally send a boilerplate message explaining that the email inbox is being cleared out. Harvard University law professor Lawrence Lessig is credited with coining this term, but he may only have popularized it.
In business.
Email was widely accepted by the business community as the first broad electronic communication medium and was the first 'e-revolution' in business communication. Email is very simple to understand and like postal mail, email solves two basic problems of communication: logistics and synchronization (see below).
LAN based email is also an emerging form of usage for business. It not only allows the business user to download mail when "offline", it also allows the small business user to have multiple users' email IDs with just "one email connection".
Cons.
Most business workers today spend from one to two hours of their working day using email: reading, ordering, sorting, 're-contextualizing' fragmented information, and writing email. The use of email is increasing worldwide:
Despite these disadvantages, email has become the most widely used medium of communication within the business world. A 2010 study on workplace communication by Paytronics found 83% of U.S. knowledge workers felt email was critical to their success and productivity at work.
Research on email marketing.
Marketing research suggests that opt-in email marketing can be viewed as useful by consumers if it contains information such as special sales offerings and new product information. Offering interesting hyperlinks or generic information on consumer trends is less useful. This research by Martin et al. (2003) also shows that if consumers find email marketing useful, they are likely to visit a store, thereby overcoming limitations of Internet marketing such as not being able to touch or try on a product.
Mobile.
Email has become widely used on smart phones. Mobile apps for email increase accessibility to the medium. While before users could only access email on computers, it is now possible for users to check their email out of the home and out of the library while on the go. Alerts can also be sent to the phone to notify them immediately of new messages. This has given email the ability to be used for more frequent communication between users and allowed them to check their email and write messages throughout the day.
It was found that US adults check their email more than they browse the web or check their Facebook accounts, making email the most popular activity for users to do on their smart phones. 78% of the respondents in the study revealed that they check their email on their phone. It was also found that 30% of consumers use only their smartphone to check their email, and 91% were likely to check their email at least once per day on their smartphone. However, the percentage of consumers using email on smartphone ranges and differs dramatically across different countries. For example, in comparison to 75% of those consumers in the US who used it, only 17% in India did.
Security.
Data confidentiality, authentication, integrity, non-repudiation, access control, and availability are the most important security services that should be considered in secure applications and systems, but they are not provided in traditional email protocols. Email is vulnerable to both passive and active attacks. Passive threats include release of message contents, and traffic analysis while active threats include modification of message contents, masquerade, replay, and denial-of-service (DoS). All the mentioned threats are applicable to the traditional email protocols:
There are also concerns regarding the email privacy.
Problems.
Attachment size limitation.
Email messages may have one or more attachments, i.e. MIME parts intended to provide copies of files. Attachments serve the purpose of delivering binary or text files of unspecified size. In principle there is no technical intrinsic restriction in the InternetMessage Format, SMTP protocol or MIME limiting the size or number of attachments. In practice, however, email service providers implement various limitations on the permissible size of files or the size of an entire message.
Furthermore, due to technical reasons, often a small attachment can increase in size when sent, which can be confusing to senders when trying to assess whether they can or cannot send a file by email, and this can result in their message being rejected.
As larger and larger file sizes are being created and traded, many users are either forced to upload and download their files using an FTP server, or more popularly, use online file sharing facilities or services, usually over web-friendly HTTP, in order to send and receive them.
Information overload.
A December 2007 New York Times blog post described information overload as "a $650 Billion Drag on the Economy", and the New York Times reported in April 2008 that "E-MAIL has become the bane of some people's professional lives" due to information overload, yet "none of the current wave of high-profile Internet start-ups focused on email really eliminates the problem of email overload because none helps us prepare replies". GigaOm posted a similar article in September 2010, that found 57% of knowledge workers were overwhelmed by the volume of email they received. Technology investors reflect similar concerns.
In October 2010, CNN published an article titled "Happy Information Overload Day" that compiled research about "email overload" from IT companies and productivity experts. According to Basex, the average knowledge worker receives 93 messages per day. Subsequent studies have reported higher numbers. Marsha Egan, an email productivity expert, called email technology both a blessing and a curse in the article. She stated, "Everyone just learns that they have to have it dinging and flashing and open just in case the boss e-mails," she said. "The best gift any group can give each other is to never use e-mail urgently. If you need it within three hours, pick up the phone."
Spamming and computer viruses.
The usefulness of email is being threatened by four phenomena: email bombardment, spamming, phishing, and email worms.
Spamming is unsolicited commercial (or bulk) email. Because of the minuscule cost of sending email, spammers can send hundreds of millions of email messages each day over an inexpensive Internet connection. Hundreds of active spammers sending this volume of mail results in information overload for many computer users who receive voluminous unsolicited email each day.
Email worms use email as a way of replicating themselves into vulnerable computers. Although the first email worm affected UNIX computers, the problem is most common today on the Microsoft Windows operating system.
The combination of spam and worm programs results in users receiving a constant drizzle of junk email, which reduces the usefulness of email as a practical tool.
A number of anti-spam techniques mitigate the impact of spam. In the United States, U.S. Congress has also passed a law, the Can Spam Act of 2003, attempting to regulate such email. Australia also has very strict spam laws restricting the sending of spam from an Australian ISP, but its impact has been minimal since most spam comes from regimes that seem reluctant to regulate the sending of spam.
Email spoofing.
Email spoofing occurs when the email message header is designed to make the message appear to come from a known or trusted source. Email spam and phishing methods typically use spoofing to mislead the recipient about the true message origin.
Email bombing.
Email bombing is the intentional sending of large volumes of messages to a target address. The overloading of the target email address can render it unusable and can even cause the mail server to crash.
Privacy concerns.
Today it can be important to distinguish between Internet and internal email systems. Internet email may travel and be stored on networks and computers without the sender's or the recipient's control. During the transit time it is possible that third parties read or even modify the content. Internal mail systems, in which the information never leaves the organizational network, may be more secure, although information technology personnel and others whose function may involve monitoring or managing may be accessing the email of other employees.
Email privacy, without some security precautions, can be compromised because:
There are cryptography applications that can serve as a remedy to one or more of the above. For example, Virtual Private Networks or the Tor anonymity network can be used to encrypt traffic from the user machine to a safer network while GPG, PGP, SMEmail, or S/MIME can be used for end-to-end message encryption, and SMTP STARTTLS or SMTP over Transport Layer Security/Secure Sockets Layer can be used to encrypt communications for a single mail hop between the SMTP client and the SMTP server.
Additionally, many mail user agents do not protect logins and passwords, making them easy to intercept by an attacker. Encrypted authentication schemes such as SASL prevent this.
Finally, attached files share many of the same hazards as those found in peer-to-peer filesharing. Attached files may contain trojans or viruses.
Tracking of sent mail.
The original SMTP mail service provides limited mechanisms for tracking a transmitted message, and none for verifying that it has been delivered or read. It requires that each mail server must either deliver it onward or return a failure notice (bounce message), but both software bugs and system failures can cause messages to be lost. To remedy this, the IETF introduced Delivery Status Notifications (delivery receipts) and Message Disposition Notifications (return receipts); however, these are not universally deployed in production. (A complete Message Tracking mechanism was also defined, but it never gained traction; see RFCs 3885 through 3888.)
Many ISPs now deliberately disable non-delivery reports (NDRs) and delivery receipts due to the activities of spammers:
In the absence of standard methods, a range of system based around the use of web bugs have been developed. However, these are often seen as underhand or raising privacy concerns, and only work with e-mail clients that support rendering of HTML. Many mail clients now default to not showing "web content". Webmail providors can also disrupt web bugs by pre-caching images.
U.S. government.
The U.S. state and federal governments have been involved in electronic messaging and the development of email in several different ways.
Starting in 1977, the U.S. Postal Service (USPS) recognized that electronic messaging and electronic transactions posed a significant threat to First Class mail volumes and revenue. The USPS explored an electronic messaging initiative in 1977 and later disbanded it. Twenty years later, in 1997, when email volume overtook postal mail volume, the USPS was again urged to embrace email, and the USPS declined to provide email as a service. The USPS initiated an experimental email service known as E-COM. E-COM provided a method for the simple exchange of text messages. In 2011, shortly after the USPS reported its state of financial bankruptcy, the USPS Office of Inspector General (OIG) began exploring the possibilities of generating revenue through email servicing. Electronic messages were transmitted to a post office, printed out, and delivered as hard copy. To take advantage of the service, an individual had to transmit at least 200 messages. The delivery time of the messages was the same as First Class mail and cost 26 cents. Both the Postal Regulatory Commission and the Federal Communications Commission opposed E-COM. The FCC concluded that E-COM constituted common carriage under its jurisdiction and the USPS would have to file a tariff. Three years after initiating the service, USPS canceled E-COM and attempted to sell it off.
The early ARPANET dealt with multiple email clients that had various, and at times incompatible, formats. For example, in the Multics, the "@" sign meant "kill line" and anything before the "@" sign was ignored, so Multics users had to use a command-line option to specify the destination system. The Department of Defense DARPA desired to have uniformity and interoperability for email and therefore funded efforts to drive towards unified inter-operable standards. This led to David Crocker, John Vittal, Kenneth Pogran, and Austin Henderson publishing RFC 733, "Standard for the Format of ARPA Network Text Message" (November 21, 1977), a subset of which provided a stable base for common use on the ARPANET, but which was not fully effective, and in 1979, a meeting was held at BBN to resolve incompatibility issues. Jon Postel recounted the meeting in RFC 808, "Summary of Computer Mail Services Meeting Held at BBN on 10 January 1979" (March 1, 1982), which includes an appendix listing the varying email systems at the time. This, in turn, lead to the release of David Crocker's RFC 822, "Standard for the Format of ARPA Internet Text Messages" (August 13, 1982). RFC 822 is a small adaptation of RFC 733's details, notably enhancing the host portion, to use Domain Names, that were being developed at the same time.
The National Science Foundation took over operations of the ARPANET and Internet from the Department of Defense, and initiated NSFNet, a new backbone for the network. A part of the NSFNet AUP forbade commercial traffic. In 1988, Vint Cerf arranged for an interconnection of MCI Mail with NSFNET on an experimental basis. The following year Compuserve email interconnected with NSFNET. Within a few years the commercial traffic restriction was removed from NSFNETs AUP, and NSFNET was privatised.
In the late 1990s, the Federal Trade Commission grew concerned with fraud transpiring in email, and initiated a series of procedures on spam, fraud, and phishing. In 2004, FTC jurisdiction over spam was codified into law in the form of the CAN SPAM Act. Several other U.S. federal agencies have also exercised jurisdiction including the Department of Justice and the Secret Service.
NASA has provided email capabilities to astronauts aboard the Space Shuttle and International Space Station since 1991 when a Macintosh Portable was used aboard Space Shuttle mission STS-43 to send the first email via AppleLink. Today astronauts aboard the International Space Station have email capabilities via the wireless networking throughout the station and are connected to the ground at 10 Mbit/s Earth to station and 3 Mbit/s station to Earth, comparable to home DSL connection speeds.
Further reading.
</dl>

</doc>
<doc id="9739" url="http://en.wikipedia.org/wiki?curid=9739" title="Emoticon">
Emoticon

An emotion icon, better known by the portmanteau emoticon () is a metacommunicative pictorial representation of a facial expression that, in the absence of body language and prosody, serves to draw a receiver's attention to the tenor or temper of a sender's nominal verbal communication, changing and improving its interpretation. It expresses — usually by means of punctuation marks (though it can include numbers and letters) — a person's feelings or mood, though as emoticons have become more popular, some devices have provided stylized pictures that do not use punctuation.
In Western countries, emoticons are usually written at a right angle to the direction of the text. Users from Japan popularized a kind of emoticons called kaomojis (, often confused with emojis in the West) that can be understood without tilting one's head to the left. This style arose on ASCII NET of Japan in 1986.
As social media has become widespread, emoticons have played a significant role in communication through technology. They offer another range of "tone" and feeling through texting that portrays specific emotions through facial gestures while in the midst of text-based cyber communication.
Origin of the term.
The word is a portmanteau word of the English words "emotion" and "icon". In web forums, instant messengers and online games, text emoticons are often automatically replaced with small corresponding images, which came to be called "emoticons" as well. Emoticons for a smiley face codice_1 and sad face codice_2 appear in the first documented use in digital form. Certain complex character combinations can only be accomplished in a double-byte language, giving rise to especially complex forms, sometimes known by their romanized Japanese name of kaomoji.
The use of emoticons can be traced back to the 19th century, and they were commonly used in casual and humorous writing. Digital forms of emoticons on the Internet were included in a proposal by Scott Fahlman of Carnegie Mellon University in Pittsburgh, Pennsylvania, in a message on 19 September 1982.
History.
Antecedents.
The "National Telegraphic Review and Operators Guide" in April 1857 documented the use of the number 73 in Morse code to express "love and kisses" (later reduced to the more formal "best regards"). "Dodge's Manual" in 1908 documented the reintroduction of "love and kisses" as the number 88. Gajadhar and Green comment that both Morse code abbreviations are more succinct than modern abbreviations such as LOL.
A "New York Times" transcript from an Abraham Lincoln speech written in 1862 contains "(applause and laughter codice_3"; there is some debate as to whether it is a typo, a legitimate punctuation construct, or an emoticon.
Four vertical typographical emoticons were published in 1881 by the U.S. satirical magazine "Puck", with the stated intention that the publication's letterpress department thus intended to "lay out ... all the cartoonists that ever walked."
In 1912, Ambrose Bierce proposed "an improvement in punctuation – the snigger point, or note of cachinnation: it is written thus ‿ and presents a smiling mouth. It is to be appended, with the full stop, to every jocular or ironical sentence".
In a 1936 Harvard Lampoon article, Alan Gregg proposed (-) for smile, (--) for laugh (more teeth showing), (#) for frown, (*) for wink, and (#) for "intense interest, attention, and incredulity". Note that the symbols are correctly oriented and are not sideways.
Emoticons had already come into use in sci-fi fandom in the 1940s, although there seems to have been a lapse in cultural continuity between the communities.
The September 1962 issue of MAD Magazine published an article titled "Typewri-toons." The piece, featuring typewriter-generated artwork credited to "Royal Portable," was entirely made up of repurposed typography, including a capital letter P having a bigger bust than a capital I, a lowercase b and d discussing their pregnancies, an asterisk on top of a letter to indicate the letter had just come inside from a snowfall, and a classroom of lowercase n's interrupted by a lowercase h "raising its hand." Two additional "Typewri-toons" articles subsequently appeared in "Mad", in 1965 and 1987.
In 1963 the "smiley face", a yellow button with two black dots representing eyes and an upturned thick curve representing a mouth was created by freelance artist Harvey Ball. It was realized on order of a large insurance company as part of a campaign to bolster the morale of its employees and soon became a big hit. This smiley presumably inspired many later emoticons; the most basic graphic emoticon that depicts this is in fact a small yellow smiley face.
In a "New York Times" interview in April 1969, Alden Whitman asked writer Vladimir Nabokov: "How do you rank yourself among writers (living) and of the immediate past?" Nabokov answered: "I often think there should exist a special typographical sign for a smile – some sort of concave mark, a supine round bracket, which I would now like to trace in reply to your question."
On the PLATO system in the 1970s, emoticons and other decorative graphics were produced as ASCII art, particularly with overprinting: typing a character, backing up, then typing another character. For example, WOBTAX and VICTORY both produced convincing smiley faces. This developed into a sophisticated set, particularly in combination with superscript and subscript.
Creation of :-) and :-(.
Scott Fahlman was the first documented person to use the emoticons codice_1 and codice_2, with a specific suggestion that they be used to express emotion. The text of his original proposal, posted to the Carnegie Mellon University computer science general board on 19 September 1982 (11:44), was thought to have been lost, but was recovered 20 years later by Jeff Baird from old backup tapes.
Other notable computer scientists who participated in this thread include David Touretzky, Guy Steele, and Jaime Carbonell.
Within a few months, it had spread to the ARPANET and Usenet. Many variations on the theme were immediately suggested by Scott and others.
Western style.
Usually, emoticons in Western style have the eyes on the left, followed by nose and the mouth. The two character version codice_6 which omits the nose is also very popular.
Common Western examples.
The most basic emoticons are relatively consistent in form, but each of them can be transformed by being rotated (making them tiny ambigrams), with or without a hyphen (nose).
There are also some possible variations to emoticons to get new definitions, like changing a character to express a new feeling, or slightly change the mood of the emoticon. For example codice_7 equals sad and codice_8 equals very sad or weeping. A blush can be expressed as codice_9. Others include wink codice_3, a grin codice_11, smug codice_12, and tongue out codice_13 for disgust or simply just to stick the tongue out for silliness, such as when blowing a raspberry. An often used combination is also codice_14 for a heart, and codice_15 for a broken heart.
A broad grin is sometimes shown with crinkled eyes to express further amusement; codice_16 and the addition of further "D" letters can suggest laughter or extreme amusement e.g. codice_17. There are hundreds of other variations including codice_18 for an evil grin or codice_19 for anger, which can be, again, used in reverse, for an unhappily angry face, in the shape of codice_20. codice_21 for vampire teeth, codice_22 for grimace, and codice_23 can be used to denote a flirting or joking tone, or may be implying a second meaning in the sentence preceding it.
Variation.
An equal sign is often used for the eyes in place of the colon, seen as codice_24, without changing the meaning of the emoticon. In these instances, the hyphen is almost always either omitted or, occasionally, replaced with an "o" as in codice_25 . In most circles it has become acceptable to omit the hyphen, whether a colon or an equal sign is used for the eyes, but in some areas of usage people still prefer the larger, more traditional emoticon codice_1 or codice_27. Similar-looking characters are commonly substituted for one another: for instance, codice_28, codice_29, and codice_30 can all be used interchangeably, sometimes for subtly different effect or, in some cases, one type of character may look better in a certain font and therefore be preferred over another. It is also common for the user to replace the rounded brackets used for the mouth with other, similar brackets, such as codice_31 instead of codice_32.
Some variants are also more common in certain countries due to keyboard layouts. For example, the smiley codice_24 may occur in Scandinavia, where the keys for codice_34 and codice_32 are placed right beside each other. However, the codice_6 variant is without a doubt the dominant one in Scandinavia, making the codice_24 version a rarity. Diacritical marks are sometimes used. The letters codice_38 and codice_39 can be seen as an emoticon, as the upright version of codice_40 (meaning that one is surprised) and codice_11 (meaning that one is very happy) respectively.
Some emoticons may be read right to left instead, and in fact can only be written using standard ASCII keyboard characters this way round; for example codice_42 which refers to being shocked or anxious, opposite to the large grin of codice_11.
Japanese style.
Users from Japan popularized a style of emoticons (, "kaomoji") that can be understood without tilting one's head to the left. This style arose on ASCII NET of Japan in 1986. Similar looking emoticons were used by Byte Information Exchange (BIX) around the same time.
These emoticons are usually found in a format similar to codice_44. The asterisks indicate the eyes; the central character, commonly an underscore, the mouth; and the parentheses, the outline of the face.
Different emotions such as codice_45, are expressed by changing the character representing the eyes, for example "T" can be used to express crying or sadness codice_46. T_T may also be used as meaning 'unimpressed'. The emphasis on the eyes is reflected in the common usage of emoticons that use only the eyes, e.g. codice_47. Looks of stress are represented by the likes of codice_48 while codice_49 is a generic emoticon for nervousness, the semicolon indicating sweat that occurs during anxiety. Repeating the /// mark can indicate embarrassment by symbolizing blushing. Characters like hyphens or periods can replace the underscore; the period is often used for a smaller, "cuter" mouth or to represent a nose, e.g. codice_50. Alternatively, the mouth/nose can be left out entirely, e.g. codice_51.
Parentheses also can often be replaced with braces, e.g. codice_52. Many times, the parentheses are left out completely, e.g. codice_47,codice_54, codice_55, codice_56, codice_57 and/or codice_58. A quotation mark ", apostrophe ', or semicolon ; can be added to the emoticon to imply apprehension or embarrassment, in the same way that a sweat drop is used in popular and common Asian animation.
Microsoft IME 2000 (Japanese) or later supports the use of both forms of emoticons by enabling Microsoft IME Spoken Language/Emotion Dictionary. In IME 2007, it was moved to Emoticons dictionary.
Further variations of emoticons may be produced by using combining characters, e.g.codice_59 and codice_60.
These emoticons can be used also with [ ] instead of ( ), or without the parentheses at all in some of the cases. There is also the \(0O0)\, indicating a hooligan or crazed behaviour, and the (ﾉ◕ヮ◕)ﾉ*:･ﾟ✧.
Western use of Japanese style.
English-language anime forums adopted those emoticons that could be used with the standard ASCII characters available on western keyboards. Because of this, they are often called "anime style" emoticons in the English-speaking Internet. They have since seen use in more mainstream venues, including online gaming, instant-messaging, and other non-anime related forums. Emoticons such as codice_61, codice_62, codice_63, codice_64, codice_65 or codice_66 which include the parentheses, mouth or nose, and arms (especially those represented by the inequality signs < or >) also are often referred to as "Kirbys" in reference to their likeness to Nintendo's video game character, Kirby. The parentheses are sometimes dropped when used in the English language context, and the underscore of the mouth may be extended as an intensifier, (e.g. codice_67 for very happy) for the emoticon in question. This emoticon codice_68 uses the eastern style, but incorporates a depiction of the western "middle-finger flick-off (commonly known as 'the bird')" using a "t" as the arm, hand, and finger. Also, one of the newer ones, codice_69, or codice_70 as such for a "vampire" or other mythical beasts with fangs.
Mixture of Western and Japanese style.
Exposure to both Western and Japanese style emoticons or kaomojis (often incorrectly called "emojis") through blogs, instant messaging, and forums featuring a blend of Western and Japanese pop culture, has given rise to emoticons that have an upright viewing format. The parentheses are similarly dropped in the English language context and the emoticons only use alphanumeric characters and the most commonly used English punctuation marks. Emoticons such as codice_71, codice_72, codice_73, codice_74, codice_75, codice_76, codice_77, and codice_78 are used to convey mixed emotions that are more difficult to convey with traditional emoticons. Characters are sometimes added to emoticons to convey an anime or manga-styled sweat drop, for example: codice_79 or codice_80 as well as: codice_81, codice_82 and codice_83
The equal sign can also be used for closed, anime looking eyes, for example: codice_84, codice_85, codice_86, codice_87 and codice_88.
There are also more faces along those lines likecodice_89using the ; as a sweat mark, and the o as a mouth, and the inequality signs as the eyes, it shows stress, or slight confusion. The number of emoticons that can be made is very large, and can express many shades of meaning.
In Brazil, sometimes combining character (accent) are added to emoticons to represent eyebrows, like: codice_90, codice_91, codice_92, codice_93 or codice_94. They can also replace (or add) codice_34 or codice_96 withcodice_97, for examplecodice_98,codice_99,codice_100,codice_101,codice_102 orcodice_103.
2channel style.
The Japanese language is usually encoded using double-byte character codes. As a result there is a bigger variety of characters that can be used in emoticons, many of which cannot be reproduced in ASCII. Most kaomoji contain Cyrillic and other foreign letters to create even more complicated expressions analogous to ASCII art's level of complexity. To type such emoticons, the input editor that is used to type Japanese on a user's system is equipped with a dictionary of emoticons, after which the user simply types the Japanese word (or something close to it) that represents the desired emoticon to convert the input into such complicated emoticons. Such expressions are known as Shift JIS art.
Users of 2channel in particular have developed a wide variety of unique emoticons using characters from various languages, such as Kannada: codice_104 (for a look of disapproval, disbelief, or confusion); these were quickly picked up by 4chan and spread to other Western sites soon after. Some have taken on a life of their own and become characters in their own right, like Mona.
Korean style.
In South Korea, emoticons use Korean Hangul letters, and the Western style is rarely used. The structures of Korean and Japanese emoticons are somewhat similar, but they have some differences. Korean style contains Korean jamo (letters) instead of other characters. There are countless number of emoticons that can be formed with such combinations of Korean jamo letters. Consonant jamos : ㅅ or ㅁ or ㅂ as the mouth/nose component and ㅇ,ㅎ,ㅍ for the eyes. For example: codice_105, codice_106, codice_107 and codice_108. Faces such as codice_109, codice_110, codice_111 and codice_112, using quotation marks " and apostrophes ' are also commonly used combinations.
Vowel jamos such as ㅜ,ㅠ depict a crying face. Example: codice_113, codice_114 and codice_115 (same function as T in western style). Sometimes ㅡ (not an em-dash "—" but a vowel jamo), a comma or an underscore is added, and the two character sets can be mixed together, as in codice_116, codice_117, codice_118, codice_119, codice_120 and codice_121. Also, semicolons and carets are commonly used in Korean emoticons; semicolons mean sweating (embarrassed). If they are used with ㅡ or - they depict a bad feeling. Examples: codice_122, codice_123, codice_124, codice_125 and codice_126. However, codice_127 means smile (almost all people use this without distinction of sex or age). Others include: codice_128, codice_129, codice_130, codice_131.
Chinese ideographic style.
The character 囧 (U+56E7), which means "bright", is also used in the Chinese computing community for a frowning face. It is also combined with posture emoticon Orz, such as 囧rz. The character existed in Oracle bone script, but its use as emoticon was documented as early as January 20, 2005.
Other ideographic variants for 囧 include 崮 (king 囧), 莔 (queen 囧), 商 (囧 with hat), 囧興 (turtle), 卣 (Bomberman).
The character 槑 (U+69D1), which sounds like the word for "plum" (梅 (U+FA44)), is used to represent double of 呆 (dull), or further magnitude of dullness. In Chinese, normally full characters (as opposed to the stylistic use of 槑) may be duplicated to express emphasis.
Posture emoticons.
Orz.
Orz (also seen as Or2, on_, OTZ, OTL, STO, JTO, _no, ＿冂○, 囧​rz, O7Z, _|7O, Sto, O|¯|_, orz, and Jto) is an emoticon representing a kneeling or bowing person, with the "o" being the head, the "r" being the arms and part of the body, and the "z" being part of the body and the legs. This stick figure represents failure and despair. It is also commonly used for representing a great admiration (sometimes with an overtone of sarcasm) for someone else's view or action.
It was first used in late 2002 at the forum on Techside, Japanese personal website. At the "Techside FAQ Forum" (TECHSIDE教えて君BBS(教えてBBS) ), a poster asked about a cable cover, typing "＿|￣|○" to show a cable and its cover. Others commented that it looked like a kneeling person, and the symbol became popular. These comments were soon deleted as they were considered off-topic. However, one of the first corresponding reactions can be found on the thread on at the Wayback Machine (archived ), on December 23, 2002. By 2005, Orz spawned a subculture: blogs have been devoted to the emoticon, and URL shortening services have been named after it. In Taiwan, Orz is associated with the phrase "nice guy" – that is, the concept of males being rejected for a date by girls they are pursuing with a phrase like "You are a nice guy."
Orz should not be confused with m(_ _)m, which means "Thank you" or an apology.
Multimedia variations.
A portmanteau of "emotion" and "sound", an emotisound is a brief sound transmitted and played back during the viewing of a message, typically an IM message or e-mail message. The sound is intended to communicate an emotional subtext. Many instant messaging clients automatically trigger sound effects in response to specific emoticons.
Some services, such as MuzIcons, combine emoticons and music player in an Adobe Flash-based widget.
In 2004, The Trillian chat application introduced a feature called "emotiblips", which allows Trillian users to stream files to their instant message recipients "as the voice and video equivalent of an emoticon".
In 2007, MTV and Paramount Home Entertainment promoted the "emoticlip" as a form of viral marketing for the second season of the show "The Hills". The emoticlips were twelve short snippets of dialogue from the show, uploaded to YouTube, which the advertisers hoped would be distributed between web users as a way of expressing feelings in a similar manner to emoticons. The emoticlip concept is credited to the Bradley & Montgomery advertising firm, which hopes they would be widely adopted as "greeting cards that just happen to be selling something".
In 2008, an emotion-sequence animation tool, called FunIcons was created. The Adobe Flash and Java-based application allows users to create a short animation. Users can then email or save their own animations to use them on similar social utility applications.
During the first half of the 2010s, there have been different forms of small audiovisual pieces to be sent through instant messaging systems to express one's emotion. These videos lack of a popular name yet and there are several ways to designate them: "emoticlips" (named above), "emotivideos" or more recently "emoticon videos". These are tiny little videos which can be easily transferred from one mobile phone to another or many other devices. The current video compression codecs (like H.264) allow these pieces of video to be light in terms of KB and very portable.
Emoticons and intellectual property rights.
In 2000, Despair, Inc. obtained a U.S. trademark registration for the "frowny" emoticon :-( when used on "greeting cards, posters and art prints." In 2001, they issued a satirical press release, announcing that they would sue Internet users who typed the frowny; the joke backfired and the company received a storm of protest when its mock release was posted on technology news website Slashdot.
A number of patent applications have been filed on inventions that assist in communicating with emoticons. A few of these have issued as US patents. <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi/fmt%3Akev%3Amtx%3Apatent&rft.number=6987991&rft.cc=US&rft.title="> , for example, discloses a method developed in 2001 to send emoticons over a cell phone using a drop down menu. The stated advantage over the prior art was that the user saved on the number of keystrokes though this may not address the obviousness criteria.
In Finland, the Supreme Administrative Court ruled in 2012 that the emoticon cannot be trademarked, thus repealing a 2006 administrative decision trademarking the emoticons :-), =), =(, :) and :(. The emoticon :-) was also filed in 2006 and registered in 2008 as a European Community Trademark (CTM).
In 2008, Russian entrepreneur Oleg Teterin claimed to have been granted the trademark on the ;-) emoticon. A license would not "cost that much – tens of thousands of dollars" for companies, but would be free of charge for individuals.
Unicode.
Emoticons are introduced in Unicode Standard version 6.0 (published in October 2010). It covers unicode range from 1F600 to 1F64F.

</doc>
<doc id="9740" url="http://en.wikipedia.org/wiki?curid=9740" title="Epoch">
Epoch

An epoch, Epoch or EPOCH may refer to:

</doc>
<doc id="9742" url="http://en.wikipedia.org/wiki?curid=9742" title="Erdős number">
Erdős number

The Erdős number (]) describes the "collaborative distance" between mathematician Paul Erdős and another person, as measured by authorship of mathematical papers.
The same principle has been applied in other fields where a particular individual has collaborated with a large and broad number of peers. The American Mathematical Society provides a free online tool to determine the Erdős number of every mathematical author listed in the Mathematical Reviews catalogue.
Overview.
Paul Erdős (1913–1996) was an influential mathematician who spent a large portion of his later life writing papers with a large number of colleagues, working on solutions to outstanding mathematical problems. He published more papers during his lifetime (at least 1,525) than any other mathematician in history. (Leonhard Euler published more total pages of mathematics but fewer separate papers: about 800.) Erdős spent a large portion of his later life living out of a suitcase, visiting his over 500 collaborators around the world.
The idea of the Erdős number was originally created by the mathematician's friends as a tribute to his enormous output. However, in later years it gained prominence as a tool to study how mathematicians cooperate to find answers to unsolved problems. Several projects are devoted to studying connectivity among researchers, using the Erdős number as a proxy. For example, Erdős collaboration graphs can tell us how authors cluster together, how the number of co-authors per paper evolves over time, or how new theories propagate.
Several studies have shown that leading mathematicians tend to have particularly low Erdős numbers. For example, only 134,007 mathematicians have an Erdős number, with a median value of 5. In contrast, the median Erdős number of Fields Medalists is 3. Only 7,097 (about 5%) of mathematicians with a collaboration path have an Erdős number of 2 or less. Collaboration distances will necessarily increase over long time scales, as mathematicians with low Erdős numbers die and become unavailable for collaboration.
Definition and application in mathematics.
To be assigned an Erdős number, someone must be a coauthor of a research paper with another person who has a finite Erdős number. Paul Erdős has an Erdős number of zero. Anybody else's Erdős number is "k" + 1 where "k" is the lowest Erdős number of any coauthor.
Erdős wrote around 1,500 mathematical articles in his lifetime, mostly co-written. He had 511 direct collaborators; these are the people with Erdős number 1. The people who have collaborated with them (but not with Erdős himself) have an Erdős number of 2 (9267 people as of 2010), those who have collaborated with people who have an Erdős number of 2 (but not with Erdős or anyone with an Erdős number of 1) have an Erdős number of 3, and so forth. A person with no such coauthorship chain connecting to Erdős has an Erdős number of infinity (or an undefined one). Since the death of Paul Erdős, the lowest Erdős number that a new researcher can obtain is 2.
There is room for ambiguity over what constitutes a link between two authors. The American Mathematical Society collaboration distance calculator uses data from Mathematical Reviews, which includes most mathematics journals but not other subjects, and which also includes some non-research publications. The Erdős Number Project web site says:
... Our criterion for inclusion of an edge between vertices u and v is some research collaboration between them resulting in a published work. Any number of additional co-authors is permitted...
but they do not include non-research publications such as elementary textbooks, joint editorships, obituaries, and the like. The “Erdős number of the second kind” restricts assignment of Erdős numbers to papers with only two collaborators.
The Erdős number was most likely first defined in print by Casper Goffman, an analyst whose own Erdős number is 2. Goffman published his observations about Erdős' prolific collaboration in a 1969 article entitled "And what is your Erdős number?" See also some comments in an obituary by Michael Golomb.
The median Erdős number among Fields medalists is as low as 3. Fields medalists with Erdős number 2 include Atle Selberg, Kunihiko Kodaira, Klaus Roth, Alan Baker, Enrico Bombieri, David Mumford, Charles Fefferman, William Thurston, Shing-Tung Yau, Jean Bourgain, Richard Borcherds, Manjul Bhargava and Terence Tao. There are no Fields medalists with Erdős number 1, however Endre Szemerédi is an Abel Prize Laureate with Erdős number 1.
Most frequent Erdős collaborators.
While Erdős collaborated with hundreds of co-authors, there were some individuals with whom he co-authored dozens of papers. This is a list of the ten persons who most frequently co-authored with Erdős and their number of papers co-authored with Erdős (i.e. their number of collaborations).
Related fields.
Physics.
Among the Nobel Prize laureates in Physics, Albert Einstein and Sheldon Lee Glashow have an Erdős number of 2. Nobel Laureates with an Erdős number of 3 include Enrico Fermi, Otto Stern, Wolfgang Pauli, Max Born, Willis E. Lamb, Eugene Wigner, Richard P. Feynman, Hans A. Bethe, Murray Gell-Mann, Abdus Salam, Steven Weinberg, Norman F. Ramsey, Frank Wilczek, David Wineland. Fields Medal-winning physicist Ed Witten has an Erdős number of 3.
Chemistry.
Nobel Prize laureates in Chemistry with an Erdős number of 3 include Lars Onsager, Kenichi Fukui, Herbert A. Hauptman, Walter Kohn.
Medicine.
Nobel Prize laureates in Medicine with an Erdős number of 3 include John Carew Eccles, Hamilton O. Smith, John E. Sulston.
Finance and economics.
Harry M. Markowitz is the only Nobel Prize laureate in Economics with an Erdős number of 2. Other financial mathematicians with Erdős number of 2 include David Donoho, Marc Yor, Henry McKean, Daniel Stroock, and Joseph Keller.
Nobel Prize laureates in Economics with an Erdős number of 3 include Kenneth J. Arrow, Herbert A. Simon, Gerard Debreu, James Mirrlees, Daniel Kahneman, Robert J. Aumann, Alvin E. Roth, and Lloyd S. Shapley.
Law.
Judge Richard Posner, having coauthored with Alvin E. Roth, has an Erdős number of at most 4.
Social network analysis.
Sociologist Barry Wellman has an Erdős number of 3 via social network analyst and statistician Ove Frank, who collaborated with graph theorist Frank Harary.
Impact.
Erdős numbers have been a part of the folklore of mathematicians throughout the world for many years. Among all working mathematicians at the turn of the millennium who have a finite Erdős number, the numbers range up to 15, the median is 5, and the mean is 4.65; almost everyone with a finite Erdős number has a number less than 8. Due to the very high frequency of interdisciplinary collaboration in science today, very large numbers of non-mathematicians in many other fields of science also have finite Erdős numbers. For example, political scientist Steven Brams has an Erdős number of 2. In biomedical research, it is common for statisticians to be among the authors of publications, and many statisticians can be linked to Erdős via John Tukey, who has an Erdős number of 2. Similarly, the prominent geneticist Eric Lander and the mathematician Daniel Kleitman have collaborated on papers, and since Kleitman has an Erdős number of 1, a large fraction of the genetics and genomics community can be linked via Lander and his numerous collaborators. Similarly, collaboration with Gustavus Simmons opened the door for 
Erdős numbers within the cryptographic research community, and many linguists have finite Erdős numbers, many due to chains of collaboration with such notable scholars as Noam Chomsky (Erdős number 4), William Labov (3), Mark Liberman (3), Geoffrey Pullum (3), or Ivan Sag (4). There are also connections with arts fields.
According to Alex Lopez-Ortiz, all the Fields and Nevanlinna prize winners during the three cycles in 1986 to 1994 have Erdős numbers of at most 9.
Earlier mathematicians published fewer papers than modern ones, and more rarely published jointly written papers. The earliest person known to have a finite Erdős number is either Richard Dedekind (born 1831, Erdős number 7) or Ferdinand Georg Frobenius (born 1849, Erdős number 3), depending on the standard of publication eligibility. It seems that older historic figures such as Leonhard Euler (born 1707) do not (yet) have finite Erdős numbers.
Tompa proposed a directed graph version of the Erdős number problem, by orienting edges of the collaboration graph from the alphabetically earlier author to the alphabetically later author and defining the "monotone Erdős number" of an author to be the length of a longest path from Erdős to the author in this directed graph. He finds a path of this type of length 12.
Also, Michael Barr suggests "rational Erdős numbers", generalizing the idea that a person who has written p joint papers with Erdős should be assigned Erdős number 1/p. From the collaboration multigraph of the second kind (although he also has a way to deal with the case of the first kind)—with one edge between two mathematicians for "each" joint paper they have produced—form an electrical network with a one-ohm resistor on each edge. The total resistance between two nodes tells how "close" these two nodes are.
It has been argued that "for an individual researcher, a measure such as Erdős number captures the structural properties of [the] network whereas the "h"-index captures the citation impact of the publications," and that "One can be easily convinced that ranking in coauthorship networks should take into account both measures to generate a realistic and acceptable ranking." Several author ranking systems based on eigenvector centrality have been proposed, for instance the Phys Author Rank Algorithm.
In 2004 William Tozier, a mathematician with an Erdös number of 4, auctioned off an co-authorship on eBay and hence providing the buyer with an Erdös number of 5. The winning bid of $1031 was posted by Spanish mathematician, who however did not intend to pay but just placed the bid to stop what he considered a mockery.
Variations.
A number of variations on the concept have been proposed to apply to other fields.
The best known is Bacon number (as in the game Six Degrees of Kevin Bacon), connecting actors that appeared in a film together to the actor Kevin Bacon. It was created in 1994, 25 years after Goffman's article on the Erdős number.
A small number of people are connected to both Erdős and Bacon and thus have an Erdős–Bacon number, which combines the two numbers by taking their sum. One example is the actress-mathematician Danica McKellar, best known for playing Winnie Cooper on the TV series, "The Wonder Years". 
Her Erdős number is 4 and her Bacon number is 2. The lowest known
Erdős–Bacon number is 3 – for Daniel Kleitman, a mathematics professor at MIT – his Erdős number is 1 and his Bacon number is 2.
Further generalization is possible. For example, Erdős–Bacon–Sabbath numbers include the band Black Sabbath in the measure. The lowest known Erdős–Bacon–Sabbath number is 8, a value shared by physicist Stephen Hawking, neuroscientist Daniel Levitin and inventor Ray Kurzweil, all of whom have an Erdős number of 4, a Bacon number of 2, and a Sabbath number of 2.
English mathematician Peter Swinnerton-Dyer has an Erdős-Morphy number of 5.
Other targets include:

</doc>
<doc id="9750" url="http://en.wikipedia.org/wiki?curid=9750" title="School voucher">
School voucher

A school voucher, also called an education voucher, with the system overall being called the voucher system, is a certificate of funding issued by the government, which the parents of a schoolchild have control of and are able to direct towards the public or private school of their own choosing to fully or partially pay for the tuition of their child at that school for that year, term or semester. In some countries, states or local jurisdictions, the voucher can be used to cover or reimburse home schooling expenses. In some countries, vouchers only exist for tuition at private schools. Under many non-voucher education systems, people who pay for private schooling are still taxed for public schools, these parents fund both public schools (through taxes) and private schools (through tuition) simultaneously; although, in some countries, states or local jurisdictions (e.g. in Australia) private schools may still receive substantial government funding.
History.
The oldest continuing school voucher programs existing today in the United States are the Town Tuitioning programs in Vermont and Maine, beginning in 1869 and 1873 respectively. Because some towns in these states operate neither local high schools nor elementary schools, students in these towns "are eligible for a voucher to attend [either] public schools in other towns or non-religious private schools. In these cases, the 'sending' towns pay tuition directly to the 'receiving' schools."
 A system of educational vouchers was introduced in the Netherlands in 1917. Today, more than 70% of pupils attend privately run but publicly funded schools, mostly split along denominational lines.
Nobel Prize–winning economist Milton Friedman argued for the modern concept of vouchers in the 1950s, stating that competition would improve schools and cost efficiency. The view further gained popularity with the 1980 TV broadcast of Friedman's series "Free to Choose" for which volume 6 was devoted entirely to promoting educational freedom through programs like school vouchers.
In some Southern states during the 1960s, school vouchers were used as a method of perpetuating segregation. In a few instances, public schools were closed outright and vouchers were issued to parents. The vouchers, in many cases, were only good at privately segregated schools, known as segregation academies. In 2005, Dr. Allah Bakhsh Malik Managing Director Punjab Education Foundation, under the supervision of Professor Henry M. Levin introduced Education Vouchers scheme in Pakistan with the features of equity, productivity, social cohesion and freedom of choice. Today, all modern voucher programs prohibit racial discrimination.
Definitions.
There are important distinctions between different kinds of schools:
Controversy.
Proponents.
Proponents assert that school voucher and education tax credit systems promote free market competition among both private and public schools. By allowing parents and students to "vote with their feet," they incentivize schools to increase accountability and school performance. Proponents argue that the competition spurred by vouchers and education tax credits increases the quality and efficiencies of both eligible private schools and local public schools, as they both must perpetually improve in order to maintain enrollment caused by the competitive nature of dollar voting and the swift accountability that results from increasing consumer sovereignty - allowing individuals to control what product or service they prefer to buy as opposed to a bureaucracy.
The argument that school vouchers increases quality and efficiencies in schools forced to compete is supported by studies such as "When Schools Compete: The Effects of Vouchers on Florida Public School Achievement" (Manhattan Institute for Policy Research, 2003), which concluded that public schools located near private schools that were eligible to accept voucher students made significantly more improvements than did similar schools not located near eligible private schools. Stanford's C.M. Hoxby, who has researched the systemic effects of school choice, determined that areas with greater residential school choice have consistently higher test scores at a lower per-pupil cost than areas with very few school districts (see Hoxby, 1998). Hoxby found that the effects of vouchers in Milwaukee and of charter schools in Arizona and Michigan on nearby public schools forced to compete made greater test score gains than schools not faced with such competition (see Hoxby, 2001), and that the so-called effect of cream skimming did not exist in any of the voucher districts examined. Hoxby's research has found that both private and public schools improved through the use of vouchers. Also, similar competition has helped in manufacturing, energy, transportation, and parcel postal (UPS, FedEx vs. USPS) sectors of government that have been socialized and later opened up to free market competition.
Similarly, it is argued that such competition has helped in higher education, with publicly funded universities directly competing with private universities for tuition money provided by the Government, such as the GI Bill and the Pell Grant in the United States. The Foundation for Educational Choice alleges that a school voucher plan "embodies exactly the same principle as the GI bills that provide for educational benefits to military veterans. The veteran gets a voucher good only for educational expense and he is completely free to choose the school at which he uses it, provided that it satisfies certain standards."
Proponents claim that frequently institutions are forced to operate at higher efficiencies when they are allowed to compete and that any resulting job losses in the public sector would be offset by the increased demand for jobs in the private sector.
Friedrich von Hayek on the privatizing of education:
As has been shown by Professor Milton Friedman (M. Friedman, The role of government in education, 1955), it would now be entirely practicable to defray the costs of general education out of the public purse without maintaining government schools, by giving the parents vouchers covering the cost of education of each child which they could hand over to schools of their choice. It may still be desirable that government directly provide schools in a few isolated communities where the number of children is too small (and the average cost of education therefore too high) for privately run schools. But with respect to the great majority of the population, it would undoubtedly be possible to leave the organization and management of education entirely to private efforts, with the government providing merely the basic finance and ensuring a minimum standard for all schools where the vouchers could be spent. (F. A. Hayek, in his 1960 book The Constitution of Liberty, section 24.3)
Other notable supporters include Newark Mayor Cory Booker, former Governor of South Carolina Mark Sanford, billionaire and American philanthropist John T. Walton, Former Mayor of Baltimore Kurt L. Schmoke, Former Massachusetts Governor Mitt Romney and John McCain.
Another prominent proponent of the voucher system was Apple co-founder and CEO, Steve Jobs, who said:
The problem is bureaucracy. I'm one of these people who believes the best thing we could ever do is go to the full voucher system.
I have a 17-year-old daughter who went to a private school for a few years before high school. This private school is the best school I've seen in my life. It was judged one of the 100 best schools in America. It was phenomenal. The tuition was $5,500 a year, which is a lot of money for most parents. But the teachers were paid less than public school teachers - so it's not about money at the teacher level. I asked the state treasurer that year what California pays on average to send kids to school, and I believe it was $4,400. While there are not many parents who could come up with $5,500 a year, there are many who could come up with $1,000 a year.
If we gave vouchers to parents for $4,400 a year, schools would be starting right and left. People would get out of college and say, "Let's start a school."—Steve Jobs
, http://www.wired.com/wired/archive/4.02/jobs_pr.html
Some proponents of school vouchers, including the Sutherland Institute and many supporters of the Utah voucher effort, see it as a remedy for the negative cultural impact caused by under-performing public schools, which falls disproportionately on demographic minorities. During the run-up to the November referendum election Sutherland issued a controversial publication: Voucher, Vows, & Vexations. Sutherland called the publication an important review of the history of education in Utah while critics just called it revisionist history. Sutherland then released the subsequent companion article in a law journal as part of an academic conference about school choice.
The Friedman Foundation for Educational Choice, founded by Milton and Rose Friedman in 1996, is a non-profit organization that promotes universal school vouchers and other forms of school choice. In defense of vouchers, it cites empirical research showing that students who were randomly assigned to receive vouchers had higher academic outcomes than students who applied for vouchers but lost a random lottery and did not receive them; and that vouchers improve academic outcomes at public schools, reduce racial segregation, deliver better services to special education students, and do not drain money from public schools.
Opponents.
The main controversy over both school vouchers and education tax credits is that they put public education in direct competition with private education, threatening to reduce and reallocate public school funding to private schools. Proponents of a voucher system are encouraged by private school sector growth as it is their view that private schools are typically more efficient at achieving results at a much lower per pupil cost when compared to public schools. A CATO Institute study of public and private school per pupil spending in Phoenix, Los Angeles, D.C., Chicago, New York City, and Houston found that public schools spend 93% more than estimated median private schools. However, much variation exists in private school spending, so an average of how much "less" private schools spend as compared to public schools can be misleading.
Jonathan Kozol, a former public school teacher and prominent public school reform thinker has called vouchers the "single worst, most dangerous idea to have entered education discourse in my adult life." Other public school teachers and teacher unions have also fought against school vouchers. In the United States, public school teacher unions, most notably the National Education Association (the largest labor union in the USA), argue against the idea of school vouchers for concern that it would erode educational standards and reduce funding, and that giving money to parents who choose to send their child to a religious or other school is unconstitutional; however, the latter issue has been struck down by the Supreme Court case Zelman v. Simmons-Harris, which upheld Ohio's voucher plan in a 5-4 ruling. In contrast, the use of public school funding for vouchers to private schools was upheld by the Louisiana Supreme Court in 2013. In its ruling the Louisiana Supreme Court did not declare vouchers unconstitutional; just the use of money earmarked for public schools via the Louisiana Constitution for funding Louisiana's voucher program. The National Education Association also points out that access to vouchers is just like “a chance in a lottery” where parents had to have luckiness in order to get a space in this program. Since almost all students and their families would like to choose the best schools, those schools, as a result, quickly reach its maximum capacity number for students that state law permits. The major “unlucky families” then have to compete again to look for some other less preferred and competitive schools or give up searching and go back to their assigned local schools.
It is interesting to note that efforts to make vouchers available aren't always noticed by those able to use them. In April 2012, a bill passed in Louisiana that made vouchers available to low-income families whose children attended poorly ranked schools. Under the new law, any student whose household income was sufficiently low (up to about $44,000 for a family of three) that attended a school ranked "C", "D", or "F" could apply for vouchers to attend another school. Of the estimated 380,000 students that were eligible for the vouchers during the 2012-2013 school year (the same year the bill was passed), only 5,000 students knew about and applied for the vouchers, and accepted them.
In 2006, the United States Department of Education released a report concluding that average test scores for reading and mathematics, when adjusted for student and school characteristics, tend to be very similar among public schools and private schools. If results were left unadjusted for factors such as race, gender, and free or reduced price lunch program eligibility, private schools performed significantly better than public schools. Other research questions assumptions that large improvements would result from a more comprehensive voucher system.
Given the limited budget for schools, it is claimed that a voucher system would weaken public schools while at the same time not necessarily providing enough money for people to attend private schools. 76% of the money handed out for Arizona’s voucher program has gone to children already in private schools.
Some sources claim that public schools' higher per pupil spending is due to having a higher proportion of students with behavioral, physical and emotional problems. They argue that some, if not all, of the cost difference between public and private schools comes from a process known as cream skimming—selecting only those students that belong to a preferred economic, religious, or ethnic group—rather than from differences in administration.
In the United States, public schools must by law accept any student regardless of race, gender, religion, disability, etc. Thus, it has been argued that a voucher system would lead students who do not belong to a preferred religious or ethnic group, or those with disabilities, to become concentrated within the public school system. Of the ten state-run voucher programs in the United States at the beginning of 2011, however, four targeted low-income students, two targeted students in failing schools, and six targeted students with special needs. (Note that Louisiana ran a single program targeting all three groups.)
Another argument against the implementation of a school voucher system is its lack of accountability to the taxpayer. In many states, members of a community's board of education are elected by voters. Similarly, a school budget faces a referendum. Meetings of the Board of Education must be announced in advance, and members of the public are permitted to voice their concerns directly to board members. Although vouchers may be used in private and religious schools, taxpayers are not able to vote on budgetary issues, elect members of the board or even attend board meetings. Kevin Welner points out that vouchers funded through a convoluted tax credit system—a policy he calls "Neovouchers"—present additional accountability concerns. With neovoucher systems, a taxpayer owing money to the state instead donates that money to a private, nonprofit organization. That organization then bundles donations and gives them to parents as vouchers to be used for private school tuition. The state then steps in and forgives (through a tax credit) some or all of the taxes that the donor has given to the organization. While conventional tax credit systems are structured to treat all private school participants equally, neovoucher systems effectively delegate to individual private taxpayers (those owing money to the state) the power to decide which private schools will benefit.
An example of lack of accountability is the voucher situation in Louisiana. In 2012, Louisiana State Superintendent of Education John White selected private schools to receive vouchers then attempted to fabricate criteria (including site visits) after schools had already received approval letters. One school of note, New Living Word in Ruston, Louisiana, did not have sufficient facilities for the over-300 students White and the state board of education had approved. Following a voucher audit in 2013, New Living Word had overcharged the state $395,000. White referred to the incident as a "lone substantive issue." 
However, most voucher schools did not undergo a complete audit for not having a separate checking account for state voucher money.
According to Susanne Wiborg, an expert on comparative education, Sweden's voucher system introduced in 1992 has "augmented social and ethnic segregation, particularly in relation to schools in deprived areas."
Tax-credit scholarships which are in most part disbursed to current private school students or to families which made substantial donations to the scholarship fund, rather than to low-income students attempting to escape from failing schools, amount to nothing more than a mechanism to use public funds in the form of foregone taxes to support private, often religiously based, private schools.
Implementations.
Chile.
In Chile, there is an extensive voucher system in which the State pays private and municipal schools directly based on student attendance. This system covers nearly 90% of its students. It was introduced in 1981. Dr. Martin Carnoy of Stanford, Patrick J. McEwan claims that based on his research, when controls for the student's background (parental income and education) are introduced, the difference in performance between public and private subsectors is not significant.
Europe.
In most European countries, education for all primary and secondary schools is fully subsidized. In some countries (e.g. Belgium), parents are free to choose which school their child attends.
Ireland.
Most schools in Ireland are state-aided parish schools, established under diocesan patronage but with capital costs, teachers salaries and a per head fee paid to the school. These are given to the school regardless of whether or not it requires its students to pay fees. (Although fee-paying schools are in the minority, there has been much criticism over the state aid they receive with opponents claiming this gives them an unfair advantage.)
There is a recent trend towards multi-denominational schools established by parents, which are organised as limited companies without share capital. Parents and students are free to choose their own school. In the event of a school failing to attract students it immediately loses its per-head fee and over time loses its teaching posts- and teachers are moved to other schools which are attracting students. The system is perceived to have achieved very successful outcomes for most Irish children.
The 1995-7 Rainbow Coalition (which contained parties of the centre right and the left) introduced free third-level education to primary degree level. Critics of the latter development charge that it has not increased the number of students from economically deprived backgrounds attending university. However, studies have shown that the removal of tuition fees at third level has increased the number of students overall and those from lower socio-economic backgrounds. This concurs with evidence from the UK of a decrease in attendance numbers after the introduction of fees. However, since the economic crisis, there has been extensive talk and debate regarding the reintroduction of third-level fees.
Sweden.
In Sweden, a system of school vouchers (called "skolpeng") were introduced in 1992 at primary and secondary school level, enabling free choice among publicly run schools and privately run "friskolor" ("free schools"). The voucher is paid with public funds from the local municipality ("kommun") directly to a school based solely on its number of students. Both public schools and free schools are funded the same way. Free schools can be run by not-for-profit groups as well as by for-profit companies, but may not charge top-up fees or select students other than on a first-come-first-serve basis. Over 10% of Swedish pupils were enrolled in free schools in 2008 and the number is growing fast, leading the country to be viewed as a pioneer of the model.
Per Unckel, governor of Stockholm and former Minister of Education, has promoted the system, saying "Education is so important that you can’t just leave it to one producer, because we know from monopoly systems that they do not fulfill all wishes." The Swedish system has been recommended to Barack Obama by some commentators,
including the Pacific Research Institute, which has released a documentary called "Not As Good As You Think: Myth of the Middle Class Schools", a movie depicting positive benefits for middle class schools resulting from Sweden's voucher programs.
A 2004 study concluded that school results in public schools improved due to the increased competition. However, Per Thulberg, director general of the Swedish National Agency for Education, has said that the system "has not led to better results" and in the 2000s Sweden's ranking in the PISA league tables worsened.
Hong Kong.
A voucher system for children three to six years-old who attend a non-profit kindergarten was implemented in Hong Kong in 2007. Each child will get HK$13,000 pa. The $13,000 subsidy will be separated into two parts. $10,000 is used to subsidize the school fee and the remaining $3,000 is used for kindergarten teachers to pursue further education and obtain a certificate in Education. Also, there are some restrictions on the voucher system. Parents can only choose those non-profit making with yearly fee less than $24,000. The government hopes that all kindergarten teachers can obtain an Education certificate by the year 2011-2012, at which point the subsidies are to be adjusted to $16000 for each student, all of which will go toward the school fee.
Milton Friedman criticised the system, saying "I do not believe that CE Mr. Tsang's proposal is properly structured." He said that the whole point of a voucher system is to provide a competitive market place so should not be limited to non-profit kindergartens. 
After protests by parents with children enrolled in for profit kindergartens, the program was extended to children in for- profit kindergartens, but only for children enrolled in or before September 2007. The government will also provide up to HK$30,000 subsidy to for profit kindergartens wanting to convert to non profit.
Pakistan.
Dr. Allah Bakhsh Malik Managing Director and Chief Executive of Punjab Education Foundation - PEF introduced Education Voucher Scheme - EVS in Punjab especially in urban slums and poorest of the poor in 2005. The initial study was sponsored by Open Society Institute New York USA. Professor Henry M Levin extended Pro-Bono services for the children of poor families from Punjab.To ensure educational justice and avoid educational apartheid, the government must ensure that the poorest of the poor have equal access to quality education. Only then will future generations be able to escape from the vicious cycle of poverty and deprivation. In collaboration with the Teachers College, Columbia University, and the Open Society Institute, the PEF designed the Education Voucher Scheme (EVS) for the slums (or katchi abadies) in the province of Punjab. The EVS aims to promote freedom of choice, efficiency, equity, and social cohesion. A pilot project was started in 2006 in the urban slums of Sukhnehar, Lahore, where a survey showed that all households were living below the poverty line. Through the EVS, the foundation would deliver education vouchers to every household with children 5–13 years of age. The vouchers would be redeemable against tuition payments at participating private schools. In the pilot stage, 1,053 households were given an opportunity to send their children to a private school of their choice. The EVS makes its partner schools accountable to the parents rather than to the bureaucrats at the Ministry of Education. In the FAS program, every school principal has the choice of admitting a
student or not. In the EVS, however, the decision regarding where a child attends school is up to the parents because they are the ones carrying the education voucher issued by the PEF. The partner schools are also accountable to the PEF: they are subject to periodic reviews of their student learning outcomes, additional private investments, and improvements in working
conditions of the teachers. The EVS provides an incentive to parents to send their children to school, and so it has become a source of competition among private schools seeking to join the program. When it comes to the selection of schools, the following criteria are applied across the board: (i) The fee paid by the PEF to EVS partner schools is PKR 300 per child per month. Schools
charging higher fees can also apply to the program, but they will not be paid more than PKR 450, and they will not be entitled to charge the difference from students’ families. (ii) Total school enrollment should be between 100 and 500 children. (iii) The school should have an adequate infrastructure and a good learning environment. (iv) EVS partner schools should be located within a half-kilometer radius of the residences of voucher holders. However, if the parents prefer a particular school that is farther away, the PEF will have no objection, provided that the school fulfills the EVS selection criteria.
(v) The PEF advertises to stimulate the interest of potential partner schools. It then gives students at short-listed schools preliminary tests in selected subjects, and conducts physical inspections of these schools. PEF offices display a list of all the EVS partner
schools so that parents may consult it and choose a school for their children.
By now more than 140, 000 students are benefiting from EVS and the program is being scaled up by financing from Government of the Punjab.
United States.
In the 1980s, the Reagan administration pushed for vouchers, as did the George W. Bush administration in the initial education-reform proposals leading up to the No Child Left Behind Act. In 2011, it was estimated that nearly 171,000 students would participate in 18 existing school choice programs in 10 states (Vermont, Maine, New Mexico and 7 others) and the District of Columbia. Most of these programs were offered to students in low-income families, low performing schools, or special-education programs. By 2014, the number participating in either vouchers or tax-credit scholarships increased to 250,000, a 30% increase from 2010, but still a small fraction compared to the 55 million in traditional schools.
In 1990, the city of Milwaukee, Wisconsin's public schools were the first to offer vouchers and has nearly 15,000 students using vouchers as of 2011. The 2006-2007 school year marked the first time in Milwaukee that more than $100 million was paid in vouchers. Twenty-six percent of Milwaukee students will receive public funding to attend schools outside the traditional Milwaukee Public School system. In fact, if the voucher program alone were considered a school district, it would mark the sixth-largest district in Wisconsin. St. Anthony Catholic School, located on Milwaukee's south side, boasts 966 voucher students, meaning that it very likely receives more public money for general school support of a parochial elementary or high school than any before it in American history. Under the current state formula for paying school vouchers, however, Milwaukee residents pay more in property taxes for voucher students than for students attending public schools. This imbalance has received considerable criticism, and is the subject of 2007 legislative proposals designed to alter the formula.
Recent analysis of the competitive effects of school vouchers in Florida suggests that more competition improves performance in the regular public schools.
Legal challenges.
The school voucher question in the United States has also received a considerable amount of judicial review in the early 2000s.
A program launched in the city of Cleveland in 1995 and authorized by the state of Ohio was challenged in court on the grounds that it violated both the federal constitutional principle of separation of church and state and the guarantee of religious liberty in the Ohio Constitution. These claims were rejected by the Ohio Supreme Court, but the federal claims were upheld by the local federal district court and by the Sixth Circuit appeals court. The fact that nearly all of the families using vouchers attended Catholic schools in the Cleveland area was cited in the decisions.
This was later reversed during 2002 in a landmark case before the US Supreme Court, Zelman v. Simmons-Harris, in which the divided court, in a 5-4 decision, ruled the Ohio school voucher plan constitutional and removed any constitutional barriers to similar voucher plans in the future, with moderate justices Anthony Kennedy and Sandra Day O'Connor and conservative justices William Rehnquist, Antonin Scalia, and Clarence Thomas in the majority.
Chief Justice William Rehnquist, writing for the majority, stated that "The incidental advancement of a religious mission, or the perceived endorsement of a religious message, is reasonably attributable to the individual aid recipients not the government, whose role ends with the disbursement of benefits." The Supreme Court ruled that the Ohio program did not violate the Establishment Clause, because it passed a five-part test developed by the Court in this case, titled the Private Choice Test.
Dissenting opinions included Justice Stevens's, who wrote "...the voluntary character of the private choice to prefer a parochial education over an education in the public school system seems to me quite irrelevant to the question whether the government's choice to pay for religious indoctrination is constitutionally permissible." and Justice Souter's, whose opinion questioned how the Court could keep "Everson v. Board of Education" on as precedent and decide this case in the way they did, feeling it was contradictory. He also found that religious instruction and secular education could not be separated and this itself violated the Establishment Clause.
In 2006, the Florida Supreme Court struck down legislation known as the Florida Opportunity Scholarship Program (OSP), which would have implemented a system of school vouchers in Florida. The court ruled that the OSP violated article IX, section 1(a) of the Florida Constitution: "Adequate provision shall be made by law for a uniform, efficient, safe, secure, and high quality system of free public schools." This decision was criticized by Clark Neily, Institute for Justice senior attorney and legal counsel to Pensacola families using Florida Opportunity Scholarships, as, "educational policymaking."
Political support.
Political support for school vouchers in the United States is mixed. On the left/right spectrum, conservatives are more likely to support vouchers. Some state legislatures have enacted voucher laws. In New Mexico, Libertarian Gary Johnson made school voucher provision the major issue of his second term as Governor. As of 2006, the federal government operates the largest voucher program, for evacuees from the region affected by Hurricane Katrina. The Federal government provided a voucher program for 7,500 residents of Washington, D.C. - the D.C. Opportunity Scholarship Program. until in early March 2009 congressional Democrats were moving to close down the program and remove children from their voucher-funded school places at the end of the 09/10 school year under the $410 billion Omnibus Appropriations Act of 2009 which, as of March 7 had passed the House and was pending in the Senate. The Obama administration stated that it preferred to allow children already enrolled in the program to finish their schooling while closing the program to new entrants. However, its preference on this matter does not appear to be strong enough to prevent the President from signing the Bill.
Whether or not the public generally supports vouchers is debatable. Majorities seem to favor improving existing schools over providing vouchers, yet as many as 40% of those surveyed admit that they do not know enough to form an opinion or do not understand the system of school vouchers.
In November 2000, a voucher system proposed by Tim Draper was placed on the California ballot as Proposition 38. It was unusual among school voucher proposals in that it required neither accreditation on the part of schools accepting vouchers, nor proof of need on the part of families applying for them; neither did it have any requirement that schools accept vouchers as payment-in-full, nor any other provision to guarantee a reduction in the real cost of private school tuition. The measure was defeated by a final percentage tally of 70.6 to 29.4.
A state-wide universal school voucher system providing a maximum tuition subsidy of $3000 was passed in Utah in 2007, but 62% of voters repealed it in a statewide referendum before it took effect. On April 27, 2011 Indiana passed a statewide voucher program, the largest in the U.S. It offers up to $4,500 to students with househould incomes under $41,000, and lesser benefits to households with higher incomes. The vouchers can be used to fund a variety of education options outside the public school system. In March 2013, the Indiana Supreme Court found that the program does not violate the state constitution.
Teaching creationism instead of evolution.
Some schools in voucher programs teach creationism instead of the theory of evolution, including religious schools that teach religious theology side-by-side with or in place of science. Over 300 schools in the USA have been documented as teaching creation and receive taxpayer money. Contrary to public belief, this was deemed constitutional in Zelman v. Simmons-Harris.

</doc>
<doc id="9751" url="http://en.wikipedia.org/wiki?curid=9751" title="E. B. White">
E. B. White

Elwyn Brooks "E. B." White (July 11, 1899 – October 1, 1985) was an American writer. He was a contributor to "The New Yorker" magazine and a co-author of the English language style guide "The Elements of Style", which is commonly known as "Strunk & White". He also wrote books for children, including "Stuart Little" (1945), "Charlotte's Web" (1952), and "The Trumpet of the Swan" (1970). "Charlotte's Web" was voted the top children's novel in a 2012 survey of "School Library Journal" readers, an accomplishment repeated in earlier surveys.
Life.
White was born in Mount Vernon, New York, the youngest child of Samuel Tilly White, the president of a piano firm, and Jessie Hart White, the daughter of Scottish-American painter William Hart. He served in the army before going to college. White graduated from Cornell University with a Bachelor of Arts degree in 1921. He picked up the nickname "Andy" at Cornell University, where tradition confers that moniker on any male student surnamed White, after Cornell co-founder Andrew Dickson White. While at Cornell, he worked as editor of "The Cornell Daily Sun" with classmate Allison Danzig, who later became a sportswriter for "The New York Times". White was also a member of the Aleph Samach and Quill and Dagger societies and Phi Gamma Delta (FIJI).
White worked for the United Press (currently the United Press International) and the American Legion News Service in 1921 and 1922, and then became a reporter for "The Seattle Times" in 1922 and 1923. He then worked for two years with the Frank Seaman advertising agency as a production assistant and copywriter before returning to New York City in 1924. Not long after "The New Yorker" was founded in 1925, White submitted manuscripts to it. Katharine Angell, the literary editor, recommended to magazine editor and founder Harold Ross that White be taken on as staff. However, it took months to convince him to come to a meeting at the office and further weeks to convince him to agree to work on the premises. Eventually he agreed to work in the office on Thursdays.
A few years later in 1929, White and Angell were married. They had a son, Joel White, a naval architect and boat builder, who owned Brooklin Boat Yard in Brooklin, Maine. Katharine's son from her first marriage, Roger Angell, has spent decades as a fiction editor for "The New Yorker" and is well known as the magazine's baseball writer.
James Thurber described author E.B White as being a quiet man, disliking publicity, who during his time at "The New Yorker" would slip out of his office via the fire escape to a nearby branch of Schrafft's to avoid visitors whom he didn't know. Most of us, out of a politeness made up of faint curiosity and profound resignation, go out to meet the smiling stranger with a gesture of surrender and a fixed grin, but White has always taken to the fire escape. He has avoided the Man in the Reception Room as he has avoided the interviewer, the photographer, the microphone, the rostrum, the literary tea, and the Stork Club. His life is his own. He is the only writer of prominence I know of who could walk through the Algonquin lobby or between the tables at Jack and Charlie's and be recognized only by his friends.
 — James Thurber, "E. B. W.", "Credos and Curios"
White died on October 1, 1985, suffering from Alzheimer's disease, at his farm home in North Brooklin, Maine. He is buried in the Brooklin Cemetery beside his wife Katharine, who died in 1977.
Career.
He published his first article in "The New Yorker" magazine in 1925, then joined the staff in 1927 and continued to contribute for around six decades. Best recognized for his essays and unsigned "Notes and Comment" pieces, he gradually became the most important contributor to "The New Yorker" at a time when it was arguably the most important American literary magazine. From the beginning to the end of his career at "The New Yorker," he frequently provided what the magazine calls "Newsbreaks" (short, witty comments on oddly worded printed items from many sources) under various categories such as "Block That Metaphor". He also served as a columnist for "Harper's Magazine" from 1938 to 1943.
In 1949, White published "Here Is New York", a short book based upon a "Holiday" magazine article that he had been asked to write. The article reflects the writer's appreciation of a city that provides its residents with both "the gift of loneliness and the gift of privacy", and concludes with a dark note touching upon the forces that may destroy the city that the writer loves. This prescient "love letter" to the city was re-published in 1999 on the one hundredth anniversary of his birth, with an introduction by his stepson, Roger Angell.
In 1959, White edited and updated "The Elements of Style". This handbook of grammatical and stylistic guidance for writers of American English had been written and published in 1918 by William Strunk, Jr., one of White's professors at Cornell. White's rework of the book was extremely well received, and further editions of the work followed in 1972, 1979, and 1999; an illustrated edition followed in 2005. The illustrator, Maira Kalman, is a contributor to "The New Yorker". That same year, a New York composer named Nico Muhly premiered a short opera based on the book. The volume is a standard tool for students and writers and remains required reading in many composition classes. The complete history of "The Elements of Style" is detailed in Mark Garvey's "Stylized: A Slightly Obsessive History of Strunk & White's The Elements of Style".
In 1978, White won a special Pulitzer Prize citing "his letters, essays and the full body of his work". Other awards he received included a Presidential Medal of Freedom in 1963 and memberships in a variety of literary societies throughout the United States.
The 1973 Canadian animated short, "The Family That Dwelt Apart", is narrated by White and based on his short story of the same name.
Children's books.
In the late 1930s, White turned his hand to children's fiction on behalf of a niece, Janice Hart White. His first children's book, "Stuart Little," was published in 1945, and "Charlotte's Web" appeared in 1952. "Stuart Little" initially received a lukewarm welcome from the literary community. However, both books went on to receive high acclaim.
White received the Laura Ingalls Wilder Medal from the U.S. professional children's librarians in 1970, recognizing his "substantial and lasting contributions to children's literature". At the time it was awarded every five years. That year he was also the U.S. nominee and a highly commended runner-up for the biennial, international Hans Christian Andersen Award, as he was again in 1976. Also in 1970, White's third children's novel was published, "The Trumpet of the Swan". In 1973 it won the Sequoya Award from Oklahoma and the William Allen White Award from Kansas, both selected by students voting for their favorite book of the year.
In 2012, "School Library Journal" sponsored a survey of readers which identified "Charlotte's Web" as top children's novel ("fictional title for readers 9–12" years old). The librarian who conducted it observed that "it is impossible to conduct a poll of this sort and expect [the novel] to be anywhere but #1".

</doc>
<doc id="9752" url="http://en.wikipedia.org/wiki?curid=9752" title="Evangelist (Latter Day Saints)">
Evangelist (Latter Day Saints)

In the Latter Day Saint movement, an evangelist is an ordained office of the ministry. In some denominations of the movement, an evangelist is referred to as a patriarch (see Patriarch (Latter Day Saints)). However, the latter term was deprecated by the Community of Christ after the church began ordaining women to the priesthood. Other denominations, such as The Church of Jesus Christ (Bickertonite), have an evangelist position independent of the original "patriarch" office instituted by the movement's founder Joseph Smith, Jr.
Early Latter Day Saint movement.
The first references to the term "evangelist" in Latter Day Saint theology were mainly consistent with how the term is used by Protestants and Catholics.
In 1833, Joseph Smith, Jr. introduced the new office of Patriarch, to which he ordained his father. The elder Smith was given the "keys of the patriarchal Priesthood over the kingdom of God on earth", the same power said to be held by the Biblical Patriarchs, which included the power to give blessings upon one's posterity. The elder Smith, however, was also called to give Patriarchal blessings to the fatherless within the church, and the church as a whole, a calling he passed onto his eldest surviving son Hyrum Smith prior to his death. Hyrum himself was killed in 1844 along with Joseph, resulting in a Succession crisis that broke the Latter Day Saint movement into several smaller denominations.
It is not known who first identified the term "evangelist" with the office of "patriarch". However, in an 1835 church publication, W. W. Phelps stated, 
Joseph Smith also identified the term "evangelist" with the office of "patriarch" in 1839, stating that "an Evangelist is a Patriarch".
The necessity of an "evangelist" office in the church organization has been reinforced repeatedly, based on the passage in Ephesians 4:11, which states, "And he gave some, apostles; and some, prophets; and some, evangelists; and some, pastors and teachers". In 1834, while writing what he called the "principles of salvation", prominent early Mormon co-founder Oliver Cowdery stated that:
Joseph Smith echoed Cowdery's statement in 1842, in a letter to a Chicago newspaper editor outlining the church's basic beliefs. Smith said that his religion "believe[s] in the same organization that existed in the primitive church, viz: apostles, prophets, pastors, teachers, evangelists".
Community of Christ.
In the Community of Christ, which was formerly known as the Reorganized Church of Jesus Christ of Latter Day Saints, an evangelist is an office in the Melchizedec Order of the priesthood. The evangelist was originally called an evangelist–patriarch. This name derived from Latter Day Saint founder Joseph Smith's statement that "“an Evangelist is a Patriarch. … Wherever the Church of Christ is established in the earth, there should be a Patriarch for the benefit of the posterity of the Saints”.
An evangelist–patriarch's primary responsibility was to provide special blessings to members of the church; these blessings were considered one of the eight sacraments in the RLDS Church. The local Evangelist–Patriarchs of the church were governed by an individual with church-wide authority known as the Presiding Patriarch.
In 1984, when the first women began to be ordained to the office of evangelist–patriarch, the RLDS Church changed the title of the local Evangelist–Patriarchs to simply "evangelist". Similarly, it changed the title of the Presiding Patriarch to the "Presiding Evangelist". To be an evangelist, a person must already be a high priest of the Melchizedec Order of the priesthood.
The primary duty of an evangelist in the Community of Christ remains the giving of sacramental "evangelist's blessings"; it is for this reason that evangelists are often referred to as "ministers of blessing". Ideally, an evangelist is free from administrative responsibilities in the church in order to allow them to be fully responsive to the Holy Spirit. Their blessings—which are given by the laying on of hands—provide counsel and advice and confer spiritual blessings upon the recipient. Evangelist's blessings may or may not be recorded. If it is recorded, a copy is stored in the church archives at Independence, Missouri. A recipient may receive multiple evangelist's blessings in their life.
All evangelists belong to the Order of Evangelists, which is directed by the Presiding Evangelist (currently David R. Brock, since 2007).
The Church of Jesus Christ (Bickertonite).
In The Church of Jesus Christ (Bickertonite), the prescribed duties of an evangelist are to preach the gospel of Jesus Christ to every nation, kindred, language, and people. An evangelist is part of the Quorum of Seventy Evangelists.
Quorum of Seventy Evangelists.
The Quorum of Seventy Evangelists is responsible for management of the International Missionary Programs of the church and assists Regions of the church with their individual Domestic Missionary Programs. The Quorum of Seventy oversees the activities of its Missionary Operating Committees to ensure the fulfilling of Christ’s commandment to take the gospel to the entire world.
In 2007, the officers of the Quorum of Seventy Evangelists were:
The Church of Jesus Christ of Latter-day Saints.
In The Church of Jesus Christ of Latter-day Saints, an evangelist is considered to be an office of the Melchizedek priesthood. However, the term "evangelist" is rarely used for this position, the church retaining the term "Patriarch", the term most commonly used in the days of founder Joseph Smith, Jr.
The most prominent reference to the term "evangelist" in the denomination's literature is found in its Articles of Faith, derived from the Wentworth letter, a statement by Joseph Smith in 1842 to a Chicago newspaper editor that the church believes in "the same organization that existed in the primitive church", including "evangelists". Although Joseph Smith said that "an Evangelist is a Patriarch", the church has retained the term "Patriarch".

</doc>
<doc id="9755" url="http://en.wikipedia.org/wiki?curid=9755" title="Elegiac couplet">
Elegiac couplet

The elegiac couplet is a poetic form used by Greek lyric poets for a variety of themes usually of smaller scale than the epic. Roman poets, particularly Ovid, adopted the same form in Latin many years later. As with the English heroic, each couplet usually makes sense on its own, while forming part of a larger work.
Each couplet consist of a hexameter verse followed by a pentameter verse. The following is a graphic representation of its scansion. Note that - is a long syllable, u a short syllable, and U is either one long syllable or two short syllables:
The form was felt by the ancients to contrast the rising action of the first verse with a falling quality in the second. The sentiment is summarized in a line from Ovid's Amores I.1.27 "Sex mihi surgat opus numeris, in quinque residat" - "Let my work rise in six steps, fall back in five." The effect is illustrated by Coleridge as:
translating Schiller,
Greek origins.
The elegiac couplet is presumed to be the oldest Greek form of epodic poetry (a form where a later verse is sung in response or comment to a previous one). Scholars, who even in the past did not know who created it, theorize the form was originally used in Ionian dirges, with the name "elegy" derived from the Greek "ε, λεγε ε, λεγε" - "Woe, cry woe, cry!" Hence, the form was used initially for funeral songs, typically accompanied by an aulos, a double-reed instrument. Archilochus expanded use of the form to treat other themes, such as war, travel, or homespun philosophy. Between Archilochus and other imitators, the verse form became a common poetic vehicle for conveying any strong emotion.
At the end of the 7th century BCE, Mimnermus of Colophon struck on the innovation of using the verse for erotic poetry. He composed several elegies celebrating his love for the flute girl Nanno, and though fragmentary today his poetry was clearly influential in the later Roman development of the form. Propertius, to cite one example, notes "Plus in amore valet Mimnermi versus Homero" - "The verse of Mimnermus is stronger in love than Homer".
The form continued to be popular throughout the Greek period and treated a number of different themes. Tyrtaeus composed elegies on a war theme, apparently for a Spartan audience. Theognis of Megara vented himself in couplets as an embittered aristocrat in a time of social change. Popular leaders were writers of elegy—Solon the lawgiver of Athens composed on political and ethical subjects—and even Plato and Aristotle dabbled with the meter. 
By the Hellenistic period, the Alexandrian school made elegy its favorite and most highly developed form. They preferred the briefer style associated with elegy in contrast to the lengthier epic forms, and made it the singular medium for short epigrams. The founder of this school was Philitas of Cos. He was eclipsed only by the school's most admired exponent, Callimachus; their learned character and intricate art would have a heavy influence on the Romans.
Roman elegy.
Like all Greek forms, elegy was adapted by the Romans for their own literature. The fragments of Ennius contain a few couplets, and scattered verses attributed to Roman public figures like Cicero and Julius Caesar also survive.
But it is the elegists of the mid-to-late first century BCE who are most commonly associated with the distinctive Roman form of the elegiac couplet. Catullus, the first of these, is an invaluable link between the Alexandrine school and the subsequent elegies of Tibullus and Propertius a generation later. His collection, for example, shows a familiarity with the usual Alexandrine style of terse epigram and a wealth of mythological learning, while his 66th poem is a direct translation of Callimachus' "Coma Berenices". Arguably the most famous elegiac couplet in Latin is his two-line 85th poem "Odi et Amo":
Many people, particularly students of Latin, who read this poem aloud often miss the metre because of the high amount of elision in this poem.
Cornelius Gallus is another important statesman/writer of this period, one who was generally regarded by the ancients as the greatest of the elegists. Other than a few scant lines, all of his work has been lost.
Elegy in the Augustan Age.
The form reached its zenith with the collections of Tibullus, Propertius, and several collections of Ovid (the "Amores, Heroides, Tristia", and "Epistulae ex Ponto"). The vogue of elegy during this time is seen in the so-called 3rd and 4th book of Tibullus. Many poems in these books were clearly not written by Tibullus but by others, perhaps part of a circle under Tibullus' patron Mesalla. Notable in this collection are the poems of Sulpicia, the only surviving Latin literature written by a woman.
Through these poets—and in comparison with the earlier Catullus—it is possible to trace specific characteristics and evolutionary patterns in the Roman form of the verse:
Post-Augustan writers.
Although no classical poet wrote collections of love elegies after Ovid, the verse retained its popularity as a vehicle for popular occasional poetry. Elegiac verses appear, for example, in Petronius' "Satyricon", and Martial's Epigrams uses it for many witty stand-alone couplets and for longer pieces. The trend continues through the remainder of the empire; short elegies appear in Apuleius's story "Psyche and Cupid" and the minor writings of Ausonius.
Medieval elegy.
After the fall of the empire, one writer who produced elegiac verse was Maximianus. Various Christian writers also adopted the form; Venantius Fortunatus wrote some of his hymns in the meter, while later Alcuin and the Venerable Bede dabbled in the verse. The form also remained popular among the educated classes for gravestone epitaphs; many such epitaphs can be found in European cathedrals. 
"De tribus puellis" is an example of a Latin "fabliau", a genre of comedy which employed elegiac couplets in imitation of Ovid. The medieval theorist John of Garland wrote that "all comedy is elegy, but the reverse is not true." Medieval Latin had a developed comedic genre known as elegiac comedy. Sometimes narrative, sometimes dramatic, it deviated from ancient practice because, as Ian Thompson writes, "no ancient drama would ever have been written in elegiacs."
Renaissance and modern period.
With the Renaissance, more skilled writers interested in the revival of Roman culture took on the form in a way which attempted to recapture the spirit of the Augustan writers. The Dutch Latinist Johannes Secundus, for example, included Catullus-inspired love elegies in his "Liber Basiorum", while the English poet John Milton wrote several lengthy elegies throughout his career. This trend continued down through the Recent Latin writers, whose close study of their Augustan counterparts reflects their general attempts to apply the cultural and literary forms of the ancient world to contemporary themes.

</doc>
<doc id="9756" url="http://en.wikipedia.org/wiki?curid=9756" title="Exabyte">
Exabyte

The exabyte is a multiple of the unit byte for digital information. The prefix "exa" indicates multiplication by the sixth power of 1000 (1018) in the International System of Units (SI). Therefore one exabyte is one quintillion bytes (short scale). The symbol for the exabyte is EB.
A related unit, the exbibyte, using a binary prefix, is , about 15% larger.
Usage examples and size comparisons.
Several filesystems use disk formats that support theoretical volume sizes of several exabytes, including Btrfs, XFS, ZFS, exFAT, NTFS, HFS Plus, and ReFS.
All words ever spoken.
A popular expression claims that "all words ever spoken by human beings" could be stored in approximately 5 exabytes of data (although this project is now outdated and therefore not entirely accurate), often citing a project at the UC Berkeley School of Information in support. The 2003 University of California Berkeley report credits the estimate to the website of Caltech researcher Roy Williams, where the statement can be found as early as May 1999. This statement has been criticized. Mark Liberman calculated the storage requirements for all human speech at 42 zettabytes (42,000 exabytes, and 8,400 times the original estimate), if digitized as 16 kHz 16-bit audio, although he did freely confess that "maybe the authors [of the exabyte estimate] were thinking about text".
Earlier studies from the University of California, Berkeley, estimated that by the end of 1999, the sum of human-produced information (including all audio, video recordings, and text/books) was about 12 exabytes of data. The 2003 Berkeley report stated that in 2002 alone, "telephone calls worldwide on both landlines and mobile phones contained 17.3 exabytes of new information if stored in digital form" and that "it would take 9.25 exabytes of storage to hold all U.S. [telephone] calls each year". International Data Corporation estimates that approximately 160 exabytes of digital information were created, captured, and replicated worldwide in 2006. Research from University of Southern California estimates that the amount of data stored in the world by 2007 was 295 exabytes and the amount of information shared on two-way communications technology, such as cell phones in 2007 as 65 exabytes.
Library of Congress.
The content of the Library of Congress is commonly estimated to hold 10 terabytes of data in all printed material. Recent estimates of the size including audio, video, and digital materials start at 3 petabytes to 20 petabytes. Therefore, one exabyte could hold a hundred thousand times the printed material, or 500 to 3000 times all content of the Library of Congress.
Google.
In 2013, Randall Munroe compiled published facts about Google's data centers, and estimated that the company has about 10 exabytes stored on disk, and additionally approximately 5 exabytes on tape backup. The company has refused to comment on Munroe's estimate."

</doc>
<doc id="9758" url="http://en.wikipedia.org/wiki?curid=9758" title="Era">
Era

An era is a span of time marked by character, events, changes on earth, etc. When used in science, for example geology, an "era" denotes a clearly defined period of time of arbitrary but well-defined length, such as for example the Mesozoic Era from 252 Ma–66 Ma, delimited by a start event and an end event. When used in social history, eras may for example denote a period of some monarch's reign. In colloquial language, eras denote longer spans of time, before and after which the practices or fashions change to a significant degree. When era is extended to a calendar system, it is known as a calendar era. In Sanskrit or Indian culture eras are known as Yugas.
Uses.
In chronology, an era is the highest level for the organization of the measurement of time. A calendar era indicates a span of many years which are numbered beginning at a specific reference date (epoch), which often marks the origin of a political state or cosmology, dynasty, ruler, the birth of a leader, or another significant historical or mythological event; it is generally called after its focus accordingly as in "Victorian era".
Geological era.
In natural science, there is need for another time perspective, independent from human activity, and indeed spanning a far longer period (mainly prehistoric), where "geologic era" refers to well-defined time spans. The next-larger division of geologic time is the eon. The Phanerozoic Eon, is subdivided into eras. There are currently three eras defined in the Phanerozoic; the following table lists them from youngest to oldest (BP is an abbreviation for "before present").
The older Proterozoic and Archean eons are also divided into eras.
Cosmological era.
In astronomy the periods are even longer, to cover the entire existence of the universe (in the order of 13.8 billion years), but usually just denoted in numerical units, as there is no significant link to any earthly reality, our planet being astronomically insignificant (except as the only known observation point).
Calendar eras.
Calendar eras count the years since a particular date (epoch), often one with religious significance. Anno mundi ("year of the world") refers to a group of calendar eras based on a calculation of the age of the world, assuming it was created as described in the Book of Genesis. In Jewish religious contexts one of the versions is still used, and many Eastern Orthodox religious calendars used another version until 1728. Hebrew year 5772 AM began at sunset on 28 September 2011 and ended on 16 September 2012. In the Western church Anno Domini (=AD = CE), counting the years since the birth of Jesus on traditional calculations, was always dominant. 
The Islamic calendar, which also has variants, counts years from the Hijra or emigration of the Islamic prophet Muhammad from Mecca to Medina, which occurred in 622 CE. The Islamic year is some days shorter than 365; January 2012 fell in 1433 AH ("After Hijra").
For a time ranging from 1872 to the Second World War, the Japanese used the imperial year system ("kōki"), counting from the year when the legendary Emperor Jimmu founded Japan which occurred in 660 BC.
Many Buddhist calendars count from the death of the Buddha, which according to the most commonly used calculations was in 545-543 BCE or 483 BCE. Dates are given as "BE" for "Buddhist Era"; 2000 CE was 2543 BE in the Thai solar calendar.
Other calendar eras of the past counted from political events, such as the Seleucid era and the Ancient Roman "ab urbe condita" ("AUC"), counting from the foundation of the city.
Regnal eras.
The word era also denotes the units used under a different, more arbitrary system where time is not represented as an endless continuum with a single reference year, but each unit starts counting from one again, as if time starts again. The use of regnal years is a rather impractical system, and a challenge for historians if a single piece of the historical chronology is missing, and often reflects the preponderance in public life of an absolute ruler in many ancient cultures. Such traditions sometimes outlive the political power of the throne, and may even be based on mythological events or rulers who may not have existed (for example Rome numbering from the rule of Romulus and Regulus). In a manner of speaking the use of the supposed date of the birth of Christ as a base year is a form of a Regnal era.
In East Asia, each emperor's reign may be subdivided into several reign periods, each being treated as a new era. The name of each was a motto or slogan chosen by the emperor. Different East Asian countries utilized slightly different systems, notably:
A similar practice survived in the United Kingdom until quite recently, but only for formal official writings: in daily life the ordinary year A.D. has been used for a long time, but Acts of Parliament were dated according to the years of the reign of the current Monarch, so that "61 & 62 Vict c. 37" refers to the Local Government (Ireland) Act 1898 passed in the session of Parliament in the 61st/62nd year of the reign of Queen Victoria.
Colloquial use.
In common speech and various contexts, the term era is also used, by extension, for any relatively long period in history with a name, often relating to common characteristic(s), even if this is not the normal way to organise time. The most relevant type are politic periods, for example: the Roman era, the Elizabethan era, the Victorian era (dynastic criteria, only formally correct within the British realm/empire/Commonwealth) and the Soviet era, or comparable literary notions like the Biblical era. 
The word era is also popularly used to denote the passing of, often shorter, periods that are only defined in terms of a specific discipline or sphere of life, such as the prominence of an artistic style, or more specifically in music, see , described in History of music, such as the Big Band era, Disco era. An event such as the death of Frank Sinatra is poetically called the "end of an era".
Etymology.
The word has been in use in English since 1615, and is derived from Late Latin "aera" "an era or epoch from which time is reckoned," probably identical to Latin "æra" "counters used for calculation," plural of "æs" "brass, money".
The Latin word use in chronology seems to have begun in 5th century Visigothic Spain, where it appears in the "History" of Isidore of Seville, and in later texts. The Spanish era is calculated from 38 BC, perhaps because of a tax (cfr. indiction) levied in that year, or due to a miscalculation of the Battle of Actium, which occurred in 31 BC. 
Like epoch, "era" in English originally meant "the starting point of an age"; the meaning "system of chronological notation" is c.1646; that of "historical period" is 1741.

</doc>
<doc id="9760" url="http://en.wikipedia.org/wiki?curid=9760" title="Eschatology">
Eschatology

Eschatology is a part of theology concerned with the final events of history, or the ultimate destiny of humanity. This concept is commonly referred to as the "end of the world" or "end time".
The word arises from the Greek ἔσχατος "eschatos" meaning "last" and "-logy" meaning "the study of", first used in English around 1550. The Oxford English Dictionary defines eschatology as "The department of theological science concerned with ‘the four last things: death, judgment, heaven and hell’." 
In the context of mysticism, the phrase refers metaphorically to the end of ordinary reality and reunion with the Divine. In many religions it is taught as an existing future event prophesied in sacred texts or folklore. More broadly, eschatology may encompass related concepts such as the Messiah or Messianic Age, the end time, and the end of days.
History is often divided into "ages" (aeons), which are time periods each with certain commonalities. One age comes to an end and a new age or world to come, where different realities are present, begins. When such transitions from one age to another are the subject of eschatological discussion, the phrase, "end of the world", is replaced by "end of the age", "end of an era", or "end of life as we know it". Much apocalyptic fiction does not deal with the "end of time" but rather with the end of a certain period of time, the end of life as it is now, and the beginning of a new period of time. It is usually a crisis that brings an end to current reality and ushers in a new way of living, thinking, or being. This crisis may take the form of the intervention of a deity in history, a war, a change in the environment, or the reaching of a new level of consciousness.
Most modern eschatology and apocalypticism, both religious and secular, involve the violent disruption or destruction of the world; whereas Christian and Jewish eschatologies view the end times as the consummation or perfection of God's creation of the world. For example, according to ancient Hebrew belief, life takes a linear (and not cyclical) path; the world began with God and is constantly headed toward God’s final goal for creation, which is the world to come.
Eschatologies vary as to their degree of optimism or pessimism about the future. In some eschatologies, conditions are better for some and worse for others, e.g. "heaven and hell".
Eschatology in religions.
Bahá'í.
In Bahá'í belief, creation has neither a beginning nor an end. Instead, the eschatology of other religions is viewed as symbolic. In Bahá'í belief, human time is marked by a series of progressive revelations in which successive messengers or prophets come from God. The coming of each of these messengers is seen as the day of judgment to the adherents of the previous religion, who may choose to accept the new messenger and enter the "heaven" of belief, or denounce the new messenger and enter the "hell" of denial. In this view, the terms "heaven" and "hell" are seen as symbolic terms for the person's spiritual progress and their nearness to or distance from God. In Bahá'í belief, the coming of Bahá'u'lláh, the founder of the Bahá'í Faith, signals the fulfilment of previous eschatological expectations of Islam, Christianity and other major religions.
Buddhism.
Some forms of Buddhism hold a belief in cycles in which the life span of human beings changes according to human nature. In the Cakkavati sutta, the Buddha explained the relationship between life span of human beings and their behaviour. According to this sutta, unwise behavior was unknown among the human race in the past. As a result, people lived for an immensely long time — 80,000 years — endowed with great beauty, wealth, pleasure, and strength. Over the course of time, though, they began behaving in various unwise ways. This caused the human life span gradually to shorten, to the point where it now stands at 100 years, with human beauty, wealth, pleasure, and strength decreasing proportionately.
Ultimately, conditions will deteriorate to the point of a "sword-interval," in which swords appear in the hands of all human beings, and they hunt one another like game. A few people, however, will take shelter in the wilderness to escape the carnage, and when the slaughter is over, they will come out of hiding and resolve to take up a life of wise and virtuous action again. With the recovery of virtue, the human life span will gradually increase again until it reaches 80,000 years, with people attaining sexual maturity at 500.
According to Tibetan Buddhist literature, the age of the first Buddha was 1,000,000 years and his height was 100 cubits while the 28th Buddha, Siddhartha Gautama (563BC–483BC) lived 80 years, and his height was 20 cubits.
In other traditions, such as Zen, a somewhat utilitarian view is taken. The notion often exists that within each moment in time, both birth and death are manifest. As the individual "dies" from moment to moment, they are equally "reborn" in each successive moment, in what one perceives as an ongoing cycle. Thus, the practitioner's focus is shifted from considerations regarding an imagined future endpoint, to mindfulness in the present moment. In this case, the worldview is taken as a functional tool for awakening the practitioner to reality as it exists, right now.
Christianity.
Christian eschatology is concerned with death, an intermediate state, Heaven, hell, the return of Jesus, and the resurrection of the dead. Several evangelical denominations include a rapture, a great tribulation, the Millennium, end of the world, the last judgment, a new heaven and a new earth (the World to Come), and the ultimate consummation of all of God's purposes. Eschatological passages are found in many places, esp. "Isaiah", "Daniel", "Ezekiel", "Matthew 24", "The Sheep and the Goats", and the "Book of Revelation", but "Revelation" often occupies a central place in Christian eschatology.
The Second Coming of Christ is the central event in Christian eschatology. Most Christians believe that death and suffering will continue to exist until Christ's return. There are, however, various views concerning the order and significance of other eschatological events.
The book of "Revelation" is at the core of Christian eschatology. The study of "Revelation" is usually divided into four approaches. In the Futurist approach, "Revelation" is chiefly seen as referring to events which as yet have not come to pass, but which will come to pass at the end of the age, and the end of the world. This is the approach which most applies to eschatological studies. In the Preterist approach, "Revelation" chiefly refers to the events of the first century, such as the struggle of Christianity to survive the persecutions of the Roman Empire, the fall of Jerusalem in 70 AD, and the desecration of the temple in the same year. In the Historicist approach, "Revelation" provides a broad view of history, and passages in "Revelation" are identified with major historical people and events. In the Idealist (or Spiritualist or Symbolic) approach, the events of "Revelation" are neither past nor future, but are purely symbolic, dealing with the ongoing struggle and ultimate triumph of good over evil.
Hinduism.
Contemporary Hindu eschatology is linked in the Vaishnavite tradition to the figure of Kalki, the tenth and last avatar of Vishnu before the age draws to a close who will reincarnate as Shiva simultaneously dissolves and regenerates the universe.
Most Hindus believe that the current period is the Kali Yuga, the last of four "Yuga" that make up the current age. Each period has seen successive degeneration in the moral order, to the point that in the Kali Yuga quarrel and hypocrisy are the norm. In Hinduism, time is cyclic, consisting of cycles or "kalpas". Each kalpa lasts 4.1 - 8.2 billion years, which is a period of one full day and night for Brahma, who in turn will live for 311 trillion, 40 billion years. The cycle of birth, growth, decay, and renewal at the individual level finds its echo in the cosmic order, yet is affected by vagaries of divine intervention in Vaishnavite belief. Some Shaivites hold the view that Shiva is incessantly destroying and creating the world.
After this larger cycle, all of creation will contract to a singularity and then again will expand from that single point, as the ages continue in a religious fractal pattern.
Islam.
Islamic eschatology is documented in the sayings of the Prophet Muhammad, regarding the Signs of the Day of Judgment. The Prophet's sayings on the subject have been traditionally divided into Major and Minor Signs. He spoke about several Minor Signs of the approach of the Day of Judgment, including:
Regarding the Major Signs, a Companion of the Prophet narrated: "Once we were sitting together and talking amongst ourselves when the Prophet appeared. He asked us what it was we were discussing. We said it was the Day of Judgment. He said: 'It will not be called until ten signs have appeared: Smoke, Dajjal (the Antichrist), the creature (that will wound the people), the rising of the sun in the West, the Second Coming of Jesus, the emergence of Gog and Magog, and three sinkings (or cavings in of the earth): one in the East, another in the West and a third in the Arabian Peninsula.'" (note: the previous events were not listed in the chronological order of appearance)
Judaism.
Judaism addresses the end times in the Book of Daniel and numerous other prophetic passages in the Hebrew scriptures, and also in the Talmud, particularly Tractate Avodah Zarah.
Futures studies and transhumanism.
Researchers in futures studies and transhumanism investigate how the accelerating rate of scientific progress may lead to a technological singularity in the 21st century that would profoundly and unpredictably change the course of human history, and result in "Homo sapiens" no longer being the dominant life form on Earth. The philosopher and Institute for Ethics and Emerging Technologies scholar Phil Torres has called this new branch "secular eschatology."
Astronomy.
The Sun will turn into a red giant in approximately 5 billion years. This red giant Sun will have a maximum radius beyond the Earth's current orbit. The Sun's expansion will not lead to the end of the Universe; its effects will be limited to the Solar System. Life on Earth will become impossible due to a rise in temperature long before the planet is actually swallowed up by the Sun.

</doc>
<doc id="9762" url="http://en.wikipedia.org/wiki?curid=9762" title="Ecumenical council">
Ecumenical council

An ecumenical council (or oecumenical council; also general council) is a conference of ecclesiastical dignitaries and theological experts convened to discuss and settle matters of Church doctrine and practice in which those entitled to vote are convoked from the whole world (oikoumene) and which secures the approbation of the whole Church.
The word "ecumenical" derives from the Greek language (ἡ) οἰκουμένη (γῆ), which literally means "the inhabited world", but which was also applied more narrowly to mean the Roman Empire. Bishops belonging to what became known as the Church of the East participated in none of the councils later than the second, and further noteworthy schisms led to non-participation by other members of what had previously been considered a single Christian Church. Later ecumenical councils thus included bishops of only parts of the Church as previously constituted and were rejected or ignored by Christians not belonging to those parts.
The first seven Ecumenical Councils, recognised by both the eastern and western branches of Chalcedonian Christianity, were convoked by Christian Roman Emperors, who also enforced the decisions of those councils within the state church of the Roman Empire.
Acceptance of councils as ecumenical and authoritative varies between different Christian denominations. Disputes over christological and other questions have led certain branches to reject some councils that others accept.
Acceptance of councils by denomination.
The Church of the East (accused by others of adhering to Nestorianism) accepts as ecumenical only the first two councils. Oriental Orthodox Churches accept the first three. Both the Eastern Orthodox Church and Roman Catholic Church recognise as ecumenical the first seven councils, held from the 4th to the 9th century. While the Eastern Orthodox Church accepts no later council or synod as ecumenical, the Roman Catholic Church continues to hold general councils of the bishops in full communion with the Pope, reckoning them as ecumenical. In all, the Roman Catholic Church recognises twenty-one councils as ecumenical. Anglicans and confessional Protestants accept either the first seven or the first four as ecumenical councils.
Infallibility of ecumenical councils.
The doctrine of the "infallibility of ecumenical councils" states that solemn definitions of ecumenical councils, approved by the Pope, which concern faith or morals, and to which the whole Church must adhere are infallible. Such decrees are often labeled as 'Canons' and they often have an attached anathema, a penalty of excommunication, against those who refuse to believe the teaching. The doctrine does not claim that every aspect of every ecumenical council is infallible.
The Roman Catholic Church holds this doctrine, as do most or all Eastern Orthodox theologians. The Eastern Orthodox Church accepts that an ecumenical council is itself infallible when pronouncing on a specific matter.
Protestant churches would generally view ecumenical councils as fallible human institutions that have no more than a derived authority to the extent that they correctly expound Scripture (as most would generally consider occurred with the first four councils in regard to their dogmatic decisions).
Council documents.
Church councils were, from the beginning, bureaucratic exercises. Written documents were circulated, speeches made and responded to, votes taken, and final documents published and distributed. A large part of what is known about the beliefs of heresies comes from the documents quoted in councils in order to be refuted, or indeed only from the deductions based on the refutations.
Most councils dealt not only with doctrinal but also with disciplinary matters, which were decided in "canons" ("laws"). Study of the canons of church councils is the foundation of the development of canon law, especially the reconciling of seemingly contradictory canons or the determination of priority between them. Canons consist of doctrinal statements and disciplinary measures – most Church councils and local synods dealt with immediate disciplinary concerns as well as major difficulties of doctrine. Eastern Orthodoxy typically views the purely doctrinal canons as dogmatic and applicable to the entire church at all times, while the disciplinary canons apply to a particular time and place and may or may not be applicable in other situations.
Circumstances of the first ecumenical councils.
Of the seven councils recognised in whole or in part by both the Roman Catholic and the Eastern Orthodox Church as ecumenical, all were called by the Roman Emperor, The emperor gave them legal status within the entire Roman Empire. All were held in the eastern part of the Roman Empire. The Pope did not attend, although he sent legates to some of them.
Church councils were traditional and the ecumenical councils were a continuation of earlier councils (also known as synods) held in the Empire before Christianity was made legal. These include the Council of Jerusalem (c. 50), the Council of Rome (155), the Second Council of Rome (193), the Council of Ephesus (193), the Council of Carthage (251), the Council of Iconium (258), the Council of Antioch (264), the Councils of Arabia (246–247), the Council of Elvira (306), the Council of Carthage (311), the Synod of Neo-Caesarea (c. 314), the Council of Ancyra (314) and the Council of Arles (314).
The first seven councils recognised in both East and West as ecumenical and several others to which such recognition is refused were called by the Byzantine emperors. In the first millennium, various theological and political differences such as Nestorianism or Dyophysitism caused parts of the Church to separate after councils such as those of Ephesus and Chalcedon, but councils recognised as ecumenical continued to be held.
The Council of Hieria of 754, held at the imperial palace of that name close to Chalcedon in Anatolia, was summoned by Byzantine Emperor Constantine V and was attended by 338 bishops, who regarded it as the seventh ecumenical council The Second Council of Nicea, which annulled that of Hieria, was itself annulled at a synod held in 815 in Constantinople under Emperor Leo V. This synod, presided over by Patriarch Theodotos I of Constantinople, declared the Council of Hieria to be the seventh ecumenical council, but, although the Council of Hieria was called by an emperor and confirmed by another, and although it was held in the east, it is not now considered ecumenical.
Similarly, the Second Council of Ephesus of 449, also held in Anatolia, was called by the Byzantine Emperor Theodosius II and, though annulled by the Council of Chalcedon, was confirmed by Emperor Basiliscus, who annulled the Council of Chalcedon. This too is not now reckoned an ecumenical council.
Roman Catholic views on those circumstances.
The Roman Catholic Church does not consider the validity of an ecumenical council's teaching to be in any way dependent on where it is held or on the granting or withholding of prior authorization or legal status by any state, in line with the attitude of the 5th-century bishops who "saw the definition of the church's faith and canons as supremely their affair, with or without the leave of the Emperor" and who "needed no one to remind them that Synodical process pre-dated the Christianisation of the royal court by several centuries".
The Roman Catholic Church recognizes as ecumenical various councils held later than the First Council of Ephesus (after which churches out of communion with the Holy See because of the Nestorian Schism did not participate), later than the Council of Chalcedon (after which there was no participation by churches that rejected Dyophysitism), later than the Second Council of Nicaea (after which there was no participation by the Eastern Orthodox Church), and later than the Fifth Council of the Lateran (after which groups that adhered to Protestantism did not participate).
Of the twenty-one ecumenical councils recognised by the Roman Catholic Church, some gained recognition as ecumenical only later. Thus the Eastern First Council of Constantinople became ecumenical only when its decrees were accepted in the West also.
List of ecumenical councils.
First seven ecumenical councils.
In the history of Christianity, the first seven Ecumenical Councils, from the First Council of Nicaea (325) to the Second Council of Nicaea (787), represent an attempt to reach an orthodox consensus and to unify Christendom.
All of the original Seven Ecumenical Councils as recognised in whole or in part were called by an emperor of the Eastern Roman Empire and all were held in the Eastern Roman Empire, a recognition denied to other councils similarly called by an Eastern Roman emperor and held in his territory, in particular the Second Council of Ephesus (449) and the Council of Hieria (754), which saw themselves as ecumenical.
Further councils recognised as ecumenical in the Roman Catholic Church.
As late as the 11th century, only seven councils were recognised as ecumenical in the Roman Catholic Church. Then, in the time of Pope Gregory VII (1073–1085), canonists who in the Investiture Controversy quoted the prohibition in canon 22 of the Council of Constantinople of 869–870 against laymen influencing the appointment of prelates elevated this council to the rank of ecumenical council. Only in the 16th century was recognition as ecumenical granted by Catholic scholars to the Councils of the Lateran, of Lyon and those that followed. The following is a list of further councils generally recognised as ecumenical by Roman Catholic theologians:
Other councils that some Eastern Orthodox individuals see as ecumenical.
Eastern Orthodox catechisms teach that there are seven ecumenical councils and there are feast days for seven ecumenical councils. Nonetheless, some Eastern Orthodox consider the Council of Constantinople of 879–880, that of Constantinople in 1341–1351 and that of Jerusalem in 1672 to be ecumenical:
It is unlikely that formal recognition as ecumenical will be granted to these councils, despite the acknowledged orthodoxy of their decisions, so that only seven are universally recognized among the Eastern Orthodox as ecumenical.
The Pan-Orthodox Council now being prepared has sometimes been referred to as a potential "Eighth Ecumenical Council".
Acceptance of the councils.
Although some Protestants reject the concept of an ecumenical council establishing doctrine for the entire Christian faith, Roman Catholics, Anglicans, Eastern Orthodox and Oriental Orthodox all accept the authority of ecumenical councils in principle. Where they differ is in which councils they accept and what the conditions are for a council to be considered "ecumenical". The relationship of the Papacy to the validity of ecumenical councils is a ground of controversy between Roman Catholicism and the Eastern Orthodox Churches. The Roman Catholic Church holds that recognition by the Pope is an essential element in qualifying a council as ecumenical; Eastern Orthodox view approval by the Bishop of Rome (the Pope) as being roughly equivalent to that of other patriarchs.
Some have held that a council is ecumenical only when all five patriarchs of the Pentarchy are represented at it. Others reject this theory in part because there were no patriarchs of Constantinople and Jerusalem at the time of the first ecumenical council.
Church of the East.
The Church of the East accepts two ecumenical councils, the First Council of Nicaea and the First Council of Constantinople. It was the formulation of Mary as the Theotokos which caused a schism with the Church of the East, now divided between the Assyrian Church of the East and the Ancient Church of the East, while the Chaldean Catholic Church entered into full communion with Rome in the 16th century. Meetings between Pope John Paul II and the Assyrian Patriarch Mar Dinkha IV led to a common Christological declaration on 11 November 1994 that "the humanity to which the Blessed Virgin Mary gave birth always was that of the Son of God himself". Both sides recognised the legitimacy and rightness, as expressions of the same faith, of the Assyrian Church's liturgical invocation of Mary as "the Mother of Christ our God and Saviour" and the Catholic Church's use of "the Mother of God" and also as "the Mother of Christ".
Oriental Orthodoxy.
Oriental Orthodoxy accepts three ecumenical councils, the First Council of Nicaea, the First Council of Constantinople, and the Council of Ephesus. The formulation of the Chalcedonian Creed caused a schism in the Alexandrian and Syriac churches. Reconciliatory efforts between Oriental Orthodox with the Eastern Orthodox and the Catholic Church in the mid- and late-20th century have led to common Christological declarations. The Oriental and Eastern Churches have also been working toward reconciliation as a consequence of the ecumenical movement.
The Oriental Orthodox hold that the Dyophysite formula of two natures formulated at the Council of Chalcedon is inferior to the Miaphysite formula of "One Incarnate Nature of God the Word" (Byzantine Greek: Mia physis tou theou logou sarkousomene) and that the proceedings of Chalcedon themselves were motivated by imperial politics. The Alexandrian Church, the main Oriental Orthodox body, also felt unfairly underrepresented at the council following the deposition of their Pope, Dioscorus of Alexandria at the council.
Eastern Orthodox Church.
The Eastern Orthodox Church accepts seven ecumenical councils, with the Council in Trullo considered a continuation of the sixth.
To be considered ecumenical, Orthodox accept a council that meets the condition that it was accepted by the whole church. That it was called together legally is also important a factor. A case in point is the Third Ecumenical Council where two groups met as duly called for by the emperor, each claiming to be the legitimate council. The Emperor had called for bishops to assemble in the city of Ephesus. Theodosius did not attend but sent his representative Candidian to preside. However, Cyril managed to open the council over Candidian's insistent demands that the bishops disperse until the delegation from Syria could arrive. Cyril was able to completely control the proceedings, completely neutralizing Candidian who favored Cyril's antagonist, Nestorius. When the pro-Nestorius Antiochene delegation finally arrived, they decided to convene their own council over which Candidian presided. The proceedings of both councils were reported to the emperor who decided ultimately to depose Cyril, Memnon and Nestorius. Nonetheless, the Orthodox accept Cyril's group as being the legitimate council because it maintained the same teaching that the church has always taught.
Paraphrasing a rule by St Vincent of Lérins Hasler states
...a teaching can only be defined if it has been held to be revealed at all times, everywhere, and by all believers.
Orthodox believe that councils could over-rule or even depose popes. At the Sixth Ecumenical Council Pope Honorius and Patriarch Sergius were declared heretics. The council anathematized them and declared them tools of the devil and cast them out of the church
It is their position that since the Seventh Ecumenical Council, there has been no synod or council of the same scope. Local meetings of hierarchs have been called "pan-Orthodox", but these have invariably been simply meetings of local hierarchs of whatever Eastern Orthodox jurisdictions are party to a specific local matter. From this point of view, there has been no fully "pan-Orthodox" (Ecumenical) council since 787. Unfortunately, the use of the term "pan-Orthodox" is confusing to those not within Eastern Orthodoxy, and it leads to mistaken impressions that these are "ersatz" ecumenical councils rather than purely local councils to which nearby Orthodox hierarchs, regardless of jurisdiction, are invited.
Others, including 20th century theologians Metropolitan Hierotheos (Vlachos) of Naupactus, Fr. John S. Romanides, and Fr. George Metallinos (all of whom refer repeatedly to the "Eighth and Ninth Ecumenical Councils"), Fr. George Dragas, and the 1848 Encyclical of the Eastern Patriarchs (which refers explicitly to the "Eighth Ecumenical Council" and was signed by the patriarchs of Constantinople, Jerusalem, Antioch, and Alexandria as well as the Holy Synods of the first three), regard other synods beyond the Seventh Ecumenical Council as being ecumenical.
From the Eastern Orthodox perspective, a council is accepted as being ecumenical if it is accepted by the Eastern Orthodox church at large – clergy, monks and assembly of believers. Teachings from councils that purport to be ecumenical, but which lack this acceptance by the church at large, are, therefore, not considered ecumenical.
Furthermore Orthodox understand councils were called for in reaction to crises within the church over matters of dogma. For Orthodox no further council would therefore be needed until such time as a major crises arose within the church.
Roman Catholic Church.
Both the Roman Catholic and Eastern Orthodox churches recognize seven councils in the early centuries of the church, but Roman Catholics also recognize fourteen councils in later times called or confirmed by the Pope. At the urging of German King Sigismund, who was to become Holy Roman Emperor in 1433, the Council of Constance was convoked in 1414 by Antipope John XXIII, one of three claimants to the papal throne, and was reconvened in 1415 by the Roman Pope Gregory XII. The Council of Florence is an example of a council accepted as ecumenical in spite of being rejected by the East, as the Councils of Ephesus and Chalcedon are accepted in spite of being rejected respectively by the Church of the East and Oriental Orthodoxy.
The Roman Catholic Church teaches that an ecumenical council is a gathering of the College of Bishops (of which the Bishop of Rome is an essential part) to exercise in a solemn manner its supreme and full power over the whole Church. It holds that "there never is an ecumenical council which is not confirmed or at least recognized as such by Peter's successor". Its present canon law requires that an ecumenical council be convoked and presided over, either personally or through a delegate, by the Pope, who is also to decide the agenda; but the church makes no claim that all past ecumenical councils observed these present rules, declaring only that the Pope's confirmation or at least recognition has always been required, and saying that the version of the Nicene Creed adopted at the First Council of Constantinople (381) was accepted by the Church of Rome only seventy years later, in 451. One writer has even claimed that this council was summoned without the knowledge of the pope.
Anglican Communion.
While the Councils are part of the "historic formularies" of Anglican tradition, it is difficult to locate an explicit reference in Anglicanism to the unconditional acceptance of all Seven Ecumenical Councils. There is little evidence of dogmatic or canonical acceptance beyond the statements of individual Anglican theologians and bishops.
Bishop Chandler Holder Jones, SSC, explains:
We indeed and absolutely believe all Seven Councils are truly ecumenical and catholic – on the basis of the received Tradition of the ancient Undivided Church of East and West. The Anglican formularies address only particular critical theological and disciplinary concerns of the sixteenth and seventeenth centuries, and that certainly by design. Behind them, however, stands the universal authority of the Holy and Apostolic Tradition, which did not have to be rehashed or redebated by Anglican Catholics.
He quotes William Tighe, Associate Professor of History at Muhlenberg College in Allentown, Pennsylvania (another member of the Anglo-Catholic wing of Anglicanism):
...despite the fact that advocates of all sides to the 16th-century religious conflict, Catholic, Lutheran and Reformed alike, were given to claiming that their particular doctrinal stances and, in some cases, distinctive practices, were in accord with those of the Early Church Fathers, or at least with those of high standing (such as St. Augustine), none [but Anglicanism] were willing to require, or even permit, their confessional stances to be judged by, or subordinated to, a hypothetical 'patristic consensus' of the first four or five centuries of Christianity.' But Anglicanism most certainly did, and does so to this day.
Article XXI teaches: "General Councils ... when they be gathered together, forasmuch as they be an assembly of men, whereof all be not governed with the Spirit and word of God, they may err and sometime have erred, even in things pertaining to God. Wherefore things ordained by them as necessary to salvation have neither strength nor authority, unless it may be declared that they be taken out of Holy Scripture."
The 19th Canon of 1571 asserted the authority of the Councils in this manner: "Let preachers take care that they never teach anything...except what is agreeable to the doctrine of the Old and New Testament, and what the Catholic Fathers and ancient Bishops have collected from the same doctrine." This remains the Church of England's teaching on the subject. A modern version of this appeal to catholic consensus is found in the Canon Law of the Church of England and also in the liturgy published in "Common Worship":
The Church of England is part of the One, Holy, Catholic, and Apostolic Church, worshipping the one true God, Father, Son, and Holy Spirit. It professes the faith uniquely revealed in the Holy Scriptures and set forth in the catholic creeds, which faith the Church is called upon to proclaim afresh in each generation. Led by the Holy Spirit, it has borne witness to Christian truth in its historic formularies, the Thirty-nine Articles of Religion, "The Book of Common Prayer" and the Ordering of Bishops, Priests and Deacons.
I, AB, do so affirm, and accordingly declare my belief in the faith which is revealed in the Holy Scriptures and set forth in the catholic creeds and to which the historic formularies of the Church of England bear witness; and in public prayer and administration of the sacraments, I will use only the forms of service which are authorized or allowed by Canon.
Lutheran and Methodist churches.
Many Protestants (especially those belonging to the magisterial traditions, such as Lutherans, or those such as Methodists, that broke away from the Anglican Communion) accept the teachings of the first seven councils but do not ascribe to the councils themselves the same authority as Roman Catholics and the Eastern Orthodox do. The Lutheran World Federation, in ecumenical dialogues with the Ecumenical Patriarch of Constantinople has affirmed all of the first seven councils as ecumenical and authoritative.
Other Protestant churches.
Some, including some fundamentalist Christians, condemn the ecumenical councils for other reasons. Independency or congregationalist polity among Protestants may involve the rejection of any governmental structure or binding authority above local congregations; conformity to the decisions of these councils is therefore considered purely voluntary and the councils are to be considered binding only insofar as those doctrines are derived from the Scriptures. Many of these churches reject the idea that anyone other than the authors of Scripture can directly lead other Christians by original divine authority; after the New Testament, they assert, the doors of revelation were closed and councils can only give advice or guidance, but have no authority. They consider new doctrines not derived from the sealed canon of Scripture to be both impossible and unnecessary whether proposed by church councils or by more recent prophets (even though the canon itself was fixed by these councils).
Nontrinitarian churches.
Ecumenical councils are not recognised by nontrinitarian churches such as The Church of Jesus Christ of Latter-day Saints (and other denominations within the Latter Day Saint movement), Jehovah's Witnesses, and Unitarians. They view the ecumenical councils as misguided human attempts to establish doctrine, and as attempts to define dogmas by debate rather than by revelation.

</doc>
<doc id="9763" url="http://en.wikipedia.org/wiki?curid=9763" title="Exoplanet">
Exoplanet

An exoplanet or extrasolar planet is a planet that orbits a star other than the Sun, a stellar remnant, or a brown dwarf. Nearly 2000 exoplanets have been discovered (1919 planets in 1212 planetary systems including 482 multiple planetary systems as of none }}). There are also rogue planets, which do not orbit any star and which tend to be considered separately, especially if they are gas giants, in which case they are often counted, like WISE 0855−0714, as sub-brown dwarfs.
The "Kepler space telescope" has also detected a few thousand candidate planets, of which about 11% may be false positives.
There is at least one planet on average per star.
Around 1 in 5 Sun-like stars have an "Earth-sized" planet in the habitable zone, with the nearest expected to be within 12 light-years distance from Earth. Assuming 200 billion stars in the Milky Way, that would be 11 billion potentially habitable Earth-sized planets in the Milky Way, rising to 40 billion if red dwarfs are included.
The rogue planets in the Milky Way possibly number in the trillions.
The nearest known exoplanet, if confirmed, would be Alpha Centauri Bb, but there is some doubt about its existence. Almost all of the planets detected so far are within the Milky Way, but there have also been a few possible detections of extragalactic planets.
, the least massive planet known is PSR B1257+12 A, which is about twice the mass of the Moon. The most massive planet listed on the NASA Exoplanet Archive is DENIS-P J082303.1-491201 b, about 29 times the mass of Jupiter, although according to most definitions of a planet, it is too massive to be a planet and may be a brown dwarf instead. There are planets that are so near to their star that they take only a few hours to orbit and there are others so far away that they take thousands of years to orbit. Some are so far out that it is difficult to tell if they are gravitationally bound to the star. (See also: List of exoplanet extremes.)
The discovery of exoplanets has intensified interest in the search for extraterrestrial life, particularly for those that orbit in the host star's habitable zone where it is possible for liquid water (and therefore life) to exist on the surface. The study of planetary habitability also considers a wide range of other factors in determining the suitability of a planet for hosting life.
Definition.
IAU.
The official definition of "planet" used by the International Astronomical Union (IAU) only covers the Solar System and thus does not apply to exoplanets. As of April 2011, the only defining statement issued by the IAU that pertains to exoplanets is a working definition issued in 2001 and modified in 2003.
That definition contains the following criteria:
Alternatives.
However, the IAU's working definition is not universally accepted. One alternate suggestion is that planets should be distinguished from brown dwarfs on the basis of formation. It is widely believed that giant planets form through core accretion, and that process may sometimes produce planets with masses above the deuterium fusion threshold; massive planets of that sort may have already been observed. Brown dwarfs form like stars from the direct collapse of clouds of gas and this formation mechanism also produces objects that are below the 13 MJup limit and can be as low as 1 MJup. Objects in this mass range that orbit their stars with wide separations of hundreds or thousands of AU and have large star/object mass ratios likely formed as brown dwarfs; their atmospheres would likely have a composition more similar to their host star than accretion-formed planets which would contain increased abundances of heavier elements. Most directly imaged planets as of April 2014 are massive and have wide orbits so probably represent the low-mass end of brown dwarf formation.
Also, the 13-Jupiter-mass cutoff does not have precise physical significance. Deuterium fusion can occur in some objects with a mass below that cutoff. The amount of deuterium fused depends to some extent on the composition of the object. The Extrasolar Planets Encyclopaedia includes objects up to 25 Jupiter masses, saying, "The fact that there is no special feature around 13 MJup in the observed mass spectrum reinforces the choice to forget this mass limit". The Exoplanet Data Explorer includes objects up to 24 Jupiter masses with the advisory: "The 13 Jupiter-mass distinction by the IAU Working Group is physically unmotivated for planets with rocky cores, and observationally problematic due to the sin i ambiguity."
The NASA Exoplanet Archive includes objects with a mass (or minimum mass) equal to or less than 30 Jupiter masses.
Another criterion for separating planets and brown dwarfs, rather than deuterium burning, formation process or location, is whether the core pressure is dominated by coulomb pressure or electron degeneracy pressure.
History of detection.
For centuries philosophers and scientists supposed that extrasolar planets existed, but there was no way of detecting them or of knowing their frequency or how similar they might be to the planets of the Solar System. Various detection claims made in the nineteenth century were rejected by astronomers. The first confirmed detection came in 1992, with the discovery of several terrestrial-mass planets orbiting the pulsar PSR B1257+12. The first confirmation of an exoplanet orbiting a main-sequence star was made in 1995, when a giant planet was found in a four-day orbit around the nearby star 51 Pegasi. Some exoplanets have been imaged directly by telescopes, but the vast majority have been detected through indirect methods such as the transit method and the radial-velocity method.
Early speculations.
In the sixteenth century the Italian philosopher Giordano Bruno, an early supporter of the Copernican theory that Earth and other planets orbit the Sun (heliocentrism), put forward the view that the fixed stars are similar to the Sun and are likewise accompanied by planets.
In the eighteenth century the same possibility was mentioned by Isaac Newton in the "General Scholium" that concludes his "Principia". Making a comparison to the Sun's planets, he wrote "And if the fixed stars are the centers of similar systems, they will all be constructed according to a similar design and subject to the dominion of "One"."
In 1952, more than 40 years before the first hot Jupiter was discovered, Otto Struve wrote that there is no compelling reason why planets could not be much closer to their parent star than is the case in the Solar System, and proposed that Doppler spectroscopy and the transit method could detect super-Jupiters in short orbits.
Discredited claims.
Claims of exoplanet detections have been made since the nineteenth century. Some of the earliest involve the binary star 70 Ophiuchi. In 1855 Capt. W. S. Jacob at the East India Company's Madras Observatory reported that orbital anomalies made it "highly probable" that there was a "planetary body" in this system. In the 1890s, Thomas J. J. See of the University of Chicago and the United States Naval Observatory stated that the orbital anomalies proved the existence of a dark body in the 70 Ophiuchi system with a 36-year period around one of the stars. However, Forest Ray Moulton published a paper proving that a three-body system with those orbital parameters would be highly unstable. During the 1950s and 1960s, Peter van de Kamp of Swarthmore College made another prominent series of detection claims, this time for planets orbiting Barnard's Star. Astronomers now generally regard all the early reports of detection as erroneous.
In 1991 Andrew Lyne, M. Bailes and S. L. Shemar claimed to have discovered a pulsar planet in orbit around PSR 1829-10, using pulsar timing variations. The claim briefly received intense attention, but Lyne and his team soon retracted it.
Confirmed discoveries.
The first published discovery to receive subsequent confirmation was made in 1988 by the Canadian astronomers Bruce Campbell, G. A. H. Walker, and Stephenson Yang of the University of Victoria and the University of British Columbia. Although they were cautious about claiming a planetary detection, their radial-velocity observations suggested that a planet orbits the star Gamma Cephei. Partly because the observations were at the very limits of instrumental capabilities at the time, astronomers remained skeptical for several years about this and other similar observations. It was thought some of the apparent planets might instead have been brown dwarfs, objects intermediate in mass between planets and stars. In 1990 additional observations were published that supported the existence of the planet orbiting Gamma Cephei, but subsequent work in 1992 again raised serious doubts. Finally, in 2003, improved techniques allowed the planet's existence to be confirmed.
On 9 January 1992, radio astronomers Aleksander Wolszczan and Dale Frail announced the discovery of two planets orbiting the pulsar PSR 1257+12. This discovery was confirmed, and is generally considered to be the first definitive detection of exoplanets. Followup observations solidified these results, and confirmation of a third planet in 1994 revived the topic in the popular press. These pulsar planets are believed to have formed from the unusual remnants of the supernova that produced the pulsar, in a second round of planet formation, or else to be the remaining rocky cores of gas giants that somehow survived the supernova and then decayed into their current orbits.
On 6 October 1995, Michel Mayor and Didier Queloz of the University of Geneva announced the first definitive detection of an exoplanet orbiting a main-sequence star, namely the nearby G-type star 51 Pegasi. This discovery, made at the Observatoire de Haute-Provence, ushered in the modern era of exoplanetary discovery. Technological advances, most notably in high-resolution spectroscopy, led to the rapid detection of many new exoplanets: astronomers could detect exoplanets indirectly by measuring their gravitational influence on the motion of their host stars. More extrasolar planets were later detected by observing the variation in a star's apparent luminosity as an orbiting planet passed in front of it.
Initially, most known exoplanets were massive planets that orbited very close to their parent stars. Astronomers were surprised by these "hot Jupiters", because theories of planetary formation had indicated that giant planets should only form at large distances from stars. But eventually more planets of other sorts were found, and it is now clear that hot Jupiters are a minority of exoplanets. In 1999, Upsilon Andromedae became the first main-sequence star known to have multiple planets. Others were found subsequently.
As of none }}, a total of 1919 confirmed exoplanets are listed in the Extrasolar Planets Encyclopaedia, including a few that were confirmations of controversial claims from the late 1980s. That count includes 1212 planetary systems, of which 482 are multiple planetary systems. Kepler-16 contains the first discovered planet that orbits around a binary main-sequence star system.
On 26 February 2014, NASA announced the discovery of 715 newly verified exoplanets around 305 stars by the Kepler Space Telescope. These exoplanets were checked using a statistical technique called "verification by multiplicity". Prior to these results, most confirmed planets were gas giants comparable in size to Jupiter or larger as they are more easily detected, but the Kepler planets are mostly between the size of Neptune and the size of Earth.
On 6 January 2015, NASA announced the 1000th confirmed exoplanet discovered by the Kepler Space Telescope. Three of the newly confirmed exoplanets were found to orbit within habitable zones of their host stars: two of the three, Kepler-438b and Kepler-442b, are near-Earth-size and likely rocky; the third, Kepler-440b, is a super-Earth. Similar confirmed small exoplanets in habitable zones found earlier by "Kepler" include: Kepler-62e, Kepler-62f, Kepler-186f, Kepler296e and Kepler-296f.
Candidate discoveries.
17 October 2012 brought news of an unverified planet, Alpha Centauri Bb, orbiting Alpha Centauri B, which is one of three stars in a triple star system nearest to Earth's Sun. Alpha Centauri Bb is an Earth-size planet, but not in the habitable zone within which liquid water can exist.
As of March 2014, NASA's Kepler mission had identified more than 2,900 planetary candidates, several of them being nearly Earth-sized and located in the habitable zone, some around Sun-like stars.
Detection methods.
Direct imaging.
Planets are extremely faint compared to their parent stars. At visible wavelengths, they usually have less than a millionth of their host star's brightness. It is difficult to detect such a faint light source, and furthermore the parent star causes a glare that tends to wash it out. It is necessary to block the light from the parent star in order to reduce the glare while leaving the light from the planet detectable; doing so is a major technical challenge.
All exoplanets that have been directly imaged are both large (more massive than Jupiter) and widely separated from their parent star. Most of them are also very hot, so that they emit intense infrared radiation; the images have then been made at infrared where the planet is brighter than it is at visible wavelengths. During the gas-accretion phase of giant-planet formation the star–planet contrast may be even better in H alpha than it is in infrared—an H alpha survey is currently underway.
The first direct detection of the visible light spectrum reflected from an exoplanet was reported 22 April 2015. Astronomers studied reflected light from exoplanet 51 Pegasi b—which is the first exoplanet that was discovered orbiting a main-sequence star, the Sun-like (G5V) 51 Pegasi. The direct detection was made using the High Accuracy Radial velocity Planet Searcher (HARPS) instrument at the European Southern Observatory's La Silla Observatory in Chile.
Specially designed direct-imaging instruments such as Gemini Planet Imager, VLT-SPHERE, and SCExAO will image dozens of gas giants, however the vast majority of known extrasolar planets have only been detected through indirect methods. The following are the indirect methods that have proven useful:
Nomenclature.
Proper names.
Most exoplanets have catalog names which are explained in the following sections, but in 2014 the IAU launched a process for giving proper names to exoplanets. The process involves public nomination and voting for the new names, and the IAU plans to announce the new names in August 2015. The decision to give the planets new names followed the private company Uwingu's exoplanet naming contest, which the IAU harshly criticized. Previously a few planets had received unofficial names: notably Osiris (HD 209458 b), Bellerophon (51 Pegasi b), and Methuselah (PSR B1620-26 b).
Multiple-star standard.
The convention for naming exoplanets is an extension of the one used by the Washington Multiplicity Catalog (WMC) for multiple-star systems, and adopted by the International Astronomical Union.
The brightest member of a star system receives the letter "A". Distinct components not contained within "A" are labeled "B", "C", etc. Subcomponents are designated by one or more suffixes with the primary label, starting with lowercase letters for the 2nd hierarchical level and then numbers for the 3rd. For example, if there is a triple star system in which two stars orbit each other closely with a third star in a more distant orbit, the two closely orbiting stars would be named Aa and Ab, whereas the distant star would named B. For historical reasons, this standard is not always followed: for example Alpha Centauri A, B and C are "not" labelled Alpha Centauri Aa, Ab and B.
Extrasolar planet standard.
Following an extension of the above standard, an exoplanet's name is normally formed by taking the name of its parent star and adding a lowercase letter. The first planet discovered in a system is given the designation "b" and later planets are given subsequent letters. If several planets in the same system are discovered at the same time, the closest one to the star gets the next letter, followed by the other planets in order of orbital size.
For instance, in the 55 Cancri system the first planet – 55 Cancri b – was discovered in 1996; two additional farther planets were simultaneously discovered in 2002 with the nearest to the star being named 55 Cancri c and the other 55 Cancri d; a fourth planet was claimed (its existence was later disputed) in 2004 and named 55 Cancri e despite lying closer to the star than 55 Cancri b; and the most recently discovered planet, in 2007, was named 55 Cancri f despite lying between 55 Cancri c and 55 Cancri d. As of April 2012 the highest letter in use is "j", for the unconfirmed planet HD 10180 j, and with "h" being the highest letter for a confirmed planet, belonging to the same host star).
If a planet orbits one member of a binary star system, then an uppercase letter for the star will be followed by a lowercase letter for the planet. Examples are 16 Cygni Bb and HD 178911 Bb. Planets orbiting the primary or "A" star should have 'Ab' after the name of the system, as in HD 41004 Ab. However, the "A" is sometimes omitted; for example the first planet discovered around the primary star of the Tau Boötis binary system is usually called simply Tau Boötis b. The star designation is necessary when more than one star in the system has its own planetary system such as in case of WASP-94 A and WASP-94 B.
If the parent star is a single star, then it may still be regarded as having an "A" designation, though the "A" is not normally written. The first exoplanet found to be orbiting such a star could then be regarded as a secondary subcomponent that should be given the suffix "Ab". For example, 51 Peg Aa is the host star in the system 51 Peg; and the first exoplanet is then 51 Peg Ab. Because most exoplanets are in single-star systems, the implicit "A" designation was simply dropped, leaving the exoplanet name with the lower-case letter only: 51 Peg b.
A few exoplanets have been given names that do not conform to the above standard. For example, the planets that orbit the pulsar PSR 1257 are often referred to with capital rather than lowercase letters. Also, the underlying name of the star system itself can follow several different systems. In fact, some stars (such as Kepler-11) have only received their names due to their inclusion in planet-search programs, previously only being referred to by their celestial coordinates.
Circumbinary planets and 2010 proposal.
Hessman et al. state that the implicit system for exoplanet names utterly failed with the discovery of circumbinary planets. They note that the discoverers of the two planets around HW Virginis tried to circumvent the naming problem by calling them "HW Vir 3" and "HW Vir 4", i.e. the latter is the 4th object – stellar or planetary – discovered in the system. They also note that the discoverers of the two planets around NN Serpentis were confronted with multiple suggestions from various official sources and finally chose to use the designations "NN Ser c" and "NN Ser d".
The proposal of Hessman et al. starts with the following two rules:
They note that under these two proposed rules all of the present names for 99% of the planets around single stars are preserved as informal forms of the IAU sanctioned provisional standard. They would rename Tau Boötis b formally as Tau Boötis Ab, retaining the prior form as an informal usage (using Rule 2, above).
To deal with the difficulties relating to circumbinary planets, the proposal contains two further rules:
They submit that the new form using parentheses is the best for known circumbinary planets and has the desirable effect of giving these planets identical sublevel hierarchical labels and stellar component names that conform to the usage for binary stars. They say that it requires the complete renaming of only two exoplanetary systems: The planets around HW Virginis would be renamed HW Vir (AB) b & (AB) c, whereas those around NN Serpentis would be renamed NN Ser (AB) b & (AB) c. In addition the previously known single circumbinary planets around PSR B1620-26 and DP Leonis) can almost retain their names (PSR B1620-26 b and DP Leonis b) as unofficial informal forms of the "(AB)b" designation where the "(AB)" is left out.
The discoverers of the circumbinary planet around Kepler-16 followed the naming scheme proposed by Hessman et al. when naming the body Kepler-16 (AB)-b, or simply Kepler-16b when there is no ambiguity.
Other naming systems.
Another nomenclature, often seen in science fiction, uses Roman numerals in the order of planets' positions from the star. (This was inspired by an old system for naming moons of the outer planets, such as "Jupiter IV" for Callisto.) But such a system is impractical for scientific use, because new planets may be found closer to the star, changing all numerals.
Formation and evolution.
Planets form within a few tens of millions of years of their star forming, and there are stars that are forming today and other stars that are ten billion years old, so unlike the planets of the Solar System, which can only be observed as they are today, studying exoplanets allows the observation of exoplanets at different stages of evolution. When planets form they have hydrogen envelopes that cool and contract over time and, depending on the mass of the planet, some or all of the hydrogen is eventually lost to space. This means that even terrestrial planets can start off with large radii. An example is Kepler-51b which has only about twice the mass of Earth but is almost the size of Saturn which is a hundred times the mass of Earth. Kepler-51b is quite young at a few hundred million years old.
Planet-hosting stars.
There is at least one planet on average per star.
Around 1 in 5 Sun-like stars have an "Earth-sized" planet in the habitable zone
Most known exoplanets orbit stars roughly similar to the Sun, that is, main-sequence stars of spectral categories F, G, or K. Lower-mass stars (red dwarfs, of spectral category M) are less likely to have planets massive enough to detect by the radial-velocity method. Although several tens of planets around red dwarfs have been discovered by the Kepler spacecraft which uses the transit method which can detect smaller planets.
Stars with a higher metallicity than the Sun are more likely to have planets, especially giant planets, than stars with lower metallicity.
Some planets orbit one member of a binary star system, and several circumbinary planets have been discovered which orbit around both members of binary star. A few planets in triple star systems are known and one in the quadruple system Kepler 64.
Orbital parameters.
Most known extrasolar planet candidates have been discovered using indirect methods and therefore only some of their physical and orbital parameters can be determined. For example, out of the six independent parameters that define an orbit, the radial-velocity method can determine four: semi-major axis, eccentricity, longitude of periastron, and time of periastron. Two parameters remain unknown: inclination and longitude of the ascending node.
Distance from star, semi-major axis and orbital period.
There are exoplanets that are much closer to their parent star than any planet in the Solar System is to the Sun, and there are also exoplanets that are much further from their star. Mercury, the closest planet to the Sun at 0.4 AU, takes 88 days for an orbit, but the smallest known orbits of exoplanets have orbital periods of only a few hours, e.g. Kepler-70b. The Kepler-11 system has five of its planets in smaller orbits than Mercury's. Neptune is 30 AU from the Sun and takes 165 years to orbit it, but there are exoplanets that are thousands of AU from their star and take tens of thousands of years to orbit, e.g. GU Piscium b.
The orbit of a planet is not centered on the star but on their common center of mass (see diagram on right). For circular orbits, the semi-major axis is the distance between the planet and the center of mass of the system. For elliptical orbits, the planet–star distance varies over the course of the orbit, in which case the semi-major axis is the average of the largest and smallest distances between the planet and the center of mass of the system. If the sizes of the star and planet are relatively small compared to the size of the orbit and the orbit is nearly circular and the center of mass is not too far from the star's center, such as in the Earth–Sun system, then the distance from any point on the star to any point on the planet is approximately the same as the semi-major axis. However, when a star's radius expands when it turns into a red giant, then the distance between the planet and the star's surface can become close to zero, or even less than zero if the planet has been engulfed by the expanding red giant, whereas the center of mass from which the semi-major axis is measured will still be near the center of the red giant.
Orbital period is the time taken to complete one orbit. For any given star, the shorter the semi-major axis of a planet, the shorter the orbital period. Also comparing planets around different stars but with the same semi-major axis, the more massive the star, the shorter the orbital period.
Over the lifetime of a star, the semi-major axes of its planets changes. This planetary migration happens especially during the formation of the planetary system when planets interact with the protoplanetary disk and each other until a relatively stable position is reached, and later in the red-giant and asymptotic-giant-branch phases when the star expands and engulfs the nearest planets that can cause them to move inwards, and when the red giant loses mass as the outer layers dissipate causing planets to move outwards as a result of the red giant's reduced gravitational field.
The radial-velocity and transit methods are most sensitive to planets with small orbits. The earliest discoveries such as 51 Peg b were gas giants with orbits of a few days. These "hot Jupiters" likely formed further out and migrated inwards. The Kepler spacecraft has found planets with even shorter orbits of only a few hours, which places them within the star's upper atmosphere or corona, and these planets are Earth-sized or smaller and are probably the left-over solid cores of giant planets that have evaporated due to being so close to the star, or even being engulfed by the star in its red-giant phase in the case of Kepler-70b. As well as evaporation, other reasons why larger planets are unlikely to survive orbits only a few hours long include orbital decay caused by tidal force, tidal-inflation instability, and Roche-lobe overflow. The Roche limit implies that small planets with orbits of a few hours are likely made mostly of iron.
The direct imaging method is most sensitive to planets with large orbits, and has discovered some planets that have planet–star separations of hundreds of AU. However, protoplanetary disks are usually only around 100 AU in radius, and core accretion models predict giant planet formation to be within 10 AU, where the planets can coalesce quickly enough before the disk evaporates.
Very-long-period giant planets may have been rogue planets that were captured, or formed close-in and gravitationally scattered outwards, or the planet and star could be a mass-imbalanced wide binary system with the planet being the primary object of its own separate protoplanetary disk. Gravitational instability models might produce planets at multi-hundred AU separations but this would require unusually large disks. For planets with very wide orbits up to several hundred thousand AU it may be difficult to observationally determine whether the planet is gravitationally bound to the star.
Most planets that have been discovered are within a couple of AU from their host star because the most used methods (radial-velocity and transit) require observation of several orbits to confirm that the planet exists and there has only been enough time since these methods were first used to cover small separations. Some planets with larger orbits have been discovered by direct imaging but there is a middle range of distances, roughly equivalent to the Solar System's gas giant region, which is largely unexplored. Direct imaging equipment for exploring that region is being installed on the world's largest telescopes and should begin operation in 2014. e.g. Gemini Planet Imager and VLT-SPHERE. The microlensing method has detected a few planets in the 1–10 AU range.
It appears plausible that in most exoplanetary systems, there are one or two giant planets with orbits comparable in size to those of Jupiter and Saturn in the Solar System. Giant planets with substantially larger orbits are now known to be rare, at least around Sun-like stars.
The distance of the habitable zone from a star depends on the type of star and this distance changes during the star's lifetime as the size and temperature of the star changes.
Eccentricity.
The eccentricity of an orbit is a measure of how elliptical (elongated) it is. All the planets of the Solar System except for Mercury have near-circular orbits (e<0.1). Most exoplanets with orbital periods of 20 days or less have near-circular orbits, i.e. very low eccentricity. That is believed to be due to tidal circularization: reduction of eccentricity over time due to gravitational interaction between two bodies. The mostly sub-Neptune-sized planets found by the Kepler spacecraft with short orbital periods have very circular orbits. By contrast, the giant planets with longer orbital periods discovered by radial-velocity methods have quite eccentric orbits. (As of July 2010, 55% of such exoplanets have eccentricities greater than 0.2, whereas 17% have eccentricities greater than 0.5.) Moderate to high eccentricities (e>0.2) of giant planets are "not" an observational selection effect, because a planet can be detected about equally well regardless of the eccentricity of its orbit. The prevalence of elliptical orbits for giant planets is a major puzzle, because current theories of planetary formation strongly suggest planets should form with circular (that is, non-eccentric) orbits.
However, for weak Doppler signals near the limits of the current detection ability the eccentricity becomes poorly constrained and biased towards higher values. It is suggested that some of the high eccentricities reported for low-mass exoplanets may be overestimates, because simulations show that many observations are also consistent with two planets on circular orbits. Reported observations of single planets in moderately eccentric orbits have about a 15% chance of being a pair of planets. This misinterpretation is especially likely if the two planets orbit with a 2:1 resonance. With the exoplanet sample known in 2009, a group of astronomers has concluded that "(1) around 35% of the published eccentric one-planet solutions are statistically indistinguishable from planetary systems in 2:1 orbital resonance, (2) another 40% cannot be statistically distinguished from a circular orbital solution" and "(3) planets with masses comparable to Earth could be hidden in known orbital solutions of eccentric super-Earths and Neptune mass planets".
Radial velocity surveys found exoplanet orbits beyond 0.1 AU to be eccentric, particularly for large planets. Kepler spacecraft transit data is consistent with the RV surveys and also revealed that smaller planets tend to have less eccentric orbits.
Inclination vs. spin–orbit angle.
Orbital inclination is the angle between a planet's orbital plane and another plane of reference. For exoplanets the inclination is usually stated with respect to an observer on Earth: the angle used is that between the normal to the planet's orbital plane and the line of sight from Earth to the star. Therefore most planets observed by the transit method are close to 90 degrees. Because the word 'inclination' is used in exoplanet studies for this line-of-sight inclination then the angle between the planet's orbit and the star's rotation must use a different word and is termed the spin–orbit angle or spin–orbit alignment. In most cases the orientation of the star's rotational axis is unknown. The Kepler spacecraft has found a few hundred multi-planet systems and in most of these systems the planets all orbit in nearly the same plane, much like the Solar System. However, a combination of astrometric and radial-velocity measurements has shown that some planetary systems contain planets whose orbital planes are significantly tilted relative to each other. More than half of hot Jupiters have orbital planes substantially misaligned with their parent star's rotation. A substantial fraction of hot-Jupiters even have retrograde orbits, meaning that they orbit in the opposite direction from the star's rotation. Rather than a planet's orbit having been disturbed, it may be that the star itself flipped early in their system's formation due to interactions between the star's magnetic field and the planet-forming disc.
Periastron precession.
Periastron precession is the rotation of a planet's orbit within the orbital plane, i.e. the axes of the ellipse change direction. Various factors cause the precession. In the Solar System perturbations from other planets are the main cause, but for close-in exoplanets the largest factor can be tidal forces between the star and planet. For close-in exoplanets, the general relativistic contribution to the precession is also significant and can be orders of magnitude larger than the same effect for Mercury. Some exoplanets have significantly eccentric orbits, which makes it easier to detect the precession. The effect of general relativity can be detectable in timescales of roughly 10 years or less.
Nodal precession.
Nodal precession is rotation of a planet's orbital plane. This differs from periastron precession, which is rotation of a planet's orbit within that plane. Nodal precession is more easily seen as distinct from periastron precession when the orbital plane is inclined to the star's rotation, the extreme case being a polar orbit.
WASP-33 is a fast-rotating star that hosts a hot Jupiter in an almost polar orbit. The quadrupole mass moment and the proper angular momentum of the star are 1900 and 400 times, respectively, larger than those of the Sun. This causes significant classical and relativistic deviations from Kepler's laws. In particular, the fast rotation causes large nodal precession because of the star's oblateness and the Lense–Thirring effect.
Rotation and axial tilt.
In April 2014 the first measurement of a planet's rotation period was announced: the length of day for the super-Jupiter gas giant Beta Pictoris b is 8 hours (based on the assumption that the axial tilt of the planet is small.) With an equatorial rotational velocity of 25 km per second, this is faster than for the giant planets of the Solar System, in line with the expectation that the more massive a giant planet, the faster it spins. Beta Pictoris b's distance from its star is 9AU. At such distances the rotation of Jovian planets is not slowed by tidal effects. Beta Pictoris b is still warm and young and over the next hundreds of millions of years, it will cool down and shrink to about the size of Jupiter, and if its angular momentum is preserved then as it shrinks the length of its day will decrease to about 3 hours and its equatorial rotation velocity will speed up to about 40 km per second. The images of Beta Pictoris b do not have high enough resolution to directly see details but doppler spectroscopy techniques were used to show that different parts of the planet were moving at different speeds and in opposite directions from which it was inferred that the planet is rotating. With the next generation of large ground-based telescopes it will be possible to use doppler imaging techniques to make a global map of the planet, like the recent mapping of the brown dwarf Luhman 16B.
Origin of spin and tilt of terrestrial planets.
Giant impacts have a large effect on the spin of terrestrial planets. The last few giant impacts during planetary formation tend to be the main determiner of a terrestrial planet's rotation rate. On average the spin angular velocity will be about 70% of the velocity that would cause the planet to break up and fly apart; the natural outcome of planetary embryo impacts at speeds slightly larger than escape velocity. In later stages terrestrial planet spin is also affected by impacts with planetesimals. During the giant impact stage, the thickness of a protoplanetary disk is far larger than the size of planetary embryos so collisions are equally likely to come from any direction in three-dimensions. This results in the axial tilt of accreted planets ranging from 0 to 180 degrees with any direction as likely as any other with both prograde and retrograde spins equally probable. Therefore, prograde spin with a small axial tilt, common for the Solar System's terrestrial planets except Venus, is not common in general for terrestrial planets built by giant impacts. The initial axial tilt of a planet determined by giant impacts can be substantially changed by stellar tides if the planet is close to its star and by satellite tides if the planet has a large satellite.
Tidal effects.
For most planets the rotation period and axial tilt (also called obliquity) are not known, but a large number of planets have been detected with very short orbits (where tidal effects are greater) and will probably have reached an equilibrium rotation that can be predicted.
Tidal effects are the result of forces acting on a body differing from one part of the body to another. For example the gravitational effect of a star varies with distance from one side of a planet to another. Also heat from a star creates a temperature gradient between the day and nightsides which is another source of tides. For example, on Earth, air pressure variations on the ground are affected more by temperature differences than gravitational ones.
Tides modify the rotation and orbit of planets until an equilibrium is reached. Whenever the rotation rate is slowed, there is an increase of the orbit semi-major axis due to the conservation of angular momentum. Most of the large moons in the Solar System, including the Moon, are tidally locked to their host planet; the same side of the moon is always facing the planet. This means the moons' rotation periods are synchronous with their orbital period. However when an orbit is eccentric, as is the case with many exoplanets' orbits of their host stars, there are equilibrium states such as spin–orbit resonances that are far more likely than synchronous rotation. A spin–orbit resonance is when the rotation period and the orbital period are in an integer ratio – this is called a commensurability. Non-resonant equilibriums such as the retrograde rotation of Venus can also occur when both gravitational and thermal atmospheric tides are both significant.
A synchronous tidal lock isn't necessarily particularly slow – there are planets with orbits that take only a few hours.
Gravitational tides tend to reduce the axial tilt to zero but over a longer time-scale than the rotation rate reaches equilibrium. However, the presence of multiple planets in a system can cause axial tilt to be captured in a resonance called a Cassini state. There are small oscillations around this state and in the case of Mars these axial tilt variations are chaotic.
Hot Jupiters' close proximity to their host star means that their spin–orbit evolution is mostly due to the star's gravity and not the other effects. Hot Jupiters rotation rate is not thought to be captured into spin–orbit resonance due to way fluid-body reacts to tides, and therefore slows down to synchronous rotation if it is on a circular orbit or slows to a non-synchronous rotation if on an eccentric orbit. Hot Jupiters are likely to evolve towards zero axial tilt even if they had been in a Cassini state during planetary migration when they were further from their star. Hot Jupiters' orbits will become more circular over time, however the presence of other planets in the system on eccentric orbits, even ones as small as Earth and as far away as the habitable zone, can continue to maintain the eccentricity of the Hot Jupiter so that the length of time for tidal circularization can be billions instead of millions of years.
The rotation rate of planet HD 80606 b is predicted to be about 1.9 days. HD 80606 b avoids spin–orbit resonance because it is a gas giant. The eccentricity of its orbit means that it avoids becoming tidally locked.
Physical parameters.
Mass.
When a planet is found by the radial-velocity method, its orbital inclination "i" is unknown and can range from 0 to 90 degrees. The method is unable to determine the true mass ("M") of the planet, but rather gives a lower limit for its mass, "M" sin"i". In a few cases an apparent exoplanet may be a more massive object such as a brown dwarf or red dwarf. However, the probability of a small value of i (say less than 30 degrees, which would give a true mass at least double the observed lower limit) is relatively low (1−(√3)/2 ≈ 13%) and hence most planets will have true masses fairly close to the observed lower limit.
If a planet's orbit is nearly perpendicular to the line of vision (i.e. "i" close to 90°), a planet can be detected through the transit method. The inclination will then be known, and the inclination combined with "M" sin"i" from radial-velocity will give the planet's true mass.
Also, astrometric observations and dynamical considerations in multiple-planet systems can sometimes provide an upper limit to the planet's true mass.
The mass of a transiting exoplanet can also be determined from the transmission spectrum of its atmosphere, as it can be used to constrain independently the atmospheric composition, temperature, pressure, and scale height.
Transit-timing variation can also be used to find planets' masses.
Radius, density and bulk composition.
Prior to recent results from the Kepler spacecraft most confirmed planets were gas giants comparable in size to Jupiter or larger because they are most easily detected. However, the planets detected by Kepler are mostly between the size of Neptune and the size of Earth.
If a planet is detectable by both the radial-velocity and the transit methods, then both its true mass and its radius can be found. The planet's density can then be calculated. Planets with low density are inferred to be composed mainly of hydrogen and helium, whereas planets of intermediate density are inferred to have water as a major constituent. A planet of high density is inferred to be rocky, like Earth and the other terrestrial planets of the Solar System.
Gas giants, puffy planets, and super-Jupiters.
Gaseous planets that are hot because they are close to their star or because they are still hot from their formation are expanded by the heat. For colder gas planets there is a maximum radius which is slightly larger than Jupiter which occurs when the mass reaches a few Jupiter-masses. Adding mass beyond this point causes the radius to shrink.
Even when taking heating from the star into account, many transiting exoplanets are much larger than expected given their mass, meaning that they have surprisingly low density.
See the magnetic field section for one possible explanation.
Besides those inflated hot Jupiters there is another type of low-density planet: occurring at around 0.6 times the size of Jupiter where there are very few planets. The planets around Kepler-51 are far less dense (far more diffuse) than the inflated hot Jupiters as can be seen in the plots on the right where the three Kepler-51 planets stand out in the diffusity vs. radius plot. A more detailed study taking into account star spots may modify these results to produce less extreme values.
Ice giants and super-Neptunes.
Kepler-101b is the first super-Neptune planet. It has three times Neptune's mass but a Neptune-like composition with more than 60% heavy elements unlike hydrogen/helium-dominated gas giants.
Super-Earths, mini-Neptunes, and gas dwarfs.
If a planet has a radius and/or mass between that of Earth and Neptune then there is a question about whether the planet is rocky like Earth, a mixture of volatiles and gas like Neptune, a small planet with a hydrogen/helium envelope (mini-Jupiter), or of some other composition.
Some of the Kepler transiting planets with radii in the range 1–4 Earth radii have had their masses measured by radial-velocity or transit-timing methods. The calculated densities show that up to 1.5 Earth radii, these planets are rocky and that density increases with increasing radius due to gravitational compression. However, between 1.5 and 4 Earth radii the density decreases with increasing radius. This indicates that above 1.5 Earth radii planets tend to have increasing amounts of volatiles and gas. Despite this general trend there is a wide range of masses at a given radius, which could be because gas planets can have rocky cores of different masses or compositions and could also be due to photoevaporation of volatiles.
Thermal evolutionary atmosphere models suggest a radius of 1.75 times that of Earth as a dividing line between rocky and gaseous planets.
Excluding close-in planets that have lost their gas envelope due to stellar irradiation, studies of the metallicity of stars suggest a dividing line of 1.7 Earth radii between rocky planets and gas dwarfs; then another dividing line at 3.9 Earth radii between gas dwarfs and gas giants. These dividing lines are statistical trends and do not necessarily apply to specific planets because there are many other factors besides metallicity that affect planet formation, including distance from star – there may be larger rocky planets formed at larger distances.
The discovery of the low-density Earth-mass planet Kepler-138d shows that there is an overlapping range of "masses" in which both rocky planets and low-density planets occur. Low-mass low-density planets could be ocean planets or super-Earths with a remnant hydrogen atmosphere, or hot planets with a steam atmosphere, or mini-Neptunes with a hydrogen-helium atmosphere.
Other possibilities for low-mass low-density planets are large atmospheres of carbon monoxide, carbon dioxide, methane, or nitrogen.
Massive solid planets.
In 2014, new measurements of Kepler-10c found that it is a Neptune-mass planet (17 Earth masses) with a density higher than Earth's, indicating that Kepler-10c is made mostly of rock with possibly up to 20% high-pressure water ice but without a hydrogen-dominated envelope. Because this is well above the 10-Earth-mass upper limit that is commonly used for the term 'super-Earth', the term mega-Earth has been proposed. A similarly massive and dense planet could be Kepler-131b, although its density is not as well measured as that of Kepler 10c. The next most massive known solid planets are half this mass: 55 Cancri e and Kepler-20b.
Gas planets can also have large solid cores: the Saturn-mass planet HD 149026 b has only two-thirds of Saturn's radius, so it may have a rock–ice core of 60 Earth masses or more.
Transit-timing variation measurements indicate that Kepler-52b, Kepler-52c and Kepler-57b have maximum-masses between 30 and 100 times the mass of Earth, although the actual masses could be much lower. With radii about 2 Earth radii in size, they might have densities larger than that of an iron planet of the same size. They orbit very close to their stars, so they could be the remnant cores (chthonian planets) of evaporated gas giants or brown dwarfs. If cores are massive enough they could remain compressed for billions of years despite losing the atmospheric mass.
Solid planets up to thousands of Earth masses may be able to form around massive stars (B-type and O-type stars; 5–120 solar masses), where the protoplanetary disk would contain enough heavy elements. Also, these stars have high UV radiation and winds that could photoevaporate the gas in the disk, leaving just the heavy elements.<ref name="mass/radius sold">Error: Bad DOI specified: 10.1086/521346</ref>
For comparison, Neptune's mass equals 17 Earth masses, Jupiter has 318 Earth masses, and the 13 Jupiter-mass limit used in the IAU's working definition of an exoplanet equals approximately 4000 Earth masses.
Another way of forming massive solid planets is when a white dwarf in a close binary system loses material to a companion neutron star. The white dwarf can be reduced to planetary-mass, leaving just its crystallised carbon–oxygen core. A likely example of this is PSR J1719-1438 b.
Cold planets have a maximum radius because adding more mass at that point causes the planet to compress under the weight instead of increasing the radius. The maximum radius for solid planets is smaller than the maximum radius for gas planets.
Shape.
When the size of a planet is described using its radius this is approximating the shape by a sphere. However, the rotation of a planet causes it to be flattened at the poles so that the equatorial radius is larger than the polar radius, making it closer to an oblate spheroid. The oblateness of transiting exoplanets will affect the transit light curves. At the limits of current technology it has been possible to show that HD 189733b is less oblate than Saturn.
If the planet is close to its star, then gravitational tides will elongate the planet in the direction of the star, so that the planet will be closer to a triaxial ellipsoid. Because tidal deformation is along a line between the planet and the star, it is difficult to detect from transit photometry—it will have an order of magnitude less effect on the transit light curves than that caused by rotational deformation even in cases where the tidal deformation is larger than rotational deformation (such as is the case for tidally locked hot Jupiters). Material rigidity of rocky planets and rocky cores of gas planets will cause further deviations from the aforementioned shapes. Thermal tides caused by unevenly irradiated surfaces are another factor.
Atmosphere.
As of February 2014, more than fifty transiting and five directly imaged exoplanet atmospheres have been observed, resulting in detection of molecular spectral features; observation of day–night temperature gradients; and constraints on vertical atmospheric structure. Also, an atmosphere has been detected on the non-transiting hot Jupiter Tau Boötis b.
Spectroscopic measurements can be used to study a transiting planet's atmospheric composition, temperature, pressure, and scale height, and hence can be used to determine its mass.
Stellar light is polarized by atmospheric molecules; this could be detected with a polarimeter. HD 189733 b has been studied by polarimetry.
Extrasolar planets have phases similar to the phases of the Moon. By observing the exact variation of brightness with phase, astronomers can calculate atmospheric-particle sizes.
Atmospheric composition.
In 2001 sodium was detected in the atmosphere of HD 209458 b.
In 2008 water, carbon monoxide, carbon dioxide and methane were detected in the atmosphere of HD 189733 b.
In 2013 water was detected in the atmospheres of HD 209458 b, XO-1b, WASP-12b, WASP-17b, and WASP-19b.
In July 2014, NASA announced finding very dry atmospheres on three exoplanets (HD 189733b, HD 209458b, WASP-12b) orbiting Sun-like stars.
In September 2014, NASA reported that HAT-P-11b is the first Neptune-sized exoplanet known to have a relatively cloud-free atmosphere and, as well, the first time molecules of any kind have been found, specifically water vapor, on such a relatively small exoplanet.
The presence of oxygen may be detectable by ground-based telescopes, which, if discovered, would suggest the presence of life on an exoplanet.
Atmospheric circulation.
The atmospheric circulation of planets that rotate more slowly or have a thicker atmosphere allows more heat to flow to the poles which reduces the temperature differences between the poles and the equator.
Clouds.
In October 2013, the detection of clouds in the atmosphere of Kepler-7b was announced, and, in December 2013, also in the atmospheres of GJ 436 b and GJ 1214 b.
Precipitation.
Precipitation in the form of liquid (rain) or solid (snow) varies in composition depending on atmospheric temperature, pressure, composition, and altitude. Hot atmospheres could have iron rain, molten-glass rain, and rain made from rocky minerals such as enstatite, corundum, spinel, and wollastonite. Deep in the atmospheres of gas giants it could rain diamonds and helium containing dissolved neon.
Abiotic oxygen.
The processes of life result in a mixture of chemicals that are not in chemical equilibrium but there are also abiotic disequilibrium processes that need to be considered. The most robust atmospheric biosignature is often considered to be molecular oxygen O2 and its photochemical byproduct ozone O3. The photolysis of water H2O by UV rays followed by hydrodynamic escape of hydrogen can lead to a build-up of oxygen in planets close to their star undergoing runaway greenhouse effect. For planets in the habitable zone it was believed that water photolysis would be strongly limited by cold-trapping of water vapour in the lower atmosphere. However the extent of H2O cold-trapping depends strongly on the amount of non-condensible gases in the atmosphere such as nitrogen N2 and argon. In the absence of such gases the likelihood of build-up of oxygen also depends in complex ways on the planet's accretion history, internal chemistry, atmospheric dynamics and orbital state. Therefore, oxygen on its own cannot be considered a robust biosignature. The ratio of nitrogen and argon to oxygen could be detected by studying thermal phase curves or by transit transmission spectroscopy measurement of the spectral Rayleigh scattering slope in a clear-sky (i.e. aerosol-free) atmosphere.
Surface.
Surface composition.
Surface features can be distinguished from atmospheric features by comparing emission and reflection spectroscopy with transmission spectroscopy. Mid-infrared spectroscopy of exoplanets may detect rocky surfaces, and near-infrared may identify magma oceans or high-temperature lavas, hydrated silicate surfaces and water ice, giving an unambiguous method to distinguish between rocky and gaseous exoplanets.
Surface temperature.
One can estimate the temperature of an exoplanet based on the intensity of the light it receives from its parent star. For example, the planet OGLE-2005-BLG-390Lb is estimated to have a surface temperature of roughly −220 °C (50 K). However, such estimates may be substantially in error because they depend on the planet's usually unknown albedo, and because factors such as the greenhouse effect may introduce unknown complications. A few planets have had their temperature measured by observing the variation in infrared radiation as the planet moves around in its orbit and is eclipsed by its parent star. For example, the planet HD 189733b has been found to have an average temperature of 1205±9 K (932±9 °C) on its dayside and 973±33 K (700±33 °C) on its nightside.
General features.
Color and brightness.
In 2013 the color of an exoplanet was found for the first time. The best-fit albedo measurements of HD 189733b suggest that it is deep dark blue.
The apparent brightness (apparent magnitude) of a planet depends on how far away the observer is, how reflective the planet is (albedo), and how much light the planet receives from its star, which depends on how far the planet is from the star and how bright the star is. So, a planet with a low albedo that is close to its star can appear brighter than a planet with high albedo that is far from the star.
The darkest known planet in terms of geometric albedo is TrES-2b, a hot Jupiter that reflects less than 1% of the light from its star, making it less reflective than coal or black acrylic paint. Hot Jupiters are expected to be quite dark due to sodium and potassium in their atmospheres but it is not known why TrES-2b is so dark—it could be due to an unknown chemical.
For gas giants, geometric albedo generally decreases with increasing metallicity or atmospheric temperature unless there are clouds to modify this effect. Increased cloud-column depth increases the albedo at optical wavelengths, but decreases it at some infrared wavelengths. Optical albedo increases with age, because older planets have higher cloud-column depths. Optical albedo decreases with increasing mass, because higher-mass giant planets have higher surface gravities, which produces lower cloud-column depths. Also, elliptical orbits can cause major fluctuations in atmospheric composition, which can have a significant effect.
There is more thermal emission than reflection at some near-infrared wavelengths for massive and/or young gas giants. So, although optical brightness is fully phase-dependent, this is not always the case in the near infrared.
Temperatures of gas giants reduce over time and with distance from their star. Lowering the temperature increases optical albedo even without clouds. At a sufficiently low temperature, water clouds form, which further increase optical albedo. At even lower temperatures ammonia clouds form, resulting in the highest albedos at most optical and near-infrared wavelengths.
Magnetic field.
Interaction between a close-in planet's magnetic field and a star can produce spots on the star in a similar way to how the Galilean moons produce aurorae on Jupiter.
Auroral radio emissions could be detected with radio telescopes such as LOFAR.
The radio emissions could enable determination of the rotation rate of a planet which is difficult to detect otherwise.
Earth's magnetic field results from its flowing liquid metallic core, but in super-Earths the mass can produce high pressures with large viscosities and high melting temperatures which could prevent the interiors from separating into different layers and so result in undifferentiated coreless mantles. Magnesium oxide, which is rocky on Earth, can be a liquid metal at the pressures and temperatures found in super-Earths and could generate a magnetic field in the mantles of super-Earths.
Hot Jupiters have been observed to have a larger radius than expected. This could be caused by the interaction between the stellar wind and the planet's magnetosphere creating an electric current through the planet that heats it up causing it to expand. The more magnetically active a star is the greater the stellar wind and the larger the electric current leading to more heating and expansion of the planet. This theory matches the observation that stellar activity is correlated with inflated planetary radii.
Plate tectonics.
On Earth-sized planets, plate tectonics is more likely if there are oceans of water; however, in 2007 two independent teams of researchers came to opposing conclusions about the likelihood of plate tectonics on larger super-earths with one team saying that plate tectonics would be episodic or stagnant and the other team saying that plate tectonics is very likely on super-earths even if the planet is dry.
If super-earths have more than 80 times as much water as Earth then they become ocean planets with all land completely submerged. However, if there is less water than this limit, then the deep water cycle will move enough water between the oceans and mantle to allow continents to exist.
Rings.
The star 1SWASP J140747.93-394542.6 is orbited by an object that is circled by a ring system much larger than Saturn's rings. However, the mass of the object is not known; it could be a brown dwarf or low-mass star instead of a planet.
The brightness of optical images of Fomalhaut b could be due to starlight reflecting off a circumplanetary ring system with a radius between 20 to 40 times that of Jupiter's radius, about the size of the orbits of the Galilean moons.
The rings of the Solar System's gas giants are aligned with their planet's equator. However for exoplanets that orbit close to their star, tidal forces from the star would lead to the outermost rings of a planet being aligned with the planet's orbital plane around the star. A planet's innermost rings would still be aligned with the planet's equator so that if the planet has a tilted rotational axis, then the different alignments between the inner and outer rings would create a warped ring system.
Moons.
In December 2013 a candidate exomoon of a rogue planet was announced. No exomoons have been confirmed so far.
Comet-like tails.
KIC 12557548 b is a small rocky planet, very close to it star, that is evaporating and leaving a trailing tail of cloud and dust like a comet. The dust could be ash erupting from volcanos and escaping due to the small planet's low surface-gravity, or it could be from metals that are vaporized by the high temperatures of being so close to the star with the metal vapor then condensing into dust.
Habitability.
Habitable zone.
The habitable zone around a star is the region where the temperature is just right to allow liquid water to exist on a planet; that is, not too close to the star for the water to evaporate and not too far away from the star for the water to freeze. The heat produced by stars varies depending on the size and age of the star so that the habitable zone can be at different distances. Also, the atmospheric conditions on the planet influence the planet's ability to retain heat so that the location of the habitable zone is also specific to each type of planet: desert planets (also known as dry planets), with very little water, will have less water vapor in the atmosphere than Earth and so have a reduced greenhouse effect, meaning that a desert planet could maintain oases of water closer to its star than Earth is to the Sun. The lack of water also means there is less ice to reflect heat into space, so the outer edge of desert-planet habitable zones is further out. Rocky planets with a thick hydrogen atmosphere could maintain surface water much further out than the Earth–Sun distance.
Planetary rotation rate is one of the major factors determining the circulation of the atmosphere and hence the pattern of clouds: slowly rotating planets create thick clouds that reflect more and so can be habitable much closer to their star. Earth with its current atmosphere would be habitable in Venus's orbit, if it had Venus's slow rotation, so Venus must have had a higher rotation rate in the past if it lost its water ocean as a result of going through a runaway greenhouse effect, but if Venus never had an ocean because water vapor was lost to space during its formation before it could cool to form an ocean, Venus could have had its slow rotation throughout its history.
Habitable zones have usually been defined in terms of surface temperature, however over half of Earth's biomass is from subsurface microbes, and the temperature increases as you go deeper underground, so the subsurface can be conducive for life when the surface is frozen and if this is considered, the habitable zone extends much further from the star, even rogue planets could have liquid water at sufficient depths underground. In an earlier era of the universe the temperature of the cosmic microwave background would have allowed any rocky planets that existed to have liquid water on their surface regardless of their distance from a star. Jupiter-like planets might not be habitable, but they could have habitable moons.
Ice ages and snowball states.
The outer edge of the habitable zone is where planets will be completely frozen but even planets well inside the habitable zone can periodically become frozen. If orbital fluctuations or other causes produce cooling then this creates more ice but ice reflects sunlight causing even more cooling creating a feedback loop until the planet is completely or nearly completely frozen. When the surface is frozen this stops carbon dioxide weathering resulting in a build-up of carbon dioxide in the atmosphere from volcanic emissions. This creates a greenhouse effect which unfreezes the planet again. Planets with a large axial tilt are less likely to enter snowball states and can retain liquid water further from their star. Large fluctuations of axial tilt can have even more of a warming effect than a fixed large tilt. Paradoxically planets around cooler stars, such as red dwarfs, are less likely to enter snowball states because the infrared radiation emitted by cooler stars is mostly at wavelengths that are absorbed by ice which heats it up.
Tidal heating.
If a planet has an eccentric orbit then tidal heating can provide another source of energy besides stellar irradiation. This means that eccentric planets in the radiative habitable zone can be too hot for liquid water (Tidal Venus). Tides also circularize orbits over time so there could be planets in the habitable zone with circular orbits that have no water because they used to have eccentric orbits. Eccentric planets further out than the radiative habitable zone would still have frozen surfaces but the tidal heating could create a subsurface ocean similar to Europa's. In some planetary systems, such as in the Upsilon Andromedae system, the eccentricity of orbits is maintained or even periodically varied by perturbations from other planets in the system. Tidal heating can cause outgassing from the mantle, contributing to the formation and replenishment of an atmosphere.
Potentially habitable planets.
Confirmed planet discoveries in the habitable zone include the Kepler-22b, the first super-Earth located in the habitable zone of a Sun-like star. In September 2012, the discovery of two planets orbiting the red dwarf Gliese 163 was announced. One of the planets, Gliese 163 c, about 6.9 times the mass of Earth and somewhat hotter, was considered to be within the habitable zone. In 2013, three more potentially habitable planets, Kepler-62 e, Kepler-62 f, and Kepler-69 c, orbiting Kepler-62 and Kepler-69 respectively, were discovered. All three planets were super-Earths and may be covered by oceans thousands of kilometers deep.
Earth-size planets.
In November 2013 it was announced that 22±8% of Sun-like stars have an Earth-sized planet in the habitable zone.
Assuming 200 billion stars in the Milky Way, that would be 11 billion potentially habitable Earths, rising to 40 billion if red dwarfs are included.
Kepler-186f is the first Earth-sized planet in a habitable zone to have been discovered, a 1.1-Earth-radius planet in the habitable zone of a red dwarf, announced in April 2014.
In February 2013, researchers calculated that up to 6% of small red dwarfs may have planets with Earth-like properties. This suggests that the closest "alien Earth" to the Solar System could be 13 light-years away. The estimated distance increases to 21 light-years when a 95 percent confidence interval is used. In March 2013 a revised estimate based on a more accurate consideration of the size of the habitable zone around red dwarfs gave an occurrence rate of 50% for Earth-size planets in the habitable zone of red dwarfs.
Cultural impact.
On 9 May 2013, a by two United States House of Representatives subcommittees discussed "", prompted by the discovery of exoplanet Kepler-62f, along with Kepler-62e and Kepler-62c. A related special issue of the journal "Science", published earlier, described the discovery of the exoplanets.
Further reading.
Water.
After hydrogen and helium, oxygen is the most common element in many planetary systems (in some systems carbon is more common than oxygen), and water H2O one of the most common compounds. Gas giants are composed mostly of hydrogen and helium, but most planets are between the size of Earth and Neptune, where many planets will have deep water oceans covering the entire surface in addition to a H–He envelope.

</doc>
<doc id="9764" url="http://en.wikipedia.org/wiki?curid=9764" title="Emma Goldman">
Emma Goldman

Emma Goldman (June 27 [O.S. June 15], 1869 – May 14, 1940) was an anarchist known for her political activism, writing, and speeches. She played a pivotal role in the development of anarchist political philosophy in North America and Europe in the first half of the 20th century.
Born in Kovno in the Russian Empire (present-day Kaunas, Lithuania), Goldman emigrated to the US in 1885 and lived in New York City, where she joined the burgeoning anarchist movement in 1889. Attracted to anarchism after the Haymarket affair, Goldman became a writer and a renowned lecturer on anarchist philosophy, women's rights, and social issues, attracting crowds of thousands. She and anarchist writer Alexander Berkman, her lover and lifelong friend, planned to assassinate industrialist and financier Henry Clay Frick as an act of propaganda of the deed. Although Frick survived the attempt on his life, Berkman was sentenced to 22 years in prison. Goldman was imprisoned several times in the years that followed, for "inciting to riot" and illegally distributing information about birth control. In 1906, Goldman founded the anarchist journal "Mother Earth".
In 1917, Goldman and Berkman were sentenced to two years in jail for conspiring to "induce persons not to register" for the newly instated draft. After their release from prison, they were arrested—along with hundreds of others—and deported to Russia. Initially supportive of that country's Bolshevik revolution, Goldman reversed her opinion in the wake of the Kronstadt rebellion and denounced the Soviet Union for its violent repression of independent voices. In 1923, she published a book about her experiences, "My Disillusionment in Russia". While living in England, Canada, and France, she wrote an autobiography called "Living My Life". After the outbreak of the Spanish Civil War, she traveled to Spain to support the anarchist revolution there. She died in Toronto on May 14, 1940, aged 70.
During her life, Goldman was lionized as a free-thinking "rebel woman" by admirers, and denounced by critics as an advocate of politically motivated murder and violent revolution. Her writing and lectures spanned a wide variety of issues, including prisons, atheism, freedom of speech, militarism, capitalism, marriage, free love, and homosexuality. Although she distanced herself from first-wave feminism and its efforts toward women's suffrage, she developed new ways of incorporating gender politics into anarchism. After decades of obscurity, Goldman's iconic status was revived in the 1970s, when feminist and anarchist scholars rekindled popular interest in her life.
Biography.
Family.
Emma Goldman's Orthodox Jewish family lived in the Lithuanian city of Kaunas (called Kovno at the time, part of the Russian Empire). Goldman's mother Taube Bienowitch had been married before, to a man with whom she had two daughters—Helena in 1860 and Lena in 1862. When her first husband died of tuberculosis, Taube was devastated. Goldman later wrote: "Whatever love she had had died with the young man to whom she had been married at the age of fifteen."
Taube's second marriage was arranged by her family and, as Goldman puts it, "mismated from the first". Her second husband, Abraham Goldman, invested Taube's inheritance in a business that quickly failed. The ensuing hardship combined with the emotional distance of husband and wife to make the household a tense place for the children. When Taube became pregnant, Abraham hoped desperately for a son; a daughter, he believed, would serve as one more sign of failure. They eventually had three sons, but their first child was Emma.
Emma Goldman was born on June 27, 1869. Her father used violence to punish his children, beating them when they disobeyed him. He used a whip only on Emma, the most rebellious of them. Her mother provided scarce comfort, calling only rarely on Abraham to tone down his beatings. Goldman later speculated that her father's furious temper was at least partly a result of sexual frustration.
Goldman's relationships with her elder half-sisters, Helena and Lena, were a study in contrasts. Helena, the oldest, provided the comfort they lacked from their mother; she filled Goldman's childhood with "whatever joy it had". Lena, however, was distant and uncharitable. The three sisters were joined by brothers Louis (who died at the age of six), Herman (born in 1872), and Moishe (born in 1879).
Adolescence.
When Emma was a young girl, the Goldman family moved to the village of Papilė, where her father ran an inn. While her sisters worked, she became friends with a servant named Petrushka, who excited her "first erotic sensations". Later in Papilė she witnessed a peasant being whipped with a knout in the street. This event traumatized her and contributed to her lifelong distaste for violent authority.
At the age of seven, Goldman moved with her family to the Prussian city of Königsberg (then part of the German Empire), and she enrolled in a Realschule. One teacher punished disobedient students—targeting Goldman in particular—by beating their hands with a ruler. Another teacher tried to molest his female students and was fired when Goldman fought back. She found a sympathetic mentor in her German teacher, who loaned her books and even took her to an opera. A passionate student, Goldman passed the exam for admission into a gymnasium, but her religion teacher refused to provide a certificate of good behavior and she was unable to attend.
The family moved to the Russian city of Saint Petersburg, where her father opened one unsuccessful store after another. Their poverty forced the children to work, and Goldman took an assortment of jobs including one in a corset shop. As a teenager Goldman begged her father to allow her to return to school, but instead he threw her French book into the fire and shouted: "Girls do not have to learn much! All a Jewish daughter needs to know is how to prepare gefilte fish, cut noodles fine, and give the man plenty of children."
Goldman pursued an independent education on her own, however, and soon began to study the political turmoil around her, particularly the Nihilists responsible for assassinating Alexander II of Russia. The ensuing turmoil intrigued Goldman, even though she did not fully understand it at the time. When she read Chernyshevsky's novel, "What Is to Be Done?" (1863), she found a role model in the protagonist Vera, who adopts a Nihilist philosophy and escapes her repressive family to live freely and organize a sewing cooperative. The book enthralled Goldman and remained a source of inspiration throughout her life.
Her father, meanwhile, continued to insist on a domestic future for her, and he tried to arrange for her to be married at the age of fifteen. They fought about the issue constantly; he complained that she was becoming a "loose" woman, and she insisted that she would marry for love alone. At the corset shop, she was forced to fend off unwelcome advances from Russian officers and other men. One persistent suitor took her into a hotel room and committed what Goldman called "violent contact"; two biographers call it rape. She was stunned by the experience, overcome by "shock at the discovery that the contact between man and woman could be so brutal and painful." Goldman felt that the encounter forever soured her interactions with men.
Rochester, New York.
In 1885, Helena made plans to move to New York to join her sister Lena and her husband. Goldman wanted to join her sister, but their father refused to allow it. Despite Helena's offer to pay for the trip, Abraham turned a deaf ear to their pleas. Desperate, Goldman threatened to throw herself into the Neva River if she could not go. He finally agreed, and on December 29, 1885, Helena and Emma arrived at New York's Castle Garden. They moved into the Rochester home Lena had made with her husband Samuel. Fleeing the rising antisemitism of Saint Petersburg, their parents and brothers joined them a year later. Goldman began working as a seamstress, sewing overcoats for more than ten hours a day, earning two and a half dollars a week. She asked for a raise and was denied; she quit and took work at a smaller shop nearby.
At her new job, Goldman met a fellow worker named Jacob Kershner, who shared her love for books, dancing, and traveling, as well as her frustration with the monotony of factory work. After four months, they married in February 1887. Once he moved in with Goldman's family, however, their relationship faltered. On their wedding night she discovered that he was impotent; they became emotionally and physically distant. Before long he became jealous and suspicious. She, meanwhile, was becoming more engaged with the political turmoil around her—particularly the fallout of the 1886 Haymarket affair in Chicago and the anti-authoritarian political philosophy of anarchism. Less than a year after the wedding, they were divorced; he begged her to return and threatened to poison himself if she did not. They reunited, but after three months she left once again. Her parents considered her behavior "loose" and refused to allow Goldman into their home. Carrying her sewing machine in one hand and a bag with five dollars in the other, she left Rochester and headed southeast to New York City.
Most and Berkman.
On her first day in the city, Goldman met two men who would forever change her life. At Sachs's Café, a gathering place for radicals, she was introduced to Alexander Berkman, an anarchist who invited her to a public speech that evening. They went to hear Johann Most, editor of a radical publication called "Freiheit" and an advocate of "propaganda of the deed"—the use of violence to instigate change. She was impressed by his fiery oration, and he took her under his wing, training her in methods of public speaking. He encouraged her vigorously, telling her that she was "to take my place when I am gone." One of her first public talks in support of "the Cause" was in Rochester. After convincing Helena not to tell their parents of her speech, Goldman found her mind a blank once on stage. Suddenly,something strange happened. In a flash I saw it—every incident of my three years in Rochester: the Garson factory, its drudgery and humiliation, the failure of my marriage, the Chicago crime...I began to speak. Words I had never heard myself utter before came pouring forth, faster and faster. They came with passionate intensity...The audience had vanished, the hall itself had disappeared; I was conscious only of my own words, of my ecstatic song.
Enchanted by the experience, she refined her public persona during subsequent engagements. Quickly, however, she found herself arguing with Most over her independence. After a momentous speech in Cleveland, she felt as though she had become "a parrot repeating Most's views" and resolved to express herself on the stage. Upon her return in New York, Most became furious and told her: "Who is not with me is against me!" She left "Freiheit" and joined with another publication, "Die Autonomie".
Meanwhile, she had begun a friendship with Berkman, whom she affectionately called Sasha. Before long they became lovers and moved into a communal apartment with his cousin Modest "Fedya" Stein and Goldman's friend, Helen Minkin, in rural Woodstock, Illinois. Although their relationship had numerous difficulties, Goldman and Berkman would share a close bond for decades, united by their anarchist principles and commitment to personal equality.
Homestead plot.
One of the first political moments that brought Berkman and Goldman together was the Homestead Strike. In June 1892, a steel plant in Homestead, Pennsylvania owned by Andrew Carnegie became the focus of national attention when talks between the Carnegie Steel Company and the Amalgamated Association of Iron and Steel Workers (AA) broke down. The factory's manager was Henry Clay Frick, a fierce opponent of the union. When a final round of talks failed at the end of June, management closed the plant and locked out the workers, who immediately went on strike. Strikebreakers were brought in and the company hired Pinkerton guards to protect them. On July 6, a fight broke out between three hundred Pinkerton guards and a crowd of armed union workers. During the twelve-hour gunfight, seven guards and nine strikers were killed.
When a majority of the nation's newspapers came out in support of the strikers, Goldman and Berkman resolved to assassinate Frick, an action they expected would inspire the workers to revolt against the capitalist system. Berkman chose to carry out the assassination, and ordered Goldman to stay behind in order to explain his motives after he went to jail. He would be in charge of the deed; she of the word. Berkman tried and failed to make a bomb, then set off for Pittsburgh to buy a gun and a suit of decent clothes. Goldman, meanwhile, decided to help fund the scheme through prostitution. Remembering the character of Sonya in Fyodor Dostoevsky's novel "Crime and Punishment" (1866), she mused: "She had become a prostitute in order to support her little brothers and sisters...Sensitive Sonya could sell her body; why not I?" Once on the street, she caught the eye of a man who took her into a saloon, bought her a beer, gave her ten dollars, informed her she did not have "the knack", and told her to quit the business. She was "too astounded for speech". She wrote to Helena, claiming illness, and asked her for fifteen dollars.
On July 23, Berkman gained access to Frick's office with a concealed handgun and shot Frick three times, then stabbed him in the leg. A group of workers—far from joining in his "attentat"—beat Berkman unconscious, and he was carried away by the police. Berkman was convicted of attempted murder and sentenced to 22 years in prison; his absence from her life was very difficult for Goldman. Convinced Goldman was involved in the plot, police raided her apartment and—finding no evidence—pressured her landlord into evicting her. Worse, the "attentat" had failed to rouse the masses: workers and anarchists alike condemned Berkman's action. Johann Most, their former mentor, lashed out at Berkman and the assassination attempt. Furious at these attacks, Goldman brought a toy horsewhip to a public lecture and demanded, onstage, that Most explain his betrayal. He dismissed her, whereupon she struck him with the whip, broke it on her knee, and hurled the pieces at him. She later regretted her assault, confiding to a friend: "At the age of twenty-three, one does not reason."
"Inciting to riot".
When the Panic of 1893 struck in the following year, the United States suffered one of its worst economic crises ever. By year's end, the unemployment rate was higher than 20%, and "hunger demonstrations" sometimes gave way to riots. Goldman began speaking to crowds of frustrated men and women in New York. On August 21, she spoke to a crowd of nearly 3,000 people in Union Square, where she encouraged unemployed workers to take immediate action. Her exact words are unclear: undercover agents insist she ordered the crowd to "take everything ... by force", while Goldman later recounted this message: "Well then, demonstrate before the palaces of the rich; demand work. If they do not give you work, demand bread. If they deny you both, take bread." Later in court, Detective-Sergeant Charles Jacobs offered yet another version of her speech.
A week later she was arrested in Philadelphia and returned to New York City for trial, charged with "inciting to riot". During the train ride, Jacobs offered to drop the charges against her if she would inform on other radicals in the area. She responded by throwing a glass of ice water in his face. As she awaited trial, Goldman was visited by Nellie Bly, a reporter for the "New York World". She spent two hours talking to Goldman, and wrote a positive article about the woman she described as a "modern Joan of Arc".
Despite this positive publicity, the jury was persuaded by Jacobs' testimony and scared by Goldman's politics. The assistant District Attorney questioned Goldman about her anarchism, as well as her atheism; the judge spoke of her as "a dangerous woman". She was sentenced to one year in the Blackwell's Island Penitentiary. Once inside she suffered an attack of rheumatism and was sent to the infirmary; there she befriended a visiting doctor and began studying medicine. She also read dozens of books, including works by the American activist-writers Ralph Waldo Emerson and Henry David Thoreau; novelist Nathaniel Hawthorne; poet Walt Whitman, and philosopher John Stuart Mill. When she was released after ten months, a raucous crowd of nearly three thousand people greeted her at the Thalia Theater in New York City. She soon became swamped with requests for interviews and lectures.
To make money, Goldman decided to pursue the medical work she had studied in prison. However, her preferred fields of specialization—midwifery and massage—were not available to nursing students in the US. Thus, she sailed to Europe, lecturing in London, Glasgow, and Edinburgh. She met with renowned anarchists like Errico Malatesta, Louise Michel, and Peter Kropotkin. In Vienna, she received two diplomas and put them immediately to use back in the US. Alternating between lectures and midwifery, she conducted the first cross-country tour by an anarchist speaker. In November 1899 she returned to Europe, where she met the anarchist Hippolyte Havel, with whom she went to France and helped organize the International Anarchist Congress on the outskirts of Paris.
McKinley assassination.
On September 6, 1901, Leon Czolgosz, an unemployed factory worker and registered Republican with a history of mental illness, shot US President William McKinley twice during a public speaking event in Buffalo, New York. McKinley was hit in the breastbone and stomach, and died eight days later. Czolgosz was arrested, and interrogated around the clock. During interrogation he claimed to be an Anarchist and said he had been inspired to act after attending a speech held by Goldman. The authorities used this as a pretext to charge Goldman with planning McKinley's assassination. They tracked her to a residence in Chicago she shared with Havel, as well as Mary and Abe Isaak, an anarchist couple. Goldman was arrested, along with Abe Isaak, Havel, and ten other anarchists.
Earlier, Czolgosz had tried but failed to become friends with Goldman and her companions. During a talk in Cleveland, Czolgosz had approached Goldman and asked her advice on which books he should read. In July 1901, he had appeared at the Isaak house, asking a series of unusual questions. They assumed he was an infiltrator, like a number of police agents sent to spy on radical groups. They had remained distant from him, and Abe Isaak sent a notice to associates warning of "another spy".
Although Czolgosz repeatedly denied Goldman's involvement, the police held her in close custody, subjecting her to what she called the "third degree". She explained their distrust of him, and it was clear she had not had any significant contact with Czolgosz. No evidence was found linking Goldman to the attack, and she was eventually released after two weeks of detention. Before McKinley died, Goldman offered to provide nursing care, referring to him as "merely a human being". Czolgosz, despite considerable evidence of mental illness, was convicted of murder and executed.
Throughout her detention and after her release, Goldman steadfastly refused to condemn Czolgosz's actions, standing virtually alone in doing so. Friends and supporters—including Berkman—urged her to quit his cause. But Goldman defended Czolgosz as a "supersensitive being" and chastised other anarchists for abandoning him. She was vilified in the press as the "high priestess of anarchy", while many newspapers declared the anarchist movement responsible for the murder. In the wake of these events, socialism gained support over anarchism among US radicals. McKinley's successor, Theodore Roosevelt, declared his intent to crack down "not only against anarchists, but against all active and passive sympathizers with anarchists".
"Mother Earth" and Berkman's release.
After Czolgosz's execution, Goldman withdrew from the world. Scorned by her fellow anarchists, vilified by the press, and separated from her love, she retreated into anonymity and nursing. "It was bitter and hard to face life anew," she wrote later. Using the name E. G. Smith, she vanished from public life and took on a series of private nursing jobs. When the US Congress passed the Anarchist Exclusion Act, however, a new wave of activism rose to oppose it, carrying Goldman back into the movement. A coalition of people and organizations across the left end of the political spectrum opposed the law on grounds that it violated freedom of speech, and she had the nation's ear once again.
When an English anarchist named John Turner was arrested under the Anarchist Exclusion Act and threatened with deportation, Goldman joined forces with the Free Speech League to champion his cause. The league enlisted the aid of Clarence Darrow and Edgar Lee Masters, who took Turner's case to the US Supreme Court. Although Turner and the League lost, Goldman considered it a victory of propaganda. She had returned to anarchist activism, but it was taking its toll on her. "I never felt so weighed down," she wrote to Berkman. "I fear I am forever doomed to remain public property and to have my life worn out through the care for the lives of others."
In 1906, Goldman decided to start a publication of her own, "a place of expression for the young idealists in arts and letters". "Mother Earth" was staffed by a cadre of radical activists, including Hippolyte Havel, Max Baginski, and Leonard Abbott. In addition to publishing original works by its editors and anarchists around the world, "Mother Earth" reprinted selections from a variety of writers. These included the French philosopher Pierre-Joseph Proudhon, Russian anarchist Peter Kropotkin, German philosopher Friedrich Nietzsche, and British writer Mary Wollstonecraft. Goldman wrote frequently about anarchism, politics, labor issues, atheism, sexuality, and feminism.
On May 18 of the same year, Alexander Berkman was released from prison. Carrying a bouquet of roses, she met him on the platform and found herself "seized by terror and pity" as she beheld his gaunt, pale form. Neither was able to speak; they returned to her home in silence. For weeks, he struggled to readjust to life on the outside; an abortive speaking tour ended in failure, and in Cleveland he purchased a revolver with the intent of killing himself. He returned to New York, however, and learned that Goldman had been arrested with a group of activists meeting to reflect on Czolgosz. Invigorated anew by this violation of freedom of assembly, he declared "My resurrection has come!" and set about securing their release.
Berkman took the helm of "Mother Earth" in 1907, while Goldman toured the country to raise funds to keep it functional. Editing the magazine was a revitalizing experience for Berkman; his relationship with Goldman faltered, however, and he had an affair with a 15-year-old anarchist named Becky Edelsohn. Goldman was pained by his rejection of her, but considered it a consequence of his prison experience. Later that year she served as a delegate from the US to the International Anarchist Congress of Amsterdam. Anarchists and syndicalists from around the world gathered to sort out the tension between the two ideologies, but no decisive agreement was reached. Goldman returned to the US and continued speaking to large audiences.
Reitman, essays, and birth control.
For the next ten years, Goldman traveled around the country nonstop, delivering lectures and agitating for anarchism. The coalitions formed in opposition to the Anarchist Exclusion Act had given her an appreciation for reaching out to those of other political persuasions. When the US Justice Department sent spies to observe, they reported the meetings as "packed". Writers, journalists, artists, judges, and workers from across the spectrum spoke of her "magnetic power", her "convincing presence", her "force, eloquence, and fire".
In the spring of 1908, Goldman met and fell in love with Ben Reitman, the so-called "Hobo doctor". Having grown up in Chicago's tenderloin district, Reitman spent several years as a drifter before attaining a medical degree from the College of Physicians and Surgeons of Chicago. As a doctor, he attended to people suffering from poverty and illness, particularly venereal diseases. He and Goldman began an affair; they shared a commitment to free love, but whereas Reitman took a variety of lovers, Goldman did not. She tried to reconcile her feelings of jealousy with a belief in freedom of the heart, but found it difficult.
Two years later, Goldman began feeling frustrated with lecture audiences. She yearned to "reach the few who really want to learn, rather than the many who come to be amused". Thus she collected a series of speeches and items she had written for "Mother Earth" and published a book called "Anarchism and Other Essays". Covering a wide variety of topics, Goldman tries to represent "the mental and soul struggles of twenty-one years". In addition to a comprehensive look at anarchism and its criticisms, the book includes essays on patriotism, women's suffrage, marriage, and prisons.
When Margaret Sanger, an advocate of access to contraception, coined the term "birth control" and disseminated information about various methods in the June 1914 issue of her magazine "The Woman Rebel", she received aggressive support from Goldman, who had already been an active in efforts to increase birth control access for several years. In 1916, Goldman was arrested for giving lessons in public on how to use contraceptives. Sanger, too, was arrested under the Comstock Law, which prohibited the dissemination of "obscene, lewd, or lascivious articles"—including information relating to birth control. Although they later split from Sanger over charges of insufficient support, Goldman and Reitman distributed copies of Sanger's pamphlet "Family Limitation" (along with a similar essay of Reitman's). In 1915 Goldman conducted a nationwide speaking tour in part to raise awareness about contraception options. Although the nation's attitude toward the topic seemed to be liberalizing, Goldman was arrested on February 11, 1916, as she was about to give another public lecture. Goldman was charged with violating the Comstock Law. Refusing to pay a $100 fine, Goldman spent two weeks in a prison workhouse, which she saw as an "opportunity" to reconnect with those rejected by society.
World War I.
Although US President Woodrow Wilson was re-elected in 1916 under the slogan "He kept us out of the war", at the start of his second term he decided that Germany's continued deployment of unrestricted submarine warfare was sufficient cause for the US to enter World War I. Shortly afterward, Congress passed the Selective Service Act of 1917, which required all males aged 21–30 to register for military conscription. Goldman saw the decision as an exercise in militarist aggression, driven by capitalism. She declared in "Mother Earth" her intent to resist conscription, and to oppose US involvement in the war.
To this end, she and Berkman organized the No Conscription League of New York, which proclaimed: "We oppose conscription because we are internationalists, antimilitarists, and opposed to all wars waged by capitalistic governments." The group became a vanguard for anti-draft activism, and chapters began to appear in other cities. When police began raiding the group's public events to find young men who had not registered for the draft, however, Goldman and others focused their efforts on spreading pamphlets and other written work. In the midst of the nation's patriotic fervor, many elements of the political left refused to support the League's efforts. The Women's Peace Party, for example, ceased its opposition to the war once the US entered it. The Socialist Party of America took an official stance against US involvement, but supported Wilson in most of his activities.
On June 15, 1917, Goldman and Berkman were arrested during a raid of their offices which yielded "a wagon load of anarchist records and propaganda" for the authorities. "The New York Times" reported that Goldman asked to change into a more appropriate outfit, and emerged in a gown of "royal purple". The pair were charged with conspiracy to "induce persons not to register" under the newly enacted Espionage Act, and were held on US$25,000 bail each. Defending herself and Berkman during their trial, Goldman invoked the First Amendment, asking how the government could claim to fight for democracy abroad while suppressing free speech at home:
We say that if America has entered the war to make the world safe for democracy, she must first make democracy safe in America. How else is the world to take America seriously, when democracy at home is daily being outraged, free speech suppressed, peaceable assemblies broken up by overbearing and brutal gangsters in uniform; when free press is curtailed and every independent opinion gagged? Verily, poor as we are in democracy, how can we give of it to the world?
The jury saw it differently, and found them guilty. Judge Julius Marshuetz Mayer imposed the maximum sentence: two years' imprisonment, a $10,000 fine each, and the possibility of deportation after their release from prison. As she was transported to Missouri State Penitentiary (now Jefferson City Correctional Center), Goldman wrote to a friend: "Two years imprisonment for having made an uncompromising stand for one's ideal. Why that is a small price."
In prison, she was again assigned to work as a seamstress, under the eye of a "miserable gutter-snipe of a 21-year-old boy paid to get results". She met the socialist Kate Richards O'Hare, who had also been imprisoned under the Espionage Act. Although they differed on political strategy—Kate O'Hare believed in voting to achieve state power—the two women came together to agitate for better conditions among prisoners. Goldman also met and became friends with Gabriella Segata Antolini, an anarchist and follower of Luigi Galleani. Antolini had been arrested transporting a satchel filled with dynamite on a Chicago-bound train. She had refused to cooperate with authorities, and was sent to prison for 14 months. Working together to make life better for the other inmates, the three women became known as "The Trinity". Goldman was released on September 27, 1919.
Deportation.
Goldman and Berkman were released during America's Red Scare of 1919–20 when public anxiety about wartime pro-German activities had morphed into a pervasive fear of Bolshevism and the prospect of an imminent radical revolution. Attorney General Alexander Mitchell Palmer and J. Edgar Hoover, head of the US Department of Justice's General Intelligence Division, were intent on using the Anarchist Exclusion Act and its 1918 expansion to deport any non-citizens they could identify as advocates of anarchy or revolution. "Emma Goldman and Alexander Berkman," Hoover wrote while they were in prison, "are, beyond doubt, two of the most dangerous anarchists in this country and return to the community will result in undue harm."
At her deportation hearing on October 27, she refused to answer questions about her beliefs on the grounds that her American citizenship invalidated any attempt to deport her under the Anarchist Exclusion Act, which could be enforced only against non-citizens of the US. She presented a written statement instead: "Today so-called aliens are deported. Tomorrow native Americans will be banished. Already some patrioteers are suggesting that native American sons to whom democracy is a sacred ideal should be exiled." Louis Post at the Department of Labor, which had ultimate authority over deportation decisions, determined that the revocation of her husband's American citizenship in 1908 had revoked hers as well. After initially promising a court fight, she decided not to appeal his ruling.
The Labor Department included Goldman and Berkman among 249 aliens it deported "en masse", mostly people with only vague associations with radical groups who had been swept up in government raids in November. "Buford", a ship the press nicknamed the "Soviet Ark," sailed from the Army's New York Port of Embarkation on December 21. Some 58 enlisted men and four officers provided security on the journey and pistols were distributed to the crew. Most of the press approved enthusiastically. The Cleveland "Plain Dealer" wrote: "It is hoped and expected that other vessels, larger, more commodious, carrying similar cargoes, will follow in her wake." The ship landed her charges in Hanko, Finland on Saturday, January 17, 1920. Upon arrival in Finland, authorities there conducted the deportees to the Russian frontier under a flag of truce.
Russia.
Goldman initially viewed the Bolshevik revolution in a positive light. She wrote in "Mother Earth" that despite its dependence on Communist government, it represented "the most fundamental, far-reaching and all-embracing principles of human freedom and of economic well-being". By the time she neared Europe, however, she expressed fears about what was to come. She was worried about the ongoing Russian Civil War and the possibility of being seized by anti-Bolshevik forces. The state, anti-capitalist though it was, also posed a threat. "I could never in my life work within the confines of the State," she wrote to her niece, "Bolshevist or otherwise."
She quickly discovered that her fears were justified. Days after returning to Petrograd (Saint Petersburg), she was shocked to hear a party official refer to free speech as a "bourgeois superstition". As she and Berkman traveled around the country, they found repression, mismanagement, and corruption instead of the equality and worker empowerment they had dreamed of. Those who questioned the government were demonized as counter-revolutionaries, and workers labored under severe conditions. They met with Vladimir Lenin, who assured them that government suppression of press liberties was justified. He told them: "There can be no free speech in a revolutionary period." Berkman was more willing to forgive the government's actions in the name of "historical necessity", but he eventually joined Goldman in opposing the Soviet state's authority.
In March 1921, strikes erupted in Petrograd when workers took to the streets demanding better food rations and more union autonomy. Goldman and Berkman felt a responsibility to support the strikers, stating: "To remain silent now is impossible, even criminal." The unrest spread to the port town of Kronstadt, where a military response was ordered. In the fighting that ensued, approximately 1,000 rebelling sailors and soldiers were killed and two thousand more were arrested. In the wake of these events, Goldman and Berkman decided there was no future in the country for them. "More and more", she wrote, "we have come to the conclusion that we can do nothing here. And as we can not keep up a life of inactivity much longer we have decided to leave."
In December 1921, they left the country and went to the Latvian capital city of Riga. The US commissioner in that city wired officials in Washington DC, who began requesting information from other governments about the couple's activities. After a short trip to Stockholm, they moved to Berlin for several years; during this time she agreed to write a series of articles about her time in Russia for Joseph Pulitzer's newspaper, the "New York World". These were later collected and published in book form as "My Disillusionment in Russia" (1923) and "My Further Disillusionment in Russia" (1924). The titles of these books were added by the publishers to be scintillating and Goldman protested, albeit in vain.
England, Canada, and France.
Goldman found it difficult to acclimate to the German leftist community. Communists despised her outspokenness about Soviet repression; liberals derided her radicalism. While Berkman remained in Berlin helping Russian exiles, she moved to London in September 1924. Upon her arrival, the novelist Rebecca West arranged a reception dinner for her, attended by philosopher Bertrand Russell, novelist H. G. Wells, and more than two hundred others. When she spoke of her dissatisfaction with the Soviet government, the audience was shocked. Some left the gathering; others berated her for prematurely criticizing the Communist experiment. Later, in a letter, Russell declined to support her efforts at systemic change in the Soviet Union and ridiculed her anarchist idealism.
In 1925, the spectre of deportation loomed again, but a Scottish anarchist named James Colton offered to marry her and provide British citizenship. Although they were only distant acquaintances, she accepted and they were married on June 27, 1925. Her new status gave her peace of mind, and allowed her to travel to France and Canada. Life in London was stressful for Goldman; she wrote to Berkman: "I am awfully tired and so lonely and heartsick. It is a dreadful feeling to come back here from lectures and find not a kindred soul, no one who cares whether one is dead or alive." She worked on analytical studies of drama, expanding on the work she had published in 1914. But the audiences were "awful" and she never finished her second book on the subject.
Goldman traveled to Canada in 1927, just in time to receive news of the impending executions of Italian anarchists Nicola Sacco and Bartolomeo Vanzetti in Boston. Angered by the many irregularities of the case, she saw it as another travesty of justice in the US. She longed to join the mass demonstrations in Boston; memories of the Haymarket affair overwhelmed her, compounded by her isolation. "Then," she wrote, "I had my life before me to take up the cause for those killed. Now I have nothing."
In 1928, she began writing her autobiography, with the support of a group of admirers, including journalist H. L. Mencken, poet Edna St. Vincent Millay, novelist Theodore Dreiser and art collector Peggy Guggenheim, who raised $4,000 for her. She secured a cottage in the French coastal city of Saint-Tropez and spent two years recounting her life. Berkman offered sharply critical feedback, which she eventually incorporated at the price of a strain on their relationship. Goldman intended the book, "Living My Life", as a single volume for a price the working class could afford (she urged no more than $5.00); her publisher Alfred A. Knopf, however, released it as two volumes sold together for $7.50. Goldman was furious, but unable to force a change. Due in large part to the Great Depression, sales were sluggish despite keen interest from libraries around the US. Critical reviews were generally enthusiastic; "The New York Times", "The New Yorker", and "Saturday Review of Literature" all listed it as one of the year's top non-fiction books.
In 1933, Goldman received permission to lecture in the United States under the condition that she speak only about drama and her autobiography—but not current political events. She returned to New York on February 2, 1934 to generally positive press coverage—except from Communist publications. Soon she was surrounded by admirers and friends, besieged with invitations to talks and interviews. Her visa expired in May, and she went to Toronto in order to file another request to visit the US. However, this second attempt was denied. She stayed in Canada, writing articles for US publications.
In February and March 1936, Berkman underwent a pair of prostate gland operations. Recuperating in Nice and cared for by his companion, Emmy Eckstein, he missed Goldman's sixty-seventh birthday in Saint-Tropez in June. She wrote in sadness, but he never read the letter; she received a call in the middle of the night that Berkman was in great distress. She left for Nice immediately but when she arrived that morning, Goldman found that he had shot himself and was in a nearly comatose paralysis. He died the next day.
Spanish Civil War.
In July 1936, the Spanish Civil War started after an attempted "coup d'état" by parts of the Spanish Army against the government of the Second Spanish Republic. At the same time, the Spanish anarchists, fighting against the Nationalist forces, started an anarchist revolution. Goldman was invited to Barcelona and in an instant, as she wrote to her niece, "the crushing weight that was pressing down on my heart since Sasha's death left me as by magic". She was welcomed by the Confederación Nacional del Trabajo (CNT) and Federación Anarquista Ibérica (FAI) organizations, and for the first time in her life lived in a community run by and for anarchists, according to true anarchist principles. "In all my life", she wrote later, "I have not met with such warm hospitality, comradeship and solidarity." After touring a series of collectives in the province of Huesca, she told a group of workers: "Your revolution will destroy forever [the notion] that anarchism stands for chaos." She began editing the weekly "CNT-FAI Information Bulletin" and responded to English-language mail.
Goldman began to worry about the future of Spain's anarchism when the CNT-FAI joined a coalition government in 1937—against the core anarchist principle of abstaining from state structures—and, more distressingly, made repeated concessions to Communist forces in the name of uniting against fascism. She wrote that cooperating with Communists in Spain was "a denial of our comrades in Stalin's concentration camps". Russia, meanwhile, refused to send weapons to anarchist forces, and disinformation campaigns were being waged against the anarchists across Europe and the US. Her faith in the movement unshaken, Goldman returned to London as an official representative of the CNT-FAI.
Delivering lectures and giving interviews, Goldman enthusiastically supported the Spanish anarcho-syndicalists. She wrote regularly for "Spain and the World", a biweekly newspaper focusing on the civil war. In May 1937, however, Communist-led forces attacked anarchist strongholds and broke up agrarian collectives. Newspapers in England and elsewhere accepted the timeline of events offered by the Second Spanish Republic at face value. British journalist George Orwell, present for the crackdown, wrote: "[T]he accounts of the Barcelona riots in May ... beat everything I have ever seen for lying."
Goldman returned to Spain in September, but the CNT-FAI appeared to her like people "in a burning house". Worse, anarchists and other radicals around the world refused to support their cause. The Nationalist forces declared victory in Spain just before she returned to London. Frustrated by England's repressive atmosphere—which she called "more fascist than the fascists"—she returned to Canada in 1939. Her service to the anarchist cause in Spain was not forgotten, however. On her seventieth birthday, the former Secretary-General of the CNT-FAI, Mariano Vázquez, sent a message to her from Paris, praising her for her contributions and naming her as "our spiritual mother". She called it "the most beautiful tribute I have ever received".
Final years.
As the events preceding World War II began to unfold in Europe, Goldman reiterated her opposition to wars waged by governments. "[M]uch as I loathe Hitler, Mussolini, Stalin and Franco", she wrote to a friend, "I would not support a war against them and for the democracies which, in the last analysis, are only Fascist in disguise." She felt that England and France had missed their opportunity to oppose fascism, and that the coming war would only result in "a new form of madness in the world". This position was vastly unpopular, as Hitler's attacks on Jewish communities reverberated throughout the Jewish diaspora.
Death.
On Saturday, February 17, 1940, Goldman suffered a debilitating stroke. She became paralyzed on her right side, and although her hearing was unaffected, she could not speak. As one friend described it: "Just to think that here was Emma, the greatest orator in America, unable to utter one word." For three months she improved slightly, receiving visitors and on one occasion gesturing to her address book to signal that a friend might find friendly contacts during a trip to Mexico. She suffered another stroke on May 8, however, and on May 14 she died in Toronto, aged 70.
The US Immigration and Naturalization Service allowed her body to be brought back to the United States. She was buried in German Waldheim Cemetery (now named Forest Home Cemetery) in Forest Park, Illinois, a western suburb of Chicago, among the graves of other labor and social activists including Ben Reitman and those executed after the Haymarket affair. The bas relief on her grave marker was created by sculptor Jo Davidson.
Philosophy.
Goldman spoke and wrote extensively on a wide variety of issues. While she rejected orthodoxy and fundamentalist thinking, she was an important contributor to several fields of modern political philosophy. She was influenced by many diverse thinkers and writers, including Mikhail Bakunin, Henry David Thoreau, Peter Kropotkin, Ralph Waldo Emerson, Nikolai Chernyshevsky, and Mary Wollstonecraft. Another philosopher who influenced Goldman was Friedrich Nietzsche. In her autobiography, she wrote: "Nietzsche was not a social theorist, but a poet, a rebel, and innovator. His aristocracy was neither of birth nor of purse; it was the spirit. In that respect Nietzsche was an anarchist, and all true anarchists were aristocrats."
Anarchism.
Anarchism was central to Goldman's view of the world and she is today considered one of the most important figures in the history of anarchism. First drawn to it during the persecution of anarchists after the 1886 Haymarket affair, she wrote and spoke regularly on behalf of anarchism. In the title essay of her book "Anarchism and Other Essays", she wrote:
Anarchism, then, really stands for the liberation of the human mind from the dominion of religion; the liberation of the human body from the dominion of property; liberation from the shackles and restraint of government. Anarchism stands for a social order based on the free grouping of individuals for the purpose of producing real social wealth; an order that will guarantee to every human being free access to the earth and full enjoyment of the necessities of life, according to individual desires, tastes, and inclinations.
Goldman's anarchism was intensely personal. She believed it was necessary for anarchist thinkers to live their beliefs, demonstrating their convictions with every action and word. "I don't care if a man's theory for tomorrow is correct," she once wrote. "I care if his spirit of today is correct." Anarchism and free association were to her logical responses to the confines of government control and capitalism. "It seems to me that "these" are the new forms of life," she wrote, "and that they will take the place of the old, not by preaching or voting, but by living them."
At the same time, she believed that the movement on behalf of human liberty must be staffed by liberated humans. While dancing among fellow anarchists one evening, she was chided by an associate for her carefree demeanor. In her autobiography, Goldman wrote:
I told him to mind his own business, I was tired of having the Cause constantly thrown in my face. I did not believe that a Cause which stood for a beautiful ideal, for anarchism, for release and freedom from conventions and prejudice, should demand denial of life and joy. I insisted that our Cause could not expect me to behave as a nun and that the movement should not be turned into a cloister. If it meant that, I did not want it. "I want freedom, the right to self-expression, everybody's right to beautiful, radiant things."
Tactical uses of violence.
Goldman, in her political youth, held targeted violence to be a legitimate means of revolutionary struggle. Goldman at the time believed that the use of violence, while distasteful, could be justified in relation to the social benefits it might accrue. She advocated propaganda of the deed—"attentat", or violence carried out to encourage the masses to revolt. She supported her partner Alexander Berkman's attempt to kill industrialist Henry Clay Frick, and even begged him to allow her to participate. She believed that Frick's actions during the Homestead strike were reprehensible and that his murder would produce a positive result for working people. "Yes," she wrote later in her autobiography, "the end in this case justified the means." While she never gave explicit approval of Leon Czolgosz's assassination of US President William McKinley, she defended his ideals and believed actions like his were a natural consequence of repressive institutions. As she wrote in "The Psychology of Political Violence": "the accumulated forces in our social and economic life, culminating in an act of violence, are similar to the terrors of the atmosphere, manifested in storm and lightning."
Her experiences in Russia led her to qualify her earlier belief that revolutionary ends might justify violent means. In the afterword to "My Disillusionment in Russia", she wrote: "There is no greater fallacy than the belief that aims and purposes are one thing, while methods and tactics are another... The means employed become, through individual habit and social practice, part and parcel of the final purpose..." In the same chapter, however, Goldman affirmed that "Revolution is indeed a violent process," and noted that violence was the "tragic inevitability of revolutionary upheavals..." Some misinterpreted her comments on the Bolshevik terror as a rejection of all militant force, but Goldman corrected this in the preface to the first US edition of "My Disillusionment in Russia:"
Goldman saw the militarization of Soviet society not as a result of armed resistance per se, but of the statist vision of the Bolsheviks, writing that "an insignificant minority bent on creating an absolute State is necessarily driven to oppression and terrorism."
Capitalism and labor.
Goldman believed that the economic system of capitalism was incompatible with human liberty. "The only demand that property recognizes," she wrote in "Anarchism and Other Essays", "is its own gluttonous appetite for greater wealth, because wealth means power; the power to subdue, to crush, to exploit, the power to enslave, to outrage, to degrade." She also argued that capitalism dehumanized workers, "turning the producer into a mere particle of a machine, with less will and decision than his master of steel and iron."
Originally opposed to anything less than complete revolution, Goldman was challenged during one talk by an elderly worker in the front row. In her autobiography, she wrote:
He said that he understood my impatience with such small demands as a few hours less a day, or a few dollars more a week... But what were men of his age to do? They were not likely to live to see the ultimate overthrow of the capitalist system. Were they also to forgo the release of perhaps two hours a day from the hated work? That was all they could hope to see realized in their lifetime.
Goldman realized that smaller efforts for improvement such as higher wages and shorter hours could be part of a social revolution.
The state – militarism, prison, voting, speech.
Goldman viewed the state as essentially and inevitably a tool of control and domination. As a result, Goldman believed that voting was useless at best and dangerous at worst. Voting, she wrote, provided an illusion of participation while masking the true structures of decision-making. Instead, Goldman advocated targeted resistance in the form of strikes, protests, and "direct action against the invasive, meddlesome authority of our moral code". She maintained an anti-voting position even when many anarcho-syndicalists in 1930s Spain voted for the formation of a liberal republic. Goldman wrote that any power anarchists wielded as a voting bloc should instead be used to strike across the country. She disagreed with the movement for women's suffrage, which demanded the right of women to vote. In her essay "Woman Suffrage", she ridicules the idea that women's involvement would infuse the democratic state with a more just orientation: "As if women have not sold their votes, as if women politicians cannot be bought!" She agreed with the suffragists' assertion that women are equal to men, but disagreed that their participation alone would make the state more just. "To assume, therefore, that she would succeed in purifying something which is not susceptible of purification, is to credit her with supernatural powers."
Goldman was also a passionate critic of the prison system, critiquing both the treatment of prisoners and the social causes of crime. Goldman viewed crime as a natural outgrowth of an unjust economic system, and in her essay "Prisons: A Social Crime and Failure", she quoted liberally from the 19th-century authors Fyodor Dostoevsky and Oscar Wilde on prisons, and wrote: Year after year the gates of prison hells return to the world an emaciated, deformed, will-less, shipwrecked crew of humanity, with the Cain mark on their foreheads, their hopes crushed, all their natural inclinations thwarted. With nothing but hunger and inhumanity to greet them, these victims soon sink back into crime as the only possibility of existence.
Goldman was a committed war resister, believing that wars were fought by the state on behalf of capitalists. She was particularly opposed to the draft, viewing it as one of the worst of the state's forms of coercion, and was one of the founders of the No-Conscription League—for which she was ultimately arrested (1917), imprisoned and deported (1919).
Goldman was routinely surveilled, arrested, and imprisoned for her speech and organizing activities in support of workers and various strikes, access to birth control, and in opposition to World War I. As a result, she became active in the early 20th century free speech movement, seeing freedom of expression as a fundamental necessity for achieving social change. Her outspoken championship of her ideals, in the face of persistent arrests, inspired Roger Baldwin, one of the founders of the American Civil Liberties Union. Goldman's and Reitman's experiences in the San Diego free speech fight (1912) were notorious examples of state and capitalist repression of the Industrial Workers of the World's campaign of free speech fights.
Feminism and sexuality.
Although she was hostile to the suffragist goals of first-wave feminism, Goldman advocated passionately for the rights of women, and is today heralded as a founder of anarcha-feminism, which challenges patriarchy as a hierarchy to be resisted alongside state power and class divisions. In 1897, she wrote: "I demand the independence of woman, her right to support herself; to live for herself; to love whomever she pleases, or as many as she pleases. I demand freedom for both sexes, freedom of action, freedom in love and freedom in motherhood."
A nurse by training, Goldman was an early advocate for educating women concerning contraception. Like many feminists of her time, she saw abortion as a tragic consequence of social conditions, and birth control as a positive alternative. Goldman was also an advocate of free love, and a strong critic of marriage. She saw early feminists as confined in their scope and bounded by social forces of Puritanism and capitalism. She wrote: "We are in need of unhampered growth out of old traditions and habits. The movement for women's emancipation has so far made but the first step in that direction."
Goldman was also an outspoken critic of prejudice against homosexuals. Her belief that social liberation should extend to gay men and lesbians was virtually unheard of at the time, even among anarchists. As German sexologist Magnus Hirschfeld wrote, "she was the first and only woman, indeed the first and only American, to take up the defense of homosexual love before the general public." In numerous speeches and letters, she defended the right of gay men and lesbians to love as they pleased and condemned the fear and stigma associated with homosexuality. As Goldman wrote in a letter to Hirschfeld, "It is a tragedy, I feel, that people of a different sexual type are caught in a world which shows so little understanding for homosexuals and is so crassly indifferent to the various gradations and variations of gender and their great significance in life."
Atheism.
A committed atheist, Goldman viewed religion as another instrument of control and domination. Her essay "The Philosophy of Atheism" quoted Bakunin at length on the subject and added:
Consciously or unconsciously, most theists see in gods and devils, heaven and hell, reward and punishment, a whip to lash the people into obedience, meekness and contentment... The philosophy of Atheism expresses the expansion and growth of the human mind. The philosophy of theism, if we can call it a philosophy, is static and fixed.
In essays like "The Hypocrisy of Puritanism" and a speech entitled "The Failure of Christianity", Goldman made more than a few enemies among religious communities by attacking their moralistic attitudes and efforts to control human behavior. She blamed Christianity for "the perpetuation of a slave society", arguing that it dictated individuals' actions on Earth and offered poor people a false promise of a plentiful future in heaven. She was also critical of Zionism, which she saw as another failed experiment in state control.
Legacy.
Goldman was well known during her life, described as—among other things—"the most dangerous woman in America". After her death and through the middle part of the 20th century, her fame faded. Scholars and historians of anarchism viewed her as a great speaker and activist, but did not regard her as a philosophical or theoretical thinker on par with, for instance, Kropotkin.
In 1970, Dover Press reissued Goldman's biography, "Living My Life", and in 1972, feminist writer Alix Kates Shulman issued a collection of Goldman's writing and speeches, "Red Emma Speaks". These works brought Goldman's life and writings to a larger audience, and she was in particular lionized by the women's movement of the late 20th century. In 1973, Shulman was asked by a printer friend for a quotation by Goldman for use on a T-shirt. She sent him the selection from "Living My Life" about "the right to self-expression, everybody's right to beautiful, radiant things", recounting that she had been admonished "that it did not behoove an agitator to dance" (see above). The printer created a statement based on these sentiments that has become one of Goldman's most famous quotations, even though she probably never said or wrote it as such: "If I can't dance I don't want to be in your revolution." Variations of this saying have appeared on thousands of T-shirts, buttons, posters, bumper stickers, coffee mugs, hats, and other items.
The women's movement of the 1970s that "rediscovered" Goldman was accompanied by a resurgent anarchist movement, beginning in the late 1960s, which also reinvigorated scholarly attention to earlier anarchists. The growth of feminism also initiated some reevaluation of Goldman's philosophical work, with scholars pointing out the significance of Goldman's contributions to anarchist thought in her time. Goldman's belief in the value of aesthetics, for example, can be seen in the later influences of anarchism and the arts. Similarly, Goldman is now given credit for significantly influencing and broadening the scope of activism on issues of sexual liberty, reproductive rights, and freedom of expression.
Goldman has been depicted in numerous works of fiction over the years, perhaps most notably by Maureen Stapleton, who won an Academy Award for her role as Goldman in Warren Beatty's 1981 film "Reds". Goldman has also been a character in two Broadway musicals, "Ragtime" and "Assassins". Plays depicting Goldman's life include Howard Zinn's play, "Emma"; Martin Duberman's "Mother Earth" (1991); Jessica Litwak's "Emma Goldman: Love, Anarchy, and Other Affairs" (Goldman's relationship with Berkman and her arrest in connection with McKinley's assassination); Lynn Rogoff's "Love Ben, Love Emma" (Goldman's relationship with Reitman); and Carol Bolt's "Red Emma". Ethel Mannin's 1941 novel "Red Rose" is also based on Goldman's Life.
Goldman has been honored by a number of organizations named in her memory. The Emma Goldman Clinic, a women's health center located in Iowa City, Iowa, selected Goldman as a namesake "in recognition of her challenging spirit." Red Emma's Bookstore Coffeehouse, an infoshop in Baltimore, Maryland adopted her name out of their belief "in the ideas and ideals that she fought for her entire life: free speech, sexual and racial equality and independence, the right to organize in our jobs and in our own lives, ideas and ideals that we continue to fight for, even today".
Paul Gailiunas and his late wife Helen Hill co-wrote the anarchist song "Emma Goldman", which was performed and released by the band Piggy: The Calypso Orchestra of the Maritimes in 1999. The song was later performed by Gailiunas' new band The Troublemakers and released on their 2004 album "Here Come The Troublemakers".
Works.
Goldman was a prolific writer, penning countless pamphlets and articles on a diverse range of subjects. She authored six books, including an autobiography, "Living My Life", and a biography of fellow anarchist Voltairine de Cleyre.
Sources.
</dl>

</doc>
<doc id="9765" url="http://en.wikipedia.org/wiki?curid=9765" title="Equuleus">
Equuleus

Equuleus is a constellation. Its name is Latin for 'little horse', a foal. It was one of the 48 constellations listed by the 2nd century astronomer Ptolemy, and remains one of the 88 modern constellations. It is the second smallest of the modern constellations (after Crux), spanning only 72 square degrees. It is also very faint, having no stars brighter than the fourth magnitude.
Notable features.
Stars.
The brightest star in Equuleus is Alpha Equulei, traditionally called Kitalpha, a yellow star magnitude 3.9, 186 light-years from Earth. Its traditional name means "the section of the horse".
There are few variable stars in Equuleus. Only around 25 are known, most of which are faint. Gamma Equulei is an alpha CVn star, ranging between magnitudes 4.58 and 4.77 over a period of around 12½ minutes. It is a white star 115 light-years from Earth, and has an optical companion of magnitude 6.1, 6 Equulei. It is divisible in binoculars. R Equulei is a Mira variable that ranges between magnitudes 8.0 and 15.7 over nearly 261 days.
Equuleus contains some double stars of interest. γ Equ consists of a primary star with a magnitude around 4.7 (slightly variable) and a secondary star of magnitude 11.6, separated by 2 arcseconds. Epsilon Equulei is a triple star also designated 1 Equulei. The system, 197 light-years away, has a primary of magnitude 5.4 that is itself a binary star; its components are of magnitude 6.0 and 6.3 and have a period of 101 years. The secondary is of magnitude 7.4 and is visible in small telescopes. The components of the primary are becoming closer together and will not be divisible in amateur telescopes beginning in 2015. δ Equ is a binary star with an orbital period of 5.7 years, which at one time was the shortest known orbital period for an optical binary. The two components of the system are never more than 0.35 arcseconds apart.
Deep-sky objects.
Due to its small size and its distance from the plane of the Milky Way, Equuleus contains no notable deep sky objects. Some very faint galaxies between magnitudes 13 and 15 include NGC 7015, NGC 7040, NGC 7045 and NGC 7046.
Mythology.
In Greek mythology, one myth associates Equuleus with the foal Celeris (meaning "swiftness" or "speed"), who was the offspring or brother of the winged horse Pegasus. Celeris was given to Castor by Mercury. Other myths say that Equuleus is the horse struck from Poseidon's trident, during the contest between him and Athena when deciding which would be the superior. Because this section of stars rises before Pegasus, it is often called Equus Primus, or the First Horse. Equuleus is also linked to the story of Philyra and Saturn.
Created by Hipparchus and included by Ptolemy, it abuts Pegasus; unlike the larger horse it is depicted as a horse's head alone.
Equivalents.
In Chinese astronomy, the stars that correspond to Equuleus are located within the Black Tortoise of the North (北方玄武, "Běi Fāng Xuán Wǔ").
External links.
Coordinates: 

</doc>
<doc id="9766" url="http://en.wikipedia.org/wiki?curid=9766" title="Eridanus">
Eridanus

The name “Eridanus” can refer to:

</doc>
<doc id="9767" url="http://en.wikipedia.org/wiki?curid=9767" title="Eucharist">
Eucharist

The Eucharist. (also called Holy Communion, the Lord's Supper, and other names) is a rite considered by most Christian churches to be a sacrament. According to some New Testament books, it was instituted by Jesus Christ during his Last Supper. Giving his disciples bread and wine during the Passover meal, Jesus commanded his followers to "do this in memory of me," while referring to the bread as "my body" and the wine as "my blood." Through the Eucharistic celebration Christians remember Christ's sacrifice of himself once and for all on the cross.
Christians generally recognize a special presence of Christ in this rite, though they differ about exactly how, where, and when Christ is present. While all agree that there is no perceptible change in the elements, some believe that they actually become the body and blood of Christ, others believe the true Body and Blood of Christ are really present in, with, and under the bread and wine (whose reality remains unchanged), others believe in a "real" but merely spiritual presence of Christ in the Eucharist, and still others take the act to be only a symbolic reenactment of the Last Supper. Many Protestants view the Eucharist as an "ordinance" in which the ceremony is seen not as a specific channel of divine grace, but as an expression of faith and of obedience to Christ.
In spite of differences between Christians about various aspects of the Eucharist, there is, according to the "Encyclopædia Britannica", "more of a consensus among Christians about the meaning of the Eucharist than would appear from the confessional debates over the sacramental presence, the effects of the Eucharist, and the proper auspices under which it may be celebrated."
The word "Eucharist" may refer not only to the rite but also to the consecrated bread (leavened or unleavened) and wine (or grape juice) used in the rite. In this sense, communicants (that is, those who partake of the communion elements) may speak of "receiving the Eucharist", as well as "celebrating the Eucharist".
Names.
Eucharist.
The Greek noun εὐχαριστία ("eucharistia"), meaning "thanksgiving", is not used in the New Testament as a name for the rite, however, the related verb is found in New Testament accounts of the Last Supper, including the earliest such account:
 For I received from the Lord what I also delivered to you, that the Lord Jesus on the night when he was betrayed took bread, and when he had given thanks, he broke it, and said, "This is my body which is for you. Do this in remembrance of me". ()
The Lord's Supper.
The Lord's Supper, in Greek Κυριακὸν δεῖπνον ("Kyriakon deipnon"), was in use in the early 50s of the 1st century, as witnessed by the First Epistle to the Corinthians (): When you come together, it is not the Lord's Supper you eat, for as you eat, each of you goes ahead without waiting for anybody else. One remains hungry, another gets drunk. Those mentioned above in relation to the term "Eucharist" rarely use the expression "the Lord's Supper", but it is the predominant term among Baptist groups, who generally avoid using the term "Communion", and is preferred also by some evangelical Anglicans and Methodists.
Communion / Holy Communion.
Communion or Holy Communion are used by some groups originating in the Protestant Reformation to mean the entire Eucharistic rite. Others, such as the Catholic Church, do not use this term for the rite, but instead mean by it the act of partaking of the consecrated elements: they speak of receiving Holy Communion even outside of the rite, and of participating in the rite without receiving Holy Communion. The term "Communion" is derived from Latin "communio" ("sharing in common"), which translates Greek κοινωνία ("koinōnía") in : The cup of blessing which we bless, is it not the "communion" of the blood of Christ? The bread which we break, is it not the "communion" of the body of Christ?.
History.
Biblical basis.
The Last Supper appears in all three Synoptic Gospels: Matthew, Mark, and Luke. It also is found in the First Epistle to the Corinthians, which suggests how early Christians celebrated what Paul the Apostle called the Lord's Supper.
Paul the Apostle and the Lord's Supper.
In his First Epistle to the Corinthians ("c" 54-55), Paul the Apostle gives the earliest recorded description of Jesus' Last Supper: "The Lord Jesus on the night when he was betrayed took bread, and when he had given thanks, he broke it, and said, 'This is my body which is for you. Do this in remembrance of me. ' In the same way he took the cup also, after supper, saying, 'This cup is the new covenant in my blood. Do this, as often as you drink it, in remembrance of me'".
Gospels.
The synoptic gospels, , , , depict Jesus as presiding over the Last Supper. There are references to Jesus' body and blood which foreshadow his crucifixion, and he identifies them as a new covenant. The versions in Matthew and Mark are almost identical; but Luke's Gospel presents a textual problem in that a few manuscripts omit the second half of verse 19 and all of v.20 ("given for you … poured out for you") which are found in the vast majority of ancient witnesses to the text. If the shorter text is the original one, then Luke's account is independent of both that of Paul and that of Matthew/Mark. If the majority longer text comes from the author of the third gospel then his version is very similar to that of Paul in 1 Corinthians being somewhat fuller in its description of the early part of the Supper, particularly in making specific mention of a cup being blessed before the bread was broken.
In the gospel of John, however, the account of the Last Supper does not mention Jesus taking bread and "the cup" and speaking of them as his body and blood; instead it recounts his humble act of washing the disciples' feet, the prophecy of the betrayal, which set in motion the events that would lead to the cross, and his long discourse in response to some questions posed by his followers, in which he went on to speak of the importance of the unity of the disciples with him and each other. In , the evangelist attributes a long discourse to Jesus which deals with the subject of the living bread and in contains echoes of Eucharistic language. The interpretation of the whole passage has been extensively debated. Hoskyns notes three main schools of thought: (a) the language is metaphorical and verse 63: "The Spirit gives life; the flesh counts for nothing. The words I have spoken to you—they are full of the Spirit" and life" gives the author's precise meaning; (b) vv 51-58 are a later interpolation which cannot be harmonized with the context; (c) the discourse is homogeneous, sacrificial and sacramental and can be harmonized though not all attempts are satisfactory.
Agape feast.
The expression "The Lord's Supper", derived from St. Paul's usage in , may have originally referred to the Agape feast (or love feast), the shared communal meal with which the Eucharist was originally associated. The Agape feast is mentioned in . But "The Lord's Supper" is now commonly used in reference to a celebration involving no food other than the sacramental bread and wine.
Early Christian sources.
The Didache (Greek: teaching) is an early Church treatise that includes instructions for Baptism and the Eucharist. Most scholars date it to the early 2nd century, and distinguish in it two separate Eucharistic traditions, the earlier tradition in chapter 10 and the later one preceding it in chapter 9. The Eucharist is mentioned again in chapter 14.
Ignatius of Antioch (c. 35 or 50-between 98 and 117), one of the Apostolic Fathers, mentions the Eucharist as "the flesh of our Saviour Jesus Christ", and Justin Martyr speaks of it as more than a meal: "the food over which the prayer of thanksgiving, the word received from Christ, has been said ... is the flesh and blood of this Jesus who became flesh ... and the deacons carry some to those who are absent."
Eucharistic theology.
Many Christian denominations classify the Eucharist as a sacrament. Some Protestants prefer to call it an "ordinance", viewing it not as a specific channel of divine grace but as an expression of faith and of obedience to Christ.
Most Christians, even those who deny that there is any real change in the elements used, recognize a special presence of Christ in this rite. But Christians differ about exactly how, where and how long Christ is present in it. Catholicism, Eastern Orthodoxy, Oriental Orthodoxy, and the Church of the East teach that the reality (the "substance") of the elements of bread and wine is wholly changed into the body and blood of Jesus Christ, while the appearances (the "species") remain. Transubstantiation (change of the reality) is the term used by Catholics to denote "what" is changed, not to explain "how" the change occurs, since the Catholic Church teaches that "the signs of bread and wine become, "in a way surpassing understanding", the Body and Blood of Christ". Lutherans believe that the body and blood of Jesus are present "in, with and under" the forms of bread and wine, a concept known as the sacramental union. The Reformed churches, following the teachings of John Calvin, believe in an immaterial, spiritual (or "pneumatic") presence of Christ by the power of the Holy Spirit and received by faith. Anglicans adhere to a range of views although the teaching on the matter in the Articles of Religion holds that the presence is real only in a heavenly and spiritual sense. Some Christians reject the concept of the real presence, believing that the Eucharist is only a ceremonial remembrance or memorial of the death of Christ.
The "Baptism, Eucharist and Ministry" document of the World Council of Churches, attempting to present the common understanding of the Eucharist on the part of the generality of Christians, describes it as "essentially the sacrament of the gift which God makes to us in Christ through the power of the Holy Spirit", "Thanksgiving to the Father", "Anamnesis or Memorial of Christ", "the sacrament of the unique sacrifice of Christ, who ever lives to make intercession for us", "the sacrament of the body and blood of Christ, the sacrament of his real presence", "Invocation of the Spirit", "Communion of the Faithful", and "Meal of the Kingdom".
The Eucharist is the symbolic bread, not the ritual, in the remembrance service. 
Acts 2:42. And they were continuing in the teaching of the Apostles, and they became partakers in prayer and in breaking of the Eucharist.
“Eucharist”- אוכרסטיא (Eu-kh-ris-tya) is supposed to be a Greek word in Aramaic letters, however, the Greek mss. do not have “Eucharist” here or in the two other places where אוכרסטיא occurs in the Peshitta NT (Aramaic). How did the Peshitta reading אוכרסטיא get here and in those two other places without any such word in the Greek texts? “Eucharist” occurs nowhere in The Greek NT! I submit that this word “Eucharist” came from the original early first century Peshitta NT, not from Greek.
"The Peshitta Aramaic-English Interlinear New Testament" The Acts of the Apostles Page - 305 - David Bauscher www.aramiacnt.com
Ritual and liturgy.
Roman Catholic.
The Catholic Church teaches that once consecrated in the Eucharist, the elements cease to be bread and wine and actually "become" the body and blood of Christ, each of which is accompanied by the other and by Christ's soul and divinity. The empirical appearance and physical properties are not changed, but for Catholics, the reality is. The consecration of the bread (known as the host) and wine represents the separation of Jesus' body from his blood at Calvary. However, since he has risen, the Church teaches that his body and blood can no longer be truly separated. Where one is, the other must be. Therefore, although the priest (or extraordinary minister of Holy Communion) says, "The body of Christ", when administering the host, and, "The blood of Christ", when presenting the chalice, the communicant who receives either one receives Christ, whole and entire.
The Catholic Church sees as the main basis for this belief the words of Jesus himself at his Last Supper: the Synoptic Gospels (; ; ) and Saint Paul's ) recount that in that context Jesus said of what to all appearances were bread and wine: "This is my body … this is my blood." The Catholic understanding of these words, from the Patristic authors onward, has emphasized their roots in the covenantal history of the Old Testament. The interpretation of Christ's words against this Old Testament background coheres with and supports belief in the Real Presence.
In 1551, the Council of Trent definitively declared: "Because Christ our Redeemer said that it was truly his body that he was offering under the species of bread, it has always been the conviction of the Church of God, and this holy Council now declares again, that by the consecration of the bread and wine there takes place a change of the whole substance of the bread into the substance of the body of Christ our Lord and of the whole substance of the wine into the substance of his blood. This change the holy Catholic Church has fittingly and properly called transubstantiation." The Fourth Council of the Lateran in 1215 had spoken of "Jesus Christ, whose body and blood are truly contained in the sacrament of the altar under the forms of bread and wine; the bread being changed ("transsubstantiatis") by divine power into the body, and the wine into the blood." The attempt by some twentieth-century Catholic theologians to present the Eucharistic change as an alteration of significance (transignification rather than transubstantiation) was rejected by Pope Paul VI in his 1965 encyclical letter In his 1968 , he reiterated that any theological explanation of the doctrine must hold to the twofold claim that, after the consecration, 1) Christ's body and blood are really present; and 2) bread and wine are really absent; and this presence and absence is "real" and not merely something in the mind of the believer.
On entering a church, Latin Church Catholics genuflect to the consecrated host in the tabernacle that holds the consecrated host, in order to acknowledge respectfully the presence of Jesus in the Blessed Sacrament, a presence to which a votive candle or sanctuary lamp kept burning close to such a tabernacle draws attention.
Eastern Christianity.
Among Eastern Christians, the Eucharistic service is called the "Divine Liturgy" (Byzantine Rite) or similar names in other rites. It comprises two main divisions: the first is the "Liturgy of the Catechumens" which consists of introductory litanies, antiphons and scripture readings, culminating in a reading from one of the Gospels and, often, a homily; the second is the "Liturgy of the Faithful" in which the Eucharist is offered, consecrated, and received as Holy Communion. Within the latter, the actual Eucharistic prayer is called the "anaphora, " literally: "offering" or "carrying up" (ἀνα- + φέρω). In the Rite of Constantinople, two different anaphoras are currently used: one is attributed to Saint John Chrysostom, the other to Saint Basil the Great. Among the Oriental Orthodox, a variety of anaphoras are used, but all are similar in structure to those of the Constantinopolitan Rite, in which the Anaphora of Saint John Chrysostom is used most days of the year; Saint Basil's is offered on the Sundays of Great Lent, the eves of Christmas and Theophany, Holy Thursday, Holy Saturday, and upon his feast day (1 January). At the conclusion of the Anaphora the bread and wine are held to be the Body and Blood of Christ. Unlike the Latin Church, the Byzantine Rite uses leavened bread, with the leaven symbolizing the presence of the Holy Spirit. The Armenian Apostolic Church, like the Latin Church, uses unleavened bread.
Conventionally this change in the elements is understood to be accomplished at the "Epiclesis" (Greek: "invocation") by which the Holy Spirit is invoked and the consecration of the bread and wine as the true and genuine Body and Blood of Christ is specifically requested, but since the anaphora as a whole is considered a unitary (albeit lengthy) prayer, no one moment within it can be readily singled out.
Anglican.
In most churches of the Anglican Communion, the Eucharist is celebrated every Sunday, having replaced Morning Prayer as the principal service. The rites for the Eucharist are found in the various prayer books of Anglican churches. Wine and unleavened wafers or leavened bread is used. Daily celebrations are the norm in many cathedrals and parish churches typically offer one or more Eucharists during the week. Only a small minority of parishes with a priest do not celebrate the Eucharist at least once each Sunday. The nature of the ceremony, however, varies according to the theological orientation of the priest, parish, diocese or regional church.
Protestant.
Baptist.
The bread and "fruit of the vine" indicated in Matthew, Mark and Luke as the elements of the "Lord's Supper" are interpreted by many Baptists as unleavened bread (although leavened bread is often used) and, in line with the historical stance of some Baptist groups (since the mid-19th century) against partaking of alcoholic beverages, grape juice, which they commonly refer to simply as "the Cup". The unleavened bread also underscores the symbolic belief attributed to Christ's breaking the bread and saying that it was his body. A soda cracker is often used.
Most Baptists do not consider the Communion or its elements to be sacramental; rather, it is considered to be an act of remembrance of Christ's atonement, and a time of renewal of personal commitment. However, with the rise of confessionalism, many Baptists have denied memorialism as a 19th-century doctrinal novelty, and have taken up a Reformed view of Communion. Confessional Baptists believe in pneumatic presence, which is expressed in the Second London Baptist Confession, specifically in Chapter 30, Articles 3 and 7:
Art. 3. The Lord Jesus hath, in this ordinance, appointed his ministers to pray, and bless the elements of bread and wine, and thereby to set them apart from a common to a holy use, and to take and break the bread; to take the cup, and, they communicating also themselves, to give both to the communicants.
Art. 7. Worthy receivers, outwardly partaking of the visible elements in this ordinance, do then also inwardly by faith, really and indeed, yet not carnally and corporally, but spiritually receive, and feed upon Christ crucified, and all the benefits of his death; the body and blood of Christ being then not corporally or carnally, but spiritually present to the faith of believers in that ordinance, as the elements themselves are to their outward senses.
This view is prevalent among Southern Baptists, those in the Founders movement (a Calvinistic movement within the some Independent Baptists, Freewill Baptists, and several individuals in other Baptist associations.
As in many churches, Communion practices and frequency vary among congregations. A typical practice is to have small cups of juice and plates of broken bread distributed to the seated congregation by a group of deacons, elders, or ushers. In others congregations, communicants may proceed to the altar to receive the elements, then return to their seats. A widely accepted practice is for all to receive and hold the elements until everyone is served, then consume the bread and cup in unison. Usually, music is performed and Scripture is read during the receiving of the elements.
Some Baptist churches are closed-Communionists (even requiring full membership in the church before partaking), with others being partially or fully open-Communionists. It is rare to find a Baptist church where The Lord's Supper is observed every Sunday; most observe monthly or quarterly, with some holding Communion only during a designated Communion service or following a worship service.
Lutheran.
Lutherans believe that the body and blood of Christ are "truly and substantially present in, with, and under the forms" of the consecrated bread and wine (the elements), so that communicants eat and drink the body and blood of Christ himself as well as the bread and wine in this sacrament. The Lutheran doctrine of the Real Presence is more accurately and formally known as the "sacramental union". It has been inaccurately called "consubstantiation". This term is specifically rejected by Lutheran churches and theologians since it creates confusion about the actual doctrine and subjects the doctrine to the control of a non-biblical philosophical concept in the same manner as, in their view, does the term "transubstantiation".
While an official movement exists in Lutheran congregations celebrate Eucharist weekly, using formal rites very similar to the Catholic and "high" Anglican services, it was historically common for congregations to celebrate monthly or even quarterly. Even in congregations where Eucharist is offered weekly, there is not a requirement that every church service be a Eucharistic service, nor that all members of a congregation must receive it weekly.
Mennonites and Anabaptists.
Traditional Mennonite and German Baptist Brethren Churches such as the Church of the Brethren churches and congregations have the Agape Meal, footwashing and the serving of the bread and wine two parts to the Communion service in the Lovefeast. In the more modern groups, Communion is only the serving of the Lord’s Supper. In the communion meal, the members of the Mennonite churches renew their covenant with God and with each other.
Open Brethren and Exclusive Brethren.
Among Open assemblies, also termed Plymouth Brethren, the Eucharist is more commonly called the Breaking of Bread or the Lord's Supper. It is seen as a symbolic memorial and entirely non-sacramental, and central to the worship of both individual and assembly. In principle the service is open to all baptised Christians, however an individual's eligibility to participate depends on the views of each particular assembly. The service takes the form of non-liturgical, open worship with all male participants allowed to pray audibly and select hymns or readings. The breaking of bread itself typically consists of one leavened loaf which is prayed over and broken by a participant in the meeting, and then shared around. The wine is poured from a single container into one or several vessels, and these are again shared around.
The Exclusive Brethren follow a similar practice to the Open Brethren. The Eucharist they also call the Breaking of Bread or the Lord's Supper.
Reformed/Presbyterian.
In the Reformed Churches the Eucharist is variously administered. The Calvinist view of the Sacrament sees a real presence of Christ in the supper which differs both from the objective ontological presence of the Catholic view, and from the real absence of Christ and the mental recollection of the memorialism of the Zwinglians and their successors.
The bread and wine become the means by which the believer has real communion with Christ in his death and Christ's body and blood are present to the faith of the believer as really as the bread and wine are present to their senses but this presence is "spiritual", that is the work of the Holy Spirit. There is no standard frequency; John Calvin desired weekly communion, but the city council only approved monthly, and monthly celebration has become the most common practice in Reformed churches today.
Many, on the other hand, follow John Knox in celebration of the Lord's supper on a quarterly basis, to give proper time for reflection and inward consideration of one's own state and sin. Recently, Presbyterian and Reformed Churches have been considering whether to restore more frequent communion, including weekly communion in more churches, considering that infrequent communion was derived from a memorialist view of the Lord's Supper, rather than Calvin's view of the sacrament as a means of grace. Some churches use bread without any raising agent (whether leaven or yeast), in view of the use of unleavened bread at Jewish Passover meals, while others use any bread available.
The Presbyterian Church (USA), for instance, prescribes "bread common to the culture". Harking back to the regulative principle of worship, the Reformed tradition had long eschewed coming forward to receive communion, preferring to have the elements distributed throughout the congregation by the presbyters (elders) more in the style of a shared meal. Wine and grape juice are both used, depending on the congregation.
Openness ranges between open communion (any believer may participate, e.g. the PCUSA) to closed (only members of the denomination may partake). Most Reformed churches would practice a balance between these, i.e., all believers who are united to a church of like faith and practice, and who are not living in sin, would be allowed to join in the Sacrament, sometimes with a requirement of pastoral or elder approval.
Methodist.
Methodists have differing views on the nature of the Sacrament, but they commonly use grape juice is used instead of wine to include those who do not take alcohol for any reason. Variations of the Eucharistic Prayer are provided for various occasions, including communion of the sick and brief forms for occasions that call for greater brevity. Though the ritual is standardized, there is great variation amongst United Methodist churches, from typically high-church to low-church, in the enactment and style of celebration. Methodist clergy are not required to be vested when celebrating the Eucharist, though the alb and stole is common.
United Methodists in the United States are encouraged to celebrate the Eucharist every Sunday, though it is typically celebrated on the first Sunday of each month, while a few go as long as celebrating quarterly (a tradition dating back to the days of circuit riders that served multiple churches). Communicants may receive standing, kneeling, or while seated. Gaining more wide acceptance is the practice of receiving by intinction (receiving a piece of consecrated bread or wafer, dipping it in the blessed wine, and consuming it). The most common alternative to intinction is for the communicants to receive the consecrated juice using small, individual, specially made glass or plastic cups known as communion cups. United Methodists practice open communion, inviting "all who intend a Christian life, together with their children" to receive Communion. 
The "Catechism for the use of the people called Methodists" states that, "[in the Eucharist] Jesus Christ is present with his worshipping people and gives himself to them as their Lord and Saviour". The "Catechism" does not identify the exact nature of this presence.
Nondenominational/Evangelical Christians.
Many Non-denominational Christians including the Churches of Christ receive communion every Sunday. Others, including Evangelical churches such as the Church of God, Calvary Chapel and many forms of Baptist, typically receive communion on a monthly or periodic basis. Many Non-denominational Christians hold to the Biblical autonomy of local churches and have no universal requirement among congregations.
The Churches of Christ, among others, use grape juice and unleavened wafers or unleavened bread and practice open communion.
Other groups.
Syriac.
Holy Qurbana or Qurbana Qadisha, the "Holy Offering" or "Holy Sacrifice", refers to the Eucharist as celebrated according to the East Syrian and West Syrian traditions of Syriac Christianity. The main Anaphora of the East Syrian tradition is the Holy Qurbana of Addai and Mari, while that of the West Syrian tradition is the Liturgy of Saint James. Both are extremely old, going back at least to the third century, and are the oldest extant liturgies continually in use.
Seventh-day Adventists.
In the Seventh-day Adventist Church the Holy Communion service customarily is celebrated once per quarter. The service includes the ordinance of footwashing and the Lord’s Supper. Unleavened bread and unfermented (non-alcoholic) grape juice is used. Open communion is practised: all who have committed their lives to the Saviour may participate. The communion service must be conducted by an ordained pastor, minister or church elder.
Jehovah's Witnesses.
Jehovah's Witnesses commemorate Christ's death as a ransom or propitiatory sacrifice by observing the Lord's Evening Meal, or Memorial, each year on the evening that corresponds to the Passover, Nisan 14, according to the ancient Jewish calendar. They believe that this is the only annual religious observance commanded for Christians in the Bible.
Of those who attend the Memorial a small minority worldwide partake of the wine and unleavened bread. Jehovah's Witnesses believe that only 144,000 people will receive heavenly salvation and thus spend eternity with God in heaven, as underpriests and co-rulers under Christ. Paralleling the anointing of kings and priests, they are referred to as the "anointed" class and are the only ones who should partake of the bread and wine.
The Memorial, held after sunset, includes a talk on the meaning of the celebration and the circulation among the audience of unadulterated red wine and unleavened bread. Jehovah's Witnesses believe the bread represents Jesus Christ's body which he gave on behalf of mankind, and that the wine represents his blood which redeems from sin. The wine and the bread (sometimes referred to as "emblems") are viewed as symbolic; they do not believe in transubstantiation or consubstantiation.
Latter-day Saints.
In The Church of Jesus Christ of Latter-day Saints (LDS Church), the "Holy Sacrament of the Lord's Supper", more simply referred to as the Sacrament, is administered every Sunday (except General Conference or other special Sunday meeting) in each LDS Ward or branch worldwide at the beginning of Sacrament meeting. The Sacrament, which consists of both ordinary bread and water (rather than wine or grape juice), is prepared by priesthood holders prior to the beginning of the meeting. At the beginning of the Sacrament, priests say specific prayers to bless the bread and water. The Sacrament is passed row-by-row to the congregation by priesthood holders (typically deacons).
The prayer recited for the bread and the water is found in the Book of Mormon and Doctrine and Covenants.
Non-observing denominations.
While the Salvation Army does not reject the Eucharistic practices of other churches or deny that their members truly receive grace through this sacrament, it does not practice the sacraments of Communion or baptism. This is because they believe that these are unnecessary for the living of a Christian life, and because in the opinion of Salvation Army founders William and Catherine Booth, the sacrament placed too much stress on outward ritual and too little on inward spiritual conversion.
Emphasizing the inward spiritual experience of their adherents over any outward ritual, Quakers (members of the Religious Society of Friends) generally do not baptize or observe Communion.
Practice and Customs.
Open and closed communion.
Christian denominations differ in their understanding of whether they may receive the Eucharist with those with whom they are not in full communion. The famed apologist St. Justin Martyr (c. 150) wrote: "No one else is permitted to partake of it, except one who believes our teaching to be true..." For the first several hundred years, non-members were forbidden even to be present at the sacramental ritual; visitors and catechumens (those still undergoing instruction) were dismissed halfway through the Liturgy, after the Bible readings and sermon but before the Eucharistic rite. The Divine Liturgy of St. John Chrysostom, used in the Byzantine Rite, still has a formula of dismissal of catechumens (not usually followed by any action) at this point.
Churches such as the Catholic and the Eastern Orthodox Churches practice closed communion under normal circumstances. However, the Catholic Church allows administration of the Eucharist, at their spontaneous request, to properly disposed members of the eastern churches (Eastern Orthodox, Oriental Orthodox and Church of the East) not in full communion with it and of other churches that the Holy See judges to be sacramentally in the same position as these churches; and in grave and pressing need, such as danger of death, it allows the Eucharist to be administered also to individuals who do not belong to these churches but who share the Catholic Church's faith in the reality of the Eucharist and have no access to a minister of their own community. Some Protestant communities exclude non-members from Communion.
The Evangelical Lutheran Church in America (ELCA) practices open communion, provided those who receive are baptized, but the much smaller Lutheran Church - Missouri Synod, and the Wisconsin Evangelical Lutheran Synod practice closed communion, excluding non-members and requiring communicants to have been given catechetical instruction. The Evangelical Lutheran Church in Canada, the Evangelical Church in Germany, the Church of Sweden, and many other Lutheran churches outside of the US also practice open communion. 
Some use the term "close communion" for restriction to members of the same denomination, and "closed communion" for restriction to members of the local congregation alone.
Most Protestant communities, including, Congregational churches, the Church of the Nazarene, the Assemblies of God, Methodists, most Presbyterians, Nondenominational Christianity (including the Churches of Christ), and Anglicans practice open communion in the sense of not limiting it to members of their own Church alone, but some of them require that the communicant be a baptized person or a member of a partner church. Some Progressive Christian congregations offer communion to any individual who wishes to commemorate the life and teachings of Christ, regardless of religious affiliation.
In the Episcopal Church (United States), those who do not receive Holy Communion may enter the communion line with their arms crossed over their chest, in order to receive a blessing from the priest, instead of receiving Holy Communion.
Most Latter-Day Saint churches practice closed communion; one notable exception is the Community of Christ, the second-largest denomination in this movement. While The Church of Jesus Christ of Latter-day Saints (the largest of the LDS denominations) technically practice a closed communion, their official direction to local Church leaders (in Handbook 2, section 20.4.1, last paragraph) is as follows: "Although the sacrament is for Church members, the bishopric should not announce that it will be passed to members only, and nothing should be done to prevent nonmembers from partaking of it."
Preparation.
Catholic.
The Catholic Church requires its members to receive the sacrament of Penance or Reconciliation before taking Communion, if they are aware of having committed a grave sin, and to prepare by fasting, prayer and other works of piety.
Eastern Orthodox.
Traditionally, the Eastern Orthodox church has required its members to have observed all church-appointed fasts (most weeks, this will be at least Wednesday and Friday) for the week prior to partaking of communion, and to fast from all food and water from midnight the night before. In addition, Orthodox Christians are to have made a recent confession to their priest (the frequency varying with one's particular priest), and they must be at peace with all others, meaning that they hold no grudges or anger against anyone. In addition, one is expected to attend Vespers or the All-Night Vigil, if offered, on the night before receiving communion. Furthermore, various pre-communion prayers have been composed, which many (but not all) Orthodox churches require or at least strongly encourage members to say privately before coming to the Eucharist.
Protestant confessions.
Many Protestant congregations generally reserve a period of time for self-examination and private, silent confession just before partaking in the Lord's Supper.
Footwashing.
Seventh Day Adventists, Mennonites, and some other groups participate in "foot washing" (cf. ) as a preparation for partaking in the Lord's Supper. At that time they are to individually examine themselves, and confess any sins they may have between one and another.
Adoration.
Eucharistic adoration is a practice in the Roman Catholic, Anglo-Catholic and some Lutheran traditions, in which the Blessed Sacrament is exposed to and adored by the faithful. When this exposure and adoration is constant (twenty-four hours a day), it is called "Perpetual Adoration". In a parish, this is usually done by volunteer parishioners; in a monastery or convent, it is done by the resident monks or nuns. In the "Exposition of the Blessed Sacrament", the Eucharist is displayed in a monstrance, typically placed on an altar, at times with a light focused on it, or with candles flanking it.
Health issues.
Gluten.
The gluten in wheat bread may be dangerous to people with celiac disease. For the Catholic Church, this issue was addressed in the 24 July 2003 of the Congregation for the Doctrine of the Faith, which summarized and clarified earlier declarations. The Catholic Church believes that the matter for the Eucharist must be wheaten bread and fermented wine from grapes: it holds that, if the gluten has been entirely removed, the result is not true wheaten bread, For celiacs, but not generally, it allows low-gluten bread. It also permits Holy Communion to be received under the form of either bread or wine alone, except by a priest who is celebrating Mass without other priests or as principal celebrant. Many Protestant churches offer communicants gluten-free alternatives to wheaten bread, usually in the form of a rice-based cracker or gluten-free bread.
Alcohol.
The Catholic Church believes that grape juice that has not begun even minimally to ferment cannot be accepted as wine, which it sees as essential for celebration of the Eucharist. For alcoholics, but not generally, it allows the use of mustum (grape juice in which fermentation has begun but has been suspended without altering the nature of the juice), and it holds that, "since Christ is sacramentally present under each of the species, communion under the species of bread alone makes it possible to receive all the fruit of Eucharistic grace. For pastoral reasons this manner of receiving communion has been legitimately established as the most common form in the Latin rite. "
As already indicated, the one exception is in the case of a priest celebrating Mass without other priests or as principal celebrant. The water that in the Latin Church is prescribed to be mixed with the wine must be only a relatively small quantity. The practice of the Coptic Church is that the mixture should be two parts wine to one part water.
Many Protestant churches allow clergy and communicants to take mustum instead of wine. In addition to, or in replacement of wine, some churches offer grape juice which has been pasteurized to stop the fermentation process the juice naturally undergoes; de-alcoholized wine from which most of the alcohol has been removed (between 0.5% and 2% remains); or water. Exclusive use of unfermented grape juice is common in Baptist churches, the United Methodist Church, Seventh-day Adventists, Christian Churches/Churches of Christ, Churches of Christ, some Lutherans, Assemblies of God, Pentecostals, Evangelicals, the Christian Missionary Alliance, and other American independent Protestant churches.,
Fear of transmission of diseases.
Risk of infectious disease transmission related to use of a common communion cup is low, to the point of being undetectable. No case of transmission of an infectious disease related to a common communion cup has ever been documented. The most likely diseases to be transmitted would be common viral illnesses such as the common cold, but a study of 681 individuals found that taking communion up to daily from a common cup did not increase the risk of infection beyond that of those who did not attend services at all.
In influenza epidemics, some churches suspend the giving of communion under the form of wine, for fear of spreading the disease. This is in full accord with Catholic Church belief that communion under the form of bread alone makes it possible to receive all the fruit of Eucharistic grace. However, the same measure has been taken also by churches that normally insist on the importance of receiving communion under both forms. This was done in 2009 by the Church of England.
Some fear contagion through the handling involved in distributing the hosts to the communicants, even if they are placed on the hand rather than on the tongue. Accordingly, some churches use mechanical wafer dispensers or "pillow packs" (communion wafers with wine inside them). While these methods of distributing communion are not accepted in Catholic Church parishes, one such church provides a mechanical dispenser to allow those intending to communicate to place in a bowl, without touching them by hand, the hosts for use in the celebration.

</doc>
<doc id="9770" url="http://en.wikipedia.org/wiki?curid=9770" title="Eclipse">
Eclipse

An eclipse is an astronomical event that occurs when an astronomical object is temporarily obscured, either by passing into the shadow of another body or by having another body pass between it and the viewer. An eclipse is a type of syzygy.
The term eclipse is most often used to describe either a solar eclipse, when the Moon's shadow crosses the Earth's surface, or a lunar eclipse, when the Moon moves into the Earth's shadow. However, it can also refer to such events beyond the Earth–Moon system: for example, a planet moving into the shadow cast by one of its moons, a moon passing into the shadow cast by its host planet, or a moon passing into the shadow of another moon. A binary star system can also produce eclipses if the plane of the orbit of its constituent stars intersects the observer's position.
Etymology.
The term is derived from the ancient Greek noun ἔκλειψις ("ékleipsis"), which means "the abandonment", "the downfall", or "the darkening of a heavenly body", which is derived from the verb ἐκλείπω ("ekleípō") which means "to abandon", "to darken", or "to cease to exist," a combination of prefix ἐκ- ("ek-"), from preposition ἐκ ("ek"), "out," and of verb λείπω ("leípō"), "to be absent".
Umbra, penumbra and antumbra.
The region of the Moon's shadow in a solar eclipse is divided into three parts:
During a lunar eclipse only the umbra and penumbra are applicable. This is because Earth's apparent diameter from the viewpoint of the Moon is nearly four times that of the Sun.
The "first contact" occurs when the Moon's disc first starts to impinge on the Sun's; "second contact" is when the Moon's disc moves completely within the Sun's; "third contact" when it starts to move out of the Sun's; and "fourth" or "last contact" when it finally leaves the Sun's disc entirely.
The same terms may be used analogously in describing other eclipses, e.g., the antumbra of Deimos crossing Mars, or Phobos entering Mars's penumbra.
A total eclipse occurs when the observer is within the umbra, an annular eclipse when the observer is within the antumbra, and a partial eclipse when the observer is within the penumbra.
For spherical bodies, when the occulting object is smaller than the star, the length ("L") of the umbra's cone-shaped shadow is given by:
where "Rs" is the radius of the star, "Ro" is the occulting object's radius, and "r" is the distance from the star to the occulting object. For Earth, on average "L" is equal to 1.384×106 km, which is much larger than the Moon's semimajor axis of 3.844×105 km. Hence the umbral cone of the Earth can completely envelop the Moon during a lunar eclipse. If the occulting object has an atmosphere, however, some of the luminosity of the star can be refracted into the volume of the umbra. This occurs, for example, during an eclipse of the Moon by the Earth—producing a faint, ruddy illumination of the Moon even at totality.
The shadow cast during an eclipse moves very approximately at 1 km per sec. This depends on the location of the shadow on the earth and the angle in which it is moving. http://www.sciforums.com/threads/speed-of-eclipse-shadow.53722/
Eclipse cycles.
An eclipse cycle takes place when a series of eclipses are separated by a certain interval of time. This happens when the orbital motions of the bodies form repeating harmonic patterns. A particular instance is the saros, which results in a repetition of a solar or lunar eclipse every 6,585.3 days, or a little over 18 years (because this is not a whole number of days, successive eclipses will be visible from different parts of the world).
Earth–Moon System.
An eclipse involving the Sun, Earth and Moon can occur only when they are nearly in a straight line, allowing one to be hidden behind another, viewed from the third. Because the orbital plane of the Moon is tilted with respect to the orbital plane of the Earth (the ecliptic), eclipses can occur only when the Moon is close to the intersection of these two planes (the nodes). The Sun, Earth and nodes are aligned twice a year (during an eclipse season), and eclipses can occur during a period of about two months around these times. There can be from four to seven eclipses in a calendar year, which repeat according to various eclipse cycles, such as a saros.
Between 1901 and 2100 there are the maximum of seven eclipses in:
Excluding penumbral lunar eclipses, there are a maximum of seven eclipses in:
Solar eclipse.
As observed from the Earth, a solar eclipse occurs when the Moon passes in front of the Sun. The type of solar eclipse event depends on the distance of the Moon from the Earth during the event. A total solar eclipse occurs when the Earth intersects the umbra portion of the Moon's shadow. When the umbra does not reach the surface of the Earth, the Sun is only partially occulted, resulting in an annular eclipse. Partial solar eclipses occur when the viewer is inside the penumbra.
The eclipse magnitude is the fraction of the Sun's diameter that is covered by the Moon. For a total eclipse, this value is always greater than or equal to one. In both annular and total eclipses, the eclipse magnitude is the ratio of the angular sizes of the Moon to the Sun.
Solar eclipses are relatively brief events that can only be viewed in totality along a relatively narrow track. Under the most favorable circumstances, a total solar eclipse can last for 7 minutes, 31 seconds, and can be viewed along a track that is up to 250 km wide. However, the region where a partial eclipse can be observed is much larger. The Moon's umbra will advance eastward at a rate of 1,700 km/h, until it no longer intersects the Earth's surface.
During a solar eclipse, the Moon can sometimes perfectly cover the Sun because its size is nearly the same as the Sun's when viewed from the Earth. A total solar eclipse is in fact an occultation while an annular solar eclipse is a transit.
When observed at points in space other than from the Earth's surface, the Sun can be eclipsed by bodies other than the Moon. Two examples include when the crew of Apollo 12 observed the in 1969 and when the Cassini probe observed in 2006.
Lunar eclipse.
Lunar eclipses occur when the Moon passes through the Earth's shadow.This occurs only when the Moon is on the far side of the Earth from the Sun, lunar eclipses only occur when there is a full moon. Unlike a solar eclipse, an eclipse of the Moon can be observed from nearly an entire hemisphere. For this reason it is much more common to observe a lunar eclipse from a given location. A lunar eclipse also lasts longer, taking several hours to complete, with totality itself usually averaging anywhere from about 30 minutes to over an hour.
There are three types of lunar eclipses: penumbral, when the Moon crosses only the Earth's penumbra; partial, when the Moon crosses partially into the Earth's umbra; and total, when the Moon crosses entirely into the Earth's umbra. Total lunar eclipses pass through all three phases. Even during a total lunar eclipse, however, the Moon is not completely dark. Sunlight refracted through the Earth's atmosphere enters the umbra and provides a faint illumination. Much as in a sunset, the atmosphere tends to more strongly scatter light with shorter wavelengths, so the illumination of the Moon by refracted light has a red hue, thus the phrase 'Blood Moon' is often found in descriptions of such lunar events as far back as eclipses are recorded.
Historical record.
Records of solar eclipses have been kept since ancient times. Eclipse dates can be used for chronological dating of historical records. A Syrian clay tablet, in the Ugaritic language, records a solar eclipse which occurred on March 5, 1223 B.C., while Paul Griffin argues that a stone in Ireland records an eclipse on November 30, 3340 B.C. Positing classical-era astronomers' use of Babylonian eclipse records mostly from the 13th century BC provides a feasible and mathematically consistent explanation for the Greek finding all three lunar mean motions (synodic, anomalistic, draconitic) to a precision of about one part in a million or better. Chinese historical records of solar eclipses date back over 4,000 years and have been used to measure changes in the Earth's rate of spin.
By the 1600s, European astronomers were publishing books with diagrams explaining how lunar and solar eclipses occurred. In order to disseminate this information to a broader audience and decrease fear of the consequences of eclipses, booksellers printed broadsides explaining the event either using the science or via astrology.
Some other planets and Pluto.
Gas giants.
The gas giant planets (Jupiter, Saturn, Uranus, and Neptune) have many moons and thus frequently display eclipses. The most striking involve Jupiter, which has four large moons and a low axial tilt, making eclipses more frequent as these bodies pass through the shadow of the larger planet. Transits occur with equal frequency. It is common to see the larger moons casting circular shadows upon Jupiter's cloudtops.
The eclipses of the Galilean moons by Jupiter became accurately predictable once their orbital elements were known. During the 1670s, it was discovered that these events were occurring about 17 minutes later than expected when Jupiter was on the far side of the Sun. Ole Rømer deduced that the delay was caused by the time needed for light to travel from Jupiter to the Earth. This was used to produce the first estimate of the speed of light.
On the other three gas giants, eclipses only occur at certain periods during the planet's orbit, due to their higher inclination between the orbits of the moon and the orbital plane of the planet. The moon Titan, for example, has an orbital plane tilted about 1.6° to Saturn's equatorial plane. But Saturn has an axial tilt of nearly 27°. The orbital plane of Titan only crosses the line of sight to the Sun at two points along Saturn's orbit. As the orbital period of Saturn is 29.7 years, an eclipse is only possible about every 15 years.
The timing of the Jovian satellite eclipses was also used to calculate an observer's longitude upon the Earth. By knowing the expected time when an eclipse would be observed at a standard longitude (such as Greenwich), the time difference could be computed by accurately observing the local time of the eclipse. The time difference gives the longitude of the observer because every hour of difference corresponded to 15° around the Earth's equator. This technique was used, for example, by Giovanni D. Cassini in 1679 to re-map France.
Mars.
On Mars, only partial solar eclipses (transits) are possible, because neither of its moons is large enough, at their respective orbital radii, to cover the Sun's disc as seen from the surface of the planet. Eclipses of the moons by Mars are not only possible, but commonplace, with hundreds occurring each Earth year. There are also rare occasions when Deimos is eclipsed by Phobos. Martian eclipses have been photographed from both the surface of Mars and from orbit.
Pluto.
Pluto, with its proportionately largest moon Charon, is also the site of many eclipses. A series of such mutual eclipses occurred between 1985 and 1990. These daily events led to the first accurate measurements of the physical parameters of both objects.
Mercury and Venus.
Eclipses are impossible on Mercury and Venus, which have no moons. However, both have been observed to transit across the face of the Sun. There are on average 13 transits of Mercury each century. Transits of Venus occur in pairs separated by an interval of eight years, but each pair of events happen less than once a century.
Eclipsing binaries.
A binary star system consists of two stars that orbit around their common centre of mass. The movements of both stars lie on a common orbital plane in space. When this plane is very closely aligned with the location of an observer, the stars can be seen to pass in front of each other. The result is a type of extrinsic variable star system called an eclipsing binary.
The maximum luminosity of an eclipsing binary system is equal to the sum of the luminosity contributions from the individual stars. When one star passes in front of the other, the luminosity of the system is seen to decrease. The luminosity returns to normal once the two stars are no longer in alignment.
The first eclipsing binary star system to be discovered was Algol, a star system in the constellation Perseus. Normally this star system has a visual magnitude of 2.1. However, every 2.867 days the magnitude decreases to 3.4 for more than nine hours. This is caused by the passage of the dimmer member of the pair in front of the brighter star. The concept that an eclipsing body caused these luminosity variations was introduced by John Goodricke in 1783.

</doc>
<doc id="9771" url="http://en.wikipedia.org/wiki?curid=9771" title="Ed (text editor)">
Ed (text editor)

ed is a line editor for the Unix operating system. It was one of the first end-user programs hosted on the system and has been the standard in Unix-based systems ever since.
The original version was written in PDP-11/20 assembler in 1971 by Ken Thompson.
History and influence.
The editor was originally written in PDP-11/20 assembler in 1971 by Ken Thompson. Many features of ed came from the qed from his alma mater University of California at Berkeley Thompson was very familiar with qed, and had reimplemented it on the CTSS and Multics systems. His versions of qed were the first to implement regular expressions. Although regular expressions are part of ed, their implementation is considerably less general than that in qed.
Dennis M. Ritchie produced what Doug McIlroy later described as the "definitive" ed, and aspects of ed went on to influence ex, which in turn spawned vi. The non-interactive Unix command grep was inspired by a common special uses of qed and later ed, where the command g/re/p means globally search for the regular expression re and print the lines containing it. The Unix stream editor, sed implemented many of the scripting features of qed that were not supported by ed on Unix. In turn sed influenced the design of the programming language AWK – which inspired aspects of Perl.
Features.
Features of ed include:
(In)famous for its terseness, ed gives almost no visual feedback. For example, the message that ed will produce in case of error, or when it wants to make sure the user wishes to quit without saving, is "?". It does not report the current filename or line number, or even display the results of a change to the text, unless requested. Older versions (ca. 1981) did not even ask for confirmation when a quit command was issued without the user saving changes. This terseness was appropriate in the early versions of Unix, when consoles were teletypes, modems were slow, and memory was precious. As computer technology improved and these constraints were loosened, editors with more visual feedback became the norm.
In current practice, ed is rarely used interactively, but does find use in some shell scripts. For interactive use, ed was subsumed by the sam, vi and Emacs editors in the 1980s. ed can be found on virtually every version of Unix and Linux available, and as such is useful for people who have to work with multiple versions of Unix. If something goes wrong, ed is sometimes the only editor available. This is often the only time when it is used interactively.
The ed commands are often imitated in other line-based editors. For example, EDLIN in early MS-DOS versions and 32-bit versions of Windows NT has a somewhat similar syntax, and text editors in many MUDs (LPMud and descendants, for example) use ed-like syntax. These editors, however, are typically more limited in function.
Example.
Here is an example transcript of an ed session. For clarity, commands and text typed by the user are in normal face, and output from ed is emphasized.
 a
 ed is the standard Unix text editor.
 This is line number two.
 2i
 .
 %l
 ed is the standard Unix text editor$.
 This is line number two$.
 3s/two/three/
 ,l
 ed is the standard Unix text editor$.
 This is line number three$.
 w text
 65
 q
The end result is a simple text file containing the following text:
 ed is the standard Unix text editor.
 This is line number three.
Started with an empty file, the "a" command appends text (all ed commands are single letters). The command put ed in "insert mode", inserting the characters that follow and is terminated by a single dot on a line. The two lines that are entered before the dot end up in the file buffer. The codice_1 command also goes into insert mode, and will insert the entered text (a single empty line in our case) before line two. All commands may be prefixed by a line number to operate on that line.
In the line codice_2, the lowercase L stands for the list command. The command is prefixed by a range, in this case codice_3 which is a shortcut for codice_4. A range is two line numbers separated by a comma (codice_5 means the last line). In return, ed lists all lines, from first to last. These lines are ended with dollar signs, so that white space at the end of lines is clearly visible.
Once the empty line is inserted in line 2, the line which reads "This is line number two." is now actually the third line. This error is corrected with codice_6, a substitution command. The codice_7 will apply it to the correct line; following the command is the text to be replaced, and then the replacement. Listing all lines with codice_8 (a lone comma is also a synonym for codice_3) the line is shown now to be correct.
codice_10 writes the buffer to the file "text" making ed respond with "65", the number of characters written to the file. codice_11 will end an ed session.
ed as a design archetype.
The influence of ed on later Unix utilities has been noted. More generally, ed continues to serve as an interface model for programs that must modify record sequences and for which scriptability is extremely important, even when the records bear little resemblance to the text lines manipulated by ed itself.
For example, is a scriptable editor/converter for version-control repositories.

</doc>
<doc id="9772" url="http://en.wikipedia.org/wiki?curid=9772" title="Edlin">
Edlin

Edlin is a line editor, and the only text editor provided with early versions of MS-DOS. Although superseded in MS-DOS 5.0 and later by the full-screen edit command, and by Notepad in Microsoft Windows, it continues to be included in the 32-bit versions of current Microsoft operating systems.
History.
Edlin was created by Tim Paterson in two weeks in 1980, for Seattle Computer Products's 86-DOS (QDOS) based on the CP/M line editor "ED".
Microsoft acquired 86-DOS and sold it as MS-DOS, so Edlin was included in v1.0–v5.0 of MS-DOS, after which the only editor included was the new full-screen EDIT.EXE in v6.0–v8.0. 
Early Windows versions ran on top of the later versions of MS-DOS, so Edlin was typically not available. 
However, Edlin is included in the 32-bit versions of Windows NT and its derivatives—up to and including Windows 8—because the NTVDM's DOS support in those operating systems is based on MS-DOS version 5.0. However, unlike most other external DOS commands, it has not been transformed into a native Win32 program. It also does not support long filenames, which were not added to MS-DOS and MS-Windows until long after Edlin was written.
Usage.
There are only a few commands. The short list can be found by entering a ? at the edlin prompt.
When a file is open, typing L lists the contents (e.g., codice_1 lists lines 1 through 6). Each line is displayed with a line number in front of it.
 *1,6L
 1: Edlin: The only text editor in early versions of DOS.
 2:
 3: Back in the day, I remember seeing web pages
 4: branded with a logo at the bottom:
 5: "This page created in edlin."
 6: The things that some people put themselves through. ;-)
The currently selected line has a *. To replace the contents of any line, the line number is entered and any text entered replaces the original. While editing a line pressing Ctrl-C cancels any changes. The * marker remains on that line.
Entering I (optionally preceded with a line number) inserts one or more lines before the * line or the line given. When finished entering lines, Ctrl-C returns to the edlin command prompt.
 *6I
 6:*(...or similar)
 7:*^C 
 *7D
 *L
 1: Edlin: The only text editor in early versions of DOS.
 2:
 3: Back in the day, I remember seeing web pages
 4: branded with a logo at the bottom:
 5: "This page created in edlin."
 6: (...or similar)
Scripts.
Edlin may be used as a non-interactive file editor in scripts by redirecting a series of edlin commands.
FreeDOS Edlin.
A GPL-licensed clone of Edlin that includes long filename support is available for download as part of the FreeDOS project. This runs on operating systems such as Linux or Unix as well as MS-DOS.

</doc>
<doc id="9773" url="http://en.wikipedia.org/wiki?curid=9773" title="EBCDIC">
EBCDIC

Extended Binary Coded Decimal Interchange Code (EBCDIC) is an 8-bit character encoding used mainly on IBM mainframe and IBM midrange computer operating systems.
EBCDIC descended from the code used with punched cards and the corresponding six bit binary-coded decimal code used with most of IBM's computer peripherals of the late 1950s and early 1960s. 
It is also employed on various non-IBM platforms such as Fujitsu-Siemens' BS2000/OSD, OS-IV, MSP, and MSP-EX, HP MPE/iX, and Unisys VS/9 and MCP.
History.
EBCDIC was devised in 1963 and 1964 by IBM and was announced with the release of the IBM System/360 line of mainframe computers. It is an 8-bit character encoding, in contrast to, and developed separately from, the 7-bit ASCII encoding scheme. It was created to extend the existing binary-coded decimal (BCD) interchange code, or BCDIC, which itself was devised as an efficient means of encoding the two "zone" and "number" punches on punched cards into 6 bits.
While IBM was a chief proponent of the ASCII standardization committee, the company did not have time to prepare ASCII peripherals (such as card punch machines) to ship with its System/360 computers, so the company settled on EBCDIC. The System/360 became wildly successful, and together with clones such as RCA Spectra 70, ICL System 4, and Fujitsu FACOM, thus so did EBCDIC.
All IBM mainframe and midrange peripherals and operating systems use EBCDIC as their inherent encoding (with toleration for ASCII, for example, ISPF in z/OS can browse and edit both EBCDIC and ASCII encoded files). Software and many hardware peripherals can translate to and from encodings, and modern mainframes (such as IBM zSeries) include processor instructions, at the hardware level, to accelerate translation between character sets.
There is an EBCDIC-oriented Unicode Transformation Format called UTF-EBCDIC proposed by the Unicode consortium, designed to allow easy updating of EBCDIC software to handle Unicode, but not intended to be used in open interchange environments. Even on systems with extensive EBCDIC support, it has not been popular. For example, z/OS supports Unicode (preferring UTF-16 specifically), but z/OS only has limited support for UTF-EBCDIC.
IBM AIX running on the RS/6000 and its descendants including the IBM Power Systems, Linux running on z Systems, and operating systems running on the IBM PC and its descendants use ASCII.
Compatibility with ASCII.
Oddly enough, the fact that all the code points were different was less of a problem for inter-operating with ASCII than the fact that sorting EBCDIC put the lowercase letters before the uppercase letters and those before the numbers, exactly the opposite order that sorting ASCII did.
Portability is hindered by a lack of many symbols commonly used in programming and in network communications, such as the curly braces.
The gaps between the letters made simple constructions that worked in ASCII fail on EBCDIC. This often caused problems when porting from ASCII systems.
By using all 8 bits EBCDIC may have encouraged the use of the 8-bit byte by IBM, while ASCII encouraged systems with 36 bits (as you could pack 5 7-bit ASCII characters in). When 8-bit bytes became popular ASCII systems often used the "unused" bit for other purposes, this made it much more difficult to transition to larger character sets.
Codepage layout.
The table below is based on CCSID 500, one of the code page variants of EBCDIC; it shows only the basic (English) EBCDIC characters. Characters 00–3F and FF are controls, 40 is space, 41 is no-break space ("RSP": "Required Space"), E1 is numeric space ("NSP": "Numeric Space"), and CA is soft hyphen. Characters are shown with their equivalent Unicode codes. Unassigned codes are typically filled with international or region-specific characters in the various EBCDIC code page variants, but the punctuation marks are often moved around as well, only the letters and numbers and space have the same assignments in all EBCDIC code pages. 
In each table cell below, the first row is an abbreviation for a control code or (for printable characters) the character itself; the second row is the Unicode code; and the third row is decimal value of the EBCDIC code.
Criticism and humor.
Open-source-software advocate and hacker Eric S. Raymond writes in his "Jargon File" that EBCDIC was almost universally loathed by early hackers and programmers. The Jargon File 4.4.7 gives the following definition:
EBCDIC design was also the source of many jokes. One such joke went: 
References to the EBCDIC character set are made in the classic Infocom adventure game series "Zork". In the "Machine Room" in "Zork II", EBCDIC is used to imply an incomprehensible language:
A similar description can be found in the "Maintenance Room" in Zork:

</doc>
<doc id="9775" url="http://en.wikipedia.org/wiki?curid=9775" title="Endoplasmic reticulum">
Endoplasmic reticulum

The endoplasmic reticulum (ER) is a type of organelle in the cells of eukaryotic organisms that forms an interconnected network of flattened, membrane-enclosed sacs or tubes known as cisternae. The endoplasm is the inner core of the cytoplasm and the membranes of the ER are continuous with the outer membrane of the nuclear envelope. Endoplasmic reticulum occurs in most types of eukaryotic cells, including the most primitive "Giardia", but is absent from red blood cells and spermatozoa. There are two types of endoplasmic reticulum, rough and smooth. The outer (cytosolic) face of the rough endoplasmic reticulum is studded with ribosomes that are the sites of protein synthesis. The rough endoplasmic reticulum is especially prominent in cells such as hepatocytes whereas active smooth endoplasmic reticulum lacks ribosomes and functions in lipid metabolism, carbohydrate metabolism, and detoxification and is especially abundant in mammalian liver and gonad cells. The lacy membranes of the endoplasmic reticulum were first seen in 1945 using electron microscopy.
Structure.
The general structure of the endoplasmic reticulum is a network of membranes called cisternae. These sac-like structures are held together by the cytoskeleton. The phospholipid membrane encloses a space, the cisternal space (or lumen), which is continuous with the perinuclear space but separate from the cytosol. The functions of the endoplasmic reticulum can be summarized as the synthesis and export of proteins and membrane lipids, but varies between ER and cell type and cell function. The quantity of both rough and smooth endoplasmic reticulum in a cell can slowly interchange from one type to the other, depending on the changing metabolic activities of the cell. Transformation can include embedding of new proteins in membrane as well as structural changes. Changes in protein content may occur without noticeable structural changes.
Rough endoplasmic reticulum.
The surface of the rough endoplasmic reticulum (often abbreviated RER or Rough ER) (also called "ergastoplasm") is studded with protein-manufacturing ribosomes giving it a "rough" appearance (hence its name). The binding site of the ribosome on the rough endoplasmic reticulum is the translocon. However, the ribosomes bound to it at any one time are not a stable part of this organelle's structure as they are constantly being bound and released from the membrane. A ribosome only binds to the RER once a specific protein-nucleic acid complex forms in the cytosol. This special complex forms when a free ribosome begins translating the mRNA of a protein destined for the secretory pathway. The first 5-30 amino acids polymerized encode a signal peptide, a molecular message that is recognized and bound by a signal recognition particle (SRP). Translation pauses and the ribosome complex binds to the RER translocon where translation continues with the nascent protein forming into the RER lumen and/or membrane. The protein is processed in the ER lumen by an enzyme (a signal peptidase), which removes the signal peptide. Ribosomes at this point may be released back into the cytosol; however, non-translating ribosomes are also known to stay associated with translocons.
The membrane of the rough endoplasmic reticulum forms large double membrane sheets that are located near, and continuous with, the outer layer of the nuclear envelope. Although there is no continuous membrane between the endoplasmic reticulum and the Golgi apparatus, membrane-bound vesicles shuttle proteins between these two compartments. Vesicles are surrounded by coating proteins called COPI and COPII. COPII targets vesicles to the Golgi apparatus and COPI marks them to be brought back to the rough endoplasmic reticulum. The rough endoplasmic reticulum works in concert with the Golgi complex to target new proteins to their proper destinations. A second method of transport out of the endoplasmic reticulum involves areas called membrane contact sites, where the membranes of the endoplasmic reticulum and other organelles are held closely together, allowing the transfer of lipids and other small molecules.
The rough endoplasmic reticulum is key in multiple functions:
Smooth endoplasmic reticulum.
The smooth endoplasmic reticulum (abbreviated SER) has functions in several metabolic processes. It synthesizes lipids, phospholipids, and steroids. Cells which secrete these products, such as those in the testes, ovaries, and sebaceous glands have an abundance of smooth endoplasmic reticulum. It also carries out the metabolism of carbohydrates, drug detoxification, attachment of receptors on cell membrane proteins, and steroid metabolism. In muscle cells, it regulates calcium ion concentration. Smooth endoplasmic reticulum is found in a variety of cell types (both animal and plant), and it serves different functions in each. The smooth endoplasmic reticulum also contains the enzyme glucose-6-phosphatase, which converts glucose-6-phosphate to glucose, a step in gluconeogenesis. It is connected to the nuclear envelope and consists of tubules that are located near the cell periphery. These tubes sometimes branch forming a network that is reticular in appearance. In some cells, there are dilated areas like the sacs of rough endoplasmic reticulum. The network of smooth endoplasmic reticulum allows for an increased surface area to be devoted to the action or storage of key enzymes and the products of these enzymes.
Sarcoplasmic reticulum.
The sarcoplasmic reticulum (SR), from the Greek σάρξ "sarx" ("flesh"), is smooth ER found in myocytes. The only structural difference between this organelle and the smooth endoplasmic reticulum is the medley of proteins they have, both bound to their membranes and drifting within the confines of their lumens. This fundamental difference is indicative of their functions: The endoplasmic reticulum synthesizes molecules, while the sarcoplasmic reticulum stores and pumps calcium ions. The sarcoplasmic reticulum contains large stores of calcium, which it sequesters and then releases when the muscle cell is stimulated. It plays a major role in excitation-contraction coupling.
Functions.
The endoplasmic reticulum serves many general functions, including the folding of protein molecules in sacs called cisternae and the transport of synthesized proteins in vesicles to the Golgi apparatus. Correct folding of newly made proteins is made possible by several endoplasmic reticulum chaperone proteins, including protein disulfide isomerase (PDI), ERp29, the Hsp70 family member BiP/Grp78, calnexin, calreticulin, and the peptidylpropyl isomerase family. Only properly folded proteins are transported from the rough ER to the Golgi apparatus. Disturbances in redox regulation, calcium regulation, glucose deprivation, and viral infection or the over-expression of proteins can lead to endoplasmic reticulum stress response (ER stress), a state in which the folding of proteins slows, leading to an increase in unfolded proteins. This stress is emerging as a potential cause of damage in hypoxia/ischemia, insulin resistance, and other disorders.
Protein transport.
Secretory proteins, mostly glycoproteins, are moved across the endoplasmic reticulum membrane. Proteins that are transported by the endoplasmic reticulum throughout the cell are marked with an address tag called a signal sequence. The N-terminus (one end) of a polypeptide chain (i.e., a protein) contains a few amino acids that work as an address tag, which are removed when the polypeptide reaches its destination. Nascent peptides reach the ER via the Translocon, a membrane-embedded multiprotein complex. Proteins that are destined for places outside the endoplasmic reticulum are packed into transport vesicles and moved along the cytoskeleton toward their destination. In human fibroblasts, the ER is always co-distributed with microtubules and the depolymerisation of the latter cause its co-aggregation with mitochondria, which are also associated with the ER. 
The endoplasmic reticulum is also part of a protein sorting pathway. It is, in essence, the transportation system of the eukaryotic cell. The majority of its resident proteins are retained within it through a retention motif. This motif is composed of four amino acids at the end of the protein sequence. The most common retention sequences are KDEL for lumen located proteins and KKXX for transmembrane protein. However, variations of KDEL and KKXX do occur, and other sequences can also give rise to endoplasmic reticulum retention. It is not known whether such variation can lead to sub-ER localizations. There are three KDEL (1, 2 and 3) receptors in mammalian cells, and they have a very high degree of sequence identity. The functional differences between these receptors remain to be established. 
Clinical significance.
Abnormalities in XBP1 lead to a heightened endoplasmic reticulum stress response and subsequently causes a higher susceptibility for inflammatory processes that may even contribute to Alzheimer's disease. In the colon, XBP1 anomalies have been linked to the inflammatory bowel diseases including Crohn's disease.
The unfolded protein response (UPR) is a cellular stress response related to the endoplasmic reticulum.The UPR is activated in response to an accumulation of unfolded or misfolded proteins in the lumen of the endoplasmic reticulum. The UPR functions to restore normal function of the cell by halting protein translation, degrading misfolded proteins, and activating the signaling pathways that lead to increasing the production of molecular chaperones involved in protein folding. Sustained overactivation of the UPR has been implicated in prion diseases as well as several other neurodegenerative diseases and the inhibition of the UPR could become a treatment for those diseases.
History.
The lacy membranes of the endoplasmic reticulum were first seen in 1945 by Keith R. Porter, Albert Claude, Brody Meskers and Ernest F. Fullam, using electron microscopy.

</doc>
<doc id="9776" url="http://en.wikipedia.org/wiki?curid=9776" title="Enemy (disambiguation)">
Enemy (disambiguation)

An enemy or foe is an individual or group that is seen as forcefully adverse or threatening.
Enemy or Enemies may refer to:

</doc>
<doc id="9778" url="http://en.wikipedia.org/wiki?curid=9778" title="Executive Order 9066">
Executive Order 9066

Executive Order 9066 is a United States presidential executive order signed and issued during World War II by the United States President Franklin D. Roosevelt on February 19, 1942, authorizing the Secretary of War to prescribe certain areas as military zones. Eventually, EO 9066 cleared the way for the deportation of Japanese Americans, Italian Americans, and German Americans to internment camps. The executive order was spurred by a combination of war hysteria and reactions to the Niihau Incident.
Transcript of Executive Order 9066.
"Executive" Order No. 9066
The President
Executive Order
Authorizing the Secretary of War to Prescribe Military Areas
Whereas the successful prosecution of the war requires every possible protection against espionage and against sabotage to national-defense material, national-defense premises, and national-defense utilities as defined in Section 4, Act of April 20, 1918, 40 Stat. 533, as amended by the Act of November 30, 1940, 54 Stat. 1220, and the Act of August 21, 1941, 55 Stat. 655 (U.S.C., Title 50, Sec. 104);
Now, therefore, by virtue of the authority vested in me as President of the United States, and Commander in Chief of the Army and Navy, I hereby authorize and direct the Secretary of War, and the Military Commanders whom he may from time to time designate, whenever he or any designated Commander deems such action necessary or desirable, to prescribe military areas in such places and of such extent as he or the appropriate Military Commander may determine, from which any or all persons may be excluded, and with respect to which, the right of any person to enter, remain in, or leave shall be subject to whatever restrictions the Secretary of War or the appropriate Military Commander may impose in his discretion. The Secretary of War is hereby authorized to provide for residents of any such area who are excluded therefrom, such transportation, food, shelter, and other accommodations as may be necessary, in the judgment of the Secretary of War or the said Military Commander, and until other arrangements are made, to accomplish the purpose of this order. The designation of military areas in any region or locality shall supersede designations of prohibited and restricted areas by the Attorney General under the Proclamations of December 7 and 8, 1941, and shall supersede the responsibility and authority of the Attorney General under the said Proclamations in respect of such prohibited and restricted areas.
I hereby further authorize and direct the Secretary of War and the said Military Commanders to take such other steps as he or the appropriate Military Commander may deem advisable to enforce compliance with the restrictions applicable to each Military area hereinabove authorized to be designated, including the use of Federal troops and other Federal Agencies, with authority to accept assistance of state and local agencies.
I hereby further authorize and direct all Executive Departments, independent establishments and other Federal Agencies, to assist the Secretary of War or the said Military Commanders in carrying out this Executive Order, including the furnishing of medical aid, hospitalization, food, clothing, transportation, use of land, shelter, and other supplies, equipment, utilities, facilities, and services.
This order shall not be construed as modifying or limiting in any way the authority heretofore granted under Executive Order No. 8972, dated December 12, 1941, nor shall it be construed as limiting or modifying the duty and responsibility of the Federal Bureau of Investigation, with respect to the investigation of alleged acts of sabotage or the duty and responsibility of the Attorney General and the Department of Justice under the Proclamations of December 7 and 8, 1941, prescribing regulations for the conduct and control of alien enemies, except as such duty and responsibility is superseded by the designation of military areas hereunder.
Franklin D. Roosevelt
The White House,
February 19, 1942." 
Exclusion under Executive Order 9066.
A month later, Roosevelt signed, which had been approved after only an hour of discussion in the Senate and thirty minutes in the House, in order to provide for enforcement of the exclusion authorized in his executive order. Authored by War Department official Karl Bendetsen — who would later be promoted to Director of the Control Administration and oversee the "evacuation" of Japanese Americans — the law made violations of military orders a misdemeanor punishable by up to $5,000 in fines and one year in prison.
As a result, approximately 120,000 men, women, and children of Japanese ancestry were evicted from the West Coast of the United States and held in internment camps across the country. Japanese Americans in Hawaii were not incarcerated in the same way, the attack on Pearl Harbor and the Battle of Niihau notwithstanding. Although the Japanese American population in Hawaii was nearly 40% of the population of Hawaii itself, only a few thousand people were detained there, suggesting that their mass removal on the West Coast was motivated by other reasons than "military necessity." 
In fact, Japanese Americans and other Asians in the U.S. suffered from prejudice and racially-motivated fear. Laws preventing Asian Americans from owning land, voting, testifying against whites in court, and other racially discriminatory laws existed long before World War II. Additionally, the FBI, Office of Naval Intelligence and Military Intelligence Division had been conducting surveillance on Japanese American communities in Hawaii and the US mainland from the early 1930s. In early 1941, President Roosevelt secretly commissioned a study to assess the possibility that Japanese Americans would pose a threat to U.S. security. The report, submitted exactly one month before Pearl Harbor was bombed, found that, "There will be no armed uprising of Japanese" in the United States. "For the most part," the Munson Report said, "the local Japanese are loyal to the United States or, at worst, hope that by remaining quiet they can avoid concentration camps or irresponsible mobs." A second investigation started in 1940, written by Naval Intelligence officer Kenneth Ringle and submitted in January 1942, likewise found no evidence of fifth column activity and urged against mass incarceration. Both were ignored.
Over two-thirds of the people of Japanese ethnicity interned, almost 70,000 — were American citizens. Many of the rest had lived in the country between 20 and 40 years. Most Japanese Americans, particularly the first generation born in the United States (the "nisei"), considered themselves loyal to the United States of America. No Japanese American or Japanese national resident in the United States was ever found guilty of sabotage or espionage.
Americans of Italian and German ancestry were also targeted by these restrictions, including internment. 11,000 people of German ancestry were interned, as were 3,000 people of Italian ancestry, along with some Jewish refugees. The interned Jewish refugees came from Germany, as the U.S. government did not differentiate between ethnic Jews and ethnic Germans (the term "Jewish" was defined as a religious practice, not an ethnicity). Some of the internees of European descent were interned only briefly, while others were held for several years beyond the end of the war. Like the Japanese internees, these smaller groups had American-born citizens in their numbers, especially among the children. A few members of ethnicities of other Axis countries were interned, but exact numbers are unknown.
World War II internment camps under Executive Order 9066.
Secretary of War Henry L. Stimson was responsible for assisting relocated people with transport, food, shelter, and other accommodations and delegated Colonel Karl Bendetsen to administer the removal of West Coast Japanese. Over the spring of 1942, Bendetsen issued Western Defense Command orders for Japanese Americans to present themselves for removal. The "evacuees" were taken first to temporary assembly centers, requisitioned fairgrounds and horse racing tracks where living quarters were often converted livestock stalls. As construction on the more permanent and isolated WRA camps was completed, the population was transferred by truck or train. These accommodations consisted of tar paper-walled frame buildings in parts of the country with bitter winters and often hot summers. The camps were guarded by armed soldiers and fenced with barbed wire (security measures not shown in published photographs of the camps). Camps held up to 18,000 people, and were small cities, with medical care, food, and education provided by the government. Adults were offered "camp jobs" with wages of $12 to $19 per month, and many camp services such as medical care and education were provided by the camp inmates themselves.
Post-World War II.
In December 1944, internees were released, often to resettlement facilities and temporary housing, and the internment camps were shut down by 1946.
In the years after the war, the interned Japanese Americans had to rebuild their lives. United States citizens and long-time residents who had been incarcerated lost their personal liberties; many also lost their homes, businesses, property, and savings. Individuals born in Japan were not allowed to become naturalized US citizens until 1952.
Executive Order 9066 was rescinded by Gerald Ford on February 19, 1976. In 1980, Jimmy Carter signed legislation to create the Commission on Wartime Relocation and Internment of Civilians (CWRIC). The CWRIC was appointed to conduct an official governmental study of Executive Order 9066, related wartime orders, and their impact on Japanese Americans in the West and Alaska Natives in the Pribilof Islands.
In December 1982, the CWRIC issued its findings in "Personal Justice Denied", concluding that the incarceration of Japanese Americans had not been justified by military necessity. The report determined that the decision to incarcerate was based on "race prejudice, war hysteria, and a failure of political leadership". The Commission recommended legislative remedies consisting of an official Government apology and redress payments of $20,000 to each of the survivors; a public education fund was set up to help ensure that this would not happen again (Public Law 100-383).
On August 10, 1988, the Civil Liberties Act of 1988, based on the CWRIC recommendations, was signed into law by Ronald Reagan. On November 21, 1989, George H. W. Bush signed an appropriation bill authorizing payments to be paid out between 1990 and 1998. In 1990, surviving internees began to receive individual redress payments and a letter of apology. This bill applied to the Japanese Americans and to members of the Aleut people inhabiting the strategic Aleutian islands in Alaska who were also relocated.
The anniversary of the signing of Executive Order 9066 is now the Day of Remembrance, an annual commemoration of the internment in the Japanese American community.

</doc>
<doc id="9779" url="http://en.wikipedia.org/wiki?curid=9779" title="Edvard Munch">
Edvard Munch

Edvard Munch (; ]; 12 December 1863 – 23 January 1944) was a Norwegian painter and printmaker whose intensely evocative treatment of psychological themes built upon some of the main tenets of late 19th-century Symbolism and greatly influenced German Expressionism in the early 20th century. One of his most well-known works is "The Scream" of 1893.
Life.
Childhood.
Edvard Munch was born in a farmhouse in the village of Ådalsbruk in Løten, Norway, to Laura Catherine Bjølstad and Christian Munch, the son of a priest. Christian was a doctor and medical officer who married Laura, a woman half his age, in 1861. Edvard had an elder sister, Johanne Sophie, and three younger siblings: Peter Andreas, Laura Catherine, and Inger Marie. Both Sophie and Edvard appear to have inherited their artistic talent from their mother. Edvard Munch was related to painter Jacob Munch and historian Peter Andreas Munch.
The family moved to Christiania (now Oslo) in 1864 when Christian Munch was appointed medical officer at Akershus Fortress. Edvard's mother died of tuberculosis in 1868, as did Munch's favorite sister Johanne Sophie in 1877. After their mother's death, the Munch siblings were raised by their father and by their aunt Karen. Often ill for much of the winters and kept out of school, Edvard would draw to keep himself occupied. He was tutored by his school mates and his aunt. Christian Munch also instructed his son in history and literature, and entertained the children with vivid ghost-stories and tales of American writer Edgar Allan Poe.
As Edvard remembered it, Christian's positive behavior toward his children was overshadowed by his morbid pietism. Munch wrote, "My father was temperamentally nervous and obsessively religious—to the point of psychoneurosis. From him I inherited the seeds of madness. The angels of fear, sorrow, and death stood by my side since the day I was born." Christian reprimanded his children by telling them that their mother was looking down from heaven and grieving over their misbehavior. The oppressive religious milieu, plus Edvard's poor health and the vivid ghost stories, helped inspire his macabre visions and nightmares; the boy felt that death was constantly advancing on him. One of Munch's younger sisters was diagnosed with mental illness at an early age. Of the five siblings, only Andreas married, but he died a few months after the wedding. Munch would later write, "I inherited two of mankind's most frightful enemies—the heritage of consumption and insanity."
Christian Munch's military pay was very low, and his attempts to develop a private side practice failed, keeping his family in genteel but perennial poverty. They moved frequently from one cheap flat to another. Munch's early drawings and watercolors depicted these interiors, and the individual objects, such as medicine bottles and drawing implements, plus some landscapes. By his teens, art dominated Munch's interests. At thirteen, Munch had his first exposure to other artists at the newly formed Art Association, where he admired the work of the Norwegian landscape school. He returned to copy the paintings, and soon he began to paint in oils.
Studies and influences.
In 1879, Munch enrolled in a technical college to study engineering, where he excelled in physics, chemistry and math. He learned scaled and perspective drawing, but frequent illnesses interrupted his studies. The following year, much to his father's disappointment, Munch left the college determined to become a painter. His father viewed art as an "unholy trade", and his neighbors reacted bitterly and sent him anonymous letters. In contrast to his father's rabid pietism, Munch adopted an undogmatic stance toward art. He wrote his goal in his diary: "in my art I attempt to explain life and its meaning to myself."
In 1881, Munch enrolled at the Royal School of Art and Design of Christiania, one of whose founders was his distant relative Jacob Munch. His teachers were sculptor Julius Middelthun and the naturalistic painter Christian Krohg. That year, Munch demonstrated his quick absorption of his figure training at the Academy in his first portraits, including one of his father and his first self-portrait. In 1883, Munch took part in his first public exhibition and shared a studio with other students. His full-length portrait of Karl Jensen-Hjell, a notorious bohemian-about-town, earned a critic's dismissive response: "It is impressionism carried to the extreme. It is a travesty of art." Munch's nude paintings from this period survive only in sketches, except for "Standing Nude" (1887). They may have been confiscated by his father.
During these early years, Munch experimented with many styles, including Naturalism and Impressionism. Some early works are reminiscent of Manet. Many of these attempts brought him unfavorable criticism from the press and garnered him constant rebukes by his father, who nonetheless provided him with small sums for living expenses. At one point, however, Munch's father, perhaps swayed by the negative opinion of Munch's cousin Edvard Diriks (an established, traditional painter), destroyed at least one painting (likely a nude) and refused to advance any more money for art supplies.
Munch also received his father's ire for his relationship with Hans Jæger, the local nihilist who lived by the code "a passion to destroy is also a creative passion" and who advocated suicide as the ultimate way to freedom. Munch came under his malevolent, anti-establishment spell. "My ideas developed under the influence of the bohemians or rather under Hans Jæger. Many people have mistakenly claimed that my ideas were formed under the influence of Strindberg and the Germans…but that is wrong. They had already been formed by then." At that time, contrary to many of the other bohemians, Munch was still respectful of women, as well as reserved and well-mannered, but he began to give in to the binge drinking and brawling of his circle. He was unsettled by the sexual revolution going on at the time and by the independent women around him. He later turned cynical concerning sexual matters, expressed not only in his behavior and his art, but in his writings as well, an example being a long poem called "The City of Free Love". Still dependent on his family for many of his meals, Munch's relationship with his father remained tense over concerns about his bohemian life.
After numerous experiments, Munch concluded that the Impressionist idiom did not allow sufficient expression. He found it superficial and too akin to scientific experimentation. He felt a need to go deeper and explore situations brimming with emotional content and expressive energy. Under Jæger's commandment that Munch should "write his life", meaning that Munch should explore his own emotional and psychological state, the young artist began a period of reflection and self-examination, recording his thoughts in his "soul's diary". This deeper perspective helped move him to a new view of his art. He wrote that his painting "The Sick Child" (1886), based on his sister's death, was his first "soul painting", his first break from Impressionism. The painting received a negative response from critics and from his family, and caused another "violent outburst of moral indignation" from the community. 
Only his friend Christian Krohg defended him:
He paints, or rather regards, things in a way that is different from that of other artists. He sees only the essential, and that, naturally, is all he paints. For this reason Munch's pictures are as a rule "not complete", as people are so delighted to discover for themselves. Oh, yes, they are complete. His complete handiwork. Art is complete once the artist has really said everything that was on his mind, and this is precisely the advantage Munch has over painters of the other generation, that he really knows how to show us what he has felt, and what has gripped him, and to this he subordinates everything else.
Munch continued to employ a variety of brushstroke technique and color palettes throughout the 1880s and early 1890s, as he struggled to define his style. His idiom continued to veer between naturalistic, as seen in "Portrait of Hans Jæger", and impressionistic, as in "Rue Lafayette". His "Inger On the Beach" (1889), which caused another storm of confusion and controversy, hints at the simplified forms, heavy outlines, sharp contrasts, and emotional content of his mature style to come. He began to carefully calculate his compositions to create tension and emotion. While stylistically influenced by the Post-Impressionists, what evolved was a subject matter which was symbolist in content, depicting a state of mind rather than an external reality. In 1889, Munch presented his first one-man show of nearly all his works to date. The recognition it received led to a two-year state scholarship to study in Paris under French painter Léon Bonnat.
Paris.
Munch arrived in Paris during the festivities of the Exposition Universelle (1889) and roomed with two fellow Norwegian artists. His picture, "Morning" (1884), was displayed at the Norwegian pavilion. He spent his mornings at Bonnat's busy studio (which included live female models) and afternoons at the exhibition, galleries, and museums (where students were expected to make copies as a way of learning technique and observation). Munch recorded little enthusiasm for Bonnat's drawing lessons—"It tires and bores me—it's numbing"—but enjoyed the master's commentary during museum trips.
Munch was enthralled by the vast display of modern European art, including the works of three artists who would prove influential: Paul Gauguin, Vincent van Gogh, and Henri de Toulouse-Lautrec—all notable for how they used color to convey emotion. Munch was particularly inspired by Gauguin's "reaction against realism" and his credo that "art was human work and not an imitation of Nature", a belief earlier stated by Whistler. As one of his Berlin friends said later of Munch, "he need not make his way to Tahiti to see and experience the primitive in human nature. He carries his own Tahiti within him."
That December, his father died, leaving Munch's family destitute. He returned home and arranged a large loan from a wealthy Norwegian collector when wealthy relatives failed to help, and assumed financial responsibility for his family from then on. Christian's death depressed him and he was plagued by suicidal thoughts: "I live with the dead—my mother, my sister, my grandfather, my father…Kill yourself and then it's over. Why live?" Munch's paintings of the following year included sketchy tavern scenes and a series of bright cityscapes in which he experimented with the pointillist style of Georges Seurat.
Berlin.
By 1892, Munch formulated his characteristic, and original, Synthetist aesthetic, as seen in "Melancholy" (1891), in which color is the symbol-laden element. Considered by the artist and journalist Christian Krohg as the first Symbolist painting by a Norwegian artist, "Melancholy" was exhibited in 1891 at the Autumn Exhibition in Oslo. In 1892, Adelsteen Normann, on behalf of the Union of Berlin Artists, invited Munch to exhibit at its November exhibition, the society's first one-man exhibition. However, his paintings evoked bitter controversy (dubbed "The Munch Affair"), and after one week the exhibition closed. Munch was pleased with the "great commotion", and wrote in a letter: "Never have I had such an amusing time—it's incredible that something as innocent as painting should have created such a stir."
In Berlin, Munch became involved in an international circle of writers, artists and critics, including the Swedish dramatist and leading intellectual August Strindberg, whom he painted in 1892. During his four years in Berlin, Munch sketched out most of the ideas that would comprise his major work, "The Frieze of Life", first designed for book illustration but later expressed in paintings. He sold little, but made some income from charging entrance fees to view his controversial paintings. Already, Munch was showing a reluctance to part with his paintings, which he termed his "children".
His other paintings, including casino scenes, show a simplification of form and detail which marked his early mature style. Munch also began to favor a shallow pictorial space and a minimal backdrop for his frontal figures. Since poses were chosen to produce the most convincing images of states of mind and psychological conditions, as in "Ashes", the figures impart a monumental, static quality. Munch's figures appear to play roles on a theatre stage ("Death in the Sick-Room"), whose pantomime of fixed postures signify various emotions; since each character embodies a single psychological dimension, as in "The Scream", Munch's men and women began to appear more symbolic than realistic. He wrote, "No longer should interiors be painted, people reading and women knitting: there would be living people, breathing and feeling, suffering and loving."
"The Scream".
"The Scream" exists in four versions: two pastels (1893 and 1895) and two paintings (1893 and 1910). There are also several lithographs of "The Scream" (1895 and later).
The 1895 pastel sold at auction on 2 May 2012 for US$119,922,500, including commission. It is the most colorful of the versions and is distinctive for the downward-looking stance of one of its background figures. It is also the only version not held by a Norwegian museum.
The 1893 version (shown here) was stolen from the National Gallery in Oslo in 1994 and recovered. The 1910 painting was stolen in 2004 from The Munch Museum in Oslo, but recovered in 2006 with limited damage.
"The Scream" is Munch's most famous work, and one of the most recognizable paintings in all art. It has been widely interpreted as representing the universal anxiety of modern man. Painted with broad bands of garish color and highly simplified forms, and employing a high viewpoint, it reduces the agonized figure to a garbed skull in the throes of an emotional crisis.
With this painting, Munch met his stated goal of "the study of the soul, that is to say the study of my own self". Munch wrote of how the painting came to be: "I was walking down the road with two friends when the sun set; suddenly, the sky turned as red as blood. I stopped and leaned against the fence, feeling unspeakably tired. Tongues of fire and blood stretched over the bluish black fjord. My friends went on walking, while I lagged behind, shivering with fear. Then I heard the enormous, infinite scream of nature." He later described the personal anguish behind the painting, "for several years I was almost mad… You know my picture, 'The Scream?' I was stretched to the limit—nature was screaming in my blood… After that I gave up hope ever of being able to love again."
In summing up the painting's effects, author Martha Tedeschi has stated: ""Whistler's Mother", Wood's "American Gothic", Leonardo da Vinci's "Mona Lisa" and Edvard Munch's "The Scream" have all achieved something that most paintings—regardless of their art historical importance, beauty, or monetary value—have not: they communicate a specific meaning almost immediately to almost every viewer. These few works have successfully made the transition from the elite realm of the museum visitor to the enormous venue of popular culture."
"Frieze of Life—A Poem about Life, Love and Death".
In December 1893, Unter den Linden in Berlin was the location of an exhibition of Munch's work, showing, among other pieces, six paintings entitled "Study for a Series: Love." This began a cycle he later called the "Frieze of Life—A Poem about Life, Love and Death." "Frieze of Life" motifs, such as "The Storm" and "Moonlight," are steeped in atmosphere. Other motifs illuminate the nocturnal side of love, such as "Rose and Amelie" and "Vampire". In "Death in the Sickroom", the subject is the death of his sister Sophie, which he re-worked in many future variations. The dramatic focus of the painting, portraying his entire family, is dispersed in the separate and disconnected figures of sorrow. In 1894, he enlarged the spectrum of motifs by adding "Anxiety", "Ashes", "Madonna" and "Women in Three Stages" (from innocence to old age).
Around the start of the 20th century, Munch worked to finish the "Frieze". He painted a number of pictures, several of them in larger format and to some extent featuring the Art Nouveau aesthetics of the time. He made a wooden frame with carved reliefs for the large painting "Metabolism" (1898), initially called "Adam and Eve". This work reveals Munch's preoccupation with the "fall of man" and his pessimistic philosophy of love. Motifs such as "The Empty Cross" and "Golgotha" (both c. 1900) reflect a metaphysical orientation, and also reflect Munch's pietistic upbringing. The entire "Frieze" was shown for the first time at the secessionist exhibition in Berlin in 1902.
"The Frieze of Life" themes recur throughout Munch's work but he especially focused on them in the mid-1890s. In sketches, paintings, pastels and prints, he tapped the depths of his feelings to examine his major motifs: the stages of life, the femme fatale, the hopelessness of love, anxiety, infidelity, jealousy, sexual humiliation, and separation in life and death. These themes are expressed in paintings such as "The Sick Child" (1885), "Love and Pain" (retitled '; 1893–94), ' (1894), and "The Bridge". The latter shows limp figures with featureless or hidden faces, over which loom the threatening shapes of heavy trees and brooding houses. Munch portrayed women either as frail, innocent sufferers (see "Puberty" and "Love and Pain") or as the cause of great longing, jealousy and despair (see "Separation", "Jealousy," and "Ashes").
Munch often uses shadows and rings of color around his figures to emphasize an aura of fear, menace, anxiety, or sexual intensity. These paintings have been interpreted as reflections of the artist's sexual anxieties, though it could also be argued that they represent his turbulent relationship with love itself and his general pessimism regarding human existence. Many of these sketches and paintings were done in several versions, such as "Madonna", "Hands" and "Puberty", and also transcribed as wood-block prints and lithographs. Munch hated to part with his paintings because he thought of his work as a single body of expression. So to capitalize on his production and make some income, he turned to graphic arts to reproduce many of his most famous paintings, including those in this series. Munch admitted to the personal goals of his work but he also offered his art to a wider purpose, "My art is really a voluntary confession and an attempt to explain to myself my relationship with life—it is, therefore, actually a sort of egoism, but I am constantly hoping that through this I can help others achieve clarity."
While attracting strongly negative reactions, in the 1890s Munch began to receive some understanding of his artistic goals, as one critic wrote, "With ruthless contempt for form, clarity, elegance, wholeness, and realism, he paints with intuitive strength of talent the most subtle visions of the soul." One of his great supporters in Berlin was Walther Rathenau, later the German foreign minister, who strongly contributed to his success.
Paris, Berlin and Christiania.
In 1896, Munch moved to Paris, where he focused on graphic representations of his "Frieze of Life" themes. He further developed his woodcut and lithographic technique. Munch's "Self-Portrait With Skeleton Arm" (1895) is done with an etching needle-and-ink method also used by Paul Klee. Munch also produced multi-colored versions of "The Sick Child" which sold well, as well as several nudes and multiple versions of "Kiss" (1892) Many of the Parisian critics still considered Munch's work "violent and brutal" but his exhibitions received serious attention and good attendance. His financial situation improved considerably and in 1897, Munch bought himself a summer house facing the fjords of Christiania, a small fisherman's cabin built in the late 18th century, in the small town of Åsgårdstrand in Norway. He dubbed this home the "Happy House" and returned here almost every summer for the next 20 years. It was this place he was missing when he was abroad and in the periods then he felt depressed and exhausted. "To walk in Åsgårdstrand is like walking among my paintings - I get so inspired to paint when I am here".
In 1897 Munch returned to Christiania, where he also received grudging acceptance—one critic wrote, "A fair number of these pictures have been exhibited before. In my opinion these improve on acquaintance." In 1899, Munch began an intimate relationship with Tulla Larsen, a "liberated" upper-class woman. They traveled to Italy together and upon returning, Munch began another fertile period in his art, which included landscapes and his final painting in "The Frieze of Life" series, "The Dance of Life" (1899). Larsen was eager for marriage, and Munch begged off. His drinking and poor health reinforced his fears, as he wrote in the third person, "Ever since he was a child he had hated marriage. His sick and nervous home had given him the feeling that he had no right to get married." Munch almost gave in to Tulla, but fled from her in 1900, also turning away from her considerable fortune, and moved to Berlin. His "Girls on the Jetty", created in eighteen different versions, demonstrated the theme of feminine youth without negative connotations. In 1902, he displayed his works thematically at the hall of the Berlin Succession, producing "a symphonic effect—it made a great stir—a lot of antagonism—and a lot of approval." The Berlin critics were beginning to appreciate Munch's work even though the public still found his work alien and strange.
The good press coverage gained Munch the attention of influential patrons Albert Kollman and Max Linde. He described the turn of events in his diary, "After twenty years of struggle and misery forces of good finally come to my aid in Germany—and a bright door opens up for me." However, despite this positive change, Munch's self-destructive and erratic behavior involved him first with a violent quarrel with another artist, then with an accidental shooting in the presence of Tulla Larsen, who had returned for a brief reconciliation, which injured two of his fingers. She finally left him and married a younger colleague of Munch. Munch took this as a betrayal, and he dwelled on the humiliation for some time to come, channeling some of the bitterness into new paintings. His paintings "Still Life (The Murderess)" and "The Death of Marat I", done in 1906-7, clearly reference the shooting incident and the emotional after effects.
In 1903-4, Munch exhibited in Paris where the coming Fauvists, famous for their boldly false colors, likely saw his works and might have found inspiration in them. When the Fauves held their own exhibit in 1906, Munch was invited and displayed his works with theirs. After studying the sculpture of Rodin, Munch may have experimented with plasticine as an aid to design, but he produced little sculpture. During this time, Munch received many commissions for portraits and prints which improved his usually precarious financial condition. In 1906 he painted the screen for an Ibsen play in the Berlin Deutsches Theater small Kammerspiele Theatre, in which the Frieze of Life was hung. The theatre's director Max Reinhardt later sold it; it is now in the Berlin Nationalgalerie. After an earlier period of landscapes, in 1907 he turned his attention again to human figures and situations.
Breakdown and recovery.
In the autumn of 1908, Munch's anxiety, compounded by excessive drinking and brawling, had become acute. As he later wrote, "My condition was verging on madness—it was touch and go." Subject to hallucinations and feelings of persecution, he entered the clinic of Dr. Daniel Jacobson. The therapy Munch received for the next eight months included diet and "electrification" (a treatment then fashionable for nervous conditions, not to be confused with electroconvulsive therapy). Munch's stay in hospital stabilized his personality, and after returning to Norway in 1909, his work became more colorful and less pessimistic. Further brightening his mood, the general public of Christiania finally warmed to his work, and museums began to purchase his paintings. He was made a Knight of the Royal Order of St. Olav "for services in art". His first American exhibit was in 1912 in New York.
As part of his recovery, Dr. Jacobson advised Munch to only socialize with good friends and avoid drinking in public. Munch followed this advice and in the process produced several full-length portraits of high quality of friends and patrons—honest portrayals devoid of flattery. He also created landscapes and scenes of people at work and play, using a new optimistic style—broad, loose brushstrokes of vibrant color with frequent use of white space and rare use of black—with only occasional references to his morbid themes. With more income, Munch was able to buy several properties giving him new vistas for his art and he was finally able to provide for his family.
The outbreak of World War I found Munch with divided loyalties, as he stated, "All my friends are German but it is France that I love." In the 1930s, his German patrons, many Jewish, lost their fortunes and some their lives during the rise of the Nazi movement. Munch found Norwegian printers to substitute for the Germans who had been printing his graphic work. Given his poor health history, during 1918 Munch felt himself lucky to have survived a bout of the Spanish Flu, the worldwide pandemic of that year.
Later years.
Munch spent most of his last two decades in solitude at his nearly self-sufficient estate in Ekely, at Skøyen, Oslo. Many of his late paintings celebrate farm life, including several in which he used his work horse "Rousseau" as a model. Without any effort, Munch attracted a steady stream of female models, whom he painted as the subjects of numerous nude paintings. He likely had sexual relations with some of them. Munch occasionally left his home to paint murals on commission, including those done for the Freia chocolate factory.
To the end of his life, Munch continued to paint unsparing self-portraits, adding to his self-searching cycle of his life and his unflinching series of takes on his emotional and physical states. In the 1930s and 1940s, the Nazis labeled Munch's work "degenerate art" (along with that of Picasso, Paul Klee, Matisse, Gauguin and many other modern artists) and removed his 82 works from German museums. Adolf Hitler announced in 1937, "For all we care, those prehistoric Stone Age culture barbarians and art-stutterers can return to the caves of their ancestors and there can apply their primitive international scratching."
In 1940, the Germans invaded Norway and the Nazi party took over the government. Munch was 76 years old. With nearly an entire collection of his art in the second floor of his house, Munch lived in fear of a Nazi confiscation. Seventy-one of the paintings previously taken by the Nazis had been returned to Norway through purchase by collectors (the other eleven were never recovered), including "The Scream" and "The Sick Child", and they too were hidden from the Nazis.
Munch died in his house at Ekely near Oslo on 23 January 1944, about a month after his 80th birthday. His Nazi-orchestrated funeral suggested to Norwegians that he was a Nazi sympathizer, a kind of appropriation of the independent artist. The city of Oslo bought the Ekely estate from Munch's heirs in 1946; his house was demolished in May 1960.
Legacy.
<poem>
From my rotting body,
flowers shall grow
and I am in them
and that is eternity.
</poem>
”
Edvard Munch
When Munch died, his remaining works were bequeathed to the city of Oslo, which built the Munch Museum at Tøyen (it opened in 1963). The museum holds a collection of approximately 1,100 paintings, 4,500 drawings, and 18,000 prints, the broadest collection of his works in the world. The Munch Museum serves as Munch's official estate, and has been active in responding to copyright infringements, as well as clearing copyright for the work, such as the appearance of Munch's "The Scream" in a 2006 M&M's advertising campaign. The U.S. copyright representative for the Munch Museum and the Estate of Edvard Munch is the Artists Rights Society.
Munch's art was highly personalized and he did little teaching. His "private" symbolism was far more personal than that of other Symbolist painters such as Gustave Moreau and James Ensor. Munch was still highly influential, particularly with the German Expressionists, who followed his philosophy, "I do not believe in the art which is not the compulsive result of Man's urge to open his heart." Many of his paintings, including "The Scream", have universal appeal in addition to their highly personal meaning.
Munch's works are now represented in numerous major museums and galleries in Norway and abroad. After the Cultural Revolution in the People's Republic of China ended, the National Gallery in Beijing chose Munch as the first Western artist whose works they exhibited. His cabin, "the Happy House," was given to the municipality of Åsgårdstrand in 1944; it serves as a small Munch museum. The inventory has been maintained exactly as he left it.
One version of "The Scream" was stolen from the National Gallery in 1994. In 2004, another version of "The Scream", along with one of "Madonna", were stolen from the Munch Museum in a daring daylight robbery. All were eventually recovered, but the paintings stolen in the 2004 robbery were extensively damaged. They have been meticulously restored and are on display again. Three Munch works were stolen from the Hotel Refsnes Gods in 2005; they were shortly recovered, although one of the works was damaged during the robbery.
In October 2006, the color woodcut "Two people. The lonely" ("To mennesker. De ensomme") set a new record for his prints when it was sold at an auction in Oslo for 8.1 million NOK (US$1.27 million). It also set a record for the highest price paid in auction in Norway. On 3 November 2008, the painting "Vampire" set a new record for his paintings when it was sold for US$38.162 million at Sotheby's New York.
Munch's image appears on the Norwegian 1,000 kroner note, along with pictures inspired by his artwork.
In February 2012, a major Munch exhibition, "Edvard Munch. The Modern Eye," opened at the Schirn Kunsthalle Frankfurt; the exhibition was opened by Mette-Marit, Crown Princess of Norway.
In May 2012, "The Scream" sold for $119.9 million, and is the second most expensive artwork ever sold at an open auction. (It was surpassed in November 2013 by "Three Studies of Lucian Freud", which sold for $142.4 million).
In 2013, four of Munch's paintings were depicted in a series of stamps by the Norwegian postal service, to commemorate the 150th anniversary of his birth in 2014.
University Aula.
In 1911 the final competition for the decoration of the large walls of the University of Oslo Aula (assembly hall) was held between Munch and Emanuel Vigeland. The episode is known as the "Aula Controversy". In 1914 Munch was finally commissioned to decorate the Aula and the work was completed in 1916. This major work in Norwegian monumental painting includes 11 paintings covering 223 m2. "The Sun", "History" and "Alma Mater" are the key works in this sequence. Munch declared: “I wanted the decorations to form a complete and independent world of ideas, and I wanted their visual expression to be both distinctively Norwegian and universally human.” In 2014 it was suggested that the Aula paintings have a value of at least 500 million kroner.
References.
</dl>

</doc>
<doc id="9781" url="http://en.wikipedia.org/wiki?curid=9781" title="Extended Industry Standard Architecture">
Extended Industry Standard Architecture

 (1993)
The Extended Industry Standard Architecture (in practice almost always shortened to EISA and frequently pronounced "eee-suh") is a bus standard for IBM PC compatible computers. It was announced in September 1988 by a consortium of PC clone vendors (the "Gang of Nine") as a counter to IBM's use of its proprietary Micro Channel architecture (MCA) in its PS/2 series.
EISA extends the AT bus, which the Gang of Nine retroactively renamed to the ISA bus to avoid infringing IBM's trademark on its PC/AT computer, to 32 bits and allows more than one CPU to share the bus. The bus mastering support is also enhanced to provide access to 4 GB of memory. Unlike MCA, EISA can accept older XT and ISA boards — the lines and slots for EISA are a superset of ISA.
EISA was much favoured by manufacturers due to the proprietary nature of MCA, and even IBM produced some machines supporting it. It was somewhat expensive to implement (though not as much as MCA), so it never became particularly popular in desktop PCs. However, it was reasonably successful in the server market, as it was better suited to bandwidth-intensive tasks (such as disk access and networking). Most EISA cards produced were either SCSI or network cards. EISA was also available on some non-IBM compatible machines such as the AlphaServer, HP 9000-D, SGI Indigo2 and MIPS Magnum.
By the time there was a strong market need for a bus of these speeds and capabilities for desktop computers, the VESA Local Bus and later PCI filled this niche and EISA vanished into obscurity.
History.
The original IBM PC included five 8-bit slots, running at the system clock speed of 4.77 MHz. The PC/AT, introduced in 1984, had three 8-bit slots and five 16-bit slots, all running at the system clock speed of 6 MHz in the earlier models and 8 MHz in the last version of the computer. The 16-bit slots were a superset of the 8-bit configuration, so "most" 8-bit cards were able to plug into a 16-bit slot (some cards used a "skirt" design that physically interfered with the extended portion of the slot) and continue to run in 8-bit mode. One of the key reasons for the success of the IBM PC (and the PC clones that followed it) was the active ecosystem of third-party expansion cards available for the machines. IBM was restricted from patenting the bus, and widely published the bus specifications.
As the PC-clone industry continued to build momentum in the mid- to late-1980s, several problems with the bus began to be apparent. First, because the "AT slot" (as it was known at the time) was not managed by any central standards group, there was nothing to prevent a manufacturer from "pushing" the standard. One of the most common issues was that as PC clones became more common, PC manufacturers began ratcheting up the processor speed to maintain a competitive advantage. Unfortunately, because the ISA bus was originally locked to the processor clock, this meant that some 286 machines had ISA buses that ran at 10, 12, or even 16 MHz. In fact, the first system to clock the ISA bus at 8 MHz was the turbo 8088 clones that clocked the processors at 8 MHz. This caused many issues with incompatibility, where a true IBM-compatible third-party card (designed for an 8 MHz or 4.77 MHz bus) might not work in a higher speed system (or even worse, would work unreliably). Most PC makers eventually decoupled the slot clock from the system clock, but there was still no standards body to "police" the industry.
As companies like Dell modified the AT bus design, the architecture was so well entrenched that no single clone manufacturer had the leverage to create a standardized alternative, and there was no compelling reason for them to cooperate on a new standard. Because of this, when the first 386-based system (the Compaq Deskpro 386) hit the market in 1986, it still supported 16-bit slots. Other 386 PCs followed suit, and the AT (later ISA) bus remained a part of most systems even into the late 1990s. Some of the 386 systems had proprietary 32-bit extensions to the ISA bus.
Meanwhile, IBM began to worry that it was losing control of the industry it had created. In 1987, IBM released the PS/2 line of computers, which included the MCA bus. MCA included numerous enhancements over the 16-bit AT bus, including bus mastering, burst mode, software configurable resources, and 32-bit capabilities. However, in an effort to reassert its dominant role, IBM patented the bus, and placed stringent licensing and royalty policies on its use. A few manufacturers did produce licensed MCA machines (most notably NCR), but overall the industry balked at IBM's restrictions.
Steve Gibson proposed that clone makers adopt NuBus. Instead. a group (the "Gang of Nine"), led by Compaq, created a new bus, which was named the Extended (or Enhanced) Industry Standard Architecture, or "EISA". (The Industry Standard Architecture, or "ISA", name replaced the "AT" name commonly used for the 16-bit bus.) This provided virtually all of the technical advantages of MCA, while remaining compatible with existing 8-bit and 16-bit cards, and (most enticing to system and card makers) minimal licensing cost.
Intel introduced their first EISA chipset (and also their first chipset in the modern sense of the word) as the 82350 in September 1989. Intel introduced a lower cost variant as the 82350DT announced in April 1991; it began shipping in June of that year.
The first EISA computer announced was the HP Vectra 486 in October 1989. The first EISA computers to hit the market were the Compaq Deskpro 486 and the SystemPro. The SystemPro, being one of the first PC-style systems designed as a network server, was built from the ground up to take full advantage of the EISA bus. It included such features as multiprocessing, hardware RAID, and bus-mastering network cards.
Ironically, one of the benefits to come out of the EISA standard was a final codification of the standard to which ISA slots and cards should be held (in particular, clock speed was fixed at an industry standard of 8.33 MHz). Thus, even systems which didn't use the EISA bus gained the advantage of having the ISA standardized, which contributed to its longevity.
The "Gang of Nine".
The "Gang of Nine" was the informal name given to the consortium of personal computer manufacturing companies who came together in 1988 to create the EISA bus. 
The member companies were:
Technical data.
Although the EISA bus had a slight performance disadvantage over MCA (bus speed of 8.33 MHz, compared to 10 MHz), EISA contained almost all of the technological benefits that MCA boasted, including bus mastering, burst mode, software configurable resources, and 32-bit data/address buses. These brought EISA nearly to par with MCA from a performance standpoint, and EISA easily defeated MCA in industry support.
EISA replaced the tedious jumper configuration common with ISA cards with software-based configuration. Every EISA system shipped with an EISA configuration utility; this was usually a slightly customized version of the standard utilities written by the EISA chipset makers. The user would boot into this utility, either from floppy disk or on a dedicated hard drive partition. The utility software would detect all EISA cards in the system, and could configure any hardware resources (interrupts, memory ports, etc.) on any EISA card (each EISA card would include a disk with information that described the available options on the card), or on the EISA system motherboard. The user could also enter information about ISA cards in the system, allowing the utility to automatically reconfigure EISA cards to avoid resource conflicts.
Similarly, Windows 95, with its Plug-and-Play capability, was not able to change the configuration of EISA cards, but it could detect the cards, read their configuration, and reconfigure Plug and Play hardware to avoid resource conflicts. Windows 95 would also automatically attempt to install appropriate drivers for detected EISA cards.
Industry Acceptance.
EISA's success was far from guaranteed. Many manufacturers, including those in the "Gang of Nine", researched the possibility of using MCA. For example, Compaq actually produced prototype DeskPro systems using the bus. However, these were never put into production, and when it was clear that MCA had lost, Compaq allowed its MCA license to expire (the license actually cost relatively little; the primary costs associated with MCA, and at which the industry revolted, were royalties to be paid per system shipped).
On the other hand, when it became clear to IBM that Micro Channel was dying, IBM actually licensed EISA for use in a few server systems. As a final jab at their competitor, Compaq (leader of the EISA consortium) didn't cash the first check sent by IBM for the EISA license. Instead, the check was framed and put on display in the company museum at Compaq's main campus in Houston, Texas.
External links.
This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the "relicensing" terms of the GFDL, version 1.3 or later.

</doc>
