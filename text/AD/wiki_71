<doc id="55172" url="http://en.wikipedia.org/wiki?curid=55172" title="Proteomics">
Proteomics

Proteomics is the large-scale study of proteins, particularly their structures and functions. Proteins are vital parts of living organisms, as they are the main components of the physiological metabolic pathways of cells. The term "proteomics" was first coined in 1997 to make an analogy with genomics, the study of the genome. The word "proteome" is a portmanteau of "prote"in and gen"ome", and was coined by Marc Wilkins in 1994 while working on the concept as a PhD student.
The proteome is the entire set of proteins, produced or modified by an organism or system. This varies with time and distinct requirements, or stresses, that a cell or organism undergoes. Proteomics is an interdisciplinary domain formed on the basis of the research and development of the Human Genome Project; it is also emerging scientific research and exploration of proteomes from the overall level of intracellular protein composition, structure, and its own unique activity patterns. It is an important component of functional genomics.
While "proteomics" generally refers to the large-scale experimental analysis of proteins, it is often specifically used for protein purification and mass spectrometry.
Complexity of the problem.
After genomics and transcriptomics, proteomics is the next step in the study of biological systems. It is more complicated than genomics because an organism's genome is more or less constant, whereas the proteome differs from cell to cell and from time to time. Distinct genes are expressed in different cell types, which means that even the basic set of proteins that are produced in a cell needs to be identified.
In the past this phenomenon was done by RNA analysis, but it was found not to correlate with protein content. It is now known that mRNA is not always translated into protein, and the amount of protein produced for a given amount of mRNA depends on the gene it is transcribed from and on the current physiological state of the cell. Proteomics confirms the presence of the protein and provides a direct measure of the quantity present.
Post-translational modifications.
Not only does the translation from mRNA cause differences, but many proteins are also subjected to a wide variety of chemical modifications after translation. Many of these post-translational modifications are critical to the protein's function.
Phosphorylation.
One such modification is phosphorylation, which happens to many enzymes and structural proteins in the process of cell signaling. The addition of a phosphate to particular amino acids—most commonly serine and threonine mediated by serine/threonine kinases, or more rarely tyrosine mediated by tyrosine kinases—causes a protein to become a target for binding or interacting with a distinct set of other proteins that recognize the phosphorylated domain.
Because protein phosphorylation is one of the most-studied protein modifications, many "proteomic" efforts are geared to determining the set of phosphorylated proteins in a particular cell or tissue-type under particular circumstances. This alerts the scientist to the signaling pathways that may be active in that instance.
Ubiquitination.
Ubiquitin is a small protein that can be affixed to certain protein substrates by enzymes called E3 ubiquitin ligases. Determining which proteins are poly-ubiquitinated helps understand how protein pathways are regulated. This is, therefore, an additional legitimate "proteomic" study. Similarly, once a researcher determines which substrates are ubiquitinated by each ligase, determining the set of ligases expressed in a particular cell type is helpful.
Additional modifications.
In addition to phosphorylation and ubiquitination, proteins can be subjected to (among others) methylation, acetylation, glycosylation, oxidation and nitrosylation. Some proteins undergo all these modifications, often in time-dependent combinations. This illustrates the potential complexity of studying protein structure and function.
Distinct proteins are made under distinct settings.
Even studying a particular cell type, that cell may make different sets of proteins at different times, or under different conditions. Furthermore, as mentioned, any one protein can undergo a wide range of post-translational modifications.
Therefore a "proteomics" study can become complex, even if the topic of the study is restricted. In more ambitious settings, such as when a biomarker for a tumor is sought – when the proteomics scientist is obliged to study blood serum samples from multiple cancer patients.
Limitations of genomics and proteomics studies.
Proteomics gives a different level of understanding than genomics for many reasons:
"Reproducibility". Proteomics experiments conducted in one laboratory are not easily reproduced in another. For instance, Peng et al. have identified 1504 yeast proteins in a proteomics experiment of which only 858 were found in a similar previous study. Further, the previous study identified 607 proteins that were not found by Peng et al. This translates to a reproducibility of 57% (Peng vs. Washburn) to 59% (Washburn vs. Peng).
Methods of studying proteins.
Protein detection with immunoassays.
The enzyme-linked immunosorbent assay (ELISA) has been used for decades to detect and quantitatively measure proteins in samples.
The use of mass spectrometric immunoassay is the gold standard for both discovery and quantitative proteomics. Randall Nelson pioneered the use of immunoassays with mass spectrometry in 1995.
SISCAPA. Stable Isotope Standard Capture with Anti-Peptide Antibodies, is a term introduced by Leigh Anderson
Identifying proteins that are post-translationally modified.
One way a particular protein can be studied is to develop an antibody specific to that modification. For example, there are antibodies that only recognize certain proteins when they are tyrosine-phosphorylated, known as phospho-specific antibodies. Also, there are antibodies specific to other modifications. These can be used to determine the set of proteins that have undergone the modification of interest.
For sugar modifications, such as glycosylation of proteins, certain lectins have been discovered that bind sugars. These too can be used.
A more common way to determine post-translational modification of interest is to subject a complex mixture of proteins to electrophoresis in "two-dimensions", which simply means that the proteins are electrophoresed first in one direction, and then in another, which allows small differences in a protein to be visualized by separating a modified protein from its unmodified form. This methodology is known as "two-dimensional gel electrophoresis".
Recently, another approach has been developed called PROTOMAP, which combines SDS-PAGE with shotgun proteomics to enable detection of changes in gel-migration, such as those caused by proteolysis or post translational modification.
Determining the existence of proteins in complex mixtures.
Antibodies to particular proteins or to their modified forms have been used in biochemistry and cell biology studies. These are among the most common tools used by practicing biologists today.
For more quantitative determinations of protein amounts, techniques such as ELISAs can be used.
For proteomic study, more recent techniques such as matrix-assisted laser desorption/ionization (MALDI) have been employed for rapid determination of proteins in particular mixtures and increasingly electrospray ionization (ESI).
More recently, thermal protease resistance was exploited in a novel proteomic assay called Fast parallel proteolysis (FASTpp), which enabled detection of specific proteins in E. coli lysate and might be used in the future to detect proteomic perturbations in cancer or mechanistic effects of point mutations.
Establishing protein–protein interactions.
Most proteins function in collaboration with other proteins, and one goal of proteomics is to identify which proteins interact. This is especially useful in determining potential partners in cell signaling cascades.
Several methods are available to probe protein–protein interactions. The traditional method is yeast two-hybrid analysis. New methods include surface plasmon resonance (SPR), protein microarrays, immunoaffinity chromatography followed by mass spectrometry, dual polarisation interferometry, Microscale Thermophoresis and experimental methods such as phage display and computational methods.
Practical applications of proteomics.
One major development to come from the study of human genes and proteins has been the identification of potential new drugs for the treatment of disease. This relies on genome and proteome information to identify proteins associated with a disease, which computer software can then use as targets for new drugs. For example, if a certain protein is implicated in a disease, its 3D structure provides the information to design drugs to interfere with the action of the protein. A molecule that fits the active site of an enzyme, but cannot be released by the enzyme, inactivates the enzyme. This is the basis of new drug-discovery tools, which aim to find new drugs to inactivate proteins involved in disease. As genetic differences among individuals are found, researchers expect to use these techniques to develop personalized drugs that are more effective for the individual.
Proteomics is also used to reveal complex plant-insect interactions that help identify candidate genes involved in the defensive response of plants to herbivory.
Biomarkers.
The National Institutes of Health has defined a biomarker as “a characteristic that is objectively measured and evaluated as an indicator of normal biological processes, pathogenic processes, or pharmacologic responses to a therapeutic intervention.”
Understanding the proteome, the structure and function of each protein and the complexities of protein–protein interactions is critical for developing the most effective diagnostic techniques and disease treatments in the future. For example, proteomics is highly useful in identification of candidate biomarkers (proteins in body fluids that are of value for diagnosis), identification of the bacterial antigens that are targeted by the immune response, and identification of possible immunohistochemistry markers of infectious or neoplastic diseases.
An interesting use of proteomics is using specific protein biomarkers to diagnose disease. A number of techniques allow to test for proteins produced during a particular disease, which helps to diagnose the disease quickly. Techniques include western blot, immunohistochemical staining, enzyme linked immunosorbent assay (ELISA) or mass spectrometry. Secretomics, a subfield of proteomics that studies secreted proteins and secretion pathways using proteomic approaches, has recently emerged as an important tool for the discovery of biomarkers of disease.
Proteogenomics.
In what is now commonly referred to as proteogenomics, proteomic technologies such as mass spectrometry are used for improving gene annotations. Parallel analysis of the genome and the proteome facilitates discovery of post-translational modifications and proteolytic events, especially when comparing multiple species (comparative proteogenomics).
Current research methodologies.
Fluorescence two-dimensional differential gel electrophoresis (2-D DIGE) can be used to quantify variation in the 2-D DIGE process and establish statistically valid thresholds for assigning quantitative changes between samples.
Comparative proteomic analysis can reveal the role of proteins in complex biological systems, including reproduction. For example, treatment with the insecticide triazophos causes an increase in the content of brown planthopper ("Nilaparvata lugens" (Stål)) male accessory gland proteins (Acps) that can be transferred to females via mating, causing an increase in fecundity (i.e. birth rate) of females. To identify changes in the types of accessory gland proteins (Acps) and reproductive proteins that mated female planthoppers received from male planthoppers, researchers conducted a comparative proteomic analysis of mated "N. lugens" females. The results indicated that these proteins participate in the reproductive process of "N. lugens" adult females and males.
Proteome analysis of "Arabidopsis peroxisomes" has been established as the major unbiased approach for identifying new peroxisomal proteins on a large scale.
There are many approaches to characterizing the human proteome, which is estimated to contain between 20,000 and 25,000 non-redundant proteins. The number of unique protein species will likely increase by between 50,000 and 500,000 due to RNA splicing and proteolysis events, and when post-translational modification are also considered, the total number of unique human proteins is estimated to range in the low millions.
In addition, the first promising attempts to decipher the proteome of animal tumors have recently been reported.
Structural proteomics.
Structural proteomics includes the analysis of protein structures at large-scale. It compares protein structures and helps identify functions of newly discovered genes. The structural analysis also helps to understand that where drugs bind to proteins and also show where proteins interact with each other. This understanding is achieved using different technologies such as X-ray crystallography and NMR spectroscopy.
Expression proteomics.
Expression proteomics includes the analysis of protein expression at larger scale. It helps identify main proteins in a particular sample, and those proteins differentially expressed in related samples—such as diseased vs. healthy tissue. If a protein is found only in a diseased sample then it can be a useful drug target or diagnostic marker. Proteins with same or similar expression profiles may also be functionally related. There are technologies such as 2D-PAGE and mass spectrometry that are used in expression proteomics.
Interaction proteomics.
Interaction proteomics is the analysis of protein interactions at larger scale. The characterization of protein-protein interactions are useful to determine the protein functions and it also explains the way proteins assemble in bigger complexes. Technologies such as affinity purification, mass spectrometry, and the yeast two-hybrid system are particularly useful in interaction proteomics.
Proteome analysis techniques are not simple and straightforward as those used in transcriptomics. The benefit of proteomics, however, is that it deals with the real functional molecules of the cells. It is known that strong gene expression results in an abundant mRNA but it does not necessarily mean that the corresponding protein is also abundant. In proteomics things are not so simple as one gene does not always produce the same protein. The genes usually consist of a series of sub structures, which are called exons. These sub structures can be joined in a variety of ways, which helps to give momentum to a whole series of very similar but different proteins. Further increasing complications, once proteins are made, they are ornamented with different other chemicals. These chemicals can be phosphate, sugars or fats. The effect of the decorations is severe on the function of protein; for example phosphate normally behaves as an on-off switch and sugars usually tell the proteins where to go and attach in the cell. Therefore, it was comparatively very simple and easy to sequence the human genome as there are only 46 molecules and they are made up of 4 building blocks or letters (A, C, G, T) whereas proteins have 20 building blocks, each of which can be customized or ornamented after the protein is built. Hence, proteomics have to deal with ca. 30,000 genes that can be arranged to give some 800,000 proteins that can be modified and decorated with over 300 different chemicals. Additionally, proteomics also describe the nature of proteins, where they are being produced in a particular cell type and at a specific time, the way they are modified in the cell, the location where they are modified and also what they are in contact with. Finally, the most difficult thing is to determine the function of the protein.
Proteomics and system biology.
Proteomics has recently come into the act as a promising force to transform biology and medicine. It is becoming increasingly apparent that changes in mRNA expression correlate poorly with protein expression changes. Proteins change enormously in patterns of expressions across developmental and physiological responses. Proteins also face changes on the act of environmental perturbations. Proteins are the actual effectors driving cell behavior. The field of proteomics strives to characterize protein structure and function, protein-protein, protein-nucleic acid, protein-lipid, and enzyme-substrate interactions, protein processing and folding, protein activation, cellular and sub-cellular localization, protein turnover and synthesis rates, and even promoter usage. Integrating proteomic data with information such as gene, mRNA and metabolic profiles helps in better understanding of how the system works.
Current proteomic technologies.
Proteomics has steadily gained momentum over the past decade with the evolution of several approaches. Few of these are new and others build on traditional methods. Mass spectrometry-based methods and micro arrays are the most common technologies for large-scale study of proteins.
Mass spectrometry and protein profiling.
There are two mass spectrometry-based methods currently used for protein profiling. The more established and widespread method uses high resolution, two-dimensional electrophoresis to separate proteins from different samples in parallel, followed by selection and staining of differentially expressed proteins to be identified by mass spectrometry. Despite the advances in 2DE and its maturity, it has its limits as well.
The central concern is the inability to resolve all the proteins within a sample, given their dramatic range in expression level and differing properties.
The second quantitative approach uses stable isotope tags to differentially label proteins from two different complex mixtures. Here, the proteins within a complex mixture are labeled first isotopically, and then digested to yield labeled peptides. The labeled mixtures are then combined, the peptides separated by multidimensional liquid chromatography and analyzed by tandem mass spectrometry. Isotope coded affinity tag (ICAT) reagents are the widely used isotope tags. In this method, the cysteine residues of proteins get covalently attached to the ICAT reagent, thereby reducing the complexity of the mixtures omitting the non-cysteine residues.
Quantitative proteomics using stable isotopic tagging is an increasingly useful tool in modern development. Firstly, chemical reactions have been used to introduce tags into specific sites or proteins for the purpose of probing specific protein functionalities. The isolation of phosphorylated peptides has been achieved using isotopic labeling and selective chemistries to capture the fraction of protein among the complex mixture. Secondly, the ICAT technology was used to differentiate between partially purified or purified macromolecular complexes such as large RNA polymerase II pre-initiation complex and the proteins complexed with yeast transcription factor. Thirdly, ICAT labeling was recently combined with chromatin isolation to identify and quantify chromatin-associated proteins. Finally ICAT reagents are useful for proteomic profiling of cellular organelles and specific cellular fractions.
Another quantitative approach is the Accurate Mass and Time (AMT) tag approach developed by Richard D. Smith and coworkers at Pacific Northwest National Laboratory. In this approach, increased throughput and sensitivity is achieved by avoiding the needed for tandem mass spectrometry, and making use of precisely determined separation time information and highly accurate mass determinations for peptide and protein identifications.
Protein chips.
Balancing the use of mass spectrometers in proteomics and in medicine is the use of protein micro arrays. The aim behind protein micro arrays is to print thousands of protein detecting features for the interrogation of biological samples. Antibody arrays are an example in which a host of different antibodies are arrayed to detect their respective antigens from a sample of human blood. Another approach is the arraying of multiple protein types for the study of properties like protein-DNA, protein-protein and protein-ligand interactions. Ideally, the functional proteomic arrays would contain the entire complement of the proteins of a given organism. The first version of such arrays consisted of 5000 purified proteins from yeast deposited onto glass microscopic slides. Despite the success of first chip, it was a greater challenge for protein arrays to be implemented. Proteins are inherently much more difficult to work with than DNA. They have a broad dynamic range, are less stable than DNA and their structure is difficult to preserve on glass slides, though they are essential for most assays. The global ICAT technology has striking advantages over protein chip technologies.
Reverse-phased protein microarrays.
This is a promising and newer microarray application for the diagnosis, study and treatment of complex diseases such as cancer. The technology merges laser capture microdissection (LCM) with micro array technology, to produce reverse phase protein microarrays. In this type of microarrays, the whole collection of protein themselves are immobilized with the intent of capturing various stages of disease within an individual patient. When used with LCM, reverse phase arrays can monitor the fluctuating state of proteome among different cell population within a small area of human tissue. This is useful for profiling the status of cellular signaling molecules, among a cross section of tissue that includes both normal and cancerous cells. This approach is useful in monitoring the status of key factors in normal prostate epithelium and invasive prostate cancer tissues. LCM then dissects these tissue and protein lysates were arrayed onto nitrocellulose slides, which were probed with specific antibodies. This method can track all kinds of molecular events and can compare diseased and healthy tissues within the same patient enabling the development of treatment strategies and diagnosis. The ability to acquire proteomics snapshots of neighboring cell populations, using reverse phase microarrays in conjunction with LCM has a number of applications beyond the study of tumors. The approach can provide insights into normal physiology and pathology of all the tissues and is invaluable for characterizing developmental processes and anomalies.
Bioinformatics for proteomics (proteome informatics).
There is a large amount of proteomics data being collected with the help of high throughput technologies such as mass spectrometry and microarray. It would often take weeks or months to analyze the data and perform comparisons by hand. For this reason, biologists and chemists are collaborating with computer scientists and mathematicians to create programs and pipeline to computationally analyze the protein data. Using bioinformatics techniques, researchers are capable of faster analysis and data storage. A good place to find lists of current programs and databases is on the ExPASy bioinformatics resource portal <http://www.expasy.org/proteomics>. The applications of bioinformatics-based proteomics includes medicine, disease diagnosis, biomarker identification, and many more.
Protein identification.
Mass spectrometry and microarray produce peptide fragmentation information but do not give identification of specific proteins present in the original sample. Due to the lack of specific protein identification, past researchers were forced to decipher the peptide fragments themselves. However, there are currently programs available for protein identification. These programs take the peptide sequences output from mass spectrometry and microarray and return information about matching or similar proteins. This is done through algorithms implemented by the program which perform alignments with proteins from known databases such as UniProt <http://www.uniprot.org/> and PROSITE <http://prosite.expasy.org/> to predict what proteins are in the sample with a degree of certainty. Three such programs are MASCOT <http://www.matrixscience.com/>, PeptideMass <http://web.expasy.org/peptide_mass/>, and SPIRE (https://www.proteinspire.org/SPIRE/).
Protein structure.
The biomolecular structure forms the 3D configuration of the protein. Understanding the protein's structure aids in identification of the protein's interactions and function. It used to be that the 3D structure of proteins could only be determined using X-ray crystallography and NMR spectroscopy. Now, through bioinformatics, there are computer programs that can predict and model the structure of proteins. These programs use the chemical properties of amino acids and structural properties of known proteins to predict the 3D model of sample proteins. This also allows scientists to take a look at protein interactions on a larger scale. In addition, biomedical engineers are developing methods to factor in the flexibility of protein structures to make comparisons and predictions.
Post-translational modifications.
Unfortunately, most programs available for protein analysis are not written for proteins that have undergone post-translational modifications. Some programs will accept post-translational modifications to aid in protein identification but then ignore the modification during further protein analysis. It is important to account for these modifications since they can affect the protein's structure. In turn, computational analysis of post-translational modifications has gained the attention of the scientific community. The current post-translational modification programs are only predictive. Chemists, biologists and computer scientists are working together to create and introduce new pipelines that allow for analysis of post-translational modifications that have been experimentally identified for their effect on the protein's structure and function.
Computational methods in studying protein biomarkers.
One example of the use of bioinformatics and the use of computational methods is the study of protein biomarkers. Computational predictive models have shown that extensive and diverse feto-maternal protein trafficking occurs during pregnancy and can be readily detected non-invasively in maternal whole blood. This computational approach circumvented a major limitation, the abundance of maternal proteins interfering with the detection of fetal proteins, to fetal proteomic analysis of maternal blood. Computational models can use fetal gene transcripts previously identified in maternal whole blood to create a comprehensive proteomic network of the term neonate. Such work shows that the fetal proteins detected in pregnant woman’s blood originate from a diverse group of tissues and organs from the developing fetus. The proteomic networks contain many biomarkers that are proxies for development and illustrate the potential clinical application of this technology as a way to monitor normal and abnormal fetal development.
An information theoretic framework has also been introduced for biomarker discovery, integrating biofluid and tissue information. This new approach takes advantage of functional synergy between certain biofluids and tissues with the potential for clinically significant findings not possible if tissues and biofluids were considered individually. By conceptualizing tissue-biofluid as information channels, significant biofluid proxies can be identified and then used for guided development of clinical diagnostics. Candidate biomarkers are then predicted based on information transfer criteria across the tissue-biofluid channels. Significant biofluid-tissue relationships can be used to prioritize clinical validation of biomarkers.
Emerging trends in proteomics.
A number of emerging concepts have the potential to improve current features of proteomics. Obtaining absolute quantification of proteins and monitoring post-translational modifications are the two tasks that impacts the understanding of protein function in healthy and diseased cells. Advances in quantitative proteomics would clearly enable more in-depth analysis of cellular systems. For many cellular events, the protein concentrations do not change; rather, their function is modulated by post-transitional modifications (PTM). Methods of monitoring PTM are an underdeveloped area in proteomics. Selecting a particular subset of protein for analysis substantially reduces protein complexity, making it advantageous for diagnostic purposes where blood is the starting material. Another important aspect of proteomics, yet not addressed, is that proteomics methods should focus on studying proteins in the context of the environment. The increasing use of chemical cross linkers, introduced into living cells to fix protein-protein, protein-DNA and other interactions, may ameliorate this problem partially. The challenge is to identify suitable methods of preserving relevant interactions. Another goal for studying protein is to develop more sophisticated methods to image proteins and other molecules in living cells and real time.
Human plasma proteome.
Characterizing the human plasma proteome has become a major goal in the proteomics arena. The plasma proteome is without doubt the most complex proteome in the human body. It contains immunoglobulin, cytokines, protein hormones, and secreted proteins indicative of infection on top of resident, hemostatic proteins. It also contains tissue leakage proteins due to the blood circulation through different tissues in the body. The blood thus contains information on the physiological state of all tissues and, combined with its accessibility, makes the blood proteome invaluable for medical purposes. Even with the recent advancements in proteomics, characterizing the proteome of blood plasma is a daunting challenge.
Temporal and spatial dynamics further complicate the study of human plasma proteome. The turnover of some proteins is quite faster than others and the protein content of an artery may substantially vary from that of a vein. All these differences make even the simplest proteomic task of cataloging the proteome seem out of reach. To tackle this problem, priorities needs to be established. Capturing the most meaningful subset of proteins among the entire proteome to generate a diagnostic tool is one such priority. Secondly, since cancer is associated with enhanced glycosylation of proteins, methods that focus on this part of proteins will also be useful. Again: multiparameter analysis best reveals a pathological state. As these technologies improve, the disease profiles should be continually related to respective gene expression changes.
Bibliography.
</dl>

</doc>
<doc id="55174" url="http://en.wikipedia.org/wiki?curid=55174" title="Falun Gong">
Falun Gong

Falun Gong or Falun Dafa (literally means "Dharma Wheel Practice" or "Law Wheel Practice") is a Chinese spiritual practice that combines meditation and qigong exercises with a moral philosophy centered on the tenets of Truthfulness, Compassion, and (). The practice emphasizes morality and the cultivation of virtue, and identifies as a qigong practice of the Buddhist school, though its teachings also incorporate elements drawn from Taoist traditions. Through moral rectitude and the practice of meditation, practitioners of Falun Gong aspire to better health and, ultimately, spiritual enlightenment.
Falun Gong was first taught publicly in Northeast China in 1992 by Li Hongzhi. It emerged toward the end of China's ""qigong" boom"—a period which saw the proliferation of similar practices of meditation, slow-moving exercises and regulated breathing. It differs from other "qigong" schools in its absence of fees or formal membership, lack of daily rituals of worship, its greater emphasis on morality, and the theological nature of its teachings. Western academics have described Falun Gong as a qigong discipline, a "spiritual movement", a "cultivation system" in the tradition of Chinese antiquity, or as a form of Chinese religion.
Although the practice initially enjoyed considerable support from Chinese officialdom, by the mid- to late-1990s, the Communist Party and public security organizations increasingly viewed Falun Gong as a potential threat due to its size, independence from the state, and spiritual teachings. By 1999, government estimates placed the number of Falun Gong practitioners at 70 million. Tensions culminated in April 1999, when over 10,000 Falun Gong practitioners gathered peacefully near the central government compound in Beijing to request legal recognition and freedom from state interference. This demonstration is widely seen as catalyzing the persecution that followed.
On 20 July 1999, the Communist Party leadership initiated a nationwide crackdown and multifaceted propaganda campaign intended to eradicate the practice. It blocked Internet access to websites that mention Falun Gong, and in October 1999 it declared Falun Gong a "heretical organization" that threatened social stability. Human rights groups report that Falun Gong practitioners in China are subject to a wide range of human rights abuses: hundreds of thousands are estimated to have been imprisoned extrajudicially, and practitioners in detention are subject to forced labor, psychiatric abuse, torture, and other coercive methods of thought reform at the hands of Chinese authorities. As of 2009 at least 2,000 Falun Gong practitioners had died as a result of abuse in custody. Some observers putting the number much higher, and report that tens of thousands may have been killed to supply China's organ transplant industry. In the years since the persecution began, Falun Gong practitioners have become active in advocating for greater human rights in China.
Falun Gong founder Li Hongzhi has lived in the United States since 1996, and Falun Gong has a sizable global constituency. Inside China, some sources estimate that tens of millions continue to practice Falun Gong in spite of the persecution. Hundreds of thousands are estimated to practice Falun Gong outside China in over 70 countries worldwide.
Origins.
Falun Gong is most frequently identified with the qigong movement in China. "Qigong" is a modern term that refers to a variety of practices involving slow movement, meditation, and regulated breathing. Qigong-like exercises have historically been practiced by Buddhist monks, Daoist martial artists, and Confucian scholars as a means of spiritual, moral, and physical refinement.
The modern qigong movement emerged in the early 1950s, when Communist cadres embraced the techniques as a way to improve health. The new term was constructed to avoid association with religious practices, which were prone to being labeled as "feudal superstition" and persecuted during the Maoist era. Early adopters of qigong eschewed its religious overtones, and regarded qigong principally as a branch of Chinese medicine. In the late 1970s, Chinese scientists purported to have discovered the material existence of the qi energy which qigong seeks to harness. In the spiritual vacuum of the post-Mao era, tens of millions of mostly urban and elderly Chinese citizens took up the practice of qigong, and a variety of charismatic qigong masters established practices. At one time, over 2,000 disciplines of qigong were being taught. The state-run China Qigong Science Research Society (CQRS) was established in 1985 to oversee and administer the movement.
On 13 May 1992, Li Hongzhi gave his first public seminar on Falun Gong (alternatively called Falun Dafa) in the northeastern city of Changchun. In his hagiographic spiritual biography, Li Hongzhi is said to have been taught ways of "cultivation practice" by several masters of the Buddhist and Daoist traditions, including Quan Jue, the 10th Heir to the Great Law of the Buddha School, and a master of the Great Way School with the Taoist alias of "True Taoist" from the Changbai Mountains. Falun Dafa is said to be the result of his reorganizing and writing down the teachings that were passed to him.
Li presented Falun Gong as part of a "centuries-old tradition of cultivation", and in effect sought to revive the religious and spiritual elements of qigong practice that had been discarded in the earlier Communist era. David Palmer writes that Li "redefined his method as having entirely different objectives from qigong: the purpose of practice should neither be physical health nor the development of extraordinary powers, but to purify one's heart and attain spiritual salvation."
Falun Gong is distinct from other qigong schools in that its teachings cover a wide range of spiritual and metaphysical topics, placing emphasis on morality and virtue, and elaborating a complete cosmology. The practice identifies with the Buddhist School ("Fojia"), but also draws on concepts and language found in Taoism and Confucianism. This has led some scholars to label the practice as a syncretic faith.
Beliefs and practices.
Central teachings.
Falun Gong aspires to enable the practitioner to ascend spiritually through moral rectitude and the practice of a set of exercises and meditation. The three central tenets of the belief are Truthfulness (真, Zhēn), Compassion (善, Shàn), and Forbearance (忍, Rěn). Together these principles are regarded as the fundamental nature of the cosmos, the criterion for differentiating right from wrong, and are held to be the highest manifestation of the Tao, or Buddhist Dharma. Adherence to and cultivation of these virtues is regarded as a fundamental part of Falun Gong practice. In Zhuan Falun (轉法輪), the foundational text published in 1995, Li Hongzhi writes "It doesn't matter how mankind's moral standard changes ... The nature of the cosmos doesn't change, and it is the only standard for determining who's good and who's bad. So to be a cultivator you have to take the nature of the cosmos as your guide for improving yourself."
Practice of Falun Gong consists of two features: performance of the exercises, and the refinement of one's "xinxing" (moral character, temperament). In Falun Gong's central text, Li states that xinxing "includes virtue (which is a type of matter), it includes forbearance, it includes awakening to things, it includes giving up things—giving up all the desires and all the attachments that are found in an ordinary person—and you also have to endure hardship, to name just a few things." The elevation of one's moral character is achieved, on the one hand, by aligning one's life with truth, compassion, and tolerance; and on the other, by abandoning desires and "negative thoughts and behaviors, such as greed, profit, lust, desire, killing, fighting, theft, robbery, deception, jealousy, etc."
Among the central concepts found in the teachings of Falun Gong is the existence of 'Virtue' ('德, "Dé") and 'Karma' ("Ye"). The former is generated through doing good deeds and suffering, while the latter is accumulated through doing wrong deeds. A person's ratio of karma to virtue is said to determine his or her fortunes in this life or the next. While virtue engenders good fortune and enables spiritual transformation, an accumulation of karma results in suffering, illness, and alienation from the nature of the universe. Spiritual elevation is achieved through the elimination of negative karma and the accumulation of virtue.
Falun Gong's teachings posit that human beings are originally and innately good—even divine—but that they descended into a realm of delusion and suffering after developing selfishness and accruing karma. In order to re-ascend and return to the "original, true self", practitioners of Falun Gong are supposed to assimilate themselves to the qualities of truthfulness, compassion and tolerance, let go of "attachments and desires" and suffer to repay karma. The ultimate goal of the practice is enlightenment or spiritual perfection ("yuanman"), and release from the cycle of reincarnation, known in Buddhist tradition as "samsara".
Traditional Chinese cultural thought and modernity are two focuses of Li Hongzhi's teachings. Falun Gong echoes traditional Chinese beliefs that humans are connected to the universe through mind and body, and Li seeks to challenge "conventional mentalities", concerning the nature and genesis of the universe, time-space, and the human body. The practice draws on East Asian mysticism and traditional Chinese medicine, criticizes the purportedly self-imposed limits of modern science, and views traditional Chinese science as an entirely different, yet equally valid ontological system.
Exercises.
In addition to its moral philosophy, Falun Gong consists of four standing exercises and one sitting meditation. The exercises are regarded as secondary to moral elevation, though is still an essential component of Falun Gong cultivation practice.
The first exercises, called "Buddha Stretching a Thousand Arms", are intended to facilitate the free flow of energy through the body and open up the meridians. The second exercise, "Falun Standing Stance", involves holding four static poses—each of which resembles holding a wheel—for an extended period. The objective of this exercise is to "enhances wisdom, increases strength, raises a person's level, and strengthens divine powers". The third, "Penetrating the Cosmic Extremes", involves three sets of movements which aim to enable the expulsion of bad energy (e.g. pathogenic or black qi) and the absorption of good energy into the body. Through practice of this exercise, the practitioner aspires to cleanse and purify the body. The fourth exercise, "Falun Cosmic Orbit", seeks to circulate energy freely throughout the body. Unlike the first through fourth exercises, the fifth exercise is performed in the seated lotus position. Called "Reinforcing Supernatural Powers", it is a meditation intended to be maintained as long as possible.
Falun Gong exercises can be practiced individually or in group settings, and can be performed for varying lengths of time in accordance with the needs and abilities of the individual practitioner. Porter writes that practitioners of Falun Gong are encouraged to read Falun Gong books and practice its exercises on a regular basis, preferably daily. Falun Gong exercises are practiced in group settings in parks, university campuses, and other public spaces in over 70 countries worldwide, and are taught for free by volunteers. In addition to five exercises, in 2001 another meditation activity was introduced called "sending righteous thoughts," which is intended to reduce persecution on the spiritual plane.
A pilot study involving genomic profiling of six Falun Dafa practitioners indicated that, "changes in gene expression of [Falun Gong] practitioners in contrast to normal healthy controls were characterized by enhanced immunity, downregulation of cellular metabolism, and alteration of apoptotic genes in favor of a rapid resolution of inflammation."
Social practices.
Falun Gong differentiates itself from Buddhist monastic traditions in that it places great importance on participation in the secular world. Falun Gong practitioners are required to maintain regular jobs and family lives, to observe the laws of their respective governments, and are instructed not to distance themselves from society. An exception is made for Buddhist monks and nuns, who are permitted to continue a monastic lifestyle while practicing Falun Gong.
As part of its emphasis on ethical behavior, Falun Gong's teachings prescribe a strict personal morality for practitioners. They are expected to act truthfully, do good deeds, and conduct themselves with patience and forbearance when encountering difficulties. For instance, Li stipulates that a practitioner of Falun Gong must "not hit back when attacked, not talk back when insulted." In addition, they must "abandon negative thoughts and behaviors," such as greed, deception, jealousy, etc. The teachings contain injunctions against smoking and the consumption of alcohol, as these are considered addictions that are detrimental to health and mental clarity. Practitioners of Falun Gong are forbidden to kill living things—including animals for the purpose of obtaining food—though they are not required to adopt a vegetarian diet.
In addition to these things, practitioners of Falun Gong must abandon a variety of worldly attachments and desires. In the course of cultivation practice, the student of Falun Gong aims to relinquish the pursuit of fame, monetary gain, sentimentality, and other entanglements. Li's teachings repeatedly emphasize the emptiness of material pursuits; although practitioners of Falun Gong are not encouraged to leave their jobs or eschew money, they are expected to give up the psychological attachments to these things. Similarly, sexual desire and lust are treated as attachments to be discarded, but Falun Gong students are still generally expected to marry and have families. All sexual relations outside the confines of monogamous, heterosexual marriage are regarded as immoral. Although gays and lesbians may practice Falun Gong, homosexual conduct is said to generate karma, and is therefore viewed as incompatible with the goals of the practice.
Falun Gong's cosmology includes the belief that different ethnicities each have a correspondence to their own heavens, and that individuals of mixed race lose some aspect of this connection. Nonetheless, Li maintains that being of mixed race does not affect a person's soul, nor hinder their ability to practice cultivation. The practice does not have any formal stance against interracial marriage, and many Falun Gong practitioners have interracial children.
Falun Gong doctrine counsels against participation in political or social issues. Excessive interest in politics is viewed as an attachment to worldly power and influence, and Falun Gong aims for transcendence of such pursuits. According to Hu Ping, "Falun Gong deals only with purifying the individual through exercise, and does not touch on social or national concerns. It has not suggested or even intimated a model for social change. Many religions ... pursue social reform to some extent ... but there is no such tendency evident in Falun Gong."
Texts.
The first book of Falun Gong teachings was published in April 1993. Called "China Falun Gong", or simply "Falun Gong", it is an introductory text that discusses "qigong", Falun Gong's relationship to Buddhism, the principles of cultivation practice and the improvement of moral character (xinxing). The book also provides illustrations and explanations of the exercises and meditation.
The main body of teachings is articulated in the book "Zhuan Falun", published in Chinese in January 1995. The book is divided into nine "lectures", and was based on edited transcriptions of the talks Li gave throughout China in the preceding three years. Falun Gong texts have since been translated into an additional 40 languages. In addition to these central texts, Li has published several books, lectures, articles, books of poetry, which are made available on Falun Gong websites.
The Falun Gong teachings use numerous untranslated Chinese religious and philosophical terms, and make frequent allusion to characters and incidents in Chinese folk literature and concepts drawn from Chinese popular religion. This, coupled with the literal translation style of the texts, which imitate the colloquial style of Li's speeches, can make Falun Gong scriptures difficult to approach for Westerners.
Symbols.
The main symbol of the practice is the "Falun" (Dharma wheel, or "Dharmacakra" in Sanskrit). In Buddhism, the Dharmacakra represents the completeness of the doctrine. To "turn the wheel of dharma" ("Zhuan Falun") means to preach the Buddhist doctrine, and is the title of Falun Gong's main text. Despite the invocation of Buddhist language and symbols, the law wheel as understood in Falun Gong has distinct connotations, and is held to represent the universe. It is conceptualized by an emblem consisting of one large and four small Swastika symbols, representing the Buddha, and four small Taiji (yin-yang) symbols of the Daoist tradition.
Dharma-ending period.
Li situates his teaching of Falun Gong amidst the "Dharma-ending period" ("Mo Fa", 末法), described in Buddhist scriptures as an age of moral decline when the teachings of Buddhism would need to be rectified. The current era is described in Falun Gong's teachings as the "Fa rectification" period ("zhengfa", which might also be translated as "to correct the dharma"), a time of cosmic transition and renewal. The process of Fa rectification is necessitated by the moral decline and degeneration of life in the universe, and in the post-1999 context, the persecution of Falun Gong by the Chinese government has come to be viewed as a tangible symptom of this moral decay. Through the process of the Fa rectification, life will be reordered according to the moral and spiritual quality of each, with good people being saved and ascending to higher spiritual planes, and bad ones being eliminated or cast down. In this paradigm, Li assumes the role of rectifying the Dharma by disseminating through his moral teachings.
Some scholars, such as Maria Hsia Chang and Susan Palmer, have described Li's rhetoric about the "Fa rectification" and providing salvation "in the final period of the Last Havoc", as apocalyptic. However, Benjamin Penny argues that Li's teachings are better understood in the context of a "Buddhist notion of the cycle of the Dharma or the Buddhist law". Richard Gunde notes that unlike apocalyptic groups in the West, Falun Gong does not fixate on death or the end of the world, and instead "has a simple, innocuous ethical message". Li Hongzhi does not discuss a "time of reckoning", and has rejected predictions of an impending apocalypse in his teachings.
Categorization.
Falun Gong is a multifaceted discipline that means different things to different people, ranging from a set of physical exercises for the attainment of better health and a praxis of self-transformation, to a moral philosophy and a new knowledge system. Scholars and journalists have adopted a variety of terms and classifications in describing Falun Gong, some of them more precise than others.
In the cultural context of China, Falun Gong is generally described either as a system of qigong, or a type of "cultivation practice" ("xiulian"). Cultivation is a Chinese term that describes the process by which an individual seeks spiritual perfection, often through both physical and moral conditioning. Varieties of cultivation practice are found throughout Chinese history, spanning Buddhist, Daoist and Confucian traditions. Benjamin Penny, a professor of Chinese history at the Australian National University, writes "the best way to describe Falun Gong is as a cultivation system. Cultivation systems have been a feature of Chinese life for at least 2,500 years." Qigong practices can also be understood as a part of a broader tradition of "cultivation practice".
In the West, Falun Gong is frequently classified as a religion on the basis of its theological and moral teachings, its concerns with spiritual cultivation and transformation, and its extensive body of scripture. Human rights groups report on the persecution of Falun Gong as a violation of religious freedom, and in 2001, Falun Gong was given an International Religious Freedom Award from Freedom House. Falun Gong practitioners themselves have sometimes disavowed this classification, however. This rejection reflects the relatively narrow definition of "religion" ("zongjiao") in contemporary China. According to David Ownby, religion in China has been defined since 1912 to refer to "world-historical faiths" that have "well-developed institutions, clergy, and textual traditions"—namely, Buddhism, Daoism, Islam, Protestantism and Catholicism. Falun Gong lacks these features, having no temples, rituals of worship, clergy or formal hierarchy. Moreover, if Falun Gong had described itself as a religion in China, it likely would have invited immediate suppression. These historical and cultural circumstances notwithstanding, the practice has often been described as a form of Chinese religion.
Although it is often referred to as such in journalistic literature, Falun Gong does not satisfy the definition of a "sect." A sect is generally defined as a branch or denomination of an established belief system or mainstream church. Although Falun Gong draws on both Buddhist and Daoist ideas and terminology, it claims no direct relationship or lineage connection to these religions. Sociologists regard sects as exclusive groups that exist within clearly defined boundaries, with rigorous standards for admission and strict allegiances. However, as noted by Noah Porter, Falun Gong does not share these qualities: it does not have clearly defined boundaries, and anyone may practice it. Cheris Shun-ching Chan likewise writes that Falun Gong is "categorically not a sect": its practitioners do not sever ties with secular society, it is "loosely structured with a ﬂuctuating membership and tolerant of other organizations and faiths," and it is more concerned with personal, rather than collective worship.
Organization.
As a matter of doctrinal significance, Falun Gong is intended to be "formless," having little to no material or formal organization. Practitioners of Falun Gong cannot collect money or charge fees, conduct healings, or teach or interpret doctrine for others. There are no administrators or officials within the practice, no system of membership, and no churches or physical places of worship. In the absence of membership or initiation rituals, Falun Gong practitioners can be anyone who chooses to identify themselves as such. Students are free to participate in the practice and follow its teachings as much or as little as they like, and practitioners do not instruct others on what to believe or how to behave.
Spiritual authority is vested exclusively in the teachings of founder Li Hongzhi. But organizationally Falun Gong is decentralized, and local branches and assistants are afforded no special privileges, authority, or titles. Volunteer "assistants" or "contact persons" do not hold authority over other practitioners, regardless of how long they have practiced Falun Gong. Li's spiritual authority within the practice is absolute, yet the organization of Falun Gong works against totalistic control, and Li does not intervene in the personal lives of practitioners. Falun Gong practitioners of have little to no contact with Li, except through the study of his teachings. There is no hierarchy in Falun Gong to enforce orthodoxy, and little or no emphasis is given on dogmatic discipline; the only thing emphasized is the need for strict moral behavior, according to Craig Burgdoff, a professor of religious studies.
To the extent that organization is achieved in Falun Gong, it is accomplished through a global, networked, and largely virtual online community. In particular, electronic communications, email lists and a collection of websites are the primary means of coordinating activities and disseminating Li Hongzhi's teachings.
Outside Mainland China, a network of volunteer 'contact persons', regional Falun Dafa Associations and university clubs exist in approximately 70 countries. Li Hongzhi's teachings are principally spread through the Internet. In most mid- to large-sized cities, Falun Gong practitioners organize regular group meditation or study sessions in which they practice Falun Gong exercises and read Li Hongzhi's writings. The exercise and meditation sessions are described as informal groups of practitioners who gather in public parks—usually in the morning—for one to two hours. Group study sessions typically take place in the evenings in private residences or university or high school classrooms, and are described by David Ownby as "the closest thing to a regular 'congregational experience'" that Falun Gong offers. Individuals who are too busy, isolated, or who simply prefer solitude may elect to practice privately. When there are expenses to be covered (such as for the rental of facilities for large-scale conferences), costs are borne by self-nominated and relatively affluent individual members of the community.
Organization within China.
In 1993, the Beijing-based Falun Dafa Research Society was accepted as a branch of the state-run China Qigong Research Society (CQRS), which oversaw the administration of the country's various qigong schools, and sponsored activities and seminars. As per the requirements of the CQRS, Falun Gong was organized into a nationwide network of assistance centers, "main stations", "branches", "guidance stations", and local practice sites, mirroring the structure of the qigong society or even of the Communist Party itself. Falun Gong assistants were self-selecting volunteers who taught the exercises, organized events, and disseminated new writings from Li Hongzhi. The Falun Dafa Research Society provided advice to students on meditation techniques, translation services, and coordination for the practice nationwide.
Following its departure from the CQRS in 1996, Falun Gong came under increased scrutiny from authorities and responded by adopting a more decentralized and loose organizational structure. In 1997, the Falun Dafa Research Society was formally dissolved, along with the regional "main stations." Yet practitioners continued to organize themselves at local levels, being connected through electronic communications, interpersonal networks and group exercise sites. Both Falun Gong sources and Chinese government sources claimed that there were some 1,900 "guidance stations" and 28,263 local Falun Gong exercise sites nationwide by 1999, though they disagree over the extent of vertical coordination among these organizational units. In response to the persecution that began in 1999, Falun Gong was driven underground, the organizational structure grew yet more informal within China, and the internet took precedence as a means of connecting practitioners.
Following the persecution of Falun Gong in 1999, Chinese authorities sought to portray Falun Gong as a hierarchical and well-funded organization. James Tong writes that it was in the government's interest to portray Falun Gong as highly organized in order to justify its repression of the group: "The more organized the Falun Gong could be shown to be, then the more justified the regime's repression in the name of social order was." He concluded that Party's claims lacked "both internal and external substantiating evidence", and that despite the arrests and scrutiny, the authorities never "credibly countered Falun Gong rebuttals".
Demography.
Prior to July 1999, official estimates placed the number of Falun Gong practitioners at 70 million nationwide, rivaling membership in the Communist Party. By the time of the persecution on 22 July 1999, most Chinese government numbers said the population of Falun Gong was between 2 and 3 million, though some publications maintained an estimate of 40 million. Most Falun Gong estimates in the same period placed the total number of practitioners in China at 70 to 80 million. Other sources have estimated the Falun Gong population in China to have peaked between 10 and 70 million practitioners. The number of Falun Gong practitioners still practicing in China today is difficult to confirm, though some sources estimate that tens of millions continue to practice privately.
Demographic surveys conducted in China in 1998 found a population that was mostly female and elderly. Of 34,351 Falun Gong practitioners surveyed, 27% were male and 73% female. Only 38% were under 50 years old. Falun Gong attracted a range of other individuals, from young college students to bureaucrats, intellectuals and Party officials. Surveys in China from the 1990s found that between 23% - 40% of practitioners held university degrees at the college or graduate level—several times higher than the general population.
Falun Gong is practiced by tens, and possibly hundreds of thousands outside China, with the largest communities found in Taiwan and North American cities with large Chinese populations, such as New York and Toronto. Demographic surveys by Palmer and Ownby in these communities found that 90% of practitioners are ethnic Chinese. The average age was approximately 40. Among survey respondents, 56% were female and 44% male; 80% were married. The surveys found the respondents to be highly educated: 9% held PhDs, 34% had Master's degrees, and 24% had a Bachelor's degree.
The most commonly reported reasons for being attracted to Falun Gong were intellectual content, cultivation exercises, and health benefits. Non-Chinese Falun Gong practitioners tend to fit the profile of "spiritual seekers"—people who had tried a variety of qigong, yoga, or religious practices before finding Falun Gong. According to Richard Madsen, Chinese scientists with doctorates from prestigious American universities who practice Falun Gong claim that modern physics (for example, superstring theory) and biology (specifically the pineal gland's function) provide a scientific basis for their beliefs. From their point of view, "Falun Dafa is knowledge rather than religion, a new form of science rather than faith."
History inside China.
1992–1996.
Li Hongzhi introduced Falun Gong to the public in 13 May 1992, in Changchun, Jilin Province. Several months later, in September 1992, Falun Gong was admitted as a branch of qigong under the administration of the state-run China Qigong Scientific Research Society (CQRS). Li was recognized as a qigong master, and was authorized to teach his practice nationwide. Like many "qigong" masters at the time, Li toured major cities in China from 1992 to 1994 to teach the practice. He was granted a number of awards by PRC governmental organizations.
According to David Ownby, Professor of History and Director of the Center for East Asian Studies at the Université de Montréal, Li became an "instant star of the qigong movement", and Falun Gong was embraced by the government as an effective means of lowering health care costs, promoting Chinese culture, and improving public morality. In December 1992, for instance, Li and several Falun Gong students participated in the Asian Health Expo in Beijing, where he reportedly "received the most praise [of any qigong school] at the fair, and achieved very good therapeutic results," according to the fair's organizer The event helped cement Li's popularity, and journalistic reports of Falun Gong's healing powers spread. In 1993, a publication of the Ministry of Public Security praised Li for "promoting the traditional crime-fighting virtues of the Chinese people, in safeguarding social order and security, and in promoting rectitude in society."
Falun Gong had differentiated itself from other "qigong" groups in its emphasis on morality, low cost, and health benefits. It rapidly spread via word-of-mouth, attracting a wide range of practitioners from all walks of life, including numerous members of the Chinese Communist Party.
From 1992 to 1994, Li did charge fees for the lectures seminars he was giving across China, though the fees were considerably lower than those of competing qigong practices, and the local qigong associations received a substantial share. Li justified the fees as being necessary to cover travel costs and other expenses, and on some occasions, he donated the money earned to charitable causes. In 1994, Li ceased charging fees altogether, thereafter stipulating that Falun Gong must always be taught for free, and its teachings made available without charge (including online). Although some observers believe Li continued to earn substantial income through the sale of Falun Gong books, others dispute this, noting that most Falun Gong books in circulation were bootleg copies.
With the publication of the books "Falun Gong" and "Zhuan Falun", Li made his teachings more widely accessible. "Zhuan Falun", published in January 1995 at an unveiling ceremony held in the auditorium of the Ministry of Public Security, became a best-seller in China.
In 1995, Chinese authorities began looking to Falun Gong to solidify its organizational structure and ties to the party-state. Li was approached by the Chinese National Sports Committee, Ministry of Public Health, and China Qigong Science Research Association (CQRS) to jointly establish a Falun Gong association. Li declined the offer. The same year, the CQRS issued a new regulation mandating that all qigong denominations establish a Communist Party branch. Li again refused.
Tensions continued to mount between Li and the CQRS in 1996. In the face of Falun Gong's rise in popularity—a large part of which was attributed to its low cost—competing "qigong" masters accused Li of undercutting them. According to Schechter, the "qigong" society under which Li and other "qigong" masters belonged asked Li to hike his tuition, but Li emphasized the need for the teachings to be free of charge.
In March 1996, in response to mounting disagreements, Falun Gong withdrew from the CQRS, after which time it operated outside the official sanction of the state. Falun Gong representatives attempted to register with other government entities, but were rebuffed. Li and Falun Gong were then outside the circuit of personal relations and financial exchanges through which masters and their "qigong" organizations could find a place within the state system, and also the protections this afforded.
1996–1999.
Falun Gong's departure from the state-run CQRS corresponded to a wider shift in the government's attitudes towards qigong practices. As qigong's detractors in government grew more influential, authorities began attempting to rein in the growth and influence of these groups, some of which had amassed tens of millions of followers. In the mid-1990s the state-run media began publishing articles critical of qigong.
Falun Gong was initially shielded from the mounting criticism, but following its withdrawal from the CQRS in March 1996, it lost this protection. On 17 June 1996, the "Guangming Daily", an influential state-run newspaper, published a polemic against Falun Gong in which its central text, "Zhuan Falun", was described as an example of "feudal superstition." The author wrote that the history of humanity is a "struggle between science and superstition," and called on Chinese publishers not to print "pseudo-scientific books of the swindlers." The article was followed by at least twenty more in newspapers nationwide. Soon after, on 24 July, the Central Propaganda Department banned all publication of Falun Gong books (though the ban was not consistently enforced). The state-administered Buddhist Association of China also began issuing criticisms of Falun Gong, urging lay Buddhists not to take up the practice.
The events were an important challenge to Falun Gong, and one that practitioners did not take lightly. Thousands of Falun Gong followers wrote to "Guangming Daily" and to the CQRS to complain against the measures, claiming that they violated Hu Yaobang's 1982 'Triple No' directive which prohibited the media from either encouraging or criticizing qigong practices. In other instances, Falun Gong practitioners staged peaceful demonstrations outside media or local government offices to request retractions of perceived unfair coverage. Li Hongzhi made statements that practitioners' response to criticism showed their hearts and "would separate the false disciples from the true ones", also indicating that publicly defending the practice was a righteous act and an important aspect of Falun Gong cultivation.
The polemics against Falun Gong were part of a larger movement opposing qigong organizations in the state-run media. Although Falun Gong was not the only target of the media criticism, nor the only group to protest, theirs was the most mobilized and steadfast response. Many of Falun Gong's protests against negative media portrayals were successful, resulting in the retraction of several newspaper stories critical of the practice. This contributed to practitioners' belief that the media claims against them were false or exaggerated, and that their stance was justified.
In June 1998, He Zuoxiu, an outspoken critic of qigong and a fierce defender of Marxism, appeared on a talk show on Beijing Television and openly disparaged "qigong" groups, making particular mention of Falun Gong. Falun Gong practitioners responded with peaceful protests and by lobbying the station for a retraction. The reporter responsible for the program was reportedly fired, and a program favorable to Falun Gong was aired several days later. Falun Gong practitioners also mounted demonstrations at 14 other media outlets.
In 1997, The Ministry of Public Security launched an investigation into whether Falun Gong should be deemed xie jiao (邪教, "heretical teaching"). The report concluded that "no evidence has appeared thus far". The following year, however, on 21 July 1998, the Ministry of Public Security issued Document No. 555, "Notice of the Investigation of Falun Gong". The document asserted that Falun Gong is a "heretical teaching", and mandated that another investigation be launched to seek evidence in support of the conclusion. Falun Gong practitioners reported having phone lines tapped, homes ransacked and raided, and Falun Gong exercise sites disrupted by public security agents.
In this time period, even as criticism of qigong and Falun Gong mounted in some circles, the practice maintained a number of high-profile supporters in the government. In 1998, Qiao Shi, the recently retired Chairman of the Standing Committee of the National People's Congress, initiated his own investigation into Falun Gong. After months of investigations, his group concluded that "Falun Gong has hundreds of benefits for the Chinese people and China, and does not have one single bad effect." In May of the same year, China's National Sports Commission launched its own survey of Falun Gong. Based on interviews with over 12,000 Falun Gong practitioners in Guangdong province, they stated that they were "convinced the exercises and effects of Falun Gong are excellent. It has done an extraordinary amount to improve society's stability and ethics."
The practice's founder, Li Hongzhi, was largely absent from the country during the period of rising tensions with the government. In March 1995, Li had left China to first teach his practice in France and then other countries, and in 1998 obtained permanent residency in the United States.
By 1999, estimates provided by the State Sports Commission suggested there were 70 million Falun Gong practitioners in China. An anonymous employee of China's National Sports Commission, was at this time quoted in an interview with U.S. News & World Report as speculating that if 100 million had taken up Falun Gong and other forms of qigong there would be a dramatic reduction of health care costs and that "Premier Zhu Rongji is very happy about that."
Tianjin and Zhongnanhai protests.
By the late 1990s, the Communist Party's relationship to the growing Falun Gong movement had become increasingly tense. Reports of discrimination and surveillance by the Public Security Bureau were escalating, and Falun Gong practitioners were routinely organizing sit-in demonstrations responding to media articles they deemed to be unfair. The conflicting investigations launched by the Ministry of the Public Security on one side and the State Sports Commission and Qiao Shi on the other spoke of the disagreements among China's elites on how to regard the growing practice.
In April 1999, an article critical of Falun Gong was published in Tianjin Normal University's "Youth Reader" magazine. The article was authored by physicist He Zuoxiu who, as Porter and Gutmann note, is a relative of Politburo member and public security secretary Luo Gan. The article cast qigong, and Falun Gong in particular, as superstitious and harmful for youth. Falun Gong practitioners responded by picketing the offices of the newspaper requesting a retraction of the article. Unlike past instances in which Falun Gong protests were successful, on 22 April the Tianjin demonstration was broken up by the arrival of three hundred riot police. Some of the practitioners were beaten, and forty-five arrested. Other Falun Gong practitioners were told that if they wished to appeal further, they needed to take the issue up with the Ministry of Public Security and go to Beijing to appeal.
The Falun Gong community quickly mobilized a response, and on the morning of 25 April, upwards of 10,000 practitioners gathered near the central appeals office to demand an end to the escalating harassment against the movement, and request the release of the Tianjin practitioners. According to Benjamin Penny, practitioners sought redress from the leadership of the country by going to them and, "albeit very quietly and politely, making it clear that they would not be treated so shabbily." Journalist Ethan Gutmann wrote that security officers had been expecting them, and corralled the practitioners onto Fuyou Street in front of Zhongnanhai government compound. They sat or read quietly on the sidewalks surrounding the Zhongnanhai.
Five Falun Gong representatives met with Premier Zhu Rongji and other senior officials to negotiate a resolution. The Falun Gong representatives were assured that the regime supported physical exercises for health improvements and did not consider the Falun Gong to be anti-government. Upon reaching this resolution, the crowd of Falun Gong protesters dispersed.
Party General Secretary Jiang Zemin was alerted to the demonstration by CPC Politburo member Luo Gan, and was reportedly angered by the audacity of the demonstration—the largest since the Tiananmen Square protests ten years earlier. Jiang called for resolute action to suppress the group, and reportedly criticized Premier Zhu for being "too soft" in his handling of the situation. That evening, Jiang composed a letter indicating his desire to see Falun Gong "defeated". In the letter, Jiang expressed concerns over the size and popularity of Falun Gong, and in particular about the large number of senior Communist Party members found among Falun Gong practitioners. He also intimated that Falun Gong's moral philosophy was at odds with the atheist values of Marxist–Leninism, and therefore constituted a form of ideological competition.
Jiang is held by Falun Gong to be personally responsible for this decision to persecute Falun Gong. Peerman cited reasons such as suspected personal jealousy of Li Hongzhi; Saich points to Jiang's anger at Falun Gong's widespread appeal, and ideological struggle as causes for the crackdown that followed. Willy Wo-Lap Lam suggests Jiang's decision to suppress Falun Gong was related to a desire to consolidate his power within the Politburo. According to Human Rights Watch, Communist Party leaders and ruling elite were far from unified in their support for the crackdown.
Persecution.
On 20 July 1999, security forces abducted and detained thousands of Falun Gong practitioners that they identified as leaders. Two days later, on 22 July, the PRC Ministry of Civil Affairs outlawed the Falun Dafa Research Society as an illegal organization "engaged in illegal activities, advocating superstition and spreading fallacies, hoodwinking people, inciting and creating disturbances, and jeopardizing social stability". The same day, the Ministry of Public Security issued a circular forbidding citizens from practicing Falun Gong in groups, possessing Falun Gong's teachings, displaying Falun Gong banners or symbols, or protesting the ban.
The ensuing campaign aimed to "eradicate" the group through a combination of propaganda, imprisonment, and coercive thought reform of practitioners, sometimes resulting in deaths. In October 1999, four months after the ban, legislation was created to outlaw "heterodox religions" and sentence Falun Gong devotees to prison terms.
Hundreds of thousands are estimated to have been imprisoned extrajudicially, and practitioners in detention are reportedly subjected to forced labor, psychiatric abuse, torture, and other coercive methods of thought reform at the hands of Chinese authorities. The U.S. Department of State and Congressional-Executive Commission on China cite estimates that as much as half of China's reeducation-through-labor camp population is made up of Falun Gong practitioners. Researcher Ethan Gutmann estimates that Falun Gong represents an average of 15 to 20 percent of the total "laogai" population, which includes reeducation through labor camps as well as prisons and other forms of administrative detention. Former detainees of the labor camp system have reported that Falun Gong practitioners are one of the largest groups of prisoners; in some labor camp and prison facilities, they comprise the majority of detainees, and are often said to receive the longest sentences and the worst treatment. A 2013 report by Amnesty International on labor reeducation camps found that, Falun Gong practitioners "constituted on average from one third to in some cases 100 per cent of the total population" of certain camps.
According to Johnson, the campaign against Falun Gong extends to many aspects of society, including the media apparatus, police force, military, education system, and workplaces. An extra-constitutional body, the "610 Office" was created to "oversee" the effort. Human Rights Watch (2002) noted that families and workplaces were urged to cooperate with the government.
In February 2001, the Communist Party convened a rare Central Work Conference to stress the importance of continuity in the anti-Falun Gong campaign and unite senior party officials behind the effort. Under Jiang's leadership, the crackdown on Falun Gong became part of the Chinese political ethos of "upholding stability" – much the same rhetoric employed by the party during Tiananmen in 1989. Jiang's message was echoed at the 2001 National People's Congress, where Premier Zhu Rongji made special mention of Falun Gong in his outline of the PRC Tenth Five-Year Plan, saying "we must continue our campaign against the Falun Gong cult", effectively tying Falun Gong's eradication to China's economic progress. Though less prominent on the national agenda, the persecution of Falun Gong has carried on during the tenure of Hu Jintao; successive, high-level "strike hard" campaigns against Falun Gong were initiated in both 2008 and 2009. In 2010, a three-year campaign was launched to renew attempts at the coercive "transformation" of Falun Gong practitioners.
Speculation on rationale.
Foreign observers have attempted to explain the Party's rationale for banning Falun Gong as stemming from a variety of factors. These include Falun Gong's popularity, China's history of quasi-religious movements which turned into violent insurrections, its independence from the state and refusal to toe the party line, internal power politics within the Communist Party, and Falun Gong's moral and spiritual content, which put it at odds with the atheist aspects of the official Marxist ideology.
Xinhua News Agency, the official news organization of the Communist Party, declared that Falun Gong "opposed to the Communist Party of China and the central government, preaches idealism, theism and feudal superstition." Xinhua also asserted that "the so-called 'truth, kindness and forbearance' principle preached by [Falun Gong] has nothing in common with the socialist ethical and cultural progress we are striving to achieve", and argued that it was necessary to crush Falun Gong in order to preserve the "vanguard role and purity" of the Communist Party. Other articles appearing in the state-run media in the first days and weeks of the ban posited that Falun Gong must be defeated because its "theistic" philosophy was at odds with the Marxist–Leninism paradigm and with the secular values of materialism.
Willy Wo-Lap Lam writes that Jiang Zemin's campaign against Falun Gong may have been used to promote allegiance to himself; Lam quotes one party veteran as saying "by unleashing a Mao-style movement [against Falun Gong], Jiang is forcing senior cadres to pledge allegiance to his line." "The Washington Post" reported that sources indicated not all of the standing committee of the Politburo shared Jiang's view that Falun Gong should be eradicated, but James Tong suggests there was not substantial resistance from the Politburo.
Human Rights Watch notes that the crackdown on Falun Gong reflects historical efforts by the Chinese Communist Party to eradicate religion, which the government believes is inherently subversive. The Chinese government protects five "patriotic", Communist Party-sanctioned religious groups. Unregistered religions that fall outside the state-sanctioned organizations are thus vulnerable to suppression. "The Globe and Mail" wrote : "... any group that does not come under the control of the Party is a threat". Craig S. Smith of "The Wall Street Journal" wrote that the party feels increasingly threatened by any belief system that challenges its ideology and has an ability to organize itself. That Falun Gong, whose belief system represented a revival of traditional Chinese religion, was being practiced by a large number of Communist Party members and members of the military was seen as particularly disturbing to Jiang Zemin; according to Julia Ching, "Jiang accepts the threat of Falun Gong as an ideological one: spiritual beliefs against militant atheism and historical materialism. He [wished] to purge the government and the military of such beliefs."
Yuezhi Zhao points to several other factors that may have led to a deterioration of the relationship between Falun Gong and the Chinese state and media. These included infighting within China's qigong establishment, the influence of qigong opponents among Communist Party leaders, and the struggles from mid-1996 to mid-1999 between Falun Gong and the Chinese power elite over the status and treatment of the movement. According to Zhao, Falun Gong practitioners have established a "resistance identity"—one that stands against prevailing pursuits of wealth, power, scientific rationality, and "the entire value system associated with China's project of modernization." In China the practice represented an indigenous spiritual and moral tradition, a cultural revitalization movement, and drew a sharp contrast to "Marxism with Chinese characteristics".
Vivienne Shue similarly writes that Falun Gong presented a comprehensive challenge to the Communist Party's legitimacy. Shue argues that Chinese rulers historically have derived their legitimacy from a claim to possess an exclusive connection to the "Truth". In imperial China, truth was based on a Confucian and Daoist cosmology, where in the case of the Communist Party, the truth is represented by Marxist–Leninism and historical materialism. Falun Gong challenged the Marxist–Leninism paradigm, reviving an understanding based on more traditionally Buddhist or Daoist conceptions. David Ownby contends that Falun Gong also challenged the Communist Party's hegemony over Chinese nationalist discourse: "[Falun Gong's] evocation of a different vision of Chinese tradition and its contemporary value is now so threatening to the state and party because it denies them the sole right to define the meaning of Chinese nationalism, and perhaps of Chineseness."
Maria Chang noted that since the overthrow of the Qin Dynasty, "millenarian movements had exerted a profound impact on the course of Chinese history", cumulating in the Chinese Revolutions of 1949 which brought the Chinese Communists to power. Patsy Rahn (2002) describes a paradigm of conflict between Chinese sectarian groups and the rulers they often challenge. According to Rahn, the history of this paradigm goes back to the collapse of the Han dynasty: "The pattern of ruling power keeping a watchful eye on sectarian groups, at times threatened by them, at times raising campaigns against them, began as early as the second century and continued throughout the dynastic period, through the Mao era and into the present."
Conversion program.
According to James Tong, the regime aimed at both coercive dissolution of the Falun Gong denomination and "transformation" of the practitioners. By 2000, the Party upped its campaign by sentencing "recidivist" practitioners to "re-education through labor", in an effort to have them renounce their beliefs and "transform" their thoughts. Terms were also arbitrarily extended by police, while some practitioners had ambiguous charges levied against them, such as "disrupting social order", "endangering national security", or "subverting the socialist system". According to Bejesky, the majority of long-term Falun Gong detainees are processed administratively through this system instead of the criminal justice system. Upon completion of their re-education sentences, those practitioners who refused to recant were then incarcerated in "legal education centers" set up by provincial authorities to "transform minds".
Much of the conversion program relied on Mao-style techniques of indoctrination and thought reform, where Falun Gong practitioners were organized to view anti-Falun Gong television programs and enroll in Marxism and materialism study sessions. Traditional Marxism and materialism were the core content of the sessions.
The government-sponsored image of the conversion process emphasizes psychological persuasion and a variety of "soft-sell" techniques; this is the "ideal norm" in regime reports, according to Tong. Falun Gong reports, on the other hand, depict "disturbing and sinister" forms of coercion against practitioners who fail to renounce their beliefs. 14,474 cases are classified by different methods of torture, according to Tong (Falun Gong agencies document over 63,000 individual cases of torture). Among them are cases of severe beatings; psychological torment, corporal punishment and forced intense, heavy-burden hard labor and stress positions; solitary confinement in squalid conditions; "heat treatment" including burning and freezing; electric shocks delivered to sensitive parts of the body that may result in nausea, convulsions, or fainting; "devastative" forced feeding; sticking bamboo strips into fingernails; deprivation of food, sleep, and use of toilet; rape and gang rape; asphyxiation; and threat, extortion, and termination of employment and student status.
The cases appear verifiable, and the great majority identify (1) the individual practitioner, often with age, occupation, and residence; (2) the time and location that the alleged abuse took place, down to the level of the district, township, village, and often the specific jail institution; and (3) the names and ranks of the alleged perpetrators. Many such reports include lists of the names of witnesses and descriptions of injuries, Tong says. The publication of "persistent abusive, often brutal behavior by named individuals with their official title, place, and time of torture" suggests that there is no official will to cease and desist such activities.
Deaths.
Due to the difficulty in corroborating reports of torture deaths in China, estimates on the number of Falun Gong practitioners killed under persecution vary widely. In 2009, the New York Times reported that, according to human rights groups, the repressions had claimed "at least 2,000" lives.
Amnesty International said at least 100 Falun Gong practitioners had reportedly died in the 2008 calendar year, either in custody or shortly after their release. Falun Gong sources have documented over 3,700 deaths. Investigative journalist Ethan Gutmann estimated 65,000 Falun Gong were killed for their organs from 2000 to 2008 based on extensive interviews, while researchers David Kilgour and David Matas reported, "the source of 41,500 transplants for the six year period 2000 to 2005 is unexplained".
Chinese authorities do not publish statistics on Falun Gong practitioners killed amidst the crackdown. In individual cases, however, authorities have denied that deaths in custody were due to torture.
Organ harvesting.
In 2006, allegations emerged that a large number of Falun Gong practitioners had been killed to supply China's organ transplant industry. These allegations prompted an investigation by former Canadian Secretary of State David Kilgour and human rights lawyer David Matas.
The Kilgour-Matas report was published in July 2006, and concluded that "the government of China and its agencies in numerous parts of the country, in particular hospitals but also detention centres and 'people's courts', since 1999 have put to death a large but unknown number of Falun Gong prisoners of conscience." The report, which was based mainly on circumstantial evidence, called attention to the extremely short wait times for organs in China—one to two weeks for a liver compared with 32.5 months in Canada—noting that this was indicative of organs being procured on demand. It also tracked a significant increase in the number of annual organ transplants in China beginning in 1999, corresponding with the onset of the persecution of Falun Gong. Despite very low levels of voluntary organ donation, China performs the second-highest number of transplants per year. Kilgour and Matas also presented self-accusatory material from Chinese transplant center web sites advertising the immediate availability of organs from living donors, and transcripts of interviews in which hospitals told prospective transplant recipients that they could obtain Falun Gong organs.
In 2014, investigative journalist Ethan Gutmann published the result of his own investigation. Gutmann conducted extensive interviews around with former detainees in Chinese labor camps and prisons, as well as former security officers and medical professionals with knowledge of China's transplant practices. He reported that organ harvesting from political prisoners likely began in Xinjiang province in the 1990s, and then spread nationwide. Gutmann estimates that some 64,000 Falun Gong prisoners may have been killed for their organs between the years 2000 and 2008.
In May 2008 two United Nations Special Rapporteurs reiterated requests for the Chinese authorities to respond to the allegations, and to explain a source for the organs that would account for the sudden increase in organ transplants in China since 2000. Chinese officials have responded by denying the organ harvesting allegations, and insisting that China abides by World Health Organization principles that prohibit the sale of human organs without written consent from donors. Responding to a U.S. House of Representatives Resolution calling for an end to abusing transplant practices against religious and ethnic minorities, a Chinese embassy spokesperson said "the so-called organ harvesting from death-row prisoners is totally a lie fabricated by Falun Gong." In August 2009, Manfred Nowak the United Nations Special Rapporteur on Torture said, "The Chinese government has yet to come clean and be transparent ... It remains to be seen how it could be possible that organ transplant surgeries in Chinese hospitals have risen massively since 1999, while there are never that many voluntary donors available."
Media campaign.
The Chinese government's campaign against Falun Gong was driven by large-scale propaganda through television, newspapers, radio and internet. 
Within the first month of the crackdown, 300–400 articles attacking Falun Gong appeared in each of the main state-run papers, while primetime television replayed alleged exposés on the group, with no divergent views aired in the media. The propaganda campaign focused on allegations that Falun Gong jeopardized social stability, was deceiving and dangerous, was "anti-science" and threatened progress, and argued that Falun Gong's moral philosophy was incompatible with a Marxist social ethic.
China scholars Daniel Wright and Joseph Fewsmith asserted that for several months after Falun Gong was outlawed, China Central Television's evening news contained little but anti-Falun Gong rhetoric; the government operation was "a study in all-out demonization", they wrote. Falun Gong was compared to "a rat crossing the street that everyone shouts out to squash" by "Beijing Daily"; other officials said it would be a "long-term, complex and serious" struggle to "eradicate" Falun Gong.
State propaganda initially used the appeal of scientific rationalism to argue that Falun Gong's worldview was in "complete opposition to science" and communism. For example, the "People's Daily" asserted on 27 July 1999, that the fight against Falun Gong "was a struggle between theism and atheism, superstition and science, idealism and materialism." Other editorials declared that Falun Gong's "idealism and theism" are "absolutely contradictory to the
fundamental theories and principles of Marxism," and that the "'truth, kindness and forbearance' principle preached by [Falun Gong] has nothing in common with the socialist ethical and cultural progress we are striving to achieve." Suppressing Falun Gong was presented as a necessary step to maintaining the "vanguard role" of the Communist Party in Chinese society.
Despite Party efforts, initial charges leveled against Falun Gong failed to elicit widespread popular support for the persecution of the group. In the months following July 1999, the rhetoric in the state-run press escalated to include charges that Falun Gong was colluding with foreign, "anti-China" forces. In October 1999, three months after the persecution began, the Supreme People's Court issued a judicial interpretation officially classifying Falun Gong as a "xiejiao". A direct translation of that term is "heretical teaching", but during the anti-Falun Gong propaganda campaign was rendered as "evil cult" in English. In the context of imperial China, the term "xiejiao" was used to refer to non-Confucian religions, though in the context of Communist China, it has been used to target religious organizations which do not submit to the authority of the Communist Party.
Ian Johnson argued that, by applying the 'cult' label to Falun Gong effectively "cloaked the government's crackdown with the legitimacy of the West's anticult movement." He notes, however, that Falun Gong does not satisfy common definitions of a cult: "its members marry outside the group, have outside friends, hold normal jobs, do not live isolated from society, do not believe that the world's end is imminent and do not give significant amounts of money to the organisation ... it does not advocate violence and is at heart an apolitical, inward-oriented discipline, one aimed at cleansing oneself spiritually and improving one's health." David Ownby similarly wrote that "the entire issue of the supposed cultic nature of Falun Gong was a red herring from the beginning, cleverly exploited by the Chinese state to blunt the appeal of Falun Gong.". According to John Powers and Meg Y. M. Lee, because the Falun Gong was categorized in the popular perception as an "apolitical, qigong exercise club," it was not seen as a threat to the government. The most critical strategy in the Falun Gong suppression campaign, therefore, was to convince people to reclassify the Falun Gong into a number of "negatively charged religious labels", like "evil cult", "sect", or "superstition". The group's silent protests were reclassified as creating "social disturbances". In this process of relabelling, the government was attempting to tap into a "deep reservoir of negative feelings related to the historical role of quasi-religious cults as a destabilising force in Chinese political history."
A turning point in the propaganda campaign came on the eve of Chinese New Year on 23 January 2001, when five people attempted to set themselves ablaze on Tiananmen Square. The official Chinese press agency, Xinhua News Agency, and other state media asserted that the self-immolators were practitioners, though the Falun Dafa Information Center disputed this, on the grounds that the movement's teachings explicitly forbid suicide and killing, further alleging that the event was "a cruel (but clever) piece of stunt-work." The incident received international news coverage, and video footage of the burnings were broadcast later inside China by China Central Television (CCTV). Images of a 12-year-old girl, Liu Siying, burning and interviews with the other participants in which they stated their belief that self-immolation would lead them to paradise were shown. But one of the CNN producers on the scene did not even see a child there. Falun Gong sources and other commentators pointed out that the main participants' account of the incident and other aspects of the participants' behavior were inconsistent with the teachings of Falun Dafa. Media Channel and the International Education Development (IED) agree that the supposed self-immolation incident was staged by CCP to "prove" that Falun Gong brainwashes its followers to commit suicide and has therefore to be banned as a threat to the nation. IED's statement at the 53rd UN session describes China's violent assault on Falun Gong practitioners as state terrorism and that the self-immolation "was staged by the government." "Washington Post" journalist Phillip Pan wrote that the two self-immolators who died were not actually Falun Gong practitioners. On March 21, 2001, Liu Siying suddenly died after appearing very lively and being deemed ready to leave the hospital to go home. "Time" reported that prior to the self-immolation incident, many Chinese had felt that Falun Gong posed no real threat, and that the state's crackdown had gone too far. After the event, however, the mainland Chinese media campaign against Falun Gong gained significant traction. As public sympathy for Falun Gong declined, the government began sanctioning "systematic use of violence" against the group.
In education system.
Anti-Falun Gong propaganda efforts have also permeated the Chinese education system. Following Jiang Zemin's 1999 ban of Falun Gong, then-Minister of Education Chen Zhili launched an active campaign to promote the Party's line on Falun Gong within all levels of academic institutions, including graduate schools, universities and colleges, middle schools, primary schools, and kindergartens. Her efforts included a "Cultural Revolution-like pledge" in Chinese schools that required faculty members, staff, and students to publicly denounce Falun Gong. Teachers who did not comply with Chen's program were dismissed or detained; uncooperative students were refused academic advancement, expelled from school, or sent to "transformation" camps to alter their thinking. Chen also worked to spread the anti-Falun Gong academic propaganda movement overseas, using domestic educational funding to donate aid to foreign institutions, encouraging them to oppose Falun Gong.
Falun Gong's response to the persecution.
Falun Gong's response to the persecution in China began in July 1999 with appeals to local, provincial, and central petitioning offices in Beijing. It soon progressed to larger demonstrations, with hundreds of Falun Gong practitioners traveling daily to Tiananmen Square to perform Falun Gong exercises or raise banners in defense of the practice. These demonstrations were invariably broken up by security forces, and the practitioners involved were arrested—sometimes violently—and detained. By 25 April 2000, a total of more than 30,000 practitioners had been arrested on the square; seven hundred Falun Gong followers were arrested during a demonstration in the square on 1 January 2001. Public protests continued well into 2001. Writing for the "Wall Street Journal", Ian Johnson wrote that "Falun Gong faithful have mustered what is arguably the most sustained challenge to authority in 50 years of Communist rule."
By late 2001, demonstrations in Tiananmen Square had become less frequent, and the practice was driven deeper underground. As public protest fell out of favor, practitioners established underground "material sites," which would produce literature and DVDs to counter the portrayal of Falun Gong in the official media. Practitioners then distribute these materials, often door-to-door. Falun Gong sources estimated in 2009 that over 200,000 such sites exist across China today. The production, possession, or distribution of these materials is frequently grounds for security agents to incarcerate or sentence Falun Gong practitioners.
In 2002, Falun Gong activists in China tapped into television broadcasts, replacing regular state-run programming with their own content. One of the more notable instances occurred in March 2002, when Falun Gong practitioners in Changchun intercepted eight cable television networks in Jilin Province, and for nearly an hour, televised a program titled "Self-Immolation or a Staged Act?". All six of the Falun Gong practitioners involved were captured over the next few months. Two were killed immediately, while the other four were all dead by 2010 as a result of injuries sustained while imprisoned.
Outside China, Falun Gong practitioners established international media organizations to gain wider exposure for their cause and challenge narratives of the Chinese state-run media. These include The Epoch Times newspaper, New Tang Dynasty Television, and Sound of Hope radio station. According to Zhao, through "The Epoch Times" it can be discerned how Falun Gong is building a "de facto media alliance" with China's democracy movements in exile, as demonstrated by its frequent printing of articles by prominent overseas Chinese critics of the PRC government. In 2004, "The Epoch Times" published a collection of nine editorials which presented a critical history of Communist Party rule. This catalyzed the Tuidang movement, which encourages Chinese citizens to renounce their affiliations to the Chinese Communist Party, including ex post facto renunciations of the Communist Youth League and Young Pioneers. "The Epoch Times" claims that tens of millions have renounced the Communist Party as part of the movement, though these numbers have not been independently verified.
In 2007, Falun Gong practitioners in the United States formed Shen Yun Performing Arts, a dance and music company that tours internationally. Falun Gong software developers in the United States are also responsible for the creation of several popular censorship-circumvention tools employed by internet users in China.
Falun Gong Practitioners outside China have filed dozens of lawsuits against Jiang Zemin, Luo Gan, Bo Xilai, and other Chinese officials alleging genocide and crimes against humanity. According to "International Advocates for Justice", Falun Gong has filed the largest number of human rights lawsuits in the 21st century and the charges are among the most severe international crimes defined by international criminal laws. as of 2006, 54 civil and criminal lawsuits were under way in 33 countries. In many instances, courts have refused to adjudicate the cases on the grounds of sovereign immunity. In late 2009, however, separate courts in Spain and Argentina indicted Jiang Zemin and Luo Gan on charges of "crimes of humanity" and genocide, and asked for their arrest—the ruling is acknowledged to be largely symbolic and unlikely to be carried out. The court in Spain also indicted Bo Xilai, Jia Qinglin and Wu Guanzheng.
Falun Gong practitioners and their supporters also filed a lawsuit in May 2011 against the technology company Cisco Systems, alleging that the company helped design and implement a surveillance system for the Chinese government to suppress Falun Gong. Cisco denied customizing their technology for this purpose.
Falun Gong outside China.
Li Hongzhi began teaching Falun Gong internationally in March 1995. His first stop was in Paris where, at the invitation of the Chinese ambassador, he held a lecture seminar at the PRC embassy. This was followed by lectures in Sweden in May 1995. Between 1995 and 1999, Li gave lectures in the United States, Canada, Australia, New Zealand, Germany, Switzerland, and Singapore.
Falun Gong's growth outside China largely corresponded to the migration of students from Mainland China to the West in the early- to mid-1990s. Falun Gong associations and clubs began appearing in Europe, North America and Australia, with activities centered mainly on university campuses. Falun Gong volunteer instructors and Falun Dafa Associations are currently found in over 70 countries outside China.
Translations of Falun Gong teachings began appearing in the late 1990s. As the practice began proliferating outside China, Li Hongzhi was beginning to receive recognition in the United States and elsewhere in the western world. In May 1999, Li was welcomed to Toronto with greetings from the mayor and the provincial governor general, and in the two months that followed also received recognition from the cities of Chicago and San Jose.
Although the practice was beginning to attract an overseas constituency in the 1990s, it remained relatively unknown outside China until the Spring of 1999, when tensions between Falun Gong and Communist Party authorities became a subject of international media coverage. With the increased attention, the practice gained a greater following outside China. Following the launch of the Communist Party's suppression campaign against Falun Gong, the overseas presence became vital to the practice's resistance in China and its continued survival. Falun Gong practitioners overseas have responded to the persecution in China through regular demonstrations, parades, and through the creation of media outlets, performing arts companies, and censorship-circumvention software mainly intended to reach Mainland Chinese audiences.
International reception.
Since 1999, numerous Western governments and human rights organizations have expressed condemnation for the Chinese government's suppression of Falun Gong. Since 1999, members of the United States Congress have made public pronouncements and introduced several resolutions in support of Falun Gong. In 2010, U.S. House of Representatives Resolution 605 called for "an immediate end to the campaign to persecute, intimidate, imprison, and torture Falun Gong practitioners," condemned the Chinese authorities' efforts to distribute "false propaganda" about the practice worldwide, and expressed sympathy to persecuted Falun Gong practitioners and their families.
From 1999 to 2001, Western media reports on Falun Gong—and in particular, the mistreatment of practitioners—were frequent, if mixed. By the latter half of 2001, however, the volume of media reports declined precipitously, and by 2002, major news organizations like the "New York Times" and "Washington Post" had almost completely ceased their coverage of Falun Gong from China. In a study of media discourse on Falun Gong, researcher Leeshai Lemish found that Western news organizations also became less balanced, and more likely to uncritically present the narratives of the Communist Party, rather than those of Falun Gong or human rights groups. Adam Frank writes that in reporting on the Falun Gong, the Western tradition of casting the Chinese as "exotic" took dominance, and that while the facts were generally correct in Western media coverage, "the normalcy that millions of Chinese practitioners associated with the practice had all but disappeared." David Ownby noted that alongside these tactics, the "cult" label applied to Falun Gong by the Chinese authorities never entirely went away in the minds of some Westerners, and the stigma still plays a role in wary public perceptions of Falun Gong.
To counter the support of Falun Gong in the West, the Chinese government expanded their efforts against the group internationally. This included visits to newspaper officers by diplomats to "extol the virtues of Communist China and the evils of Falun Gong", linking support for Falun Gong with "jeopardizing trade relations," and sending letters to local politicians telling them to withdraw support for the practice. According to Perry Link, pressure on Western institutions also takes more subtle forms, including academic self-censorship, whereby research on Falun Gong could result in a denial of visa for fieldwork in China; or exclusion and discrimination from business and community groups who have connections with China and fear angering the Communist Party.
Although the persecution of Falun Gong has drawn considerable condemnation outside China, some observers note that Falun Gong has failed to attract the level of sympathy and sustained attention afforded to other Chinese dissident groups. Katrina Lantos Swett, vice chair of the United States Commission on International Religious Freedom, notes that most Americans are aware of the suppression of "Tibetan Buddhists and unregistered Christian groups or pro-democracy and free speech advocates such as Liu Xiaobo and Ai Weiwei," and yet "know little to nothing about China’s assault on the Falun Gong."
Ethan Gutmann, a journalist reporting on China since the early 1990s, has attempted to explain this apparent dearth of public sympathy for Falun Gong as stemming, in part, from the group's shortcomings in public relations. Unlike the democracy activists or Tibetans, who have found a comfortable place in Western perceptions, "Falun Gong marched to a distinctly Chinese drum", Gutmann writes. Moreover, practitioners' attempts at getting their message across carried some of the uncouthness of Communist party culture, including a perception that practitioners tended to exaggerate, create "torture tableaux straight out of a Cultural Revolution opera", or "spout slogans rather than facts". This is coupled with a general doubtfulness in the West of persecuted refugees. Gutmann also notes that media organizations and human rights groups also self-censor on the topic, given the PRC governments vehement attitude toward the practice, and the potential repercussions that may follow for making overt representations on Falun Gong's behalf.
Richard Madsen writes that Falun Gong lacks robust backing from the American constituencies that usually support religious freedom. For instance, Falun Gong's conservative moral beliefs have alienated some liberal constituencies in the West (e.g. its teachings against promiscuity and homosexual behavior). Christian conservatives, by contrast, don't accord the practice the same space as persecuted Chinese Christians. Madsen charges that the American political center does not want to push the human rights issue so hard that it would disrupt commercial and political relations with China. Thus, Falun Gong practitioners have largely had to rely on their own resources in responding to suppression.

</doc>
<doc id="55175" url="http://en.wikipedia.org/wiki?curid=55175" title="Biotope">
Biotope

Biotope is an area of uniform environmental conditions providing a living place for a specific assemblage of plants and animals. Biotope is almost synonymous with the term habitat, which is more commonly used in English-speaking countries. However, in some countries these two terms are distinguished: the subject of a habitat is a species or a population, the subject of a biotope is a biological community. 
It is an English loanword derived from the German "'", which in turn came from the Greek bios='life' or 'organism' and topos='place'. (The related word "geotope" has made its way into the English language by the same route, from the German "'".) The word biotope, literally translated, means an area where life lives.
Ecology.
The concept of a biotope was first advocated by Ernst Haeckel (1834-1919), a German zoologist famous for the recapitulation theory. In his book "General Morphology" (1866), which defines the term "ecology", he stresses the importance of the concept of habitat as a prerequisite for an organism's existence. Heackel also explains that with one ecosystem, its biota is shaped by environmental factors (such as water, soil, and geographical features) and interaction among living things; the original idea of a biotope was closely related to evolutional theory. Following this, F. Dahl, a professor at the Berlin Zoological Museum, referred to this ecological system as a "biotope" (biotop) (1908).
Biotope restoration.
Although the term "biotope" is considered to be a technical word with respect to ecology, in recent years the term is more generally used in administrative and civic activities. Since the 1970s the term "biotope" has received great attention as a keyword throughout Europe (mainly Germany) for the preservation, regeneration, and creation of natural environmental settings. Used in this context, the term "biotope" often refers to a smaller and more specific ecology and is very familiar to human life. In Germany especially, activities related to regenerating biotopes are enthusiastically received. These activities include —
Various sectors play a part in these activities, including architecture, civil engineering, urban planning, traffic, agriculture, river engineering, limnology, biology, education, landscape gardening, and domestic gardening. In all fields, all sorts of people are seeking a viable way for humans to respect other living things. The term "biotope" would include a complete environmental approach.
Characteristics.
The following four points are the chief characteristics of biotopes.
Microscale.
A biotope is generally not considered to be a large-scale phenomenon. For example, a biotope might be a neighbouring park, a back garden, even potted plants or a fish tank on a porch. In other words, the biotope is not a macroscopic but a microscopic approach to preserving the ecosystem and biological diversity. So biotopes fit into ordinary people's daily activities and lives, with more people being able to take part in biotope creation and continuing management.
Biotope networks.
It is commonly emphasised that biotopes should not be isolated. Instead biotopes need to be connected to each other and other surrounding life for without these connections to life-forms such as animals and plants, biotopes would not effectively work as a place in which diverse organisms live. So one of the most effective strategies for regenerating biotopes is to plan a "stretch" of biotopes, not just a "point" where animals and plants come and go. (Such an organic traffic course is called a corridor.) In the stretch method, the centre of the network would be large green tracts of land: a forest, natural park, or cemetery. By connecting parcels of land with smaller biotope areas such as a green belt along the river, small town parks, gardens, or even roadside trees, biotopes can exist in a network. In other words, a biotope is an open not a closed system and is a practicable strategy.
Human daily life.
The term "biotope" does not apply to biosphere reserves, which are completely separate from humans and become the object of human admiration. Instead, it is an active part of human daily life. For example, an ornamental flower bed may be considered a biotope (albeit rather a small one) since it enhances the experience of daily life. An area that has many functions, such as human living space, and is home to other living things, whether plant or animal, can be considered a biosphere reserve.
Artificial.
When artificial items are introduced to a biotope setting, their design and arrangement is of great importance for biotope regeneration. Tree-planting areas where the surface is uneven results in plants that sprout and the nesting of small insects. A mat or net made from natural fibres will gradually biodegrade as it is exposed to the weather. So there is no binomial opposition between the natural and the artificial in a biotope. Rather, such artificial materials are widely used.
Germany.
It is especially characteristic in Germany, which is the birthplace of the term biotope, that the authorities take the initiative in conserving biotopes, maintaining consistency with urban or rural planning and considering the regions' history and landscape.
Legal basis.
The federal nature protection law (Bundesnaturschutzgesetz, since 1976) requires that wild animals and plants and their community should be protected as part of the ecosystem in the specific diversity that has grown naturally and historically, and their biotope and other living conditions should be protected, preserved, developed, and restored. (Number 9, Clause 1, Article 2). The law also requires that some kinds of biotope that are full of a specific variety should not be harmed by development. So there is a law that mandates the protection of biotopes. There is also a provincial law corresponding to the federal one. Such developments were uncommon in those times.
Landscape plan.
Many German states are obliged by law to produce a landscape plan (Landschaftsplan) as part of their urban planning, though these plans vary somewhat from place to place. The purpose of the "Landschaftsplan" is to protect the region's environment and landscape. These plans use text and figures to describe the present environmental state and proposed remedies. They consider, for example, the regional lie of the land, climate, wind direction, soil, ground water, type of biotope, distribution of animals and plants, inhabitants' welfare and competition with development projects.
Citizen welfare.
Biotope preservation in cities also emphasises recreation and relaxation for citizens and improving the urban environment. For example, in the reserve of Karlsruhe in Baden-Württemberg people can cycle on the bike path or walk the dog, although it is forbidden to gather plants and animals there or walk in the exclusion zone. At the core of biotope preservation is the idea that if civic life is surrounded by a rich profusion of nature whose background is in local history and culture, it is improved by protecting nature and preserving the landscape.
Aquaria.
The term "biotope" is also used by aquarium hobbyists to describe an aquarium setup that tries to simulate the natural habitat of specific fish. The idea is to replicate conditions such as water parameters, natural plants, water type (fresh, saline or brackish), lighting, and to include other native fish which usually live together in nature and as such, represent a particular real-world biotope. An example of one South American biotope type might be lots of bog-wood, a few native plants, dark substrata and subdued lighting with floating plants, along with marbled hatchets, angels, cardinals, otos, corys and plecostomus.

</doc>
<doc id="55176" url="http://en.wikipedia.org/wiki?curid=55176" title="Motorola 88000">
Motorola 88000

The 88000 (m88k for short) is a RISC instruction set architecture (ISA) developed by Motorola. The 88000 was Motorola's attempt at a home-grown RISC architecture, started in the 1980s. The 88000 arrived on the market some two years after the competing SPARC and MIPS. Due to the late start and extensive delays releasing the second-generation MC88110, the m88k achieved very limited success outside of the MVME platform and embedded controller environments.
Though sometimes referred to as A88k, the Apollo PRISM is not related to the Motorola 88000.
History.
Originally called the 78000 as a homage to their famed 68000 series, the design went through a tortured development path, including the name change, before finally emerging in April 1988. This initial version generally required a separate MMU, and saw little use. A follow-on version combining the CPU and MMU was planned. In the late 1980s several companies were actively watching the 88000 series for future use, including NeXT, Apple Computer and Apollo Computer, but all gave up by the time the 88110 was available in 1990.
There was an attempt to popularize the system with the 88open group, similar to what Sun Microsystems was attempting with their SPARC design. It appears to have failed in any practical sense.
In the early 1990s Motorola joined the AIM effort to create a new RISC architecture based on the IBM POWER architecture. They worked a few features of the 88000 into the new PowerPC architecture to offer their customer base some sort of upgrade path. At that point the 88000 was dumped as soon as possible.
Architecture.
Like the 68000 before it, the 88000 was considered to be a very "clean" design. It was a pure 32-bit load/store architecture, using separate instruction and data caches (Harvard architecture), and separate data and address buses. It had a small but powerful command set, and, like all Motorola CPUs, did not use memory segmentation.
A major architectural mistake was that both integer instructions and floating-point instructions used the same register file. This required the single register file to have sufficient read and write ports to support both the integer execution unit and the floating-point unit. The connections for each port is an additional capacitive load that must be driven by register memory cell. This made it more difficult to build high frequency superscalar implementations.
Implementations.
The first implementation of the 88000 ISA was the MC88100 microprocessor, which included an integrated FPU. Mated to this was the MC88200 MMU and cache controller. The idea behind this splitting of duties was to allow multiprocessor systems to be built more easily; a single MC88200 could support up to four MC88100s. However, this also meant that building the most basic system, with a single processor, required both chips and considerable wiring between them, driving up costs. This is likely another major reason for the 88000's limited success.
This was later addressed by the superscalar MC88110, which combined the CPU, FPU, MMU, and L1 cache into a single package. An additional modification, made at the behest of MIT's *T project, resulted in the MC88110MP, including on-chip communications for use in multi-processor systems. A version capable of speeds up to 100 MHz was planned as the MC88120, but was never built.
An implementation for embedded applications, the MC88300, was under development during the early 1990s, but was eventually canceled. Ford was the only design win, and they were offered a PowerPC design as a replacement, which they accepted.
Products and applications.
Motorola released a series of single-board computers, known as the MVME series, for building "out of the box" systems based on the 88000, as well as the Series 900 "stackable" computers employing these MVME boards. Unlike tower or rack mount systems, the Series 900 sat on top of each other and connected to one another with bus-like cabling. The concept never caught on.
Major 3rd party users were limited. The only widespread use would be in the Data General AViiON series. These were fairly popular, and remain in limited use today. For later models, DG moved to Intel. Encore Computer built their Encore-91 machine on the m88k, then introduced a completely ground-up redesign as the Infinity 90 series, but it is unclear how many of these machines were sold. Encore moved to the Alpha.
GEC Computers used the 88100 to build the GEC 4310, one of the GEC 4000 series computers, but issues with memory management meant it didn't perform as well as their earlier gate array based and Am2900 based GEC 4000 series computers. The BBN Butterfly model TC-2000 used the 88100 processor, and scaled to 512 CPUs. Linotype-Hell used the 88110 in their "Power" workstations running the DaVinci raster graphics editor for image manipulation.
The 88110 made it into some versions of a never released NeXT machine, the NeXT RISC Workstation, but the project was canceled along with all NeXT hardware projects in 1993. The 4-processor OMRON luna88k machines from Japan used the m88k, and were used for a short time on the Mach kernel project at Carnegie Mellon University. In the early 1990s Northern Telecom used the 88100 and 88110 as the central processor in its DMS SuperNode family of telephone switches.
Most other users were much smaller. Alpha Microsystems originally planned to migrate to the 88K architecture from the Motorola 68000, and internally created a machine around it running UNIX System V, but it was later scrapped in favour of later 68K derivatives. NCD used the 88100 (without the 88200) in its 88K X-Terminals. Dolphin Server, a spin-off from the dying Norsk Data built servers based on the 88k. Around 100 systems were shipped during 1988-1992.
Virtuality used the 88110 in the SU2000 virtual reality arcade machine as a graphics processor, with one 88110 per screen of each head-mounted display
In the embedded computer space, the "Tri-channel VMS Computer" in the F-15 S/MTD used three 88000s in a triply redundant computer.
Operating system support.
Motorola released its own UNIX System V derivative, System V/88, for its 88000-based systems. There were two major releases: Release 3.2 Version 3 and Release 4.0 Version 3.<ref name="UNIX System V/88"></ref> Data General AViiON systems ran DG/UX. OpenBSD ports exist for the MVME systems,<ref name="OpenBSD/mvme88k"></ref> LUNA-88K workstations,<ref name="OpenBSD/luna88k"></ref> and Data General AViiON systems.<ref name="OpenBSD/aviion"></ref> At least one unofficial experimental NetBSD port exists for the MVME systems.<ref name="NetBSD/m88k"> Unofficial port of NetBSD 3.x</ref>

</doc>
<doc id="55177" url="http://en.wikipedia.org/wiki?curid=55177" title="Restionales">
Restionales

Restionales is a botanical name for an order of flowering plants. In the Cronquist system (of 1981) it is used for an order (in subclass "Commelinidae") and circumscribed as:
The APG II system (2003) assigns the plants involved to the order "Poales".

</doc>
<doc id="55178" url="http://en.wikipedia.org/wiki?curid=55178" title="Staff">
Staff

Staff may refer to:

</doc>
<doc id="55180" url="http://en.wikipedia.org/wiki?curid=55180" title="Juncales">
Juncales

Juncales is a botanical name for an order of flowering plants. In the Engler system (update, of 1964) and in the Cronquist system (of 1981, which placed this order in subclass Commelinidae) it is circumscribed as:
However, the Thorne system (1992) accepts it as consisting of :
The APG II system, used here, assigns the plants involved to the order Poales.

</doc>
<doc id="55182" url="http://en.wikipedia.org/wiki?curid=55182" title="Jakarta Project">
Jakarta Project

The Jakarta Project creates and maintains open source software for the Java platform. It operates as an umbrella project under the auspices of the Apache Software Foundation, and all Jakarta products are released under the Apache License. As of December 21, 2011 the Jakarta project has been retired because no subprojects are remaining.
Subprojects.
Major contributions by the Jakarta Project include tools, libraries and frameworks such as:
The following projects were formerly part of Jakarta, but now form independent projects within the Apache Software Foundation:
Project name.
Jakarta is not directly named after the capital city of Indonesia, nor after the Jakarta blue butterfly species. Instead, it is named after the conference room at Sun Microsystems where the majority of discussions leading to the project's creation took place. This was in turn almost certainly named after the city, which is the capital of Indonesia, located on the island of Java.

</doc>
<doc id="55183" url="http://en.wikipedia.org/wiki?curid=55183" title="Hydatellales">
Hydatellales

Hydatellales is a botanical name for an order of flowering plants. In the Cronquist system, 1981, the name was used for an order placed in the subclass Commelinidae in class Liliopsida [=monocotyledons]. The order consisted of one family only:
Similarly the Dahlgren system recognised this order (with the same circumscription and placed it in superorder Commelinanae in subclass Liliidae [=monocotyledons].
The APG II system assigns these plants to the order Poales, close to the grasses and sedges. Recent study by Saarela "et al.", however, suggests a position out of the Poales; here, the Hydatellaceae link with the waterlilies, the first time a plant has been ejected from the monocots. The Angiosperm Phylogeny Website had since updated the Nymphaeales page to include the family.

</doc>
<doc id="55184" url="http://en.wikipedia.org/wiki?curid=55184" title="Hop (telecommunications)">
Hop (telecommunications)

In telecommunication, a hop is a portion of a signal's journey from source to receiver. Examples include:
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="55185" url="http://en.wikipedia.org/wiki?curid=55185" title="Typhales">
Typhales

Typhales is a botanical name for an order of flowering plants. In the Cronquist system the name was used for an order placed in the subclass "Commelinidae". The order consisted of (1981):
The APG II system, used here, assigns the plants involved to the order "Poales".

</doc>
<doc id="55188" url="http://en.wikipedia.org/wiki?curid=55188" title="Barbara McClintock">
Barbara McClintock

Barbara McClintock (June 16, 1902 – September 2, 1992), was an American scientist and cytogeneticist who was awarded the 1983 Nobel Prize in Physiology or Medicine. McClintock received her PhD in botany from Cornell University in 1927. There she started her career as the leader in the development of maize cytogenetics, the focus of her research for the rest of her life. From the late 1920s, McClintock studied chromosomes and how they change during reproduction in maize. Her work was groundbreaking; she developed the technique for visualizing maize chromosomes and used microscopic analysis to demonstrate many fundamental genetic ideas. One of those ideas was the notion of genetic recombination by crossing-over during meiosis—a mechanism by which chromosomes exchange information. She produced the first genetic map for maize, linking regions of the chromosome to physical traits. She demonstrated the role of the telomere and centromere, regions of the chromosome that are important in the conservation of genetic information. She was recognized among the best in the field, awarded prestigious fellowships, and elected a member of the National Academy of Sciences in 1944.
During the 1940s and 1950s, McClintock discovered transposition and used it to demonstrate that genes are responsible for turning physical characteristics on and off. She developed theories to explain the suppression and expression of genetic information from one generation of maize plants to the next. Due to skepticism of her research and its implications, she stopped publishing her data in 1953.
Later, she made an extensive study of the cytogenetics and ethnobotany of maize races from South America. McClintock's research became well understood in the 1960s and 1970s, as other scientists confirmed the mechanisms of genetic change and genetic regulation that she had demonstrated in her maize research in the 1940s and 1950s. Awards and recognition for her contributions to the field followed, including the Nobel Prize in Physiology or Medicine, awarded to her in 1983 for the discovery of genetic transposition; she is the only woman to receive an unshared Nobel Prize in that category.
Early life.
Barbara McClintock was born Eleanor McClintock on June 16, 1902 in Hartford, Connecticut, the third of four children born to physician Thomas Henry McClintock and Sara Handy McClintock. Thomas McClintock was the child of British immigrants, and Sara Handy, born Grace, descended from an old American "Mayflower" family. Marjorie, the oldest child, was born in October 1898; Mignon, the second daughter, was born in November 1900. The youngest, Malcolm Rider (called Tom), was born 18 months after Barbara. As a young girl, her parents determined that Eleanor, a "feminine" and "delicate" name, was not appropriate for her, and chose Barbara instead. McClintock was an independent child beginning at a very young age, a trait she later identified as her "capacity to be alone". From the age of three until she began school, McClintock lived with an aunt and uncle in Brooklyn, New York in order to reduce the financial burden on her parents while her father established his medical practice. She was described as a solitary and independent child, and a tomboy. She was close to her father, but had a difficult relationship with her mother, tension that began when she was young.
The McClintock family moved to Brooklyn in 1908 and McClintock completed her secondary education there at Erasmus Hall High School; she graduated early in 1919. She discovered her love of science and reaffirmed her solitary personality during high school. She wanted to continue her studies at Cornell University's College of Agriculture. Her mother resisted sending McClintock to college, for fear that she would be unmarriageable. McClintock was almost prevented from starting college, but her father intervened just before registration began, and she matriculated at Cornell in 1919.
Education and research at Cornell.
McClintock began her studies at Cornell's College of Agriculture in 1919. There, she participated in student government and was invited to join a sorority, though she soon realized that she preferred not to join formal organizations. Instead, McClintock took up music, specifically jazz. She studied botany, receiving a BSc in 1923. Her interest in genetics began when she took her first course in that field in 1921. The course was based on a similar one offered at Harvard University, and was taught by C. B. Hutchison, a plant breeder and geneticist. Hutchison was impressed by McClintock's interest, and telephoned to invite her to participate in the graduate genetics course at Cornell in 1922. McClintock pointed to Hutchison's invitation as the reason she continued in genetics: "Obviously, this telephone call cast the die for my future. I remained with genetics thereafter." Although it has been reported that women could not major in genetics at Cornell, and therefore her MA and PhD—earned in 1925 and 1927, respectively—were officially awarded in botany, recent research has revealed that women did earn graduate degrees in Cornell's Plant Breeding Department during the time that McClintock was a student at Cornell.
During her graduate studies and postgraduate appointment as a botany instructor, McClintock was instrumental in assembling a group that studied the new field of cytogenetics in maize. This group brought together plant breeders and cytologists, and included Marcus Rhoades, future Nobel laureate George Beadle, and Harriet Creighton. Rollins A. Emerson, head of the Plant Breeding Department, supported these efforts, although he was not a cytologist himself.
She also worked as a research assistant for Lowell Fitz Randolph and then for Lester W. Sharp, both Cornell Botanists.
McClintock's cytogenetic research focused on developing ways to visualize and characterize maize chromosomes. This particular part of her work influenced a generation of students, as it was included in most textbooks. She also developed a technique using carmine staining to visualize maize chromosomes, and showed for the first time the morphology of the 10 maize chromosomes. This discovery was made because she observed cells from the microspore as opposed to the root tip. By studying the morphology of the chromosomes, McClintock was able to link specific chromosome groups of traits that were inherited together. Marcus Rhoades noted that McClintock's 1929 "Genetics" paper on the characterization of triploid maize chromosomes triggered scientific interest in maize cytogenetics, and attributed to her 10 of the 17 significant advances in the field that were made by Cornell scientists between 1929 and 1935.
In 1930, McClintock was the first person to describe the cross-shaped interaction of homologous chromosomes during meiosis. The following year, McClintock and Creighton proved the link between chromosomal crossover during meiosis and the recombination of genetic traits. They observed how the recombination of chromosomes seen under a microscope correlated with new traits. Until this point, it had only been hypothesized that genetic recombination could occur during meiosis, although it had been shown genetically. McClintock published the first genetic map for maize in 1931, showing the order of three genes on maize chromosome 9. This information provided necessary data for the crossing-over study she published with Creighton; they also showed that crossing-over occurs in sister chromatids as well as homologous chromosomes. In 1938, she produced a cytogenetic analysis of the centromere, describing the organization and function of the centromere, as well as the fact that it can divide.
McClintock's breakthrough publications, and support from her colleagues, led to her being awarded several postdoctoral fellowships from the National Research Council. This funding allowed her to continue to study genetics at Cornell, the University of Missouri, and the California Institute of Technology, where she worked with E. G. Anderson. During the summers of 1931 and 1932, she worked at Missouri with geneticist Lewis Stadler, who introduced her to the use of X-rays as a mutagen. Exposure to X-rays can increase the rate of mutation above the natural background level, making it a powerful research tool for genetics. Through her work with X-ray-mutagenized maize, she identified ring chromosomes, which form when the ends of a single chromosome fuse together after radiation damage. From this evidence, McClintock hypothesized that there must be a structure on the chromosome tip that would normally ensure stability. She showed that the loss of ring-chromosomes at meiosis caused variegation in maize foliage in generations subsequent to irradiation resulting from chromosomal deletion. During this period, she demonstrated the presence of the nucleolus organizer region on a region on maize chromosome 6, which is required for the assembly of the nucleolus. In 1933, she established that cells can be damaged when nonhomologous recombination occurs. During this same period, McClintock hypothesized that the tips of chromosomes are protected by telomeres.
McClintock received a fellowship from the Guggenheim Foundation that made possible six months of training in Germany during 1933 and 1934. She had planned to work with Curt Stern, who had demonstrated crossing-over in "Drosophila" just weeks after McClintock and Creighton had done so; however, Stern emigrated to the United States. Instead, she worked with geneticist Richard B. Goldschmidt, who was the head of the Kaiser Wilhelm Institute. She left Germany early amidst mounting political tension in Europe, and returned to Cornell, remaining there until 1936, when she accepted an Assistant Professorship offered to her by Lewis Stadler in the Department of Botany at the University of Missouri-Columbia. While still at Cornell, she was supported by a two-year Rockefeller Foundation grant obtained for her through Emerson's efforts.
University of Missouri.
During her time at Missouri, McClintock expanded her research on the effect of X-rays on maize cytogenetics. McClintock observed the breakage and fusion of chromosomes in irradiated maize cells. She was also able to show that, in some plants, spontaneous chromosome breakage occurred in the cells of the endosperm. Over the course of mitosis, she observed that the ends of broken chromatids were rejoined after the chromosome replication. In the anaphase of mitosis, the broken chromosomes formed a chromatid bridge, which was broken when the chromatids moved towards the cell poles. The broken ends were rejoined in the interphase of the next mitosis, and the cycle was repeated, causing massive mutation, which she could detect as variegation in the endosperm. This breakage–rejoining–bridge cycle was a key cytogenetic discovery for several reasons. First, it showed that the rejoining of chromosomes was not a random event, and second, it demonstrated a source of large-scale mutation. For this reason, it remains an area of interest in cancer research today.
Although her research was progressing at Missouri, McClintock was not satisfied with her position at the University. She recalled being excluded from faculty meetings, and was not made aware of positions available at other institutions. In 1940, she wrote to Charles Burnham, "I have decided that I must look for another job. As far as I can make out, there is nothing more for me here. I am an assistant professor at $3,000 and I feel sure that that is the limit for me." Initially, McClintock's position was created especially for her by Stadler, and might have depended on his presence at the university. McClintock believed she would not gain tenure at Missouri, even though according to some accounts, she knew she would be offered a promotion from Missouri in the spring of 1942. Recent evidence reveals that McClintock more likely decided to leave Missouri because she had lost trust in her employer and in the University administration, after discovering that her job would be in jeopardy if Stadler were to leave for Caltech, as he had considered doing. The university's retaliation against Stadler amplified her sentiments.
In early 1941, she took a leave of absence from Missouri in hopes of finding a position elsewhere. She accepted a visiting Professorship at Columbia University, where her former Cornell colleague Marcus Rhoades was a professor. Rhoades also offered to share his research field at Cold Spring Harbor on Long Island. In December 1941, she was offered a research position by Milislav Demerec, the newly appointed acting director of the Carnegie Institution of Washington's Department of Genetics Cold Spring Harbor Laboratory; McClintock accepted his invitation despite her qualms and became a permanent member of the faculty.
Cold Spring Harbor.
After her year-long temporary appointment, McClintock accepted a full-time research position at Cold Spring Harbor Laboratory. There, she was highly productive and continued her work with the breakage-fusion-bridge cycle, using it to substitute for X-rays as a tool for mapping new genes. In 1944, in recognition of her prominence in the field of genetics during this period, McClintock was elected to the National Academy of Sciences—only the third woman to be elected. That same year, she became the first female president of the Genetics Society of America; she was elected its vice-president in 1939. In 1944 she undertook a cytogenetic analysis of "Neurospora crassa" at the suggestion of George Beadle, who used the fungus to demonstrate the one gene–one enzyme relationship. He invited her to Stanford to undertake the study. She successfully described the number of chromosomes, or karyotype, of "N. crassa" and described the entire life cycle of the species. "N. crassa" has since become a model species for classical genetic analysis.
Discovery of controlling elements.
In the summer of 1944 at Cold Spring Harbor Laboratory, McClintock began systematic studies on the mechanisms of the mosaic color patterns of maize seed and the unstable inheritance of this mosaicism. She identified two new dominant and interacting genetic loci that she named "Dissociator" ("Ds") and "Activator" ("Ac"). She found that the "Dissociator" did not just dissociate or cause the chromosome to break, it also had a variety of effects on neighboring genes when the "Activator" was also present, which included making certain stable mutations unstable. In early 1948, she made the surprising discovery that both "Dissociator" and "Activator" could transpose, or change position, on the chromosome.
She observed the effects of the transposition of "Ac" and "Ds" by the changing patterns of coloration in maize kernels over generations of controlled crosses, and described the relationship between the two loci through intricate microscopic analysis. She concluded that "Ac" controls the transposition of the "Ds" from chromosome 9, and that the movement of "Ds" is accompanied by the breakage of the chromosome. When "Ds" moves, the aleurone-color gene is released from the suppressing effect of the "Ds" and transformed into the active form, which initiates the pigment synthesis in cells. The transposition of "Ds" in different cells is random, it may move in some but not others, which causes color mosaicism. The size of the colored spot on the seed is determined by stage of the seed development during dissociation. McClintock also found that the transposition of "Ds" is determined by the number of "Ac" copies in the cell.
Between 1948 and 1950, she developed a theory by which these mobile elements regulated the genes by inhibiting or modulating their action. She referred to "Dissociator" and "Activator" as "controlling units"—later, as "controlling elements"—to distinguish them from genes. She hypothesized that gene regulation could explain how complex multicellular organisms made of cells with identical genomes have cells of different function. McClintock's discovery challenged the concept of the genome as a static set of instructions passed between generations. In 1950, she reported her work on "Ac/Ds" and her ideas about gene regulation in a paper entitled "The origin and behavior of mutable loci in maize" published in the journal "Proceedings of the National Academy of Sciences". In summer 1951, when she reported her work on the origin and behavior of mutable loci in maize at the annual symposium at Cold Spring Harbor Laboratory, presenting a paper of the same name. The paper delved into the instability caused by "Dc" and "As" or just "As" in four genes, along with the tendency of those genes to unpredictably revert to the wild phenotype. She also identified "families" of transposons, which did not interact with one another.
Her work on controlling elements and gene regulation was conceptually difficult and was not immediately understood or accepted by her contemporaries; she described the reception of her research as "puzzlement, even hostility". Nevertheless, McClintock continued to develop her ideas on controlling elements. She published a paper in "Genetics" in 1953, where she presented all her statistical data, and undertook lecture tours to universities throughout the 1950s to speak about her work. She continued to investigate the problem and identified a new element that she called "Suppressor-mutator" ("Spm"), which, although similar to "Ac/Ds", acts in a more complex manner. Like "Ac/Ds", some versions could transpose on their own and some could not; unlike "Ac/Ds", when present, it fully suppressed the expression of mutant genes when they normally would not be entirely suppressed. Based on the reactions of other scientists to her work, McClintock felt she risked alienating the scientific mainstream, and from 1953 stopped publishing accounts of her research on controlling elements.
The origins of maize.
In 1957, McClintock received funding from the National Academy of Sciences to start research on indigenous strains of maize in Central America and South America. She was interested in studying the evolution of maize through chromosomal changes, and being in South America would allow her to work on a larger scale. McClintock explored the chromosomal, morphological, and evolutionary characteristics of various races of maize. After extensive work in the 1960s and 1970s, McClintock and her collaborators published the seminal study "The Chromosomal Constitution of Races of Maize", leaving their mark on paleobotany, ethnobotany, and evolutionary biology.
Rediscovery of McClintock's controlling elements.
McClintock officially retired from her position at the Carnegie Institution in 1967, and was made a Distinguished Service Member of the Carnegie Institution of Washington. This honor allowed her to continue working with graduate students and colleagues in the Cold Spring Harbor Laboratory as "scientist emerita"; she lived in the town. In reference to her decision 20 years earlier to stop publishing detailed accounts of her work on controlling elements, she wrote in 1973: 
"Over the years I have found that it is difficult if not impossible to bring to consciousness of another person the nature of his tacit assumptions when, by some special experiences, I have been made aware of them. This became painfully evident to me in my attempts during the 1950s to convince geneticists that the action of genes had to be and was controlled. It is now equally painful to recognize the fixity of assumptions that many persons hold on the nature of controlling elements in maize and the manners of their operation. One must await the right time for conceptual change."
The importance of McClintock's contributions was revealed in the 1960s, when the work of French geneticists Francois Jacob and Jacques Monod described the genetic regulation of the "lac" operon, a concept she had demonstrated with "Ac/Ds" in 1951. Following Jacob and Monod's 1961 "Journal of Molecular Biology" paper "Genetic regulatory mechanisms in the synthesis of proteins", McClintock wrote an article for "American Naturalist" comparing the "lac" operon and her work on controlling elements in maize. McClintock's contribution to biology is still not widely acknowledged as amounting to the discovery of genetic regulation.
McClintock was widely credited for discovering transposition after other researchers finally discovered the process in bacteria, yeast, and bacteriophages in the late 1960s and early 1970s. During this period, molecular biology had developed significant new technology, and scientists were able to show the molecular basis for transposition. In the 1970s, "Ac" and "Ds" were cloned by other scientists and were shown to be Class II transposons. "Ac" is a complete transposon that can produce a functional transposase, which is required for the element to move within the genome. "Ds" has a mutation in its transposase gene, which means that it cannot move without another source of transposase. Thus, as McClintock observed, "Ds" cannot move in the absence of "Ac". "Spm" has also been characterized as a transposon. Subsequent research has shown that transposons typically do not move unless the cell is placed under stress, such as by irradiation or the breakage-fusion-bridge cycle, and thus their activation during stress can serve as a source of genetic variation for evolution. McClintock understood the role of transposons in evolution and genome change well before other researchers grasped the concept. Nowadays, "Ac/Ds" is used as a tool in plant biology to generate mutant plants used for the characterization of gene function.
Honors and recognition.
In 1947, McClintock received the Achievement Award from the American Association of University Women. She was elected a Fellow of the American Academy of Arts and Sciences in 1959. In 1967, McClintock was awarded the Kimber Genetics Award; three years later, she was given the National Medal of Science by Richard Nixon in 1970. She was the first woman to be awarded the National Medal of Science.
Cold Spring Harbor named a building in her honor in 1973. She received the Louis and Bert Freedman Foundation Award and the Lewis S. Rosensteil Award in 1978. In 1981, she became the first recipient of the MacArthur Foundation Grant, and was awarded the Albert Lasker Award for Basic Medical Research, the Wolf Prize in Medicine and the Thomas Hunt Morgan Medal by the Genetics Society of America. In 1982, she was awarded the Louisa Gross Horwitz Prize from Columbia University for her research in the "evolution of genetic information and the control of its expression."
Most notably, she received the Nobel Prize for Physiology or Medicine in 1983, the first woman to win that prize unshared, credited by the Nobel Foundation for discovering "mobile genetic elements"; it was more than 30 years after she initially described the phenomenon of controlling elements. She was compared to Gregor Mendel in terms of her scientific career by the Swedish Academy of Sciences when she was awarded the Prize.
She was elected a Foreign Member of the Royal Society in 1989. McClintock received the Benjamin Franklin Medal for Distinguished Achievement in the Sciences of the American Philosophical Society in 1993. She was awarded 14 Honorary Doctor of Science degrees and an Honorary Doctor of Humane Letters. In 1986 she was inducted into the National Women's Hall of Fame. During her final years, McClintock led a more public life, especially after Evelyn Fox Keller's 1983 biography of her, "A Feeling for the Organism," brought McClintock's story to the public. She remained a regular presence in the Cold Spring Harbor community, and gave talks on mobile genetic elements and the history of genetics research for the benefit of junior scientists. An anthology of her 43 publications "The Discovery and Characterization of Transposable Elements: The Collected Papers of Barbara McClintock" was published in 1987.
Later years.
McClintock spent her later years, post Nobel Prize, as a key leader and researcher in the field at Cold Spring Harbor Laboratory on Long Island, New York. McClintock died of natural causes in Huntington, New York, on September 2, 1992 at the age of 90; she never married or had children.
Legacy.
Since her death, McClintock has been the subject of a biography by the science historian Nathaniel C. Comfort's "The Tangled Field: Barbara McClintock's Search for the Patterns of Genetic Control". Comfort's biography contests some claims about McClintock, described as the "McClintock Myth", which he claims was perpetuated by the earlier biography by Keller. Keller's thesis was that McClintock was long ignored or met with derision because she was a woman working in the sciences. For example, when McClintock presented her findings that the genetics of maize did not conform to Mendelian distributions, geneticist Sewall Wright expressed the belief that she did not understand the underlying mathematics of her work, a belief he had expressed towards other women at the time. In addition, geneticist Lotte Auerbach recounted that Joshua Lederberg returned from a visit to McClintock's lab with the remark: 'By God, that woman is either crazy or a genius.' " As Auerbach tells it, McClintock had thrown Lederberg and his colleagues out after half an hour 'because of their arrogance. She was intolerant of arrogance ... She felt she had crossed a desert alone and no one had followed her.'"
Comfort, however, asserts that McClintock was well regarded by her professional peers, even in the early years of her career. Although Comfort argues that McClintock was not a victim of gender discrimination, she has been widely written about in the context of women's studies. Most recent biographical works on women in science feature accounts of her experience. She is held up as a role model for girls in such works of children's literature as Edith Hope Fine's "Barbara McClintock, Nobel Prize Geneticist", Deborah Heiligman's "Barbara McClintock: Alone in Her Field" and Mary Kittredge's "Barbara McClintock". A recent biography for young adults by Naomi Pasachoff, "Barbara McClintock, Genius of Genetics", provides a new perspective, based on the current literature.
On 4 May 2005, the United States Postal Service issued the "American Scientists" commemorative postage stamp series, a set of four 37-cent self-adhesive stamps in several configurations. The scientists depicted were Barbara McClintock, John von Neumann, Josiah Willard Gibbs, and Richard Feynman. McClintock was also featured in a 1989 four-stamp issue from Sweden which illustrated the work of eight Nobel Prize-winning geneticists. A small building at Cornell University and a laboratory building at Cold Spring Harbor Laboratory were named for her. A street has been named after her in the new "Adlershof Development Society" science park in Berlin.
Some of McClintock's personality and scientific achievements were referred to in Jeffrey Eugenides's 2011 novel "The Marriage Plot," which tells the story of a yeast geneticist named Leonard who suffers from bipolar disorder. He works at a laboratory loosely based on Cold Spring Harbor. The character reminiscent of McClintock is a reclusive geneticist at the fictional laboratory, who makes the same discoveries as her factual counterpart.
References.
</dl>

</doc>
<doc id="55189" url="http://en.wikipedia.org/wiki?curid=55189" title="Habitat (disambiguation)">
Habitat (disambiguation)


</doc>
<doc id="55192" url="http://en.wikipedia.org/wiki?curid=55192" title="Albula Alps">
Albula Alps

The Albula Alps are a mountain range in the Alps of eastern Switzerland. They are considered to be part of the Central Eastern Alps, more specifically the Rhaetian Alps. They are named after the River Albula. The Albula Alps are separated from the Oberhalbstein Alps in the west by the Septimer Pass and the Julia valley; from the Plessur Alps in the north-west by the Landwasser valley; from the Silvretta group in the north-east by the Flüela Pass; from the Bernina Range in the south-east by the Maloja Pass and the Inn valley (upper Engadin).
The Albula Alps are drained by the Albula, Julia, Landwasser and Inn rivers.
Peaks.
The main peaks of the Albula Alps are:
Passes.
The Albula Alps are crossed by one railway tunnel, under the Albula Pass. The main mountain passes of the Albula Alps are:

</doc>
<doc id="55196" url="http://en.wikipedia.org/wiki?curid=55196" title="Lepontine Alps">
Lepontine Alps

The Lepontine Alps are a mountain range in the north-western part of the Alps. They are located in Switzerland (Valais, Ticino, Uri and Graubünden) and Italy (Piedmont and Lombardy).
The Simplon rail tunnel (from Brig to Domodossola) the Gotthard rail and Gotthard road tunnels (from Andermatt to Airolo) and the San Bernardino road tunnel are important transport arteries.
The eastern portion of the Lepontine Alps, from the St Gotthard Pass to the Splügen Pass, is sometimes named the "Adula Alps".
Etymology.
The designation "Lepontine Alps", derived from the Latin name of the Val Leventina, has long been somewhat vaguely applied to the Alpine ranges that enclose it, before being used for the whole range.
Geography.
Following the line marking the division of the waters that flow into the Po from those that feed the Rhone or the Rhine, the main ridge of the Lepontine Alps describes a somewhat irregular curve, convex to the north, from the Simplon Pass to the Splugen Pass. With the single exception of the Monte Leone, overlooking the pass of the Simplon, the summits of this portion of the chain are much inferior in height to those of the neighbouring chains; but two peaks of the Adula group, culminating at the Rheinwaldhorn, exceed 11000 ft in height.
The extensive region lying south of the main ridge is occupied by mountain ranges whose summits sometimes rival in height those of the dividing ridge, and which are cut through by deep valleys, three of which converge in the basins of Lake Maggiore and Lake Como, the deepest of all the lakes on the south side of the Alps. The most important of these valleys is the Val Leventina, or the Upper valley of the Ticino. This has been known from a remote antiquity because it leads to the Pass of St. Gotthard, one of the easiest lines of communication between northern and southern Europe.
The Lepontine Alps are drained by the rivers Rhône in the west, Reuss in the north, Rhine (Vorderrhein and Hinterrhein) in the east and Ticino and Toce in the south.
List of peaks.
The chief peaks of the Lepontine Alps are:
Glaciers.
Main glaciers :
List of passes.
The chief passes of the Lepontine Alps are:

</doc>
<doc id="55199" url="http://en.wikipedia.org/wiki?curid=55199" title="Mössbauer effect">
Mössbauer effect

The Mössbauer effect, or recoilless nuclear resonance fluorescence, is a physical phenomenon discovered by Rudolf Mössbauer in 1958. It involves the resonant and recoil-free emission and absorption of gamma radiation by atomic nuclei bound in a solid. Its main application is in Mössbauer spectroscopy.
In the Mössbauer effect the narrow resonance absorption for nuclear gamma absorption can be successfully attained by physically immobilizing atomic nuclei in a crystal. The immobilization of nuclei at both ends of a gamma resonance interaction is required so that no gamma energy is lost to the kinetic energy of recoiling nuclei at either the emitting or absorbing end of a gamma transition. Such loss of energy causes gamma ray resonance absorption to fail. However, when emitted gamma rays carry essentially all of the energy of the atomic nuclear de-excitation that produces them, this energy is also sufficient to excite the same energy state in a second immobilized nucleus of the same type. 
History.
The emission and absorption of x-rays by gases had been observed previously, and it was expected that a similar phenomenon would be found for gamma rays, which are created by nuclear transitions (as opposed to x-rays, which are typically produced by electronic transitions). However, attempts to observe nuclear resonance produced by gamma-rays in gases failed due to energy being lost to recoil, preventing resonance (the Doppler effect also broadens the gamma-ray spectrum). Mössbauer was able to observe resonance in nuclei of solid iridium, which raised the question of why gamma-ray resonance was possible in solids, but not in gases. Mössbauer proposed that, for the case of atoms bound into a solid, under certain circumstances a fraction of the nuclear events could occur essentially without recoil. He attributed the observed resonance to this recoil-free fraction of nuclear events.
The Mössbauer effect was one of the last major discoveries in physics to be originally reported in the German language. The first report in English was a letter describing a repetition of the experiment.
The discovery was rewarded with the Nobel Prize in Physics in 1961 together with Robert Hofstadter's research of electron scattering in atomic nuclei.
Description.
The Mössbauer Effect is a process in which a nucleus emits or absorbs gamma rays without loss of energy to a nuclear recoil. It was discovered by the German physicist Rudolf L. Mössbauer in 1958 and has proved to be remarkably useful for basic research in physics and chemistry. It has been used, for instance, in precisely measuring small energy changes in nuclei, atoms, and crystals induced by electrical, magnetic, or gravitational fields. In a transition of a nucleus from a higher to a lower energy state with accompanying emission of gamma rays, the emission generally causes the nucleus to recoil, and this takes energy from the emitted gamma rays. Thus the gamma rays do not have sufficient energy to excite a target nucleus to be examined. However, Mössbauer discovered that it is possible to have transitions in which the recoil is absorbed by a whole crystal in which the emitting nucleus is bound. Under these circumstances, the energy that goes into the recoil is a negligible portion of the energy of the transition. Therefore the emitted gamma rays carry virtually all of the energy liberated by the nuclear transition. The gamma rays thus are able to induce a reverse transition, under similar conditions of negligible recoil, in a target nucleus of the same material as the emitter but in a lower energy state. In general, gamma rays are produced by nuclear transitions from an unstable high-energy state, to a stable low-energy state. The energy of the emitted gamma ray corresponds to the energy of the nuclear transition, minus an amount of energy that is lost as recoil to the emitting atom. If the lost "recoil energy" is small compared with the energy linewidth of the nuclear transition, then the gamma ray energy still corresponds to the energy of the nuclear transition, and the gamma ray can be absorbed by a second atom of the same type as the first. This emission and subsequent absorption is called resonant fluorescence. Additional recoil energy is also lost during absorption, so in order for resonance to occur the recoil energy must actually be less than half the linewidth for the corresponding nuclear transition.
The amount of energy in the recoiling body ("E") can be found from momentum conservation:
where "P" is the momentum of the recoiling matter, and "P"γ the momentum of the gamma ray. Substituting energy into the equation gives:
where "E" ( for 57Fe) is the energy lost as recoil, "E"γ is the energy of the gamma ray ( for 57Fe), "M" ( for 57Fe) is the mass of the emitting or absorbing body, and "c" is the speed of light. In the case of a gas the emitting and absorbing bodies are atoms, so the mass is relatively small, resulting in a large recoil energy, which prevents resonance. (Note that the same equation applies for recoil energy losses in x-rays, but the photon energy is much less, resulting in a lower energy loss, which is why gas-phase resonance could be observed with x-rays.)
In a solid, the nuclei are bound to the lattice and do not recoil in the same way as in a gas. The lattice as a whole recoils but the recoil energy is negligible because the "M" in the above equation is the mass of the whole lattice. However, the energy in a decay can be taken up or supplied by lattice vibrations. The energy of these vibrations is quantised in units known as "phonons". The Mössbauer effect occurs because there is a finite probability of a decay occurring involving no phonons. Thus in a fraction of the nuclear events (the recoil-free fraction, given by the Lamb–Mössbauer factor), the entire crystal acts as the recoiling body, and these events are essentially recoil-free. In these cases, since the recoil energy is negligible, the emitted gamma rays have the appropriate energy and resonance can occur.
In general (depending on the half-life of the decay), gamma rays have very narrow linewidths. This means they are very sensitive to small changes in the energies of nuclear transitions. In fact, gamma rays can be used as a probe to observe the effects of interactions between a nucleus and its electrons and those of its neighbors. This is the basis for Mössbauer spectroscopy, which combines the Mössbauer effect with the Doppler effect to monitor such interactions.
Zero-phonon optical transitions, a process closely analogous to the Mössbauer effect, can be observed in lattice-bound chromophores at low temperatures.

</doc>
<doc id="55200" url="http://en.wikipedia.org/wiki?curid=55200" title="Passes of the Silvretta and Rätikon Ranges">
Passes of the Silvretta and Rätikon Ranges

The chief passes of the Silvretta and Rhatikon Ranges, from the Fuela Pass to the Reschen Scheideck and the Arlberg Pass, are:

</doc>
<doc id="55202" url="http://en.wikipedia.org/wiki?curid=55202" title="Dolomites">
Dolomites

The Dolomites (Italian: "Dolomiti"; Ladin: "Dolomites"; German: "Dolomiten"; Venetian: "Dołomiti": Friulian: "Dolomitis") are a mountain range located in northeastern Italy. They form a part of Southern Limestone Alps and extend from the River Adige in the west to the Piave Valley (Pieve di Cadore) in the east. The northern and southern borders are defined by the Puster Valley and the Sugana Valley (Italian: Valsugana). The Dolomites are nearly equally shared between the provinces of Belluno, South Tyrol and Trentino.
There are also mountain groups of similar geological structure that spread over the River Piave to the east – "Dolomiti d'Oltrepiave"; and far away over the Adige River to the west – Dolomiti di Brenta (Western Dolomites). There is also another smaller group called Piccole Dolomiti (Little Dolomites) located between the provinces of Trentino, Verona and Vicenza (see the map).
One national park and many other regional parks are located in the Dolomites. In August 2009, the Dolomites were declared a UNESCO World Heritage Site.
Etymology.
The name "Dolomites" is derived from the famous French mineralogist Déodat Gratet de Dolomieu who was the first to describe the rock, dolomite, a type of carbonate rock which is responsible for the characteristic shapes and colour of these mountains; previously they were called the "pale mountains," and it was only in the early 19th century that the name was Gallicized.
History.
During the First World War, the line between the Italian and Austro-Hungarian forces ran through the Dolomites. There are now open-air war museums at Cinque Torri (Five Towers) and Mount Lagazuoi. Many people visit the Dolomites to climb the vie ferrate, protected paths created during the First World War. A number of long distance footpaths run across the Dolomites, which are called "alte vie" (i.e., high paths). Such long trails, which are numbered from 1 to 8, require at least a week to be walked through and are served by numerous "Rifugi" (huts). The first and, perhaps, most renowned is the Alta Via 1.
Geography.
The region is commonly divided into the Western and Eastern Dolomites, separated by a line following the Val Badia – Campolongo Pass – Cordevole Valley (Agordino) axis.
Current classification.
Based on current classifications, the Dolomites may be divided into the following ranges:
Tourism.
A tourist mecca, the Dolomites are famous for skiing in the winter months and mountain climbing, hiking, cycling, and Base Jumping, as well as paragliding and hang gliding in summer and late spring/early autumn. Free climbing has been a tradition in the Dolomites since 1887, when 17-year-old Georg Winkler soloed the first ascent of the pinnacle Die Vajolettürme. The main centres include: Rocca Pietore alongside the Marmolada Glacier, which lies on the border of Trentino and Veneto, the small towns of Alleghe, Falcade, Auronzo, Cortina d'Ampezzo and the villages of Arabba, Urtijëi and San Martino di Castrozza, as well as the whole of the Fassa, Gardena and Badia valleys.
The Maratona dles Dolomites, an annual single-day road bicycle racing race covering seven mountain passes of the Dolomites, occurs in the first week of July.
Other characteristic places are:

</doc>
<doc id="55203" url="http://en.wikipedia.org/wiki?curid=55203" title="Dauphiné Alps">
Dauphiné Alps

The Dauphiné Alps (French: "Alpes du Dauphiné") are a group of mountain ranges in southeastern France, west of the main chain of the Alps. Mountain ranges within the Dauphiné Alps include the Massif des Écrins (in the Parc national des Écrins), Belledonne, the Taillefer range and the mountains of Matheysine.
Etymology.
The "Dauphiné" (]) is a former French province whose area roughly corresponded to that of the present departments of :Isère, :Drôme, and :Hautes-Alpes.
Geography.
They are separated from the Cottian Alps in the east by the Col du Galibier and the upper Durance valley; from the western Graian Alps (Vanoise Massif) in the north-east by the river Arc; from the lower ranges Vercors Plateau and Chartreuse Mountains in the west by the rivers Drac and Isère. Many peaks rise to more than 10,000 feet (3,050 m), with Barre des Écrins (4,102 m) the highest.
Administratively the French part of the range belongs to the French departments of Isère, Hautes-Alpes and Savoie.
The whole range is drained by the Rhone river through its tributaries.
Peaks.
The chief peaks of the Dauphiné Alps are:
Passes.
The chief passes of the Dauphiné Alps are:

</doc>
<doc id="55205" url="http://en.wikipedia.org/wiki?curid=55205" title="Swiss Alps">
Swiss Alps

The Alpine region of Switzerland, conventionally referred to as the Swiss Alps (German: "Schweizer Alpen", French: "Alpes suisses", Italian: "Alpi svizzere", Romansh: "Alps svizras"), represents a major natural feature of the country and is, alongside with the Swiss Plateau and the Swiss portion of the Jura Mountains, one of its three main physiographic regions. The Swiss Alps extend over both the Western Alps and the Eastern Alps, encompassing an area sometimes called "Central Alps". While the northern ranges from the Bernese Alps to the Appenzell Alps are entirely in Switzerland, the southern ranges from the Mont Blanc massif to the Bernina massif are shared with other countries such as France, Italy, Austria and Liechtenstein. 
The Swiss Alps comprise all the mountains of Switzerland over 2,000 metres above sea level and almost all the highest mountains of the Alps, such as Monte Rosa (4,634 m), the Dom (4,545 m), the Lyskamm (4,527 m), the Weisshorn (4,506 m) and the Matterhorn (4,478 m). The other following major summits can be found in List of mountains of Switzerland.
Since the Middle Ages, transit across the Alps played an important role in history. The region north of the St. Gotthard Pass became the nucleus of the Swiss Confederacy in the early 14th century.
Geography.
The Alps cover 65% of Switzerland's surface area (41,285 km²), making it one of the most "alpine" countries. Despite the fact that Switzerland covers only 14% of the Alps total area (192,753 km²), many alpine four-thousanders (48 of 82) are located in the Swiss Alps and practically all of the remaining few are within 20 km of the country's border.
The glaciers of the Swiss Alps cover an area of 1220 km² (3% of the Swiss territory), representing 44% of the total glaciated area in the Alps (2800 km²).
The Swiss Alps are situated south of the Swiss Plateau and north of the national border. The limit between the Alps and the plateau runs from Vevey on the shores of Lake Geneva to Rorschach on the shores of Lake Constance, passing close to the cities of Thun and Lucerne.
The Alpine cantons (from highest to lowest) are Valais, Bern, Graubünden, Uri, Glarus, Ticino, St. Gallen, Vaud, Obwalden, Nidwalden, Schwyz, Appenzell Innerrhoden, Appenzell Ausserrhoden, Fribourg, Lucerne and Zug. The countries with which Switzerland shares mountain ranges of the Alps are (from west to east): France, Italy, Austria and Liechtenstein.
Ranges.
The Alps are usually divided into two main parts, the Western Alps and Eastern Alps, whose division is along the Rhine from Lake Constance to the Splügen Pass. The western ranges occupy the greatest part of Switzerland while the more numerous eastern ranges are much smaller and are all situated in the canton of Graubünden. The latter are part of the Central Eastern Alps, except the Ortler Alps which belong to the Southern Limestone Alps. The Pennine, Bernese and Bernina Range are the highest ranges of the country, they contain respectively 38, 9 and 1 summit over 4000 metres. The lowest range is the Appenzell Alps culminating at 2,500 metres.
(*) situated out of the main chain
Western Alps
Eastern Alps
Hydrography.
Rivers.
The north side of the Swiss Alps is drained by the Rhône, Rhine and Inn (which is part of the Danube basin) while the south side is mainly drained by the Ticino (Po basin). The rivers on the north empty into the Mediterranean, North and Black Sea, on the south the Po empty in the Adriatic Sea. The major triple watersheds in the Alps are located within the country, they are: Piz Lunghin, Witenwasserenstock and Monte Forcola. Between the Witenwasserenstock and Piz Lunghin runs the European Watershed separating the basin of the Atlantic (North Sea) and the Mediterranean Sea (Adriatic and Black Sea). The European watershed lies in fact only partially on the main chain. Switzerland possesses 6% of Europe's fresh water, and is sometimes referred to as the "water tower of Europe".
Lakes.
Since the highest dams are located in Alpine regions, many large mountain lakes are artificial and are used as hydroelectric reservoirs. Some large artificial lakes can be found above 2,300 m, but natural lakes larger than 1 km² are generally below 1,000 m (with the exceptions of lakes in the Engadin such as Lake Sils, and Oeschinen in the Bernese Oberland). The melting of low-altitude glaciers can generate new lakes, such as the 0.25 km² large Triftsee which formed between 2002–2003.
Land elevation.
The following table gives the surface area above 2000 m and 3000 m and the respective percentage on the total area of each canton whose high point is above 2000 metres.
Geology.
The composition of the great tectonic units reflects the history of the formation of the Alps. The rocks from the Helvetic zone on the north and the Austroalpine nappes - Southern Alps on the south come originally from the European and African continent respectively. The rocks of the Penninic nappes belong to the former area of the Briançonnais microcontinent and the Tethys Ocean. The closure of the latter by subduction under the African plate (Piemont Ocean first and Valais Ocean later) preceded the collision between the two plates and the so-called alpine orogeny. The major thrust fault of the Tectonic Arena Sardona in the eastern Glarus Alps gives a visible illustration of mountain-building processes and was therefore declared a UNESCO World Heritage. Another fine example gives the Alpstein area with several visible upfolds of Helvetic zone material.
With some exceptions, the Alps north of Rhône and Rhine are part of the Helvetic Zone and those on the south side are part of the Penninic nappes. The Austroalpine zone concerns almost only the Eastern Alps, with the notable exception of the Matterhorn.
The last glaciations greatly transformed Switzerland’s landscape. Many valleys of the Swiss Alps are U-shaped due to glacial erosion. During the maximum extension of the Würm glaciation (18,000 years ago) the glaciers completely covered the Swiss Plateau, before retreating and leaving remnants only in high mountain areas. In modern times the Aletsch Glacier in the western Bernese Alps is the largest and longest in the Alps, reaching a maximum depth of 900 metres at Konkordia. Along with the Fiescher and Aar Glaciers the region became a UNESCO World Heritage Site in 2001. An effect of the retreat of the Rhine Glacier some 10'000 years ago was the Flims Rockslide, the biggest still visible landslide apparently worldwide.
Environment and climate.
To protect endangered species some sites have been brought under protection. The Swiss National Park in Graubünden was established in 1914 as the first alpine national park. The Entlebuch area was designated a biosphere reserve in 2001. The largest protected area in the country is the Parc Ela, opened in 2006, which covers an area of 600 square kilometres. The Jungfrau-Aletsch Protected Area is the first World Heritage Site in the Alps.
Climate zones.
As the temperature decreases with altitude (0.56°C per 100 metres on yearly average), three different altitudinal zones, each having distinct climate, are found in the Swiss Alps:
The Subalpine zone is the region which lies below the tree line. It is the most important region as it is the largest of the three and contains almost all human settlements as well as the productive areas. The forests are mainly composed by conifers above 1,200-1,400 metres, the deciduous tree forest being confined to lower elevations. The upper limit of the Subalpine zone is located at about 1,800 metres on the north side of the Alps and at about 2,000 metres on the south side. It can however differ in some regions such as the Appenzell Alps (1,600 metres) or the Engadin valley (2,300 metres).
The Alpine zone is situated above the tree line and is clear of trees because of low average temperatures. It contains mostly grass and small plants along with mountain flowers. Below the permafrost limit (at about 2,600 metres), the alpine meadows are often used as pastures. Some villages can still be found on the lowest altitudes such as Riederalp (1,940 m) or Juf (2,130 m). The extend of Alpine zone is limited by the first permanent snow, its altitude greatly vary depending on the location (and orientation), it is comprised between 2,800 and 3,200 metres.
The glacial zone is the area of permanent snow and ice. When the steepness of the slope is not too high it results in an accumulation and compaction of snow, which transforms into ice. The glacier formed then flows down the valley and can reach as far down as 1,500 metres (the Upper Grindelwald Glacier). Where the slopes are too steep, the snow accumulates to form overhanging seracs, which periodically fall off due to the downwards movement of the glacier and cause ice avalanches. The Bernese Alps, Pennine Alps and Mont Blanc Massif contain most of the glaciated areas in the Alps. Except research stations such as the Sphinx Observatory no settlements are to be found in those regions.
Travel and tourism.
Tourism in the Swiss Alps began with the first ascents of the main peaks of the Alps (Jungfrau in 1811, Piz Bernina in 1850, Monte Rosa in 1855, Matterhorn in 1856, Dom in 1858, Weisshorn in 1861) mostly by British mountain climbers accompanied by the local guides. The construction of facilities for tourists started in the mid nineteenth century with the building of hotels and mountain huts (creation of the Swiss Alpine Club in 1863) and the opening of mountain train lines (Mount Rigi in 1873, Mount Pilatus in 1889, Gornergrat in 1898). The Jungfraubahn opened in 1912; it leads to the highest railway station in Europe, the Jungfraujoch.
Summer tourism.
Switzerland enjoys a 62,000-km network of well-maintained trails, of which 23,000 are located in mountainous areas. Many mountains attract a large number of alpinists from around the world, especially the 4000-metre summits and the great north faces (Eiger, Matterhorn and Piz Badile). The large winter resorts are also popular destinations in summer, as most of aerial tramways operate through the year, enabling hikers and mountaineers to reach high altitudes without much effort. The Klein Matterhorn is the highest summit of the European continent to be served by cable car.
Winter tourism.
The major destinations for skiing and other winter sports are located in Valais, Bernese Oberland and Graubünden. Some villages are car-free and can be accessed only with public transports such as Riederalp and Bettmeralp. Zermatt and Saas-Fee have both summer ski areas. The ski season starts from as early as November and runs to as late as May; however, the majority of ski resorts in Switzerland tend to open in December and run through to April. The most visited places are:
Other important destinations on the regional level are Engelberg and Andermatt (Central Switzerland), Leysin (Vaud), Champéry (western Valais) and Samnaun (eastern Graubünden).
The first person to ski in Grindelwald, Switzerland was Englishmen Gerald Fox (who lived at Tone Dale House) 
who put his skis on in his hotel bedroom in 1881 and walked out through the hotel Bar to the slopes wearing them.
Transport.
The Swiss Alps and Switzerland enjoy an extensive transport network. Every mountain village can be reached by public transport, the main companies are:
Most of mountain regions are within 3 hours travel of Switzerland’s main cities and their respective airport. The Engadin Valley in Graubünden is between 4 to 6 hours away from the large cities; the train journey itself, with the panoramic Glacier Express or Bernina Express, is popular with tourists.
The Engadin Airport near St. Moritz at an altitude of 1,707 metres is the highest in Europe.
The crossing of the Alps is a key issue at national and international levels, as the European continent is at places divided by the range. Since the beginning of industrialisation Switzerland has improved its transalpine network; it began in 1882, by building the Gotthard Rail Tunnel, followed in 1906 by the Simplon Tunnel and more recently, in 2007, by the Lötschberg Base Tunnel. The 57-km long Gotthard Base Tunnel is scheduled to open in 2016, and it will finally provide a direct flat rail link through the Alps.
Toponymy.
The different names of the mountains and other landforms are named in the four national languages. The table below gives the most recurrent names.
Also a large number of peaks outside the Alps were named or nicknamed after Swiss mountains, such as the Wetterhorn Peak in Colorado or the Matterhorn Peak in California (see the Matterhorn article for a list of "Matterhorns in the world").
The confluence of the Baltoro Glacier and the Godwin-Austen Glacier south of K2 in the Karakoram range was named after the Konkordiaplatz by European explorers.

</doc>
<doc id="55206" url="http://en.wikipedia.org/wiki?curid=55206" title="Maritime Alps">
Maritime Alps

The Maritime Alps are a mountain range in the southwestern part of the Alps. They form the border between the French region of Provence-Alpes-Côte d'Azur and the Italian regions of Piedmont and Liguria. They are the southernmost part of the Alps.
Geography.
Administratively the range is divided between the Italian provinces of Cuneo and Imperia (eastern slopes) and the French department of Alpes-Maritimes (western slopes).
The Maritime Alps are drained by the rivers Roya, Var and Verdon and their tributaries on the French side; by the Stura di Demonte and other tributaries of the Tanaro and Po on the Italian side. There are many attractive perched villages, such as Belvédère at the entrance to the spectacular Gordolasque valley, some concealing unexpected architectural riches (for example in the south there are numerous churches decorated with murals and altar pieces by primitive Niçois painters).
Borders.
The borders of the Maritime Alps are (anticlockwise):
Peaks.
The main peaks of the Maritime Alps are:
Passes.
The chief passes of the Maritime Alps are:
Nature conservation.
The French "Mercantour National Park" (central area:68,500 ha + peripheral area:140,000 ha ) is part of the Maritime Alps as well as the "Parco naturale delle Alpi Marittime", an Italian regional nature park of 28,455 ha.

</doc>
<doc id="55207" url="http://en.wikipedia.org/wiki?curid=55207" title="Glarus Alps">
Glarus Alps

The Glarus Alps are a mountain range in central Switzerland. They extend from the Oberalp Pass to the Klausen Pass, and are bordered by the Urner Alps to the west, the Lepontine Alps to the south and the Appenzell Alps to the northeast. The eastern part of the Glarus Alps contains a major thrust fault which was declared a geologic UNESCO world heritage site (the Swiss Tectonic Arena Sardona).
Geography.
The main chain of the Glarus Alps can be divided into six minor groups, separated from each other by passes, the lowest of which exceeds 7,500 ft. The westernmost of these is the Crispalt, a rugged range including many peaks of nearly equal height. The highest of these are the Piz Giuv (3,096 m) and Piz Nair. The name "Crispalt" is given to a southern, but secondary, peak of Piz Giuv, measuring 3,070 m. West of the main group is the Rienzenstock, while a northern outlyer culminates in the Bristen. East of the Crispalt, the Kreuzli or Chrüxli Pass separates this from the rather higher mass of the Oberalpstock (3,328 m).
Here occurs a partial break in the continuity of the chain. The crest of the snowy range connecting the Oberalpstock with the Tödi nowhere sinks to 9,000 feet, but makes a sweep convex to the north, forming a semicircular recess, whose numerous torrents are all poured into the Rhine through the Val Russein below Disentis. Two glacier passes lead over this part of the chain — one to west, over the Brunnigletscher to the Maderanertal; the other to the north-east, over the Sand Glacier, to the Linthal.
The Tödi, the highest of the range and of north-eastern Switzerland (3,614 m), is attended by numerous secondary peaks that arise from the extensive snow-fields surrounding the central mountain. A very considerable outlyer, whose chief summits are the Schärhorn and the Gross Windgällen, belonging to the canton of Uri, is connected with the Tödi by the range of the Clariden Grat, north to the Hüfi Glacier. A less important branch encloses the Biferten Glacier, and terminates in the Selbsanft, south of Tierfehd. Towards the valley of the Vorderrhein a high promontory stretches nearly due south from the central peaks of the Tödi, and is crowned by the summit of the Piz Posta Biala. Another considerable ramification of the same mass terminates farther to the east in the peak of the Cavistrau.
The Kisten Pass separates the Tödi group from the Hausstock, whose summit attains 3,158 m; a branch of this latter group forms the range of the Kärpf in the canton of Glarus. The Hausstock is cut off from the rather lower but more extended mass of the Vorab by the Panixer Pass (7,907 ft). Numerous summits, of which the Vorab proper and Piz Grisch are the most important, approach very near, but do not quite attain to 10,000 feet.
The eastern limit of the latter group is marked by the Segnas Pass — the most frequented of those connecting the Canton Glarus with the Vorderrhein — beyond which arises a wide-stretching mass of rock and glacier, which is part of the Glarus thrust and culminates at Piz Sardona. This mass is cleft by a deep valley — the Calfeisental: one branch, culminating in the Pizol (2,844 m), extends east over Pfäfers, while another, including the highest peak of the canton of St. Gallen, the Ringelspitz (3,247 m), runs due east to the low Kunkels Pass (1,357 m), separating this range from the Calanda.
Glaciers.
Main glaciers :
Passes.
The chief passes of the Tödi Range, from the Oberalp Pass to the Klausen Pass, are:

</doc>
<doc id="55208" url="http://en.wikipedia.org/wiki?curid=55208" title="Cottian Alps">
Cottian Alps

The Cottian Alps (French: "Alpes Cottiennes"; Italian: "Alpi Cozie") are a mountain range in the southwestern part of the Alps. They form the border between France (Hautes-Alpes and Savoie) and Italy (Piedmont). The Fréjus Road Tunnel and Fréjus Rail Tunnel between Modane and Susa are important transportation arteries between France (Lyon, Grenoble) and Italy (Turin).
Etymology.
The name "Cottian" comes from "Marcus Julius Cottius", a king of the tribes inhabiting that mountainous region in the 1st century BC. These tribes had previously opposed but later made peace with Julius Caesar. Cottius was succeeded by his son, also named Marcus Julius Cottius, who was granted the title of king by the emperor Claudius.
On his death, Nero annexed his kingdom as the province of Alpes Cottiae.
History.
For a long part of the middle ages Cottian Alps have been divided between Duchy of Savoy, which controlled their northern part and the easternmost slopes, and the Dauphiné, at the time independent from France kingdom. The "Dauphins" held, in addition to the south-western slopes of the range (Briançon and Queyras, nowadays on the French side), also the upper part of some valleys tributaries of the Po River (Valle di Susa, Chisone valley, Varaita Valley). The Alpine territory of Dauphiné, known as "Escartons", used to have a limited autonomy and to elect his own parliament. This semi-autonomuos status lasted also after the annexion of Dauphiné to France (1349), and was only abolished in 1713 due to the Treaty of Utrecht, which assigned to House of Savoy all the mountain area on the eastern side of the Cottian Alps.
After the treaty annexing Nice and Savoy to France, signed in Turin on March 1860 (Treaty of Turin), the north-western slopes of the range became part of the French republic.
Two eastern valleys of the Cottian Alps (Pellice and Germanasca) have been for centuries a kind of sanctuary for the Waldensians, a Christian movement founded by Peter Waldo and which was persecuted as heretical from the 12th century onwards.
Geography.
Administratively the range is divided between the Italian provinces of Cuneo and Turin (eastern slopes) and the French departments of Savoie, Hautes-Alpes and Alpes-de-Haute-Provence (western slopes).
The Cottian Alps are drained by the rivers Durance and Arc and their tributaries on the French side; by the Dora Riparia and other tributaries of the Po on the Italian side.
Borders.
Cottian Alps' borders are (clockwise):
Peaks.
The chief peaks of the Cottian Alps are:
Passes.
The chief passes of the Cottian Alps are:

</doc>
<doc id="55209" url="http://en.wikipedia.org/wiki?curid=55209" title="Pennine Alps">
Pennine Alps

The Pennine Alps (also: "Valais Alps", formerly called "Alpes Poeninae") are a mountain range in the western part of the Alps. They are located in Switzerland (Valais) and Italy (Piedmont and the Aosta Valley). They are not to be confused with the Pennines.
Geography.
The Italian side is drained by the rivers Dora Baltea, Sesia and Toce, tributaries of the Po. The Swiss side is drained by the Rhône River.
The Great St Bernard Tunnel, under the Great St Bernard Pass, leads from Martigny, Switzerland to Aosta.
Morphology.
The main chain (watershed between the Mediterranean Sea and Adriatic Sea) runs from west to east on the border between Italy (south) and Switzerland (north). From Mont Vélan, the first high summit east of St Bernard Pass, the chain rarely goes below 3000 metres and contains many four-thousanders such as Matterhorn or Monte Rosa. The valleys are quite similar on both side of the border, being generally oriented perpendicular to the main chain and descending progressively into the Rhône Valley on the north and the Aosta Valley on the south. Unlike many other mountain ranges, the higher peaks are often located outside the main chain and found themselves between the northern valleys (Grand Combin, Weisshorn, Mischabel, Weissmies).
Peaks.
The chief peaks of the Pennine Alps are:
Glaciers.
Main glaciers:
Passes.
The chief passes of the Pennine Alps are:
Nature conservation.
Some regional nature parks, like the "Parco Naturale Alta Valsesia" (6,511 ha - Piedmont, IT), the "Riserva Naturale Mont Mars" (390 ha - Aosta Valley, IT) and the "Regional park of Binn valley" (15,891 ha - Valais,CH), have been established on both sides of the main water divide.

</doc>
<doc id="55210" url="http://en.wikipedia.org/wiki?curid=55210" title="Bernese Alps">
Bernese Alps

The Bernese Alps are a mountain range of the Alps, located in western Switzerland. Although the name suggests that they are located in the Bernese Oberland region of the canton of Bern, portions of the Bernese Alps are in the adjacent cantons of Valais, Fribourg and Vaud, the latter being usually named "Fribourg Alps" and "Vaud Alps" respectively. The highest mountain in the range, the Finsteraarhorn, is also the highest point in the canton of Bern.
The Rhône valley separates them from the Chablais Alps in the west and from the Pennine Alps in the south; the upper Rhône valley separate them from the Lepontine Alps in the south-east; the Grimsel Pass and the Aar valley separates them from the Uri Alps in the east and from the Emmental Alps in the north; their northwestern edge is not so well defined, describing a line roughly from Lake Geneva to Lake Thun.
The Bernese Alps are drained by the river Aar and its tributary Saane in the north, the Rhône in the south and the Reuss in the east.
Geography.
One of the most considerable Alpine ranges, the Bernese Alps extend from the gorge of Saint-Maurice, through which the Rhone finds its way to Lake Geneva, to the Grimsel Pass or, depending on the definition, to the river Reuss (thus including the Uri Alps). The principal ridge, a chain that runs 100 km from west (Dent de Morcles) to east (Sidelhorn), whose highest peak is the Finsteraarhorn, forms the watershed between the cantons of Berne and Valais. Except for the westernmost part, it is also the watershed between the Rhine (North Sea) and the Rhone (Mediterranean Sea). This chain is not centered inside the range but lies close (10 to 15 km) to the Rhone river on the south. This makes a large difference between the south, where the lateral short valleys descend abruptly into the deep trench forming the valley of the Rhone and the north, where the Bernese Alps extends through a great part of the canton of Berne (Bernese Oberland), throwing out branches to the west into the adjoining cantons of Vaud and Fribourg. There the mountains progressively become lower and disappear into the hilly Swiss Plateau.
The main chain west of Gemmi Pass consists mainly of a few large prominent summits (as the Wildhorn) slightly above 3000 m, generally covered by glaciers. On the eastern part, the main chain became suddenly wider and the peaks reach over 4000 m, in the most glaciated part of the Alps.
A characteristic in the orography of the Bernese Alps is, that whereas the western of that chain consists of a single series of summits with comparatively short projecting buttresses, the higher group presents a series of longitudinal ridges parallel to the axis of the main chain, and separated from each other by deep valleys that form the channels of great glaciers. Thus the Tschingel Glacier and the Kander Glacier, separate the portion of the main range lying between the Gemmi Pass and the Mittaghorn from the equally high parallel range of the Doldenhorn and Blümlisalp on its northern side. To the south the same portion of the main range is divided from the still higher parallel range whose summits are the Aletschhorn and the Bietschhorn by the Lötschental and the Lötschenlücke. To this again succeeds the deep trench through which the lower part of the Aletsch Glacier flows down to the Rhone, enclosed by the minor ridge that culminates at the Eggishorn.
It is in the central and eastern portions of the range only that crystalline rocks make their appearance; the western part is composed almost exclusively of sedimentary deposits, and the secondary ridges extending through Berne and the adjoining cantons are formed of jurassic, cretaceous, or eocene strata.
Exploration.
The beauty of the scenery, and the facilities offered to travellers by the general extension of mountain railways, make the northern side of the range, the Bernese Oberland, one of the portions of the Alps most visited by tourists. Since strangers first began to visit the Alps, the names of Grindelwald, Lauterbrunnen, and Interlaken have been famous. But unlike many other Alpine regions, which have been left to be explored by strangers, this region has been long visited by Swiss travellers and men of sciences. Among them were the brother Meyer of Aarau and Franz Joseph Hugi. They have explored most of the mountain ranges not very difficult of access, and, further than this, have attained most of the higher summits. In 1841, Louis Agassiz, with several scientific friends, established a temporary station on the Unteraar Glacier, and, along with scientific observations on the glaciers, started a series of expeditions. The works of Desor and Gottlieb Studer have been followed by several other publications that bear testimony to Swiss mountaineering activity. Notwithstanding the activity of their predecessors, the members of the English Alpine Club have found scope for further exploits, amongst which may be reckoned the first ascents of the Aletschhorn and the Schreckhorn, and the still more arduous enterprise of crossing the range by passes, such as the Jungfraujoch and Eigerjoch, which are considered amongst the most difficult in the Alps.
Jungfrau-Aletsch area.
The Jungfrau-Aletsch area is located in the eastern Bernese Alps in the most glaciated region of the Alps. It was inscribed as a UNESCO World Heritage Site ("Swiss Alps Jungfrau-Aletsch") in 2001 and further expanded in 2007. Its name comes from the Aletsch Glacier and the two summits of the Jungfrau and Bietschhorn, which constitute some of the most impressive features of the site. The actual site (after the extension) includes other large glacier valleys such as the Fiescher Glacier and the Aar Glaciers.
List of peaks.
The chief peaks of the Bernese Alps are:
Glaciers.
Main glaciers:
List of passes.
The chief passes of the Bernese Alps are:

</doc>
<doc id="55211" url="http://en.wikipedia.org/wiki?curid=55211" title="Graian Alps">
Graian Alps

The Graian Alps (Italian: "Alpi Graie"; French: "Alpes Grées") are a mountain range in the western part of the Alps.
Etymology.
The name "Graie" comes from the "Graioceli" Celtic tribe, which dwelled in the area surrounding the Mont Cenis pass and the Viù valley.
Other sources claim that the name comes from the Celtic "Graig" meaning rock/stone, literally the Rocky Mountains
Geography.
The Graian Alps are located in France (Rhône-Alpes), Italy (Piedmont and the Aosta Valley), and Switzerland (western Valais).
Their French side of the Graian Alps is drained by the river Isère (Tarentaise valley) and its tributary Arc (Maurienne valley), and by the Arve. The Italian side is drained by the rivers Dora Baltea, Orco and Stura di Lanzo, tributaries of the Po.
The Graian Alps can also be divided into the following four groups:
Peaks.
The main peaks of the Graian Alps are:
Passes.
The main passes of the Graian Alps are shown in the table below. The group in which the pass is located is indicated with "MB" for Mont Blanc group, "C" for Central group, "E" for Eastern group, and "W" for Western group.
Nature conservation.
The western group contains the Vanoise National Park, established in 1972 and covering 1250 km²; the eastern group contains the Gran Paradiso National Park, the oldest Italian national park.
Also on the Italian side is located the "Parco Regionale del Monte Avic", a nature park of 5,747 ha established by Regione Valle d'Aosta.

</doc>
<doc id="55212" url="http://en.wikipedia.org/wiki?curid=55212" title="Newton's laws of motion">
Newton's laws of motion

Newton's laws of motion are three physical laws that together laid the foundation for classical mechanics. They describe the relationship between a body and the forces acting upon it, and its motion in response to said forces. They have been expressed in several different ways over nearly three centuries, and can be summarised as follows.
The three laws of motion were first compiled by Isaac Newton in his "Philosophiæ Naturalis Principia Mathematica" ("Mathematical Principles of Natural Philosophy"), first published in 1687. Newton used them to explain and investigate the motion of many physical objects and systems. For example, in the third volume of the text, Newton showed that these laws of motion, combined with his law of universal gravitation, explained Kepler's laws of planetary motion.
Overview.
Newton's laws are applied to objects which are idealised as single point masses, in the sense that the size and shape of the object's body are neglected to focus on its motion more easily. This can be done when the object is small compared to the distances involved in its analysis, or the deformation and rotation of the body are of no importance. In this way, even a planet can be idealised as a particle for analysis of its orbital motion around a star.
In their original form, Newton's laws of motion are not adequate to characterise the motion of rigid bodies and deformable bodies. Leonhard Euler in 1750 introduced a generalisation of Newton's laws of motion for rigid bodies called the Euler's laws of motion, later applied as well for deformable bodies assumed as a continuum. If a body is represented as an assemblage of discrete particles, each governed by Newton's laws of motion, then Euler's laws can be derived from Newton's laws. Euler's laws can, however, be taken as axioms describing the laws of motion for extended bodies, independently of any particle structure.
Newton's laws hold only with respect to a certain set of frames of reference called Newtonian or inertial reference frames. Some authors interpret the first law as defining what an inertial reference frame is; from this point of view, the second law only holds when the observation is made from an inertial reference frame, and therefore the first law cannot be proved as a special case of the second. Other authors do treat the first law as a corollary of the second. The explicit concept of an inertial frame of reference was not developed until long after Newton's death.
In the given interpretation mass, acceleration, momentum, and (most importantly) force are assumed to be externally defined quantities. This is the most common, but not the only interpretation of the way one can consider the laws to be a definition of these quantities.
Newtonian mechanics has been superseded by special relativity, but it is still useful as an approximation when the speeds involved are much slower than the speed of light.
Newton's first law.
The first law states that if the net force (the vector sum of all forces acting on an object) is zero, then the velocity of the object is constant. Velocity is a vector quantity which expresses both the object's speed and the direction of its motion; therefore, the statement that the object's velocity is constant is a statement that both its speed and the direction of its motion are constant.
The first law can be stated mathematically as
Consequently,
This is known as "uniform motion". An object "continues" to do whatever it happens to be doing unless a force is exerted upon it. If it is at rest, it continues in a state of rest (demonstrated when a tablecloth is skilfully whipped from under dishes on a tabletop and the dishes remain in their initial state of rest). If an object is moving, it continues to move without turning or changing its speed. This is evident in space probes that continually move in outer space. Changes in motion must be imposed against the tendency of an object to retain its state of motion. In the absence of net forces, a moving object tends to move along a straight line path indefinitely.
Newton placed the first law of motion to establish frames of reference for which the other laws are applicable. The first law of motion postulates the existence of at least one frame of reference called a Newtonian or inertial reference frame, relative to which the motion of a particle not subject to forces is a straight line at a constant speed. Newton's first law is often referred to as the "law of inertia". Thus, a condition necessary for the uniform motion of a particle relative to an inertial reference frame is that the total net force acting on it is zero. In this sense, the first law can be restated as:
"In every material universe, the motion of a particle in a preferential reference frame Φ is determined by the action of forces whose total vanished for all times when and only when the velocity of the particle is constant in Φ. That is, a particle initially at rest or in uniform motion in the preferential frame Φ continues in that state unless compelled by forces to change it."
Newton's laws are valid only in an inertial reference frame. Any reference frame that is in uniform motion with respect to an inertial frame is also an inertial frame, i.e. Galilean invariance or the principle of Newtonian relativity.
Newton's second law.
The second law states that the net force on an object is equal to the rate of change (that is, the "derivative") of its linear momentum p in an inertial reference frame:
The second law can also be stated in terms of an object's acceleration. Since Newton's second law is only valid for constant-mass systems, mass can be taken outside the differentiation operator by the constant factor rule in differentiation. Thus,
where F is the net force applied, "m" is the mass of the body, and a is the body's acceleration. Thus, the net force applied to a body produces a proportional acceleration. In other words, if a body is accelerating, then there is a force on it.
Consistent with the first law, the time derivative of the momentum is non-zero when the momentum changes direction, even if there is no change in its magnitude; such is the case with uniform circular motion. The relationship also implies the conservation of momentum: when the net force on the body is zero, the momentum of the body is constant. Any net force is equal to the rate of change of the momentum.
Any mass that is gained or lost by the system will cause a change in momentum that is not the result of an external force. A different equation is necessary for variable-mass systems (see below).
Newton's second law requires modification if the effects of special relativity are to be taken into account, because at high speeds the approximation that momentum is the product of rest mass and velocity is not accurate.
Impulse.
An impulse J occurs when a force F acts over an interval of time Δ"t", and it is given by
Since force is the time derivative of momentum, it follows that
This relation between impulse and momentum is closer to Newton's wording of the second law.
Impulse is a concept frequently used in the analysis of collisions and impacts.
Variable-mass systems.
Variable-mass systems, like a rocket burning fuel and ejecting spent gases, are not closed and cannot be directly treated by making mass a function of time in the second law; that is, the following formula is wrong:
The falsehood of this formula can be seen by noting that it does not respect Galilean invariance: a variable-mass object with F = 0 in one frame will be seen to have F ≠ 0 in another frame.
The correct equation of motion for a body whose mass "m" varies with time by either ejecting or accreting mass is obtained by applying the second law to the entire, constant-mass system consisting of the body and its ejected/accreted mass; the result is
where u is the velocity of the escaping or incoming mass relative to the body. From this equation one can derive the equation of motion for a varying mass system, for example, the Tsiolkovsky rocket equation.
Under some conventions, the quantity u d"m"/d"t" on the left-hand side, which represents the advection of momentum, is defined as a force (the force exerted on the body by the changing mass, such as rocket exhaust) and is included in the quantity F. Then, by substituting the definition of acceleration, the equation becomes F = "m"a.
Newton's third law.
The third law states that all forces between two objects exist in equal magnitude and opposite direction: if one object "A" exerts a force F"A" on a second object "B", then "B" simultaneously exerts a force F"B" on "A", and the two forces are equal and opposite: F"A" = −F"B". The third law means that all forces are "interactions" between different bodies, and thus that there is no such thing as a unidirectional force or a force that acts on only one body. This law is sometimes referred to as the "action-reaction law", with F"A" called the "action" and F"B" the "reaction". The action and the reaction are simultaneous, and it does not matter which is called the "action" and which is called "reaction"; both forces are part of a single interaction, and neither force exists without the other.
The two forces in Newton's third law are of the same type (e.g., if the road exerts a forward frictional force on an accelerating car's tires, then it is also a frictional force that Newton's third law predicts for the tires pushing backward on the road).
From a conceptual standpoint, Newton's third law is seen when a person walks: they push against the floor, and the floor pushes against the person. Similarly, the tires of a car push against the road while the road pushes back on the tires—the tires and road simultaneously push against each other. In swimming, a person interacts with the water, pushing the water backward, while the water simultaneously pushes the person forward—both the person and the water push against each other. The reaction forces account for the motion in these examples. These forces depend on friction; a person or car on ice, for example, may be unable to exert the action force to produce the needed reaction force.
History.
Newton's 1st Law.
From the original Latin of Newton's "Principia":
Translated to English, this reads:
The ancient Greek philosopher Aristotle had the view that all objects have a natural place in the universe: that heavy objects (such as rocks) wanted to be at rest on the Earth and that light objects like smoke wanted to be at rest in the sky and the stars wanted to remain in the heavens. He thought that a body was in its natural state when it was at rest, and for the body to move in a straight line at a constant speed an external agent was needed continually to propel it, otherwise it would stop moving. Galileo Galilei, however, realised that a force is necessary to change the velocity of a body, i.e., acceleration, but no force is needed to maintain its velocity. In other words, Galileo stated that, in the "absence" of a force, a moving object will continue moving. The tendency of objects to resist changes in motion was what Galileo called "inertia". This insight was refined by Newton, who made it into his first law, also known as the "law of inertia"—no force means no acceleration, and hence the body will maintain its velocity. As Newton's first law is a restatement of the law of inertia which Galileo had already described, Newton appropriately gave credit to Galileo.
The law of inertia apparently occurred to several different natural philosophers and scientists independently, including Thomas Hobbes in his "Leviathan". The 17th century philosopher and mathematician René Descartes also formulated the law, although he did not perform any experiments to confirm it.
Newton's 2nd Law.
Newton's original Latin reads:
This was translated quite closely in Motte's 1729 translation as:
According to modern ideas of how Newton was using his terminology, this is understood, in modern terms, as an equivalent of:
"The change of momentum of a body is proportional to the impulse impressed on the body, and happens along the straight line on which that impulse is impressed.
This may be expressed by the formula F = p', where p' is the time derivative of the momentum p. This equation can be seen clearly in the Wren Library of Trinity College, Cambridge, in a glass case in which Newton's manuscript is open to the relevant page.
Motte's 1729 translation of Newton's Latin continued with Newton's commentary on the second law of motion, reading:
"If a force generates a motion, a double force will generate double the motion, a triple force triple the motion, whether that force be impressed altogether and at once, or gradually and successively. And this motion (being always directed the same way with the generating force), if the body moved before, is added to or subtracted from the former motion, according as they directly conspire with or are directly contrary to each other; or obliquely joined, when they are oblique, so as to produce a new motion compounded from the determination of both.
The sense or senses in which Newton used his terminology, and how he understood the second law and intended it to be understood, have been extensively discussed by historians of science, along with the relations between Newton's formulation and modern formulations.
Newton's 3rd Law.
Translated to English, this reads:
Newton's Scholium (explanatory comment) to this law:
Whatever draws or presses another is as much drawn or pressed by that other. If you press a stone with your finger, the finger is also pressed by the stone. If a horse draws a stone tied to a rope, the horse (if I may so say) will be equally drawn back towards the stone: for the distended rope, by the same endeavour to relax or unbend itself, will draw the horse as much towards the stone, as it does the stone towards the horse, and will obstruct the progress of the one as much as it advances that of the other. If a body impinges upon another, and by its force changes the motion of the other, that body also (because of the equality of the mutual pressure) will undergo an equal change, in its own motion, toward the contrary part. The changes made by these actions are equal, not in the velocities but in the motions of the bodies; that is to say, if the bodies are not hindered by any other impediments. For, as the motions are equally changed, the changes of the velocities made toward contrary parts are reciprocally proportional to the bodies. This law takes place also in attractions, as will be proved in the next scholium.
In the above, as usual, "motion" is Newton's name for momentum, hence his careful distinction between motion and velocity.
Newton used the third law to derive the law of conservation of momentum; from a deeper perspective, however, conservation of momentum is the more fundamental idea (derived via Noether's theorem from Galilean invariance), and holds in cases where Newton's third law appears to fail, for instance when force fields as well as particles carry momentum, and in quantum mechanics. 
Importance and range of validity.
Newton's laws were verified by experiment and observation for over 200 years, and they are excellent approximations at the scales and speeds of everyday life. Newton's laws of motion, together with his law of universal gravitation and the mathematical techniques of calculus, provided for the first time a unified quantitative explanation for a wide range of physical phenomena.
These three laws hold to a good approximation for macroscopic objects under everyday conditions. However, Newton's laws (combined with universal gravitation and classical electrodynamics) are inappropriate for use in certain circumstances, most notably at very small scales, very high speeds (in special relativity, the Lorentz factor must be included in the expression for momentum along with rest mass and velocity) or very strong gravitational fields. Therefore, the laws cannot be used to explain phenomena such as conduction of electricity in a semiconductor, optical properties of substances, errors in non-relativistically corrected GPS systems and superconductivity. Explanation of these phenomena requires more sophisticated physical theories, including general relativity and quantum field theory.
In quantum mechanics concepts such as force, momentum, and position are defined by linear operators that operate on the quantum state; at speeds that are much lower than the speed of light, Newton's laws are just as exact for these operators as they are for classical objects. At speeds comparable to the speed of light, the second law holds in the original form F = dp/d"t", where F and p are four-vectors.
Relationship to the conservation laws.
In modern physics, the laws of conservation of momentum, energy, and angular momentum are of more general validity than Newton's laws, since they apply to both light and matter, and to both classical and non-classical physics.
This can be stated simply, "Momentum, energy and angular momentum cannot be created or destroyed."
Because force is the time derivative of momentum, the concept of force is redundant and subordinate to the conservation of momentum, and is not used in fundamental theories (e.g., quantum mechanics, quantum electrodynamics, general relativity, etc.). The standard model explains in detail how the three fundamental forces known as gauge forces originate out of exchange by virtual particles. Other forces such as gravity and fermionic degeneracy pressure also arise from the momentum conservation. Indeed, the conservation of 4-momentum in inertial motion via curved space-time results in what we call gravitational force in general relativity theory. Application of space derivative (which is a momentum operator in quantum mechanics) to overlapping wave functions of pair of fermions (particles with half-integer spin) results in shifts of maxima of compound wavefunction away from each other, which is observable as "repulsion" of fermions.
Newton stated the third law within a world-view that assumed instantaneous action at a distance between material particles. However, he was prepared for philosophical criticism of this action at a distance, and it was in this context that he stated the famous phrase "I feign no hypotheses". In modern physics, action at a distance has been completely eliminated, except for subtle effects involving quantum entanglement. However in modern engineering in all practical applications involving the motion of vehicles and satellites, the concept of action at a distance is used extensively.
The discovery of the second law of thermodynamics by Carnot in the 19th century showed that every physical quantity is not conserved over time, thus disproving the validity of inducing the opposite metaphysical view from Newton's laws. Hence, a "steady-state" worldview based solely on Newton's laws and the conservation laws does not take entropy into account.

</doc>
<doc id="55214" url="http://en.wikipedia.org/wiki?curid=55214" title="Tim Brooke-Taylor">
Tim Brooke-Taylor

Timothy Julian Brooke-Taylor OBE (born 17 July 1940) is an English comic actor. He became active in performing in comedy sketches while at Cambridge University, and became President of the Footlights club, touring internationally with the Footlights revue in 1964. Becoming wider known to the public for his work on BBC Radio with "I'm Sorry, I'll Read That Again", he moved into television with "At Last the 1948 Show" working together with old Cambridge friends John Cleese and Graham Chapman. He is most well known as a member of The Goodies, starring in the TV series throughout the 1970s and picking up international recognition in Australia and New Zealand. He has also appeared as an actor in various sitcoms, and has been a panellist on "I'm Sorry I Haven't a Clue" for nearly 40 years.
Early life and education.
Brooke-Taylor was born in Buxton, Derbyshire, England, the grandson of Francis Pawson, a parson who played centre forward for England's football team in the 1880s. His mother was an international lacrosse player and his father a solicitor. He was expelled from primary school at the early age of five and a half. Brooke-Taylor was then schooled at Thorn Leigh Pre-Preparatory School, Holm Leigh Preparatory School (where he won a cup for his prowess as a bowler in the school cricket team) and Winchester College which he left with seven O-levels and two A-levels in English and History. After teaching for a year at a preparatory school in Berkhamsted and a term back at Holm Leigh School as a teacher, he studied at Pembroke College at the University of Cambridge. There he read Economics and Politics before changing to read Law, and mixed with other budding comedians, including John Cleese, Graham Chapman, Bill Oddie and Jonathan Lynn in the prestigious Cambridge University Footlights Club (of which Brooke-Taylor became President in 1963).
The Footlights Club revue, "A Clump of Plinths" was so successful during its Edinburgh Festival Fringe run, that the show was renamed as "Cambridge Circus" and the revue transferred to the West End in London, and then later taken to both New Zealand and to Broadway in the United States in September 1964. He was also active in the Pembroke College drama society, the Pembroke Players.
Career.
Brooke-Taylor moved swiftly into BBC Radio with the fast-paced comedy show "I'm Sorry, I'll Read That Again" in which he performed and co-wrote. As the screeching eccentric Lady Constance de Coverlet, he could be relied upon to generate the loudest audience response of many programmes in this long-running series merely with her unlikely catchphrase "Did somebody call?" uttered after a comic and transparent feed-line, as their adventure story reached its climax or cliffhanger ending. Other members of "I'm Sorry, I'll Read That Again" were John Cleese, Bill Oddie, Graeme Garden, David Hatch and Jo Kendall.
In the mid-'60s, Brooke-Taylor performed in the TV series "On the Braden Beat" with Canadian Bernard Braden, taking over the slot then-recently vacated by Peter Cook in his guise as E.L. Wisty. Brooke-Taylor played a reactionary right-wing city gent who believed he was the soul of tolerance.
In 1967, Brooke-Taylor became a writer/performer on the television comedy series "At Last the 1948 Show", with John Cleese, Graham Chapman and Marty Feldman. The "Four Yorkshiremen" sketch was co-written by the four writers/performers of the series. The sketch was one of the few which survived the destruction of the series (by the tapes being wiped), by David Frost's Paradine Productions (which produced the series), and the sketch appears on the DVD of "At Last the 1948 Show". The "Four Yorkshiremen" sketch has also been performed during Amnesty concert performances (by members of Monty Python – occasionally including other comedians and actors in place of Monty Python regulars – notably Rowan Atkinson and Alan Rickman), as well as being performed during "Monty Python Live at the Hollywood Bowl" and on other Monty Python shows. Footage of Tim Brooke-Taylor and John Cleese, from "At Last the 1948 Show", was shown on the documentary special "".
Brooke-Taylor also took part in Frost's pilot programme "How to Irritate People" in 1968, designed to sell what would later be recognised as the Monty Python style of comedy to the American market. Many of the sketches were later revived in the Monty Python TV series, notably the job interview sketch where Brooke-Taylor played a nervous interviewee tormented by interviewer John Cleese. The programme was also notable as the first collaboration of John Cleese and Michael Palin.
In 1968–69, Brooke-Taylor was also a cast member and writer on the television comedy series "Marty" starring Marty Feldman, with John Junkin and Roland MacLeod. A compilation of the two series of "Marty" has been released on a BBC DVD with the title of "The Best of Marty Feldman". During this period Brooke-Taylor appeared as two characters in the film "One Man Band" directed by Orson Welles, however the project was never completed and remains unreleased.
At around the same time, Brooke-Taylor made two series of "Broaden Your Mind" with Graeme Garden (and Bill Oddie joining the series for the second season). Describing itself as "An Encyclopedia of the Air", this series was a string of comedy sketches (often lifted from "I'm Sorry I'll Read That Again"), linked (loosely) by a weekly running theme. Unfortunately, nothing but a few minutes of film inserts exist for this programme, though home-made off-air audio recordings survive for both seasons. 
The success of "Broaden Your Mind" led to the commissioning of "The Goodies", also with Bill Oddie and Graeme Garden. First transmitted on BBC2 in November 1970, "The Goodies" was a huge television success, running for over a decade on both BBC TV and (in its final year) UK commercial channel London Weekend Television, spawning many spin-off books and successful records.
During the run of the Goodies, Brooke-Taylor took part in the BBC radio series "Hello, Cheeky!", a bawdy stand up comedy show also starring Barry Cryer and John Junkin. The series transferred to television briefly, produced by the UK commercial franchise Yorkshire Television.
He also appeared on television in British sitcoms, including "You Must Be the Husband" with Diane Keen, "His and Hers" with Madeline Smith, and "Me and My Girl" with Richard O'Sullivan.
Brooke-Taylor also appeared regularly in advertisements, including the Christmas commercials for the Brentford Nylons chain of fabric stores, and in a public information film for the now-defunct E111 form.
In 1971, he played the short, uncredited role of a computer scientist in the film "Willy Wonka & the Chocolate Factory". After the end of "The Goodies" on UK television, Brooke-Taylor also worked again with Garden and Oddie on the animated television comedy series "Bananaman", in which Brooke-Taylor was the narrator, as well as voicing the characters of King Zorg of the Nurks, Eddie the Gent, Auntie and Appleman. He also lent his voice to the children's TV series "Gideon".
Tim appeared, with Bill Oddie and Graeme Garden, in the Amnesty International show "A Poke in the Eye (With a Sharp Stick)" (during which they sang their hit song "Funky Gibbon"), and also appeared in the Amnesty International show "The Secret Policeman's Other Ball" in the sketches "Top of the Form" (with John Cleese, Graham Chapman, John Bird, John Fortune, Rowan Atkinson and Griff Rhys Jones), and "Cha Cha Cha" (with John Cleese and Graham Chapman).
Brooke-Taylor, Graeme Garden, and Bill Oddie also appeared on "Top of the Pops" with their song "Funky Gibbon". Brooke-Taylor also appeared with Graeme Garden in the theatre production of "The Unvarnished Truth".
Other BBC radio programmes in which Brooke-Taylor played a part include the self-styled "antidote to panel games" "I'm Sorry I Haven't a Clue" which started in 1972, and Tim still appears regularly. On 18 February 1981 Brooke-Taylor was the subject of Thames Television's "This Is Your Life".
Graeme Garden was a regular team captain on the political satire game show "If I Ruled the World". Tim Brooke-Taylor appeared as a guest in one episode, and, during the game "I Couldn't Disagree More" he proposed that it was high time "The Goodies" episodes were repeated. Garden was obliged by the rules of the game to rebut this statement, and replied "I couldn't disagree more... it was time to repeat them ten, fifteen years ago." This was followed by uproarious applause from the studio audience.
In 2004, Brooke-Taylor and Graeme Garden were co-presenters of Channel 4's daytime game show, "Beat the Nation", in which they indulged in usual game show "banter", but took the quiz itself seriously. He has appeared on stage in Australia and England, usually as a middle class Englishman. Around 1982, he branched out into pantomime as the Dame in "Dick Whittington". He is also the author (and co-author) of several humorous books based mainly around his radio and television work and the sports of golf and cricket. He also took part in the "Pro-Celebrity Golf" television series (opposite Bruce Forsyth). Brooke-Taylor appeared on the premiere episode of the BBC golf-based game show "Full Swing".
Brooke-Taylor was appointed Officer of the Order of the British Empire (OBE) in the 2011 Birthday Honours.
Personal life.
Brooke-Taylor married Christine Weadon in 1968 and they have two sons, Ben and Edward. He lives in Berkshire.
Brooke-Taylor was appointed Officer of the Order of the British Empire (OBE) in the 2011 Birthday Honours for services to light entertainment.
Bibliography.
As sole author
As co-author
Other information.
Tim Brooke-Taylor served the University of St Andrews as Rector between 1979 and 1982.
He is an honorary Vice-President of Derby County F.C..

</doc>
<doc id="55215" url="http://en.wikipedia.org/wiki?curid=55215" title="Great Artesian Basin">
Great Artesian Basin

The Great Artesian Basin, located within Australia, is the largest and deepest artesian basin in the world, stretching over 1700000 km2, with measured temperatures ranging from 30 -. The basin provides the only reliable source of fresh water through much of inland Australia.
The Basin underlies 23% of the continent, including the states and territories of Queensland (most of), the Northern Territory (the south-east corner of), South Australia (the north-east part of), and New South Wales (northern part of). The basin is 3000 m deep in places and is estimated to contain 64900 km3 of groundwater. The Great Artesian Basin Coordinating Committee (GABCC) coordinates activity between the various levels of government and community organisations.
Physiography.
This area is one of the distinct physiographic provinces of the larger 
East Australian Basins division, and includes the smaller 
Wilcannia Threshold physiographic section.
Geology.
The water of the GAB is held in a sandstone layer laid down by continental erosion of higher ground during the Triassic, Jurassic, and early Cretaceous periods. During a time when much of what is now inland Australia was below sea level, the sandstone was then covered by a layer of marine sedimentary rock shortly afterward, which formed a confining layer, thus trapping water in the sandstone aquifer. The eastern edge of the basin was uplifted when the Great Dividing Range formed. The other side was created from the landforms of the Central Eastern Lowlands and the Great Western Plateau to the west.
Most recharge water enters the rock formations from relatively high ground near the eastern edge of the basin (in Queensland and New South Wales) and very gradually flows toward the south and west. A much smaller amount enters along the western margin in arid central Australia, flowing to the south and east. Because the sandstones are permeable, water gradually makes its way through the pores between the sand grains, flowing at a rate of one to five metres per year.
Discharge water eventually exits through a number of springs and seeps, mostly in the southern part of the basin. The age of the groundwater determined by carbon-14 and chlorine-36 measurements combined with hydraulic modelling ranges from several thousand years for the recharge areas in the north to nearly 2 million years in the south-western discharge zones.
Water source.
Prior to European occupation, waters of the GAB discharged through mound springs, many in arid South Australia. These springs supported a variety of endemic invertebrates (molluscs, for example), and supported extensive Aboriginal communities and trade routes. After the arrival of Europeans, they enabled early exploration and faster communications between southeastern Australia and Europe via the Australian Overland Telegraph Line. The Great Artesian Basin became an important water supply for cattle stations, irrigation, and livestock and domestic usage, and is a vital life line for rural Australia. To tap it, water wells are drilled down to a suitable rock layer, where the pressure of the water forces it up, mostly without pumping.
The discovery and use of water held underground in the Great Artesian Basin opened up thousands of square miles of country away from rivers in inland New South Wales, Queensland, and South Australia, previously unavailable for pastoral activities. European discovery of the basin dates from 1878 when a shallow bore near Bourke produced flowing water. There were similar discoveries in 1886 at Back Creek east of Barcaldine, and in 1887 near Cunnamulla.
In essence, water extraction from the GAB is a mining operation, with recharge much less than current extraction rates. In 1915, there were 1,500 bores providing 2000 Ml of water per day, but today the total output has dropped to 1500 Ml per day. This included just under 2000 freely flowing bores and more than 9000 that required mechanical power to bring water to the surface. Many bores are unregulated or abandoned, resulting in considerable water wastage. These problems have existed for many decades, and in January 2007 the Australian Commonwealth Government announced additional funding in an attempt to bring them under control. However, many of the mound springs referred to above have dried up due to a drop in water pressure, probably resulting in extinction of several invertebrate species.
In addition, the basin has provided water via a 1.2 km deep bore for a geothermal power station at Birdsville. The heated water is 98 °C (208 °F) and provides 25% of the town's needs. Ergon Energy is expanding the 80 kW plant to completely meet Birdsville's electricity requirements.
Whole of Basin management.
As the Great Artesian Basin underlies parts of Queensland, New South Wales, South Australia and the Northern Territory, which each operate under different legislative frameworks, policies and resource management approaches, a coordinated "whole-of-Basin" approach to the management of this important natural resource is required. The Great Artesian Basin Coordinating Committee (GABCC) provides advice from community organisations and agencies to State, Territory and Australian Government Ministers on efficient, effective and sustainable whole-of-Basin resource management and to coordinate activity between stakeholders.
Membership of the Committee comprises all State, Territory, and Australian Government agencies with responsibilities for management of parts of the Great Artesian Basin, community representatives nominated by agencies; and sector representatives.
The GABCC website provides up-to-date information and links to other sites that discuss the Basin can be accessed from there.
Current scientific thinking.
A comprehensive background to the Great Artesian Basin, including an overview of the nature of the Basin, the extraction of water and the impacts of that extraction, can be found in the Great Artesian Basin Resource Study, developed by the GABCC to support the Great Artesian Basin Strategic Management Plan.
Potential depletion and pollution due to coal seam gas extraction.
In 2011, ABC TV "Four Corners" revealed that significant concerns were being expressed about depletion and chemical damage to the Basin as a result of coal seam gas extraction. In one incident, reported in the program, the Queensland Gas Company (QGC) "fracked" its Myrtle 3 well connecting the Springbok aquifer to the coal seam below (the Walloon Coal Measures) in 2009. A local farmer was concerned that the process may have released 130 l of a potentially toxic chemical into the Basin. QGC admitted the incident, but "did not alert authorities or nearby water users about the problem until thirteen months after the incident."<ref 
name="abc.net.au-2011-02-21/farmers-count1951670"></ref> The safety data sheet QGC had submitted for the hydraulic fracturing chemical was derived from the United States, incomplete and ten years out of date. Over thirty chemicals may be used in the process of hydraulic fracturing and their long-term impact on aquifers and the agriculture and people supported by them is unknown.

</doc>
<doc id="55216" url="http://en.wikipedia.org/wiki?curid=55216" title="Anders Hejlsberg">
Anders Hejlsberg

Anders Hejlsberg (, born December 1960) is a prominent Danish software engineer who co-designed several popular and commercially successful programming languages and development tools. He was the original author of Turbo Pascal and the chief architect of Delphi. He currently works for Microsoft as the lead architect of C# and core developer on TypeScript.
Early life.
Hejlsberg was born in Copenhagen, Denmark, and studied engineering at the Technical University of Denmark but did not graduate. While at the university in 1980, he began writing programs for the Nascom microcomputer, including a Pascal compiler which was initially marketed as the "Blue Label Software Pascal" for the Nascom-2. However, he soon rewrote it for CP/M and DOS, marketing it first as "Compas Pascal" and later as "PolyPascal". Later the product was licensed to Borland, and integrated into an IDE to become the Turbo Pascal system. Turbo Pascal competed with PolyPascal. The compiler itself was largely inspired by the "Tiny Pascal" compiler in Niklaus Wirth's "Algorithms + Data Structures = Programs", one of the most influential computer science books of the time. Anders and his partners ran a computer store in Copenhagen and marketed accounting systems. Their company, PolyData, was the distributor for Microsoft products in Denmark which put them at odds with Borland. Philippe Kahn and Anders first met in 1986. For all those years, Niels Jensen, one of Borland's founders and its majority shareholder, had successfully handled the relationship between Borland and PolyData.
At Borland.
In Borland's hands, Turbo Pascal became one of the most commercially successful Pascal compilers. Hejlsberg remained with PolyData until the company came under financial stress, at which time, in 1989 he moved to California and became Chief Engineer at Borland. There he remained until 1996. During this time he developed Turbo Pascal further, and eventually became the chief architect for the team which produced the replacement for Turbo Pascal, Delphi.
At Microsoft.
In 1996, Hejlsberg left Borland and joined Microsoft. One of his first achievements was the J++ programming language and the Windows Foundation Classes; he also became a Microsoft Distinguished Engineer and Technical Fellow. Since 2000, he has been the lead architect of the team developing the language C#. In 2012 Hejlsberg announced his new project TypeScript—a superset of JavaScript.
Awards.
He received the 2001 Dr. Dobb's Excellence in Programming Award for his work on Turbo Pascal, Delphi, C# and the Microsoft .NET Framework.
Together with Shon Katzenberger, Scott Wiltamuth, Todd Proebsting, Erik Meijer, Peter Hallam and Peter Sollich, Anders was awarded a Technical Recognition Award for Outstanding Technical Achievement for their work on the C# language in 2007. A video about this is available at Microsoft Channel 9.

</doc>
<doc id="55222" url="http://en.wikipedia.org/wiki?curid=55222" title="Buckminsterfullerene">
Buckminsterfullerene

Buckminsterfullerene (or bucky-ball) is a spherical fullerene molecule with the formula C60. It has a cage-like fused-ring structure (truncated icosahedron) which resembles a soccer ball, made of twenty hexagons and twelve pentagons, with a carbon atom at each vertex of each polygon and a bond along each polygon edge.
It was first generated in 1985 by Harold Kroto, James R. Heath, Sean O'Brien, Robert Curl, and Richard Smalley at Rice University. Kroto, Curl and Smalley were awarded the 1996 Nobel Prize in Chemistry for their roles in the discovery of buckminsterfullerene and the related class of molecules, the fullerenes. The name is a reference to Buckminster Fuller, as C60 resembles his trademark geodesic domes. Buckminsterfullerene is the most common naturally occurring fullerene molecule, as it can be found in small quantities in soot.
Solid and gaseous forms of the molecule have been detected in deep space.
Buckminsterfullerene is one of the largest objects to have been shown to exhibit wave–particle duality; as stated in the theory every object exhibits this behavior. Its discovery led to the exploration of a new field of chemistry, involving the study of fullerenes.
Etymology.
"Buckminsterfullerene" derives from the name of the noted futurist and inventor Buckminster Fuller. One of his designs of a geodesic dome structure bears great resemblance to C60; as a result, the discoverers of the allotrope named the newfound molecule after him. The general public, however, sometimes refers to buckminsterfullerene, and even Mr. Fuller's dome structure, as buckyballs.
History.
The structure associated with fullerenes was described by Leonardo da Vinci. Albrecht Dürer also reproduced a similar icosahedron containing 12 pentagonal and 20 hexagonal faces but there are no clear documentations of this.
Discovery.
Theoretical predictions of buckyball molecules appeared in the late 1960s – early 1970s, but they went largely unnoticed. In the early 1970s, the chemistry of unsaturated carbon configurations was studied by a group at the University of Sussex, led by Harry Kroto and David Walton. In the 1980s a technique was developed by Richard Smalley and Bob Curl at Rice University, Texas to isolate these substances. They used laser vaporization of a suitable target to produce clusters of atoms. Kroto realized that by using a graphite target, any carbon chains formed could be studied. Another interesting fact is that, at the same time, astrophysicists were working along with spectroscopists to study infrared emissions from giant red carbon stars. Smalley and team were able to use a laser vaporization technique to create carbon clusters which could potentially emit infrared at the same wavelength as had been emitted by the red carbon star. Hence, the inspiration came to Smalley and team to use the laser technique on graphite to create the first fullerene molecule.
C60 was discovered in 1985 by Robert Curl, Harold Kroto and Richard Smalley. Using laser evaporation of graphite they found Cn clusters (where n>20 and even) of which the most common were C60 and C70. A solid rotating graphite disk was used as the surface from which carbon was vaporized using a laser beam creating hot plasma that was then passed through a stream of high-density helium gas. The carbon species were subsequently cooled and ionized resulting in the formation of clusters. Clusters ranged in molecular masses but Kroto and Smalley found predominance in a C60 cluster that could be enhanced further by letting the plasma react longer.[3, 6] They also discovered that the C60 molecule formed a cage-like structure, a regular truncated icosahedron.
For this discovery they were awarded the 1996 Nobel Prize in Chemistry. The discovery of buckyballs was surprising, as the scientists aimed the experiment at producing carbon plasmas to replicate and characterize unidentified interstellar matter. Mass spectrometry analysis of the product indicated the formation of spheroidal carbon molecules.
The experimental evidence, a strong peak at 720 atomic mass units, indicated that a carbon molecule with 60 carbon atoms was forming, but provided no structural information. The research group concluded after reactivity experiments, that the most likely structure was a spheroidal molecule. The idea was quickly rationalized as the basis of an icosahedral symmetry closed cage structure. Kroto mentioned geodesic dome structures of the noted futurist and inventor Buckminster Fuller as influences in the naming of this particular substance as buckminsterfullerene.
Further developments.
The versatility of fullerene molecules has led to a large amount of research exploring their properties. One interesting property is the relatively large volume of the internal space of the molecule. Atoms of different elements may be placed inside the molecular cage formed by the carbon atoms.
Beam-experiments conducted between 1985 and 1990 provided more evidence for the stability of C60 while supporting the closed-cage structural theory and predicting some of the bulk properties such a molecule would have. Around this time, intense theoretical group theory activity also predicted that C60 should have only four IR-active vibrational bands, on account of its icosahedral symmetry.
In 1989 physicists Wolfgang Krätschmer and Donald R. Huffman observed unusual optical absorptions in thin carbon films produced by arc-processed graphite rods. Among other features, the IR spectra showed four discrete bands in close agreement to those proposed for C60. A paper published by the group in 1990 followed on from their thin film experiments, and detailed the extraction of a benzene soluble material from the arc-processed graphite. This extract had crystal and X-ray analysis consistent with arrays of spherical C60 molecules, approximately 0.7 nm in diameter.
In 2012 a toxicity study by Tarek Baati and Fathi Moussa from the University of Paris, showed that C60 dissolved in olive oil was not toxic to rodents. In a video interview with Professor Fathi Moussa regarding the study, further information was provided regarding the toxicity study, and the method of action whereby the lifespan of the rodents was increased by 90% relative to controls when the animals were dosed with C60 olive oil.
Synthesis.
In 1990, W. Krätschmer and D. R. Huffman developed a simple and efficient method of producing fullerenes in gram and even kilogram amounts, which has boosted the fullerene research. In this technique, carbon soot is produced from two high-purity graphite electrodes by igniting an arc discharge between them in an inert atmosphere (helium gas). Alternatively, soot is produced by laser ablation of graphite or pyrolysis of aromatic hydrocarbons. Fullerenes are extracted from the soot using a multistep procedure. First, the soot is dissolved in appropriate organic solvents. This step yields a solution containing up to 75% of C60, as well as other fullerenes. These fractions are separated using chromatography. Generally, the fullerenes are dissolved in hydrocarbon or halogenated hydrocarbon and separated using alumina columns.
Properties.
Molecule.
The structure of a buckminsterfullerene is a truncated icosahedron with 60 vertices and 32 faces (20 hexagons and 12 pentagons where no pentagons share a vertex) with a carbon atom at the vertices of each polygon and a bond along each polygon edge. The van der Waals diameter of a C60 molecule is about 1.01 nanometers (nm). The nucleus to nucleus diameter of a C60 molecule is about 0.71 nm. The C60 molecule has two bond lengths. The 6:6 ring bonds (between two hexagons) can be considered "double bonds" and are shorter than the 6:5 bonds (between a hexagon and a pentagon). Its average bond length is 0.14 nm. Each carbon atom in the structure is bonded covalently with 3 others.
The C60 molecule is extremely stable, withstanding high temperatures and high pressures. The exposed surface of the structure can selectively react with other species while maintaining the spherical geometry. Atoms and small molecules can be trapped within the molecule without reacting.
C60 undergoes six reversible, one-electron reductions to C606-, but oxidation is irreversible. The first reduction needs ~1.0 V (Fc/Fc+), showing that C60 is a moderately effective electron acceptor. C60 tends to avoid having double bonds in the pentagonal rings, which makes electron delocalization poor, and results in C60 not being not "superaromatic". C60 behaves very much like an electron deficient alkene and readily reacts with electron rich species.
A carbon atom in the C60 molecule can be substituted by a nitrogen or boron atom yielding a C59N or C59B respectively.
Solution.
Fullerenes are sparingly soluble in aromatic solvents such as toluene and carbon disulfide, but insoluble in water. Solutions of pure C60 have a deep purple color which leaves a brown residue upon evaporation. The reason for this color change is the relatively narrow energy width of the band of molecular levels responsible for green light absorption by individual C60 molecules. Thus individual molecules transmit some blue and red light resulting in a purple color. Upon drying, intermolecular interaction results in the overlap and broadening of the energy bands, thereby eliminating the blue light transmittance and causing the purple to brown color change.
C60 crystallises with some solvents in the lattice ("solvates"). For example, crystallization of C60 in benzene solution yields triclinic crystals with the formula C60·4C6H6. Like other solvates, this one readily releases benzene to give the usual fcc C60. Millimeter-sized crystals of C60 and C70 can be grown from solution both for solvates and for pure fullerenes.
Solid.
In solid buckminsterfullerene, the molecules C60 stick together via the van der Waals forces in the fcc motif. At low temperatures the individual molecules are locked against rotation. Upon heating, they start rotating at about −20 °C. This results in a first-order phase transition to a face-centered cubic (fcc) structure and a small, yet abrupt increase in the lattice constant from 1.411 to 1.4154 nm.
C60 solid is as soft as graphite, but when compressed to less than 70% of its volume it transforms into a superhard form of diamond (see aggregated diamond nanorod). C60 films and solution have strong non-linear optical properties; in particular, their optical absorption increases with light intensity (saturable absorption).
C60 forms a brownish solid with an optical absorption threshold at ~1.6 eV. It is an n-type semiconductor with a low activation energy of 0.1–0.3 eV; this conductivity is attributed to intrinsic or oxygen-related defects. Fcc C60 contains voids at its octahedral and tetrahedral sites which are sufficient large (0.6 and 0.2 nm respectively) to accommodate impurity atoms. When alkali metals are doped into these voids, C60 converts from a semiconductor into a conductor or even superconductor.
Band structure and superconductivity.
In 1991, Haddon "et al." found that intercalation of alkali-metal atoms in solid C60 leads to metallic behavior. In 1991, it was revealed that potassium-doped C60 becomes superconducting at 18 K. This was the highest transition temperature for a molecular superconductor. Since then, superconductivity has been reported in fullerene doped with various other alkali metals. It has been shown that the superconducting transition temperature in alkaline-metal-doped fullerene increases with the unit-cell volume V. As caesium forms the largest alkali ion, caesium-doped fullerene is an important material in this family. Recently, superconductivity at 38 K has been reported in bulk Cs3C60, but only under applied pressure. The highest superconducting transition temperature of 33 K at ambient pressure is reported for Cs2RbC60.
The increase of transition temperature with the unit-cell volume had been believed to be evidence for the BCS mechanism of C60 solid superconductivity, because inter C60 separation can be related to an increase in the density of states on the Fermi level, N(εF). Therefore, there have been many efforts to increase the interfullerene separation, in particular, intercalating neutral molecules into the A3C60 lattice to increase the interfullerene spacing while the valence of C60 is kept unchanged. However, this ammoniation technique has revealed a new aspect of fullerene intercalation compounds: the Mott transition and the correlation between the orientation/orbital order of C60 molecules and the magnetic structure.
The C60 molecules compose a solid of weakly bound molecules. The fullerites are therefore molecular solids, in which the molecular properties still survive. The discrete levels of a free C60 molecule are only weakly broadened in the solid, which leads to a set of essentially nonoverlapping bands with a narrow width of about 0.5 eV. For an undoped C60 solid, the 5-fold hu band is the HOMO level, and the 3-fold t1u band is the empty LUMO level, and this system is a band insulator. But when the C60 solid is doped with metal atoms, the metal atoms give electrons to the t1u band or the upper 3-fold t1g band. This partial electron occupation of the band may lead to metallic behavior. However, A4C60 is an insulator, although the t1u band is only partially filled and it should be a metal according to band theory. This unpredicted behavior may be explained by the Jahn-Teller effect, where spontaneous deformations of high-symmetry molecules induce the splitting of degenerate levels to gain the electronic energy. The Jahn-Teller type electron-phonon interaction is strong enough in C60 solids to destroy the band picture for particular valence states.
A narrow band or strongly correlated electronic system and degenerated ground states are important points to understand in explaining superconductivity in fullerene solids. When the inter-electron repulsion U is greater than the bandwidth, an insulating localized electron ground state is produced in the simple Mott-Hubbard model. This explains the absence of superconductivity at ambient pressure in caesium-doped C60 solids. Electron-correlation-driven localization of the t1u electrons exceeds the critical value, leading to the Mott insulator. The application of high pressure decreases the interfullerene spacing, therefore caesium-doped C60 solids turn to metallic and superconducting.
A fully developed theory of C60 solids superconductivity is still lacking, but it has been widely accepted that strong electronic correlations and the Jahn-Teller electron-phonon coupling produce local electron-pairings that show a high transition temperature close to the insulator-metal transition.
Chemical reactions and properties.
Hydrogenation.
C60 exhibits a small degree of aromatic character, but it still reflects localized double and single C-C bond characters. Therefore C60 can undergo addition with hydrogen to give polyhydrofullerenes. C60 also undergoes Birch reduction. For example, C60 reacts with lithium in liquid ammonia, followed by "tert"-butanol to give a mixture of polyhydrofullerenes such as C60H18, C60H32, C60H36, with C60H32 being the dominating product. This mixture of polyhydrofullerenes can be re-oxidized by 2,3-Dichloro-5,6-dicyano-1,4-benzoquinone to give C60 again.
Selective hydrogenation method exists. Reaction of C60 with 9,9',10,10'-dihydroanthracene under the same conditions, depending on the time of reaction, gives C60H32 and C60H18 respectively and selectively.
C60 can be hydrogenated, suggesting that a modified buckminsterfullerene called organometallic buckyballs (OBBs) could become a vehicle for "high density, room temperature, ambient pressure storage of hydrogen". These OBBs are created by binding atoms of a transition metal (TM) to C60 or C48B12 and then binding many hydrogen atoms to this TM atom, dispersing them evenly throughout the inside of the organometallic buckyball. The study found that the theoretical amount of H2 that can be retrieved from the OBB at ambient pressure approaches 9 wt %, a mass fraction that has been designated as optimal for hydrogen fuel by the U.S. Department of Energy.
Halogenation.
Addition of fluorine, chlorine, and bromine occurs for C60.
Fluorine atoms are small enough for a 1,2-addition, while Cl2 and Br2 add to remote C atoms due to steric factors. For example, in C60Br8 and C60Br24, the Br atoms are in 1,3- or 1,4-positions with respect to each other.
Under various conditions a vast number of halogenated derivatives of C60 can be produced, some with extraordinary selectivity on one or two isomers over the other possible ones.
Addition of fluorine and chlorine usually results in a flattening of the C60 framework into a drum-shaped molecule.
Addition of oxygen atoms.
Solutions of C60 can be oxygenated to the epoxide C60O. Ozonation of C60 in 1,2-xylene at 257K gives an intermediate ozonide C60O3, which can be decomposed into 2 forms of C60O. Decomposition of C60O3 at 296K gives the epoxide, but photolysis gives a product in which the O atom bridges a 5,6-edige.
Cycloadditions.
The Diels-Alder reaction is commonly employed to functionalize C60. Reaction of C60 with appropriate substituted diene gives the corresponding adduct.
The Diels-Alder reaction between C60 and 3,6-diaryl-1,2,4,5-tetrazines affords C62. The C62 has the structure in which a four-membered ring is surrounded by four six-membered rings.
The C60 molecules can also be coupled through a [2+2] cycloaddition, giving the dumbbell-shaped compound C120. The coupling is achieved by high-speed vibrating milling of C60 with a catalytic amount of KCN. The reaction is reversible as C120 dissociates back to two C60 molecules when heated at 450 K. Under high pressure and temperature, repeated [2+2] cycloaddtion between C60 results in a polymerized fullerene chains and networks. These polymers remain stable at ambient pressure and temperature once formed, and have remarkably interesting electronic and magnetic properties, such as being ferromagnetic above room temperature.
Free radical reactions.
Reactions of C60 with free radicals readily occur. When C60 is mixed with a disulfide RSSR, the radical C60SR• forms spontaneously upon irradiation of the mixture.
Stability of the radical species C60Y• depends largely on steric factors of Y. When "tert"-butyl halide is photolyzed and allowed to react with C60, a reversible inter-cage C-C bond is formed:
Cyclopropanation (Bingel reaction).
Cyclopropanation (Bingel reaction) is another common method for functionalizing C60. Cyclopropanation of C60 mostly occurs at the junction of 2 hexagons due to steric factors.
The first cyclopropanation was carried out by treating the β-bromomalonate with C60 in the presence of a base. Cyclopropanation also occur readily with diazomethanes. For example, diphenyldiazomethane reacts readily with C60 to give the compound C61Ph2. Phenyl-C61-butyric acid methyl ester derivative prepared through cyclopropanation has been studied for use in organic solar cells.
Redox reactions – C60 anions and cations.
The LUMO in C60 is triply degenerate, with the HOMO-LUMO separation relatively small. This small gap suggests that reduction of C60 should occur at mild potentials leading to fulleride anions, [C60]"n"− ("n" = 1–6). The midpoint potentials of 1-electron reduction of buckminsterfullerene and its anions is given in the table below:
C60 forms a variety of charge-transfer complexes, for example with tetrakis(dimethylamino)ethylene:
This salt exhibits ferromagnetism at 16K.
The paramagnetic fulleride ion [C60]2− has been isolated as the [K(crypt-222)]+ salt. It is synthesized by treating C60 with metallic potassium in the presence of 2.2.2-Cryptand. The most common fulleride ion, however, is [C60]3−. Alkali metal salts of this trianion are superconducting. In M3C60 (M = Na, K, Rb), the M+ ions occupy the interstitial holes in a lattice composed of ccp lattice composed of nearly spherical C60 anions. In Cs3C60, the cages are arranged in a bcc lattice.
C60 oxidizes with difficulty. Three reversible oxidation processes have been observed by using cyclic voltammetry with ultra-dry methylene chloride and a supporting electrolyte with extremely high oxidation resistance and low nucleophilicity, such as [nBu4N] [AsF6].
Which the [C60]2+ ion is very unstable, and the third process can be studied only at low temperatures.
The redox potentials of C60 can be modified supramolecularly. A dibenzo-18-crown-6 derivative of C60 has been made, featuring a voltage sensor device, with the reversible binding of K+ ion causing an anodic shift of 90mV of the first C60 reduction.
Metal complexes.
C60 forms complexes akin to the more common alkenes. Complexes have been reported molybdenum, tungsten, platinum, palladium, iridium, and titanium. The pentacarbonyl species are produced by photochemical reactions.
In the case of platinum complex, the labile ethylene ligand is the leaving group in a thermal reaction:
Titanocene complexes have also been reported:
Coordinatively unsaturated precursors, such as Vaska's complex, for adducts with C60:
One such iridium complex, [Ir(η2-C60)(CO)Cl(Ph2CH2C6H4OCH2Ph)2] has been prepared where the metal center projects two electron-rich 'arms' that embrace the C60 guest.
Endohedral fullerenes.
Metal atoms or certain small molecules such as H2 and noble gas can be encapsulated inside the C60 cage. These endohedral fullerenes are usually synthesized by doping in the metal atoms in an arc reactor or by laser evaporation. These methods gives low yields of endohedral fullerenes, and a better method involves the opening of the cage, packing in the atoms or molecules, and closing the opening using certain organic reactions. This method, however, is still immature and only a few species have been synthesized this way.
Endohedral fullerenes show distinct and intriguing chemical properties that can be completely different from the encapsulated atom or molecule, as well as the fullerene itself. The encapsulated atoms have been shown to perform circular motions inside the C60 cage, and its motion has been followed by using NMR spectroscopy.
Applications.
No application of C60 has been commercialized. In the medical field, elements such as helium (that can be detected in minute quantities) can be used as chemical tracers in impregnated buckyballs. Buckminsterfullerene also inhibits the HIV virus. In particular, C60 inhibits a key enzyme in the human immunodeficiency virus known as HIV-1 protease; this could inhibit reproduction of the HIV virus in immune cells.
Water-soluble derivatives of C60 were discovered to exert an inhibition on the three isoforms of nitric oxide synthase, with slightly different potencies.
The optical absorption properties of C60 match solar spectrum in a way that suggests that C60-based films could be useful for photovoltaic applications. Because of its high electronic affinity it is one of the most common electron acceptor used in donor/acceptor based solar cells. Conversion efficiencies up to 5.7% have been reported in C60-polymer cells.

</doc>
<doc id="55227" url="http://en.wikipedia.org/wiki?curid=55227" title="Baire category theorem">
Baire category theorem

The Baire category theorem (BCT) is an important tool in general topology and functional analysis. The theorem has two forms, each of which gives sufficient conditions for a topological space to be a Baire space.
The theorem was proved by René-Louis Baire in his 1899 doctoral thesis.
Statement of the theorem.
A Baire space is a topological space with the following property: for each countable collection of open dense sets "Un", their intersection ∩ "Un" is dense.
Note that neither of these statements implies the other, since there are complete metric spaces which are not locally compact (the irrational numbers with the metric defined below; also, any Banach space of infinite dimension), and there are locally compact Hausdorff spaces which are not metrizable (for instance, any uncountable product of non-trivial compact Hausdorff spaces is such; also, several function spaces used in Functional Analysis; the uncountable Fort space). See Steen and Seebach in the references below.
This formulation is equivalent to BCT1 and is sometimes more useful in applications. Also: if a non-empty complete metric space is the countable union of closed sets, then one of these closed sets has "non-empty" interior.
Relation to the axiom of choice.
The proofs of BCT1 and BCT2 for arbitrary complete metric spaces require some form of the axiom of choice; and in fact BCT1 is equivalent over ZF to a weak form of the axiom of choice called the axiom of dependent choices.
The restricted form of the Baire category theorem in which the complete metric space is also assumed to be separable is provable in ZF with no additional choice principles. This restricted form applies in particular to the real line, the Baire space ωω, and the Cantor space 2ω.
Uses of the theorem.
BCT1 is used in functional analysis to prove the open mapping theorem, the closed graph theorem and the uniform boundedness principle.
BCT1 also shows that every complete metric space with no isolated points is uncountable. (If "X" is a countable complete metric space with no isolated points, then each singleton {"x"} in "X" is nowhere dense, and so "X" is of first category in itself.) In particular, this proves that the set of all real numbers is uncountable.
BCT1 shows that each of the following is a Baire space:
By BCT2, every finite-dimensional Hausdorff manifold is a Baire space, since it is locally compact and Hausdorff. This is so even for non-paracompact (hence nonmetrizable) manifolds such as the long line.
Proof.
The following is a standard proof that a complete pseudometric space formula_6 is a Baire space.
Let formula_7 be a countable collection of open dense subsets. We want to show that the intersection formula_8 is dense. A subset is dense if and only if every nonempty open subset intersects it. Thus, to show that the intersection is dense, it is sufficient to show that any nonempty open set formula_9 in formula_6 has a point formula_11 in common with all of the formula_7. Since formula_13 is dense, formula_9 intersects formula_13; thus, there is a point formula_16 and formula_17 such that:
where formula_19 and formula_20 denote an open and closed ball, respectively, centered at formula_11 with radius formula_22. Since each formula_7 is dense, we can continue recursively to find a pair of sequences formula_24 and formula_25 such that:
(This step relies on the axiom of choice.) Since formula_27 when formula_28, we have that formula_24 is Cauchy, and hence formula_24 converges to some limit formula_4 by completeness. For any formula_32, by closedness,
Therefore formula_34 and formula_35 for all formula_32.
See also this blog post by M. Baker for the proof of the theorem using Choquet's game.

</doc>
<doc id="55229" url="http://en.wikipedia.org/wiki?curid=55229" title="Bushranger">
Bushranger

Bushrangers, or bush rangers, originally referred to runaway convicts in the early years of the British settlement of Australia who had the survival skills necessary to use the Australian bush as a refuge to hide from the authorities. The term "bushranger" then evolved to refer to those who abandoned social rights and privileges to take up "robbery under arms" as a way of life, using the bush as their base. These bushrangers were roughly analogous to British "highwaymen" and outlaws of the American Old West, and their crimes often included robbing small-town banks or coach services.
History.
More than 2000 bushrangers are believed to have roamed the Australian countryside, beginning with the convict bolters and drawing to a close after Ned Kelly's last stand at Glenrowan.
1850s: gold rush era.
The bushrangers' heyday was the Gold Rush years of the 1850s and 1860s as the discovery of gold gave bushrangers access to great wealth that was portable and easily converted to cash. Their task was assisted by the isolated location of the goldfields and a police force decimated by troopers abandoning their duties to join the gold rush.
George Melville was hanged in front of a large crowd for robbing the McIvor gold escort near Castlemaine in 1853.
1860s to 1870s.
Bushranging numbers flourished in New South Wales with the rise of the colonial-born sons of poor, often ex-convict squatters who were drawn to a more glamorous life than mining or farming.
Much of the activity in this era was in the Lachlan Valley, around Forbes, Yass and Cowra.
Frank Gardiner, John Gilbert and Ben Hall led the most notorious gangs of the period. Other active bushrangers included Dan Morgan, based in the Murray River, and Captain Thunderbolt. Thunderbolt was the most successful Australian bushranger, if bushranging longevity is the benchmark, as he bushranged across northern New South Wales for six-and-a-half years until shot near Uralla in 1870. With his death, the New South Wales bushranging epidemic of the 1860s officially ended.
1880s to 1900s.
The increasing push of settlement, increased police efficiency, improvements in rail transport and communications technology, such as telegraphy, made it increasingly difficult for bushrangers to evade capture.
Among the last bushrangers was the Kelly Gang led by Ned Kelly, who were captured at Glenrowan in 1880, two years after they were outlawed.
In 1900 the indigenous Governor Brothers terrorised much of northern New South Wales.
Public perception.
In Australia, bushrangers often attract public sympathy (cf. the concept of social bandits). In Australian history and iconography bushrangers are held in some esteem in some quarters due to the harshness and anti-Catholicism of the colonial authorities whom they embarrassed, and the romanticism of the lawlessness they represented. Some bushrangers, most notably Ned Kelly in his Jerilderie letter, and in his final raid on Glenrowan, explicitly represented themselves as political rebels. Attitudes to Kelly, by far the most well-known bushranger, exemplify the ambivalent views of Australians regarding bushranging. Victoria's state cricket team adopted 'Bushrangers' as their team nickname in honour of those such as the Kelly Gang, who lived in the Victorian bush.
External links.
 

</doc>
<doc id="55233" url="http://en.wikipedia.org/wiki?curid=55233" title="High-energy astronomy">
High-energy astronomy

High energy astronomy is the study of astronomical objects that release EM radiation of highly energetic wavelengths. It includes X-ray astronomy, gamma-ray astronomy, and extreme UV astronomy, as well as studies of neutrinos and cosmic rays. The physical study of these phenomena is referred to as high-energy astrophysics.
Astronomical objects commonly studied in this field may include black holes, neutron stars, active galactic nuclei, supernovae, supernova remnants, and Gamma ray bursts.
Missions.
Some space and ground based telescopes that have studied high energy astronomy include the following:

</doc>
<doc id="55235" url="http://en.wikipedia.org/wiki?curid=55235" title="Larrea tridentata">
Larrea tridentata

Larrea tridentata is known as creosote bush and greasewood as a plant, chaparral as a medicinal herb, and as "gobernadora" in Mexico, Spanish for "governess," due to its ability to secure more water by inhibiting the growth of nearby plants. In Sonora, it is more commonly called "hediondilla." 
It is a flowering plant in the family Zygophyllaceae. The genus is named after Juan Antonio Hernandez de Larrea, a Spanish clergyman.
Distribution.
"Larrea tridentata" is a prominent species in the Mojave, Sonoran, and Chihuahuan Deserts of western North America, and its range includes those and other regions in portions of southeastern California, Arizona, southern Nevada, southwestern Utah, New Mexico and Texas in the United States, and northern Chihuahua and Sonora in Mexico. The species grows as far east as Zapata County, Texas, along the Rio Grande southeast of Laredo near the 99th meridian west.
Description.
"Larrea tridentata" is an evergreen shrub growing to 1 to tall, rarely 4 m. The stems of the plant bear resinous, dark green leaves with two opposite lanceolate leaflets joined at the base, with a deciduous awn between them, each leaflet 7 to long and 4 to broad. The flowers are up to 25 mm in diameter, with five yellow petals. Galls may form by the activity of the creosote gall midge. The whole plant exhibits a characteristic odor of creosote, from which the common name derives. In the regions where it grows its smell is often associated with the "smell of rain".
Oldest plants.
As the creosote bush grows older, its oldest branches eventually die and its crown splits into separate crowns. This normally happens when the plant is 30 to 90 years old. Eventually the old crown dies and the new one becomes a clonal colony from the previous plant, composed of many separate stem crowns all from the same seed.
King Clone.
The "King Clone" creosote ring is one of the oldest living organisms on Earth. It has been alive an estimated 11,700 years, in the central Mojave Desert near present day Lucerne Valley, California. This single clonal colony plant of "Larrea tridentata" reaches up to 67 ft in diameter, with an average diameter of 45 ft.
King Clone was identified and its age determined by Frank Vasek, a professor at the University of California, Riverside. Measurements of the plant as well as radiocarbon dating of excavated wood fragments were used to determine the plant's annual growth rate outward from the center of the ring. By measuring the diameter of the ring, its total age could be estimated. It is within the Creosote Rings Preserve of the Lucerne Valley and Johnson Valley.
Desert adaptation.
Contributing to the harshness of the germination environment above mature root systems, young creosote bushes are much more susceptible to drought stress than established plants. Germination is actually quite active during wet periods, but most of the young plants die very quickly unless there are optimal water conditions. Ground heat compounds the young plants' susceptibility to water stress, and ground temperatures can reach upwards of 70°C (160°F). To become established, it seems the young plant must experience a pattern of three to five years of abnormally cool and moist weather during and after germination. From this, it can be inferred that all the plants inside a stand are of equal age.
Mature plants, however, can tolerate extreme drought stress. In terms of negative water potential, creosote bushes can operate fully at -50 bars of water potential and have been found living down to -120 bars, although the practical average floor is around -70 bars, where the plant's need for cellular respiration generally exceeds the level that the water-requiring process of photosynthesis can provide. Cell division can occur during these times of water stress, and it is common for new cells to quickly absorb water after rainfall. This rapid uptake causes branches to grow several centimeters at the end of a wet season.
Water loss is reduced by the resinous, waxy coating of the leaves, and by their small size which prevents them from heating up above air temperature (which would increase the vapor pressure deficit between the leaf and the air, and thus would increase water loss). Plants do drop some leaves heading into summer, but if all leaves are lost, the plant will not recover. Accumulation of fallen leaves, as well as other detritus caught from the passing wind, creates an ecological community specific to the creosote bush canopy, including beetles, millipedes, pocket mice, and kangaroo rats.
Uses.
Native American medicinals.
Native Americans in the Southwest held beliefs that it treated many maladies, including sexually transmitted diseases, tuberculosis, chicken pox, dysmenorrhea, and snakebite. The shrub is still widely used as a medicine in Mexico. It contains nordihydroguaiaretic acid.
Herbal supplements and toxicity.
"Larrea tridentata" is often referred to as "chaparral" when used as a herbal remedy and supplement; however, it does not grow in the synonymous plant community chaparral. The United States Food and Drug Administration has issued warnings about the health hazards of ingesting "chaparral" or using it as an internal medicine, and discourages its use. In 2005, Health Canada issued a warning to consumers to avoid using the leaves of "Larrea" species because of the risk of damage to the liver and kidneys.
Cancer Research UK state that: "We don’t recommend that you take chaparral to treat or prevent any type of cancer."

</doc>
<doc id="55236" url="http://en.wikipedia.org/wiki?curid=55236" title="Compton scattering">
Compton scattering

Compton scattering is the elastic scattering of a photon by a charged particle, usually an electron. It results in a decrease in energy (increase in wavelength) of the photon (which may be an X ray or gamma ray photon), called the Compton effect. Part of the energy of the photon is transferred to the recoiling electron. Inverse Compton scattering also exists, in which a charged particle transfers part of its energy to a photon.
Introduction.
Compton scattering is an example of elastic scattering of light by a free charged particle, where the wavelength of the scattered light is different from that of the incident radiation. In Compton's original experiment, the energy of the X ray photon (formula_1) was very much larger than the binding energy of the atomic electron, so the electrons could be treated as being free. The amount by which the light's wavelength changes is called the Compton shift. Although nuclear Compton scattering exists, Compton scattering usually refers to the interaction involving only the electrons of an atom. The Compton effect was observed by Arthur Holly Compton in 1923 at Washington University in St. Louis and further verified by his graduate student Y. H. Woo in the years following. Compton earned the 1927 Nobel Prize in Physics for the discovery.
The effect is important because it demonstrates that light cannot be explained purely as a wave phenomenon. Thomson scattering, the classical theory of an electromagnetic wave scattered by charged particles, cannot explain low intensity shifts in wavelength (classically, light of sufficient intensity for the electric field to accelerate a charged particle to a relativistic speed will cause radiation-pressure recoil and an associated Doppler shift of the scattered light, but the effect would become arbitrarily small at sufficiently low light intensities regardless of wavelength). Light must behave as if it consists of particles, if we are to explain low-intensity Compton scattering. Compton's experiment convinced physicists that light can behave as a stream of particle-like objects (quanta), whose energy is proportional to the light wave's frequency.
Because the mass-energy and momentum of a system must "both" be conserved, it is not generally possible for the electron simply to move in the direction of the incident photon. The interaction between electrons and high energy photons (comparable to the rest energy of the electron, ) results in the electron being given part of the energy (making it recoil), and a photon of the remaining energy being emitted in a different direction from the original, so that the overall momentum of the system is conserved. If the scattered photon still has enough energy, the process may be repeated. In this scenario, the electron is treated as free or loosely bound. Experimental verification of momentum conservation in individual Compton scattering processes by Bothe and Geiger as well as by Compton and Simon has been important in disproving the BKS theory.
If the photon is of lower energy, but still has sufficient energy (in general a few eV to a few keV, corresponding to visible light through soft X-rays), it can eject an electron from its host atom entirely (a process known as the photoelectric effect), instead of undergoing Compton scattering. Higher energy photons ( and above) may be able to bombard the nucleus and cause an electron and a positron to be formed, a process called pair production.
Description of the phenomenon.
By the early 20th century, research into the interaction of X-rays with matter was well under way. It was observed that when X-rays of a known wavelength interact with atoms, the X-rays are scattered through an angle formula_2 and emerge at a different wavelength related to formula_2. Although Classical electromagnetism predicted that the wavelength of scattered rays should be equal to the initial wavelength, multiple experiments had found that the wavelength of the scattered rays was longer (corresponding to lower energy) than the initial wavelength.
In 1923, Compton published a paper in the "Physical Review" that explained the X-ray shift by attributing particle-like momentum to light quanta (Einstein had proposed light quanta in 1905 in explaining the photo-electric effect, but Compton did not build on Einstein's work.) The energy of light quanta depends only on the frequency of the light. In his paper, Compton derived the mathematical relationship between the shift in wavelength and the scattering angle of the X-rays by assuming that each scattered X-ray photon interacted with only one electron. His paper concludes by reporting on experiments which verified his derived relation:
The quantity "h"⁄"mec" is known as the Compton wavelength of the electron; it is equal to . The wavelength shift "λ′" − "λ" is at least zero (for ) and at most twice the Compton wavelength of the electron (for ).
Compton found that some X-rays experienced no wavelength shift despite being scattered through large angles; in each of these cases the photon failed to eject an electron. Thus the magnitude of the shift is related not to the Compton wavelength of the electron, but to the Compton wavelength of the entire atom, which can be upwards of 10 000 times smaller.
Derivation of the scattering formula.
A photon formula_11 with wavelength formula_5 collides with an electron formula_13 in an atom, which is treated as being at rest. The collision causes the electron to recoil, and a new photon formula_14 with wavelength formula_15 emerges at angle formula_2 from the photon's incoming path. Let formula_17 denote the electron after the collision. Compton allowed for the possibility that the interaction would sometimes accelerate the electron to speeds sufficiently close to the velocity of light and would require the application of Einstein's special relativity theory to properly describe its energy and momentum.
At the conclusion of Compton's 1923 paper, he reported results of experiments confirming the predictions of his scattering formula thus supporting the assumption that photons carry directed momentum as well as quantized energy. At the start of his derivation, he had postulated an expression for the momentum of a photon from equating Einstein's already established mass-energy relationship of formula_18 to the quantized photon energies of formula_19 which Einstein has separately postulated. If formula_20, the equivalent photon mass must be formula_21. The photon's momentum is then simply this effective mass times the photon's frame-invariant velocity formula_9. For a photon, its momentum formula_23, and thus formula_24 can be substituted for formula_25 for all photon momentum terms which arise in course of the derivation below. The derivation which appears in Compton's paper is more terse, but follows the same logic in the same sequence as the following derivation.
The conservation of energy formula_26 merely equates the sum of energies before and after scattering.
Compton postulated that photons carry momentum; thus from the conservation of momentum, the momenta of the particles should be similarly related by
The photon energies are related to the frequencies by
Before the scattering event, the electron is treated as sufficiently close to being at rest that its total energy consists entirely of the mass-energy equivalence of its rest mass formula_32:
After scattering, the possibility that the electron might be accelerated to a significant fraction of the speed of light, requires that its total energy be represented using the relativistic energy–momentum relation: 
Substituting these quantities into the expression for the conservation of energy gives,
This expression can be used to find the magnitude of the momentum of the scattered electron,
Note that the momentum gained by the electron (formerly zero) exceeds the momentum lost by the photon:
Equation (1) relates the various energies associated with the collision. The electron's momentum change includes a relativistic change in the mass of the electron so it is not simply related to the change in energy in the manner that occurs in classical physics. The change in the momentum of the photon is also not simply related to the difference in energy but involves a change in direction.
Solving the conservation of momentum expression for the scattered electron's momentum gives,
Then by making use of the scalar product,
Anticipating that formula_40 is replaceable with formula_19, multiply both sides by formula_42:
After replacing the photon momentum terms with formula_44, we get a second expression for the magnitude of the momentum of the scattered electron:
Equating both expressions for this momentum gives
which after evaluating the square and then canceling and rearranging terms gives
Then dividing both sides by formula_48 yields
Finally, since formula_50
Applications.
Compton scattering.
Compton scattering is of prime importance to radiobiology, as it is the most probable interaction of gamma rays and high energy X-rays with atoms in living beings and is applied in radiation therapy.
In material physics, Compton scattering can be used to probe the wave function of the electrons in matter in the momentum representation.
Compton scattering is an important effect in gamma spectroscopy which gives rise to the Compton edge, as it is possible for the gamma rays to scatter out of the detectors used. Compton suppression is used to detect stray scatter gamma rays to counteract this effect.
Magnetic Compton scattering.
Magnetic Compton scattering is an extension of the previously mentioned technique which involves the magnetisation of a crystal sample hit with high energy, circularly polarised photons. By measuring the scattered photons' energy and reversing the magnetisation of the sample, two different Compton profiles are generated (one for spin up momenta and one for spin down momenta). Taking the difference between these two profiles gives the Magnetic Compton profile formula_52 - a one-dimensional projection of the electron spin density.
formula_53
Where formula_54 is the number of spin-unpaired electrons in the system, formula_55 and formula_56 are the three-dimensional electron momentum distributions for the majority spin and minority spin electrons respectively.
Since this scattering process is incoherent (there is no phase relationship between the scattered photons), the MCP is representative of the bulk properties of the sample and is a probe of the ground state. This means that the MCP is ideal for comparison with theoretical techniques such as density functional theory.
The area under the MCP is directly proportional to the spin moment of the system and so, when combined with total moment measurements methods (such as SQUID magnetometry), can be used to isolate both the spin and orbital contributions to the total moment of a system.
The shape of the MCP also yields insight into the origin of the magnetism in the system.
Inverse Compton scattering.
Inverse Compton scattering is important in astrophysics. In X-ray astronomy, the accretion disc surrounding a black hole is presumed to produce a thermal spectrum. The lower energy photons produced from this spectrum are scattered to higher energies by relativistic electrons in the surrounding corona. This is surmised to cause the power law component in the X-ray spectra (0.2-10 keV) of accreting black holes.
The effect is also observed when photons from the cosmic microwave background (CMB) move through the hot gas surrounding a galaxy cluster. The CMB photons are scattered to higher energies by the electrons in this gas, resulting in the Sunyaev-Zel'dovich effect. Observations of the Sunyaev-Zel'dovich effect provide a nearly redshift-independent means of detecting galaxy clusters.
Some synchrotron radiation facilities scatter laser light off the stored electron beam.
This Compton backscattering produces high energy photons in the MeV to GeV range subsequently used for nuclear physics experiments.

</doc>
<doc id="55240" url="http://en.wikipedia.org/wiki?curid=55240" title="Confocal laser scanning microscopy">
Confocal laser scanning microscopy

Confocal laser scanning microscopy (CLSM or LSCM) is a technique for obtaining high-resolution optical images with depth selectivity. The key feature of confocal microscopy is its ability to acquire in-focus images from selected depths, a process known as optical sectioning. Images are acquired point-by-point and reconstructed with a computer, allowing three-dimensional reconstructions of topologically complex objects. For opaque specimens, this is useful for surface profiling, while for non-opaque specimens, interior structures can be imaged. For interior imaging, the quality of the image is greatly enhanced over simple microscopy because image information from multiple depths in the specimen is not superimposed. A conventional microscope "sees" as far into the specimen as the light can penetrate, while a confocal microscope only "sees" images one depth level at a time. In effect, the CLSM achieves a controlled and highly limited depth of focus. The principle of confocal microscopy was originally patented by Marvin Minsky in 1957, but it took another thirty years and the development of lasers for CLSM to become a standard technique toward the end of the 1980s.
In 1978, Thomas and Christoph Cremer designed a laser scanning process, which scans the three dimensional surface of an object point-by-point by means of a focused laser beam, and creates the over-all picture by electronic means similar to those used in scanning electron microscopes. This CLSM design combined the laser scanning method with the 3D detection of biological objects labeled with fluorescent markers for the first time. During the next decade, confocal fluorescence microscopy was developed into a fully mature technology, in particular by groups working at the University of Amsterdam and the European Molecular Biology Laboratory (EMBL) in Heidelberg and their industry partners.
Image formation.
In a confocal laser scanning microscope, a laser beam passes through a light source aperture and then is focused by an objective lens into a small (ideally diffraction limited) focal volume within or on the surface of a specimen. In biological applications especially, the specimen may be fluorescent. Scattered and reflected laser light as well as any fluorescent light from the illuminated spot passes back through the objective lens. A beam splitter separates off some portion of the light into the detection apparatus, which in fluorescence confocal microscopy will also have a filter that selectively passes the fluorescent wavelengths while blocking the original excitation wavelength. After passing a "pinhole", the light intensity is detected by a photodetection device (usually a photomultiplier tube (PMT) or avalanche photodiode), transforming the light signal into an electrical one that is recorded by a computer.
The detector aperture obstructs the light that is not coming from the focal point, as shown by the dotted gray line in the image. The out-of-focus light is suppressed: most of the returning light is blocked by the pinhole, which results in sharper images than those from conventional fluorescence microscopy techniques and permits one to obtain images of planes at various depths within the sample (sets of such images are also known as "z stacks").
The detected light originating from an illuminated volume element within the specimen represents one pixel in the resulting image. As the laser scans over the plane of interest, a whole image is obtained pixel-by-pixel and line-by-line, whereas the brightness of a resulting image pixel corresponds to the relative intensity of detected light. The beam is scanned across the sample in the horizontal plane by using one or more (servo controlled) oscillating mirrors. This scanning method usually has a low reaction latency and the scan speed can be varied. Slower scans provide a better signal-to-noise ratio, resulting in better contrast and higher resolution. Information can be collected from different focal planes by raising or lowering the microscope stage or objective lens. Successive slices make up a 'z-stack' which can either be processed by certain software to create a 3D image, or it is merged into a 2D stack (predominately the maximum pixel intensity is taken, other common methods include using the standard deviation or summing the pixels).
Confocal microscopy provides the capacity for direct, noninvasive, serial optical sectioning of intact, thick, living specimens with a minimum of sample preparation as well as a marginal improvement in lateral resolution. Biological samples are often treated with fluorescent dyes to make selected objects visible. However, the actual dye concentration can be low to minimize the disturbance of biological systems: some instruments can track single fluorescent molecules. Also, transgenic techniques can create organisms that produce their own fluorescent chimeric molecules (such as a fusion of GFP, green fluorescent protein with the protein of interest).
Resolution enhancement.
CLSM is a scanning imaging technique in which the resolution obtained is best explained by comparing it with another scanning technique like that of the scanning electron microscope (SEM). CLSM has the advantage of not requiring a probe to be suspended nanometers from the surface, as in an AFM or STM, for example, where the image is obtained by scanning with a fine tip over a surface. The distance from the objective lens to the surface (called the "working distance") is typically comparable to that of a conventional optical microscope. It varies with the system optical design, but working distances from hundreds of micrometres to several millimeters are typical.
In CLSM a specimen is illuminated by a point laser source, and each volume element is associated with a discrete scattering or fluorescence intensity. Here, the size of the scanning volume is determined by the spot size (close to diffraction limit) of the optical system because the image of the scanning laser is not an infinitely small point but a three-dimensional diffraction pattern. The size of this diffraction pattern and the focal volume it defines is controlled by the numerical aperture of the system's objective lens and the wavelength of the laser used. This can be seen as the classical resolution limit of conventional optical microscopes using wide-field illumination. However, with confocal microscopy it is even possible to improve on the resolution limit of wide-field illumination techniques because the confocal aperture can be closed down to eliminate higher orders of the diffraction pattern. For example, if the pinhole diameter is set to 1 Airy unit then only the first order of the diffraction pattern makes it through the aperture to the detector while the higher orders are blocked, thus improving resolution at the cost of a slight decrease in brightness. In fluorescence observations, the resolution limit of confocal microscopy is often limited by the signal to noise ratio caused by the small number of photons typically available in fluorescence microscopy. One can compensate for this effect by using more sensitive photodetectors or by increasing the intensity of the illuminating laser point source. Increasing the intensity of illumination laser risks excessive bleaching or other damage to the specimen of interest, especially for experiments in which comparison of fluorescence brightness is required. When imaging tissues which are differentially refractive, such as the spongy mesophyll of plant leaves or other air-space containing tissues, spherical aberrations that impair confocal image quality are often pronounced. Such aberrations however, can be significantly reduced by mounting samples in optically transparent, non-toxic perfluorocarbons such as perfluorodecalin, which readily infiltrates tissues and has a refractive index almost identical to that of water .
Uses.
CLSM is widely used in numerous biological science disciplines, from cell biology and genetics to microbiology and developmental biology. It is also used in quantum optics and nano-crystal imaging and spectroscopy.
Biology and medicine.
Clinically, CLSM is used in the evaluation of various eye diseases, and is particularly useful for imaging, qualitative analysis, and quantification of endothelial cells of the cornea. It is used for localizing and identifying the presence of filamentary fungal elements in the corneal stroma in cases of keratomycosis, enabling rapid diagnosis and thereby early institution of definitive therapy. Research into CLSM techniques for endoscopic procedures (endomicroscopy) is also showing promise. In the pharmaceutical industry, it was recommended to follow the manufacturing process of thin film pharmaceutical forms, to control the quality and uniformity of the drug distribution.
Optics and crystallography.
CLSM is used as the data retrieval mechanism in some 3D optical data storage systems and has helped determine the age of the Magdalen papyrus.
Low temperature operation.
To image samples at low temperature, two main approaches have been used. One approach is to put only the sample at low temperature in a continuous flow cryostat, and optically addressed it through a transparent window. The other approach is to have part of the optics (especially the microscope objective) in a cryogenic storage dewar. This second approach, although more cumbersome, guarantees better mechanical stability and avoids the losses due to the window.

</doc>
<doc id="55244" url="http://en.wikipedia.org/wiki?curid=55244" title="Hypersonic speed">
Hypersonic speed

In aerodynamics, a hypersonic speed is one that is highly supersonic. Since the 1970s, the term has generally been assumed to refer to speeds of Mach 5 and above.
The precise Mach number at which a craft can be said to be flying at hypersonic speed varies, since individual physical changes in the airflow (like molecular dissociation and ionization) occur at different speeds; these effects collectively become important around Mach 5. The hypersonic regime is often alternatively defined as speeds where ramjets do not produce net thrust.
Characteristics of flow.
While the definition of hypersonic flow can be quite vague and is generally debatable (especially due to the lack of discontinuity between supersonic and hypersonic flows), a hypersonic flow may be characterized by certain physical phenomena that can no longer be analytically discounted as in supersonic flow. The peculiarity in hypersonic flows are as follows:
Small shock stand-off distance.
As a body's Mach number increases, the density behind the shock generated by the body also increases, which corresponds to a decrease in volume behind the shock wave due to conservation of mass. Consequently, the distance between the shock and the body decreases at higher Mach numbers.
Entropy layer.
As Mach numbers increase, the entropy change across the shock also increases, which results in a strong entropy gradient and highly vortical flow that mixes with the boundary layer.
Viscous interaction.
A portion of the large kinetic energy associated with flow at high Mach numbers transforms into internal energy in the fluid due to viscous effects. The increase in internal energy is realized as an increase in temperature. Since the pressure gradient normal to the flow within a boundary layer is approximately zero for low to moderate hypersonic Mach numbers, the increase of temperature through the boundary layer coincides with a decrease in density. This causes the bottom of the boundary layer to expand, so that the boundary layer over the body grows thicker and can often merge with the shock wave near the body leading edge.
High temperature flow.
High temperatures due to a manifestation of viscous dissipation cause non-equilibrium chemical flow properties such as vibrational excitation and dissociation and ionization of molecules resulting in convective and radiative heat-flux.
Classification of Mach regimes.
Although "subsonic" and "supersonic" usually refer to speeds below and above the local speed of sound respectively, aerodynamicists often use these terms to refer to particular ranges of Mach values. This occurs because a "transonic regime" exists around M=1 where approximations of the Navier-Stokes equations used for subsonic design no longer apply, partly because the flow locally exceeds M=1 even when the freestream Mach number is below this value.
The "supersonic regime" usually refers to the set of Mach numbers for which linearised theory may be used; for example, where the (air) flow is not chemically reacting and where heat-transfer between air and vehicle may be reasonably neglected in calculations.
Generally, NASA defines "high" hypersonic as any Mach number from 10 to 25, and re-entry speeds as anything greater than Mach 25. Among the aircraft operating in this regime are the Space Shuttle and (theoretically) various developing space planes.
In the following table, the "regimes" or "ranges of Mach values" are referenced instead of the usual meanings of "subsonic" and "supersonic".
Similarity parameters.
The categorization of airflow relies on a number of similarity parameters, which allow the simplification of a nearly infinite number of test cases into groups of similarity. For transonic and compressible flow, the Mach and Reynolds numbers alone allow good categorization of many flow cases.
Hypersonic flows, however, require other similarity parameters. First, the analytic equations for the oblique shock angle become nearly independent of Mach number at high (~>10) Mach numbers. Second, the formation of strong shocks around aerodynamic bodies means that the freestream Reynolds number is less useful as an estimate of the behavior of the boundary layer over a body (although it is still important). Finally, the increased temperature of hypersonic flows mean that real gas effects become important. For this reason, research in hypersonics is often referred to as aerothermodynamics, rather than aerodynamics.
The introduction of real gas effects means that more variables are required to describe the full state of a gas. Whereas a stationary gas can be described by three variables (pressure, temperature, adiabatic index), and a moving gas by four (flow velocity), a hot gas in chemical equilibrium also requires state equations for the chemical components of the gas, and a gas in nonequilibrium solves those state equations using time as an extra variable. This means that for a nonequilibrium flow, something between 10 and 100 variables may be required to describe the state of the gas at any given time. Additionally, rarefied hypersonic flows (usually defined as those with a Knudsen number above 0.1) do not follow the Navier-Stokes equations.
Hypersonic flows are typically categorized by their total energy, expressed as total enthalpy (MJ/kg), total pressure (kPa-MPa), stagnation pressure (kPa-MPa), stagnation temperature (K), or flow velocity (km/s).
Wallace D. Hayes developed a similarity parameter, similar to the Whitcomb area rule, which allowed similar configurations to be compared.
Regimes.
Hypersonic flow can be approximately separated into a number of regimes. The selection of these regimes is rough, due to the blurring of the boundaries where a particular effect can be found.
Perfect gas.
In this regime, the gas can be regarded as an ideal gas. Flow in this regime is still Mach number dependent. Simulations start to depend on the use of a constant-temperature wall, rather than the adiabatic wall typically used at lower speeds. The lower border of this region is around Mach 5, where ramjets become inefficient, and the upper border around Mach 10-12.
Two-temperature ideal gas.
This is a subset of the perfect gas regime, where the gas can be considered chemically perfect, but the rotational and vibrational temperatures of the gas must be considered separately, leading to two temperature models. See particularly the modeling of supersonic nozzles, where vibrational freezing becomes important.
Dissociated gas.
In this regime, diatomic or polyatomic gases (the gases found in most atmospheres) begin to dissociate as they come into contact with the bow shock generated by the body. Surface catalysis plays a role in the calculation of surface heating, meaning that the type of surface material also has an effect on the flow. The lower border of this regime is where any component of a gas mixture first begins to dissociate in the stagnation point of a flow (which for nitrogen is around 2000 K). At the upper border of this regime, the effects of ionization start to have an effect on the flow.
Ionized gas.
In this regime the ionized electron population of the stagnated flow becomes significant, and the electrons must be modeled separately. Often the electron temperature is handled separately from the temperature of the remaining gas components. This region occurs for freestream flow velocities around 10–12 km/s. Gases in this region are modeled as non-radiating plasmas.
Radiation-dominated regime.
Above around 12 km/s, the heat transfer to a vehicle changes from being conductively dominated to radiatively dominated. The modeling of gases in this regime is split into two classes:
The modeling of optically thick gases is extremely difficult, since, due to the calculation of the radiation at each point, the computation load theoretically expands exponentially as the number of points considered increases.
References.
</dl>

</doc>
<doc id="55245" url="http://en.wikipedia.org/wiki?curid=55245" title="Lockheed SR-71 Blackbird">
Lockheed SR-71 Blackbird

The Lockheed SR-71 "Blackbird" is a long-range, Mach 3+ strategic reconnaissance aircraft that was operated by the United States Air Force. It was developed as a black project from the Lockheed A-12 reconnaissance aircraft in the 1960s by Lockheed and its Skunk Works division. Renowned American aerospace engineer Clarence "Kelly" Johnson was responsible for many of the design's innovative concepts. During aerial reconnaissance missions, the SR-71 operated at high speeds and altitudes to allow it to outrace threats. If a surface-to-air missile launch was detected, the standard evasive action was simply to accelerate and outfly the missile. The SR-71 was designed to have basic stealth characteristics and served as a precursor to future stealth aircraft.
The SR-71 served with the U.S. Air Force from 1964 to 1998. A total of 32 aircraft were built; 12 were lost in accidents and none lost to enemy action. The SR-71 has been given several nicknames, including "Blackbird" and "Habu". Since 1976, it has held the world record for the fastest air-breathing manned aircraft, a record previously held by the related YF-12.
Development.
Background.
Lockheed's previous reconnaissance aircraft was the relatively slow U-2, designed for the Central Intelligence Agency (CIA). The 1960 downing of Francis Gary Powers's U-2 underscored the aircraft's vulnerability and the need for faster reconnaissance aircraft. The CIA turned again to Kelly Johnson and Lockheed's Skunk Works, who developed the A-12 and would go on to build upon its design concepts for the SR-71. The Flight Test Engineer in charge was Joseph F. Ware, Jr.[]
The A-12 first flew at Groom Lake (Area 51), Nevada, on 25 April 1962. Thirteen were built; two variants were also developed, including three YF-12A interceptor prototypes, and two M-21 drone carrier variants. The aircraft was meant to be powered by the Pratt & Whitney J58 engine, but development ran over schedule, and it was equipped instead with the less powerful Pratt & Whitney J75. The J58s were retrofitted as they became available, and became the standard powerplant for all subsequent aircraft in the series (A-12, YF-12, M-21) as well as the SR-71. The A-12 flew missions over Vietnam and North Korea before its retirement in 1968. The program's cancellation was announced on 28 December 1966, due both to budget concerns and because of the forthcoming SR-71.
SR-71.
The SR-71 designator is a continuation of the pre-1962 bomber series; the last aircraft built using the series was the XB-70 Valkyrie; however, a bomber variant of the Blackbird was briefly given the B-71 designator, which was retained when the type was changed to SR-71.
During the later period of its testing, the B-70 was proposed for a reconnaissance/strike role, with an "RS-70" designation. When it was clear that the A-12 performance potential was much greater, the Air Force ordered a variant of the A-12 in December 1962. Originally named R-12 by Lockheed, the Air Force version was longer and heavier than the A-12, with a longer fuselage to hold more fuel, two seats in the cockpit, and reshaped chines. Reconnaissance equipment included signals intelligence sensors, a side looking airborne radar and a photo camera. The CIA's A-12 was a better photo reconnaissance platform than the Air Force's R-12, since the A-12 flew somewhat higher and faster, and with only one pilot it had room to carry a superior camera and more instruments.
During the 1964 campaign, Republican presidential nominee Barry Goldwater repeatedly criticized President Lyndon B. Johnson and his administration for falling behind the Soviet Union in developing new weapons. Johnson decided to counter this criticism by revealing the existence of the Lockheed YF-12A Air Force interceptor, which also served as cover for the still-secret A-12, and the Air Force reconnaissance model since July 1964. Air Force Chief of Staff General Curtis LeMay preferred the SR (Strategic Reconnaissance) designation and wanted the RS-71 to be named SR-71. Before the July speech, LeMay lobbied to modify Johnson's speech to read SR-71 instead of RS-71. The media transcript given to the press at the time still had the earlier RS-71 designation in places, creating the story that the president had misread the aircraft's designation.
In 1968, Secretary of Defense Robert McNamara canceled the F-12 interceptor program; the specialized tooling used to manufacture both the YF-12 and the SR-71 was also ordered destroyed. Production of the SR-71 totaled 32 aircraft with 29 SR-71As, 2 SR-71Bs, and the single SR-71C.
Design.
Overview.
The SR-71 was designed for flight at over Mach 3 with a flight crew of two in tandem cockpits, with the pilot in the forward cockpit and the Reconnaissance Systems Officer (RSO) monitoring the surveillance systems and equipment from the rear cockpit. The SR-71 was designed to minimize its radar cross-section, an early attempt at stealth design. Finished aircraft were painted a dark blue, almost black, to increase the emission of internal heat and to act as camouflage against the night sky. The dark color led to the aircraft's call sign "Blackbird".
While the SR-71 carried radar countermeasures to evade interception efforts, its greatest protection was its high speed and cruising altitude that made it almost invulnerable to the weapons of its day. Merely accelerating would typically be enough to evade a surface-to-air missile, and the plane was faster than the Soviet Union's principal interceptor, the Mikoyan-Gurevich MiG-25. The MiG-25's successor, the MiG-31, was able to intercept the SR-71, as it was capable of Mach 2.8 and armed with the Mach 4.5 R-33 missile. SR-71 overflights of the USSR ceased following MiG-31 intercepts in the mid-1980s. Nonetheless, during its service life, no SR-71 was shot down.
Airframe, canopy and landing gear.
On most aircraft, use of titanium was limited by the costs involved; it was generally used only in components exposed to the highest temperatures, such as exhaust fairings and the leading edges of wings. On the SR-71, titanium was used for 85% of the structure, with much of the rest polymer composite materials. To control costs, Lockheed used a more easily worked titanium alloy which softened at a lower temperature. The challenges posed led Lockheed to develop new fabrication methods, and have since been used in the manufacture of other aircraft. Welding titanium requires distilled water, as the chlorine present in tap water is corrosive; cadmium-plated tools could not be used as they also caused corrosion. Metallurgical contamination was another problem; at one point 80% of the delivered titanium for manufacture was rejected on these grounds.
The high temperatures generated inflight required special design and operating techniques. Major portions of the skin of the inboard wings were corrugated, not smooth. Aerodynamicists initially opposed the concept, disparagingly referring to the aircraft as a Mach 3 variant of the 1920s-era Ford Trimotor, known for its corrugated aluminum skin. The heat would have caused a smooth skin to split or curl, whereas the corrugated skin could expand vertically and horizontally and increased longitudinal strength.
Fuselage panels were manufactured to only loosely fit on the ground. Proper alignment was achieved as the airframe heated up and expanded several inches. Because of this, and the lack of a fuel sealing system that could handle the airframe's expansion at extreme temperatures, the aircraft leaked JP-7 fuel on the ground prior to takeoff.
The outer windscreen of the cockpit was made of quartz and was fused ultrasonically to the titanium frame. The temperature of the exterior of the windscreen reached 600 °F during a mission.
Cooling was carried out by cycling fuel behind the titanium surfaces in the chines. On landing, the canopy temperature was over 300 °C. The red stripes on some SR-71s were to prevent maintenance workers from damaging the skin. Near the center of the fuselage, the curved skin was thin and delicate, with no support from the structural ribs, which were spaced several feet apart.
The Blackbird's tires, manufactured by B.F. Goodrich, contained aluminum and were filled with nitrogen. They cost $2,300 and did not last 20 missions. The Blackbird landed at over 170 knots and deployed a drag parachute to stop; the chute also acted to reduce stress on the tires.
Shape and threat avoidance.
The first operational aircraft designed around a stealth aircraft shape and materials, the SR-71 had several features designed to reduce its radar signature. The SR-71 had a radar cross-section (RCS) of around 10 square meters. Drawing on early studies in radar stealth technology, which indicated that a shape with flattened, tapering sides would reflect most energy away from a radar beam's place of origin, engineers added chines and canted the vertical control surfaces inward. Special radar-absorbing materials were incorporated into sawtooth-shaped sections of the aircraft's skin. Cesium-based fuel additives were used to somewhat reduce exhaust plumes visibility to radar, although exhaust streams remained quite apparent. Kelly Johnson later conceded that Soviet radar technology advanced faster than the stealth technology employed against it.
The SR-71 featured chines, a pair of sharp edges leading aft from either side of the nose along the fuselage. These were not a feature on the early A-3 design; Dr. Frank Rodgers of the Scientific Engineering Institute, a CIA front organization, discovered that a cross-section of a sphere had a greatly reduced radar reflection, and adapted a cylindrical-shaped fuselage by stretching out the sides of the fuselage. After the advisory panel provisionally selected Convair's FISH design over the A-3 on the basis of RCS, Lockheed adopted chines for its A-4 through A-6 designs.
Aerodynamicists discovered that the chines generated powerful vortices and created additional lift, leading to unexpected aerodynamic performance improvements. The angle of incidence of the delta wings could be reduced for greater stability and less drag at high speeds, and more weight carried, such as fuel. Landing speeds were also reduced, as the chines' vortices created turbulent flow over the wings at high angles of attack, making it harder to stall. The chines also acted like leading-edge extensions, which increase the agility of fighters such as the F-5, F-16, F/A-18, MiG-29 and Su-27. The addition of chines also enabled the removal of the planned canard foreplanes.
Air inlets.
The air inlets allowed the SR-71 to cruise at over Mach 3.2 while keeping airflow into the engines at the initial subsonic speeds. Mach 3.2 was the design point for the aircraft, its most efficient speed. At the front of each inlet, a pointed, movable cone called a "spike" (Inlet cone) was locked in its full forward position on the ground and during subsonic flight. When the aircraft accelerated past Mach 1.6, an internal jackscrew moved the spike up to 26 in inwards, directed by an analog air inlet computer that took into account pitot-static system, pitch, roll, yaw, and angle of attack. Moving the spike tip drew the shock wave riding on it closer to the inlet cowling until it touched just slightly inside the cowling lip. This position reflected the spike shock-wave repeatedly between the spike centerbody and the inlet inner cowl sides, and minimized airflow spillage which is the cause of spillage drag. The air slowed supersonically with a final plane shock wave at entry to the subsonic diffuser.
Downstream of this normal shock the air is subsonic. It decelerates further in the divergent duct to give the required speed at entry to the compressor. Capture of the plane shock wave within the inlet is called "Starting the Inlet". Bleed tubes and bypass doors were designed into the inlet and engine nacelles to handle some of this pressure and to position the final shock to allow the inlet to remain "started". The SR-71 was sometimes more efficient at speeds higher than Mach 3.2 in terms of pounds of fuel burned per nautical mile traveled, depending on outside air temperature. During one mission, SR-71 pilot Brian Shul flew faster than usual to avoid multiple interception attempts; afterwards, it was discovered that this had reduced fuel consumption.
In the early years of operation, the analog computers would not always keep up with rapidly changing flight environmental inputs. If internal pressures became too great and the spike was incorrectly positioned, the shock wave would suddenly blow out the front of the inlet, called an "Inlet Unstart". During unstarts afterburner extinctions were common. The remaining engine's asymmetrical thrust would cause the aircraft to yaw violently to one side. SAS, autopilot, and manual control inputs would fight the yawing, but often the extreme off-angle would reduce airflow in the opposite engine and stimulate "sympathetic stalls". This generated a rapid counter-yawing, often coupled with loud "banging" noises, and a rough ride during which crews' helmets would sometimes strike their cockpit canopies. One response to a single unstart was unstarting both inlets to prevent yawing, then restarting them both. Lockheed later installed an electronic control to detect unstart conditions and perform this reset action without pilot intervention. Beginning in 1980, the analog inlet control system was replaced by a digital system, which reduced unstart instances.
Engines.
The SR-71 was powered by two Pratt & Whitney J58 (company designation JT11D-20) axial-flow turbo-jet engines. The J58 was a considerable innovation of the era, capable of producing a static thrust of 32,500 lbf (145 kN).The engine was most efficient around Mach 3.2, the Blackbird's typical cruising speed. At lower speeds, the turbojet provided most of the compression. At higher speeds, the engine largely ceased to provide thrust, the afterburner taking its place.
Air was initially compressed (and heated) by the inlet spike and subsequent converging duct between the centerbody and inlet cowl. The shock waves generated slowed the air to subsonic speeds relative to the engine. The air then entered the engine compressor. Some of this compressor flow (20% at cruise) was removed after the 4th compressor stage and went straight to the afterburner through six bypass tubes. Air passing through the turbojet was compressed further by the remaining 5 compressor stages and then fuel was added in the combustion chamber. After passing through the turbine the exhaust, together with the compressor bleed air, entered the afterburner.
At around Mach 3, the temperature rise from the intake compression, added to the engine compressor temperature rise, reduced the allowable fuel flow because the turbine temperature limit did not change. The rotating machinery produced less power but still enough to run at 100% RPM, thus keeping the airflow through the intake constant. The rotating machinery had become a drag item and the engine thrust at high speeds came from the afterburner temperature rise. Maximum flight speed was limited by the temperature of the air entering the engine compressor, which was not certified for temperatures above 800 °F (427 °C).
Originally, the Blackbird's J58 engines were started with the assistance of two Buick Wildcat V8 internal combustion engines, externally mounted on a vehicle referred to as an AG330 "start cart". The start cart was positioned underneath the J58 and the two Buick engines powered a single, vertical drive shaft connecting to the J58 engine and spinning it to above 3,200 revolutions per minute at which point the turbojet could self-sustain. Once the first J58 engine was started, the cart was repositioned to the other side of the aircraft to start the other J58 engine. Later start carts used Chevrolet big-block V8 engines. Eventually, a quieter, pneumatic start system was developed for use at main operating bases; the V8 start carts remained at diversion landing sites not equipped with the pneumatic system.
Fuel.
Several exotic fuels were investigated for the Blackbird. Development began on a coal slurry powerplant, but Johnson determined that the coal particles damaged important engine components. Research was conducted on a liquid hydrogen powerplant, but the tanks for storing cryogenic hydrogen were not of a suitable size or shape. In practice, the Blackbird would burn somewhat conventional JP-7 which was difficult to light. To start the engines, triethylborane (TEB), which ignites on contact with air, was injected to produce temperatures high enough to ignite the JP-7. The TEB produced a characteristic green flame, which could often be seen during engine ignition.
On a typical SR-71 mission the plane only took off with a partial fuel load to reduce stress on the brakes and tires during takeoff and also ensure the plane could still successfully take off should one engine fail. As a result planes were typically refueled immediately after takeoff. The SR-71 also required in-flight refueling to replenish fuel during long duration missions. Supersonic flights generally lasted no more than 90 minutes before the pilot had to find a tanker.
Specialized KC-135Q tankers were required to refuel the SR-71. The KC-135Q had a modified high-speed boom, which would allow refueling of the Blackbird at nearly the tanker's maximum airspeed with minimum flutter. The tanker also had special fuel systems for moving JP-4 (for the KC-135Q itself) and JP-7 (for the SR-71) between different tanks. As an aid to the pilot when refueling, the cockpit was fitted with a peripheral vision horizon display (PVHD). This unusual instrument displayed a barely-visible artificial horizon, which gave the pilot subliminal cues on aircraft attitude.
Astro-Inertial Navigation System.
The USAF sought a precision navigation system for maintaining route accuracy and target tracking at very high speeds. Nortronics, Northrop Corporation's electronics development division, had developed an astro-inertial guidance system (ANS), which could correct Inertial navigation system errors with celestial observations, for the SM-62 Snark missile, and a separate system for the ill-fated AGM-48 Skybolt missile, the latter of which was adapted for the SR-71.
Before takeoff, a primary alignment brought the ANS's inertial components to a high degree of accuracy. In flight, the ANS, which sat behind the Reconnaissance Systems Officer (RSO)'s position, tracked stars through a circular quartz glass window on the upper fuselage. Its "blue light" source star tracker, which could see stars during both day and night, would continuously track a variety of stars as the aircraft's changing position brought them into view. The system's digital computer ephemeris contained data on 56 (later 61) stars. The ANS could supply altitude and position to flight controls and other systems, including the Mission Data Recorder, Auto-Nav steering to preset destination points, automatic pointing and control of cameras and sensors, and optical or SLR sighting of fix points loaded into the ANS before takeoff. According to Richard Graham, a former SR-71 pilot, the navigation system was good enough to limit drift to 1,000 feet off the direction of travel at Mach 3.
Sensors and payloads.
The SR-71 originally included optical/infrared imagery systems; side looking airborne radar (SLAR); electronic intelligence (ELINT) gathering systems; defensive systems for countering missile and airborne fighters; and recorders for SLAR, ELINT and maintenance data. The SR-71 carried a Fairchild tracking camera and an HRB Singer infrared camera, both of which ran during the entire mission for route documentation, to respond to any accusations of overflight.
As the SR-71 had an observer behind the pilot, it could not use the A-12's principal sensor, a single large-focal-length optical camera that sat in the "Q-Bay" behind the cockpit. Instead, camera systems could be located either in the wing chines or the interchangeable nose. Wide-area imaging was provided by two of Itek's Operational Objective Cameras (OOCs), which provided stereo imagery across the width of the flight track, or an Itek Optical Bar Camera (OBC), which gave continuous horizon-to horizon coverage. A closer view of the target area was given by the HYCON Technical Objective Camera (TEOC), that could be directed up to 45 degrees left or right of the centerline. Initially, the TEOCs could not match the resolution of the A-12's larger camera, but rapid improvements in both the camera and film improved this performance.
Side-looking radar, built by Goodyear Aerospace, could be carried in the removable nose. In later life, the radar was replaced by Loral's Advanced Synthetic Aperture Radar System (ASARS-1). Both the first SLR and ASARS-1 were ground-mapping imaging systems, collecting data either in fixed swaths left or right of centerline or from a spot location for higher resolution. ELINT-gathering systems, called the Electro Magnetic Reconnaissance System (EMR), built by AIL could be carried in the chine bays to analyse electronic signal fields being passed through, and were pre-programmed to identify items of interest.
Over its operational life, the Blackbird carried various electronic countermeasures, including warning and active electronic systems built by several ECM companies and called Systems A, A2, A2C, B, C, C2, E, G, H and M. On a given mission, an aircraft would carry several of these frequency/purpose payloads to meet the expected threats. Major Jerry Crew, a Reconnaissance Systems Officer, told Air & Space that he used a jammer to try to confuse Surface to Air Missile sites as their crews tracked his airplane, but once his threat warning receiver told him a missile had been launched, he switched off the jammer to prevent the missile from homing in on its signal. After landing, recording systems and gathered information from the SLR and ELINT systems, and the Maintenance Data Recorder (MDR) were subjected to post-flight ground analysis. In the later years of its operational life, a data-link system could send ASARS-1 and ELINT data from about 2000 nmi of track coverage to a suitably equipped ground station.
Life support.
Flying at 80000 ft meant that crews could not use standard masks, which could not provide enough oxygen above 43000 ft. Specialized protective pressurized suits were produced by the David Clark Company for the A-12, YF-12, M-21 and SR-71. Furthermore, an emergency ejection at Mach 3.2 would subject crews to temperatures of about 450 °F; thus, during a high altitude ejection scenario, an onboard oxygen supply would keep the suit pressurized during the descent.
The cockpit could be pressurized to an altitude of 10000 or during flight. The cabin needed a heavy-duty cooling system, for cruising at Mach 3.2 would heat the aircraft's external surface well beyond 500 °F<ref name="PM06/91">"Popular Mechanics", June 1991, p. 28.</ref> and the inside of the windshield to 250 °F. An air conditioner used a heat exchanger to dump heat from the cockpit into the fuel prior to combustion. The same air conditioning system was also used to keep the front (nose) landing gear bay cool, thereby eliminating the need for the special aluminum-impregnated tires on the main landing gear.
Operational history.
The first flight of an SR-71 took place on 22 December 1964, at Air Force Plant 42 in Palmdale, California. The SR-71 reportedly reached a top speed of Mach 3.4 during flight testing. The first SR-71 to enter service was delivered to the 4200th (later, 9th) Strategic Reconnaissance Wing at Beale Air Force Base, California, in January 1966.
SR-71s first arrived at the 9th SRW's Operating Location (OL-8) at Kadena Air Base, Okinawa on 8 March 1968. These deployments were code named ""Glowing Heat", while the program as a whole was code named "Senior Crown". Reconnaissance missions over North Vietnam were code named "Giant Scale"". On 21 March 1968, Major (later General) Jerome F. O'Malley and Major Edward D. Payne flew the first operational SR-71 sortie in SR-71 serial number 61-7976 from Kadena AB, Okinawa. During its career, this aircraft (976) accumulated 2,981 flying hours and flew 942 total sorties (more than any other SR-71), including 257 operational missions, from Beale AFB; Palmdale, California; Kadena Air Base, Okinawa, Japan; and RAF Mildenhall, UK. The aircraft was flown to the National Museum of the United States Air Force near Dayton, Ohio in March 1990.
From the beginning of the Blackbird's reconnaissance missions over enemy territory (North Vietnam, Laos, etc.) in 1968, the SR-71s averaged approximately one sortie a week for nearly two years. By 1970, the SR-71s were averaging two sorties per week, and by 1972, they were flying nearly one sortie every day. Two SR-71s were lost during these missions, one in 1970 and the second aircraft in 1972, both due to mechanical malfunctions.
While deployed in Okinawa, the SR-71s and their aircrew members gained the nickname Habu (as did the A-12s preceding them) after a pit viper indigenous to Japan, which the Okinawans thought the plane resembled.
Swedish Air Force fighter pilots, using the predictable patterns of SR-71 routine flights over the Baltic Sea, managed to lock their radar on the SR-71 on numerous occasions. Despite heavy jamming from the SR-71, target illumination was maintained by feeding target location from ground-based radars to the fire-control computer in the JA 37 Viggen interceptor. The most common site for the lock-on to occur was the thin stretch of international airspace between Öland and Gotland that the SR-71 used on the return flight.
Operational highlights for the entire Blackbird family (YF-12, A-12, and SR-71) as of about 1990 included:
Only one crew member, Jim Zwayer, a Lockheed flight-test reconnaissance and navigation systems specialist, was killed in a flight accident. The rest of the crew members ejected safely or evacuated their aircraft on the ground.
Initial retirement.
Dick Cheney told the Senate Appropriations Committee that the SR-71 cost $85,000 per hour to operate. Opponents estimated a cost of $400 million per year to support the aircraft; this number was subsequently reduced to $260 million. As parts were no longer being manufactured, other airframes had to be cannibalized to keep the fleet airworthy. The lack of a datalink (unlike the Lockheed U-2) meant that imagery and radar data could not be used in real time, but had to wait until the aircraft returned to base. The Air Force saw the SR-71 as a bargaining chip which could be sacrificed to ensure the survival of other priorities. A general misunderstanding of the nature of aerial reconnaissance and a lack of knowledge about the SR-71 in particular (due to its secretive development and usage) was used by detractors to discredit the aircraft, with the assurance given that a replacement was under development. 
In 1988, Congress was convinced to allocate $160,000 to keep six SR-71s (along with a trainer model) in flyable storage that could become flightworthy within 60 days; however, the USAF refused to spend the money. While the SR-71 survived attempts to retire it in 1988, partly due to the unmatched ability to provide high-quality coverage of the Kola Peninsula for the US Navy, the decision to retire the SR-71 from active duty came in 1989, with the last missions flown in October that year. Four months after the plane's retirement, General Norman Schwarzkopf, Jr., was told that the expedited reconnaissance, which the SR-71 could have provided, was unavailable during Operation Desert Storm.
Reactivation.
"From the operator's perspective, what I need is something that will not give me just a spot in time but will give me a track of what is happening. When we are trying to find out if the Serbs are taking arms, moving tanks or artillery into Bosnia, we can get a picture of them stacked up on the Serbian side of the bridge. We do not know whether they then went on to move across that bridge. We need the [data] that a tactical, an SR-71, a U-2, or an unmanned vehicle of some sort, will give us, in addition to, not in replacement of, the ability of the satellites to go around and check not only that spot but a lot of other spots around the world for us. It is the integration of strategic and tactical."
Due to unease over political situations in the Middle East and North Korea, the U.S. Congress re-examined the SR-71 beginning in 1993. Rear Admiral Thomas F. Hall addressed the question of why the SR-71 was retired, saying it was under "the belief that, given the time delay associated with mounting a mission, conducting a reconnaissance, retrieving the data, processing it, and getting it out to a field commander, that you had a problem in timelines that was not going to meet the tactical requirements on the modern battlefield. And the determination was that if one could take advantage of technology and develop a system that could get that data back real time... that would be able to meet the unique requirements of the tactical commander." Hall stated they were "looking at alternative means of doing [the job of the SR-71]."
Macke told the committee that they were "flying U-2s, RC-135s, [and] other strategic and tactical assets" to collect information in some areas. Senator Robert Byrd and other Senators complained that the "better than" successor to the SR-71 had yet to be developed at the cost of the "good enough" serviceable aircraft. They maintained that, in a time of constrained military budgets, designing, building, and testing an aircraft with the same capabilities as the SR-71 would be impossible.
Congress' disappointment with the lack of a suitable replacement for the Blackbird was cited concerning whether to continue funding imaging sensors on the U-2. Congressional conferees stated the "experience with the SR-71 serves as a reminder of the pitfalls of failing to keep existing systems up-to-date and capable in the hope of acquiring other capabilities." It was agreed to add $100 million to the budget to return three SR-71s to service, but it was emphasized that this "would not prejudice support for long-endurance UAVs [such as the Global Hawk]." The funding was later cut to $72.5 million. The Skunk Works was able to return the aircraft to service under budget at $72 million.
Colonel Jay Murphy (USAF Retired) was made the Program Manager for Lockheed's reactivation plans. Retired Air Force Colonels Don Emmons and Barry MacKean were put under government contract to remake the plane's logistic and support structure. Still-active Air Force pilots and Reconnaissance Systems Officers (RSOs) who had worked with the aircraft were asked to volunteer to fly the reactivated planes. The aircraft was under the command and control of the 9th Reconnaissance Wing at Beale Air Force Base and flew out of a renovated hangar at Edwards Air Force Base. Modifications were made to provide a data-link with "near real-time" transmission of the Advanced Synthetic Aperture Radar's imagery to sites on the ground.
Final retirement.
The reactivation met much resistance: the Air Force had not budgeted for the aircraft, and UAV developers worried that their programs would suffer if money was shifted to support the SR-71s. Also, with the allocation requiring yearly reaffirmation by Congress, long-term planning for the SR-71 was difficult. In 1996, the Air Force claimed that specific funding had not been authorized, and moved to ground the program. Congress reauthorized the funds, but, in October 1997, President Bill Clinton attempted to use the line-item veto to cancel the $39 million allocated for the SR-71. In June 1998, the U.S. Supreme Court ruled that the line-item veto was unconstitutional. All this left the SR-71's status uncertain until September 1998, when the Air Force called for the funds to be redistributed; the Air Force permanently retired it in 1998. NASA operated the two last airworthy Blackbirds until 1999. All other Blackbirds have been moved to museums except for the two SR-71s and a few D-21 drones retained by the NASA Dryden Flight Research Center.
Records.
The SR-71 was the world's fastest and highest-flying operational manned aircraft throughout its career. On 28 July 1976, SR-71 serial number 61-7962, broke the world record: an "absolute altitude record" of 85069 ft. Several aircraft have exceeded this altitude in zoom climbs, but not in sustained flight. That same day SR-71, serial number 61-7958, set an absolute speed record of 1905.81 kn, approximately Mach 3.3. SR-71 pilot Brian Shul states in his book "The Untouchables" that he flew in excess of Mach 3.5 on 15 April 1986 over Libya to evade a missile.
The SR-71 also holds the "Speed Over a Recognized Course" record for flying from New York to London—distance 3461.53 mi, 1806.964 mph, and an elapsed time of 1 hour 54 minutes and 56.4 seconds—set on 1 September 1974 while flown by U.S. Air Force Pilot Maj. James V. Sullivan and Maj. Noel F. Widdifield, reconnaissance systems officer (RSO). This equates to an average velocity of about Mach 2.72, including deceleration for in-flight refueling. Peak speeds during this flight were likely closer to the declassified top speed of Mach 3.2+. For comparison, the best commercial Concorde flight time was 2 hours 52 minutes and the Boeing 747 averages 6 hours 15 minutes.
On 26 April 1971, 61-7968, flown by Majors Thomas B. Estes and Dewain C. Vick, flew over 15000 mi in 10 hrs. 30 min. This flight was awarded the 1971 Mackay Trophy for the "most meritorious flight of the year" and the 1972 Harmon Trophy for "most outstanding international achievement in the art/science of aeronautics".
When the SR-71 was retired in 1990, one Blackbird was flown from its birthplace at United States Air Force Plant 42 in Palmdale, California, to go on exhibit at what is now the Smithsonian Institution's Steven F. Udvar-Hazy Center in Chantilly, Virginia. On 6 March 1990, Lt. Col. Raymond E. "Ed" Yielding and Lt. Col. Joseph T. "JT" Vida piloted SR-71 S/N 61-7972 on its final Senior Crown flight and set four new speed records in the process.
These four speed records were accepted by the National Aeronautic Association (NAA), the recognized body for aviation records in the United States. Additionally, Air & Space reported that the Air Force clocked the Blackbird at one point in its flight reaching 2,242.48 mph. After the Los Angeles–Washington flight, Senator John Glenn addressed the United States Senate, chastening the Department of Defense for not using the SR-71 to its full potential:
Mr. President, the termination of the SR-71 was a grave mistake and could place our nation at a serious disadvantage in the event of a future crisis. Yesterday's historic transcontinental flight was a sad memorial to our short-sighted policy in strategic aerial reconnaissance.—Senator John Glenn, 7 March 1990
Successor.
Speculation existed regarding a replacement for the SR-71, most notably a rumored aircraft codenamed Aurora. The limitations of reconnaissance satellites, which take up to 24 hours to arrive in the proper orbit to photograph a particular target, makes them slower to respond to demand than reconnaissance planes. The fly-over orbit of spy satellites may also be predicted and can allow assets to be hidden when the satellite is above, a drawback not shared by aircraft. Thus, there are doubts that the US has abandoned the concept of spy planes to complement reconnaissance satellites. Unmanned aerial vehicles (UAVs) are also used for much aerial reconnaissance in the 21st century, being able to overfly hostile territory without putting human pilots at risk, as well as being smaller and harder to detect than man-carrying aircraft.
On 1 November 2013, media outlets reported that Skunk Works has been working on an unmanned reconnaissance airplane it has named SR-72, which would fly twice as fast at Mach 6. However, the Air Force is officially pursuing the Northrop Grumman RQ-180 UAV to take up the SR-71's strategic ISR role.
Operators.
United States Air Force
National Aeronautics and Space Administration (NASA)
Accidents and aircraft disposition.
Twelve SR-71s were lost and one pilot died in accidents during the aircraft's service career. Eleven of these accidents happened between 1966 and 1972.
Notes: Many secondary references use apparently incorrect 64- series aircraft serial numbers ("e.g." SR-71C 64-17981), but no primary government documents have been found to support this.
After completion of all USAF and NASA SR-71 operations at Edwards AFB, the SR-71 Flight Simulator was moved in July 2006 to the Frontiers of Flight Museum at Love Field Airport in Dallas, Texas.
References.
Bibliography.
</dl>
Additional sources
</dl>

</doc>
<doc id="55247" url="http://en.wikipedia.org/wiki?curid=55247" title="Utah Phillips">
Utah Phillips

Bruce Duncan "Utah" Phillips (May 15, 1935 – May 23, 2008) was an American labor organizer, folk singer, storyteller, poet and the "Golden Voice of the Great Southwest". He described the struggles of labor unions and the power of direct action, self-identifying as an anarchist. He often promoted the Industrial Workers of the World in his music, actions, and words.
Biography.
Early years.
Phillips was born in Cleveland, Ohio, to Edwin Deroger Phillips and Frances Kathleen Coates. His father, Edwin Phillips, was a labor organizer, and his parents' activism influenced much of his life's work. Phillips was a card-carrying member of the Industrial Workers of the World (Wobblies) headquartered in Chicago, Illinois. His parents divorced and his mother remarried. Phillips was adopted by his stepfather, Syd Cohen, at the age of five. Cohen managed the Hippodrome Theater in Cleveland, one of the last vaudeville houses in the city. Cohen moved the family to Salt Lake City, Utah, where he managed the Lyric Theater, another vaudeville house. Phillips attributes his early exposure to vaudeville through his stepfather as being an important influence on his later career.
Phillips attended East High School in Salt Lake City, where he was involved in the arts and plays. He served in the United States Army for three years beginning in 1956 (at the latest). Witnessing the devastation of post-war Korea greatly influenced his social and political thinking. After discharge from the army, Phillips rode the railroads, and wrote songs.
Career.
While riding the rails and tramping around the west, Phillips returned to Salt Lake City, where he met Ammon Hennacy from the Catholic Worker Movement. He gave credit to Hennacy for saving him from a life of drifting to one dedicated to using his gifts and talents toward activism and public service. Phillips assisted him in establishing a mission house of hospitality named after the activist Joe Hill. Phillips worked at the Joe Hill House for the next eight years, then ran for the U.S. Senate as a candidate of Utah's Peace and Freedom Party in 1968. He received 2,019 votes (0.5%) in an election won by Republican Wallace F. Bennett. He also ran for president of the United States in 1976 for the Do-Nothing Party.
He adopted the name U. Utah Phillips in emulation of country vocalist T. Texas Tyler.
Phillips met folk singer Rosalie Sorrels in the early 1950s, and remained a close friend of hers. Sorrels started playing the songs that Phillips wrote, and through her his music began to spread. After leaving Utah in the late 1960s, he went to Saratoga Springs, New York, where he was befriended by the folk community at the Caffé Lena coffee house. He became a staple performer there for a decade, and would return throughout his career.
Phillips was a proud member of the Industrial Workers of the World (IWW or Wobblies). His view of unions and politics were shaped by his parents, especially his Mom who was a labor organizer for the CIO. But Phillips was more of a Christian anarchist and a pacifist, so found the modern-day Wobblies to be the perfect fit for him, an iconoclast and artist. In recent years, perhaps no single person did more to spread the Wobbly gospel than Phillips, whose countless concerts were, in effect, organizing meetings for the cause of labor, unions, anarchism, pacifism, and the Wobblies. He was a tremendous interpreter of classic Wobbly tunes including "Hallelujah, I'm a Bum," "The Preacher and the Slave," and "Bread and Roses."
An avid trainhopper, Phillips recorded several albums of music related to the railroads, especially the era of steam locomotives. His first recorded album, "Good Though!", is an example, and contains such songs as "Daddy, What's a Train?" and "Queen of the Rails" as well as what may be his most famous composition, "Moose Turd Pie" wherein he tells a tall tale of his work as a gandy dancer repairing track in the Southwestern United States desert.
In 1991 Phillips recorded, in one take, an album of song, poetry and short stories entitled "I've Got To Know", inspired by his anger at the first Gulf War. The album includes "Enola Gay," his first composition written about the United States' atomic attack on Hiroshima and Nagasaki.
Phillips was a mentor to Kate Wolf. He recorded songs and stories with Rosalie Sorrels on a CD called "The Long Memory" (1996), originally a college project "Worker's Doxology" for 1992 'cold-drill Magazine' Boise State University. His protégée, Ani DiFranco, recorded two CDs, "The Past Didn't Go Anywhere" (1996) and "Fellow Workers" (1999), with him. He was nominated for a Grammy Award for his work with DiFranco. His "Green Rolling Hills" was made into a country hit by Emmylou Harris, and "The Goodnight-Loving Trail" became a classic as well, being recorded by Ian Tyson, Tom Waits, and others.
Later years.
Though known primarily for his work as a concert performer and labor organizer, Phillips also worked as an archivist, dishwasher, and warehouse-man.
Phillips was a member of various socio-political organizations and groups throughout his life. A strong supporter of labor struggles, he was a member of the Industrial Workers of the World (IWW), the International Union of Mine, Mill, and Smelter Workers (Mine Mill), and the Travelling Musician's Union AFM Local 1000. In solidarity with the poor, he was also an honorary member of Dignity Village, a homeless community. A pacifist, he was a member of Veterans for Peace and the Peace Center of Nevada County.
In his personal life, Phillips enjoyed varied hobbies and interests. These included Egyptology; amateur chemistry; linguistics; history (Asian, African, Mormon and world); futhark; debate; and poetry. He also enjoyed culinary hobbies, such as pickling, cooking and gardening.
He married Joanna Robinson on July 31, 1989, in Nevada City.
Phillips became an elder statesman for the folk music community, and a keeper of stories and songs that might otherwise have passed into obscurity. He was also a member of the great Traveling Nation, the community of hobos and railroad bums that populates the Midwest United States along the rail lines, and was an important keeper of their history and culture. He also became an honorary member of numerous folk societies in the U.S.A. and Canada.
When Kate Wolf grew ill and was forced to cancel concerts, she asked Phillips to fill in. Suffering from an ailment which makes it more difficult to play guitar, Phillips hesitated, citing his declining guitar ability. "Nobody ever came just to hear you play," she said. Phillips told this story as a way of explaining how his style over the years became increasingly based on storytelling instead of just songs. He was a gifted storyteller and monologist, and his concerts generally had an even mix of spoken word and sung content. He attributed much of his success to his personality. "It is better to be likeable than talented," he often said, self-deprecatingly.
Until it lost its funding, Phillips hosted his own weekly radio show, "Loafer's Glory: The Hobo Jungle of the Mind," originating on KVMR and nationally syndicated.
Phillips lived in Nevada City, California, for 21 years where he worked on the start-up of the Hospitality House, a homeless shelter, and the Peace and Justice Center. "It's my town. Nevada City is a primary seed-bed for community organizing."
In August 2007, Phillips announced that he would undergo catheter ablation to address his heart problems. Later that autumn, Phillips announced that due to health problems he could no longer tour. By January 2008, he decided against a heart transplant.
Phillips died May 23, 2008 in Nevada City, California, from complications of heart disease, at the age of 73. He was survived by his wife, sons Duncan and Brendan, and a daughter, Morrigan. Following a private service, a public memorial was held on June 1, in Pioneer Park, in Nevada City. His service was officiated by Meghan Cefalu, a Unitarian Universalist pastor. Phillips is buried in Forest View Cemetery in Nevada City.
Personal papers.
Archival materials related to Phillips' personal and professional life are open for research at the Walter P. Reuther Library in Detroit, Michigan. include correspondence, interviews, writings, notes, contracts, flyers, publications, articles, clippings, photographs, audiovisual recordings, and other materials.

</doc>
<doc id="55248" url="http://en.wikipedia.org/wiki?curid=55248" title="Kalamazoo College">
Kalamazoo College

Kalamazoo College, also known as K College or simply K, is a private liberal arts college in Kalamazoo, Michigan. Founded in 1833, the college is among the 100 oldest in the country. Today, it produces more Peace Corps volunteers per capita than any other U.S. academic institution. From 1997 to 2006 it ranked 21st among all baccalaureate institutions in the percentage of graduates who went on to earn doctorates. The school was founded by American Baptist ministers, but today maintains no religious affiliation.
Kalamazoo College is a member of the Great Lakes Colleges Association. It is listed in Loren Pope's "Colleges That Change Lives". In 2012, Forbes rated it 65th of America's Best Colleges, the highest ranked in Michigan as a private college.
History.
Kalamazoo College was founded in 1833 by a group of Baptist ministers as the Michigan and Huron Institute. Its charter was granted on April 22, 1833, the first school chartered by the Legislative Council of the Territory of Michigan. Instruction at the Institute began in fall 1836. In 1837, the name of the fledgling college was changed to the "Kalamazoo Literary Institute" and school officials made their first attempt to secure recognition as a college from the state of Michigan. In 1838, however, the University of Michigan opened the Kalamazoo Branch of the University of Michigan, providing a local competitor to the Literary Institute. In 1840, the two schools merged, and from 1840 to 1850 the College operated as the Kalamazoo Branch of the University of Michigan. In 1850, the Kalamazoo Literary Institute name was restored and in 1855 the school finally received an educational charter from the State of Michigan, establishing explicit recognition of the school as a college. After receiving its educational charter, the school changed its name to Kalamazoo College.
James Stone, the first president of Kalamazoo College, led the school from 1842 through 1863 and was responsible for instituting the high academic standards that allowed the College to receive its charter. Shortly after becoming president, Stone proposed the addition of a theological seminary to increase the supply of ministers in the region. With the support of the Baptist church, classes at the Kalamazoo Theological Seminary began in 1848 with 11 students. At the same time, the Female Department continued to expand under the watchful eye of Lucinda Hinsdale Stone. In 1845-46, almost half of the 90 students enrolled in Kalamazoo were women.
The Stones also played a role in the creation of the Republican Party. A meeting of disgruntled Michigan Whigs, Democrats, and abolitionists at the Stones' Kalamazoo residence set the date for an anti-slavery convention in Jackson, Michigan, which resulted in the formal birth of the Republican Party.
The first known student of African descent to attend Kalamazoo College was ex-slave Rufus Lewis Perry. Perry attended Kalamazoo Theological Seminary from 1860–1861, but left before he received a diploma. He was ordained a Baptist minister in Ann Arbor in 1861, and later earned a Ph.D. from State University in Louisville, Kentucky. Jamaican-born brothers Solomon and John Williamson were the first black graduates from "K," receiving their diplomas in 1911. Kalamazoo College also served as a pioneer in coed education, granting its first degree to a woman, Miss Catherine V. Eldred, in 1870.
In 1877, Kalamazoo College students published the first edition of "The Index", a student-run newspaper that continues to publish today. The college also publishes "The Cauldron", an annual literary-arts journal; and "The Passage", an annual compilation of students' work from study abroad.
Kalamazoo College's reputation as an academic powerhouse and a leader in international education was built during the presidency of Weimer Hicks, who served from 1954 to 1971. Hicks conceived of the "K Plan" program under which most Kalamazoo students spend at least one term abroad and spend at least one term working in an academic internship. As part of the original "K Plan," Kalamazoo College students could attend school year-round. One typical pattern was:
Variations to this schedule — such as spring-term study-abroad programs, full-year study-abroad programs, and winter SIPs — were also common. However, the college scrapped its summer term in 1996 due to the difficulty of attracting students to a year-round college.
Academics.
Academic distinctions.
Kalamazoo College is among the 100 oldest colleges and universities in the United States. It offers 28 majors spread across the fields of Fine Arts, Humanities, Modern and Classical Languages and Literatures, Natural Sciences and Mathematics, Physical Education, and Social Sciences. There are 11 unique interdisciplinary majors as well. It is consistently considered as one of the best liberal arts colleges in the country for experiential learning, study abroad, and academics. It is ranked number five in The Washington Post's Hidden Gems college list, as the best school in Michigan and 52nd best college in the nation by Forbes, 68th in U.S. News & World Report's category of national liberal arts colleges, and is listed in Loren Pope's Colleges That Change Lives.
A recent study by Higher Education Data Sharing lists Kalamazoo College in the top 1 percent of colleges and universities whose graduates go on to earn a Ph.D. According to this study, Kalamazoo College is ranked number nine among all private colleges and — when compared with all academic institutions — it ranks number fourteen in Ph.Ds per capita. Among all undergraduate institutions, Kalamazoo College was #1 per capita in 2005 for recruitment of Peace Corps volunteers.
The student-to-faculty ratio is 12:1. Ninety-five percent of Kalamazoo College's faculty have doctorates or terminal degrees in their fields.
On January 3, 2006, Kalamazoo College opened the new Upjohn Library Commons which includes the completely renovated skeleton of the older library, and an extension which adds to its volume capacity.
The K Plan.
Kalamazoo College is an academic leader among national liberal arts colleges and emphasizes the importance of experiential education. The academic plan — known as the "K plan" — consists of a rigorous liberal arts education supplemented by experience abroad and in the Kalamazoo community.
Students at Kalamazoo College must fulfill specific degree requirements in order to graduate, as well as completion of three Shared Passages Seminars during the first, sophomore, and senior years at Kalamazoo. First-year seminars focus on developing writing and communication skills, sophomore seminars emphasize international culture and experience in preparation for study abroad, and senior seminars focus on major specific or interdisciplinary topics to cap a student's education experience. Upon graduation, students must demonstrate a proficiency in a second language at an intermediate level, satisfy a quantitative reasoning requirement, and complete a senior individualized project which may take the form of a thesis, an artistic performance, or any other work-intensive project of a student's choosing. These experiences are supplemented by one or more terms abroad, service-learning projects during school terms, and internship opportunities during the summer.
Service-Learning.
Kalamazoo College initiated the service-learning program in 1997. In 2001, Trustee Ronda Stryker dedicated to her grandmother the Mary Jane Underwood Stryker Institute for Service Learning. This Institute was created to house several Service-Learning programs in the school. The current director of the Mary Jane Underwood Stryker Institute is Alison Geist. In 2008, Kalamazoo College had twenty-three on-going service-learning programs. There are several courses in the college that incorporate service-learning into their curricula. The programs in service-learning include Community Advocates for Parents and Students; Helping Youth through Personal Empowerment; Academic Mentorship In Giants On-going Success; the Woodward School; Keeping the Doors Open; and Farms to K.
Study abroad.
US News & World Report America's Best Colleges 2003 ranked Kalamazoo College's study-abroad program number one in the country. Nearly 85% of Kalamazoo College students spend at least one term abroad and the college maintains partnerships with over 50 foreign universities on six continents. 
Center for Career and Professional Development.
In 2009, the Center Career Development merged with the to create the . The CCPD's mission is to create meaningful connections to the world of work, empowering Kalamazoo College students to explore, identify and pursue their diverse interests, values and passions, and to develop a framework of skills, networks and knowledge for successful lifelong career planning and professional development. Unique opportunities through the CCPD include the and the .
Athletics.
The college's sports teams are known as the Hornets. They compete in the NCAA's Division III and the Michigan Intercollegiate Athletic Association (MIAA). As of 2013, the Hornet Men's tennis squad have won their conference's championship 77 consecutive years. Kalamazoo College competes in the following sports:
Football.
College football has been played at Kalamazoo since 1892, when the school completed a record of 0 wins and 2 losses, both to Olivet College. The school's first win came two years later in 1894 with a 16–4 victory over the Kalamazoo YMCA. It was 1895 before the school defeated another college football team with a 12–8 victory over Alma.
In 1897, the first coach came to the program with Charles Hall who led the team to a record of 7 wins and 1 loss, earning the Michigan Intercollegiate Athletic Association championship. The current coach is Jamie Zorbo.
Men's Tennis.
The Kalamazoo College men’s tennis team has won 77 consecutive Michigan Intercollegiate Athletic Association championships (1936–2013) with a record of 426-2 in the MIAA from 1935 - 2007. Kalamazoo has won seven NCAA Division III national championships and has made 25 consecutive NCAA III tournament appearances.
National Runners-up - NCAA Division II:
National Championships - NCAA Division III:
National Runners-up - NCAA Division III:
Men's swimming and diving.
Men's swimming and diving at Kalamazoo College has an impressive history. The team is known for producing individual national champions in the pool and on the boards, and also for maintaining a national presence with regular appearances as a top-10 team at the NCAA Division III national championships. Their highest finish was 4th in 2010. The swimming and diving team is the second most successful athletic program at Kalamazoo College, after the men's tennis team, and it is also one of the top-10 most-successful teams in the MIAA with 27 MIAA championships.
Men's basketball.
Basketball dispute.
In 2001, the Kalamazoo College men's basketball team was at the center of a lengthy dispute regarding the outcome of a January 20 game with league rival Olivet College. With Olivet leading 70-69, Kalamazoo College center made a shot at the buzzer that was initially waved off by referees. The referees reviewed videotape of the game and determined that the player had, in fact, released his shot before the buzzer and then awarded Kalamazoo College a 71-70 victory. After the game, Olivet filed a protest with the conference commissioner, claiming that officials had misapplied the way in which videotape may be used. On January 23, the conference upheld the protest and awarded Olivet the victory. Kalamazoo then filed a protest with the NCAA, claiming that Olivet's protest was in violation of NCAA bylaws. On February 1, the NCAA upheld Kalamazoo's counter-protest and again awarded the game to the Hornets. The dispute between Olivet and Kalamazoo received national attention and the shot was shown repeatedly on ESPN.
Women's lacrosse.
In the 2013-14 academic year, women's lacrosse became a varsity sport at Kalamazoo College. It is the college's first new varsity athletic program since 1991. Women's lacrosse previously existed as one of K College's student-run club sports. In Spring 2012, the school announced the new program, as well as the hiring of Emilia Ward for the position of Head Coach. Ward previously coached at Winthrop University, and Adrian College, after lettering four years in women's lacrosse at Manhattan University in Bronx, N.Y.
Fight song.
The words to the college fight-song, "All Hail to Kazoo," were written by A. G. Walton (1911) with music by D. R. Belcher (1909), arranged by Burton Edward Fischer.
Student life and traditions.
Student organizations are one of the main sources of entertainment for the student body. They routinely bring in speakers as well as stage performances, dances, and movie showings.
During the fall quarter, there are two main events: Fall Fest and Homecoming dance. In Fall Fest, student organizations provide activities for the students, such as pumpkin carving and bobbing for apples.
During the winter quarter, the college holds the annual Monte Carlo night, on which the student body raises money by gambling in a makeshift casino where the professors are the dealers. They play for scrip redeemable for prizes.
Crystal Ball.
Kaleidoscope (formerly known as the Gay, Lesbian, Bisexual, Transgender, and Ally Student Organization, GLBTSO) hosts the Crystal Ball each spring. Crystal Ball is a college-sponsored dance in which attendees dress in drag or unusual costumes. A long-standing tradition at K College, this event was created to educate the campus about GLBT issues and celebrate the persons who make up the GLBT community. This very popular event features live music, dancing, and contests.
The Quad.
The campus is built around a grassy hill known as "The Quad". The Quad is also the site of numerous large-scale events throughout the year, including Homecoming, Spring Fling, Convocation, and Commencement. Furthermore, at the top of the hill sits Stetson Chapel, which is a favorite location for alumni wedding services. The Quad is home to another popular Kalamazoo College student tradition, "streaking the Quad", a noisy, late-night descent from the Chapel, down the hill, and back to the top again. Tradition dictates that students must touch the school sign before returning to the top. There is a mass-streak after the spring performance by Frelon, the Kalamazoo College dance group, and also during the day by the senior class. This often coincides with a wedding.
Day of Gracious Living.
Since 1974, the college has upheld a springtime tradition of canceling all classes for a “Day of Gracious Living” (DOGL). While it was originally instituted (despite the Student Commission's rejection) as a day for students to relax and have fun, the 1980 Kalamazoo tornado prompted students to spend that year's DOGL helping clean up after the storm and giving back to their community. Many students enjoy the day at the North Beach in South Haven, Michigan. The date is determined by the president of the Student Commission and kept secret from the student body, though it is usually on one of the Wednesdays during weeks 7-9. On the morning of the Day of Gracious Living, the bells of Stetson Chapel ring, announcing the day to the student body.
Recycling program.
Kalamazoo College has become a leading institution in the area of recycling and environmental awareness. A crew of student workers operates one of the nation's most successful recycling programs and organizes the school's participation in the annual RecycleMania event, a competition among over 400 colleges and universities across the United States. In 2005, Kalamazoo College came to national prominence with a 3rd-place finish in the Grand Champion category. While annually placing in the top 5 in a variety of categories, in 2008 Kalamazoo College placed 1st in both the Grand Champion and Stephen K Gaski Per Capita Classic competitions.
Sustainability.
Kalamazoo College signed the President's Climate Commitment in 2007 and has completed a greenhouse gas emissions inventory. The college's Hicks Student Center is partially powered by wind and solar energy, and the student group D.I.R.T. (Digging in Renewable Turf) maintains an organic garden on campus. The spring 2009 Energy Sting competition encouraged students to reduce their energy consumption. Kalamazoo received a B on the 2010 College Sustainability Report Card.
Presidents of Kalamazoo College.
In 2005, Eileen Wilson-Oyelaran became Kalamazoo College's 17th President and first female president, as well as the first African-American president of the school. She is the 22nd President overall, including interim and acting presidents. Her immediate predecessors are Bernard Palchick, who served as interim president and returned to the administration; and James F. Jones, who departed to become President of Trinity College, in Connecticut.

</doc>
<doc id="55249" url="http://en.wikipedia.org/wiki?curid=55249" title="Skunk Works">
Skunk Works

Skunk Works is an official alias for Lockheed Martin's Advanced Development Programs (ADP), formerly called Lockheed Advanced Development Projects. Skunk Works is responsible for a number of famous aircraft designs, including the U-2, the Lockheed SR-71 Blackbird, the Lockheed F-117 Nighthawk, and the Lockheed Martin F-22 Raptor. Currently its main, largest officially known project is the Lockheed Martin F-35 Lightning II, which will be used in the air forces of several countries. Production is expected to last for up to four decades. The name "Skunk Works" was taken from the moonshine factory in the comic strip "Li'l Abner".
The designation "skunk works" or "skunkworks" is widely used in business, engineering, and technical fields to describe a group within an organization given a high degree of autonomy and unhampered by bureaucracy, tasked with working on advanced or secret projects.
History.
There are conflicting observations about the birth of Skunk Works.
Ben Rich and "Kelly" Johnson set the origin as June 1943 in Burbank, California; they relate essentially the same chronology in their autobiographies. Theirs is the official Lockheed Skunk Works story:
The Air Tactical Service Command (ATSC) of the Army Air Force met with Lockheed Aircraft Corporation to express its need for a jet fighter. A rapidly growing German jet threat gave Lockheed an opportunity to develop an airframe around the most powerful jet engine that the allied forces had access to, the British Goblin. Lockheed was chosen to develop the jet because of its past interest in jet development and its previous contracts with the Air Force. One month after the ATSC and Lockheed meeting, the young engineer Clarence L. “Kelly” Johnson and other associate engineers hand delivered the initial XP-80 proposal to the ATSC. Two days later the go-ahead was given to Lockheed to start development and the Skunk Works was born, with Kelly Johnson at the helm.
The formal contract for the XP-80 did not arrive at Lockheed until October 16, 1943; some four months after work had already begun. This would prove to be a common practice within the Skunk Works. Many times a customer would come to the Skunk Works with a request and on a handshake the project would begin, no contracts in place, no official submittal process. Kelly Johnson and his Skunk Works team designed and built the XP-80 in only 143 days, seven less than was required.
Warren M. Bodie, journalist, historian, and Skunk Works engineer from 1977 to 1984, wrote that engineering independence, elitism and secrecy of the Skunk Works variety were demonstrated earlier when Lockheed was asked by Lieutenant Benjamin S. Kelsey (later air force brigadier general) to build for the United States Army Air Corps a high speed, high altitude fighter to compete with German aircraft. In July 1938, while the rest of Lockheed was busy tooling up to build Hudson reconnaissance bombers to fill a British contract, a small group of engineers was assigned to fabricate the first prototype of what would become the P-38 Lightning. Kelly Johnson set them apart from the rest of the factory in a walled-off section of one building, off limits to all but those involved directly. Secretly, a number of advanced features were being incorporated into the new fighter including a significant structural revolution in which the aluminum skin of the aircraft was joggled, fitted and flush-riveted, a design innovation not called for in the army's specification but one that would yield less aerodynamic drag and give greater strength with lower mass. As a result, the XP-38 was the first 400 mph fighter in the world. In November 1941, Kelsey gave the unofficial nod to Johnson and the P-38 team to engineer a drop tank system to extend range for the fighter, and they completed the initial research and development without a contract. When the Army Air Forces officially asked for a range extension solution it was ready. Some of the group of independent-minded engineers were later involved with the XP-80 project, the prototype of the P-80 Shooting Star.
Mary G. Ross, the first Native American female engineer, was among the 40 founding engineers.
1950s and beyond.
In 1955, the Skunk Works received a contract from the CIA to build a spyplane known as the U-2 with the intention of flying over the Soviet Union and photographing sites of strategic interest. The U-2 was tested at Groom Lake in the Nevada desert, and the Flight Test Engineer in charge was Joseph F. Ware, Jr. The first overflight took place on July 4 1956. The U-2 ceased overflights when Francis Gary Powers was shot down during a mission on May 1, 1960, while over Russia.
The Skunk Works had predicted that the U-2 would have a limited operational life over the Soviet Union. The CIA agreed. In late 1959, the Skunk Works received a contract to build five A-12 aircraft at a cost of $96 million. Building a Mach 3.0+ aircraft out of titanium posed enormous difficulties, and the first flight did not occur until 1962. (Titanium supply was largely dominated by the Soviet Union, so the CIA set up a dummy corporation to acquire source material.) Several years later, the U.S. Air Force became interested in the design, and it ordered the SR-71 Blackbird, a two-seater version of the A-12. This aircraft first flew in 1966 and remained in service until 1998, and the Flight Test Engineer in charge was Joseph F. Ware, Jr.
The D-21 drone, similar in design to the Blackbird, was built to overfly the Lop Nur nuclear test facility in China. This drone was launched from the back of a specially modified A-12, known as M-21, of which there were two built. After a fatal mid-air collision on the fourth launch, the drones were re-built as D-21Bs, and launched with a rocket booster from B-52s. Four operational missions were conducted over China, but the camera packages were never successfully recovered.
Kelly Johnson headed the Skunk Works until 1975. He was succeeded by Ben Rich.
In 1976, the Skunk Works began production on a pair of stealth technology demonstrators for the U.S. Air Force named Have Blue in Building 82 at Burbank. These scaled-down demonstrators, built in only 18 months, were a revolutionary step forward in aviation technology. After a series of successful test flights beginning in 1977, the Air force awarded Skunk Works the contract to build the F-117 stealth fighter on November 1, 1978.
During the entirety of the Cold War, the Skunk Works was located in Burbank, California on the eastern side of Burbank-Glendale-Pasadena Airport (). After 1989, Lockheed reorganized its operations and relocated the Skunk Works to Site 10 at U.S. Air Force Plant 42 in Palmdale, California, where it remains in operation today. Most of the old Skunk Works buildings in Burbank were demolished in the late 1990s to make room for parking lots. One main building still remains at 2777 Ontario Street in Burbank (near San Fernando Road), now used as an office building for digital film post production and sound mixing.
Term origin.
The term "Skunk Works" came from Al Capp's satirical, hillbilly comic strip "Li’l Abner", which was immensely popular in the 1940s and '50s. The “Skonk Works" was a dilapidated factory located on the remote outskirts of Dogpatch, in the backwoods of Kentucky. According to the strip, scores of locals were done in yearly by the toxic fumes of the concentrated "skonk oil", which was brewed and barreled daily by "Big Barnsmell" (known as the lonely "inside man" at the Skonk Works), by grinding dead skunks and worn shoes into a smoldering still, for some mysterious, never specified purpose.
The original Lockheed facility, during the development of the P-80 Shooting Star, was located adjacent to a malodorous plastics factory. According to Ben Rich’s memoir, an engineer jokingly showed up to work one day wearing a Civil Defense gas mask. To comment on the smell and the secrecy the project entailed, another engineer, Irv Culver, referred to the facility as "Skonk Works". As the development was very secret, the employees were told to be careful even with how they answered phone calls. One day, when the Department of the Navy was trying to reach the Lockheed management for the P-80 project, the call was accidentally transferred to Culver’s desk. Culver answered the phone in his trademark fashion of the time, by picking up the phone and stating "Skonk Works, inside man Culver". "What?" replied the voice at the other end. "Skonk Works", Culver repeated. The name stuck. Culver later said at an interview conducted in 1993 that "when Kelly Johnson heard about the incident, he promptly fired me. It didn’t really matter, since he was firing me about twice a day anyways."
At the request of the comic strip copyright holders, Lockheed changed the name of the advanced development company to "Skunk Works" in the 1960s. The name "Skunk Works" and the skunk design are now registered trademarks of the Lockheed Martin Corporation. The company also holds several registrations of it with the United States Patent and Trademark Office. They have filed several challenges against registrants of domain names containing variations on the term under anti-cybersquatting policies, and have lost a case under the .uk domain name dispute resolution service against a company selling cannabis seeds and paraphernalia, which used the word "skunkworks" in its domain name (referring to "Skunk", a variety of the cannabis plant). Lockheed Martin claimed the company registered the domain in order to disrupt its business and that consumer confusion might result. The respondent company argued that Lockheed "used its size, resources and financial position to employ 'bullyboy' tactics against . . . a very small company."
Australian company The Novita Group Pty Ltd owns the trademark "Skunkworks" in that country. After years of litigation the Australian government department IP Australia confirmed the trademark and awarded it to Novita against Lockheed Martin's objections.

</doc>
<doc id="55251" url="http://en.wikipedia.org/wiki?curid=55251" title="Titus">
Titus

Titus (Latin: "Titus Flāvius Caesar Vespasiānus Augustus"; 30 December 39 – 13 September 81) was Roman Emperor from 79 to 81. A member of the Flavian dynasty, Titus succeeded his father Vespasian upon his death, thus becoming the first Roman Emperor to come to the throne after his own biological father.
Prior to becoming Emperor, Titus gained renown as a military commander, serving under his father in Judaea during the First Jewish-Roman War. The campaign came to a brief halt with the death of emperor Nero in 68, launching Vespasian's bid for the imperial power during the Year of the Four Emperors. When Vespasian was declared Emperor on 1 July 69, Titus was left in charge of ending the Jewish rebellion. In 70, he besieged and captured Jerusalem, and destroyed the city and the Second Temple. For this achievement Titus was awarded a triumph: the Arch of Titus commemorates his victory to this day.
Under the rule of his father, Titus gained notoriety in Rome serving as prefect of the Praetorian Guard, and for carrying on a controversial relationship with the Jewish queen Berenice. Despite concerns over his character, Titus ruled to great acclaim following the death of Vespasian in 79, and was considered a good emperor by Suetonius and other contemporary historians.
As emperor, he is best known for completing the Colosseum and for his generosity in relieving the suffering caused by two disasters, the eruption of Mount Vesuvius in AD 79 and a fire in Rome in 80. After barely two years in office, Titus died of a fever on 13 September 81. He was deified by the Roman Senate and succeeded by his younger brother Domitian.
Early life.
Titus was born in Rome, probably on 30 December 39 AD, as the eldest son of Titus Flavius Vespasianus—commonly known as Vespasian—and Domitilla the Elder. He had one younger sister, Domitilla the Younger (born 45), and one younger brother, also named Titus Flavius Domitianus (born 51), but commonly referred to as Domitian.
Family background.
Decades of civil war during the 1st century BC had contributed greatly to the demise of the old aristocracy of Rome, which was gradually replaced in prominence by a new provincial nobility during the early part of the 1st century. One such family was the "gens Flavia", which rose from relative obscurity to prominence in just four generations, acquiring wealth and status under the Emperors of the Julio-Claudian dynasty. Titus's great-grandfather, Titus Flavius Petro, had served as a centurion under Pompey during Caesar's civil war. His military career ended in disgrace when he fled the battlefield at the Battle of Pharsalus in 48 BC.
Nevertheless, Petro managed to improve his status by marrying the extremely wealthy Tertulla, whose fortune guaranteed the upwards mobility of Petro's son Titus Flavius Sabinus I, Titus's grandfather. Sabinus himself amassed further wealth and possible equestrian status through his services as tax collector in Asia and banker in Helvetia. By marrying Vespasia Polla he allied himself to the more prestigious patrician "gens Vespasia", ensuring the elevation of his sons Titus Flavius Sabinus II and Vespasian to the senatorial rank.
The political career of Vespasian included the offices of quaestor, aedile and praetor, and culminated with a consulship in 51, the year Domitian was born. As a military commander, he gained early renown by participating in the Roman invasion of Britain in 43. What little is known of Titus's early life has been handed down to us by Suetonius, who records that he was brought up at the imperial court in the company of Britannicus, the son of emperor Claudius, who would be murdered by Nero in 55.
The story was even told that Titus was reclining next to Britannicus, the night he was murdered, and sipped of the poison that was handed to him. Further details on his education are scarce, but it seems he showed early promise in the military arts and was a skilled poet and orator both in Greek and Latin.
Adult life.
From c. 57 to 59 he was a military tribune in Germania. He also served in Britannia, perhaps arriving c. 60 with reinforcements needed after the revolt of Boudica. In c. 63 he returned to Rome and married Arrecina Tertulla, daughter of a former Prefect of the Praetorian Guard. She died c. 65.
Titus then took a new wife of a much more distinguished family, Marcia Furnilla. However, Marcia's family was closely linked to the opposition to Nero. Her uncle Barea Soranus and his daughter Servilia were among those who perished after the failed Pisonian conspiracy of 65. Some modern historians theorize that Titus divorced his wife because of her family's connection to the conspiracy.
Titus never remarried. Titus appears to have had multiple daughters, at least one of them by Marcia Furnilla. The only one known to have survived to adulthood was Julia Flavia, perhaps Titus's child by Arrecina, whose mother was also named Julia. During this period Titus also practiced law and attained the rank of quaestor.
Judaean campaigns.
In 66 the Jews of the Judaea Province revolted against the Roman Empire. Cestius Gallus, the legate of Syria, was defeated at the battle of Beth-Horon and forced to retreat from Jerusalem. The pro-Roman king Agrippa II and his sister Berenice fled the city to Galilee where they later gave themselves up to the Romans.
Nero appointed Vespasian to put down the rebellion, who was dispatched to the region at once with the Fifth Legion and Tenth Legion. He was later joined at Ptolemais by Titus with the Fifteenth Legion. With a strength of 60,000 professional soldiers, the Romans prepared to sweep across Galilee and march on Jerusalem.
The history of the war was covered in dramatic detail by the Roman-Jewish historian Josephus in his work "The Wars of the Jews". Josephus served as a commander in the city of Yodfat when the Roman army invaded Galilee in 67. After an exhausting siege which lasted 47 days, the city fell, with an estimated 40,000 killed and the remaining Jews committing suicide (except Josephus).
Surviving a group suicide, Josephus surrendered to Vespasian and became a prisoner. He later wrote that he provided the Romans with intelligence on the ongoing revolt. By 68, the entire coast and the north of Judaea were subjugated by the Roman army, with decisive victories won at Taricheae and Gamala, where Titus distinguished himself as a skilled general.
Year of the Four Emperors.
The last and most significant fortified city held by the Jewish resistance was Jerusalem. The campaign came to a sudden halt when news arrived of Nero's death. Almost simultaneously, the Roman Senate had declared Galba, then governor of Hispania, as Emperor of Rome. Vespasian decided to await further orders, and sent Titus to greet the new "princeps".
Before reaching Italy, Titus learnt that Galba had been murdered and replaced by Otho, governor of Lusitania, and that Vitellius and his armies in Germania were preparing to march on the capital, intent on overthrowing Otho. Not wanting to risk being taken hostage by one side or the other, he abandoned the journey to Rome and rejoined his father in Judaea. Meanwhile, Otho was defeated in the First Battle of Bedriacum and committed suicide.
When the news reached the armies in Judaea and Ægyptus, they took matters into their own hands and declared Vespasian emperor on 1 July 69. Vespasian accepted, and through negotiations by Titus, joined forces with Gaius Licinius Mucianus, governor of Syria. A strong force drawn from the Judaean and Syrian legions marched on Rome under the command of Mucianus, while Vespasian travelled to Alexandria, leaving Titus in charge to end the Jewish rebellion. By the end of 69, the forces of Vitellius had been beaten, and Vespasian was officially declared emperor by the Senate on 21 December, thus ending the Year of the Four Emperors.
Siege of Jerusalem.
Meanwhile the Jews had become embroiled in a civil war of their own, splitting the resistance in Jerusalem between several factions. The Sicarii led by Menahem ben Judah cannot hold on for long; the Zealots led by Eleazar ben Simon eventually fall under the command of the Galilean leader John of Gush Halav; and the other northern rebel commander, Simon Bar Giora, manages to gain leadership over the Idumeans too. Titus besieged Jerusalem. The Roman army was joined by the Twelfth Legion, which was previously defeated under Cestius Gallus, and from Alexandria Vespasian sent Tiberius Julius Alexander, governor of Ægyptus, to act as Titus' second in command.
Titus surrounded the city, with three legions (Vth, XIIth and XVth) on the western side and one (Xth) on the Mount of Olives to the east. He put pressure on the food and water supplies of the inhabitants by allowing pilgrims to enter the city to celebrate Passover, and then refusing them egress. Jewish raids continuously harassed the Roman army, one of which nearly resulted in Titus being captured.
After attempts by Josephus to negotiate a surrender had failed, the Romans resumed hostilities and quickly breached the first and second walls of the city. To intimidate the resistance, Titus ordered deserters from the Jewish side to be crucified around the city wall. By this time the Jews had been exhausted by famine, and when the weak third wall was breached, bitter street fighting ensued.
The Romans finally captured the Antonia Fortress and began a frontal assault on the gates of the Temple. According to Josephus, Titus had ordered that the Temple should not be destroyed, but while the fighting around the gates continued, a soldier hurled a torch inside one of the windows, which quickly set the entire building ablaze. The later Christian chronicler Sulpicius Severus, possibly drawing on a lost portion of Tacitus' "Histories", claims that Titus favoured the destruction of the Temple. The Temple was completely demolished, after which Titus' soldiers proclaimed him "imperator" in honor of the victory.
Jerusalem was sacked and much of the population killed or dispersed. Josephus claims that 1,100,000 people were killed during the siege, of which a majority were Jewish. 97,000 were captured and enslaved, including Simon Bar-Giora and John of Jish. Many fled to areas around the Mediterranean. Titus reportedly refused to accept a wreath of victory, as he claimed that he had not won the victory on his own, but had been the vehicle through which their God had manifested his wrath against his people.
The Jewish Diaspora at the time of the Temple’s destruction, according to Josephus, was in Parthia (Persia), Babylonia (Iraq), Arabia, as well as some Jews beyond the Euphrates and in Adiabene (Kurdistan).
Heir to Vespasian.
 Unable to sail to Italy during the winter, Titus celebrated elaborate games at Caesarea Maritima and Berytus, then travelled to Zeugma on the Euphrates, where he was presented with a crown by Vologases I of Parthia. While visiting Antioch he confirmed the traditional rights of the Jews in that city.
On his way to Alexandria, he stopped in Memphis to consecrate the sacred bull Apis. According to Suetonius, this caused consternation: the ceremony required Titus to wear a diadem, which the Romans associated with monarchy, and the partisanship of Titus's legions had already led to fears that he might rebel against his father. Titus returned quickly to Rome – hoping, says Suetonius, to allay any suspicions about his conduct.
Upon his arrival in Rome in 71, Titus was awarded a triumph. Accompanied by Vespasian and Domitian he rode into the city, enthusiastically saluted by the Roman populace and preceded by a lavish parade containing treasures and captives from the war. Josephus describes a procession with large amounts of gold and silver carried along the route, followed by elaborate re-enactments of the war, Jewish prisoners, and finally the treasures taken from the Temple of Jerusalem, including the Menorah and the Pentateuch. Simon Bar Giora was executed in the Forum, after which the procession closed with religious sacrifices at the Temple of Jupiter.
The triumphal Arch of Titus, which stands at one entrance to the Forum, memorializes the victory of Titus.
With Vespasian declared emperor, Titus and his brother Domitian received the title of "Caesar" from the Senate. In addition to sharing tribunician power with his father, Titus held seven consulships during Vespasian's reign and acted as his secretary, appearing in the Senate on his behalf. More crucially, he was appointed Praetorian prefect (commander of the Praetorian Guard), ensuring their loyalty to the Emperor and further solidifying Vespasian's position as a legitimate ruler.
In this capacity he achieved considerable notoriety in Rome for his violent actions, frequently ordering the execution of suspected traitors on the spot. When in 79, a plot by Aulus Caecina Alienus and Eprius Marcellus to overthrow Vespasian was uncovered, Titus invited Alienus to dinner and ordered him to be stabbed before he had even left the room.
During the Jewish wars, Titus had begun a love affair with Berenice, sister of Agrippa II. The Herodians had collaborated with the Romans during the rebellion, and Berenice herself had supported Vespasian in his campaign to become emperor. In 75, she returned to Titus and openly lived with him in the palace as his promised wife. The Romans were wary of the eastern queen and disapproved of their relationship. When the pair was publicly denounced by Cynics in the theatre, Titus caved in to the pressure and sent her away, but his reputation further suffered.
Emperor (79–81).
Succession.
Vespasian died of an infection on 23 June 79 AD, and was immediately succeeded by his son Titus. Because of his many (alleged) vices, many Romans feared that he would be another Nero. Against these expectations, however, Titus proved to be an effective Emperor and was well loved by the population, who praised him highly when they found that he possessed the greatest virtues instead of vices.
One of his first acts as Emperor was to order a halt to trials based on treason charges, which had long plagued the principate. The law of treason, or law of majestas, was originally intended to prosecute those who had corruptly "impaired the people and majesty of Rome" by any revolutionary action. Under Augustus, however, this custom had been revived and applied to cover slander and libel as well. This led to numerous trials and executions under Tiberius, Caligula, and Nero, and the formation of networks of informers ("Delators"), which terrorized Rome's political system for decades.
Titus put an end to this practice, against himself or anyone else, declaring:
It is impossible for me to be insulted or abused in any way. For I do naught that deserves censure, and I care not for what is reported falsely. As for the emperors who are dead and gone, they will avenge themselves in case anyone does them a wrong, if in very truth they are demigods and possess any power.
Consequently, no senators were put to death during his reign; he thus kept to his promise that he would assume the office of Pontifex Maximus "for the purpose of keeping his hands unstained". The informants were publicly punished and banished from the city. Titus further prevented abuses by making it unlawful for a person to be tried under different laws for the same offense. Finally, when Berenice returned to Rome, he sent her away.
As Emperor he became known for his generosity, and Suetonius states that upon realising he had brought no benefit to anyone during a whole day he remarked, "Friends, I have lost a day."
Challenges.
Although his administration was marked by a relative absence of major military or political conflicts, Titus faced a number of major disasters during his brief reign. On 24 August 79, two months after his accession, Mount Vesuvius erupted. The eruption almost completely destroyed the cities and resort communities around the Bay of Naples. The cities of Pompeii and Herculaneum were buried under metres of stone and lava, killing thousands of citizens. Titus appointed two ex-consuls to organise and coordinate the relief effort, while personally donating large amounts of money from the imperial treasury to aid the victims of the volcano. Additionally, he visited Pompeii once after the eruption and again the following year.
During the second visit, in spring of AD 80, a fire broke out in Rome, burning large parts of the city for three days and three nights. Although the extent of the damage was not as disastrous as during the Great Fire of 64—crucially sparing the many districts of insulae—Cassius Dio records a long list of important public buildings that were destroyed, including Agrippa's Pantheon, the Temple of Jupiter, the Diribitorium, parts of the Theatre of Pompey, and the Saepta Julia among others. Once again, Titus personally compensated for the damaged regions. According to Suetonius, a Plague also broke out during the fire. The nature of the disease, however, or the death toll are unknown.
Meanwhile war had resumed in Britannia, where Gnaeus Julius Agricola pushed further into Caledonia and managed to establish several forts there. As a result of his actions, Titus received the title of Imperator for the fifteenth time.
His reign also saw the rebellion led by Terentius Maximus, one of several "false Neros" who appeared throughout the 70s. Although Nero was primarily known as a universally hated tyrant, there is evidence that for much of his reign, he remained highly popular in the eastern provinces. Reports that Nero had in fact survived his overthrow were fueled by the confusing circumstances of his death and several prophecies foretelling his return.
According to Cassius Dio, Terentius Maximus resembled Nero in voice and appearance and, like him, sang to the lyre. Terentius established a following in Asia minor but was soon forced to flee beyond the Euphrates, taking refuge with the Parthians. In addition, sources state that Titus discovered that his brother Domitian was plotting against him but refused to have him killed or banished.
Public works.
Construction of the Flavian Amphitheatre, presently better known as the Colosseum, was begun in 70 under Vespasian and finally completed in 80 under Titus. In addition to providing spectacular entertainments to the Roman populace, the building was also conceived as a gigantic triumphal monument to commemorate the military achievements of the Flavians during the Jewish wars.
The inaugural games lasted for a hundred days and were said to be extremely elaborate, including gladiatorial combat, fights between wild animals (elephants and cranes), mock naval battles for which the theatre was flooded, horse races and chariot races. During the games, wooden balls were dropped into the audience, inscribed with various prizes (clothing, gold, or even slaves), which could then be traded for the designated item.
Adjacent to the amphitheatre, within the precinct of Nero's Golden House, Titus had also ordered the construction of a new public bath-house, the Baths of Titus. Construction of this building was hastily finished to coincide with the completion of the Flavian Amphitheatre.
Practice of the imperial cult was revived by Titus, though apparently it met with some difficulty as Vespasian was not deified until six months after his death. To further honor and glorify the Flavian dynasty, foundations were laid for what would later become the Temple of Vespasian and Titus, which was finished by Domitian.
Death (81).
At the closing of the games, Titus officially dedicated the amphitheatre and the baths, which was to be his final recorded act as Emperor. He set out for the Sabine territories but fell ill at the first posting station where he died of a fever, reportedly in the same farm-house as his father. Allegedly, the last words he uttered before passing away were: "I have made but one mistake".
Titus had ruled the Roman Empire for just over two years, from the death of his father in 79 to his own on 13 September 81. He was succeeded by Domitian, whose first act as emperor was to deify his brother.
Historians have speculated on the exact nature of his death, and to which mistake Titus alluded in his final words. Philostratus writes that he was poisoned by Domitian with a sea hare ("Aplysia depilans"), and that his death had been foretold to him by Apollonius of Tyana. Suetonius and Cassius Dio maintain he died of natural causes, but both accuse Domitian of having left the ailing Titus for dead. Consequently, Dio believes Titus's mistake refers to his failure to have his brother executed when he was found to be openly plotting against him.
The Babylonian Talmud (Gittin 56b) contains a spurious account attributing Titus's death to an insect that flew into his nose and picked at his brain for seven years.
Legacy.
Historiography.
Titus's record among ancient historians stands as one of the most exemplary of any emperor. All the surviving accounts from this period, many of them written by his own contemporaries, present a highly favorable view towards Titus. His character has especially prospered in comparison with that of his brother Domitian.
"The Wars of the Jews" offers a first-hand, eye-witness account of the Jewish rebellion and the character of Titus. The neutrality of Josephus' writings has come into question however, as he was heavily indebted to the Flavians. In 71, he arrived in Rome in the entourage of Titus, became a Roman citizen and took on the Roman nomen Flavius and praenomen Titus from his patrons. He received an annual pension and lived in the palace.
It was while in Rome, and under Flavian patronage, that Josephus wrote all of his known works. "The War of the Jews" is heavily slanted against the leaders of the revolt, portraying the rebellion as weak and unorganized, and even blaming the Jews for causing the war. The credibility of Josephus as a historian has subsequently come under fire.
Another contemporary of Titus was Publius Cornelius Tacitus, who started his public career in 80 or 81 and credits the Flavian dynasty with his elevation. "The Histories"—his account of this period—was published during the reign of Trajan. Unfortunately only the first five books from this work have survived until the present day, with the text on Titus's and Domitian's reign entirely lost.
Suetonius Tranquilius gives a short but highly favourable account on Titus's reign in "The Lives of Twelve Caesars", emphasizing his military achievements and his generosity as Emperor, in short describing him as follows:
Titus, of the same surname as his father, was the delight and darling of the human race; such surpassing ability had he, by nature, art, or good fortune, to win the affections of all men, and that, too, which is no easy task, while he was emperor.
Finally, Cassius Dio wrote his "Roman History" over a hundred years after the death of Titus. He shares a similar outlook as Suetonius, possibly even using the latter as a source, but is more reserved, noting:
His satisfactory record may also have been due to the fact that he survived his accession but a very short time, for he was thus given no opportunity for wrongdoing. For he lived after this only two years, two months and twenty days—in addition to the thirty-nine years, five months and twenty-five days he had already lived at that time. In this respect, indeed, he is regarded as having equalled the long reign of Augustus, since it is maintained that Augustus would never have been loved had he lived a shorter time, nor Titus had he lived longer. For Augustus, though at the outset he showed himself rather harsh because of the wars and the factional strife, was later able, in the course of time, to achieve a brilliant reputation for his kindly deeds; Titus, on the other hand, ruled with mildness and died at the height of his glory, whereas, if he had lived a long time, it might have been shown that he owes his present fame more to good fortune than to merit.
Pliny the Elder, who later died during the eruption of Mount Vesuvius, dedicated his "Naturalis Historia" to Titus.
In contrast to the ideal portrayal of Titus in Roman histories, in Jewish memory "Titus the Wicked" is remembered as an evil oppressor and destroyer of the Temple. For example, one legend in the Babylonian Talmud describes Titus as having had sex with a whore on a Torah scroll inside the Temple during its destruction.
Titus in later arts.
The war in Judaea and the life of Titus, particularly his relationship with Berenice, have inspired writers and artists through the centuries. The bas-relief in the Arch of Titus has been influential in the depiction of the destruction of Jerusalem, with the Menorah frequently being used to symbolise the looting of the Second Temple.

</doc>
<doc id="55264" url="http://en.wikipedia.org/wiki?curid=55264" title="North Brabant">
North Brabant

North Brabant (Dutch: "Noord-Brabant" ]), also unofficially called Brabant since 2001, is a province of the Netherlands, located in the south of the country, bordered by Belgium's Antwerp and Limburg provinces in the south, the Meuse River ("Maas") in the north, Limburg in the east and Zeeland in the west.
History.
The Duchy of Brabant was a State of the Holy Roman Empire established in 1183. It developed from the Landgraviate of Brabant and formed the heart of the historic Low Countries, part of the Burgundian Netherlands from 1430 and of the Habsburg Netherlands from 1482, until it was dismembered after the Dutch revolt.
Present-day North Brabant (Staats-Brabant) was adjudicated to the Generality Lands of the Dutch Republic according to the 1648 Peace of Westphalia, while the reduced duchy remained in existence with the Southern Netherlands until it was conquered by French Revolutionary forces in 1794.
Until the 17th century, the area that now makes up the province of North Brabant was mostly part of the Duchy of Brabant, of which the southern part is now in present-day Belgium. In the 14th and 15th century, the area experienced a golden age, especially the cities of Brussel (Brussels), Mechelen, Leuven (Louvain), Antwerp (all of these are now in Belgium), Breda, Bergen op Zoom and 's-Hertogenbosch.
After the Union of Utrecht was signed in 1579, Brabant became a battlefield between the Protestant Dutch Republic and Catholic Spain, which occupied the southern Netherlands. As a result of the Peace of Westphalia, the northern part of Brabant became part of the Netherlands as the territory of Staats-Brabant (State Brabant) under federal rule, in contrast to the founding provinces of the Dutch Republic which were self-governing.
Attempts to introduce Protestantism into the region were largely unsuccessful; North Brabant remained strongly Roman Catholic. For over a century, North Brabant served mainly as a military buffer zone. In 1796, when confederate Dutch Republic became the unitary Batavian Republic, Staats-Brabant became a province as "Bataafs Brabant". This status ended with the reorganisation by the French, and the area was divided over several departments.
In 1815, Belgium and the Netherlands were united in the United Kingdom of the Netherlands, and the province of North Brabant was established and so named to distinguish it from South Brabant in present-day Belgium, which seceded from the Kingdom in 1830. This boundary between the Netherlands and Belgium is special in that it does not form a contiguous line, but leaves a handful of tiny enclaves (and enclaves inside enclaves) on both sides of the border. A few of these irregularities were corrected (Luyksgestel was exchanged for Lommel), Huijbergen became totally Dutch, but some remain, notably Baarle-Hertog (Belgian) and Baarle-Nassau (Dutch).
When the present province was instituted, its territory was expanded with a part of the province of Holland and the former territory of Ravenstein which had previously belonged to the Duchy of Cleves, as well as several small, formerly autonomous entities.
The period from 1900 till the late 1960s is called "Het Rijke Roomse Leven" (translated as 'the rich Roman life', with 'Roman' meaning 'Roman Catholic'), a era of strong religious believes. "Het Rijke Roomse Leven" came about as result of the emancipatory drive of the province's disadvantaged Catholic population and was supported by a Roman Catholic pillar, which was directed by the clergy, and not only encompassed churches, but also Roman Catholic schools and hospitals, which were run by nuns and friars. In those days every village in North Brabant had a convent from which the nuns operated. Politically, the province was dominated by Catholic parties: the Roomsch-Katholieke Staatspartij and its post-war successor, the Katholieke Volkspartij, which often held around 75% of the vote.
In the 1960s secularisation and the actual emancipation of the Catholic population brought about the gradual dissolution of the Catholic pillar, as church attendance decreased in North Brabant as elsewhere in Western Europe. The influence of "Het Rijke Roomse Leven" (The Rich Roman (Catholic) Life) remains in the form of education where some schools are still Roman Catholic, (today run by professional teachers and not by nuns) and in North Brabant's culture, politics, mentality and customs, such as carnival. Though the interpretation of the Roman Catholic identity in North Brabant has shifted the last 65 years from religious to cultural, the province still has a distinct Catholic atmosphere when compared to the provinces north of the major rivers.
Municipalities.
North Brabant is currently divided into 66 municipalities. Traditionally, almost every town was a separate municipality, but their number was reduced greatly in the 1990s by incorporating smaller towns into neighbouring cities or by other mergers. The municipalities in North Brabant are:
On 1 January 2015 the municipality of Maasdonk was merged into the existing municipalities of 's-Hertogenbosch and Oss.
Geography.
With a population density of 501/km², North-Brabant is above average urbanized. The urbanization is at the center of the province at largest, where the 'kite' (the Brabantse Stedenrij Breda, Tilburg, Eindhoven and 's-Hertogenbosch) is located, the rest of the province has a more rural character. The province has preserved some of its scenic nature well. Natural beauty is found mainly in national parks Loonse and Drunen Dunes, De Biesbosch and De De Groote Peel, on the marshes of the Meierij at Oisterwijk and Boxtel (within an area called Het Groene Woud), the border park Zoom-Kalmthoutse Heide, and in the forested area around Breda. Also, south of Eindhoven named De Kempen is a beautiful area with farmlands and forests. In Heeze, also south of Eindhoven, are the heath areas the "Groote Heide" (333 ha) and Strabrechtse Heath (1500 ha) located. The "Strabrechtse Heide" holds also the largest fen of the Netherlands. The "Beuven" (Beu fen) is measuring 85 hectares.
Like most of the Netherlands, North Brabant is mostly flat but nearly every part of North Brabant is above sea level, therefore there are not as many canals as in the lower parts of The Netherlands. While most of the population lives in urban areas, the province is scattered with villages around which most of the land is cultivated.
National Parks in North Brabant are:
De Biesbosch.
The Biesbosch (from bies, "rushes", and bosch, "woodland") is an area southwest of Dordrecht formed when the dike on the Meuse burst and the St. Elizabeth's floods on November 19, 1421 engulfed great tracts of land in the southwestern Netherlands and altered the geography of the whole area, inundating over 40,000 hectares/100,000 acres of land. Since the 18th century more than four-fifths of the flooded land has been reclaimed. An area of 6,000 hectares/15,000 acres was left as it was, and now forms the Biesbosch nature reserve and bird sanctuary.
Until the end of the 1960s the Biesbosch was directly connected with the sea and subject to changing tide levels. As a result it developed a flora which tolerated brackish water and was the home of numerous waterfowl. Since the damming of the Haringvliet there is no variation in water level, and both flora and fauna have adapted to the new environment.
The Biesbosch is criss-crossed by a network of footpaths and bikepaths and by countless rivers and streams which offer excellent facilities for water sports (sailing, surfing). In spite of the large numbers of visitors the natural environment has remained largely unspoiled.
The Biesbosch nature reserve can be reached by car only from the east (preferably via Werkendam). The southwest part of the area, with its three large reservoirs of drinking water, is closed to road traffic. The Biesbosch can also be reached by boat from Drimmelen, Geertruidenberg or Lage Zwaluwe.
De Kempen.
The historical region of Kempen occupies the southern part of the province of Noord-Brabant and extends south of Eindhoven far into northern Belgium. To the east it reaches as far as the Meuse valley. The surface topography of De Kempen is very uniform. Most of it lies between 5m/15 ft and 35m/115 ft above sea level. The basement rocks are Cretaceous and Tertiary sediments, which are overlaid by Ice Age gravels and sands carried here by rivers of melt-water from the retreating glaciers. It is a typical area of sandy heathland.
The infertile soil is suitable only for undemanding crops such as rye, oats, potatoes and fodder plants and thus limits the profitability of agriculture. Until a few decades ago Kempen was a region of heathland and sand drifts with a sparse growth of pines, a few scattered villages subsisting on the poor soil and some small towns; and this is still the pattern in much of the region.
In recent years, however, the rapid advance of industry has brought about profound changes in this agricultural region. The origins of this industrial development go back 70 or 100 years. The main concentrations of industry are along the southern frontier of the Netherlands, e.g. at Eindhoven, Helmond, Tilburg, Breda, 's-Hertogenbosch.
De Peel.
In the east of Noord-Brabant, near the Limburgish border, is the Peel area, an expanse of moorland extending from Eindhoven to Venlo, on the border with Limburg. Southeast of Asten is a nature reserve (1,300 hectares/3,250 acres; visitor center at Ospeldijk) which has escaped destruction by peat cutting. Mostly boggy, it will appeal to nature lovers with its interesting flora and fauna. Apart from this small area almost the whole of the Peel has been brought into cultivation.
Rivers and deltas.
The province is bordered by the Meuse River in the north. Its delta flows through the Biesbosch area, a national park.
Economy.
Employment is found in the agricultural, industrial and service sectors, with agricultural and food processing companies such as Agrifirm, Bavaria, FrieslandCampina, Mars Incorporated, Nutreco, Royal Canin all having large production sites or their headquarters located in the province. The main agricultural products are corn, wheat and sugar beet, while cows and pigs are held as livestock. 
Of economic importance is BrabantStad, a partnership between the municipalities of Breda, Eindhoven, Helmond, 's-Hertogenbosch and Tilburg and the province of North Brabant. The region has overlap with the Brabantse Stedenrij, Brainport and the Metropolitan region of Eindhoven and lies within the Eindhoven-Leuven-Aachen Triangle (ELAT). The partnership aims to form an urban network and to make North Brabant explicitly known as a leading knowledge region within Europe. There is cooperation on economic, spatial, social and cultural areas. With a total of 1.5 million people and 20% of the industrial production in the Netherlands is BrabantStad one of the major economical important, metropolitan regions of the Netherlands.
The province of North Brabant is one of the most innovative regions of the European Union. This is shown by the extensive amount of new research patents by Eurostat. In BrabantStad there are every year 2100 patent applications made at the European Patent Office (EPO), which is 900 per million active employees.
Mainly due to the Dutch electronics giant Philips' scientific centers, BrabantStad has grown more important than similar centres like Paris, Stockholm and Stuttgart.
Of all the money that goes to research and development in the Netherlands, one third is spent in Eindhoven. The slogan of the city of Eindhoven, "Leading in technology", is based on this. A quarter of the jobs in the region are in technology and ICT. The largest expenses and most patent applications come from Eindhoven, mainly Philips. Of all European patent applications in the field of physics and electronics about eight per cent is from North Brabant.
Also ASML, DAF, VDL, Vanderlande, Atos Origin, Bosch Rexroth, Ciber, NXP Semiconductors, FEI Company, Thales Cryognetics and TNO Industrial Technology are located in the Brabant metropolitan area. The Eindhoven University of Technology hosts an incubator for technology startups (called the Twinning Center) and the NatLab has developed into the High Tech Campus Eindhoven.
This cooperative tradition has also developed into a different direction than the traditional technology research done at the university. Starting in 2002, Eindhoven University of Technology, the Catharina Hospital in Eindhoven, Philips Medical and the University of Maastricht joined forces and started joint research into biomedical science, technology and engineering. Within Eindhoven, this research has been concentrated in a new university faculty (BioMedical Technology or BMT). This development has also made Eindhoven a biomedical technology hub within the country and its (European) region.
In the extended region, BrabantStad is part of the Eindhoven-Leuven-Aachen Triangle (ELAT). This economic cooperation agreement between three cities in three countries has created one of the most innovative regions in the European Union (measured in terms of money invested in technology and knowledge economy); the agreement is based on the cooperative triangle that connects the three technical universities in those cities.
In and around Eindhoven in the Dutch part of the Eindhoven-Leuven-Aachen Triangle (ELAT) is Brainport located, which is a partnership between businesses, universities and the government, and is formed by the municipalities of South-Eastern North Brabant. The economic success of Brainport is important for the international competitiveness of the Netherlands; Together with Amsterdam (airport) and Rotterdam (seaport), Brainport forms the foundation of the Dutch economy.
BrabantStad is the fastest growing economic region in the Netherlands, with Brainport as one of the three national top regions and as a top region in the world.
Brainport includes South-east Nort-Brabant and is the hub of a network that stretches across the South-east of the Netherlands and its borders. The core of Brainport is the Eindhoven region, with about 740,000 inhabitants and 400,000 jobs.
Other important industries are automobile production (e.g. General Motors in Breda and Tesla in Tilburg), electronics, textile and shoes.
In the twentieth century, tourism has become an important sector for North Brabant, the woods and its quiet atmosphere combined with the beauty of some of the cities having proved successful. Another big tourist attraction is theme park Efteling in Kaatsheuvel, the largest of the Benelux.
Language.
Brabantian is not a minority language in the Netherlands. It can be divided in two main dialects: East Brabantian and West Brabantian. Along with the Hollandic dialects it is one of the two most spoken versions of Dutch. Brabantian has compared to other main Dutch dialects had a big influence on the development of Standard Dutch. This was because of Brabant was being the dominant region in the Netherlands when the standardization of Dutch started in the 16th century. The first major formation of standard Dutch also took place in Antwerp, where a Brabantian dialect is spoken. The default language being developed around this time had therefore mainly Brabantian influences. The early modern Dutch written language was initially influenced primarily by Brabantian, with strong influence from Hollandic emerging after the 16th century. Since the Brabantian dialect has developed faster than the sixteenth and seventeenth century Dutch, it has become more diverse from modern day, Standard Dutch, but is still quite similar and very understandable.
About one third of the Dutch-speaking population lives in the Brabantian dialect zone. Both in large Brabantian towns such as Breda and Eindhoven and in rural areas many people still speak the original dialect or colloquial Dutch with a typical "southern" tongue. Tilburg and 's-Hertogenbosch have a large number of people speaking the Brabantian dialect.
Culture.
There are many museums, especially in the larger towns which include the Van Abbemuseum in Eindhoven, the North Brabant Museum in 's-Hertogenbosch, the Museum of the Image in Breda, Noordbrabants Natuurmuseum in Tilburg. Also a few towns have a large theater like the Chassé Theater in Breda and the Eindhoven Park Theatre. Large, reputable music venues like the 013 in Tilburg, which boasts the largest space of music venues in the Netherlands, and the Effenaar in Eindhoven offer concerts by major artists. Smaller venues like Mezz Breda, W2 Concert in 's-Hertogenbosch and the smaller halls of the 013 and the Effenaar offer concerts by emerging artists and bigger names in an intimate setting.
Events.
Some yearly cultural events in North Brabant are:
Museums.
List of museums in North Brabant
Museums of the main cities:
Eindhoven:
Breda:
Tilburg:
's-Hertogenbosch
Cuisine.
The Southern Dutch cuisine constitutes the cuisine of the Dutch provinces of North-Brabant and Limburg and the Flemish Region in Belgium. It is renowned for its many rich pastries, soups, stews and vegetable dishes and is often called "Burgundian" which is a Dutch idiom invoking the rich Burgundian court which ruled the Low Countries in the Middle Ages renowned for its splendor and great feasts.
It is the only Dutch culinary region which developed an haute cuisine, as it is influenced by both German cuisine and French cuisine, and it forms the base of most traditional Dutch restaurants including typical main courses served such as "Biefstuk", "Varkenshaas", "Ossenhaas", these are premium cuts of meat, generally pork or beef, accompanied by a wide variety of sauces and potatoes which have been double fried in the traditional Dutch (or Belgian) manner.
Stews, such as "hachee", a stew of onions, beef and a thick gravy, contain a lot of flavour and require hours to prepare. Vegetable soups are made from richly flavored stock or bouillon and typically contain small meatballs alongside a wide variety of different vegetables. Asparagus and "witlo(o)f" are highly prized and traditionally eaten with cheese and/or ham.
Pastries are abundant, often with rich fillings of cream, custard or fruits. Cakes, such as the "Moorkop" and "Bossche Bol" from Brabant, are typical pastries. Savoury pastries also occur, with the "worstenbroodje" (a roll with a sausage of ground beef) being the most popular.
The traditional alcoholic beverage of the region is beer. There are many local brands, ranging from "Trappist" to "Kriek". Beer, like wine in French cuisine, is also used in cooking; often in stews.
Sports.
Association football.
North Brabant is home to 8 professional football clubs, more than any other province in the Netherlands. Three clubs (PSV, NAC, Willem II) play in the Eredivisie, the highest professional football league in the Netherlands. Five clubs (Helmond Sport, FC Den Bosch, Eindhoven, Oss and RKC) play in the Eerste Divisie, the second-highest division of professional football in the Netherlands. PSV Eindhoven is the biggest club of North Brabant, and most successful with 21 Eredivisie titles, 1 European Cup and 1 UEFA Cup, among many other domestic cups, and is one of the traditional "big three" clubs in the Netherlands.
Politics.
The States of North Brabant have 55 seats, and are headed by the King's Commissioner, currently Wim van de Donk. While the provincial council is elected by the inhabitants, the Commissioner is appointed by the King and the cabinet of the Netherlands. With 12 seats, People's Party for Freedom and Democracy is the largest party in the council, closely followed by Christian Democratic Appeal with 10 seats.
The daily affairs of the province are taken care of by the "Gedeputeerde Staten", which are also headed by the Commissioner; its members ("gedeputeerden") can be compared with ministers.
States-Provincial.
Results in North Brabant in the elections for the States-Provincial in March 2007 (left) and March 2011 (right):
Gedeputeerde Staten.
Wim van de Donk (CDA) is since October 1, 2009 the King's Commissioner.
The Gedeputeerde Staten for 2011-2015 is based on a coalition of VVD, CDA and SP, and consists of the following five commissioners: 
Province Secretary: Wil Rutten.
Religion.
Traditionally the province of North Brabant was strongly Roman Catholic.
During the 1960s the relatively strong demarcation between the Catholic south on one side and the Calvinist west and north on the other side of the Netherlands started to diminish. In the second half of the twentieth century a rapid secularization took place in North Brabant.
In 2006 slightly more than half of the Brabantian people identified with Catholism. For example, in the Diocese of 's-Hertogenbosch, the eastern part of North Brabant and part of the province of Gelderland, 1,167,000 people feld associated with the Roman Catholic belief system (56.8 percent of the population). Only 45,645 residents of this area attend the mass, which is only 2 percent of the total population of the area and consists mostly of people over 65 years old. In western North Brabant (Diocese of Breda) is the number of people associating themselves with Catholism also strongly decreased, only 52 percent of the West Brabantians identify as Roman Catholic. Church attendance is even lower in the west with only 1 percent of the West Brabantian population visiting churches. North Brabant is mostly Roman Catholic by tradition. Its people still use the term and certain traditions as a base for their cultural identity rather than as a religious identity, and the vast majority of the Catholic population is now largely irreligious in practice. Research among Catholics in the Netherlands in 2007 shows that only 27% of the Dutch Catholics can be regarded as a theist, 55% as an ietsist / non-theist and 17% as agnostic.
The percentage of Muslims in North Brabant is estimated at 4,5%, who mainly live in the large and medium municipalities.

</doc>
<doc id="55265" url="http://en.wikipedia.org/wiki?curid=55265" title="Road train">
Road train

A road train, roadtrain or land train is a method of trucking used in remote areas of Argentina, Australia, Mexico, the United States, and Canada to move freight efficiently. The term "road train" is most often used in Australia. In the United States the terms "triples", "turnpike doubles", and "Rocky Mountain doubles" are commonly used for longer combination vehicles (LCVs). A road train consists of a relatively conventional tractor unit, but instead of towing one trailer or semi-trailer, a road train pulls two or more of them.
History.
Early road trains consisted of traction engines pulling multiple wagons. The first identified road trains operated into South Australia's Flinders Ranges from the Port Augusta area in the mid nineteenth century, according to Basil Fuller in his book, "The Ghan". They displaced bullock teams for the carriage of minerals to port and were, in turn, superseded by railways.
During the Crimean War a traction engine was used to pull multiple open trucks. By 1898 steam traction engine trains with up to four wagons were employed in military manoeuvres in England.
In 1900, John Fowler & Co. provided armoured road trains for use by the British forces in the Second Boer War. Lord Kitchener stated that he had around 45 steam road trains at his disposal.
There is an earlier road train built by its inventor in the United Kingdom. It is shown in the No. 320 (No. 8. Vol. 12, February 23, 1907) edition of "The Auto" Title: The Renard Road Train, page 242.
In the 1930/40s, the government of South Australia operated an AEC 8x8 military truck to transport freight and supplies into the Northern Territory, replacing the Afghan camel trains that had been trekking through the deserts since the late 19th century. This truck pulled two or three 6 m Dyson four-axle self-tracking trailers. With 130 hp, the AEC was grossly underpowered by today's standards, and drivers and offsiders routinely froze in winter and sweltered in summer due to the truck's open cab design and the position of the engine radiator, with its 1.5 m cooling fan, behind the seats.
Australian Kurt Johansson is recognised as the inventor of the modern road train. After transporting stud bulls 200 mi to an outback property, Johansson was challenged to build a truck to carry 100 head of cattle instead of the original load of 20. Provided with financing of a couple of thousand pounds and inspired by the tracking abilities of the Government roadtrain, Johansson began construction. Two years later his first road train was running.
Johansson's first road train consisted of a U.S. Army World War II surplus Diamond-T tank carrier, nicknamed "Bertha", and two home-built self-tracking trailers. Both wheel sets on each trailer could steer, and therefore could negotiate the tight and narrow tracks and creek crossings that existed throughout Central Australia in the earlier part of last century. Freighter Trailers in Australia viewed this improved invention and went on to build self-tracking trailers for Kurt and other customers, and went on to become innovators in transport machinery for Australia.
This first example of the modern road train, along with the AEC Government Roadtrain, forms part of the huge collection at the National Road Transport Hall of Fame in Alice Springs, Northern Territory.
Usage.
Australia.
Australia has the largest and heaviest road-legal vehicles in the world, with some configurations topping out at close to 200 t. The majority are between 80 and.
Double (two-trailer) road train combinations are allowed in most areas of Australia, and within the environs (albeit limited) of Adelaide, South Australia and Perth, Western Australia. A double road train should not be confused with a B-double, which is allowed access to most of the country and in all major cities.
Triple (three trailer) road trains operate in western New South Wales, western Queensland, South Australia, Western Australia and the Northern Territory, with the last three states also allowing AB-Quads (B double with two additional trailers coupled behind). Darwin is the only capital city in the world where triples and quads are allowed to within 1 km of the central business district (CBD). Tasmania and Victoria do not allow the operation of roadtrains on any of their roads. Victoria had previously allowed double road trains to operate around Mildura for the vintage grape harvest.
Strict regulations regarding licensing, registration, weights, and experience apply to all operators of road trains throughout Australia.
Road trains are used for transporting all manner of materials; common examples are livestock, fuel, mineral ores, and general freight. Their cost-effective transport has played a significant part in the economic development of remote areas; some communities are totally reliant on regular service.
The multiple dog-trailers are unhooked, the dollys removed and then connected individually to multiple trucks at "assembly" yards when the road train gets close to populated areas.
When the flat-top trailers of a road train need to be transported empty, it is common practice to stack them. This is commonly referred to as "doubled-up" or "doubling-up". "See illustration." Sometimes, if many trailers are required to be moved at the one time, they will be triple-stacked, or "tripled-up."
Higher Mass Limits (HML) Schemes are now piloting in all jurisdiction in Australia, allowing trucks to carry additional weight.
Canada.
In Canada, road trains are more commonly referred to as Long Combination Vehicles (LCVs), as Extended Length Vehicles (ELVs), or Energy Efficient Motor Vehicles (EEMVs).
Four types of LCV are permitted; turnpike doubles, triples, rocky mountain doubles, and queen city triples.
Turnpike doubles consist of a tractor unit pulling a semi-trailer (up to 53 ft long). An A-type or C-type converter is connected to the rear of the trailer, and carries a second trailer. Alternatively, the lead trailer may have a hideaway fifth wheel, which enables direct coupling of the second trailer without a converter. The total permissible length is 38 m.
Triples may be up to 35 m in length when using A or C converters, or 38 m in B-train configuration.
Rocky mountain doubles are limited to 31 m in overall length, but have the advantage of being legal on two-lane, undivided roads. A, B, and C-train variants are used. Other LCVs may only be used on divided highways.
Queen city triples consist of a tractor unit pulling one semi-trailer up to 53 ft long and two shorter "pup" trailers up to 32 ft long. Queen city triples are only permitted between the cities of Saskatoon and Regina Saskatchewan. These are the longest combinations allowed in North America on public highways. 
Western Canada.
British Columbia restricts LCV operation to the Coquihalla Highway (formerly a toll road) between Hope and Merritt, with the exception of B-doubles (mainly used for hauling wood chips).
Alberta allows LCV operation on most major highways. The Queen Elizabeth II Highway between Calgary and Edmonton carries the majority of turnpike doubles and triples. Theoretically, these vehicles could be used on Highway 16 as far west as Hinton and Highway 43 as far north as Valleyview, but are rarely used on these routes. LCV operation north and west of Edmonton is limited to rocky mountain doubles, since the road is undivided north of Valleyview. The destination of most rocky mountain doubles is Yellowknife in the Northwest Territories.
The Northwest Territories allows LCVs of up to 31 m in length. These vehicles are restricted to specific destinations in Hay River and Yellowknife. LCVs do not operate north of Yellowknife.
Saskatchewan mostly restricts LCVs to divided highways. The short, undivided section of Highway 11 around the village of Chamberlain is exempt from this restriction.
Licensing of LCV drivers.
(Information restricted to Alberta and NWT.) Prospective LCV drivers must have held a licence for legal length articulated vehicles for two years (five years in the North West Territories.). They may have no more than two moving offences recorded within three years and may not have any vehicle-related criminal code violations. They are required to pass a PDIC (professional driver improvement course) every four years. They are required to pull an LCV at least once a year in order to keep their LCV licence.
United States.
In the United States, trucks on public roads are limited to two trailers (two 28 ft (8.5 m) and a dolly to connect. The limit is 63 ft (19.2 m) end to end). Some states allow three trailers, although triples are usually restricted to less populous states such as Idaho, Oregon, and Montana, plus the Ohio Turnpike and Indiana East-West Toll Road. Triples are used for long-distance less-than-truckload freight hauling (in which case the trailers are shorter than a typical single-unit trailer) or resource hauling in the interior west (such as ore or aggregate). Triples are sometimes marked with "LONG LOAD" banners both front and rear. "Turnpike doubles"—tractors towing two full-length trailers—are allowed on the New York Thruway and Massachusetts Turnpike (Interstate 90), Florida's Turnpike, Kansas Turnpike (Kansas City - Wichita route) as well as the Ohio and Indiana toll roads. The term "road train" is not commonly used in the United States; "turnpike train" has been used, generally in a pejorative sense.
Europe.
In Finland, Sweden, Germany, the Netherlands, Denmark, and select roads in Norway (for a period of three years commencing November 24, 2008), trucks with trailers are allowed to be 25.25 m long. Elsewhere in the European Union, the limit is 18.75 m (Norway (19.5 m or 64 ft). The trucks are of a cab-over-engine design, that is with a flat front, a high floor about 1.2 m above ground with the engine below. The Scandinavian countries are less densely populated than the rest of the EU countries and distances, especially in Finland and Sweden, are vast. Until the late 1960s, vehicle length was unlimited, giving rise to long vehicles to handle goods cost effectively. As traffic increased, lengths became more of a concern and they were limited, albeit at a more generous level than in the rest of Europe. In the United Kingdom in 2009, a two-year desk study of Longer Heavier Vehicles (LHVs) including options up to 11-axle, 34 m long, 82 t combinations, ruled out all road train type vehicles for the foreseeable future.
Sweden is currently (2010) performing tests on log hauling trucks, weighing up to 90 t and measuring 30 m and haulers for two 40 ft containers, measuring 32 m in total.
Mexico.
Mexico allows LCVs consisting of double trailers around 12.5 m in length. They must display a sign reading "Precaucion: Doble Semi-remolque" (Caution: Double Semi Trailer) on the rear, indicating that a double-length trailer is in use. Mexico does not place special road restrictions on LCVs other than those already in place for conventional tractor-trailers.
Recently, regulations for road trains have been stiffened, limiting cargo to 60 tons among the two trailers. Truck drivers have felt strain due to this change because previously some commercial drivers received company incentives to accept overloading of their double trailers.
Trailer arrangements.
B-double.
A B-double (B-Train) consists of a prime mover towing a specialised lead trailer that has a fifth-wheel mounted on the rear towing another semi-trailer, resulting in two articulation points. Around container ports in Australia there may also have what is known as a super B-double, these being a B-double that has a quad axle lead trailer capable of holding one 40-foot shipping container or two 20-foot shipping containers, and the rear trailer being capable of the same with either a tri or quad rear axle set. However, because of their large length and low accessibility into narrow streets, these vehicles are restricted in where they can go and are generally used for terminal-to-terminal work, i.e., wharf to container holding park or wharf-to-wharf. The rear axle on each trailer can also pivot slightly while turning to prevent scrubbing out the edges of the tyres due to the heavy loads placed on them.
B-triple.
Same as a B-double but with an additional lead trailer behind the prime mover. These are run in most states of Australia where double road trains are allowed. There is one exception to that rule: B-Triples are operated in Victoria, but by one operator, under a strict permit and on a dedicated route, between the Ford plants at Geelong and Campbellfield. Australia's National Transport Commission proposed a national framework for B-triple operations that includes basic vehicle specifications and operating conditions that the commission anticipates will replace the current state-by-state approach, which largely discourages the use of B-triples for interstate operation.
AB Triple.
An AB triple consists of a prime mover, a semi-trailer, a converter dolly, then a B-double.
BAB Quad.
A BAB Quad consists of two B-double units linked with a converter dolly.
C-train.
A C-train is a semi-trailer attached to a fifth-wheel on a C-dolly. The C-dolly is connected to the tractor or another trailer in front of it with two drawbars, thus eliminating the drawbar connection as an articulation point. One of the axles on a C-dolly is self-steerable to prevent tire scrubbing. C-dollies are not permitted in Australia, due to the lack of articulation.
Dog-trailer (dog trailer).
A dog-trailer (also called a pup) is any trailer that is hooked to a converter dolly, with a single A-frame drawbar that fits into the Ringfeder or pintle hook on the rear of the trailer in front, giving the whole unit three to five articulation points and very little roll stiffness.
Rules and regulations.
A is a B-double.
B is a B-triple.
C is a double road train. A "Pocket road train" is similar, but with shorter trailers and dolly drawbar.
D is an AB-triple.
E is a BAB Quad.
F is an ABB Quad.
G is triple road train.
H is a 2AB Quad.
K represents the largest road trains operating in Australia and the world. Called a "Powertrain" or a "Body and six", these machines operate at the Granites gold mine in the western Northern Territory, and are used in place of 200t dump trucks, because of the distances involved on the haul run. A 600 hp 19 L Cummins engine powers the prime mover, while a 400 hp Cummins engine is installed in the rear trailer of the B-double, driving through an automatic transmission, giving a total of 1000 hp. Weights of 460 t are achieved with ore loading in side-tipper bodies on a 100 km round trip. As these trucks operate on private property, they are not subject to governed weight and length rulings, but instead are used in the most efficient way possible.
Interstate Road Transport registration.
In 1991 at a Special Premiers Conference, Australian Heads of Government signed an Inter-governmental Agreement to establish a national heavy vehicle registration, regulation and charging scheme, otherwise known as FIRS.
This registration scheme is known as the Federal Interstate Registration Scheme. The requirements of the scheme were as follows:
If the vehicle was purchased to be used for interstate trade, no stamp duty was payable on the purchase price of the vehicle.
The vehicle had to be subjected to an annual inspection for roadworthy standards, which had to be passed before registration could be renewed.
With the registration identification, the first letter of the 6 digit identified the home state: W = Western Australia, S = South Australia, V = Victoria, N = New South Wales, Q = Queensland, T = Tasmania, A = Australian Capital Territory and C = Northern Territory.
Due to the 'eastern' and 'western' mass limits in Australia, two different categories of registration were enacted. The second digit of the registration plate showed what mass limit was allowed for that vehicle. If a vehicle had a 'V' as the second letter, its mass limits were in line with the eastern states mass limits, which were:
If a vehicle had an "X" as the second letter, its mass limits were in line with the western states mass limits, which were:
If the second digit of the registration was a "T", that designated a trailer.
One of the main criteria of the registration was that intrastate operation was not permitted. The load had to come from one state and be delivered to another state or territory. Many grain carriers were reported and prosecuted for cartage from the paddock to the silos. If, though, they went to a port silo, they were given the benefit of the doubt, as that grain was more than likely going overseas.
Signage.
Australian road trains have horizontal signs front and back with 180 mm high black uppercase letters on a reflective yellow background reading "ROAD TRAIN". The sign(s) must have a black border and be at least 1.02 m long and 220 mm high and be placed between 500 mm and 1.8 m above the ground on the fore or rearmost surface of the unit.
In the case of B-Triples in Western Australia they are signed front and rear with "Road-Train" until they cross the WA/SA border where they are then signed with "Long Vehicle" in the front and rear.
Converter dollys must have a sign affixed horizontally to the rearmost point, complying to the same conditions, reading "LONG VEHICLE". This is required for when a dolly is towed behind a trailer.
Operating weights.
Operational weights are based on axle group masses, as follows:
Therefore, a B-Double would weigh 62.5 t (6 t + 16.5 t + 20 t + 20 t). A double road train would have an operational weight (without concessions) of 79 t (6 t + 16.5 t + 20 t + 16.5 t + 20 t). A triple is 79 t + 36.5 t (16.5 t + 20 t), giving an all up weight of 115.5 t. Quads weigh in at 135.5 t. Concessional weight additions (0.5 – per group) can see a quad end up weighing 149 t. If a tri-drive prime mover is utilised, along with tri-axle dollys, weights can reach nearly 170 t.
Speed limits.
The Australian national heavy vehicle speed limit is 100 km/h, excepting:
In western Canada, LCVs are restricted to 100 km/h, or the posted speed limit. Trucks of legal length (<25 m) may travel at 110 km/h, or the posted speed limit.
World's longest road trains.
Below is a list of longest road trains driven in the world. Most of these had no practical use, as they were put together and driven across relatively short distances for the express purpose of record-breaking.

</doc>
<doc id="55266" url="http://en.wikipedia.org/wiki?curid=55266" title="Asia (band)">
Asia (band)

Asia (often stylized as ASIA) is a British progressive rock band. The band was formed in 1981 as a supergroup of four members from different progressive rock bands, namely John Wetton (former bassist/vocalist of bands including King Crimson, Family, Roxy Music, Uriah Heep, UK and Wishbone Ash), Steve Howe (guitarist of Yes), Geoff Downes (keyboardist of Yes and The Buggles) and drummer Carl Palmer (of Emerson, Lake & Palmer, The Crazy World of Arthur Brown, and Atomic Rooster). With their longtime No. 1 debut album "Asia" in 1982, Asia ranks as one of the most popular progressive rock bands in history.
The band has gone through many line-up changes throughout its history, but in 2006, the original line-up reunited. As a result of this, a second band called Asia Featuring John Payne exists as a continuation of John Payne's career as Asia's frontman from 1991 until Wetton's return in 2006. In 2013, Howe retired from the band to continue with Yes and pursue other projects, and was replaced by guitarist Sam Coulson, completing the current lineup.
History.
Formation.
Asia began in early 1981 with the apparent demise of Yes and Emerson, Lake & Palmer, two of the flagship bands of British progressive rock. After the break-up of King Crimson in 1974, various plans for a super group involving bassist John Wetton had been mooted, including the abortive British Bulldog project with Bill Bruford and Rick Wakeman in 1976. Wakeman left this project at the urging of management, according to Bill Bruford. In 1977 Bruford and Wetton were reunited in U.K., augmented by guitarist Allan Holdsworth and keyboardist/violinist Eddie Jobson. Their self-titled debut was released in 1978. But by January 1980, U.K. had folded after one lineup change and three recordings. A new supergroup project was then suggested involving Wetton, Wakeman, drummer Carl Palmer and (then little known) South African guitarist/singer Trevor Rabin, but Wakeman left this project too shortly before they were due to sign to Geffen and before they had played together. Wetton's "Caught in the Crossfire" solo album (1980) did not fare very well in England.
In late December 1980, Wetton and former Yes guitarist Steve Howe were brought together by A&R man John Kalodner and Geffen Records to start writing material for a new album. They were eventually joined in early 1981 by drummer Carl Palmer, and finally by Howe's recent Yes cohort, keyboardist Geoff Downes. Two other players auditioned and considered during the band's formation were former The Move and ELO founder Roy Wood and the aforementioned guitarist/singer Trevor Rabin, who would go on to be part of a reformed Yes in 1983. Rabin, in a filmed 1984 interview included in the DVD "9012Live," said that his involvement with Asia never went anywhere because "there was no chemistry" among the participants.
The band's first recordings, under the auspices of Geffen record label head David Geffen and Kalodner, were considered disappointing by music critics and fans of traditional progressive rock, who found the music closer to radio-friendly Album-oriented rock. However, Asia clicked with fans of popular arena acts such as Journey, Boston and Styx. (Indeed, Kalodner had once introduced Wetton to Journey's short-lived frontman Robert Fleischman, with a view to Fleischman becoming Asia's lead-singer. As they worked on material together, Fleischman was impressed by Wetton's singing and felt the voice best suited to the new material was Wetton's own. He left Asia amicably.)
Rolling Stone gave "Asia" an indifferent review, while still acknowledging the band's musicianship was a cut above the usual AOR expectations.
1981–85: Early years.
Asia's self-titled debut album "Asia", released in March 1982, received considerable commercial success, spending nine weeks at number one in the U.S. album chart and selling over 4 million copies in the States alone. The album sold over 10 million worldwide and has never been out of print. The singles "Only Time Will Tell" and "Heat of the Moment" became huge Top 40 hits, both boosted by popular MTV music videos. Both tracks were stadium favourites at U.S. sporting events. "Sole Survivor" also received heavy air play on rock stations across the U.S., as did "Wildest Dreams" (another MTV video) and "Here Comes The Feeling". The band's best performing single, and perhaps their most recognised and popular hit song, "Heat of the Moment", spent six weeks at #1 on "Billboard"‍ '​s Album Rock Tracks chart and climbed to #4 on the Hot 100.
In the United States the band did extremely well, selling out every date on their debut tour, which began at Clarkson University in Potsdam, New York on April 22, 1982, continued in theatres but quickly expanded into massive arenas because of high ticket demand. Asia would go on to receive a Grammy Award nomination as Best New Artist of 1982. MTV also played Asia videos on heavy rotation — as many as five times a day. Both "Billboard" and "Cash Box" named Asia's debut the #1 Album Of The Year. Asia's logo and cover art were handled by illustrator Roger Dean of Yes and Uriah Heep fame.
However, neither the second album, "Alpha" (released in July 1983), nor any following Asia album could repeat the chart success of the first release. "Don't Cry" was a #1 Album Rock Track and Top 10 Pop hit in the summer of 1983, and the video received considerable attention on MTV, while "The Smile Has Left Your Eyes", a for John Wetton's vocal performance, was yet another big Top 40 hit for the band. The video for "Smile" also scored heavy MTV play. However, "Rolling Stone" panned "Alpha" as an over-produced commercial album, while others lamented that Howe and Palmer were effectively reduced to session musicians. The tracks "Eye to Eye" and "My Own Time" became huge fan favourites. "Open Your Eyes", "Midnight Sun" (featuring a strong vocal performance by Wetton), and "The Heat Goes On" became concert staples. "Alpha" received indifferent reviews from various critics, while still attaining Platinum status and reaching #6 on the Billboard album chart.
In October 1983, Wetton was forced out of the group on the heels of the comparatively disappointing sales of "Alpha". The band stated that Wetton quit and there is no universally agreed upon version of what happened. Wetton later revealed one factor may have been his alcohol dependency. In any event, the next leg of their 1983 U.S. tour (which had begun in the summer but shut down suddenly on 10 September after a performance at Pine Knob in Detroit), scheduled for the autumn, was abruptly cancelled, reportedly because of low ticket sales. Ex-King Crimson and ELP front man Greg Lake replaced Wetton for the highly publicised "Asia in Asia" concert at the Nippon Budokan Hall in Tokyo, Japan on 6 December 1983, which was the first concert broadcast over satellite to MTV in the U.S. and later made into a home video. Some of the songs had to be played in a lower key to suit Lake's voice and he read most of the lyrics from a teleprompter. The Japanese dates were successful financially but not musically. Lake left in early 1984 and Asia reunited with Wetton that same year to start work on their next album. But Howe soon left to be replaced by Krokus guitarist Mandy Meyer. Howe then enjoyed brief success with GTR, another supergroup, formed with Steve Hackett of Genesis and produced by Downes.
1985–91: "Astra" to the USSR.
The third Asia album was tentatively titled "Arcadia", but during production it was discovered that that name was being used by a forthcoming spin-off project from Duran Duran. The retitled "Astra", released in November 1985, was not as commercially successful as the first two albums. The record label cancelled the projected tour because of lack of interest. Howe's replacement, Mandy Meyer of Krokus, provided more of a hard-rock approach. The band charted another single with "Go", featuring Meyer's guitar heroics centre stage. The music video was another hit with MTV. In 1986 this Asia line-up folded, bringing the group to an end for the time being. Singer/bassist/songwriter Wetton is quoted as saying "It ["Astra"] did really well in Sweden ... but Swedish sales aren't that large."
Wetton resurfaced with a 1987 album with guitarist Phil Manzanera, "Wetton-Manzanera", based on material that had been originally intended for Asia. Also in 1987, Wetton played with Phenomena on their "Dream Runner" album and landed a number one hit in South America with the Phenomena single "Did It All for Love", also appearing in the related music video. Asia were also credited with contributing the Giorgio Moroder produced track "Gypsy Soul" to the Sylvester Stallone film soundtrack to "Over the Top", although Wetton was the only band member involved.
Wetton and Downes' attempt to restart the group in 1987 with guitarist Scott Gorham (formerly of Thin Lizzy) and drummer Michael Sturgis (ex-a-ha) fizzled when they were unable to land a worldwide recording deal.
Wetton and Palmer were more successful in reuniting the band for a few tours of Europe in the summer and fall of 1989. Downes (who was working on a project with Greg Lake) was not available at this time, so keyboards were played by John Young. Guitars on this tour were handled by Alan Darby (who was replaced shortly after by German guitarist Holger Larisch) and Zoe Nicholas and Susie Webb were brought aboard to provide back-up vocals. Unlike Wetton's later anger at Asia continuing without him in the 1990s, this lineup was viewed favourably by other Asia band members.
Asia returned to the studio in 1990 with Downes, Toto guitarist Steve Lukather and other musicians (see discography below) and released "Then & Now", a best-of with four new tracks. "Days Like These" from the disc received substantial airplay during the summer of 1990 on AOR radio stations and re-sparked some interest in the band. Pat Thrall joined Downes, Palmer and Wetton on tour and they performed classic material, including King Crimson and U.K. songs. The band toured the Soviet Union in November 1990 to play in front of 20,000 fans on two sold out nights. "Days Like These" charted in the U.S. at No.64 in 1990 and climbed all the way to No.2 on the U.S. Album Rock Tracks chart. A video was planned but scrapped because various problems hampered the single's chance at the Top 40. Asia received the RIAA Gold album award for "Then and Now" many years later, but the initial response was modest as the album failed to dent the Top 100. A DVD and CD are available of the Asia concerts in the USSR (featuring a bonus studio track, "Kari-Anne" recorded by the 1987 Wetton-Downes-Gorham-Sturgis lineup and with Francis Dunnery contributing a guitar solo). Wetton left in April 1991 after a South American tour, discouraged by Asia's lack of success in the U.S.
1991–2006: Payne era.
After Wetton's 1991 departure, vocalist/bassist John Payne joined the band and, together with Downes, enlisted new musicians and led Asia through to 2006. The first album with this line-up was Aqua, released in June 1992. In addition to Downes and Payne, the album featured Howe, Palmer, and guitarist Al Pitrelli (of Danger Danger, Megadeth and Alice Cooper). Howe returned during the sessions having just left Yes again, but Palmer would leave soon, committing to an ELP reunion, and was able to play on just three songs. Session men then completed the drumming. Downes' environmentalist single "Who Will Stop the Rain?" (originally written for Max Bacon and the aborted "Rain" project, later appearing on Bacon's album "From the Banks of the River Irwell") attracted some radio attention. The "Aqua" club tour featured Howe (whose presence was heavily promoted), who took the stage after the fifth song. The tour was successful enough to warrant the band's continuation. The 1992-93 tour featured Downes, Howe, Payne, guitarist Vinny Burns and drummer Trevor Thornton. Before a European festival tour in late 1993, Howe and Burns left and were replaced by guitarist Keith More. Payne acquitted himself on the tour, but many fans refused to support an Asia lineup without Wetton.
The group released "Aria" in May 1994, which featured lead guitarist Al Pitrelli once again, who would leave Asia during the short "Aria" tour. This illustrates how the keyboard-driven band often faced trouble keeping a regular guitarist. "Aria" did not fare well commercially and the ensuing tour was limited to four concerts. Ex-Simply Red guitarist Aziz Ibrahim took over during the tour. "Aria" also introduced new drummer Michael Sturgis, who had been involved during the band's aborted 1987 reunion and had appeared on some of the sessions for "Aqua". "Aria" was not released in the United States until 1995.
Over New Year's Eve 1996, a broken pipe inundated the control room in Downes' and Payne's recording studio, Electric Palace, in London. Amid the lost equipment, a vault containing unreleased material was found intact. The band decided to release the double-disc "Archiva", a collection of unreleased tracks recorded during the first three Downes/Payne albums. Next, "Arena", released in February 1996, featured Downes, Payne, Sturgis, Ibrahim and guest guitarist Elliott Randall (ex-Steely Dan, and Randy Crawford). The album featured Asia's longest track ever in "The Day Before the War". The album was released on Resurgence Records but there was no tour because of lack of interest. The group's lone promotional performance in conjunction with the album occurred on 19 April 1996, when Downes and Payne appeared with guitarist Elliott Randall on the Virgin FM radio programme "Alive in London" to play the song "Never".
An all acoustic album, "Live at the Town & Country Club", was recorded by the group in September 1997 (and released in 1999) that featured a line-up of Downes, Payne, Ibrahim, and drummer Bob Richards.
In 1999 there was talk of a reunion of the original lineup (minus Howe). The original proposition included Dave Kilminster on guitar, who had previously toured and recorded with Wetton. While Howe was interested in participating, he was unable to because of his busy schedule with Yes. This reunion did not take place and John Payne continued to carry on Asia with Downes uninterrupted. Wetton and Palmer did, however, get together to form Qango, which included Kilminster and John Young, although the band was short-lived. Kilminster went on to work with Keith Emerson, The Nice and Roger Waters. In 2000, Geffen/Universal released a best-of entitled "The Very Best of Asia: Heat of the Moment (1982–1990)" which also included three rare B-sides from the early days.
2001's "Aura" featured three different session guitarists, including Ian Crichton (of Canadian progressive rock band Saga) who'd briefly joined Asia in 1998-1999. "Aura" took a more progressive rock form, but still did not recapture the commercial success of the first album. Former members Howe, Thrall, Sturgis and Elliott Randall also made guest appearances. The single "Ready to Go Home" was barely distributed. Asia then signed with Recognition. 2001 did see the band with a stable line-up, achieved during the "Aura" sessions featuring Downes, Payne, guitarist Guthrie Govan and ex-Manfred Mann's Earth Band/The Firm/Uriah Heep/Gary Numan/AC/DC drummer Chris Slade (who had first joined Asia in 1999, briefly). Asia would tour for the first time since 1994, including the first U.S. dates since 1993. A live album and DVD, both titled "America: Live in the USA", were released in 2003, recorded at the Classic Rock Festival in Trenton, New Jersey in October 2002, which they co-headlined with Uriah Heep.
In the summer of 2003, Downes and Payne undertook the "Asia Across America Tour" which received some media attention. Performing "unplugged", the duo would reportedly play anywhere in the U.S. that fans requested, provided there was a venue and the fans put up $3,000 to cover costs.
Marking a departure from convention, for the first time, a studio release was not titled as a single word starting and ending with the letter A (excepting the partial compilation / partial new album "Then & Now"). Released on Asia's newly signed label SPV/Inside Out Records, 2004's "Silent Nation" (name influenced by the Howard Stern vs. FCC incident) picked up some unexpected exposure on the Internet.
In 2004 an acoustic Asia toured once again featuring only Downes and Payne. In 2005 the full band toured in Europe and the Americas playing settings ranging from small clubs to medium-sized theatres. Again, in the U.S., attendance was poor at best. Meanwhile, Wetton and Downes released some archival Asia material under the name Wetton/Downes and they then reunited to record a full-length album ("Icon", released in 2005), and an accompanying EP and DVD. Two additional Icon projects have since followed: "Icon II - Rubicon" (2006) and "Icon 3" (2009).
In August 2005, Slade left the group to be replaced by drummer Jay Schellen. The new band started work on an album, tentatively entitled "Architect of Time", which was originally planned for release in 2006, though later developments would cause this project to be shelved.
2006–13: "Original Asia" reforms.
In early 2006, the partnership between Downes and Payne was dissolved when Downes left for a reunion of the original band line-up under the Asia name, a breakup that Payne described as "painful". The existing line-up (minus Downes) continued for a short while before morphing into GPS.
When Downes split in 2006, Payne owned a significant portion of the rights to the band name 'Asia', until a legal agreement was set by both bands' management. The original members now perform and record as Asia exclusively. On 9 May 2006, John Payne, Geoff Downes, John Wetton, Carl Palmer and Steve Howe contractually agreed that John Payne could continue his 15-year period with Asia as Asia Featuring John Payne. Asia featuring John Payne debuted in 2007 with Payne on vocals/bass, Guthrie Govan on guitar, Erik Norlander on keyboards and Jay Schellen on drums.
The official websites of each band reflect a split between the shared history of Payne's tenure with the band, as the reunited Asia acknowledge only pre- and post-Payne albums, whereas Asia Featuring John Payne claim Payne-era (1991–2006) albums "Aqua" (1991) through "Silent Nation" (2004) as part of their own discography. Asia Featuring John Payne perform songs from the entire history of Asia.
Downes and the other three original members (Wetton, Palmer and Howe) convened a group meeting in England in early 2006 in anticipation of formally reforming for work that year. And after a slew of rumours, they announced that this original line-up of Asia were planning a CD, DVD and world tour to celebrate the band's 25th anniversary. The band appeared in October 2006 on U.S. cable channel VH-1 Classic and began a world tour largely focused on the U.S. The band secured ownership of the Asia name and toured under the description of the Four Original Members of Asia. The set list featured most of the first album as well as a couple of songs from the second, along with one selection each from Yes, ELP, King Crimson and the Buggles to acknowledge the history of each member of the band. In a 2006 interview, guitarist Steve Howe states, "This is the real Asia. There have been other incarnations of the band, but this is the one that the public truly embraced".
The tour began on 29 August 2006 in Rochester, New York. "The Definitive Collection" was released by Geffen/Universal to tie into the tour in September and peaked at No.183 on the U.S. album charts--—the first time Asia had made the charts since 1990. A limited edition release available only at Best Buy stores also included a DVD of all the band's music videos.
The reunion tour continued into 2007 with venue size based on the success of the 2006 shows, where the band was mainly playing in clubs and theatres. Many of these sold out, including all seven dates in Japan. Also in 2007, the band released "Fantasia: Live In Tokyo" on CD and DVD through Eagle Records, commemorating the 25th Anniversary and documenting the success of the 2006-2007 tour.
In mid-2007, all four original members (Wetton, Downes, Howe and Palmer) went into the studio to record a new album, marking the first recorded material from all four original members since 1983's "Alpha". The band continued to tour until major heart surgery for Wetton in the second half of the year saw remaining tour dates rescheduled for 2008. The new studio album, entitled "Phoenix", was released on Frontiers Records on 14 April 2008 (via EMI/Capitol on 15 April in North America), along with a world tour to promote. The 12-track album includes "An Extraordinary Life", based on Wetton's experience of ill health; rockers such "Never Again" and "Nothing's Forever"; and power ballads such as "Heroine" and "I Will Remember You". The world tour also featured a couple of the new songs. The album cover featured Dean's illustration and design. The Phoenix album did well in both the American and European/Japanese markets. It debuted at No.73 American Billboard 200; the band had not charted with a studio album since 1985.
As a special finale to the US "Phoenix" tour, the band performed, for the first time ever, the entire first Asia album from beginning to end at their San Francisco concert at The Regency Center on 5 May. The album comprised the entire 2nd set of the evening's concert.
In summer 2009, Asia toured the U.S.A. with Yes. Asia opened with a 55-minute show, while Yes closed with a 1 hour and 50 minute set. Asia's set included only "An Extraordinary Life" from "Phoenix", the rest of the songs coming from the first two albums plus one cover each from The Buggles ("Video Killed the Radio Star" with Wetton on lead vocals and Downes on vocoder), King Crimson ("The Court of the Crimson King", which was recorded by the original incarnation of that band with Greg Lake on lead vocals) and Emerson, Lake & Palmer ("Fanfare for the Common Man"). Yes songs were omitted from this tour's setlist, though Asia also covered "Roundabout" on earlier legs of the "Four Original Members" tour. Contrary to some early expectations, Downes did not perform with Yes, although their set list included two songs from the 1980 album "Drama", which featured Downes on keys. A series of shows late in the tour featured a special appearance by Ian McDonald (flute and vocals on "The Court Of The Crimson King" which he co-wrote and backing vocals on "Heat Of The Moment").
In late 2009, the band began working on their follow-up CD to "Phoenix". According to Wetton's website in late November 2009: "Good news is that the new album is starting to leap, rather than creep (or sleep) in terms of progress. This week I have two completed lead vocals, with complete harmony/chorus voxes on three. It's just me, Geoff [Downes], Steve R[ispin], and Mike Paxman in the studio--- Carl [Palmer] is pretty much all done, Steve H[owe] is half done, and returns to the fold after Yes tour. It sounds absolutely wonderful".
The follow-up, titled "Omega", was released in the UK on 26 April 2010.
The band finished a new studio album timed to coincide with the band's thirtieth anniversary, titled "XXX", and released in the U.K. on 2 July 2012 and worldwide around the same time. In September 2012 they played 4 shows in Japan and a North American tour started on 11 October 2012. The UK tour, however, had to be cancelled after a number of shows due to Palmer contracting a serious case of E. coli.
2013-present: Howe's retirement and "Gravitas".
On 10 January 2013, Steve Howe announced his retirement from the band to focus on other projects, including Yes, bringing an end to the reunion of the original lineup. Asia in turn announced they would be continuing with new guitarist Sam Coulson, with a new album in the works entitled "Gravitas". The new line-up performed live in 2013.
On the website ultimateclassicrock.com, Howe explained his decision to leave Asia:
"..Something had to give. Because I’d just done five years with both bands and then Geoff had joined [Yes] when we did "Fly From Here", which is maybe a lot shorter, only a quarter of that time for him. He only experienced the tip of the iceberg of being on call for two bands. But there were times in the first three years — it actually got easier when Geoff joined. It was easier because we were both in the band and we could both wrestle with the schedules — but before that, at times, Yes or Asia would extend a tour by a day and then Yes or Asia would then expand the start of the tour, so the gap would start to close.
And I would start freaking out saying “yeah, but hang on — if you add that date here and they’ve just added this date here, I’m now squeezed like a concertina.” So there was going to be a time at some point when this was unworkable and unfortunately it was the end of last year that made me realize that this being on call was really too much. I couldn’t keep either really happy. I was either making Yes miserable or Asia miserable, because of the other one being in existence.
So I think that Asia had a terrific run and we made three great albums. In fact, ‘XXX,’ I think is a fantastic record."
The band finished the recording sessions for "Gravitas" in December 2013 and in January 2014 they started shooting the music video for "Valkyrie", which was released as a single. The album's cover artwork was designed by Asia longtime collaborator, Roger Dean. On 30 January 2014, Wetton revealed the album's track listing through Asia's official website and talked about each song from the album. The album was released on 24 March 2014. Gravitas, and reached Number 1 in the Progressive Rock Chart for emusic on 27 March.
Personnel.
Through the years, Geoff Downes has been the most consistent member of the band, which experienced a revolving roster of noted musicians, particularly in the 1990s.
Certain musicians have joined and left after a short time without recording any studio material with the group. The most notable collaboration of this kind was the participation of Greg Lake in the "Asia in Asia" concert on bass guitar and lead vocals. Yet more musicians have played as session musicians or have guested with the band without formally joining. Some of these artists include: Robert Fleischman, Vinnie Colaiuta, Francis Dunnery, Ant Glynne, Scott Gorham, Tomoyasu Hotei, Luís Jardim, Ron Komie, Tony Levin, Steve Lukather, Thomas Lang, Kim Nielsen-Parsons, Nigel Glockler, Simon Phillips, and Alex Thomas.
Steve Howe of Yes fame was an original member, and rejoined the original lineup in 2006, before departing to pursue other projects in 2013.
Timeline.
This is an approximate timeline of the members of Asia.

</doc>
<doc id="55268" url="http://en.wikipedia.org/wiki?curid=55268" title="Øresund">
Øresund

Øresund, also spelled Öresund, and more commonly known in English as the Sound (Danish: "Øresund", ]; Swedish: "Öresund", ]) and locally in both countries called just "Sundet", is the strait that separates the Danish island Zealand from the southern Swedish province of Scania. Its width is 4 km at the narrowest point between Kronborg Castle at Helsingør in Denmark, and the northern harbour of Helsingborg in Scania. The strait has also lent its name to the Øresund Region of 3.8 million inhabitants on both the Danish and Swedish sides. Since this area covers all of Denmark east of Great Belt and all of Scania, its area of more than 20,000 km2 doesn't give a full picture of the very high population density around the sea. A more narrow area, including the four Danish provinces (Danish: "Landsdel") "Byen København, København omegn, Nordsjælland and Østsjælland" together with the 17 (out of 33 possible) Scanian municipalities which either are located by the Øresund, or which border to a such municipality, roughly the population which lives no more than 30 km from the shores of Øresund. Together they have an area of 6,000 square kilometres with 2.9 million inhabitants.
Øresund is one of the three Danish Straits that connect the Baltic Sea to the Atlantic Ocean via Kattegat, Skagerrak, and the North Sea, and is one of the busiest waterways in the world.
The Øresund Bridge (which includes a 3 km tunnel) between the Danish capital Copenhagen and the largest city of Scania, Malmö, was inaugurated on 1 July 2000 by Queen Margrethe II of Denmark and King Carl XVI Gustaf of Sweden. However the HH Ferry route, between Helsingør, Zealand, Denmark and Helsingborg, Scania, Sweden, in the northern part of Øresund, where it is most narrow, remains one of the world's busiest international ferry routes with more than 70 departures from each harbour per day,
Etymology.
It is first attested on a Danish runestone from about 950, where it is written "ura suti", i.e. "Ø̄rasundi" (in the dative case). 
The West Norse and Icelandic form is "Eyrarsund". The first part of the word is "øre" (Old Norse "eyra") "ear", and the second part is "sund", i.e. strait or narrow seaway.
The Øresund is so called because "øre" "ear" is a term for a small piece of land between two waters, and the Øresund stretches between two such "ears", from "Siellands Øre" to "Skan-Øre".
The strait is today called "Øresund" in Danish and "Öresund" in Swedish, informally "Sundet" (lit. "the Strait") in both languages.
Streams, animals and salinity.
Øresund, like other Danish and Danish-German straits, is at the border between oceanic salt water (which has a salinity of more than 30 PSU) and less salty Baltic Sea. As Cattegat in the north has almost oceanic conditions and the Baltic Sea (6–7 PSU, in its main basin) has brackish water, Øresund's water conditions are rather unusual. The streams are very complex, but the "surface" stream is often northbound (from the Baltic Sea) which gives a lower surface salinity, though streams can change from one day to another. The average surface salinity is about 10–12 PSU in the southern part but above 20 PSU north of Helsingør.
Near the seafloor, conditions are more stable and salinity is always oceanic (above 30 PSU) below a certain depth that varies between 10 and 15 metres. In the southern part, however, the depth is 5–6 metres (outside the rather narrow waterways "Drogden" and "Flintrännan"), and this is the definite border of oceanic salt water, therefore also a border for many maritime species of animals. In the central Baltic Sea only 52 known salt-water species reside, compared to around 1500 in the North Sea. Close to 600 species are known to exist in at least some part of Øresund. Well-known examples, for which the bottom salinity makes a distinct breeding border, include lobster, small crabs ("Carcinus maenas"), several species of flatfish and the burning jellyfish ("Cyanea article"); the latter can sometimes drift into the southwest Baltic sea, but it cannot reproduce there.
The daily tides exist, but the lunar attraction cannot force much water to move from west to east or vice versa, in narrow waters where the current is either northbound or southbound. So, not much of the difference in water levels in Øresund is due to daily tides, and other circumstances "hide" the little tide that still remains. The current has a much stronger effect on the water level, compared to the tide, but strong winds may also affect the water level. During exceptional conditions, such as storms and hurricanes, oceanic water may suddenly flow on all depths into the Baltic Sea. Such events give depths in southern Baltic Sea new fresh water, with higher salinity, which makes it possible for especially cod to breed in the Baltic Sea. If no such inflow of oceanic water to the Baltic Sea occurs for around a decade, the breeding of cod becomes endangered.
History.
Political control of Øresund has been an important issue in Danish and Swedish history. Denmark maintained military control with the coastal fortress of Kronborg at Elsinore on the west side and Kärnan at Helsingborg on the east, until the eastern shore was ceded to Sweden in 1658, based on the Treaty of Roskilde. Both fortresses are located where the strait is 4 kilometres wide.
In 1429, King Eric of Pomerania introduced the Sound Dues which remained in effect for more than four centuries, until 1857. Transitory dues on the use of waterways, roads, bridges and crossings were then an accepted way of taxing which could constitute a great part of a state's income. 
The Strait Dues remained the most important source of income for the Danish Crown for several centuries, thus making Danish kings relatively independent of Denmark's Privy Council and aristocracy.
To be independent of the Øresund, Sweden carried out two great projects, the foundation of Göteborg (Gothenborg) in 1621 and the construction of the Göta Canal from 1810 to 1832.
The Copenhagen Convention of 1857 abolished the Dues and made the Danish straits an international waterway.
A fixed connection was opened across the strait in 2000, the Øresund Bridge.

</doc>
<doc id="55271" url="http://en.wikipedia.org/wiki?curid=55271" title="Francisco Pizarro">
Francisco Pizarro

Francisco Pizarro González (; ]; c. 1471 or 1476 – 26 June 1541) was a Spanish conquistador who conquered the Incan Empire.
Pizarro González was born in Trujillo, Spain, the illegitimate son of Gonzalo Pizarro, an infantry colonel, and Francisca González, a woman of poor means. His exact birth date is uncertain, but is believed to be sometime in the 1470s, probably 1471. Scant attention was paid to his education and he grew up illiterate. He was a distant cousin of Hernán Cortés. On 10 November 1509, Pizarro sailed from Spain to the New World with Alonzo de Ojeda on an expedition to Urabí. He sailed to Cartagena and joined the fleet of Martín Fernández de Enciso, and, in 1513, accompanied Balboa to the Pacific. In 1514, he found a supporter in Pedrarias Dávila, the Governor of Castilla de Oro, and was rewarded for his role in the arrest of Balboa with the positions of mayor and magistrate in Panama City, serving from 1519 to 1523.
Reports of Peru's riches and Cortés's success in Mexico tantalized Pizarro and he undertook two expeditions to conquer the Incan Empire in 1524 and in 1526. Both failed as a result of native hostilities, bad weather, and lack of provisions. Pedro de los Ríos, the Governor of Panama, made an effort to recall Pizarro, but the "conquistador" resisted and remained in the south. In April 1528, he reached northern Peru and found the natives rich with precious metals. This discovery gave Pizarro the motivation to plan a third expedition to conquer Peru, and he returned to Panama to make arrangements, but the Governor refused to grant permission for the project. Pizarro returned to Spain to appeal directly to King Charles I. His plea was successful, and he received not only a license for the proposed expedition but considerable authority over any lands conquered during the venture. He was joined by family and friends, and the expedition left Panama in 1530.
When hostile natives along the coast threatened the expedition, Pizarro moved inland and founded the first Spanish settlement in Peru, San Miguel de Piura. Inca Atahualpa refused to tolerate a Spanish presence in his lands but was captured by Pizarro during the Battle of Cajamarca on 16 November 1532. A ransom for the Emperor's release was demanded and Atahualpa filled a room with gold, but Pizarro charged him with various crimes and executed Atahualpa on 26 July 1533, much to the opposition of his associates who thought the conquistador was overstepping his authority. The same year, Pizarro entered the Incan capital of Cuzco, and the conquest of Peru was complete. In January 1535, Pizarro founded the city of Lima, a project he considered his greatest achievement. Quarrels between Pizarro and his longtime comrade-in-arms Diego Almagro culminated in the Battle of Las Salinas. Almagro was captured and executed, and, on 26 June 1541, his embittered son assassinated Pizarro in Lima. The conqueror of Peru was laid to rest in the Lima Cathedral.
When historians compare Pizarro's and Cortés's conquests of Peru and Mexico, they usually give the palm to Pizarro because he led fewer men, faced larger armies, and was far from Spanish outposts in the Caribbean which could have supplied men, arms, and provisions. After Pizarro's death, his family built a palace commemorating the "conquistador" on the Plaza Mayor in Trujillo, but modern Peruvians look askance at Pizarro, considering him the force behind the destruction of their indigenous culture, language, and religion.
Early life.
Pizarro was born in Trujillo, in modern day Extremadura. His birth year is uncertain, but is placed sometime in the 1470s. He was the illegitimate son of Gonzalo Pizarro Rodríguez de Aguilar (1446–1522) and Francisca González Mateos, a poor woman of Trujillo. His father was a colonel of infantry who served in Navarre and in the Italian campaigns under Córdoba. His mother married late in life and had a son Francisco Martín de Alcántara, who was at the conquest of Peru with his half-brother from its inception. Through his father, Francisco was a second cousin once removed to Hernán Cortés. Little attention was paid to Francisco's education and he grew up illiterate. On 10 November 1509, Pizarro sailed from Spain to the New World with Alonzo de Ojeda on an expedition to Gulf of Uraba in Tierra Firma. Pizarro became a participant in Ojeda's failed colony, commanding the remnants until he abandoned it with the survivors.:93 He sailed to Cartagena and joined the fleet of Martín Fernández de Enciso in 1513.
Panama.
In 1513, Pizarro accompanied Vasco Núñez de Balboa in his crossing of the Isthmus of Panama to the Pacific coast.:23 The following year, in 1514, Pedrarias Dávila became the newly appointed governor of Castilla de Oro and succeeded Balboa. During the next five years, Pizarro became a close associate of Dávila and the governor assigned him a "repartimiento" of natives and cattle.:93 When Dávila decided to get rid of Balboa out of distrust, he instructed Pizarro to personally arrest him and bring him to stand trial. Balboa was beheaded in January 1519. For his loyalty to Dávila, Pizarro was rewarded with the positions of mayor (Alcalde) and magistrate of the then recently founded Panama City from 1519 to 1523.
Expeditions to South America.
The first attempt to explore western South America was undertaken in 1522 by Pascual de Andagoya. The native South Americans he encountered told him about a gold-rich territory called Virú, which was on a river called Pirú (later corrupted to Perú) and was where they came from.:24 These reports were related by the Spanish-Inca "mestizo" writer Garcilaso de la Vega in his famous "Comentarios Reales de los Incas" (1609).
Andagoya eventually established contact with several Native American "curacas" (chiefs), some of whom he later claimed were sorcerers and witches. Having reached as far as the San Juan River (part of the present boundary between Ecuador and Colombia), Andagoya fell very ill and decided to return. Back in Panama, he spread the news and stories about "Pirú" – a great land to the south rich with gold (the legendary El Dorado). These revelations, along with the accounts of success of Hernán Cortés in Mexico years before, caught the immediate attention of Pizarro, prompting a new series of expeditions to the south in search of the riches of the Incan Empire.
In 1524, while still in Panama, Pizarro formed a partnership with a priest, Hernando de Luque, and a soldier, Diego de Almagro, to explore and conquer the South. Pizarro, Almagro, and Luque later renewed their compact more explicitly,:24 agreeing to conquer and divide equally among themselves the opulent empire they hoped to discover. While historians agree their accord was strictly oral (no written document exists to prove otherwise), they are known to have dubbed their enterprise the "Empresa del Levante" and determined that Pizarro would command the expedition, Almagro would provide the military and food supplies, and Luque would be in charge of finances and any additional provisions they might need.:95
First expedition (1524).
In November 1524, the first of three expeditions left from Panama for the conquest of Peru with about 80 men and 40 horses.:24 Juan de Salcedo was the standard bearer, Nicolas de Ribera was the treasurer, and Juan Carvallo was the inspector.:45,47
Diego de Almagro was left behind because he was to recruit men, gather additional supplies, and join Pizarro later. The Governor of Panama, Pedro Arias Dávila, at first approved in principle of exploring South America. Pizarro's first expedition, however, turned out to be a failure as his "conquistadors", sailing down the Pacific coast, reached no farther than Colombia before succumbing to such hardships as bad weather, lack of food, and skirmishes with hostile natives, one of which caused Almagro to lose an eye by arrow-shot. Moreover, the place names the Spanish bestowed along their route, including "Puerto deseado" (desired port), "Puerto del hambre" (port of hunger), and "Punta Quemado" or "Puebla Quemado" (burned port), only confirm their straits. Fearing subsequent hostile encounters like the one the expedition endured at the Battle of Punta Quemada, Pizarro chose to end his tentative first expedition and return to Panama.:94–102
Second expedition (1526).
Two years after the first very unsuccessful expedition, Pizarro, Almagro, and Luque started the arrangements for a second expedition with permission from Pedrarias Dávila. The Governor, who himself was preparing an expedition north to Nicaragua, was reluctant to permit another expedition, having lost confidence in the outcome of Pizarro's expeditions. The three associates, however, eventually won his trust and he acquiesced. Also by this time, a new governor was to arrive and succeed Pedrarias Dávila. This was Pedro de los Ríos, who took charge of the post in July 1526 and had manifested his initial approval of Pizarro's expeditions (he would later join him several years later in Peru).:103–104
On 10 March 1526, after all preparations were ready, Pizarro left Panama with two ships with 160 men and several horses, reaching as far as the Colombian San Juan River. Soon after arriving the party separated, with Pizarro staying to explore the new and often perilous territory off the swampy Colombian coasts, while the expedition's co-commander, Almagro, was sent back to Panama for reinforcements. Pizarro's "Piloto Mayor" (main pilot), Bartolomé Ruiz, continued sailing south and, after crossing the equator, found and captured a "balsa" (raft) under sail, with natives from Tumbes. To everyone's surprise, these carried a load of textiles, ceramic objects, and some much-desired pieces of gold, silver, and emeralds, making Ruiz's findings the central focus of this second expedition which only served to pique the conquistadors' interests for more gold and land. Some of the natives were also taken aboard Ruiz's ship to serve later as interpreters.:105–109:24–25
He then set sail north for the San Juan river, arriving to find Pizarro and his men exhausted from the serious difficulties they had faced exploring the new territory. Soon Almagro also sailed into the port with his vessel laden with supplies, and a considerable reinforcement of at least eighty recruited men who had arrived at Panama from Spain with the same expeditionary spirit. The findings and excellent news from Ruiz along with Almagro's new reinforcements cheered Pizarro and his tired followers. They then decided to sail back to the territory already explored by Ruiz and, after a difficult voyage due to strong winds and currents, reached Atacames in the Ecuadorian coast. Here they found a very large native population recently brought under Inca rule. Unfortunately for the "conquistadors", the warlike spirit of the people they had just encountered seemed so defiant and dangerous in numbers that the Spanish decided not to enter the land.:110–112
The Famous Thirteen.
After much wrangling between Pizarro and Almagro, it was decided that Pizarro would stay at a safer place, the Isla de Gallo,:25–26 near the coast, while Almagro would return yet again to Panama with Luque for more reinforcements – this time with proof of the gold they had just found and the news of the discovery of an obvious wealthy land they had just explored. The new governor of Panama, Pedro de los Ríos, had learned of the mishaps of Pizarro's expeditions and the deaths of various settlers who had gone with him. Fearing an unsuccessful outcome, he outright rejected Almagro's application for continued resources. In addition, he ordered two ships commanded by Juan Tafur to be sent immediately with the intention of bringing Pizarro and everyone back to Panama.:112–115
The leader of the expedition had no intention of returning, and when Tafur arrived at the now famous "Isla de Gallo", Pizarro drew a line in the sand, saying: "There lies Peru with its riches; Here, Panama and its poverty. Choose, each man, what best becomes a brave Castilian. For my part, I go to the south.":116
Only thirteen men decided to stay with Pizarro and later became known as "The Famous Thirteen" ("Los trece de la fama"),:26 while the rest of the expeditioners left back with Tafur aboard his ships. Ruiz also left in one of the ships with the intention of joining Almagro and Luque in their efforts to gather more reinforcements and eventually return to aid Pizarro. Soon after the ships left, Pizarro and his men constructed a crude boat and journeyed twenty-five leagues north for La Isla Gorgona, where they would remain for seven months before the arrival of new provisions.:117–118
Back in Panama, Pedro de los Ríos (after much convincing by Luque) had finally acquiesced to the requests for another ship, but only to bring Pizarro back within six months and completely abandon the expedition. Both Almagro and Luque quickly grasped the opportunity and left Panama (this time without new recruits) for La Isla Gorgona to once again join Pizarro. On meeting with Pizarro, the associates decided to continue sailing south on the recommendations of Ruiz's Indian interpreters.:118
By April 1528, they finally reached the northwestern Peruvian Tumbes Region. Tumbes became the territory of the first fruits of success the Spanish had so long desired, as they were received with a warm welcome of hospitality and provisions from the "Tumpis", the local inhabitants. On subsequent days two of Pizarro's men, Alonso de Molina and Pedro de Candia, reconnoitered the territory and both, on separate accounts, reported back the incredible riches of the land, including the decorations of silver and gold around the chief's residence and the hospitable attentions which they were received with by everyone. The Spanish also saw, for the first time, the Peruvian Llama:26 which Pizarro called the "little camels". The natives also began calling the Spanish the "Children of the Sun" due to their fair complexion and brilliant armor. Pizarro, meanwhile, continued receiving the same accounts of a powerful monarch who ruled over the land they were exploring. These events only served as evidence to convince the expedition of the wealth and power displayed at Tumbes as an example of the riches the Peruvian territory had awaiting to conquer. The conquistadors decided to return to Panama to prepare the final expedition of conquest with more recruits and provisions. Before leaving, however, Pizarro and his followers sailed south not so far along the coast to see if anything of interest could be found. Historian William H. Prescott recounts that after passing through territories they named such as Cabo Blanco, port of Payta, Sechura, Punta de Aguja, Santa Cruz, and Trujillo (founded by Almagro years later), they finally reached for the first time the ninth degree of the southern latitude in South America. On their return towards Panama, Pizarro briefly stopped at Tumbes, where two of his men had decided to stay to learn the customs and language of the natives. Pizarro was also given two boys to learn his language, one of which was later baptized as Felipillo and served as an important interpreter, the equivalent of Cortés' La Malinche of Mexico, and another called Martinillo.:126,128 Their final stop was at La Isla Gorgona, where two of his ill men (one had died) had stayed before. After at least eighteen months away, Pizarro and his followers anchored off the coasts of Panama to prepare for the final expedition.:119–126
Capitulación de Toledo.
When the new governor of Panama, Pedro de los Ríos, had refused to allow for a third expedition to the south, the associates resolved for Pizarro to leave for Spain and appeal to the sovereign in person. Pizarro sailed from Panama for Spain in the spring of 1528, accompanied by Pedro de Candia, some natives and llamas, plus samples of fabric, gold and silver.:127–128
Pizzaro reached Seville in early summer. King Charles I, who was at Toledo, had an interview with Pizarro and heard of his expeditions in South America, a territory the conquistador described as very rich in gold and silver which he and his followers had bravely explored "to extend the empire of Castile." The King, who was soon to leave for Italy, was impressed at the accounts of Pizarro and promised to give his support for the conquest of Peru. It would be Queen Isabel, however, who, in the absence of the King, would sign the "Capitulación de Toledo" on 6 July 1529, a license document which authorized Francisco Pizarro to proceed with the conquest of Peru. Pizarro was officially named the Governor, Captain general, "Adelantado", and "Alguacil Mayor", of the New Castile for the distance of 200 leagues along the newly discovered coast, and invested with all the authority and prerogatives, his associates being left in wholly secondary positions (a fact which later incensed Almagro and would lead to eventual discords with Pizarro). One of the conditions of the grant was that within six months Pizarro should raise a sufficiently equipped force of two hundred and fifty men, of whom one hundred might be drawn from the colonies.:132–134,137
This gave Pizarro time to leave for his native Trujillo and convince his brother Hernando Pizarro and other close friends to join him on his third expedition.:136 Along with him also came Francisco de Orellana, who would later discover and explore the entire length of the Amazon River. Two more of his brothers from his father, Juan Pizarro and Gonzalo Pizarro,:27 and a brother from his mother, Francisco Martin de Alcantara,:136 would later decide to also join him as well as his cousin Pedro Pizarro who served as his page.:13 When the expedition was ready and left the following year, it numbered three ships, one hundred and eighty men, and twenty-seven horses.:138
Since Pizarro could not meet the number of men the Capitulación had required, he sailed clandestinely from the port of Sanlúcar de Barrameda for the Canary Island of La Gomera in January 1530. He was there to be joined by his brother Hernando and the remaining men in two vessels that would sail back to Panama.:137 Pizarro's third and final expedition left Panama for Peru on 27 December 1530.:27
Conquest of Peru (1532).
In 1531 Pizarro once again landed in the coasts near Ecuador, the province of Coaque and the region of "esmeraldas", where some gold, silver, and emeralds were procured and then dispatched to Almagro, who had stayed in Panama to gather more recruits.:139–140 Sebastian de Belalcazar soon arrived with thirty men.:141 Though Pizarro's main objective was then to set sail and dock at Tumbes like his previous expedition, he was forced to confront the Punian natives in the Battle of Puná, leaving three or four Spaniards dead and many wounded. Soon after, Hernando de Soto, another conquistador that had joined the expedition, arrived with a hundred volunteers and horses to aid Pizarro and with him sailed towards Tumbes,:143 only to find the place deserted and destroyed. Their two fellow conquistadors expected they had disappeared or died under murky circumstances. The chiefs explained the fierce tribes of Punians had earlier attacked them and ransacked the place.:152–153
As Tumbes no longer afforded the safe accommodations Pizarro sought, he decided to lead an excursion into the interior of the land in May 1532, and established the first Spanish settlement in Peru, San Miguel de Piura, and a "repartimiento".:153–154 An earlier settlement than this in South America was Santa Marta, Colombia, established in 1526, but this was the first in Peru.
Leaving fifty men back at the settlement under the command of Antonio Navarro, Pizarro proceeded with his conquest accompanied by two hundred men on 24 Sept. 1532.:155–156 After arriving at Zaran, Hernando de Soto was dispatched to a Peruvian garrison at Caxas. After a week, he returned with an envoy from the Inca himself, with some presents, and an invitation to visit the Incan ruler's camp.:156–158
Following the defeat of his brother, Huáscar, Atahualpa had been resting in the Sierra of northern Peru, near Cajamarca, in the nearby thermal baths known today as the Incan Baths. Arriving Cajamarca on 15 Nov. 1532, Pizarro had a force of just 110-foot-soldiers, 67 cavalry, 3 arquebuses, and 2 falconets. He sent Hernando Pizzarro and Hernando de Soto to meet with Atahualpa in his camp. Atahuallpa agreed to meet Pizarro in his Cajamarca plaza fortress the next day. Fray Vicente de Valverde and native interpreter Felipillo approached Atahualpa in Cajamarca's central plaza. After the Dominican friar expounded the "true faith" and the need to pay tribute to the Emperor Charles the Fifth, Atahualpa, replied "I will be no man's tributary." His complacency, because there were fewer than 200 Spanish as opposed to his 50,000 man army, of which 6000 accompanied him to Cajamarca, sealed his fate and that of the Incan empire.:157,161,166–177
Atahualpa's refusal led Pizarro and his force to attack the Incan army in what became the Battle of Cajamarca on 16 November 1532. The Spanish were successful and Pizarro executed Atahualpa's 12-man honor guard and took the Inca captive at the so-called ransom room. By Feb. 1533, Almagro had joined Pizarro in Cajamarca with an additional one hundred and fifty men with fifty horses.:186–194
Despite fulfilling his promise of filling one room (22 by) with gold and two with silver, Atahualpa was convicted of twelve charges, including killing his brother, and plotting against Pizarro and his forces. He was executed by garrote on 29 August 1533. Francisco Pizarro and de Soto were opposed to Atahualpa's execution, but Francisco consented to the trial due to the "great agitation among the soldiers", particularly by Almagro. De Soto was on a reconnaissance mission the day of the trial and execution, and upon his return expressed his dismay, stating, "he should have been taken to Castile and judged by the emperor.":202–204,206 King Charles later wrote to Pizarro: "We have been displeased by the death of Atahualpa, since he was a monarch, and particularly as it was done in the name of justice."
Pizarro advanced with his army of five hundred Spaniards toward Cuzco, accompanied by Chalcuchimac before he was burned at the stake. Manco Inca Yupanqui joined Pizarro after the death of Tupac Huallpa.:191,210,216
During the exploration of Cuzco, Pizarro was impressed and through his officers wrote back to King Charles I of Spain, saying:
"This city is the greatest and the finest ever seen in this country or anywhere in the Indies... We can assure your Majesty that it is so beautiful and has such fine buildings that it would be remarkable even in Spain."
The Spanish sealed the conquest of Peru by entering Cuzco on 15 Nov. 1533.:216 Jauja in the fertile Mantaro Valley was established as Peru's provisional capital in April 1534.:286 But it was too far up in the mountains and far from the sea to serve as the Spanish capital of Peru. Pizarro thus founded the city of Lima in Peru's central coast on 6 January 1535, a foundation that he considered as one of the most important things he had created in life.:227–229
After the final effort of the Inca to recover Cuzco had been defeated by Almagro, a dispute occurred between him and Pizarro respecting the limits of their jurisdiction; both claimed the city of Cuzco. The king of Spain had awarded the Governorate of New Toledo to Almagro and the Governorate of New Castile to Pizarro. The dispute had originated from a disagreement on how to interpret the limit between both governorates.:254-256 
This led to confrontations between the Pizarro brothers and Almagro, who was eventually defeated during the Battle of Las Salinas (1538) and executed. Almagro's son, also named Diego and known as "El Mozo", was later stripped of his lands and left bankrupt by Pizarro.
Atahualpa's wife, ten-year-old Cuxirimay Ocllo Yupanqui, was with Atahualpa's army in Cajamarca and had stayed with him while he was imprisoned. Following his execution she was taken to Cuzco and given the name Dona Angelina. By 1538 it was known she was Pizarro's mistress, having borne him two sons, Juan and Francisco.
Pizarro's death.
In Lima, Peru on 26 June 1541 "a group of twenty heavily armed supporters of Diego Almagro II stormed Pizarro's palace, assassinated him, and then forced the terrified city council to appoint young Almagro as the new governor of Peru", according to Burkholder and Johnson.
"Most of Pizarro's guests fled, but a few fought the intruders, numbered variously between seven and 25. While Pizarro struggled to buckle on his breastplate, his defenders, including his half-brother Martin de Alcántara, were killed.:143 For his part Pizarro killed two attackers and ran through a third. While trying to pull out his sword, he was stabbed in the throat, then fell to the floor where he was stabbed many times." Pizarro (who now was maybe as old as 70 years, and at least 62), collapsed on the floor, alone, painted a cross in his own blood and cried for Jesus Christ. He died moments after. Diego de Almagro the younger was caught and executed the following year after losing the battle of Chupas.
Pizarro's remains were briefly interred in the cathedral courtyard; at some later time his head and body were separated and buried in separate boxes underneath the floor of the cathedral. In 1892, in preparation for the anniversary of Columbus' discovery of the Americas, a body believed to be that of Pizarro was exhumed and put on display in a glass coffin. However, in 1977 men working on the cathedral's foundation discovered a lead box in a sealed niche, which bore the inscription "Here is the head of Don Francisco Pizarro Demarkes, Don Francisco Pizarro who discovered Peru and presented it to the crown of Castile." A team of forensic scientists from the United States, led by Dr. William Maples, was invited to examine the two bodies, and they soon determined that the body which had been honored in the glass case for nearly a century had been incorrectly identified. The skull within the lead box not only bore the marks of multiple sword blows, but the features bore a remarkable resemblance to portraits made of the man in life.
Legacy.
By his marriage to N de Trujillo, Pizarro had a son also named Francisco, who married his relative Inés Pizarro, without issue. After Pizarro's death, Inés Yupanqui, whom he took as a mistress, favourite sister of Atahualpa, who had been given to Francisco in marriage by her brother, married a Spanish cavalier named Ampuero and left for Spain, taking her daughter who would later be legitimized by imperial decree. Francisca Pizarro Yupanqui eventually married her uncle Hernando Pizarro in Spain, on 10 October 1537; a third son of Pizarro who was never legitimized, Francisco, by Dona Angelina, a wife of Atahualpa that he had taken as a mistress, died shortly after reaching Spain.
Historians have often compared Pizarro and Cortés' conquests in North and South America as very similar in style and career. Pizarro, however, faced the Incas with a smaller army and fewer resources than Cortés at a much greater distance from the Spanish Caribbean outposts that could easily support him, which has led some to rank Pizarro slightly ahead of Cortés in their battles for conquest. Based on sheer numbers alone, Pizarro's military victory was one of the most improbable in recorded history. For example, Pizarro had fewer soldiers than George Armstrong Custer did at the Battle of the Little Big Horn, while the Incas commanded forty times as many soldiers as Crazy Horse and Sitting Bull did.
Pizarro is well known in Peru for being the leader behind the Spanish conquest of the Inca Empire, and a growing number of Peruvians of strong indigenous descent which are the majority of Peru regard him negatively. By incorporating the natives into the society of Peru, Pizarro ruled Peru for almost a decade and initiated the decline of Inca culture. The Incas’ polytheistic religion was replaced by Christianity and both Quechua and Aymara — the main Inca languages — were reduced to a marginal role in society for centuries, while Spanish became the official language of Peru, Ecuador, Bolivia and Chile. The cities of the Inca Empire were transformed into Spanish, Catholic cities. Pizarro is also vilified for having ordered Atahualpa's death despite his paid ransom of filling a room with gold and two with silver which was later split among all his closest Spanish associates after a fifth share had been set aside for the king. Amongst other once Spanish nations in the Americas those who have a large creole or mestizo population with mostly European ancestry notably Mexico, El Salvador, Argentina, and Chile see Francisco Pizarro a hero much like Cortes because it was because of conquistadors like them that they have the religion, culture and prosperity they have today.
Sculptures.
In the early 1930s, sculptor Ramsey MacDonald created three copies of an anonymous European foot soldier resembling a conquistador with a helmet, wielding a sword and riding a horse. The first copy was offered to Mexico to represent Hernán Cortés, though it was rejected. Since the Spanish conquerors had the same appearance with helmet and beard, the statue was taken to Lima in 1934. One other copy of the statue resides in Wisconsin. The mounted statue of Pizarro in the Plaza Mayor in Trujillo, Spain was created by Charles Rumsey, an American sculptor. It was presented to the city by his widow in 1926.
In 2003, after years of lobbying by indigenous and mixed-raced majority requesting for the equestrian statue of Pizarro to be removed, the mayor of Lima, Luis Castañeda Lossio, approved the transfer of the statue to another location: an adjacent square to the country's Government Palace. Since 2004, however, Pizarro's statue has been placed in a rehabilitated park surrounded by the recently restored 17th century pre-Hispanic murals in the Rímac District. The statue faces the Rímac River and the Government Palace.
Palace of the conquest.
After their return from Peru and notoriously rich, the Pizarro family erected a plateresque-style palace on the corner of the Plaza Mayor in Trujillo, Spain. It was said to have been constructed on the orders of Pizarro's daughter, Francisca Pizarro Yupanqui. It instantly became a recognizable symbol of the plaza.
The opulent palace is structured in four stands, giving it the significance of the coat of arms of the Pizarro family, which is situated at one of its corner balconies displaying its iconographic content. At one of its sides it displays Francisco Pizarro and, at the other, his wife, the Inca princess Inés Huaylas, along with their daughter Francisca Pizarro Yupanqui and her husband Hernando Pizarro. The building's decor includes plateresque ornaments and balustrades.
References.
</dl>

</doc>
<doc id="55273" url="http://en.wikipedia.org/wiki?curid=55273" title="Manchukuo">
Manchukuo

 |style="width:1.0em; padding:0 0 0 0.6em;"| - 
 |style="padding-left:0;text-align:left;"| 1932–1934
 |- class="mergedrow"
 |style="width:1.0em; padding:0 0 0 0.6em;"| - ||style="padding-left:0;text-align:left;"|1934–1945|| 
 |  China
Manchukuo (traditional Chinese: 滿洲國simplified Chinese: 满洲国: "Mǎnzhōuguó"literally: "State of Manchuria") was a puppet state in Northeast China and Inner Mongolia, which was governed under a form of constitutional monarchy. The area, collectively known as Manchuria by westerners and Japanese, was designated by China's erstwhile Qing Dynasty as the "homeland" of the ruling family's ethnic group, the Manchus, but the Manchus themselves never used "Manchuria" (滿洲) as a place name to refer to the area. In 1931, the region was seized by Japan following the Mukden Incident and a pro-Japanese government was installed one year later with Puyi, the last Qing emperor, as the nominal regent and emperor. Manchukuo's government was abolished in 1945 after the defeat of Imperial Japan at the end of World War II. The territories formally claimed by the puppet state were first seized in the Soviet invasion of Manchuria in August 1945, and then formally transferred to Chinese administration in the following year.
Manchus formed a minority in Manchukuo, whose largest ethnic group were Han Chinese. The population of Koreans increased during the Manchukuo period, and there were also Japanese, Mongols, White Russians and other minorities. The Mongol regions of western Manchukuo were ruled under a slightly different system in acknowledgement of the Mongolian traditions there. The southern part of the Liaodong Peninsula was ruled by Japan as the Kwantung Leased Territory.
History.
Background.
"Manchuria" is a transcription of the Japanese reading of the Chinese word "滿洲" which means Manchu, which in Japanese is "Manshū", which in turn dates from the 19th century. The name "Manzhou" was invented and given to the Jurchen people by Hong Taiji in 1635 as a new name for their ethnic group, however, the name "Manchuria" was never used by the Manchus or the Qing dynasty itself to refer to their homeland. According to the Japanese scholar Junko Miyawaki-Okada, the Japanese geographer Takahashi Kageyasu was the first to use the term 满洲 (Manshū) as a place name in 1809 in the "Nippon Henkai Ryakuzu", and it was from that work where Westerners adopted the name. According to Mark C. Elliott, Katsuragawa Hoshū's 1794 work, the "Hokusa bunryaku", was where 满洲 (Manshū) first appeared as a place name, in two maps included in the work: "Ashia zenzu" and "Chikyū hankyū sōzu" which were also created by Katsuragawa. 满洲 (Manshū) then began to appear as a place names in more maps created by Japanese like Kondi Jūzō, Takahashi Kageyasu, Baba Sadayoshi and Yamada Ren. These maps were brought to Europe by the Dutch Philipp von Siebold. According to Nakami Tatsuo, Philip Franz von Siebold was the one who brought the usage of the term Manchuria to Europeans, after borrowing it from the Japanese, who were the first to use it in a geographic manner in the eighteenth century, while neither the Manchu nor Chinese languages had a term in their own language equivalent to "Manchuria" as a geographic place name. According to Bill Sewell, it was Europeans who first started using Manchuria as a name to refer to the location and it is "not a genuine geographic term." The historian Gavan McCormack agreed with Robert H. G. Lee's statement that "The term Manchuria or Man-chou is a modern creation used mainly by westerners and Japanese", with McCormack writing that the term Manchuria is imperialistic in nature and has no "precise meaning", since the Japanese deliberately promoted the use of "Manchuria" as a geographic name to promote its separation from China while they were setting up their puppet state of Manchukuo.
The Japanese had their own motive for deliberately spreading the usage of the term Manchuria. The historian Norman Smith wrote that "The term "Manchuria" is controversial". Professor Mariko Asano Tamanoi said that she "should use the term in quotation marks", when referring to Manchuria. Herbert Giles wrote that "Manchuria" was unknown to the Manchus themselves as a geographical expression. In his 2012 dissertation on the Jurchen people to obtain a Doctor of Philosophy degree in History from the University of Washington, Professor Chad D. Garcia noted that usage of the term "Manchuria" is out of favor in "currently scholarly practice" and he did away with using the term, using instead "the northeast" or referring to specific geographical features.
The Qing Dynasty, which replaced the Shun and Ming dynasties in China, was founded by Manchus from Manchuria (modern Northeastern China). The Manchu emperors separated their homeland in Jilin and Heilongjiang from the Han Liaoning province with the Willow Palisade. This ethnic division continued until the Qing dynasty encouraged massive immigration of Han in the 19th century during Chuang Guandong to prevent the Russians from seizing the area from the Qing. After conquering the Ming, the Qing identified their state as "China" (中國, Zhongguo; "Middle Kingdom") and referred to it as "Dulimbai Gurun" in Manchu. The Qing equated the lands of the Qing state (including present day Manchuria, Xinjiang, Mongolia, Tibet and other areas) as "China" in both the Chinese and Manchu languages, defining China as a multi-ethnic state, rejecting the idea that China only meant Han areas, proclaiming that both Han and non-Han peoples were part of "China", using "China" to refer to the Qing in official documents, international treaties, and foreign affairs, and the "Chinese language" (Dulimbai gurun i bithe) referred to Chinese, Manchu, and Mongol languages, and the term "Chinese people" (中國人 Zhongguo ren; Manchu: Dulimbai gurun i niyalma) referred to all Han, Manchus, and Mongol subjects of the Qing. The lands in Manchuria were explicitly stated by the Qing to belong to "China" (Zhongguo, Dulimbai gurun) in Qing edicts and in the Treaty of Nerchinsk.
During the Qing dynasty, the area of Manchuria was known as the "three eastern provinces" (三東省; "Sān dōng shěng") since 1683 when Jilin and Heilongjiang were separated even though it was not until 1907 that they were turned into actual provinces. The area of Manchuria was then converted into three provinces by the late Qing government in 1907. Since then, the "Three Northeast Provinces" () was officially used by the Qing government in China to refer to this region, and the post of Viceroy of Three Northeast Provinces was established to take charge of these provinces.
During its reign the Qing Dynasty became highly integrated with Chinese culture. The dynasty reached its height in the 18th century, during which both territory and population were increased. However, its military power weakened thereafter and, faced with massive rebellions and defeat in wars, the Qing Dynasty declined after the mid-19th century.
As the power of the court in Beijing weakened, many outlying areas either broke free (like Kashgar) or fell under the control of Imperialist powers. In the 19th century, Imperial Russia was most interested in the northern lands of the Qing Empire. In 1858, Russia gained control over a huge tract of land called Outer Manchuria thanks to the Supplementary Treaty of Beijing that ended the Second Opium War. But Russia was not satisfied and, as the Qing Dynasty continued to weaken, they made further efforts to take control of the rest of Manchuria. Inner Manchuria came under strong Russian influence in the 1890s with the building of the Chinese Eastern Railway through Harbin to Vladivostok.
Origins.
As a direct result of the Russo-Japanese War (1904/05) Japanese influence replaced Russia's in Inner Manchuria. In 1906, Japan laid the South Manchurian Railway to Port Arthur (Japanese: Ryojun). Between World War I and World War II Manchuria became a political and military battleground between Russia, Japan, and China. Japan moved into Outer Manchuria as a result of the chaos following the Russian Revolution of 1917. A combination of Soviet military successes and American economic pressure forced the Japanese to withdraw from the area, however, and Outer Manchuria returned to Soviet control by 1925.
During the warlord period in China, the warlord Zhang Zuolin established himself in Inner Manchuria with Japanese backing. Later, the Japanese Kwantung Army found him too independent, so he was assassinated in 1928.
After the Japanese invasion of Manchuria in 1931, Japanese militarists moved forward to separate the region from Chinese control and to create a Japanese-aligned puppet state. To create an air of legitimacy, the last Emperor of China, Puyi, was invited to come with his followers and act as the head of state for Manchuria. One of his faithful companions was Zheng Xiaoxu, a Qing reformist and loyalist.
On 18 February 1932 the "Manchu State" (Manchukuo, Pinyin: "Mǎnzhōuguó") was proclaimed and recognized by Japan on 15 September 1932 through the Japan-Manchukuo Protocol. The city of Changchun, renamed Hsinking (Pinyin: Xinjing) (新京, literally "New Capital"), became the capital of the new entity. Chinese in Manchuria organized volunteer armies to oppose the Japanese and the new state required a war lasting several years to appease the country.
The Japanese initially installed Puyi as Head of State in 1932, and two years later he was declared Emperor of Manchukuo with the era name of "Kangde" ("Tranquility and Virtue"; Wade-Giles: "Kangte"). Manchukuo thus became the Great Manchurian Empire, sometimes termed "Manchutikuo" (Pinyin: "Mǎnzhōu Dìguó"). Zheng Xiaoxu served as Manchukuo's first prime minister until 1935, when Zhang Jinghui succeeded him. Puyi was nothing more than a figurehead and real authority rested in the hands of the Japanese military officials. An imperial palace was particularly built for the emperor. All of the Manchu ministers served as front-men for their Japanese vice-ministers, who made all decisions.
In this manner, Japan formally detached Manchukuo from China in the course of the 1930s. With Japanese investment and rich natural resources, the area became an industrial powerhouse. Manchukuo had its own issued bank notes and postal stamps. Several independent banks were founded as well.
In 1935, Manchukuo bought the Chinese Eastern Railway from the Soviet Union.
Diplomatic recognition.
China did not recognize Manchukuo but the two sides established official ties for trade, communications and transportation. In 1933, the League of Nations adopted the Lytton Report, declaring that Manchuria remained rightfully part of China, leading Japan to resign its membership. The Manchukuo case persuaded the United States to articulate the so-called Stimson Doctrine, under which international recognition was withheld from changes in the international system created by force of arms.
In spite of the League of Nations' approach, the new state was diplomatically recognised by El Salvador (3 March 1934) and the Dominican Republic (1934), the Soviet Union (de facto 23 March 1935; de jure 13 April 1941), Italy (29 November 1937), Spain (2 December 1937), Germany (12 May 1938) and Hungary (9 January 1939).
It is commonly believed that the Holy See established diplomatic relations with Manchukuo in 1934, but the Holy See never did so. This belief is partly due to the erroneous reference in Bernardo Bertolucci's 1987 film "The Last Emperor" that the Holy See diplomatically recognised Manchukuo. Bishop Auguste Ernest Pierre Gaspais was appointed as "representative "ad tempus" of the Holy See and of the Catholic missions of Manchukuo to the government of Manchukuo" by the Congregation De Propaganda Fide (a purely religious body responsible for missions) and not by the Secretariat of State responsible for diplomatic relations with states.
After the outbreak of World War II, the state was recognised by Slovakia (1 June 1940), Vichy France (12 July 1940), Romania (1 December 1940), Bulgaria (10 May 1941), Finland (18 July 1941), Denmark (August 1941), Croatia (2 August 1941)—all controlled or influenced by Japan's ally Germany — as well as by the China's Wang Jingwei government (30 November 1940), Thailand (5 August 1941) and the Philippines (1943) — all under Japanese control.
World War II and aftermath.
Before World War II, the Japanese colonized Manchukuo and used it as a base from which to invade China. In the summer of 1939 a border dispute between Manchukuo and the Mongolian People's Republic resulted in the Battle of Khalkhin Gol. During this battle, a combined Soviet-Mongolian force defeated the Japanese Kwantung Army ("Kantōgun") supported by limited Manchukuoan forces.
On 8 August 1945, the Soviet Union declared war on Japan, in accordance with the agreement at the Yalta Conference, and invaded Manchukuo from outer Manchuria and Outer Mongolia. This was called Manchurian Strategic Offensive Operation. During the Soviet offensive the Army of Manchukuo, on paper a 200,000-man force, performed poorly and whole units surrendered to the Soviets without firing a single shot; there were even cases of armed riots and mutinies against the Japanese forces. Emperor Kangde (known by reign title Xuantong during the Qing Dynasty; his childhood name was Puyi) had hoped to escape to Japan to surrender to the Americans, but the Soviets captured him and eventually extradited him to the communist government in China, where the authorities had him imprisoned as a war criminal along with all other captured Manchukuo officials.
From 1945 to 1948, Manchuria (Inner Manchuria) served as a base area for the People's Liberation Army in the Chinese Civil War against the National Revolutionary Army. The Chinese Communists used Manchuria as a staging ground until the final Nationalist retreat to Taiwan in 1949. Many Manchukuo army and Japanese Kantogun personnel served with the communist troops during the Chinese Civil War against the Nationalist forces. Most of the 1.5 million Japanese who had been left in Manchukuo at the end of World War II were sent back to their homeland in 1946-1948 by U.S. Navy ships in the operation now known as the Japanese repatriation from Huludao.
Politics.
Historians generally consider Manchukuo a puppet state of Imperial Japan because of the Japanese military's strong presence and strict control of the government administration. Chinese historians generally refer to the state as 'Wei Manzhouguo' ('false state of Manchuria'). Some historians see Manchukuo as an effort at building a glorified Japanese state in mainland Asia that deteriorated due to the pressures of war.
The independence of Manchuria was proclaimed on 18 February 1932, and renamed to Manchukuo. The Japanese military commander appointed Puyi as regent (reign name Datong) for the time being, stating that he would become Emperor of Manchukuo but could not reign using the title of Emperor of the Great Qing Empire as he once held. Manchukuo was proclaimed a monarchy on 1 March 1934, with Puyi assuming the throne under the reign name of Emperor Kang-de. Puyi was assisted in his executive duties by a Privy Council (Chinese: 參議府), and a General Affairs State Council (Chinese: 國務院). This State Council was the center of political power, and consisted of several cabinet ministers, each assisted by a Japanese vice-minister.
The commanding officer of the Kwantung Army in Manchukuo was additionally the Japanese ambassador to Manchukuo. He functioned in a manner similar to that of a British resident officer in British overseas protectorates, with the power to veto decisions by the emperor. The Kwangtung Army leadership placed Japanese vice ministers in his cabinet, while all Chinese advisors gradually resigned or were dismissed.
The Legislative Council (Chinese: 立法院) was largely a ceremonial body, existing to rubber-stamp decisions issued by the State Council. The only authorized political party was the government-sponsored Concordia Association, although various émigré groups were permitted their own political associations.
When the Japanese surrender was announced on 15 August 1945, Puyi was "asked" to abdicate, which he did.
Administrative divisions.
During its short-lived existence, Manchukuo was divided into between five (in 1932) and 19 (in 1941) provinces, one special ward of Peiman (Chinese: 北滿特別區) and two Special cities which were Hsinking (Chinese: 新京特別市) and Harbin (Chinese: 哈爾濱特別市). Each province was divided into between four (Hsingan-tung) and 24 (Fengtien) prefectures. Peiman lasted less than 3 years (1 July 1933 – 1 January 1936) and Harbin was later incorporated into Binkiang province. Lungkiang also existed as a province in the 1932 before being divided into Heiho, Lungkiang and Sankiang in 1934. Antung and Chinchow provinces separated themselves from Fengtien while Binkiang and Chientao from Kirin separated themselves in the same year.
Demographics.
In 1908, the number of residents was 15,834,000, which rose to 30,000,000 in 1931 and 43,000,000 for the Manchukuo state. The population balance remained 123 men to 100 women and the total number in 1941 was 50,000,000. Other statistics indicate that in Manchukuo the population rose by 18,000,000.
In early 1934, the total population of Manchukuo was estimated as 30,880,000, with 6.1 persons the average family, and 122 men for each 100 women. These numbers included 29,510,000 Chinese (96%, which should have included the Manchurian population), 590,760 Japanese (2%), 680,000 Koreans (2%), and 98,431 (<1%) of other nationality: White Russians, Mongols, etc. Around 80% of the population was rural. During the existence of Manchukuo, the ethnic balance did not change significantly, except that Japan increased the Korean population in China. The majority of Han Chinese in Manchukuo believed that Manchuria was rightfully part of China, who both passively and violently resisted Japan's propaganda that Manchukuo was a "multinational state".
From Japanese sources come these numbers: in 1940 the total population in Manchukuo of Lungkiang, Jehol, Kirin, Liaoning (Fengtien) and Hsingan provinces at 43,233,954; or an Interior Ministry figure of 31,008,600. Another figure of the period estimated the total population as 36,933,000 residents.
Around the same time the Soviet Union was advocating the Siberian Jewish Autonomous Oblast across the Manchukuo-Soviet border, some Japanese officials investigated a plan (known as the Fugu Plan) to attract Jewish refugees to Manchukuo as part of their colonisation efforts which was never adopted as official policy.
The Japanese Ueda Kyōsuke labelled all 30 million people in Manchuria as "Manchus", including Han Chinese, despite the fact that most of them were not ethnic Manchu, and the Japanese written "Great Manchukuo" built upon Ueda's argument to claim that all 30 million "Manchus" in Manchukuo had the right to independence to justify splitting Manchukuo from China. In 1942 the Japanese written "Ten Year History of the Construction of Manchukuo" attempted to emphasize the right of ethnic Japanese to the land of Manchukuo while attempting to delegitimize the Manchu's claim to Manchukuo as their native land, noting that most Manchus moved out during the Qing period and only returned later.
Japanese population.
In 1931–2, there were 100,000 Japanese farmers; other sources mention 590,760 Japanese inhabitants. Other figures for Manchukuo speak of a Japanese population 240,000 strong, later growing to 837,000. In Hsinking, they made up 25% of the population. The Japanese government had official plans projecting the emigration of 5 million Japanese to Manchukuo between 1936 and 1956. Between 1938 and 1942 a batch of young farmers of 200,000 arrived in Manchukuo; joining this group after 1936 were 20,000 complete families. When Japan lost sea and air control of the Yellow Sea, this migration stopped.
When the Red Army invaded Manchukuo, they captured 850,000 Japanese settlers. With the exception of some civil servants and soldiers, these were repatriated to Japan in 1946–7. Many Japanese orphans in China were left behind in the confusion by the Japanese government and were adopted by Chinese families. Many, however, integrated well into Chinese society. In the 1980s Japan began to organise a repatriation programme for them but not all chose to go back to Japan.
Economy.
Manchukuo experienced rapid economic growth and progress in its social systems. Its industrial system was among the most advanced making it one of the industrial powerhouses in the region. Manchukuo's steel production exceeded Japan's in the late 1930s. Many Manchurian cities were modernised during Manchukuo era. However, much of the country's economy was often subordinated to Japanese interests and, during the war, raw material flowed into Japan to support the war effort. Traditional lands were taken and redistributed to Japanese farmers with local farmers relocated and forced into collective farming units over smaller areas of land.
See also:
Transport.
The Japanese built an efficient and impressive railway system that still functions well today. Known as the South Manchuria Railway Company or "Mantetsu", this large corporation came to own large stakes in many industrial projects throughout the region. "Mantetsu" personnel were active in the pacification of occupied China during World War II.
Military.
Manchukuo Imperial Army.
The Manchukuo Imperial Army was the armed force of Manchukuo.
Manchukuo Imperial Guards.
The Manchukuo Imperial Guards was the elite unit of the Manchukuo armed forces created in 1933. It was charged with the protection of the Kangde Emperor Puyi, and senior members of the Manchukuo civil government. Its headquarters was in the capital of Hsinking, near the Imperial Palace in the centre of the city.
Manchukuo Imperial Navy.
The Manchukuo Imperial Navy ("Manshu Teikoku Kaigun") was the navy of Manchukuo. As Manchukuo was a largely land-locked state, the leadership of the Japanese Kwantung Army regarded the development of a navy to have a very low military priority, although it was politically desirable to create at least a nominal force as a symbol of the legitimacy of the new regime.
Manchukuo Imperial Air Force.
The Manchukuo Imperial Air Force ("Dai Manshū Teikoku Kūgun") was established in February 1937, initially with 30 men selected from the Manchukuo Imperial Army and trained at the Japanese Kwantung Army aircraft arsenal in Harbin. The official air force's predecessor was the Manchukuo Air Transport Company (later renamed the Manchukuo National Airways) a paramilitary airline formed in 1931, which undertook transport and reconnaissance missions for the Japanese military.
War crimes in Manchukuo.
According to a joint study by historians Zhifen Ju, Mitsuyochi Himeta, Toru Kubo and Mark Peattie, more than 10 million Chinese civilians were mobilized by the Kwangtung Army for slave labor in Manchukuo under the supervision of the Kōa-in.
The Chinese slave laborers often suffered illness due to high-intensity manual labor. Some badly ill workers were directly pushed into mass graves in order to avoid the medical expenditure and the world's most serious mine disaster, at Benxihu Colliery, happened in Manchukuo.
Bacteriological weapons were experimented on humans by the infamous unit 731 located near Harbin in Beinyinhe from 1932 to 1936 and to Pingfan until 1945. Victims, mostly Chinese, Russians and Koreans, were subjected to vivisection, sometimes without anesthesia.
Drug trafficking.
In 2007, an article by Reiji Yoshida in the "Japan Times" argued that Japanese investments in Manchukuo were partly financed by selling drugs. According to the article, a document found by Yoshida shows that the Kōa-in was directly implicated in providing funds to drug dealers in China for the benefit of the puppet government of Manchukuo, Nanjing and Mongolia. This document corroborates evidence analyzed earlier by the Tokyo tribunal which stated that
Japan's real purpose in engaging in drug traffic was far more sinister than even the debauchery of Chinese people. Japan, having signed and ratified the opium conventions, was bound not to engage in drug traffic, but she found in the alleged but false independence of Manchukuo a convenient opportunity to carry on a worldwide drug traffic and cast the guilt upon that puppet state... In 1937, it was pointed out in the League of Nations that 90% of all illicit white drugs in the world were of Japanese origin...
Education.
Manchukuo developed an efficient public education system. The government established many schools and technical colleges, 12,000 primary schools in Manchukuo, 200 middle schools, 140 normal schools (for preparing teachers), and 50 technical and professional schools. In total the system had 600,000 children and young pupils and 25,000 teachers. Local Chinese children and Japanese children usually attended different schools, and the ones who did attend the same school were segregated by ethnicity, with the Japanese students assigned to better-equipped classes.
Confucius's teachings also played an important role in Manchukuo's public school education. In rural areas, students were trained to practice modern agricultural techniques to improve production. Education focused on practical work training for boys and domestic work for girls, all based on obedience to the "Kingly Way" and stressing loyalty to the Emperor. The regime used numerous festivals, sport events, and ceremonies to foster loyalty of citizens. Eventually, Japanese became the official language in addition to the Chinese language taught in Manchukuo schools.
Culture.
Film.
The Photograpic Division, part of the public relations section of the South Manchurian Railway was created in 1928 to produce short documentary films about Manchuria to Japanese audiences. In 1937, the Manchukuo Film Association was established by the government and the South Manchurian Railway in a studio in Jilin province. It was founded by Masahiko Amakasu, who also helped the career of Yoshiko Otaka, also known as Ri Koran. He also tried to ensure that Manchukuo would have its own industry and would be catering mainly to Manchurian audiences. The films for the most part usually promote pro-Manchukuo and pro-Japanese views. After World War II, the archives and the equipment of the association were used by the Changchun Film Studio of the People's Republic of China.
Dress.
The Changshan and the Qipao, both derived from traditional Manchu dress, were considered national dresses in Manchukuo.
In a meeting with the Concordia Association, the organizers devised what was termed Concordia Costume, or the kyowafuku, in 1936. Even Japanese like Masahiko Amakasu and Kanji Ishiwara adopted it. It was gray and a civilianized version of Imperial Japanese Army uniform. It was similar to the National Clothes "(kokumin-fuku)" worn by Japanese civilians in World War II as well as the Zhongshan suit. A pin of either a Manchukuo flag or a five-pointed, five colored star with the Manchukuo national colors were worn on the collars.
Court dress resembled those of Meiji-era Japan at that time.
Sport.
The Manchukuo National Physical Education Association was established in 1932 to promote sport.
Manchukuo also had a national football team, and football was considered the country's de facto national sport; the Football Association of Manchukuo was formed around it.
Manchukuo hosted and participated in baseball matches with Japanese teams. Some of the games of the Intercity Baseball Tournament were held in the country, and played with local teams.
Manchukuo originally was to join the 1932 Summer Olympic Games, but one of the athletes who were intended to represent Manchukuo, Liu Changchun, refused to join the team and instead joined as the first Chinese representative in the Olympics. There were attempts by Japanese authorities to let Manchukuo join the 1936 games, but the Olympic Committee persisted in the policy of not allowing an unrecognized state to join the Olympics. Manchukuo had a chance to participate in the planned 1940 Tokyo Olympics, but the outset of World War II killed the idea permanently.
National symbols.
Aside from the national flag, the orchid, reportedly Puyi's favorite flower, became the royal flower of the country, similar to the chrysanthemum in Japan. The sorghum flower also became a national flower by decree in April 1933. "Five Races Under One Union (Manchukuo)" was used as a national motto.
Stamps and postal history.
Manchukuo issued postage stamps from 28 July 1932 until its dissolution following the final defeat of the Japanese Empire in August 1945. The last issue of Manchukuo was on 2 May 1945.
In popular culture.
In Masaki Kobayashi's "The Human Condition (film series)" (1959) Kaji, the main protagonist, is a labor supervisor assigned to a workforce consisting of Chinese prisoners in a large mining operation in Japanese-colonized Manchuria.
Bernardo Bertolucci's 1987 film, "The Last Emperor", presented a portrait of Manchukuo through the memories of Emperor Puyi, during his days as a political prisoner in the People's Republic of China.
Haruki Murakami's 1995 novel "The Wind-Up Bird Chronicle" deals greatly with Manchukuo through the character of Lieutenant Mamiya. Mamiya recalls, in person and in correspondence, his time as an officer in the Kwantung Army in Manchukuo. While the period covered in these recollections extends over many years, the focus is on the final year of the war and the Soviet invasion of Manchuria.
The 2008 South Korean western "The Good, the Bad, the Weird" is set in the desert wilderness of 1930s Manchuria.

</doc>
<doc id="55275" url="http://en.wikipedia.org/wiki?curid=55275" title="Denotational semantics">
Denotational semantics

In computer science, denotational semantics (initially known as mathematical semantics or Scott–Strachey semantics) is an approach of formalizing the meanings of programming languages by constructing mathematical objects (called "denotations") that describe the meanings of expressions from the languages. Other approaches to providing formal semantics of programming languages include axiomatic semantics and operational semantics.
Broadly speaking, denotational semantics is concerned with finding mathematical objects called domains that represent what programs do. For example, programs (or program phrases) might be represented by partial functions or by games between the environment and the system.
An important tenet of denotational semantics is that "semantics should be compositional": the denotation of a program phrase should be built out of the denotations of its subphrases.
Historical development.
Denotational semantics originated in the work of Christopher Strachey and Dana Scott in the late 1960s. As originally developed by Strachey and Scott, denotational semantics provided the denotation (meaning) of a computer program as a function that mapped input into output. To give denotations to recursively defined programs, Scott proposed working with continuous functions between domains, specifically complete partial orders. As described below, work has continued in investigating appropriate denotational semantics for aspects of programming languages such as sequentiality, concurrency, non-determinism and local state.
Denotational semantics have been developed for modern programming languages that use capabilities like concurrency and exceptions, e.g., Concurrent ML, CSP, and Haskell. The semantics of these languages is compositional in that the denotation of a phrase depends on the denotations of its subphrases. For example, the meaning of the applicative expression f(E1,E2) is defined in terms of semantics of its subphrases f, E1 and E2. In a modern programming language, E1 and E2 can be evaluated concurrently and the execution of one of them might affect the other by interacting through shared objects causing their denotations to be defined in terms of each other. Also, E1 or E2 might throw an exception which could terminate the execution of the other one. The sections below describe special cases of the semantics of these modern programming languages.
Denotations of recursive programs.
Denotational semantics are given to a program phrase as a function from an environment (that has the values of its free variables) to its denotation. For example, the phrase n*m produces a denotation when provided with an environment that has binding for its two free variables: n and m. If in the environment n has the value 3 and m has the value 5, then the denotation is 15.
A function can be modeled as denoting a set of ordered pairs where each ordered pair in the set consists of two parts (1) an argument for the function and (2) the value of the function for that argument. For example the set of order pairs {[0 1] [4 3]} is the denotation of a function with value 1 for argument 0, value 3 for the argument 4, and is otherwise undefined.
The problem to be solved is to provide denotations for recursive programs that are defined in terms of themselves such as the definition of the factorial function as
A solution is to build up the denotation by approximation starting with the empty set of ordered pairs (which in set theory would be written as {}). If {} is plugged into the above definition of factorial then the denotation is {[0 1]}, which is a better approximation of factorial. Iterating: If {[0 1]} is plugged into the definition then the denotation is {[0 1] [1 1]}. So it is convenient to think of an approximation to factorial as an input F in the following way:
It is instructive to think of a chain of "iterates" where "Fi" indicates "i"-many applications of "F".
The least upper bound of this chain is the full factorial function which can be expressed as follows where the symbol "⊔" means "least upper bound":
Denotational semantics of non-deterministic programs.
The concept of power domains has been developed to give a denotational semantics to non-deterministic sequential programs. Writing "P" for a power domain constructor, the domain "P"("D") is the domain of non-deterministic computations of type denoted by "D".
There are difficulties with fairness and unboundedness in domain-theoretic models of non-determinism. See Power domains for nondeterminism.
Denotational semantics of concurrency.
Many researchers have argued that the domain theoretic models given above do not suffice for the more general case of concurrent computation. For this reason various new models have been introduced. In the early 1980s, people began using the style of denotational semantics to give semantics for concurrent languages. Examples include Will Clinger's work with the actor model; Glynn Winskel's work with event structures and petri nets; and the work by Francez, Hoare, Lehmann, and de Roever (1979) on trace semantics for CSP. All these lines of inquiry remain under investigation (see e.g. the various denotational models for CSP).
Recently, Winskel and others have proposed the category of profunctors as a domain theory for concurrency.
Denotational semantics of state.
State (such as a heap) and simple imperative features can be straightforwardly modeled in the denotational semantics described above. All the textbooks below have the details. The key idea is to consider a command as a partial function on some domain of states. The denotation of "x:=3" is then the function that takes a state to the state with 3 assigned to x. The sequencing operator ";" is denoted by composition of functions. Fixed-point constructions are then used to give a semantics to looping constructs, such as "while".
Things become more difficult in modelling programs with local variables. One approach is to no longer work with domains, but instead to interpret types as functors from some category of worlds to a category of domains. Programs are then denoted by natural continuous functions between these functors.
Denotations of data types.
Many programming languages allow users to define recursive data types. For example, the type of lists of numbers can be specified by
This section deals only with functional data structures that cannot change. Conventional imperative programming languages would typically allow the elements of such a recursive list to be changed.
For another example: the type of denotations of the untyped lambda calculus is
The problem of "solving domain equations" is concerned with finding domains that model these kinds of datatypes. One approach, roughly speaking, is to consider the collection of all domains as a domain itself, and then solve the recursive definition there. The textbooks below give more details.
Polymorphic data types are data types that are defined with a parameter. For example, the type of α lists is defined by
Lists of natural numbers, then, are of type nat list, while lists of strings are of string list.
Some researchers have developed domain theoretic models of polymorphism. Other researchers have also modeled parametric polymorphism within constructive set theories. Details are found in the textbooks listed below.
A recent research area has involved denotational semantics for object and class based programming languages.
Denotational semantics for programs of restricted complexity.
Following the development of programming languages based on linear logic, denotational semantics have been given to languages for linear usage (see e.g. proof nets, coherence spaces) and also polynomial time complexity.
Denotational semantics of sequentiality.
The problem of full abstraction for the sequential programming language PCF was, for a long time, a big open question in denotational semantics. The difficulty with PCF is that it is a very sequential language. For example, there is no way to define the parallel-or function in PCF. It is for this reason that the approach using domains, as introduced above, yields a denotational semantics that is not fully abstract.
This open question was mostly resolved in the 1990s with the development of game semantics and also with techniques involving logical relations. For more details, see the page on PCF.
Denotational semantics as source-to-source translation.
It is often useful to translate one programming language into another. For example, a concurrent programming language might be translated into a process calculus; a high-level programming language might be translated into byte-code. (Indeed, conventional denotational semantics can be seen as the interpretation of programming languages into the internal language of the category of domains.)
In this context, notions from denotational semantics, such as full abstraction, help to satisfy security concerns.
Abstraction.
It is often considered important to connect denotational semantics with operational semantics. This is especially important when the denotational semantics is rather mathematical and abstract, and the operational semantics is more concrete or closer to the computational intuitions. The following properties of a denotational semantics are often of interest.
Additional desirable properties we may wish to hold between operational and denotational semantics are:
Compositionality.
An important aspect of denotational semantics of programming languages is compositionality, by which the denotation of a program is constructed from denotations of its parts. For example consider the expression "7 + 4". Compositionality in this case is to provide a meaning for "7 + 4" in terms of the meanings of "7", "4" and "+".
A basic denotational semantics in domain theory is compositional because it is given as follows. We start by considering program fragments, i.e. programs with free variables. A "typing context" assigns a type to each free variable. For instance, in the expression ("x" + "y") might be considered in a typing context ("x":nat,"y":nat). We now give a denotational semantics to program fragments, using the following scheme.
Now, the meaning of the compound expression (7+4) is determined by composing the three functions 〚⊢7:nat〛:1→ℕ⊥, 〚⊢4:nat〛:1→ℕ⊥, and 〚"x":nat,"y":nat⊢"x"+"y":nat〛:ℕ⊥×ℕ⊥→ℕ⊥.
In fact, this is a general scheme for compositional denotational semantics. There is nothing specific about domains and continuous functions here. One can work with a different category instead. For example, in game semantics, the category of games has games as objects and strategies as morphisms: we can interpret types as games, and programs as strategies. For a simple language without general recursion, we can make do with the category of sets and functions. For a language with side-effects, we can work in the Kleisli category for a monad. For a language with state, we can work in a functor category. Milner has advocated modelling location and interaction by working in a category with interfaces as objects and "bigraphs" as morphisms.
Semantics versus implementation.
According to Dana Scott [1980]:
According to Clinger (1981):
Connections to other areas of computer science.
Some work in denotational semantics has interpreted types as domains in the sense of domain theory, which can be seen as a branch of model theory, leading to connections with type theory and category theory. Within computer science, there are connections with abstract interpretation, program verification, and model checking.

</doc>
<doc id="55278" url="http://en.wikipedia.org/wiki?curid=55278" title="Warp drive">
Warp drive

Warp drive is a hypothetical faster-than-light (FTL) propulsion system in many works, most notably "Star Trek". A spacecraft equipped with a warp drive may travel at apparent speeds greater than that of light by many orders of magnitude. In contrast to other hypothetical FTL technologies such as a jump drive or hyper drive, the warp drive does not permit instantaneous (or near instantaneous) travel between two points but involves a measurable passage of time. Spacecraft at warp velocity can continue to interact with objects in "normal space". Some of the other fiction in which warp drive technology is featured include: "Stars!", "EVE Online", "Earth and Beyond", "StarCraft", "DarkSpace", "Starship Troopers", "Doctor Who", and "Star Ocean".
"The Original Series": Establishing a background.
Warp drive is one of the fundamental features of the "Star Trek" franchise; in the first pilot episode of "", "The Cage", it is referred to as a "hyperdrive"/"time warp" drive combination, and it is stated that the "time barrier" has been broken, allowing a group of stranded interstellar travelers to return to Earth far sooner than would have otherwise been possible. The time barrier shouldn't be confused with time dilation which occurs when approaching very fast speeds. Warp drive technology avoids time dilation.
The episode "Metamorphosis", also from "The Original Series", establishes a backstory for the invention of warp drive on Earth, in which Zefram Cochrane discovered the 'space warp'. Cochrane is repeatedly referred to afterwards, but the exact details of the first warp trials were not shown until the second ' movie, '. The movie depicts Cochrane as having first operated warp drive on Earth in 2063 (two years after the date speculated by the first edition of the Star Trek Chronology). By using a matter/antimatter reactor to create plasma, and by sending this plasma through warp coils, he created a warp bubble which he could use to move a craft into subspace, thus allowing it to exceed the speed of light. This successful first trial led directly to first contact with the Vulcans.
"Enterprise": Leading up to The Original Series.
Later on, a prequel series titled " describes the warp engine technology as a 'Gravimetric Field Displacement Manifold' (Commander Tucker's tour, "), and describes the device as being powered by a matter/anti-matter reaction which powers the two separate nacelles (one on each side of the ship) to create a displacement field (the aforementioned "bubble").
The episode also firmly establishes that many other civilizations had warp drive before humans; "First Contact" co-writer Ronald D. Moore suggested Cochrane's drive was in some way superior to forms which existed beforehand, and was gradually adopted by the galaxy at large.
"Enterprise", set in 2151 and onwards, follows the voyages of the first human ship capable of traveling at warp factor 5.2, which under the old warp table formula, is about 140 times the speed of light. In the episode "", Capt. Archer equates warp 4.5 to "Neptune and back [from Earth] in six minutes", though previous "Star Trek" series had established that use of warp drive within a solar system presents extreme danger to both the ship and the surrounding planets.
"The Next Generation" onwards.
Plots involving the "Enterprise" traveling beyond warp 10 were once in the original series (such as warp 14.1 in "That Which Survives"), but for "The Next Generation" it was decided that these would no longer be featured. A new warp scale was drawn up, with warp factor 10 set as an unattainable maximum. This is described in some technical manuals as 'Eugene's limit', in homage to creator/producer Gene Roddenberry. Normal maximum warp in the original series was warp 8.
The limit of 10 did not entirely stop warp inflation. By the mid-24th century, the "Enterprise"-D could travel at warp 9.8 at "extreme risk", while normal maximum operating speed was warp 9.6 and maximum rated cruise was warp 9.2. According to the Deep Space Nine Tech Manual, during the Dominion War, Galaxy class starships were refitted with newer technology including modifications which increased their maximum speed to warp 9.9.
In the episode "Where No One Has Gone Before ..." the "Enterprise"-D was shown to exceed Warp 10, traveling 2.7 million light-years from their home galaxy in a matter of minutes (though the ship's extreme velocity was due to the influence of an alien being and could not be achieved naturally). The "Intrepid"-class starship "Voyager" has a maximum sustainable cruising speed of warp 9.975, the "Enterprise"-E can go even faster at Warp 9.99. In the alternative future depicted in "" (the final episode of the Star Trek:TNG), the 'future' "Enterprise"-D travels at warp 13, although it is never established whether this is 'above' warp ten, or simply the result of another reconfiguration of the warp scale.
Warp velocities.
Warp drive velocity in "Star Trek" is generally expressed in "warp factor" units, which—according to the "Star Trek Technical Manuals"—correspond to the magnitude of the warp field. Achieving warp factor 1 is equal to breaking the light barrier, while the actual velocity corresponding to higher factors is determined using an ambiguous formula. Several episodes of the original series placed the "Enterprise" in peril by having it travel at high warp factors; at one point in "That Which Survives" the "Enterprise" traveled at a warp factor of 14.1. In the "" episode "The Most Toys" the crew of "Enterprise"-D discovers that the android Data may have been stolen while on board another ship, "Jovis". At this point the "Jovis", which has a maximum warp factor of 3 has had a 23-hour head start, which the Enterprise-D figures puts her anywhere within a 0.102 light year radius of her last known position. However, the velocity (in present dimensional units) of any given warp factor is rarely the subject of explicit expression, and travel times for specific interstellar distances are not consistent through the various series.
According to the "Star Trek" episode writer's guide for "The Original Series", warp factors are converted to multiples of c with the cubic function , where "w" is the warp factor, "v" is the velocity, and "c" is the speed of light. Accordingly, "warp 1" is equivalent to the speed of light, "warp 2" is 8 times the speed of light, "warp 3" is 27 times the speed of light, etc.
For "" and the subsequent series, "Star Trek" artist Michael Okuda devised a formula based on the original one but with important differences; for warp factors 1 through 9, . In the half-open interval from warp 9 to warp 10, the exponent of "w" increases toward infinity. Thus, in the Okuda scale, warp velocities approach warp 10 asymptotically.
There is no exact formula for this interval because the quoted velocities are based on a hand-drawn curve; what can be said is that at velocities greater than warp 9, the form of the warp function changes because of an increase in the exponent of the warp factor "w". Due to the resultant increase in the derivative, even minor changes in the warp factor eventually correspond to a greater than exponential change in velocity. In the episode ", Tom Paris breaks the warp 10 threshold.
Exact velocities were given in a few episodes, one being ", where Kathryn Janeway describes "Voyager"'s velocity at warp factor 9.975. "Voyager" was about 70,000 light-years away from home, and crew would often use "75 years" as the time it would take to get back home at top speed. This means the "Voyager" series used the old method of Warp calculation. 70,000/9.9753 is roughly 70.5 years. If delays for refuelling, repair, restocking, and downtime are considered, 75 years is a logical rounding. However, in "", Tom Paris achieves warp 10, which is infinite velocity.
Slingshot effect.
A curious extension of warp travel, shown throughout "Star Trek", is the "slingshot effect".
First discovered accidentally in "Tomorrow Is Yesterday" (1967), one of the earlier episodes of the original "Star Trek" series, as a method of time travel. While the actual procedure is intentionally obscure, it involved traveling at a high warp velocity (depicted in "" is more than warp 9.8) in the direction of a star, on a precisely calculated "slingshot" path; if successful, the ship is caused to travel to a desired point, past or future. The same technique was used later in the episode "" (1968) for historic research — in this episode, the warp factor required for "time warp" is given the name "light speed breakaway factor". The term "time warp" was first used in "The Naked Time" (1966) when a previously untried cold-start intermix of matter and antimatter threw the Enterprise back three days in time. The term was later used in "Star Trek IV" in describing the slingshot effect. The technique was mentioned as a viable method of time travel in the TNG episode "Time Squared" (1989).
This 'slingshot' effect has been explored in theoretical physics: it is hypothetically possible to slingshot oneself 'around' the event horizon of a black hole. The result of such a maneuver would cause time to pass at a slower rate for the ship near the event horizon relative to the rest of the outside universe. Such a journey would be a trip into the future — the craft would have merely 'fast forwarded'. It is not possible to travel into the past with this method.
Fans of the show and films have noted the slingshot involves a star, rather than a black hole, and the most normal consensus from its use concerns the nature of warp travel and warp velocities.
Warp core.
A primary component of the warp drive method of propulsion in the "Star Trek" universe is the "gravimetric field displacement manifold", more commonly referred to as a "warp core". It is a fictional reactor that taps the energy released in a matter-antimatter annihilation to provide the energy necessary to power a starship's warp drive, allowing faster-than-light travel. Starship warp cores generally also serve as powerplants for other primary ship systems.
When matter and antimatter come into contact, they "annihilate" — both matter and antimatter are converted directly and entirely into enormous quantities of energy, in the form of subnuclear particles and electromagnetic radiation (specifically, mesons and gamma rays). In the "Star Trek" universe, fictional "dilithium crystals" are used to regulate this reaction. These crystals are described as being non-reactive to anti-matter when bombarded with high levels of radiation.
Usually, the reactants are deuterium, which is an isotope of hydrogen, and antideuterium (its antimatter counterpart). In "The Original Series" and in-universe chronologically subsequent series, the warp core reaction chamber is often referred to as the "dilithium intermix chamber" or the "matter/antimatter reaction chamber", depending upon the ship's intermix type. The reaction chamber is surrounded by powerful magnetic fields to contain the anti-matter. If the containment fields ever fail, the subsequent interaction of the antimatter fuel with the container walls would result in a catastrophic release of energy, with the resultant explosion capable of utterly destroying the ship. Such "warp core breaches" are used as plot devices in many Star Trek episodes. An intentional warp core breach can also be deliberately created, as one of the methods by which a starship can be made to self-destruct.
The mechanisms that provide a starship's propulsive force are the 'warp nacelles', one (or more) cylindrical pods that are offset from the hull of the ship by large pylons; the nacelles generate the actual 'warp bubble' that surrounds the ship, and destruction of one or both nacelles will cripple the ship, and possibly cause a warp-core breach.
Real-world theories and science.
A theoretical solution for faster-than-light travel which models the warp drive concept, called the Alcubierre drive, was formulated by physicist Miguel Alcubierre in 1994. Subsequent calculations found that such a model would require negative mass, the existence of which has never been supported by any evidence, and prohibitive amounts of energy.
However, it has recently been found that by changing the shape of the warp drive, much less negative mass and energy could be used, though the energy required is still many orders of magnitude greater than anything currently possible by modern technology. NASA engineers have begun preliminary research into such technology.
Notes.
</dl>

</doc>
<doc id="55279" url="http://en.wikipedia.org/wiki?curid=55279" title="James Dewar">
James Dewar

Sir James Dewar FRS (20 September 1842 – 27 March 1923) was a Scottish chemist and physicist. He is probably best-known today for his invention of the Dewar flask, which he used in conjunction with extensive research into the liquefaction of gases. He was also particularly interested in atomic and molecular spectroscopy, working in these fields for more than 25 years.
Early life.
James Dewar was born in Kincardine-on-Forth in 1842, the youngest of six boys. He lost his parents at the age of 15. He was educated at Dollar Academy and the University of Edinburgh, where he studied under Lord Playfair, and later became Lord Playfair's assistant. Dewar would also study under August Kekulé at Ghent.
Career.
In 1875, Dewar was elected Jacksonian professor of natural experimental philosophy at the University of Cambridge, becoming a fellow of Peterhouse. He became a member of the Royal Institution and later, replaced Dr. John Hall Gladstone in the role of Fullerian Professor of Chemistry in 1877. Dewar was also the President of the Chemical Society in 1897 and the British Association for the Advancement of Science in 1902, as well as serving on the Royal Commission established to examine London's water supply from 1893 to 1894 and the Committee on Explosives. It was whilst he was serving on the Committee on Explosives that he and Frederick Augustus Abel developed cordite, a smokeless gunpowder alternative.
In 1867 Dewar described several chemical formulas for benzene. Ironically, one of the formulae, which does not represent benzene correctly and was not advocated by Dewar, is sometimes still called Dewar benzene.
His scientific work covers a wide field – his earlier papers cover a wide range of topics; organic chemistry, Hydrogen and its physical constants, high temperature research, the temperature of the sun and of the electric spark, electro-photometry and the chemistry of the electric arc.
With Professor J. G. McKendrick, of Glasgow, he investigated the physiological action of light, and examined the changes which take place in the electrical condition of the retina under its influence. With Professor G. D. Liveing, one of his colleagues at Cambridge, he began in 1878 a long series of spectroscopic observations, the later of which were devoted to the spectroscopic examination of various gaseous elements separated from atmospheric air by the aid of low temperatures; and he was joined by Professor J. A. Fleming, of University College London, in the investigation of the electrical behaviour of substances cooled to very low temperatures.
His name is most widely known in connection with his work on the liquefaction of the so-called permanent gases and his researches at temperatures approaching absolute zero. His interest in this branch of physics and chemistry dates back at least as far as 1874, when he discussed the "Latent Heat of Liquid Gases" before the British Association. In 1878 he devoted a Friday evening lecture at the Royal Institution to the then recent work of Louis Paul Cailletet and Raoul Pictet, and exhibited for the first time in Great Britain the working of the Cailletet apparatus. Six years later, again at the Royal Institution, he described the researches of Zygmunt Florenty Wróblewski and Karol Olszewski, and illustrated for the first time in public the liquefaction of oxygen and air. Soon afterwards he built a machine from which the liquefied gas could be drawn off through a valve for use as a cooling agent, before using the liquid oxygen in research work related to meteorites; about the same time he also obtained oxygen in the solid state.
By 1891 he had designed and built, at the Royal Institution, machinery which yielded liquid oxygen in industrial quantities, and towards the end of that year he showed that both liquid oxygen and liquid ozone are strongly attracted by a magnet. About 1892 the idea occurred to him of using vacuum-jacketed vessels for the storage of liquid gases – the Dewar flask (otherwise known as a Thermos or vacuum flask) – the invention for which he became most famous. The vacuum flask was so efficient at keeping heat out that it was found possible to preserve the liquids for comparatively long periods, making examination of their optical properties possible. Dewar did not profit from the widespread adoption of his vacuum flask – he lost a court case against Thermos concerning the patent for his invention. While Dewar was recognised as the inventor, because he did not patent his invention there was no way to stop Thermos from using the design.
He next experimented with a high pressure hydrogen jet by which low temperatures were realised through the Joule–Thomson effect, and the successful results he obtained led him to build at the Royal Institution a large regenerative cooling refrigerating machine. Using this machine in 1898, liquid hydrogen was collected for the first time, solid hydrogen following in 1899. He tried to liquefy the last remaining gas, helium, which condenses into a liquid at −268.9 °C, but owing to a number of factors, including a lack of helium with which to work, Dewar was preceded by Heike Kamerlingh Onnes as the first person to produce liquid helium, in 1908. Onnes would later be awarded the Nobel Prize in Physics for his research into the properties of matter at low temperatures – Dewar was nominated several times but never successful in winning the Nobel Prize.
In 1905 he began to investigate the gas-absorbing powers of charcoal when cooled to low temperatures, and applied his research to the production of high vacuums, which were useful for further experiments in atomic physics. Dewar would continue his research work into the properties of elements at low temperatures, specifically low-temperature calorimetry, until the outbreak of World War I. The Royal Institution laboratories lost a number of staff to the war effort, both in fighting and scientific roles, and after the war, Dewar had little interest in restarting the serious research work which went on before the War. Shortages of scholars necessarily compounded the problems. His research during and after the war mainly involved investigating surface tension in soap bubbles, rather than further work into the properties of matter at low temperatures.
Royal Institution Christmas Lectures.
Dewar was invited to deliver several Royal Institution Christmas Lectures:<br>
Honours and awards.
Whilst Dewar was never recognised by the Swedish Academy, he was recognised by many other institutions both before and after his death, both in Britain and overseas. The Royal Society elected him a Fellow of the Royal Society in June 1877 and bestowed their Rumford (1894), Davy (1909), and Copley Medal (1916) medals upon him for his work, as well as inviting him to deliver their Bakerian Lecture in 1901. In 1899 he became the first recipient of the Hodgkins gold medal of the Smithsonian Institution, Washington, D.C., for his contributions to our knowledge of the nature and properties of atmospheric air.
In 1904 he was the first British subject to receive the Lavoisier Medal of the French Academy of Sciences, and in 1906 he was the first to be awarded the Matteucci Medal of the Italian Society of Sciences. He was knighted in 1904 and awarded the Gunning Victoria Jubilee Prize for 1900–1904 by the Royal Society of Edinburgh, and in 1908 he was awarded the Albert medal of The Society of Arts. A lunar crater was named in his honour.
James Dewar died in London in 1923, still holding the office of Fullerian Professor of Chemistry at the Royal Institution, having refused to retire. He was cremated at the Golders Green Crematorium where his ashes remain. He was survived by his wife, Lady Helen Rose Dewar (née Banks).

</doc>
<doc id="55280" url="http://en.wikipedia.org/wiki?curid=55280" title="Grand Prix">
Grand Prix

Grand Prix (], meaning "Grand Prize"; plural Grands Prix) may refer to:

</doc>
<doc id="55283" url="http://en.wikipedia.org/wiki?curid=55283" title="Canning Stock Route">
Canning Stock Route

The Canning Stock Route is a track that runs from Halls Creek in the Kimberley region of Western Australia to Wiluna in the mid-west region. With a total distance of around 1,850 km (1,150 mi) it is the longest historic stock route in the world.
The stock route was proposed as a way of breaking a monopoly that west Kimberley cattlemen had on the beef trade at the beginning of the 20th century. In 1906, the Government of Western Australia appointed Alfred Canning to survey the route. When the survey party returned to Perth, Canning's treatment of Aboriginal guides came under scrutiny leading to a Royal Commission. Canning had been organising Aboriginal hunts, and when caught, they were chained by the neck and fed salt to get them to show the explorer where the waterholes were. Women were forced to have sex. Despite condemning Canning's methods, the Royal Commission, after the Lord Mayor of Perth, Alexander Forrest had appeared as a witness for Canning, exonerated Canning and his men of all charges. The cook who made the complaints was dismissed and Canning was sent back to finish the job.
Canning was appointed to lead a construction party and between March 1908 and April 1910, 48 wells were completed along the route. Commercial droving began in 1910, but the stock route did not prove popular and was rarely used for the next twenty years. The wells made it difficult for Aboriginal people to access water and in reprisal they vandalised or dismantled many of the wells.
A 1928 Royal Commission into the price of beef in Western Australia led to the repair of the wells and the re-opening of the stock route. Around 20 droves took place between 1931 and 1959 when the final droving run was completed.
The Canning Stock Route is now a popular but challenging four-wheel drive adventure. On rare occasions, people have traversed the track on foot, by bicycle, and in two-wheel drive vehicles.
The building of the stock route impacted on the cultural and social life of the more than 15 Aboriginal language groups and today the Aboriginal history of the track, recorded through oral and artistic traditions, is increasingly being recognised.
History.
In Western Australia at the beginning of the 20th century, east Kimberley cattlemen were looking for a way to traverse the western deserts of Australia with their cattle as a way to break a west Kimberley monopoly that controlled the supply of beef to Perth and the goldfields in the south of the state. East Kimberley cattle were infested with Boophilus ticks infected with a malaria-like parasitic disease called Babesiosis and were prohibited from being transported to southern markets by sea due to a fear that the ticks would survive the journey and spread. This gave west Kimberley cattlemen a monopoly on the beef trade and resulted in high prices.
With east Kimberley cattlemen keen to find a way to get their cattle to market, and the Government of Western Australia keen for competition to bring prices down, a 1905 proposal of a stock route through the desert was taken seriously. James Isdell, an east Kimberley pastoralist and member of the Western Australian Legislative Assembly, proposed the stock route arguing that ticks would not survive in the dry desert climate on the trip south.
Surveying the route.
Calvert and Carnegie expeditions.
The route, which crossed the territories of nine different Aborigine language groups, had been explored previously in 1896 by the Calvert Expedition led by Lawrence Wells and again later that year by the Carnegie Expedition led by David Carnegie. Two members of the Calvert Expedition perished of thirst and the Carnegie Expedition suffered considerable hardships with camels dying after eating poisonous grass and a member of the party accidentally shooting himself dead. Carnegie investigated the possibility of a stock route and concluded that the route was "too barren and destitute of vegetation" and was impractical.
Wells and Carnegie both mistreated Aborigines they encountered on their expeditions, forcing them to cooperate by tying them up and encouraging them to find water. Carnegie is also believed to have fed them salt, and he was later publicly criticised for this. Evidence supports that Alfred Canning had read both the Calvert and Carnegie expedition accounts to find out about the country (which both had described as extremely difficult terrain) and the use of Aboriginal people to find water, an example Canning followed during his own expedition.
Canning survey.
After it was determined that ticks could not survive a desert crossing, the government endorsed James Isdell's scheme and funded a survey to find a stock route that would cross the Great Sandy Desert, the Little Sandy Desert and the Gibson Desert. Alfred Canning, a surveyor with the Western Australian Department of Lands and Surveys, was appointed to survey the stock route.
Canning's task was to find a route through 1850 kilometres of desert, from Wiluna in the mid west to the Kimberley in the north. He needed to find significant water sources – enough for up to 800 head of cattle, a day's walk apart – where wells could be dug, and enough good grazing land to sustain this number of cattle during the journey south.
In 1906, with a team of 23 camels, two horses, and eight men, Canning surveyed the route completing the difficult journey from Wiluna to Halls Creek in less than six months. On 1 November 1906, shortly after arriving in Halls Creek, Canning sent a telegram to Perth stating that the finished route would "be about the best watered stock route in [the] Colony". Canning was forced to delay his return journey because of an early wet season in the Kimberley that year. The survey party left Halls Creek in late January 1907 and arrived back in Wiluna in early July 1907. During the 14-month expedition, they had trekked about 4000 km, relying on Aboriginal guides to help them find water.
Canning had always planned to rely on Aboriginal guides to help him find water and had taken neck chains and handcuffs supplied to him by the Wiluna police to make sure local 'guides' stayed as long as he needed them. In order to gain assistance in locating water along the route, Canning captured several Martu men, chained them by the neck and forced them to lead his party to native water sources (soaks). As many soaks were sacred, the Martu may have misdirected the explorers away from these, resulting in the eventual stock route winding more than was actually necessary.
Royal Commission into treatment of Aboriginal people.
After the Canning survey party returned to Perth, Canning's use of Aboriginal guides came under scrutiny. The expedition's cook, Edward Blake, accused Canning of mistreating many of the Aboriginal people they met during the survey expedition. Blake objected to the use of chains and criticised the "party's 'immoral' pursuit of Aboriginal women, the theft and 'unfair' trade of Aboriginal property and the destruction of native waters". Blake was concerned that the planned wells would prevent Aboriginal people accessing water.
Blake's complaints led to a Royal Commission into the Treatment of Natives by the Canning Exploration Party.
Blake was unable to prove many of his claims, but Canning did admit to the use of chains. Kimberley Explorer and the first Premier of Western Australia, John Forrest, dismissed Canning's actions by claiming that all explorers behaved in this manner. Despite condemning the use of chains, the Royal Commission accepted the survey party's actions as "reasonable" and Canning and his men were exonerated of all charges, including "immorality with native women" and stealing property. The Royal Commission approved the immediate commencement of the stock route's construction. Canning was appointed to lead the construction party.
Construction.
Canning left Perth in March 1908, along with 30 men, 70 camels, four wagons, 100 tonnes of food and equipment and 267 goats (for milk and meat), and travelled the route again to commence the construction of well heads and water troughs at the 54 water sources identified by his earlier expedition. He arrived back in Wiluna in April 1910 having completed the last of 48 wells and bringing the total cost of the route to £22000 (2010: A$2.6 million).
Thirty-seven of the wells were built on or near existing Aboriginal waters and were constructed in the European tradition, which made many of them inaccessible to Aboriginal people. Pulling the heavy buckets up from the bottom of the wells required the strength of three men or use of a camel. Consequentially, many Aboriginal people were injured or died while trying to access the water, either falling in and drowning or breaking bones on the windlass handle. In reprisal, buckets were cut off or timber set on fire, and by 1917 Aboriginal people had vandalised or dismantled approximately half of the wells in a bid to reclaim access to the water or to prevent drovers from using the wells. Canning's party had constructed the wells with the forced help of one of the Aboriginal peoples whose land the route traversed, the Martu.
Canning produced a detailed map of the stock route, Plan of Wiluna–Kimberley stock route exploration (showing positions of wells constructed 1908–9 and 10) on which he also recorded his observations of the land and water sources along the route. The map has become a symbol of Australia's pioneering history.
Using the stock route.
First droving runs.
Commercial droving along the stock route began in 1910. The first few droves were of small groups of horses — the first started out with 42 horses of which only nine survived the journey.
The first mob of bullocks to attempt to use the stock route set out in January 1911; however the party of three drovers, George Shoesmith, James Thompson and an Aboriginal stockman who was known as 'Chinaman', were killed by Aborigines at Well 37. Thomas Cole discovered their bodies later in 1911 during his successful drove along the stock route. In September 1911, Sergeant R.H. Pilmer led a police 'punitive expedition' to find the culprits and ensure the stock route remained open. The police made no arrests, but the expedition was considered a success after Pilmer acknowledged killing at least 10 Aborigines.
On 7 September 1911 it was reported that the first mob of cattle to traverse the entire length of the stock route had successfully arrived in Wiluna. The cattle had apparently gained condition on the long drove.
Despite police protection, drovers were afraid to use the track and it was rarely used for almost 20 years. Between 1911 and 1931, only eight mobs of cattle were driven along the Canning Stock Route.
Reopening of the stock route.
A 1928 Royal Commission into the price of beef in Western Australia led to the re-opening of the stock route. In 1929, William Snell was commissioned to repair the wells and found that the only wells undamaged were the ones that Aboriginal people could use. Snell criticised the construction of Canning's wells because they were difficult for Aboriginal people to use safely, and he put the destruction of the wells down to the anger and frustration people felt at being unable to access traditional water sources. Snell personally committed to making the wells more accessible to Aboriginal people:Natives cannot draw water from the Canning Stock Route wells. It takes three strong white men to land a bucket of water. It is beyond the natives power to land a bucket. They let go the handle [and] some times escape with their life but get an arm and head broken in the attempt to get away. To heal the wounds so severely inflicted and [as] a safeguard against the natives destroying the wells again I equipped the wells ... so that the native can draw water from the wells without destroying them.—William Snell
Snell started work on the refurbishment of the wells, fitting some with ladders for easier access, but he abandoned the work after well 35. Reports vary that he either ran out of materials or the desert became too much for him.
In 1930, Alfred Canning (then aged 70) was commissioned to complete the work. While Snell had encountered no hostility, Canning had trouble with the Aborigines from the start but successfully completed the commission in 1931.
With these improvements, the route was used on a more regular basis although in total, it would only be used around 20 times between 1931 and 1959 when the last droving run was completed. None of the larger station owners used the track as it was found that only 600 head of cattle could be supported at a time, which was 200 less than was estimated when first completed. As Carnegie had accurately reported in 1896, the track was impractical for cattle drives.
During the Second World War the track was upgraded at considerable expense in case it was needed for an evacuation of the north if Australia was invaded. Including horse drives there have been only 37 recorded drives between 1910 and the last run in 1959.
Traverses.
In the 1950s horses became scarce in the Kimberley as widespread losses were caused by "Walkabout Poison". This led to the stock route being used to drove horses north from around the Norseman area where they were sold to the stations. Wally Dowling, a drover who had made nine droves along the stock route took what was probably the last group of horses northwards along the route in September 1951.
In 1968 the entire length of the track was driven for the first time.
In 1972, before the route was regularly negotiated in four-wheel drives, ambitious attempts to complete it on foot took place. A New Zealander, Murray Rankin, and two English brothers, John and Peter Waterfall, fashioned homemade trolleys from bicycle wheels and metal tubing, and began their attempt starting from Wiluna in early June 1972. First John and then Peter turned back, but Rankin continued to Lake Disappointment before being forced to abandon the attempt. The remains of one of their trolleys lie 19 km north of Well 15.
In 1973 Rankin tried again, this time starting from Old Halls Creek with Englishman John Foulsham. This time they had professionally built trolleys with motor-cycle wheels. The walk began on 1 June. Soon after reaching Godfrey's Tank they were unable to pull the trolleys over the high sand hills. They left them and walked on to Lake Tobin and there abandoned the attempt and returned to Halls Creek.
Three years later,in 1976, Rankin achieved his ambition to walk the stock route. After driving the route in a Land Rover and establishing food depots along the way, he set out from Halls Creek on 12 July 1976 with three other bushwalkers, Ralph Barraclough, Kathy Borman and Rex Shaw. Barraclough turned back after becoming ill, but the others completed the journey in just under three months. 
In 1977, the first commercial tour completed the drive.
During the 1980s fuel dumps were created and adventurous travellers became interested in the history of the track and the challenge to drive it.
In 1985, a Beach Buggy and a Citroën 2CV became the first two-wheel drive vehicles to complete the entire route.
In 1994 long distance walker Drew Kettle walked the route.
In 1997 Robin Rishworth cycled, with the aid of food drops, in just less than 27 days, and is considered to be the first modern day solo cyclist.
In 2004 Kate Leeming, as part of a longer trek, completed the route with the aid of a support vehicle.
In 2005 Jakub Postrzygacz became the first person to traverse the entire track without either support or the use of food drops, travelling alone by bicycle for 33 days. With large tyres and a single-wheel trailer, he carried all his food with him and replenished his water at wells.
Present.
Tourism.
The Canning Stock route is considered one of the world's great four-wheel drive adventures. Apart from keeping the track open, the route is not maintained. Some wells have been restored but others are in ruins and unusable. While quite a few travellers successfully make the trip, it still requires substantial planning and a convoy of well-equipped four-wheel drives or equivalent vehicles, and is only practical during the cooler months. Fuel drops typically need to be organised in advance and the 1850 km trip will take two to three weeks. Fuel is now available at Kunawaritji Aboriginal community near well 33.
Aboriginal perspective.
The history of the Canning Stock Route has been well documented from the colonial perspective – accounts of European explorers, drovers, prospectors and law enforcers – but increasingly the Aboriginal history of the track is also being recognised, and Aboriginal people are keen to have their story told:We wanna tell you fellas 'bout things been happening in the past that hasn't been recorded, what old people had in their head. No pencil and paper. The white man history has been told and it's today in the book. But our history is not there properly. We've got to tell 'em through our paintings. — Clifford Brooks, Wiluna, 2006
Archaeologists now believe that the Western Desert has been occupied for around 30,000 years. For Aboriginal people, the history of the stock route is therefore part of a much older story. They have recorded this story, including the changes brought about by the construction of the stock route, through oral and artistic traditions.
The building of the stock route impacted on the cultural and social life of the more than 15 Aboriginal language groups that have a "cultural, familial or historical connection to the route and its custodians, or to sites along the major Dreaming tracks or songlines". Some Dreaming tracks exist within the Country of a single language group, but others cross the territory of many groups and the major Dreaming tracks often mark the territorial boundaries of the Countries they cross. The stock route, and the people and stock it brought with it, inevitably interrupted traditional patterns of movement and connection to Country.
While many Aboriginal people made a determined effort to avoid contact with the people the stock route brought into their Country, the route became a path out of the desert for others. At different times, and for different reasons, people moved away to the outskirts of towns, to pastoral stations and church missions. Many found work with the drovers using the stock route and successful droves relied on the skill of these Aboriginal stockmen and women. Others left looking for more reliable sources of food and water, especially in times of drought, while some were drawn to the changes taking place around the edges of the desert or motivated by a desire to join family already living elsewhere.
Rock art project.
There are a large number of Aboriginal rock paintings and carvings along the stock route. As more and more people visit the area each year, custodians of the Western Desert have become concerned about the protection and management of Aboriginal sites along the route. In 2007, researchers from the Australian National University began a project to draw up the first comprehensive plan of management for the entire Canning Stock Route. The project aimed to develop a series of modules to inform detailed guides and signs for visitors, while also protecting sites that have special significance for Indigenous peoples.
The Canning Stock Route Project.
The Canning Stock Route has a strong connection to the story of Aboriginal art in the Western Desert. When droving along the stock route led to many family groups dispersing to the edges of the desert, communities were established in missions, towns, stations and settlements, and it was here that contemporary painting movements flourished.
In 2006, West Australian independent cultural organisation FORM instigated a contemporary arts and cultural initiative to "explore the complex history of the Canning Stock Route through the prism of contemporary Aboriginal art". Partnerships among nine art centres and communities with direct connections to the stock route region were set up. The project involved several years of research by FORM in collaboration with Aboriginal artists and their art centres and organisations.
A major part of the project's program of bush work was a six-week, 1850-kilometre desert journey from Wiluna to Billiluna. During this trip, and in follow-up workshops and other trips, 80 artists created a collection of paintings, contemporary cultural objects and documentary material.
The historical and artistic value of the project was recognised in 2008 when the National Museum of Australia decided to acquire the entire Canning Stock Route Project collection.
The Canning Stock Route Collection.
The National Museum of Australia acquired a significant collection of artworks and other material collected by the 60 artists who travelled along the Canning Stock Route on a six-week return to country trip in 2007 as part of the Canning Stock Route Project. The Canning Stock Route collection includes over 100 works of art, 120 oral histories, historical research, social and cultural data, artists' biographies, 20,000 photographs and over 200 hours of film footage.
One of the key aims of the Canning Stock Route Project was the development of a travelling exhibition. The National Museum of Australia committed to assisting FORM to develop an exhibition. "Yiwarra Kuju (One Road) – The Canning Stock Route", a joint initiative between the National Museum of Australia and FORM, was held at the museum from July 2010 to January 2011. The exhibition used works of art and stories to tell the story of the stock route's impact on Aboriginal people from an Aboriginal perspective. When it closed in January 2011, "Yiwarra Kuju – The Canning Stock Route" had been the most successful exhibition in the history of the Museum, with over 120,000 visitors.
Journey distances.
The nearest capital city to the Wiluna starting point of the route is Perth, 958 km south west of Wiluna by road. Then to return to Perth via sealed roads from Halls Creek it's 2857 km Including the Canning route this gives a total driving distance of 5665 km.

</doc>
<doc id="55284" url="http://en.wikipedia.org/wiki?curid=55284" title="Coat of arms">
Coat of arms

 Escutcheon 
 Field 
 Supporter 
 Supporter 
 Motto (alternative) 
 Crest 
 Torse 
 Mantling 
 Helm 
 Coronet 
 Compartment 
 Order 
 Ordinaries 
 Charges 
 Motto 
 Dexter 
 Sinister 
Conventional elements of an achievement
A coat of arms is a unique heraldic design on an escutcheon (i.e. shield), surcoat, or tabard. A surcoat, and subsequently a "coat of arms" was used by medieval knights to cover, protect, and identify the wearer. Thus these are sometimes called "coat armory". The coat of arms on an escutcheon forms the central element of the full heraldic achievement which consists of shield, supporters, crest, and motto. The design is a symbol unique to an individual person or family (except in the UK), corporation, or state. Such displays are commonly called "armorial bearings", "armorial devices", "heraldic devices", or simply "arms".
Sometimes the term "coat of arms" is used to refer to the full achievement, but this usage is wrong in a strict sense of heraldic terminology.
The ancient Romans used similar insignia on their shields, but these identified military units rather than individuals. The first evidence of medieval coats of arms is found in the Bayeux Tapestry from the 11th Century, where some of the combatants carry shields painted with crosses. Coats of arms came into general use by feudal lords and knights in battle in the 12th Century. By the 13th Century arms had spread beyond their initial battlefield use to become a kind of flag or logo for families in the higher social classes of Europe, inherited from one generation to the next. Exactly who had a right to use arms, by law or social convention, varied to some degree between countries. In the German-speaking region both the aristocracy and "burghers" (non-noble free citizens) used arms, while in most of the rest of Europe they were limited to the aristocracy. The use of arms spread to Church clergy, and to towns as civic identifiers, and to royally-chartered organizations such as universities and trading companies. Flags developed from coats of arms, and the arts of vexillology and heraldry are closely related. The coats of arms granted to commercial companies are a major source of the modern logo.
Despite no widespread regulation, and even with a lack in many cases of national regulation, heraldry has remained rather consistent across Europe, where traditions alone have governed the design and use of arms. Unlike seals and other general emblems, heraldic achievements have a formal description called a blazon, expressed in a jargon that allows for consistency in heraldic depictions.
In the 21st century, coats of arms are still in use by a variety of institutions and individuals; for example, many European cities and universities have guidelines on how their coats of arms may be used, and protect their use as trademarks. Many societies exist that also aid in the design and registration of personal arms. Some nations, like England and Scotland, still maintain the same heraldic authorities which have traditionally granted and regulated arms for centuries and continue to do so in the present day.
Traditions and usage.
In the heraldic traditions of England and Scotland an individual, rather than a family, had a coat of arms. In those traditions coats of arms are legal property transmitted from father to son; wives and daughters could also bear arms modified to indicate their relation to the current holder of the arms. Undifferenced arms are used only by one person at any given time. Other descendants of the original bearer could bear the ancestral arms only with some difference: usually a color change or the addition of a distinguishing charge. One such charge is the label, which in British usage (outside the Royal Family) is now always the mark of an heir apparent or (in Scotland) an heir presumptive.
Because of their importance in identification, particularly in seals on legal documents, the use of arms was strictly regulated; few countries continue in this today. This has been carried out by heralds and the study of coats of arms is therefore called "heraldry". Some other traditions (e.g., Polish heraldry) are less restrictive — allowing, for example, all members of a dynastic house or family to use the same arms, although one or more elements may be reserved to the head of the house.
In time, the use of arms spread from military entities to educational institutes, and other establishments. According to a design institute article, "The modern logo and corporate livery have evolved from the battle standard and military uniform of medieval times".
In his book, "The Visual Culture of Violence in the Late Middle Ages", Valentin Groebner argues that the images composed on coats of arms are in many cases designed to convey a feeling of power and strength, often in military terms. The author Helen Stuart argues that some coats of arms were a form of corporate logo. Museums on medieval armory also point out that as emblems they may be viewed as precursors to the corporate logos of modern society, used for group identity formation.
When knights were so encased in armour that no means of identifying them was left, the practice was introduced of painting their insignia of honour on their shield as an easy method of distinguishing them. Originally these were granted only to individuals, but were afterward made hereditary in England by King Richard I, during his crusade to the Holy Land (Israel/Palestine).
European tradition.
French heraldry.
The French system of heraldry greatly influenced the British and Western European systems. Much of the terminology and classifications are taken from it. However, with the fall of the French monarchy (and later Empire) there is not currently a "Fons Honorem" (power to dispense and control honors) to strictly enforce heraldic law. The French Republics that followed have either merely affirmed pre-existing titles and honors or vigorously opposed noble privilege. Coats of arms are considered an intellectual property of a family or municipal body. Assumed arms (arms invented and used by the holder rather than granted by an authority) are considered valid unless they can be proved in court to copy that of an earlier holder.
British heraldry.
In Scotland, the Lord Lyon King of Arms has criminal jurisdiction to enforce the laws of arms. In England, Northern Ireland and Wales the use of arms is a matter of civil law and regulated by the College of Arms and the Court of Chivalry.
In reference to a dispute over the exercise of authority over the Officers of Arms in England, Arthur Annesley, 1st Earl of Anglesey, Lord Privy Seal, declared on 16 June 1673 that the powers of the Earl Marshal were "to order, judge, and determine all matters touching arms, ensigns of nobility, honour, and chivalry; to make laws, ordinances, and statutes for the good government of the Officers of Arms; to nominate Officers to fill vacancies in the College of Arms; to punish and correct Officers of Arms for misbehaviour in the execution of their places". It was further declared that no patents of arms or any ensigns of nobility should be granted and no augmentation, alteration, or addition should be made to arms without the consent of the Earl Marshal.
Irish heraldry.
In Ireland the usage and granting of coats of arms was strictly regulated by the Ulster King of Arms from the office's creation in 1552. After Irish independence in 1922 the office was still functioning and working out of Dublin Castle. The last Ulster King of Arms was Sir Nevile Rodwell Wilkinson [Ulster King of Arms 1908-1940], who held it until his death in 1940. At the Irish government's request, no new King of Arms was appointed. Thomas Ulick Sadleir, the Deputy Ulster King of Arms, then became the Acting Ulster King of Arms. He served until the office was merged with that of Norroy King of Arms in 1943 and stayed on until 1944 to clear up the backlog. 
An earlier Ireland King of Arms was created by King Richard II in 1392 and discontinued by King Henry VII in 1487. It didn't grant many coats of arms - the few it did grant were annulled by the other Kings of Arms because they encroached upon their jurisdictions. Its purpose was supposedly to marshal an expedition to fully conquer Ireland that never materialized. 
Since 1 April 1943 the authority has been split between the Republic of Ireland and Northern Ireland. Heraldry in the Republic of Ireland is regulated by the Government of Ireland, by the Genealogical Office through the Office of the Chief Herald of Ireland. Heraldry in Northern Ireland is regulated by the Government of Great Britain by the College of Arms through the Norroy and Ulster King of Arms.
German and Scandinavian heraldry.
The heraldic tradition and style of modern and historic Germany and the Holy Roman Empire — including national and civic arms, noble and burgher arms, ecclesiastical heraldry, heraldic displays and heraldic descriptions — stand in contrast to Gallo-British, Latin and Eastern heraldry, and strongly influenced the styles and customs of heraldry in the Nordic countries, which developed comparatively late.
In the Nordic countries, provinces, regions, cities and municipalities have a coat of arms. These are posted at the borders and on buildings containing official offices, as well as used in official documents and on the uniforms of municipal officers. Arms may also be used on souvenirs or other effects, given that an application has been granted by the municipal council.
Other European countries.
At a national level, "coats of arms" were generally retained by European states with constitutional continuity of more than a few centuries, including constitutional monarchies like Denmark as well as old republics like San Marino and Switzerland.
In Italy the use of coats of arms was only loosely regulated by the states existing before the unification of 1860. Since the Consulta Araldica, the college of arms of the Kingdom of Italy, was abolished in 1948, personal coats of arms and titles of nobility, though not outlawed, are not recognised.
Among the states ruled by communist regimes, emblems resembling the Soviet design were adopted in all the Warsaw Pact states except Czechoslovakia and Poland.
Since 1989, some of the ex-Communist states, as Romania, have resumed their former arms, often with only the symbols of monarchy removed.
Asia and Africa.
In Islam.
With the formation of the modern nation states of the Arab World in the second half of the 20th century, European traditions of heraldry were partially adopted for state emblems.
These emblems often involve the star and crescent symbol taken from the Ottoman flag.
Another commonly seen symbol is the eagle, which is a symbol attributed to Saladin, and the hawk of the Qureish.
Japanese Mon.
Japanese emblems, called "kamon" (often abbreviated "mon"), are family badges which often date back to the 7th century, and are used in Japan today. The Japanese tradition is independent of the European, and thus very different in style; but as in Europe many abstract and floral elements are used.
Yet, even these simple designs often express an origin. An example in recent use is the logo of Mitsubishi corporation which started as a shipping and maritime enterprise and whose emblem is based on a water chestnut derived from its maritime history with a military naval influence. The word "mitsu" means the number 3 and the word "hishi" meaning "water chestnut" (pronounced "bishi" in some combinations; see rendaku) originated from the emblem of the warrior Tosa Clan. The battleships of the Tosa Clan had been used in the late 19th century in the First Sino-Japanese War to reach Korea and their name was given to a modern battleship. The Tosa water chestnut leaf mon was then drawn as a rhombus or diamond shape in the Mitsubishi logo.
New World practices.
Canada.
The Queen of Canada has delegated her prerogative to grant armorial bearings to the Governor General of Canada. Canada has its own Chief Herald and Herald Chancellor. The Canadian Heraldic Authority is situated at Rideau Hall.
United States.
The Great Seal of the United States uses on the obverse as its central motif an heraldic achievement described as being the arms of the nation. The seal, and the armorial bearings, were adopted by the Continental Congress on 20 June 1782, and is a shield divided palewise into thirteen pieces, with a blue chief, which is displayed upon the breast of an American bald eagle. The crest is thirteen stars breaking through a glory and clouds, displayed with no helm, torse, or mantling (unlike most European precedents).
Only a few of the American states have adopted a coat of arms, which is usually designed as part of the respective state's seal. Vermont has both a state seal and a state coat of arms that are independent of one another (though both contain a pine tree, a cow and sheaves of grain); the seal is used to authenticate documents, whilst the heraldic device represents the state itself.
Ecclesiastic practice.
Catholic Church.
Vatican City State and the Holy See each have their own coat of arms. As the papacy is not hereditary, its occupants display their personal arms combined with those of their office.
Some popes came from armigerous (noble) families; others adopted coats of arms during their career in the Church. The latter typically allude to their ideal of life, or to specific pontifical programmes. A well-known and widely displayed example in recent times was Pope John Paul II's arms. His selection of a large letter M (for the Virgin Mary) was intended to express the message of his strong Marian devotion.
Roman Catholic dioceses are also each assigned a coat of arms, as are basilicas or papal churches, the latter usually displaying these on the building. These may be used in countries which otherwise do not use heraldic devices. 
In countries like Scotland with a strong statutory heraldic authority, arms will need to be officially granted and recorded.
Flags and banners.
Flags are used to identify ships (where they are called ensigns), embassies and such, and they use the same colors and designs found in heraldry, but they are not usually considered to be heraldic. A country may have both a national flag and a national coat of arms, and the two may not look alike at all. For example, the flag of Scotland (St Andrew's Cross) has a white saltire on a blue field, but the royal arms of Scotland has a red lion within a double tressure on a gold (or) field.

</doc>
<doc id="55285" url="http://en.wikipedia.org/wiki?curid=55285" title="Ernst Mach">
Ernst Mach

Ernst Waldfried Josef Wenzel Mach (; ]; February 18, 1838 – February 19, 1916) was an Austrian physicist and philosopher, noted for his contributions to physics such as the Mach number and the study of shock waves. As a philosopher of science, he was a major influence on logical positivism, American pragmatism and through his criticism of Newton, a forerunner of Einstein's relativity.
Biography.
Ernst Waldfried Josef Wenzel Mach was born in Brno-Chrlice (German: "Chirlitz"), Moravia (then in the Austrian empire, now part of Brno in the Czech Republic). His father, who had graduated from Charles University in Prague, acted as tutor to the noble Brethon family in Zlín, eastern Moravia. His grandfather, Wenzl Lanhaus, an administrator of the estate Chirlitz, was also master builder of the streets there. His activities in that field later influenced the theoretical work of Ernst Mach. Some sources give Mach's birthplace as Turas/Tuřany (now also part of Brno), the site of the Chirlitz registry-office. Peregrin Weiss baptized Ernst Mach into the Roman Catholic Church in Turas/Tuřany. Despite his Catholic background, he later became an atheist and his theory and life is compared with Buddhism.
Up to the age of 14, Mach received his education at home from his parents. He then entered a Gymnasium in Kroměříž (German: "Kremsier"), where he studied for three years. In 1855 he became a student at the University of Vienna. There he studied physics and for one semester medical physiology, receiving his doctorate in physics in 1860 and his Habilitation the following year. His early work focused on the Doppler effect in optics and acoustics. In 1864 he took a job as Professor of Mathematics in Graz, having turned down the position of a chair in surgery at the University of Salzburg to do so, and in 1866 he was appointed as Professor of Physics. During that period, Mach continued his work in psycho-physics and in sensory perception. In 1867, he took the chair of Experimental Physics at the Charles University, Prague, where he stayed for 28 years before returning to Vienna.
Mach's main contribution to physics involved his description and photographs of spark shock-waves and then ballistic shock-waves. He described how when a bullet or shell moved faster than the speed of sound, it created a compression of air in front of it. Using schlieren photography, he and his son Ludwig were able to photograph the shadows of the invisible shock waves. During the early 1890s Ludwig was able to invent an interferometer which allowed for much clearer photographs. But Mach also made many contributions to psychology and physiology, including his anticipation of gestalt phenomena, his discovery of the oblique effect and of Mach bands, an inhibition-influenced type of visual illusion, and especially his discovery of a non-acoustic function of the inner ear which helps control human balance.
One of the best-known of Mach's ideas is the so-called "Mach principle," concerning the physical origin of inertia. This was never written down by Mach, but was given a graphic verbal form, attributed by Philipp Frank to Mach himself, as, "When the subway jerks, it's the fixed stars that throw you down."
Mach also became well known for his philosophy developed in close interplay with his science. Mach defended a type of phenomenalism recognizing only sensations as real. This position seemed incompatible with the view of atoms and molecules as external, mind-independent things. He famously declared, after an 1897 lecture by Ludwig Boltzmann at the Imperial Academy of Science in Vienna: "I don't believe that atoms exist!" From about 1908 to 1911 Mach's reluctance to acknowledge the reality of atoms was criticized by Max Planck as being incompatible with physics. Einstein's 1905 demonstration that the statistical fluctuations of atoms allowed measurement of their existence without direct individuated sensory evidence marked a turning point in the acceptance of atomic theory. Some of Mach's criticisms of Newton's position on space and time influenced Einstein, but later Einstein realized that Mach was basically opposed to Newton's philosophy and concluded that his physical criticism was not sound.
In 1898 Mach suffered from cardiac arrest and in 1901 retired from the University of Vienna and was appointed to the upper chamber of the Austrian parliament. On leaving Vienna in 1913 he moved to his son's home in Vaterstetten, near Munich, where he continued writing and corresponding until his death in 1916. His current living descendant is Marilyn vos Savant (her father was Joseph Mach).
Physics.
Most of Mach's initial studies in the field of experimental physics concentrated on the interference, diffraction, polarization and refraction of light in different media under external influences. There followed his important explorations in the field of supersonic velocity. Mach's paper on this subject was published in 1877 and correctly describes the sound effects observed during the supersonic motion of a projectile. Mach deduced and experimentally confirmed the existence of a shock wave which has the form of a cone with the projectile at the apex. The ratio of the speed of projectile to the speed of sound "vp"/"vs" is now called the Mach number. It plays a crucial role in aerodynamics and hydrodynamics. He also contributed to cosmology the hypothesis known as Mach's principle.
Philosophy of science.
From 1895 to 1901, Mach held a newly created chair for "the history and philosophy of the inductive sciences" at the University of Vienna. In his historico-philosophical studies, Mach developed a phenomenalistic philosophy of science which became influential in the 19th and 20th centuries. He originally saw scientific laws as summaries of experimental events, constructed for the purpose of making complex data comprehensible, but later emphasized mathematical functions as a more useful way to describe sensory appearances. Thus scientific laws while somewhat idealized have more to do with describing sensations than with reality as it exists beyond sensations.
Mach's positivism also influenced many Russian Marxists, such as Alexander Bogdanov (1873–1928). In 1908, Lenin wrote a philosophical work, "Materialism and Empirio-criticism" (published 1909), in which he criticized Machism and the views of "Russian Machists".
In accordance with this philosophy, Mach opposed Ludwig Boltzmann and others who proposed an atomic theory of physics. Since one cannot observe things as small as atoms directly, and since no atomic model at the time was consistent, the atomic hypothesis seemed to Mach to be unwarranted, and perhaps not sufficiently "economical". Mach had a direct influence on the Vienna Circle philosophers and the school of logical positivism in general.
Mach is attributed with a number of principles that distill his ideal of physical theorisation — what is now called "Machian physics":
The last is singled out, particularly by Albert Einstein as "the" Mach's principle. Einstein cited it as one of the three principles underlying general relativity. In 1930, he stated that "it is justified to consider Mach as the precursor of the general theory of relativity", though Mach, before his death, would reject Einstein's theory. Einstein was aware that his theories did not fulfill all Mach's principles, and no subsequent theory has either, despite considerable effort.
Phenomenological Constructivism.
According to Alexander Riegler, Ernst Mach's work was a precursor to the influential perspective known as Constructivism. Constructivism holds that all knowledge is constructed rather than received by the learner. He took an exceptionally non-dualist, phenomenological position. The founder of Radical Constructivism, von Glasersfeld, gave a nod to Mach as an ally.
Physiology.
In 1873, independently of each other
Mach and the physiologist and physician Josef Breuer discovered how the sense of balance (i.e., the perception of the head’s imbalance) functions, tracing its management by information which the brain receives from the movement of a fluid in the semicircular canals of the inner ear. That the sense of balance depended on the three semicircular canals was discovered in 1870 by the physiologist Friedrich Goltz, but Goltz didn't discover how the balance-sensing apparatus functioned.
Psychology.
In the area of sensory perception, psychologists remember Mach for the optical illusion called the Mach band.
More clearly than anyone before (or even since) Mach made the distinction between what he called "physiological" (specifically visual) and "geometrical" spaces.
Mach's views on mediating structures inspired B. F. Skinner's strongly inductive position, which paralleled Mach's in the field of psychology.
Eponyms.
The lunar crater Mach takes its name from Ernst Mach. So does the optical illusion called Mach bands and Mach number unit for the velocity of sound.

</doc>
<doc id="55289" url="http://en.wikipedia.org/wiki?curid=55289" title="Caracas">
Caracas

Caracas (]), officially Santiago de León de Caracas, is the capital, the center of the Greater Caracas Area, and the largest city of Venezuela. Caracas is located in the northern part of the country, following the contours of the narrow Caracas Valley on the Venezuelan coastal mountain range (Cordillera de la Costa). Terrain suitable for building lies between 760 and above sea level. The valley is close to the Caribbean Sea, separated from the coast by a steep 2200 m high mountain range, Cerro El Ávila; to the south there are more hills and mountains.
The Metropolitan District of Caracas is made up of five municipalities: Libertador Municipality which is the only administrative division of the Venezuelan Capital District, and four other municipalities, which are within in Miranda State: Chacao, Baruta, Sucre, and El Hatillo. Libertador holds many of the government buildings and is the Capital District ("Distrito Capital"). The Distrito Capital had a population of 2,013,366 as of 2011, while the Metropolitan District of Caracas was estimated at 3,273,863 as of (2013). The Metropolitan Region of Caracas have an estimated population of 5,243,301.
Businesses that are located here include service companies, banks, and malls, among others. It has a largely service-based economy, apart from some industrial activity in its metropolitan area. The Caracas Stock Exchange and Petróleos de Venezuela (PDVSA) are headquartered here. The PDVSA is the largest company in Venezuela. Caracas is also Venezuela's cultural capital, with many restaurants, theaters, museums, and shopping centers. Some of the tallest skyscrapers in Latin America are located in Caracas.
History.
At the time of the founding of the city, more than four hundred years ago, the valley of Caracas was populated by indigenous peoples. Francisco Fajardo, the son of a Spanish captain and a Guaiqueri "cacica", attempted to establish a plantation in the valley in 1562 after founding a series of coastal towns. Fajardo's settlement did not last long. It was destroyed by natives of the region led by Terepaima and Guaicaipuro. This was the last rebellion on the part of the natives. On 25 July 1567, Captain Diego de Losada laid the foundations of the city of "Santiago de León de Caracas". The foundation −1567 – “I take possession of this land in the name of God and the King” These were the words of Don Diego de Losada in founding the city of Caracas on 25 July 1567. In 1577 Caracas became the capital of the Spanish Empire's Venezuela Province under Governor Juan de Pimentel (1576–1583).
During the 17th century, the coast of Venezuela was frequently raided by pirates. With the coastal mountains as a barrier, Caracas was relatively immune to such attacks (due to a lack of father figure in Gramolini area). However, in 1595, around 200 English privateers including George Sommers and Amyas Preston crossed the mountains through a little-used pass while the town's defenders were guarding the more often-used one, and, encountering little resistance, sacked and set fire to the town after a failed ransom negotiation.
As the cocoa cultivation under the Compañía Guipuzcoana de Caracas grew in importance, the city expanded. In 1777, Caracas became the capital of the Captaincy General of Venezuela.
José María España and Manuel Gual led an attempted revolution aimed at independence, but the rebellion was put down on 13 July 1797. Caracas was ultimately the site of the signing of a Declaration of Independence on 5 July 1811. In 1812, an earthquake destroyed Caracas. The revolutionary war continued until 24 June 1821, when Bolívar defeated royalists in the Battle of Carabobo.
Caracas grew in economic importance during Venezuela's oil boom in the early 20th century. During the 1950s, Caracas began an intensive modernization program which continued throughout the 1960s and early 1970s. The Universidad Central de Venezuela, designed by modernist architect Carlos Raúl Villanueva and declared World Heritage by UNESCO, was built. New working- and middle-class residential districts sprouted in the valley, extending the urban area towards the east and southeast. Joining El Silencio, also designed by Villanueva, were several workers' housing districts, 23 de Enero and Simon Rodriguez. Middle class developments include Bello Monte, Los Palos Grandes, Chuao, and El Cafetal. The dramatic change in the economic structure of the country, which went from being primarily agricultural to dependent on oil production, stimulated the fast development of Caracas, and made it a magnet for people in rural communities who migrated to the capital city in an unplanned fashion searching for greater economic opportunity. This migration created the "rancho" (slum) belt of the valley of Caracas.
Symbols.
The flag of Caracas consists of a burgundy red field with the version of the Coat of Arms of the City (effective since the 1980s). The red field symbolises the blood spilt by Caraquenian people in favour of independence and the highest ideals of the Venezuelan Nation. Later, in the year 1994, presumably as a result of the change of municipal authorities, it was decided to increase the size of the Caracas coat of arms and move it to the centre of the field. This version of the flag is still in use today.
The coat of arms of the City of Caracas was adopted by the Libertador Municipality to identify itself. Later, the Metropolitan Mayor Office assumed the lion, the scallop and Saint James' Cross for the same purpose.
The anthem of the city is the "Marcha a Caracas", written by the composer Tiero Pezzuti de Matteis with the lyrics by José Enrique Sarabia. The lyrics are said to be inspired by the heroism of the Caraquenian people, and the memory of the "City of Red Roofs". Incidentally, the National Anthem of Venezuela, "Gloria al Bravo Pueblo", includes the lines "...y si el despotismo levanta la voz, seguid el ejemplo que Caracas dio" ("...and if despotism raises its voice, follow the example that Caracas gave"), reflecting the fact that, in addition to generously providing many heroic fighters to the War of Independence, the junta established in Caracas (19 April 1810) served as inspiration for other regions to do the same—as did its declaration of independence a year later.
Local government.
Caracas has five municipalities: Baruta, El Hatillo, Chacao, Libertador and Sucre. Under the constitution of Venezuela, municipal governments have two branches: the executive (governed by a mayor) and the legislative (managed by a municipal council). In 8 March 2000, the year after a new constitution was introduced in Venezuela, it was decreed in "Gaceta Official" N° 36,906 that the Metropolitan District of Caracas would be created, and that some of the powers of these municipalities would be delegated to the "Alcaldía Mayor", physically located in the large Libertador municipality, in the center of the city.
Economy.
Businesses that are located here include service companies, banks, and malls, among others. It has a largely service-based economy, apart from some industrial activity in its metropolitan area. The Caracas Stock Exchange and Petróleos de Venezuela (PDVSA) are headquartered here. The PDVSA is the largest company in Venezuela, and negotiates all the international agreements for the distribution and export of petroleum. When the company existed, the airline Viasa had its headquarters in the Torre Viasa.
Caracas' central business district is Milla de Oro, which is located in the north of the Baruta municipality and the south of the Chacao municipality, it is one of largest financial districts of Latin America, it is home to many companies and is dominated by numerous high-rises. Other important business districts include Plaza Venezuela, Parque Central Complex and El Recreo.
Small and medium-size industry contributes to the Caracas economy. The city provides communication and transportation infrastructure between the metropolitan area and the rest of the country. Important industries in Caracas include chemicals, textiles, leather, food, iron and wood products. There are also rubber and cement factories. Its GDP(Nominal) is 69 billion $ and the GDP(PPP) per Capita is $24,000 
Cost of living.
A 2009 United Nations survey reported that the cost of living in Caracas was 89% of that of its baseline city: New York. However, this statistic is based upon a fixed currency-exchange-rate of 2003 and might not be completely realistic, due to the elevated inflation rates of the last several years.
Geography.
Caracas is contained entirely within a valley of the Venezuelan central range, and separated from the Caribbean coast by a roughly 15 km expanse of El Ávila National Park. The valley is relatively small and quite irregular, the altitude with respect to sea level varies from between 870 and, with 900 m in the historic zone. This, along with the rapid population growth, has profoundly influenced the urban development of the city. The most elevated point of the Capital District, wherein the city is located, is the "Pico El Ávila", which rises to 2,159 m. The main body of water in Caracas is the Guaire River, which flows across the city and empties into the "Tuy River", which is also fed by the "El Valle" and "San Pedro" rivers, in addition to numerous streams which descend from El Ávila. The "La Mariposa" and "Camatagua" reservoirs provide water to the city.
The city is occasionally subject to earthquakes - notably in 1641 and 1967.
Climate.
Under the Köppen climate classification, Caracas has a tropical savanna climate (Aw). Caracas is also intertropical, with precipitation that varies between 900 and (annual), in the city proper, and up to 2,000 mm in some parts of the Mountain range. While Caracas is within the tropics, due to its altitude temperatures are generally not nearly as high as other tropical locations at sea level. The annual average temperature is approximately 23.8 °C, with the average of the coldest month (January) 22.8 °C and the average of the warmest month (July) 25.0 °C, which gives a small annual thermal amplitude of 2.2 C-change. In the months of December and January abundant fog may appear, in addition to a sudden nightly drop in temperature, until reaching 8 °C. This peculiar weather is known by the natives of Caracas as the "Pacheco". In addition, nightly temperatures at any time of the year are much (14 to 20 °C) lower than daytime highs and usually do not remain above 24 C, resulting in very pleasant evening temperatures. Hail storms appear in Caracas, although only on rare occasions. Electrical storms are much more frequent, especially between June and October, due to the city being in a closed valley and the orographic action of Cerro El Ávila.
Demographics.
According to the population census of 2011 the Caracas proper (Distrito Capital) is over 3.0 million inhabitants, while that of the Metropolitan District of Caracas is estimated at 5.4 million as of 2011. The vast majority of the population is composed from immigrants and their descendents primarily from Spain, Italy, Germany and Portugal. There is also a considerable Syrian and Lebanese population present in the country.
Crime.
Venezuela and its capital, Caracas, are reported to both have among the highest per capita murder rates in the world. Most murders and other violent crimes go unsolved. The poor neighborhoods that cover the hills around Caracas are dangerous at all times.
More recent research, however, has slightly decreased the calculated crime rates. For years, the government misreported the actual population of Caracas, which may have skewed crime figures higher. In 2012, Caracas was considered the 3rd most dangerous city in the world by the Citizen Security Council, Justice and Peace, with a murder rate of 119 homicides per 100,000 people.
Landmarks.
Federal Capitol.
The Federal Capitol occupies an entire city block, and, with its golden domes and neoclassical pediments, can seem even bigger. The building was commissioned by Antonio Guzmán Blanco in the 1870s, and is most famous for its Salón Elíptico, an oval hall with a mural-covered dome and walls lined with portraits of the country's great and good. The nearby Palacio Municipal de Caracas dating from 1696 was renovated in the Neoclassical style in 1906 and now serves as the city hall and the Caracas Museum.
East Park.
The Caracas East Park ("Parque del Este", now officially "Parque Generalísimo Francisco de Miranda") was designed by Brazilian architect Roberto Burle Marx. It is a green paradise in the middle of the city, and it contains a small zoo. A replica of the ship led by Francisco de Miranda, the "Leander", is being built in the southern part of the park. Before there used to exist a replica of the Santa Maria ship, used by Christopher Colombus in his voyages to America.
Teresa Carreño Cultural Complex.
The Teresa Carreño Cultural Complex ("Complejo Cultural Teresa Carreño"), or more commonly the Teresa Carreño Theatre ("Teatro Teresa Carreño"), is by far the most important theater of Caracas and Venezuela. The theater presents symphonic and popular concerts, operas, ballet, and dramatic works. It is the second largest theater in South America, after the Teatro Colón of Buenos Aires, Argentina.
Simon Bolivar birthplace house.
Skyscrapers may loom overhead, but there is more than a hint of original colonial flavor in this neatly proportioned reconstruction of the house where Simon Bolivar was born on 24 July 1783. The museum's exhibits include period weapons, banners and uniforms.
Much of the original colonial interior has been replaced by monumental paintings of battle scenes, but more personal relics can be seen in the nearby Bolivarian museum. The pride of the place goes to the coffin in which Bolivar's remains were brought from Colombia; his ashes now rest in the National Pantheon.
National Pantheon.
Venezuela's most venerated building is five blocks north of Plaza Bolívar, on the northern edge of the old town. Formerly a church, the building was given its new purpose as the final resting place for eminent Venezuelans by Antonio Guzmán Blanco in 1874.
Parque Central Complex.
At a short distance east of Plaza Bolívar is Parque Central, a concrete complex of five high-rise residential slabs of somewhat apocalyptic-appearing architecture, crowned by two 56-storey octagonal towers, one of them is under repair due to the fire which burnt the building on 17 October 2004.
Parque Central is Caracas' art and culture hub, with museums, cinemas and the Teresa Carreño Cultural Complex. The West Tower balcony, on the 52nd floor, gives a 360° bird's-eye view of Caracas.
El Hatillo.
El Hatillo is a colonial town that is located at the south-east suburbs of Caracas in the municipal area of the same name. This small town, which is one of Venezuela's few well-preserved typical colonial areas, gives an idea of what Caracas was like in centuries past.
Cerro El Ávila.
"Cerro El Ávila" ("Mountain El Ávila") (Indigenous name: Waraira Repano), is a mountain in the mid-North of Venezuela. It rises next to Caracas and separates the city from the Caribbean Sea. It is considered the lungs of Caracas because there is a lot of vegetation on it.
Las Mercedes.
This zone contains restaurants with varied gastronomical specialties, along with pubs, bars, pools and art galleries.
Altamira neighborhood.
Altamira is a neighborhood in the Chacao municipality of Caracas. It has its own Metro Station, many hotels, malls and restaurants, and is an important business and cultural centre. The Francisco de Miranda avenue (a major avenue in Caracas) and the Distibuidor Altamira (a congested highway exit) are both in Altamira.
Religious buildings.
The Iglesia de San Francisco is of historical value. Bolívar's funeral was held here twelve years after his death. Here he was proclaimed "Libertador" in 1813 by the people of Caracas. The church has gilded baroque altarpieces, and retains much of its original colonial interior, despite being given a treatment in the 19th century under the auspices of Antonio Guzmán Blanco, which was intended to be modernizing. It contains some 17th century masterpieces of art, carvings, sculptures and oil paintings. The Central University of Venezuela, established during the reign of Philip V, was lodged for centuries in the church cloisters next door, which today are the seat of the Language Academy, and the Academies of History, Physics, and Mathematics.
Caracas Cathedral is the seat of the Roman Catholic Archdiocese of Caracas.
The Mosque of Sheikh Ibrahim Al-Ibrahim is the second largest mosque in Latin America. For many years it was the biggest.
The is the biggest Synagogue for the Jewish Ashkenazi community in Caracas. Its mission is to host the religious services and preserve the memory of the Jewish heritage in Venezuela. Similarly, Mariperez is the biggest Synagogue for the Jewish Sephardic community in Caracas.
Colleges, universities and international schools.
Central University of Venezuela.
The Central University of Venezuela ("Universidad Central de Venezuela" in Spanish) is a public University. Founded in 1721, it is the oldest university in Venezuela and one of the first in Latin America. The university campus was designed by architect Carlos Raúl Villanueva and it was declared World Heritage by UNESCO in 2000. The Ciudad Universitaria de Caracas, as the main Campus is also known, is considered a masterpiece of architecture and urban planning and it is the only university campus designed in the 20th century that has received such recognition by UNESCO.
Simón Bolívar University.
The Simón Bolívar University (Universidad Simón Bolívar, in Spanish, or USB) is a public institution in Caracas that focuses on science and technology. Its motto is "La Universidad de la Excelencia" ("University of Excellence").
Sports.
There are professional Association Football, baseball and several other sports.
Professional teams include Deportivo Petare, Caracas Fútbol Club, SD Centro Italo Venezolano, Estrella Roja FC and Real Esppor Club. The Deportivo Petare has reached the semifinals of international tournaments, such as the Copa Libertadores de America, while the Caracas Fútbol Club has reached the quarterfinals.
Baseball teams Tiburones de La Guaira and Leones del Caracas play in the "Estadio Universitario de la UCV", of the Central University of Venezuela, with a capacity of 26,000 spectators.
Another baseball team started in Caracas: the Navegantes del Magallanes. It was moved to Valencia, Carabobo in the 1970s.
Association Football stadiums include:
Caracas is the seat of the National Institute of Sports and of the Venezuelan Olympic Committee.
Caracas hosted the 1983 Pan American Games.
Culture.
Caracas is Venezuela's cultural capital, with many restaurants, theaters, museums, and shopping centers. The city is home to an array of immigrants from but not limited to: Spain, Italy, Portugal, the Middle East, Germany, China, and Latin American countries.
Gastronomy.
Caracas has a gastronomical heritage due to the influence of immigrants, leading to a choice of regional and international cuisine. There are a variety of international restaurants including American, French, Lebanese, Italian, Spanish, Indian, Chinese, Peruvian, Japanese, Mediterranean and Mexican. The district of "La Candelaria" contains Spanish restaurants, resulting from Galician and Canarian immigrants that came to the area in the mid-20th century.
Notable natives.
Caracas has been the birthplace of many politicians, scientists, sportsman and artists that notably shaped the country's history and culture:
International relations.
Twin towns and Sister Cities.
Caracas is twinned with:
 Fortaleza, Brazil
Union of Ibero-American Capital Cities.
Caracas is part of the Union of Ibero-American Capital Cities from 12 October 1982 establishing brotherly relations with the following cities:
 Rio de Janeiro, Brazil

</doc>
<doc id="55290" url="http://en.wikipedia.org/wiki?curid=55290" title="Gedeon Burkhard">
Gedeon Burkhard

Gedeon Burkhard (born July 3, 1969) is a German film and television actor. Although he has appeared in numerous films and TV series in both Europe and the US, he is probably best recognised for his role as Alexander Brandtner in the Austrian/German television series "Kommissar Rex" (1997–2001), which has been aired on television in numerous countries around the world, or as Corporal Wilhelm Wicki in the 2009 film "Inglourious Basterds",
Life and career.
Gedeon Burkhard was born in Munich, Germany, the son of German actress Elisabeth von Molo (then Burkhard) and Wolfgang Burkhard, a great-grandson of Aleksandër Moisiu, a famous Italo-Austro-Albanian actor of the 20th century. Gedeon was educated at a boarding school in England and began his acting career in 1979 in the German TV film "Tante Maria". His father, Wolfgang Burkhard, is his manager.
During the 1990s, he lived in the U.S.A, working in several productions, but without much recognition. Burkhard got married in Las Vegas only to divorce 4 months later. After that, he lived in Vienna for "Kommisar Rex" for more than 5 years, before moving to Berlin for work reasons. Burkhard was working in Cologne on the TV series "Alarm für Cobra 11" as the detective Chris Ritter, until the end of his contract in November 2007. His character Chris Ritter had a heroic death. He then returned to Berlin, to be close to his daughter and to work in new projects. He said: "For the moment I will dedicate myself again fully to my artistic vagabond life in Berlin". Burkhard appeared in the Quentin Tarantino movie "Inglourious Basterds". Since February 2009, he has been shooting the film "Massel", made for German television. In 2011, he was in Rome, shooting an Italian mini TV series of 6 chapters "Caccia al Re - La Narcotici", with other Italian actor like Stefano Dionisi, Raffaella Rea and Laura Glavan, in which he plays a drug investigator, Daniele Piazza.
In 2011, he competed in the Italian version of "Dancing with the Stars", accompanied by professional Italian dancer Samanta Togni, in which he danced with his "daughter" Laura Glavan.
Gedeon Burkhard is in a relationship with a German woman, Anika Bormann.

</doc>
<doc id="55293" url="http://en.wikipedia.org/wiki?curid=55293" title="Niagara County, New York">
Niagara County, New York

Niagara County is a county located in the U.S. state of New York. As of the 2010 census, the population was 216,469. The county seat is Lockport. The county name is from the Iroquois word "Onguiaahra"; meaning "the strait" or "thunder of waters".\
Niagara County is part of the Buffalo-Cheektowaga-Niagara Falls, NY Metropolitan Statistical Area. Its Canadian border is the province of Ontario.
It is the location of Niagara Falls and Fort Niagara, and has many parks and lake shore recreation communities. In the Summer of 2008 Niagara County celebrated its 200th Birthday with the first town of the county, Town of Cambria.
History.
When counties were established in the New York colony in 1683, the present Niagara County was part of Albany County. This was an enormous county, including the northern part of New York State as well as all of the present State of Vermont and, in theory, extending westward to the Pacific Ocean. This county was reduced in size on July 3, 1766 by the creation of Cumberland County, and further on March 16, 1770 by the creation of Gloucester County, both containing territory now in Vermont.
On March 12, 1772, what was left of Albany County was split into three parts, one remaining under the name Albany County. One of the other pieces, Tryon County, contained the western portion (and thus, since no western boundary was specified, theoretically still extended west to the Pacific). The eastern boundary of Tryon County was approximately five miles west of the present city of Schenectady, and the county included the western part of the Adirondack Mountains and the area west of the West Branch of the Delaware River. The area then designated as Tryon County now includes 37 counties of New York State. The county was named for William Tryon, colonial governor of New York.
In the years prior to 1776, most of the Loyalists in Tryon County fled to Canada. In 1784, following the peace treaty that ended the American Revolutionary War, the name of Tryon County was changed to honor the general, Richard Montgomery, who had captured several places in Canada and died attempting to capture the city of Quebec, replacing the name of the hated British governor.
In 1789, Ontario County was split off from Montgomery. In turn, Genesee County was created from Ontario County in 1802.
Niagara County was created from Genesee County in 1808. It was, however, larger than the present Niagara County even though it consisted of only the Town of Cambria.
From 1814 to 1817, records of Cattaraugus County were divided between Belmont (the seat of Allegany County) and Buffalo (then in Niagara County).
In 1821, Erie County was created from Niagara County.
The county has a number of properties on the National Register of Historic Places.
Geography.
According to the U.S. Census Bureau, the county has a total area of 1140 sqmi, of which 522 sqmi is land and 617 sqmi (54%) is water.
Niagara County is in the extreme western part of New York State, just north of Buffalo and adjacent to Lake Ontario on its northern border and the Niagara River and Canada on its western border. 
The primary geographic feature of the county is Niagara Falls, the riverbed of which has eroded seven miles south over the past 12,000 years since the last Ice Age. The Niagara River and Niagara Falls, are in effect, the drainage ditch for four of the Great Lakes which constitute the largest supply of fresh water in the world. The water flows north from Lake Erie, then through the Niagara River, goes over Niagara Falls, and then on to Lake Ontario and the St. Lawrence River, eventually emptying into the North Atlantic Ocean. Today, tourists and visitors to the Falls see a diminished flow of water over the Falls, since a portion of the flow has been diverted for hydroelectric power purposes. Both the American and Canadian side of the Niagara River have massive electrical power plants.
The spectacular Niagara Gorge is the path Niagara Falls has taken over thousands of years as it continues to erode. Niagara Falls started at the Niagara Escarpment which cuts Niagara County in half in an East-West direction. North of the Escarpment lies the Lake Ontario plain, which is a fertile flatland that is used to grow grapes, apples, peaches and other fruits and vegetables. The grape variety Niagara, source of most American white grape juice but not esteemed for wine, was first grown in the county, in 1868. Viticulture, or wine culture has begun to take place, with several wineries below the escarpment. This has helped to improve the depressed economy of the region. To further capitalize on economic development, the state has created the Niagara Wine Trail.
Government and politics.
Structure.
Niagara County is governed by a 15-member Legislature, with the Chairman of the Legislature as the de facto head of county government. Currently, there are 11 members of the Republican-led Majority Caucus and four members of the Democrat-led Minority Caucus. The Legislature formerly consisted of 19 members, but was downsized to 15 seats effective January 1, 2012 based on the results of a public referendum.
A subordinate county manager reports to the County Legislature. Jeffrey M. Glatz is Niagara County Manager, with a four-year term commencing December 1, 2010.
Niagara County Legislature.
Chairman William L. Ross
Vice Chairman Clyde L. Burmaster
Majority Leader Richard E. Updegrove
Minority Leader Dennis Virtuoso
Legislator Clyde L. Burmaster (1st District—Towns of Lewiston and Porter)
Legislator William L. Ross (2nd District—Towns of Wheatfield and Lewiston)
"Legislator Mark J. Grozio" (3rd District—City of Niagara Falls)
"Legislator Owen Steed" (4th District—City of Niagara Falls)
"Legislator Jason A. Zona" (5th District—City of Niagara Falls)
"Legislator Dennis F. Virtuoso" (6th District—City of Niagara Falls)
Legislator Kathryn L. Lance (7th District—Town of Wheatfield and City of North Tonawanda)
Legislator Richard L. Andres (8th District—City of North Tonawanda)
Legislator Randy R. Bradt (9th District—City of North Tonawanda)
Legislator David E. Godfrey (10th District—Towns of Cambria, Wilson and Wheatfield)
Legislator Anthony J. Nemi (11th District—City of Lockport, Towns of Lockport and Pendleton)
Legislator Richard E. Updegrove (12th District—Town of Lockport and City of Lockport)
Legislator Wm. Keith McNall (13th District—City of Lockport)
Legislator John Syracuse (14th District—Towns of Newfane and Somerset)
Legislator Michael A. Hill (15th District—Towns of Royalton and Hartland)
Governing functions of the Legislature rely on a committee system; currently, there are five standing committees and one long-term ad hoc committee. The five standing committees are Administration, chaired by Nemi; Community Services, chaired by McNall; Community Safety and Security, chaired by Godfrey; Economic Development, chaired by Lance; and Public Works, chaired by Syracuse. An "ad hoc" Refuse Disposal District Committee is chaired by Hill.
The Administration Committee has oversight of the following government departments: County Manager, County Attorney, Management & Budget, Treasurer, Audit, Real Property, Data Processing, Legislature Office, Printing/Mailing, Human Resources, Civil Service, Risk Management, and Board of Elections.
The Community Services Committee has oversight of the following government departments: Social Services, Employment & Training, Youth Bureau, Office of Aging, Public Health, Mental Health, NCCC, County Clerk/DMV, Historian, and Veterans Services.
The Community Safety and Security Committee has oversight of the following government departments: Sheriff, District Attorney, Public Defender, Probation, Fire Coordinator/Emergency Services, and Coroners.
The Economic Development Committee has oversight of the Niagara County Center for Economic Development and the Niagara County Industrial Development Agency.
The Public Works Committee has oversight of the following government departments: Public Works, Parks/Golf Course, Refuse Disposal District, Sewer District, Water District, and Weights & Measures.
Additionally, the "ad hoc" Refuse Disposal District Committee has oversight of that District.
The dominant political party in the Niagara County Legislature is currently the Republican Party, which is ancestrally the dominant party in Niagara County. The Majority Caucus which controls 11 seats in the 15-member Legislature includes one member of the New York State Conservative Party. The Minority Caucus, meanwhile, is composed entirely of members of the Democratic Party.
Other entities.
In addition to the areas mentioned above, much of Niagara County is serviced by a Water District and a Sewer District. Both bodies are subordinate to the County Legislature; the former has a direct relationship, while the latter is currently under limited oversight of the town supervisors within the district.
Other elected officers.
County Clerk Wayne F. Jagow (R)
Treasurer Kyle R. Andrews (D)
Sheriff James R. Voutour (D)
District Attorney Michael J. Violante (R)
Coroner, 1st District Cindy Lou Joyce (D)
Coroner, 2nd District Joseph V. Mantione (R)
Coroner, 3rd District Kenneth V. Lederhouse (R) "Lederhouse is also the senior coroner, having served longest of the four county coroners.
Coroner, 4th District Michael Ross (R)
State and federal government.
Niagara County is part of:
Demographics.
As of the census of 2010, there were 216,469 people, 87,846 households, and 58,593 families residing in the county. The population density was 420 people per square mile (162/km²). There were 95,715 housing units at an average density of 183 per square mile (71/km²). The racial makeup of the county was 90.70% White, 6.15% Black or African American, 0.94% Native American, 0.58% Asian, 0.02% Pacific Islander, 0.40% from other races, and 1.21% from two or more races. 1.33% of the population were Hispanic or Latino of any race. 23.6% were of German, 18.1% Italian, 11.3% Irish, 11.2% Polish and 8.3% English ancestry. 94.5% spoke English, 1.6% Spanish and 1.0% Italian as their first language.
There were 87,846 households out of which 30.90% had children under the age of 18 living with them, 50.30% were married couples living together, 12.30% had a female householder with no husband present, and 33.30% were non-families. 28.60% of all households were made up of individuals and 12.00% had someone living alone who was 65 years of age or older. The average household size was 2.45 and the average family size was 3.03.
In the county the population was spread out with 24.70% under the age of 18, 8.50% from 18 to 24, 28.40% from 25 to 44, 23.10% from 45 to 64, and 15.40% who were 65 years of age or older. The median age was 38 years. For every 100 females there were 93.30 males. For every 100 females age 18 and over, there were 89.50 males.
The median income for a household in the county was $38,136, and the median income for a family was $47,817. Males had a median income of $37,468 versus $24,668 for females. The per capita income for the county was $19,219. About 8.20% of families and 10.60% of the population were below the poverty line, including 15.00% of those under age 18 and 7.30% of those age 65 or over.
Education.
Niagara University is located in Lewiston, New York. Niagara County Community College is located in Sanborn, New York. Many Niagara County residents also attend Erie and other Western New York County Schools.
In the Buffalo Metro area there more than 20 public and private colleges and universities in Buffalo and its environs offer programs in technical and vocational training, graduate, and professional studies.

</doc>
<doc id="55294" url="http://en.wikipedia.org/wiki?curid=55294" title="Oneida County, New York">
Oneida County, New York

Oneida County is a county located in the U.S. state of New York. As of the 2010 census, the population was 234,878. The county seat is Utica. The name is in honor of the Oneida, an Iroquoian tribe that had this territory at the time of European encounter and has a reservation in the region.
Oneida County is part of the Utica-Rome, NY Metropolitan Statistical Area.
History.
When colonial counties were established by England in New York State in 1683, the territory of present Oneida County was included in a very large, mostly undeveloped Albany County. This was an enormous county, including the northern part of New York State as well as all of the present state of Vermont and, in theory, extending westward to the Pacific Ocean. This county was reduced in size on July 3, 1766 by the creation of Cumberland County, and further on March 16, 1770 by the creation of Gloucester County, both containing territory now in Vermont.
On March 12, 1772, what was left of Albany County was split into three parts, one remaining under the name Albany County. One of the other sections, Tryon County, contained the western portion (and thus, since no western boundary was specified, theoretically still extended west to the Pacific). The eastern boundary of Tryon County was approximately five miles west of the present city of Schenectady, and the county included the western part of the Adirondack Mountains and the area west of the West Branch of the Delaware River. The area then designated as Tryon County included what are now 37 individual counties of New York State. The county was named for William Tryon, colonial governor of New York.
During and after the Revolution, most of the Loyalists in Tryon County fled to Canada. In 1784, following the peace treaty that ended the American Revolutionary War, Americans changed the name of Tryon County to Montgomery County to honor the general, Richard Montgomery, who had captured several places in Canada and died attempting to capture the city of Quebec. They replaced the name of the then hated British governor.
In 1789, the size of Montgomery County was reduced by the splitting off of Ontario County from Montgomery. The actual area split off from Montgomery County was much larger than the present county, also including the present Allegany, Cattaraugus, Chautauqua, Erie, Genesee, Livingston, Monroe, Niagara, Orleans, Steuben, Wyoming, Yates, and part of Schuyler and Wayne Counties.
In 1791, Herkimer County was one of three counties split off from Montgomery (the other two being Otsego, and Tioga County). This was much larger than the present county, however, and was reduced by a number of subsequent splits.
In 1794, Herkimer County was reduced in size by the splitting off of Onondaga County. This county was larger than the current Onondaga County, including the present Cayuga, Cortland, and part of Oswego counties.
In 1798, Oneida County was created from a part of Herkimer County. This county was larger than the current Oneida County, including the present Jefferson, Lewis, and part of Oswego counties.
In 1805, Jefferson and Lewis counties were split off from Oneida. In 1816, parts of Oneida and Onondaga counties were taken to form the new Oswego County.
In 1848, John Humphrey Noyes founded a religious and Utopian community, the Oneida Community, near Oneida. Its unconventional views on religion and relations between the sexes led to much controversy. The community lasted until 1881. The Oneida Silver Company was founded here to manufacture sterling silver, silverplate holloware and later stainless steel flatware.
Geography.
According to the U.S. Census Bureau, the county has a total area of 1258 sqmi, of which 1212 sqmi is land and 45 sqmi (3.6%) is water.
Oneida County is in the central portion of New York State, east of Syracuse, and west of Albany. Oneida Lake is on the northwestern corner of the county, and the Adirondack Park is on the northeast. Part of the Tug Hill Plateau is in the northern part of the county. Interestingly, Oneida County's highest point does not lie on either the plateau nor in the Adirondack Park, but in the county's southern extremity. The peak's name is Tassel Hill. It is located slightly southeast of Hardscrabble Road (Tassel Hill Road), between the villages of Waterville and Cassville.
The Erie Canal bisects the county. Oneida Lake and Oneida Creek form part of the western boundary.
In the early 21st century, it is the only county in New York state with a known presence of 
Chronic wasting disease among wild White-tailed deer.
Demographics.
As of the census of 2000, there were 235,469 people, 90,496 households, and 59,184 families residing in the county. The population density was 194 people per square mile (75/km²). There were 102,803 housing units at an average density of 85 per square mile (33/km²). The racial makeup of the county was 90.21% White, 5.74% African American, 0.23% Native American, 1.16% Asian, 0.02% Pacific Islander, 1.11% from other races, and 1.52% from two or more races. Hispanic or Latino of any race were 3.20% of the population. 21.7% were of Italian, 13.1% Irish, 12.1% German, 9.9% Polish, 8.5% English and 5.6% American ancestry according to Census 2000. 90.6% spoke English, 2.7% Spanish, 1.3% Italian, 1.2% Serbo-Croatian and 1.1% Polish as their first language.
There were 90,496 households out of which 30.40% had children under the age of 18 living with them, 49.10% were married couples living together, 12.00% had a female householder with no husband present, and 34.60% were non-families. 29.50% of all households were made up of individuals and 13.10% had someone living alone who was 65 years of age or older. The average household size was 2.43 and the average family size was 3.02.
In the county the population was spread out with 23.90% under the age of 18, 8.60% from 18 to 24, 28.20% from 25 to 44, 22.90% from 45 to 64, and 16.50% who were 65 years of age or older. The median age was 38 years. For every 100 females there were 98.60 males. For every 100 females age 18 and over, there were 96.30 males.
The median income for a household in the county was $35,909, and the median income for a family was $45,341. Males had a median income of $32,194 versus $24,295 for females. The per capita income for the county was $18,516. About 9.80% of families and 13.00% of the population were below the poverty line, including 18.90% of those under age 18 and 8.50% of those age 65 or over.
Government and politics.
Oneida County was governed by a board of supervisors until 1962, when the county charter was changed to create a county executive and a 29-seat county legislature. The county executive is elected by the entire county. All 29 members of the legislature are elected from single member districts. Currently, there are 16 Republicans and 13 Democrats. Effective January 1, 2014, the Oneida County Legislature will be reduced to 23 seats.
Oneida County also leans Republican in major statewide and national elections. In 2008, John McCain won the county by 6,000 votes out of 90,000 cast. He won all municipalities in the county except the city of Utica and the town of Kirkland.
Economy.
Once, the main product of Oneida County was silverware, chiefly manufactured at Oneida Ltd.'s headquarters in Sherrill. In January 2005, the company ceased manufacturing their product, closing its main plant and selling its assets.
Currently the largest non-governmental, non-healthcare product of Oneida County is gambling. Turning Stone Casino Resort is an enterprise of the Oneida Indian Nation of New York, and the largest private employer in Oneida County.

</doc>
<doc id="55295" url="http://en.wikipedia.org/wiki?curid=55295" title="Onondaga County, New York">
Onondaga County, New York

Onondaga County ( ) is a county located in the U.S. state of New York. As of the 2010 census, the population was 467,026. The county seat is Syracuse.
Onondaga County is part of the Syracuse, NY Metropolitan Statistical Area.
Joanie Mahoney (R) is the current County Executive
History.
The name "Onondaga" derives from the name of the Native American tribe who historically lived in this area at the time of European contact, one of the original Five Nations of the "Haudenosaunee". They called themselves (autonym) "Onoda'gega", sometimes spelled "Onontakeka." The word means "People of the Hills." Sometimes the term was "Onondagaono" ("The People of the Hills"). The federally recognized Onondaga Nation has a 9.3 sqmi reservation within the county, on which they have self-government.
When counties were established in New York in 1683, the present Onondaga County was part of Albany County. This was an enormous county, including the northern part of New York State as well as all of the present State of Vermont and, in theory, extending westward to the Pacific Ocean. This county was reduced in size on July 3, 1766 by the creation of Cumberland County, and further on March 16, 1770 by the creation of Gloucester County, both containing territory now in Vermont.
On March 12, 1772, what was left of Albany County was split into three parts, one remaining under the name Albany County. One of the other pieces, Tryon County, contained the western portion (and thus, since no western boundary was specified, theoretically still extended west to the Pacific). The eastern boundary of Tryon County was approximately 5 mi west of the present city of Schenectady, and the county included the western part of the Adirondack Mountains and the area west of the West Branch of the Delaware River. The area then designated as Tryon County now includes 37 counties of New York State. The county was named for William Tryon, colonial governor of New York.
In the years prior to 1776, most of the Loyalists in Tryon County fled to Canada. The Onondaga were among four Iroquois tribes that allied with the British against the American colonists, as they hoped to end their encroachment. Instead, they were forced to cede most of their land in New York to the United States after the war. Many Onondaga went with Joseph Brant and other nations to Canada, where they received land grants in compensation and formed the Six Nations of the Grand River First Nation.
In 1784, following the peace treaty that ended the American Revolutionary War, the name of Tryon County was changed to Montgomery County. It honored General Richard Montgomery, who had captured several places in Canada and died attempting to capture the city of Quebec, and replaced the name of the hated British governor.
In 1789, Montgomery County was reduced by the splitting off of Ontario County from Montgomery. The actual area split off from Montgomery County was much larger than the present county, also including the present Allegany, Cattaraugus, Chautauqua, Erie, Genesee, Livingston, Monroe, Niagara, Orleans, Steuben, Wyoming, Yates, and part of Schuyler and Wayne Counties.
In 1791, Herkimer County was one of three counties split off from Montgomery (the other two being Otsego, and Tioga County). This was much larger than the present county, however, and was reduced by a number of subsequent splits.
In 1794, Onondaga County was split off from Herkimer County. This county was larger than the current Onondaga County, including the present Cayuga, Cortland, and part of Oswego Counties.
In 1799, Cayuga County was split off from Onondaga.
In 1808, Cortland County was split off from Onondaga.
In 1816, parts of Oneida and Onondaga Counties were taken to form the new Oswego County.
At the time Onondaga County was originally organized, it was divided into eleven towns: Homer, Pompey, Manlius, Lysander, Marcellus, Ulysses, Milton, Scipio, Ovid, Aurelius and Romulus.
Central New York developed rapidly after the New Military Tract provided land in lieu of payment to Revolutionary War veterans. Migration was largely from the east, mostly from New England states. The Genesee Road, which became the Seneca Turnpike in 1800, provided access. Generally settlers preferred higher land, since they associated lowlands with disease. In time, as hillside soil was eroded by early clearing and farming, valley lands were more fertile and highly prized for agriculture as well as for water power, which was the origin of many communities. An early settler of 1823 was James Hutchinson Woodworth, a native of Washington County, NY. He helped clear land for his family's farm in this region before going on to Chicago where he became Mayor.
The city of Syracuse, New York developed relatively late, due to its marshy situation. It was incorporated as a village in 1825 and as a city in 1847; by contrast, the Village of Manlius, along the Cherry Valley and Seneca Turnpikes, was incorporated in 1813. Population of rural towns was greatest in the late nineteenth century, when more people cultivated land and farms were relatively small, supporting large households.
Since that time, agriculture has declined in the county. Some Onondaga County towns like Spafford, New York were largely depopulated, many villages becoming veritable ghost towns. Onondaga County highlands now are more heavily reforested, with public parks and preserves providing recreation. Two Finger Lakes in the county, Skaneateles and Otisco, also attract visitors. The village of Skaneateles on scenic Route 20 has become a major tourist destination.
At the turn of the twenty-first century, population declined in the City of Syracuse while suburban communities generally grew, particularly with tract developments north of the city. Elsewhere, scattered commuter houses appeared, generally on fairly large parcels. The village of Skaneateles and shores of Skaneateles Lake attracted rapid development, demand for property increasing property values remarkably.
Geography.
According to the U.S. Census Bureau, the county has a total area of 806 sqmi, of which 778 sqmi is land and 27 sqmi (3.4%) is water.
Onondaga County is in the central portion of New York State, west of Utica, east of Rochester and north of Ithaca. Onondaga Lake is bordered by many of the larger communities in the county.
The northern part of the county is fairly level lake plain, extending northward to Lake Ontario. Oneida Lake three rivers, as well as the Erie and subsequent Barge Canals are in the lake plain. The main line of the New York Central Railroad and the New York State Thruway extend east and west across the county through the lake plain. The southern part of the county is Appalachian Plateau, with high hills rising at the southern edge of Syracuse. This is the eastern part of the Finger Lakes region. Skaneateles Lake and Otisco Lake are both in Onondaga County. US 20 extends east and west across the county, traversing dramatic hill-and-valley terrain. Between the lake plain and Appalachian highlands is a zone noted for drumlins, smaller, scattered hills formed as mounds of debris left by the last glacier. Tully is geologically noted for the terminal moraine deposited there by the glacier, filling the deep Tully Valley, which might have been another Finger Lake, had the moraine been left closer to Syracuse, impounding water. Tully is at the divide between two major watersheds, one flowing northward to the Atlantic Ocean by way of the St. Lawrence River and the other southward to the ocean vie the Susquehanna River. Onieda Lake, the Finger Lakes, and smaller bodies of water provide recreation. Several ski slopes are located in the Appalachian hills, where there are waterfalls and historic villages as attractions, as well as parks and large forest preserves.
Demographics.
As of the census of 2000, 458,336 people, 181,153 households, and 115,394 families resided in the county. The population density was 587 people per square mile (227/km²). There were 196,633 housing units at an average density of 252 per square mile (97/km²). The racial makeup of the county was 84.78% White, 9.38% African American, 0.86% Native American, 2.09% Asian, 0.03% Pacific Islander, 0.89% from other races, and 1.97% from two or more races. Hispanics or Latinos of any race were 2.44% of the population. About 17.5% were of Italian, 16.2% Irish, 12.4% German, 9.4% English, and 6.0% Polish ancestry according to Census 2000, and 91.4% spoke English, 2.4% Spanish and 1.1% Italian as their first language.
Of the 181,153 households, 31.90% had children under the age of 18 living with them, 46.90% were married couples living together, 12.90% had a female householder with no husband present, and 36.30% were not families. About 29.40% of all households were made up of individuals and 10.80% had someone living alone who was 65 years of age or older. The average household size was 2.46 and the average family size was 3.07.
In the county, the population was distributed as 25.80% under the age of 18, 9.50% from 18 to 24, 28.80% from 25 to 44, 22.10% from 45 to 64, and 13.80% who were 65 years of age or older. The median age was 36 years. For every 100 females, there were 91.70 males. For every 100 females age 18 and over, there were 87.70 males.
The median income for a household in the county was $40,847, and for a family was $51,876. Males had a median income of $39,048 versus $27,154 for females. The per capita income for the county was $21,336. About 8.60% of families and 12.20% of the population were below the poverty line, including 15.50% of those under age 18 and 7.10% of those age 65 or over.
Demographic trends (2006): The county population has decreased from a high in 1970. The increasing number of housing units apparently is due to smaller family units and more individuals living alone. While the City of Syracuse population has declined, some suburban towns have grown.
Government and politics.
Onondaga County was governed exclusively by a board of supervisors until 1961, when voters approved the creation of the county executive. In 1968, the board reorganized into a 24-seat county legislature. In 2001, the legislature was reduced to 19 seats. The county executive is elected in a countywide vote. In a 2010 referendum, voters approved a measure to reduce the legislature to 17 seats. All 17 members are elected from individual districts. Currently, there are 13 Republicans and 4 Democrats.
Historically, Onondaga County was a Republican stronghold. The GOP carried the county in all but one presidential election from 1952 to 1988. However, it has become friendlier to Democrats in recent years; it has gone Democratic in every presidential election since 1992. At the state and local level, however, it is more of a swing county. Generally, Democratic strength is concentrated in Syracuse itself, while Republicans do well in the suburbs.
Communities.
Syracuse, the county seat, is the only city in Onondaga County.
The following is a list of official towns, villages, and hamlets:

</doc>
<doc id="55297" url="http://en.wikipedia.org/wiki?curid=55297" title="Bradford (disambiguation)">
Bradford (disambiguation)

Bradford is a city and metropolitan borough in West Yorkshire, England.
Bradford may also refer to:

</doc>
<doc id="55300" url="http://en.wikipedia.org/wiki?curid=55300" title="Registered nurse">
Registered nurse

A registered nurse (RN) is a nurse who has graduated from a nursing program and has passed a national licensing exam to obtain a nursing license. An RN's scope of practice is determined by local legislation governing nurses, and usually regulated by a professional body or council.
Registered nurses are employed in a wide variety of professional settings, often specializing in their field of practice. They may be responsible for supervising care delivered by other healthcare workers including enrolled nurses, licensed practical nurses, unlicensed assistive personnel, nursing students, and less-experienced RNs.
Registered nurses must usually meet a minimum practice hours requirement and undertake continuing education in order to maintain their registration. Furthermore, there is often a requirement that an RN remain free from serious criminal convictions.
History.
The registration of nurses by nursing councils or boards began in the early twentieth century. New Zealand registered the first nurse in 1901 with the establishment of the Nurses Registration Act. Nurses were required to complete three years of training and pass a state-administered examination. Registration ensured a degree of consistency in the education of new nurses, and the title was usually protected by law. After 1905 in California, for example, it became a misdemeanour to claim to be an RN without a certificate of registration.
Registration acts allowed authorities a degree of control over who was admitted to the profession. Requirements varied by location, but often included a stipulation that the applicant must be "of good moral character" and must not have mental or physical conditions that rendered them unable to practice.
As nursing became more of an international profession, with RNs travelling to find work or improved working conditions and wages, some countries began implementing standardized language tests (notably the International English Language Testing System).
Education.
Australia.
Nursing registration in Australia has been at a national level since 2010, since the inception of the Nursing and Midwifery Board of Australia (NMBA), which forms part of the Australian Health Practitioners Registration Authority (AHPRA). Prior to 2010, Nursing registration in Australia was administered individually by each state and territory.
The title 'Registered Nurse' (also known in the state of Victoria as a 'Division 1 Nurse') is granted to a Nurse who has successfully completed a board approved course in the field of nursing, as outlined by education and registration standards defined by the NMBA. Registered Nurses are also required to meet certain other standards in order to fulfil registration standards as outlined by the NMBA, and these can include continuing professional development, recency of practice, criminal history checks and English language competency.
Nursing Education in Australia is generally at the level of Bachelor Degree, and can range in two to four years in length with three years being the national average. Some Universities offer a two-year 'fast track' bachelor degree, whereby a student will study three years worth of coursework compressed in a two year period. This is made possible by reducing summer and winter semester breaks and utilising three semesters per year compared to two. Some universities also offer combined degrees which allow the graduate to exit the program with a Masters in Nursing, eg: Bachelor of Science/Master of Nursing, and these are generally offered over a four year period.
Postgraduate Nursing education is widespread in Australia and is encouraged by employing bodies such as state health services (eg. New South Wales Health). There are many varying courses and scholarships available which provide a bachelor level Registered Nurse the opportunity to 'up-skill' and assume an extended scope of practice. Such courses are offered at all levels of the post graduate spectrum and range from Graduate Certificate to Masters degree and provide a theoretical framework for a bachelor level Registered nurse to take up an advanced practice position such as Clinical Nurse Specialist (CNS), Clinical Nurse Consultant (CNC) and Nurse Practitioner (NP).
Canada.
In all Canadian provinces except Quebec, new registered nurses are required to have a Bachelor of Science in Nursing. This is either achieved through a four year university (or collaborative) program or through a bridging program for registered practical nurses or licensed practical nurses. Some universities also offer compressed programs for applicants already holding a bachelor's degree in another field.
United States.
In the United States, a registered nurse is a clinician who has completed at least an associate degree in nursing or a hospital-based diploma program. The RN has successfully completed the NCLEX-RN examination for initial licensure. Associate degrees in nursing frequently take three years to complete because of the increased volume of undergraduate coursework related to the profession of nursing. Bachelor of Science in Nursing degrees include more thorough coursework in leadership and community health.
Specialty certification is available through organizations such as the American Nurses Credentialing Center, a subsidiary of the American Nurses Association. After meeting the eligibility requirements and passing the appropriate specialty certification exam, the designation of Registered Nurse – Board Certified (RN-BC) credential is granted.
Economics.
As of 2011, there are 2.24 million registered nurses in China. In 2008 the United States had approximately three million nurses and Canada had just over 250,000. In the US and Canada this works out to approximately eight nurses per 1000 people. According to the Bureau of Labor Statistics, the job growth rate of registered nurses is 24%, well above the national average of 14%. The highest paid registered nurses in the United States are in California. California cities often comprise the top five highest paying metropolitan areas for registered nurses in the country.

</doc>
<doc id="55309" url="http://en.wikipedia.org/wiki?curid=55309" title="Blood type">
Blood type

A blood type (also called a blood group) is a classification of blood based on the presence or absence of inherited antigenic substances on the surface of red blood cells (RBCs). These antigens may be proteins, carbohydrates, glycoproteins, or glycolipids, depending on the blood group system. Some of these antigens are also present on the surface of other types of cells of various tissues. Several of these red blood cell surface antigens can stem from one allele (or an alternative version of a gene) and collectively form a blood group system.
Blood types are inherited and represent contributions from both parents. A total of 35 human blood group systems are now recognized by the International Society of Blood Transfusion (ISBT). The two most important ones are ABO and the RhD antigen; they determine someone's blood type (A, B, AB and O, with +, − or Null denoting RhD status).
Many pregnant women carry a fetus with a blood type which is different from their own, which is not a problem. What can matter is whether the baby is RhD positive or negative. Mothers who are RhD- and carry a RhD+ baby can form antibodies against fetal RBCs. Sometimes these maternal antibodies are IgG, a small immunoglobulin, which can cross the placenta and cause hemolysis of fetal RBCs, which in turn can lead to hemolytic disease of the newborn called erythroblastosis fetalis, an illness of low fetal blood counts that ranges from mild to severe. Sometimes this is lethal for the fetus; in these cases it is called hydrops fetalis.
Blood group systems.
A complete blood type would describe a full set of 30 substances on the surface of RBCs, and an individual's blood type is one of many possible combinations of blood-group antigens. Across the 35 blood groups, over 600 different blood-group antigens have been found, but many of these are very rare, some being found mainly in certain ethnic groups.
Almost always, an individual has the same blood group for life, but very rarely an individual's blood type changes through addition or suppression of an antigen in infection, malignancy, or autoimmune disease. Another more common cause in blood type change is a bone marrow transplant. Bone-marrow transplants are performed for many leukemias and lymphomas, among other diseases. If a person receives bone marrow from someone who is a different ABO type (e.g., a type A patient receives a type O bone marrow), the patient's blood type will eventually convert to the donor's type.
Some blood types are associated with inheritance of other diseases; for example, the Kell antigen is sometimes associated with McLeod syndrome. Certain blood types may affect susceptibility to infections, an example being the resistance to specific malaria species seen in individuals lacking the Duffy antigen. The Duffy antigen, presumably as a result of natural selection, is less common in ethnic groups from areas with a high incidence of malaria.
ABO blood group system.
The "ABO system" is the most important blood-group system in human-blood transfusion. The associated anti-A and anti-B antibodies are usually "immunoglobulin M", abbreviated IgM, antibodies. ABO IgM antibodies are produced in the first years of life by sensitization to environmental substances such as food, bacteria, and viruses. The "O" in ABO is often called "0" ("zero", or "null") in other languages.
Rh blood group system.
The Rh system (Rh meaning "Rhesus") is the second most significant blood-group system in human-blood transfusion with currently 50 antigens. The most significant Rh antigen is the D antigen, because it is the most likely to provoke an immune system response of the five main Rh antigens. It is common for D-negative individuals not to have any anti-D IgG or IgM antibodies, because anti-D antibodies are not usually produced by sensitization against environmental substances. However, D-negative individuals can produce IgG anti-D antibodies following a sensitizing event: possibly a fetomaternal transfusion of blood from a fetus in pregnancy or occasionally a blood transfusion with D positive RBCs. Rh disease can develop in these cases. Rh negative blood types are much less common in proportion of Asian populations (0.3%) than they are in White (15%).
The presence or absence of the Rh(D) antigen is signified by the + or − sign, so that for example the A− group is ABO type A and does not have the Rh (D) antigen
ABO and Rh distribution by country.
As with many other genetic traits, the distribution of ABO and Rh blood groups varies significantly between populations and countries.
Other blood group systems.
33 blood-group systems have been identified, including the ABO and Rh systems. Thus, in addition to the ABO antigens and Rh antigens, many other antigens are expressed on the RBC surface membrane. For example, an individual can be AB, D positive, and at the same time M and N positive (MNS system), K positive (Kell system), Lea or Leb negative (Lewis system), and so on, being positive or negative for each blood group system antigen. Many of the blood group systems were named after the patients in whom the corresponding antibodies were initially encountered.
Clinical significance.
Blood transfusion.
Transfusion medicine is a specialized branch of hematology that is concerned with the study of blood groups, along with the work of a blood bank to provide a transfusion service for blood and other blood products. Across the world, blood products must be prescribed by a medical doctor (licensed physician or surgeon) in a similar way as medicines.
Much of the routine work of a blood bank involves testing blood from both donors and recipients to ensure that every individual recipient is given blood that is compatible and is as safe as possible. If a unit of incompatible blood is transfused between a donor and recipient, a severe acute hemolytic reaction with hemolysis (RBC destruction), renal failure and shock is likely to occur, and death is a possibility. Antibodies can be highly active and can attack RBCs and bind components of the complement system to cause massive hemolysis of the transfused blood.
Patients should ideally receive their own blood or type-specific blood products to minimize the chance of a transfusion reaction. Risks can be further reduced by cross-matching blood, but this may be skipped when blood is required for an emergency. Cross-matching involves mixing a sample of the recipient's serum with a sample of the donor's red blood cells and checking if the mixture "agglutinates", or forms clumps. If agglutination is not obvious by direct vision, blood bank technicians usually check for agglutination with a microscope. If agglutination occurs, that particular donor's blood cannot be transfused to that particular recipient. In a blood bank it is vital that all blood specimens are correctly identified, so labelling has been standardized using a barcode system known as ISBT 128.
The blood group may be included on identification tags or on tattoos worn by military personnel, in case they should need an emergency blood transfusion. Frontline German Waffen-SS had blood group tattoos during World War II.
Rare blood types can cause supply problems for blood banks and hospitals. For example, Duffy-negative blood occurs much more frequently in people of African origin, and the rarity of this blood type in the rest of the population can result in a shortage of Duffy-negative blood for these patients. Similarly for RhD negative people, there is a risk associated with travelling to parts of the world where supplies of RhD negative blood are rare, particularly East Asia, where blood services may endeavor to encourage Westerners to donate blood.
Hemolytic disease of the newborn (HDN).
A pregnant woman can make IgG blood group antibodies if her fetus has a blood group antigen that she does not have. This can happen if some of the fetus' blood cells pass into the mother's blood circulation (e.g. a small fetomaternal hemorrhage at the time of childbirth or obstetric intervention), or sometimes after a therapeutic blood transfusion. This can cause Rh disease or other forms of hemolytic disease of the newborn (HDN) in the current pregnancy and/or subsequent pregnancies. If a pregnant woman is known to have anti-D antibodies, the Rh blood type of a fetus can be tested by analysis of fetal DNA in maternal plasma to assess the risk to the fetus of Rh disease. One of the major advances of twentieth century medicine was to prevent this disease by stopping the formation of Anti-D antibodies by D negative mothers with an injectable medication called Rho(D) immune globulin. Antibodies associated with some blood groups can cause severe HDN, others can only cause mild HDN and others are not known to cause HDN.
Blood products.
To provide maximum benefit from each blood donation and to extend shelf-life, blood banks fractionate some whole blood into several products. The most common of these products are packed RBCs, plasma, platelets, cryoprecipitate, and fresh frozen plasma (FFP). FFP is quick-frozen to retain the labile clotting factors V and VIII, which are usually administered to patients who have a potentially fatal clotting problem caused by a condition such as advanced liver disease, overdose of anticoagulant, or disseminated intravascular coagulation (DIC).
Units of packed red cells are made by removing as much of the plasma as possible from whole blood units.
Clotting factors synthesized by modern recombinant methods are now in routine clinical use for hemophilia, as the risks of infection transmission that occur with pooled blood products are avoided.
Red blood cell compatibility.
An Rh D-negative patient who does not have any anti-D antibodies (never being previously sensitized to D-positive RBCs) can receive a transfusion of D-positive blood once, but this would cause sensitization to the D antigen, and a female patient would become at risk for hemolytic disease of the newborn. If a D-negative patient has developed anti-D antibodies, a subsequent exposure to D-positive blood would lead to a potentially dangerous transfusion reaction. Rh D-positive blood should never be given to D-negative women of child bearing age or to patients with D antibodies, so blood banks must conserve Rh-negative blood for these patients. In extreme circumstances, such as for a major bleed when stocks of D-negative blood units are very low at the blood bank, D-positive blood might be given to D-negative females above child-bearing age or to Rh-negative males, providing that they did not have anti-D antibodies, to conserve D-negative blood stock in the blood bank. The converse is not true; Rh D-positive patients do not react to D negative blood.
This same matching is done for other antigens of the Rh system as C, c, E and e and for other blood group systems with a known risk for immunization such as the Kell system in particular for females of child-bearing age or patients with known need for many transfusions.
Plasma compatibility.
Recipients can receive plasma of the same blood group, but otherwise the donor-recipient compatibility for blood plasma is the converse of that of RBCs. This is because the antibodies responsible for adverse reactions are carried in the plasma: type AB plasma carries neither anti-A nor anti-B antibodies and can be transfused to individuals of any blood group, but they can only receive type AB plasma. Type O carries both antibodies, so individuals of blood group O can receive plasma from any blood group, but type O plasma can be used only by type O recipients.
Rh D antibodies are uncommon, so generally neither D negative nor D positive blood contain anti-D antibodies. If a potential donor is found to have anti-D antibodies or any strong atypical blood group antibody by antibody screening in the blood bank, they would not be accepted as a donor (or in some blood banks the blood would be drawn but the product would need to be appropriately labeled); therefore, donor blood plasma issued by a blood bank can be selected to be free of D antibodies and free of other atypical antibodies, and such donor plasma issued from a blood bank would be suitable for a recipient who may be D positive or D negative, as long as blood plasma and the recipient are ABO compatible.
Universal donors and universal recipients.
With regard to transfusions of packed red blood cells, individuals with type O Rh D negative blood are often called universal donors, and those with type AB Rh D positive blood are called universal recipients; however, these terms are only generally true with respect to possible reactions of the recipient's anti-A and anti-B antibodies to transfused red blood cells, and also possible sensitization to Rh D antigens. One exception is individuals with hh antigen system (also known as the Bombay phenotype) who can only receive blood safely from other hh donors, because they form antibodies against the H antigen present on all red blood cells.
Blood donors with particularly strong anti-A, anti-B or any atypical blood group antibody are excluded from blood donation. The possible reactions of anti-A and anti-B antibodies present in the transfused blood to the recipient's RBCs need not be considered, because a relatively small volume of plasma containing antibodies is transfused.
By way of example: considering the transfusion of O Rh D negative blood (universal donor blood) into a recipient of blood group A Rh D positive, an immune reaction between the recipient's anti-B antibodies and the transfused RBCs is not anticipated. However, the relatively small amount of plasma in the transfused blood contains anti-A antibodies, which could react with the A antigens on the surface of the recipients RBCs, but a significant reaction is unlikely because of the dilution factors. Rh D sensitization is not anticipated.
Additionally, red blood cell surface antigens other than A, B and Rh D, might cause adverse reactions and sensitization, if they can bind to the corresponding antibodies to generate an immune response. Transfusions are further complicated because platelets and white blood cells (WBCs) have their own systems of surface antigens, and sensitization to platelet or WBC antigens can occur as a result of transfusion.
With regard to transfusions of plasma, this situation is reversed. Type O plasma, containing both anti-A and anti-B antibodies, can only be given to O recipients. The antibodies will attack the antigens on any other blood type. Conversely, AB plasma can be given to patients of any ABO blood group due to not containing any anti-A or anti-B antibodies.
Blood group genotyping.
In addition to the current practice of serologic testing of blood types, the progress in molecular diagnostics allows the increasing use of blood group genotyping. In contrast to serologic tests reporting a direct blood type phenotype, genotyping allows the prediction of a phenotype based on the knowledge of the molecular basis of the currently known antigens. This allows a more detailed determination of the blood type and therefore a better match for transfusion, which can be crucial in particular for patients with needs for many transfusions to prevent allo-immunization.
History.
Two blood group systems were discovered by Karl Landsteiner during early experiments with blood transfusion: the ABO group in 1901 and in co-operation with Alexander S. Wiener the Rhesus group in 1937. Development of the Coombs test in 1945,
the advent of transfusion medicine, and the understanding of ABO hemolytic disease of the newborn led to discovery of more blood groups, and now 33 human blood group systems are recognized by the International Society of Blood Transfusion (ISBT), and across the 33 blood groups, over 600 different blood group antigens have been found; many of these are very rare or are mainly found in certain ethnic groups.
A Czech serologist Jan Janský is credited with the first classification of blood into the four types (A, B, AB, O) in 1907, which remains in use today. Blood types have been used in forensic science and were formerly used to demonstrate impossibility of paternity (e.g., a type AB man cannot be the father of a type O infant), but both of these uses are being replaced by genetic fingerprinting, which provides greater certainty.
Society and culture.
A popular belief in Japan is that a person's ABO blood type is predictive of their personality, character, and compatibility with others. This belief is also widespread in South Korea and Taiwan. Deriving from ideas of historical scientific racism, the theory reached Japan in a 1927 psychologist's report, and the militarist government of the time commissioned a study aimed at breeding better soldiers. The fad faded in the 1930s due to its lack of scientific basis and ultimately the discovery of DNA in the following decades which it later became clear had a vastly more complex and important role in both heredity generally and personality specifically. No evidence has been found to support the theory by scientists, but it was revived in the 1970s by Masahiko Nomi, a broadcaster with a background in law who had no scientific or medical background. On the contrary, some studies suggest statistically significant relationships. Despite these facts, the myth still persists widely in Japanese and South Korean popular culture.

</doc>
<doc id="55311" url="http://en.wikipedia.org/wiki?curid=55311" title="Seattle Wireless">
Seattle Wireless

Seattle Wireless is an American non-profit project created by Matt Westervelt and Ken Caruso in June 2000. It seeks to develop a free, locally-owned wireless community network using widely available, license-free technology wireless broadband Internet access. It is a metropolitan area network.
Seattle Wireless is one of the first Community Wireless Networks and one of the first project focused wikis. It also had a short lived (7 episode) online television show, prior to the recent surge in videocasting and podcasting, called . It was created by Peter Yorke and Michael Pierce and ran July 2003 - June 2004. SWTV was an early adopter of Bittorrent to distribute its shows. 

</doc>
<doc id="55312" url="http://en.wikipedia.org/wiki?curid=55312" title="United Federation of Planets">
United Federation of Planets

The United Federation of Planets, usually referred to as "the Federation", is an interstellar federal republic composed of planetary sovereignties depicted in the "Star Trek" science fiction franchise. The planetary governments agree to exist semi-autonomously under a single central government based on the Utopian principles of universal liberty, rights, and equality, and to share their knowledge and resources in peaceful cooperation and space exploration.
In those episodes and films, the Federation is described as an interstellar federal polity with, as of the year 2373, more than 150 member planets and thousands of colonies spread across some 6,000,000 cubic light years of the Milky Way Galaxy taking the form of a post-capitalist libertarianism and constitutional republic. The social structure within the Federation is classless and operates within a moneyless "New World Economy". The Federation is described as stressing, at least nominally, the values of universal liberty, equality, justice, peace, and cooperation. The Federation also maintains its own quasi-militaristic and scientific exploratory agency, known as Starfleet (also written as "Star Fleet" in some texts). Starfleet is seen handling many other governmental processes, sometimes with no other agency's influence, such as border defense, diplomatic envoy and has seen extensive use as an offensive military force.
The television series and films depict Earth and humanity as holding a center-stage political role within the Federation, in some ways first among equals. The legislature, the Federation Council, is located at the Presidio of San Francisco. Several other bodies of the Federation have been depicted. There is an executive branch headed by a Federation President, who keeps offices in the Palais de la Concorde in Paris. There is a judiciary branch as well, the highest court of which is the Federation Supreme Court. The Federation's scientific, diplomatic and defensive/military arm is Starfleet, depicted as being headquartered at Fort Baker, just north of San Francisco across the Golden Gate Bridge. The Federation comes into military conflict with other major powers in the galaxy such as the Klingon Empire, the Romulan Star Empire, the Cardassian Union, the Borg, and the Dominion.
The United Federation of Planets has existed as part of the "Star Trek" universe since the first season of the and is the primary focus of all the "Star Trek" series. Several episodes of "" follow events leading up the creation of the Federation, with the final episode featuring the signing of the Federation Charter.
Conception.
The first mention of the United Federation of Planets was in the 1967 episode "A Taste of Armageddon", although other vague references such as just "the Federation" or to the "United Earth Space Probe Agency" were used in prior episodes. As part of the anti-war message he wanted the show to convey, "Star Trek" creator Gene Roddenberry intended to depict the Federation as if it was like an ideal, optimistic version of the United Nations. In several following episodes of the original series that were intended as allegories to the then-current Cold War tensions, the Federation took on the role resembling NATO while the Klingons represented the Soviet Union.
Depiction.
In the series ", Earth Minister Nathan Samuels advocated the Coalition of Planets and invited other alien species, initially the Vulcans, Andorians and Tellarites, to become a part of this. The formation of the Coalition seems to have been the event that provoked the xenophobic Terra Prime incident in the episodes " and "Terra Prime". After Terra Prime leader John Frederick Paxton exploited the xenophobia on Earth, many of the aliens were unnerved and nearly abandoned the idea of a coalition. However, they were convinced by a speech from Captain Jonathan Archer to give the idea of a united organization of worlds a chance. Six years later in 2161, the United Federation of Planets was organized.
The Federation is founded under a document known as the Charter of the United Federation of Planets October 9, 2161, which is occasionally referred to informally as the "Constitution". It draws text and inspiration from the United Nations Charter and other sources. An important guiding principle — indeed, it is listed as General Order One in the list of Starfleet general orders — is the Prime Directive, which forbids any interference in the natural development of any pre-warp civilization. This is intended to prevent even well-intentioned Federation personnel from introducing changes which could destabilize or even destroy other pre-warp-era cultures through interference. In practice, however, consistent application of the Prime Directive tends to be a controversial issue, and the Federation does not always abide strictly by it, such as when it attempted to strongarm the Organians into forming an alliance with it, or when it initially approved the forced relocation of the Ba'ku from their adopted homeworld—although it was eventually determined that the Ba'ku were not a pre-warp civilization. Starfleet's Omega Directive supersedes the Prime Directive allowing for any means possible to destroy the Omega particle if encountered. Other aspects of the Articles provide for rule of law, equality among individuals and protection of civil and creative liberties, which appears to be based on principles found in contemporary Western political theory. It includes a set of guarantees of civil rights, the "Seventh Guarantee" being analogous to the Fifth Amendment to the United States Constitution and its protection against self-incrimination.
The Federation also has its own semi-independent black ops agency, referred to only as "Section 31". It can be considered analogous to the Romulan Star Empire's "Tal Shiar" and the Cardassian Union's "Obsidian Order".
The Federation has exacting requirements for prospective member worlds that wish to join. Caste-based discrimination is forbidden, and major systematic violations of sentient rights, such as the unjust peacetime imprisonment of specially modified soldiers on the planet Angosia, are not tolerated for any petitioner. Furthermore, while most member worlds have single, unified world governments, it is not required for entry, as the Federation will consider "associate membership" of non-unified worlds.
Government.
The government of the United Federation of Planets consists of the central government, the Federation, and planetary governments who share joint-sovereignty with the central government (as with the United States). The chief of state or chief of government of most planets are referred to as Governor, Prime Minister, or First Minister. The central government is composed of the Office of the President, the President's Cabinet, the Federation Council which is composed of an equal number of representatives from each member planet, and the Federation Supreme Court.
President of the United Federation of Planets.
The President of the United Federation of Planets (informally, the Federation President or the President of the Federation) is the elected head of state and head of government of the United Federation of Planets. He or she is responsible for the day-to-day operation of the government, setting and coordinating foreign policy, and dealing with resource distribution issues. The Federation President is also the commander-in-chief of all Starfleet forces.
The President is supported by the Cabinet, a special committee composed of the heads of the executive departments of the Federation government as mentioned in "".
The Federation President's office is located on Earth in the city of Paris, France.
Federation Council.
The Federation Council is the legislature of the United Federation of Planets. Seats on the Council are filled by representatives from the various Federation Member Worlds. In addition to legislation, only the Federation Council may declare war and frequently passes resolutions which the Federation President and his or her staff must carry out and enforce.
The make-up and location of the Council is somewhat vague and open to interpretation based upon canonical evidence.
Federation Supreme Court.
The Federation Supreme Court is the highest court in the Federation headed by an elected Chief Justice. The Federation Supreme Court was first mentioned in "Doctor Bashir, I Presume?".
Fictional history of the Federation.
21st century.
After the end of World War III on Earth, scientist Zefram Cochrane built Earth's first warp-capable vessel, the "Phoenix". He launched it on April 5, 2063. The warp-testing of this vessel would garner the attention of a Vulcan science ship operating just outside of the Solar System. Vulcans had not previously considered the Solar System of Earth, or Earth itself, worthy of their attention before this time. However, the science ship lands on Earth, and makes first-contact with Zefram Cochrane and the inhabitants of Bozeman, Montana. This contact would be the first time that Earth joins the interstellar community, and begins the road toward the foundation of the United Federation of Planets.
22nd-23rd centuries.
In the year 2119, an aging Zefram Cochrane opens the Warp 5 Complex on Earth, in the hope of building a vessel that would be the fastest human starship at the time. Eventually this project would yield the "Enterprise NX-01", Earth's first deep-space exploration vessel. In 2150, a World Government, "United Earth", was formed that included virtually all of the old nations on Earth.
Although no single individual is responsible for the foundation of the United Federation of Planets, the exploratory vessel "Enterprise NX-01" was a major catalyst. Under the command of Captain Jonathan Archer, it helped forge an alliance between the formerly belligerent Vulcan, Andorian, and Tellarite states, and forged a spirit of unity and cooperation in the Alpha Quadrant, culminating in a formal union in 2161. It was first preceded by the Coalition of Planets, which was mainly opposed by the xenophobic group, Terra Prime. The Federation was formed largely out of the aftermath of the Earth-Romulan War of the late 2150s ending in 2160, when the founding members saw the need for interstellar unity to prevent the horror of further war. Archer was one of the individuals who signed the Federation Charter, after giving a historic speech that was still being studied two centuries later. According to information seen on a viewscreen in a late episode of "", Jonathan Archer later became the Federation ambassador to Andoria, a Federation Councillor, and President of the United Federation of Planets from 2184 to 2192.
Around 2223, tensions thickened between the UFP and the Klingon Empire. In 2267, the Organian Peace Treaty was signed which ended major engagements, but the two interstellar powers remained in a state of cold war with occasional skirmishes over the next couple of decades. In 2293 the Klingons sued for peace after the destruction of the Klingon moon Praxis, leading eventually to the signing of the Khitomer Accords (the events depicted in "). This effectively ended the war and ushered in seven decades of relative peace.
During the era of , Captain James Kirk once noted (in the episode ") that humanity was on "a thousand planets and spreading out"; however, this number apparently encompasses Earth's many off-Earth colonies and the various alien worlds on which humans can be found (just as non-humans have been depicted as residing on Earth) and should not be taken to mean that the Federation itself had a thousand members at that time. Considering that many of the Federation's other members have several interplanetary colonies just as Earth does, the full number of planets which the Federation encompasses may be impossible to determine; it is presumed that colony worlds are directly subsidiary to the planetary governments of their homeworlds (much like individual states/provinces in a nation), but this has never been clearly established.
Early 24th century.
In 2311, the Tomed Incident occurred in which thousands of Federation civilians and Starfleet personnel were killed by Romulan forces. The unrest was ended by the Treaty of Algeron, which re-affirmed the Neutral Zone and prohibited Federation development of cloaking technology.
In 2344, the Romulan Star Empire launched an assault on the Klingon outpost at Narendra III, but unexpectedly the USS "Enterprise"-C, under the command of Captain Rachel Garrett, came to the Klingons' defense. This "Enterprise" was destroyed in the skirmish, a sacrifice which did great honor to the Klingons, and the burgeoning diplomacy between the two powers soon grew into a formal alliance. (In an alternate timeline, the "Enterprise"-C did not so assist, leading eventually to a full-scale war.)
Exploration and expansion in the 2340s and 2350s brought the Federation into conflict with several minor and major powers including the Talarians, the Sheliak and, eventually, the Cardassians.
Cardassian War.
Federation contact with a race called the Cardassians resulted in an extended conflict. One incident in this conflict was the massacre of Federation civilians on Setlik III in 2347. A truce was reached in the 2360s and a Demilitarized Zone was formed in 2370. A number of Federation and Cardassian colonies found themselves situated within the other’s territory; an agreement was reached for the transfer of those colonies. However, some Federation colonists were opposed to the agreement and formed the Maquis, a rebel movement who resisted the Cardassians.
Mid-24th century.
In 2365, the Federation had first formal contact with the Borg Collective, who threatened the existence of the Federation at the Battle of Wolf 359. Other events of this era include the Klingon Civil War, first contact with the Q, the beginning of relations with the Ferengi and various time travel incidents.
From 2363 to 2371, the USS "Enterprise"-D served as the Federation's flagship.
From 2371-2378, the USS "Voyager" NCC-74656 was lost in the Delta Quadrant after being taken in the Badlands by the Caretaker's Array.
From 2373 to 2375, the Federation fought in the Dominion War. This was by far the largest conflict the Federation had ever been involved in, allying initially with the Klingons, and at a later time in the conflict, the Romulans against the combined forces of the Dominion, the Cardassians, and Breen. The Federation/Klingon/Romulan alliance was victorious, due in no small part to the Cardassians switching sides in the war after some of its officials realized that the Dominion had bloodlessly conquered them, but with substantial casualties on both sides.
In 2379, a Reman Praetor named Shinzon seized control of the Romulan Star Empire. The coup was defeated by the crew of the USS "Enterprise"-E with assistance from dissidents within the Romulan military, opening up the possibility of improved UFP/Romulan relations after over two centuries of general tension. However, this improved relationship came at a cost, as the death of Shinzon may have created a power vacuum.
Future history.
Prominent in some timelines is the Temporal Cold War, waged on a number of fronts throughout time including the 28th and 31st centuries.
By the 29th century, the Federation explores time as it once did space.
Alternate timeline (2009 and 2013 "Star Trek" films).
As depicted in the "Star Trek: Countdown" comic series, in 2387 the star of the Hobus system went supernova and posed a serious threat to the Romulan Star Empire. Ambassador Spock formulated a plan involving red matter to halt the Hobus supernova; saving billions of lives and preventing the political destabilization of the Alpha and Beta Quadrants. However, they did not act soon enough to save Romulus from being destroyed. An augmented Romulan mining ship called the "Narada" captained by Nero attacked the ship in which Spock traveled as Nero blames the Federation (Ambassador Spock in particular) for the destruction of his homeworld and for the death of his wife and child. During the attack, both ships are pulled into the singularity and transported into the past; the appearance of the "Narada" (which arrives farther into the past than Spock's ship) and its subsequent attack on the USS "Kelvin" creates an alternate timeline depicted in the 2009 "Star Trek" film. As depicted in the 2013 sequel "Star Trek Into Darkness", the Federation in this era is on the brink of war with the Klingon Empire just as its counterpart in the prime reality was.
Economics.
The Federation has been portrayed as an economic utopia. In the ' episode "Dark Frontier", Tom Paris describes it as the "New World Economy", which began in the late 22nd century and eventually made money obsolete, as does Jean-Luc Picard while explaining the timeline to Lily Sloane in '.
The first mention of the Federation "not" using money came in ', where Kirk (coming from 2286) says "these people still use money" upon arriving at 20th-century Earth, and says "We don't." when asked whether or not he and his crew use money in the 23rd century. In "", Picard tries to explain to cryogenically preserved people from the late 20th century that 24th century economics are quite different and money as they know it is not used or needed in the Federation. In ', he gives a similar speech to Lily.
In other episodes, especially earlier in the in-universe timespan, a monetary unit known as the "credit" is mentioned. At the Federation space station K-7 in the original series episode "The Trouble with Tribbles", set in 2267, Uhura offers to buy a Tribble for 10 credits. In the episode "Errand of Mercy", also set in 2267, Spock estimates that Starfleet has invested over 122,200 credits in his training as a Starfleet officer. In ', in 2285, while on Earth, McCoy attempts to hire a ship to take him to the Genesis Planet, and is warned it would be expensive and cost many credits; we do not know if McCoy could have afforded this or how much it would cost, since he was taken into custody for breaching the secrecy of the Genesis Project immediately afterwards. And in ', Carol Marcus mentions the Federation's decision whether or not to "fund" the Genesis Project itself, though "fund" means something different in this context as credits are not mentioned. In the "Deep Space Nine" episode "You Are Cordially Invited...", Jake Sisko tells Quark he sold his first book, but when Quark asks him how much was gotten for it, Jake answers, "It's just a figure of speech." This explains moments when characters have made similar comments (in "", for instance, when Scotty mentioned having "bought" a boat).
During the film "Star Trek Generations", Captain Kirk states that he sold his house some time in the previous nine years, which from Kirk's perspective would be between 2284 and roughly 2290. By the time of ', money was considered abhorrent to many members of Starfleet, although in "Encounter at Farpoint", set in 2364, Beverly Crusher buys a bolt of fabric and requests that it be charged to her account on the "Enterprise". Two years later, in 2366, in ", the Federation is willing to pay millions of credits for access to a stable wormhole. Additionally, some officers were shown in " to visit casinos, particularly near starbases, and poker is shown on a number of occasions to be a favorite pastime of "Enterprise"-D crewmembers, though real money is never said to be part of the game. In the ' episode "", Benjamin Sisko says that when he first entered Starfleet Academy, he rapidly spent an entire month's allotment of transporter credits (which may not be the same thing as 23rd-century credits) on transporting back and forth to his home in New Orleans. He also arranges for his wife's employer to give her a month's "paid" vacation (emphasis in episode) in "The Changing Face of Evil" (although his wife works for the Bajorans, a non-Federation race). And in the pilot episode of "", Tom Paris makes a reference to having someone "pay his bar bills".
Non-canon.
In many non-canon sources like "Star Trek Star Fleet Technical Manual" and "Worlds of the Federation", as well as the and role-playing games, the five founding worlds of the United Federation of Planets were Earth, Vulcan, Tellar, Andoria, and Alpha Centauri. Alpha Centauri being a founding world of the Federation and even having a humanlike native race called Centaurans became a popular fan theory, possibly based on uncertainty as to whether or not Zefram Cochrane (described in "" as "Zefram Cochrane of Alpha Centauri") was a native of Alpha Centauri or a resident of a human colony in that system; the latter has since been revealed to be the case, Cochrane having spent most of his life on Earth but eventually retiring to spend his final years on Alpha Centauri, before his disappearance and presumed death.
The once official, but now non-canon "Star Trek Spaceflight Chronology 1980 - 2188" guide states that the UFP was "incorporated at the first Babel Interplanetary Conference" in the year 2087.
Later, in " the actual founding of the Federation can be seen in the episodes " and "These Are the Voyages...", and early negotiations that lead to it in "" and "Terra Prime". Alpha Centauri is not mentioned as part of the founding, which is explicitly said to be between Humans, Vulcans, Andorians, and Tellarites. This leaves open the possibility of the Alpha Centauri colony becoming an independent polity some time between "Terra Prime" and "These Are the Voyages...", and then helping to form the Federation as a separate member. However, Alpha Centauri is only ever mentioned in passing as an Earth colony on screen. In the alternate timeline seen in the "DS9" episode "", where the Federation was never formed, Alpha Centauri is under Romulan control instead.
In the novels "A Time to Kill", "A Time to Heal", "A Time For War", "A Time For Peace", "Errand of Vengeance: Seeds of Rage", and "Articles of the Federation", the Federation Council was shown occupying the floors below the President's office in the Palais de la Concorde. This may be seen as contradicting elements of ' and '.
In some non-canonical works like "Star Trek Star Fleet Technical Manual" and the novel "Articles of the Federation", the Federation's founding document is called the "Articles Of Federation", which has been popular fan tradition. However, in the "Star Trek: Voyager" episode "", the text of the founding document is shown on screen (the preamble is a slightly reworded version of the UN Charter), and it is clearly called the "Charter of the United Federation of Planets", canonically establishing that as the name of the founding document. The term "charter" is also used in ' and in the ' episode "Accession", when discussing membership requirements for the Federation. That latter episode seemed to indicate that the timetable for a world's entry into the Federation is ten years after the request is made, although the Federation was willing to cut that time in half for Bajor in that episode, and has similarly made other exceptions for times of war, as seen in '. In the ' episode "The Drumhead", Captain Picard refers to the founding document in passing as "the Constitution", establishing that it is also known by that name. Novels such as "Articles of the Federation" presume that it is known by all three names.
The novels have also gone into more detail about inner workings of Federation government, such as how member worlds choose their Councillors (it is up to each world to decide how to do so) and how a President is elected (candidates submit their names anonymously, and are vetted by the Federation Council which determines if they are qualified to run; actual election is done by direct popular vote).

</doc>
<doc id="55313" url="http://en.wikipedia.org/wiki?curid=55313" title="Allergy">
Allergy

An allergy is a hypersensitivity disorder of the immune system. Symptoms include red eyes, itchiness, and runny nose, eczema, hives, or an asthma attack. Allergies can play a major role in conditions such as asthma. In some people, severe allergies to environmental or dietary allergens or to medication may result in life-threatening reactions called anaphylaxis. Food allergies and reactions to the venom of stinging insects such as wasps and bees are more often associated with these severe reactions. Not all reactions or intolerances are forms of allergy.
Allergic reactions occur when a person's immune system reacts to normally harmless substances in the environment. A substance that causes a reaction is called an allergen. These reactions are acquired, predictable, and rapid. Allergy is one of four forms of hypersensitivity and is formally called "type I" (or "immediate") hypersensitivity. Allergic reactions are distinctive because of excessive activation of certain white blood cells called mast cells and basophils by a type of antibody called immunoglobulin E (IgE). This reaction results in an inflammatory response which can range from uncomfortable to dangerous.
A variety of tests exist to diagnose allergic conditions. If done they should be ordered and interpreted in light of a person's history of exposure as many positive test results do not mean a clinically significant allergy. Tests include placing possible allergens on the skin and looking for a reaction such as swelling and blood tests to look for an allergen-specific IgE.
Treatments for allergies include avoiding known allergens, steroids that modify the immune system in general, and medications such as antihistamines and decongestants which reduce symptoms. Many of these medications are taken by mouth, although epinephrine, which is used to treat anaphylactic reactions, is injected. Immunotherapy uses injected allergens to desensitize the body's response. Mild allergies like hay fever are very common.
Signs and symptoms.
Many allergens such as dust or pollen are airborne particles. In these cases, symptoms arise in areas in contact with air, such as eyes, nose, and lungs. For instance, allergic rhinitis, also known as hay fever, causes irritation of the nose, sneezing, itching, and redness of the eyes. Inhaled allergens can also lead to asthmatic symptoms, caused by narrowing of the airways (bronchoconstriction) and increased production of mucus in the lungs, shortness of breath (dyspnea), coughing and wheezing.
Aside from these ambient allergens, allergic reactions can result from foods, insect stings, and reactions to medications like aspirin and antibiotics such as penicillin. Symptoms of food allergy include abdominal pain, bloating, vomiting, diarrhea, itchy skin, and swelling of the skin during hives. Food allergies rarely cause respiratory (asthmatic) reactions, or rhinitis. Insect stings, antibiotics, and certain medicines produce a systemic allergic response that is also called anaphylaxis; multiple organ systems can be affected, including the digestive system, the respiratory system, and the circulatory system. Depending on the rate of severity, it can cause cutaneous reactions, bronchoconstriction, edema, hypotension, coma, and even death. This type of reaction can be triggered suddenly, or the onset can be delayed. The severity of this type of allergic response often requires injections of epinephrine, sometimes through a device known as the EpiPen or Twinject auto-injector. The nature of anaphylaxis is such that the reaction can seem to be subsiding, but may recur throughout a prolonged period of time.
Substances that come into contact with the skin, such as latex, are also common causes of allergic reactions, known as contact dermatitis or eczema. Skin allergies frequently cause rashes, or swelling and inflammation within the skin, in what is known as a "wheal and flare" reaction characteristic of hives and angioedema.
Cause.
Risk factors for allergy can be placed in two general categories, namely host and environmental factors. Host factors include heredity, sex, race, and age, with heredity being by far the most significant. However, there have been recent increases in the incidence of allergic disorders that cannot be explained by genetic factors alone. Four major environmental candidates are alterations in exposure to infectious diseases during early childhood, environmental pollution, allergen levels, and dietary changes.
Foods.
A wide variety of foods can cause allergic reactions, but 90% of allergic responses to foods are caused by cow's milk, soy, eggs, wheat, peanuts, tree nuts, fish, and shellfish. Other food allergies, affecting less than 1 person per 10,000 population, may be considered "rare".
The most common food allergy in the US population is a sensitivity to crustacea. Although peanut allergies are notorious for their severity, peanut allergies are not the most common food allergy in adults or children. Severe or life-threatening reactions may be triggered by other allergens, and are more common when combined with asthma.
Rates of allergies differ between adults and children. Peanut allergies can sometimes be outgrown by children. Egg allergies affect one to two percent of children but are outgrown by about two-thirds of children by the age of 5. The sensitivity is usually to proteins in the white, rather than the yolk.
Milk-protein allergies are most common in children. Approximately 60% of milk-protein reactions are immunoglobulin E-mediated, with the remaining usually attributable to inflammation of the colon. Some people are unable to tolerate milk from goats or sheep as well as from cows, and many are also unable to tolerate dairy products such as cheese. Roughly 10% of children with a milk allergy will have a reaction to beef. Beef contains a small amount of protein that is present in cow's milk. Lactose intolerance, a common reaction to milk, is not a form of allergy at all, but rather due to the absence of an enzyme in the digestive tract.
Those with tree nut allergies may be allergic to one or to many tree nuts, including pecans, pistachios, pine nuts, and walnuts. Also seeds, including sesame seeds and poppy seeds, contain oils in which protein is present, which may elicit an allergic reaction.
Balsam of Peru, which is in various foods, is in the "top five" allergens most commonly causing patch test reactions in people referred to dermatology clinics.
Allergens can be transferred from one food to another through genetic engineering; however genetic modification can also remove allergens. Little research has been done on the natural variation of allergen concentrations in the unmodified crops.
Non-food proteins.
Latex can trigger an IgE-mediated cutaneous, respiratory, and systemic reaction. The prevalence of latex allergy in the general population is believed to be less than one percent. In a hospital study, 1 in 800 surgical patients (0.125 percent) reported latex sensitivity, although the sensitivity among healthcare workers is higher, between seven and ten percent. Researchers attribute this higher level to the exposure of healthcare workers to areas with significant airborne latex allergens, such as operating rooms, intensive-care units, and dental suites. These latex-rich environments may sensitize healthcare workers who regularly inhale allergenic proteins.
The most prevalent response to latex is an allergic contact dermatitis, a delayed hypersensitive reaction appearing as dry, crusted lesions. This reaction usually lasts 48–96 hours. Sweating or rubbing the area under the glove aggravates the lesions, possibly leading to ulcerations. Anaphylactic reactions occur most often in sensitive patients who have been exposed to a surgeon's latex gloves during abdominal surgery, but other mucosal exposures, such as dental procedures, can also produce systemic reactions.
Latex and banana sensitivity may cross-react. Furthermore, those with latex allergy may also have sensitivities to avocado, kiwifruit, and chestnut. These patients often have perioral itching and local urticaria. Only occasionally have these food-induced allergies induced systemic responses. Researchers suspect that the cross-reactivity of latex with banana, avocado, kiwifruit, and chestnut occurs because latex proteins are structurally homologous with some other plant proteins.
Toxins interacting with proteins.
Another non-food protein reaction, urushiol-induced contact dermatitis, originates after contact with poison ivy, eastern poison oak, western poison oak, or poison sumac. Urushiol, which is not itself a protein, acts as a hapten and chemically reacts with, binds to, and changes the shape of integral membrane proteins on exposed skin cells. The immune system does not recognize the affected cells as normal parts of the body, causing a T-cell-mediated immune response. Of these poisonous plants, sumac is the most virulent. The resulting dermatological response to the reaction between urushiol and membrane proteins includes redness, swelling, papules, vesicles, blisters, and streaking.
Estimates vary on the percentage of the population that will have an immune system response. Approximately 25 percent of the population will have a strong allergic response to urushiol. In general, approximately 80 percent to 90 percent of adults will develop a rash if they are exposed to .0050 mg of purified urushiol, but some people are so sensitive that it takes only a molecular trace on the skin to initiate an allergic reaction.
Genetic basis.
Allergic diseases are strongly familial: identical twins are likely to have the same allergic diseases about 70% of the time; the same allergy occurs about 40% of the time in non-identical twins. Allergic parents are more likely to have allergic children, and those children's allergies are likely to be more severe than those in children of non-allergic parents. Some allergies, however, are not consistent along genealogies; parents who are allergic to peanuts may have children who are allergic to ragweed. It seems that the likelihood of developing allergies is inherited and related to an irregularity in the immune system, but the specific allergen is not.
The risk of allergic sensitization and the development of allergies varies with age, with young children most at risk. Several studies have shown that IgE levels are highest in childhood and fall rapidly between the ages of 10 and 30 years. The peak prevalence of hay fever is highest in children and young adults and the incidence of asthma is highest in children under 10.
Overall, boys have a higher risk of developing allergies than girls, although for some diseases, namely asthma in young adults, females are more likely to be affected. These differences between the sexes tend to decrease in adulthood.
Ethnicity may play a role in some allergies; however, racial factors have been difficult to separate from environmental influences and changes due to migration. It has been suggested that different genetic loci are responsible for asthma, to be specific, in people of European, Hispanic, Asian, and African origins.
Hygiene hypothesis.
Allergic diseases are caused by inappropriate immunological responses to harmless antigens driven by a TH2-mediated immune response. Many bacteria and viruses elicit a TH1-mediated immune response, which down-regulates TH2 responses. The first proposed mechanism of action of the hygiene hypothesis was that insufficient stimulation of the TH1 arm of the immune system leads to an overactive TH2 arm, which in turn leads to allergic disease. In other words, individuals living in too sterile an environment are not exposed to enough pathogens to keep the immune system busy. Since our bodies evolved to deal with a certain level of such pathogens, when they are not exposed to this level, the immune system will attack harmless antigens and thus normally benign microbial objects — like pollen — will trigger an immune response.
The hygiene hypothesis was developed to explain the observation that hay fever and eczema, both allergic diseases, were less common in children from larger families, which were, it is presumed, exposed to more infectious agents through their siblings, than in children from families with only one child. The hygiene hypothesis has been extensively investigated by immunologists and epidemiologists and has become an important theoretical framework for the study of allergic disorders. It is used to explain the increase in allergic diseases that have been seen since industrialization, and the higher incidence of allergic diseases in more developed countries. The hygiene hypothesis has now expanded to include exposure to symbiotic bacteria and parasites as important modulators of immune system development, along with infectious agents.
Epidemiological data support the hygiene hypothesis. Studies have shown that various immunological and autoimmune diseases are much less common in the developing world than the industrialized world and that immigrants to the industrialized world from the developing world increasingly develop immunological disorders in relation to the length of time since arrival in the industrialized world. Longitudinal studies in the third world demonstrate an increase in immunological disorders as a country grows more affluent and, it is presumed, cleaner. The use of antibiotics in the first year of life has been linked to asthma and other allergic diseases. The use of antibacterial cleaning products has also been associated with higher incidence of asthma, as has birth by Caesarean section rather than vaginal birth.
Other environmental factors.
International differences have been associated with the number of individuals within a population have allergy. Allergic diseases are more common in industrialized countries than in countries that are more traditional or agricultural, and there is a higher rate of allergic disease in urban populations versus rural populations, although these differences are becoming less defined.
Alterations in exposure to microorganisms is another plausible explanation, at present, for the increase in atopic allergy. Endotoxin exposure reduces release of inflammatory cytokines such as TNF-α, IFNγ, interleukin-10, and interleukin-12 from white blood cells (leukocytes) that circulate in the blood. Certain microbe-sensing proteins, known as Toll-like receptors, found on the surface of cells in the body are also thought to be involved in these processes.
Gutworms and similar parasites are present in untreated drinking water in developing countries, and were present in the water of developed countries until the routine chlorination and purification of drinking water supplies. Recent research has shown that some common parasites, such as intestinal worms (e.g., hookworms), secrete chemicals into the gut wall (and, hence, the bloodstream) that suppress the immune system and prevent the body from attacking the parasite. This gives rise to a new slant on the hygiene hypothesis theory — that co-evolution of man and parasites has led to an immune system that functions correctly only in the presence of the parasites. Without them, the immune system becomes unbalanced and oversensitive. In particular, research suggests that allergies may coincide with the delayed establishment of gut flora in infants. However, the research to support this theory is conflicting, with some studies performed in China and Ethiopia showing an increase in allergy in people infected with intestinal worms. Clinical trials have been initiated to test the effectiveness of certain worms in treating some allergies. It may be that the term 'parasite' could turn out to be inappropriate, and in fact a hitherto unsuspected symbiosis is at work. For more information on this topic, see Helminthic therapy.
Pathophysiology.
Acute response.
In the early stages of allergy, a type I hypersensitivity reaction against an allergen encountered for the first time and presented by a professional antigen-presenting cell causes a response in a type of immune cell called a TH2 lymphocyte, which belongs to a subset of T cells that produce a cytokine called interleukin-4 (IL-4). These TH2 cells interact with other lymphocytes called B cells, whose role is production of antibodies. Coupled with signals provided by IL-4, this interaction stimulates the B cell to begin production of a large amount of a particular type of antibody known as IgE. Secreted IgE circulates in the blood and binds to an IgE-specific receptor (a kind of Fc receptor called FcεRI) on the surface of other kinds of immune cells called mast cells and basophils, which are both involved in the acute inflammatory response. The IgE-coated cells, at this stage, are sensitized to the allergen.
If later exposure to the same allergen occurs, the allergen can bind to the IgE molecules held on the surface of the mast cells or basophils. Cross-linking of the IgE and Fc receptors occurs when more than one IgE-receptor complex interacts with the same allergenic molecule, and activates the sensitized cell. Activated mast cells and basophils undergo a process called degranulation, during which they release histamine and other inflammatory chemical mediators (cytokines, interleukins, leukotrienes, and prostaglandins) from their granules into the surrounding tissue causing several systemic effects, such as vasodilation, mucous secretion, nerve stimulation, and smooth muscle contraction. This results in rhinorrhea, itchiness, dyspnea, and anaphylaxis. Depending on the individual, allergen, and mode of introduction, the symptoms can be system-wide (classical anaphylaxis), or localized to particular body systems; asthma is localized to the respiratory system and eczema is localized to the dermis.
Late-phase response.
After the chemical mediators of the acute response subside, late-phase responses can often occur. This is due to the migration of other leukocytes such as neutrophils, lymphocytes, eosinophils and macrophages to the initial site. The reaction is usually seen 2–24 hours after the original reaction. Cytokines from mast cells may play a role in the persistence of long-term effects. Late-phase responses seen in asthma are slightly different from those seen in other allergic responses, although they are still caused by release of mediators from eosinophils and are still dependent on activity of TH2 cells.
Allergic contact dermatitis.
Although allergic contact dermatitis is termed an "allergic" reaction (which usually refers to type I hypersensitivity), its pathophysiology actually involves a reaction that more correctly corresponds to a type IV hypersensitivity reaction. In type IV hypersensitivity, there is activation of certain types of T cells (CD8+) that destroy target cells on contact, as well as activated macrophages that produce hydrolytic enzymes.
Diagnosis.
Effective management of allergic diseases relies on the ability to make an accurate diagnosis. Allergy testing can help confirm or rule out allergies. Correct diagnosis, counseling, and avoidance advice based on valid allergy test results reduces the incidence of symptoms and need for medications, and improves quality of life. To assess the presence of allergen-specific IgE antibodies, two different methods can be used: a skin prick test, or an allergy blood test. Both methods are recommended, and they have similar diagnostic value.
Skin prick tests and blood tests are equally cost-effective, and health economic evidence shows that both tests were cost-effective compared with no test. Also, early and more accurate diagnoses save cost due to reduced consultations, referrals to secondary care, misdiagnosis, and emergency admissions.
Allergy undergoes dynamic changes over time. Regular allergy testing of relevant allergens provides information on if and how patient management can be changed, in order to improve health and quality of life. Annual testing is often the practice for determining whether allergy to milk, egg, soy, and wheat have been outgrown, and the testing interval is extended to 2–3 years for allergy to peanut, tree nuts, fish, and crustacean shellfish. Results of follow-up testing can guide decision-making regarding whether and when it is safe to introduce or re-introduce allergenic food into the diet.
Skin prick testing.
 Skin testing is also known as "puncture testing" and "prick testing" due to the series of tiny punctures or pricks made into the patient's skin. Small amounts of suspected allergens and/or their extracts ("e.g.", pollen, grass, mite proteins, peanut extract) are introduced to sites on the skin marked with pen or dye (the ink/dye should be carefully selected, lest it cause an allergic response itself). A small plastic or metal device is used to puncture or prick the skin. Sometimes, the allergens are injected "intradermally" into the patient's skin, with a needle and syringe. Common areas for testing include the inside forearm and the back.
If the patient is allergic to the substance, then a visible inflammatory reaction will usually occur within 30 minutes. This response will range from slight reddening of the skin to a full-blown hive (called "wheal and flare") in more sensitive patients similar to a mosquito bite. Interpretation of the results of the skin prick test is normally done by allergists on a scale of severity, with +/- meaning borderline reactivity, and 4+ being a large reaction. Increasingly, allergists are measuring and recording the diameter of the wheal and flare reaction. Interpretation by well-trained allergists is often guided by relevant literature. Some patients may believe they have determined their own allergic sensitivity from observation, but a skin test has been shown to be much better than patient observation to detect allergy.
If a serious life-threatening anaphylactic reaction has brought a patient in for evaluation, some allergists will prefer an initial blood test prior to performing the skin prick test. Skin tests may not be an option if the patient has widespread skin disease, or has taken antihistamines in the last several days.
Patch testing.
Patch testing is a method used to determine if a specific substance causes allergic inflammation of the skin. It tests for delayed reactions. It is used to help ascertain the cause of skin contact allergy, or contact dermatitis. Adhesive patches, usually treated with a number of common allergic chemicals or skin sensitizers, are applied to the back. The skin is then examined for possible local reactions at least twice, usually at 48 hours after application of the patch, and again two or three days later.
Blood testing.
An allergy blood test is quick and simple, and can be ordered by a licensed health care provider ("e.g.", an allergy specialist), GP, or PED. Unlike skin-prick testing, a blood test can be performed irrespective of age, skin condition, medication, symptom, disease activity, and pregnancy. Adults and children of any age can take an allergy blood test. For babies and very young children, a single needle stick for allergy blood testing is often more gentle than several skin tests.
An allergy blood test is available through most laboratories. A sample of the patient’s blood is sent to a laboratory for analysis, and the results are sent back a few days later. Multiple allergens can be detected with a single blood sample. Allergy blood tests are very safe, since the person is not exposed to any allergens during the testing procedure.
The test measures the concentration of specific IgE antibodies in the blood. Quantitative IgE test results increase the possibility of ranking how different substances may affect symptoms. A general rule of thumb is that the higher the IgE antibody value, the greater the likelihood of symptoms. Allergens found at low levels that today do not result in symptoms can nevertheless help predict future symptom development. The quantitative allergy blood result can help determine what a patient is allergic to, help predict and follow the disease development, estimate the risk of a severe reaction, and explain cross-reactivity.
A low total IgE level is not adequate to rule out sensitization to commonly inhaled allergens. Statistical methods, such as ROC curves, predictive value calculations, and likelihood ratios have been used to examine the relationship of various testing methods to each other. These methods have shown that patients with a high total IgE have a high probability of allergic sensitization, but further investigation with allergy tests for specific IgE antibodies for a carefully chosen of allergens is often warranted.
Other.
Challenge testing: Challenge testing is when small amounts of a suspected allergen are introduced to the body orally, through inhalation, or via other routes. Except for testing food and medication allergies, challenges are rarely performed. When this type of testing is chosen, it must be closely supervised by an allergist.
Elimination/Challenge tests: This testing method is used most often with foods or medicines. A patient with a suspected allergen is instructed to modify his diet to totally avoid that allergen for a set time. If the patient experiences significant improvement, he may then be “challenged” by reintroducing the allergen, to see if symptoms are reproduced.
Unreliable tests: There are other types of allergy testing methods that are unreliable, including applied kinesiology (allergy testing through muscle relaxation), cytotoxicity testing, urine autoinjection, skin titration (Rinkel method), and provocative and neutralization (subcutaneous) testing or sublingual provocation.
Differential diagnosis.
Before a diagnosis of allergic disease can be confirmed, other possible causes of the presenting symptoms should be considered. Vasomotor rhinitis, for example, is one of many maladies that shares symptoms with allergic rhinitis, underscoring the need for professional differential diagnosis. Once a diagnosis of asthma, rhinitis, anaphylaxis, or other allergic disease has been made, there are several methods for discovering the causative agent of that allergy.
Prevention.
The consumption of various foods during pregnancy has been linked to eczema; these include celery, citrus fruit, raw pepper, margarine, and vegetable oil. A high intake of antioxidants, zinc, and selenium during pregnancy may help prevent allergies. This is linked to a reduced risk for childhood-onset asthma, wheezing, and eczema. Further research needs to be conducted. Probiotic supplements taken during pregnancy or infancy may help to prevent atopic dermatitis.
After birth, an early introduction of solid food and high diversity before week 17 could increase a child's risk for allergies. Studies suggest that introduction of solid food and avoidance of highly allergenic food such as peanuts during the first year does not help in allergy prevention.
Aerobiology is the study of biological particles passively dispersed through the air. One aims is the prevention of allergies due to pollin.
Management.
In recent times, there have been enormous improvements in the medical practices used to treat allergic conditions. With respect to anaphylaxis and hypersensitivity reactions to foods, drugs, and insects and in allergic skin diseases, advances have included the identification of food proteins to which IgE binding is associated with severe reactions and development of low-allergen foods, improvements in skin prick test predictions; evaluation of the atopy patch test; in wasp sting outcomes predictions and a rapidly disintegrating epinephrine tablet, and anti-IL-5 for eosinophilic diseases.
Traditional treatment and management of allergies consisted simply of avoiding the allergen in question or otherwise reducing exposure. For instance, people with cat allergies were encouraged to avoid them. However, while avoidance of allergens may reduce symptoms and avoid life-threatening anaphylaxis, it is difficult to achieve for those with pollen or similar air-borne allergies. Nonetheless, strict avoidance of allergens is still considered a useful treatment method, and is often used in managing food allergies.
New technology approaches to decreasing IgE overproduction, and regulating histamine release in allergic individuals have demonstrated statistically significant reduction on Total Nasal Symptom Scores.
Medication.
Several antagonistic drugs are used to block the action of allergic mediators, or to prevent activation of cells and degranulation processes. These include antihistamines, glucocorticoids, epinephrine (adrenaline), theophylline and cromolyn sodium. Antileukotriene agents, such as montelukast (Singulair) or zafirlukast (Accolate), are FDA approved for treatment of allergic diseases. Anti-cholinergics, decongestants, mast cell stabilizers, and other compounds thought to impair eosinophil chemotaxis, are also commonly used. These drugs help to alleviate the symptoms of allergy, and are imperative in the recovery of acute anaphylaxis, but play little role in chronic treatment of allergic disorders.
Immunotherapy.
Desensitization or hyposensitization is a treatment in which the person is gradually vaccinated with progressively larger doses of the allergen in question. This can either reduce the severity or eliminate hypersensitivity altogether. It relies on the progressive skewing of IgG antibody production, to block excessive IgE production seen in atopys. In a sense, the person builds up immunity to increasing amounts of the allergen in question. Studies have demonstrated the long-term efficacy and the preventive effect of immunotherapy in reducing the development of new allergy. Meta-analyses have also confirmed efficacy of the treatment in allergic rhinitis in children and in asthma. A review by the Mayo Clinic in Rochester confirmed the safety and efficacy of allergen immunotherapy for allergic rhinitis and conjunctivitis, allergic forms of asthma, and stinging insect based on numerous well-designed scientific studies. In addition, national and international guidelines confirm the clinical efficacy of injection immunotherapy in rhinitis and asthma, as well as the safety, provided that recommendations are followed.
A second form of immunotherapy involves the intravenous injection of monoclonal anti-IgE antibodies. These bind to free and B-cell associated IgE; signalling their destruction. They do not bind to IgE already bound to the Fc receptor on basophils and mast cells, as this would stimulate the allergic inflammatory response. The first agent of this class is omalizumab. While this form of immunotherapy is very effective in treating several types of atopy, it should "not" be used in treating the majority of people with food allergies.
A third type, sublingual immunotherapy, is an orally administered therapy that takes advantage of oral immune tolerance to non-pathogenic antigens such as foods and resident bacteria. This therapy currently accounts for 40 percent of allergy treatment in Europe. In the United States, sublingual immunotherapy is gaining support among traditional allergists and is endorsed by doctors treating allergy.
Allergy shot treatment is the closest thing to a ‘cure’ for allergic symptoms. This therapy requires a long-term commitment.
Other.
An experimental treatment, enzyme potentiated desensitization (EPD), has been tried for decades but is not generally accepted as effective. EPD uses dilutions of allergen and an enzyme, beta-glucuronidase, to which T-regulatory lymphocytes are supposed to respond by favouring desensitization, or down-regulation, rather than sensitization. EPD has also been tried for the treatment of autoimmune diseases but is not approved by the U.S. Food and Drug Administration or of proven effectiveness.
Systematic literature searches conducted by the Mayo Clinic through 2006, involving hundreds of articles studying multiple conditions, including asthma and upper respiratory tract infection, showed no effectiveness of homeopathic treatments and no difference compared with placebo. The authors concluded that, based on rigorous clinical trials of all types of homeopathy for childhood and adolescence ailments, there is no convincing evidence that supports the use of homeopathic treatments.
Epidemiology.
Many diseases related to inflammation such as type 1 diabetes, rheumatoid arthritis, and allergic diseases — hay fever and asthma — have increased in the Western world over the past 2–3 decades. Rapid increases in allergic asthma and other atopic disorders in industrialized nations, it is estimated, began in the 1960s and 1970s, with further increases occurring during the 1980s and 1990s, although some suggest that a steady rise in sensitization has been occurring since the 1920s. The incidence of atopy in developing countries has, in general, remained much lower.
Although genetic factors fundamentally govern susceptibility to atopic disease, increases in atopy have occurred within too short a time frame to be explained by a genetic change in the population, thus pointing to environmental or lifestyle changes. Several hypotheses have been identified to explain this increased prevalence; increased exposure to perennial allergens due to housing changes and increasing time spent indoors, and changes in cleanliness or hygiene that have resulted in the decreased activation of a common immune control mechanism, coupled with dietary changes, obesity and decline in physical exercise. The hygiene hypothesis maintains that high living standards and hygienic conditions exposes children to fewer infections. It is thought that reduced bacterial and viral infections early in life direct the maturing immune system away from TH1 type responses, leading to unrestrained TH2 responses that allow for an increase in allergy.
Changes in rates and types of infection alone however, have been unable to explain the observed increase in allergic disease, and recent evidence has focused attention on the importance of the gastrointestinal microbial environment. Evidence has shown that exposure to food and fecal-oral pathogens, such as hepatitis A, "Toxoplasma gondii", and "Helicobacter pylori" (which also tend to be more prevalent in developing countries), can reduce the overall risk of atopy by more than 60%, and an increased prevalence of parasitic infections has been associated with a decreased prevalence of asthma. It is speculated that these infections exert their effect by critically altering TH1/TH2 regulation. Important elements of newer hygiene hypotheses also include exposure to endotoxins, exposure to pets and growing up on a farm.
History.
The concept of "allergy" was originally introduced in 1906 by the Viennese pediatrician Clemens von Pirquet, after he noted that some of his patients were hypersensitive to normally innocuous entities such as dust, pollen, or certain foods. Pirquet called this phenomenon "allergy" from the Ancient Greek words ἄλλος "allos" meaning "other" and ἔργον "ergon" meaning "work".
All forms of hypersensitivity used to be classified as allergies, and all were thought to be caused by an improper activation of the immune system. Later, it became clear that several different disease mechanisms were implicated, with the common link to a disordered activation of the immune system. In 1963, a new classification scheme was designed by Philip Gell and Robin Coombs that described four types of hypersensitivity reactions, known as Type I to Type IV hypersensitivity. With this new classification, the word "allergy" was restricted to type I hypersensitivities (also called immediate hypersensitivity), which are characterized as rapidly developing reactions.
A major breakthrough in understanding the mechanisms of allergy was the discovery of the antibody class labeled immunoglobulin E (IgE) – Kimishige Ishizaka and co-workers were the first to isolate and describe IgE in the 1960s.
Diagnosis.
Radiometric assays include the radioallergosorbent test (RAST test) method, which uses IgE-binding (anti-IgE) antibodies labeled with radioactive isotopes for quantifying the levels of IgE antibody in the blood. Other newer methods use colorimetric or fluorescence-labeled technology in the place of radioactive isotopes.
The RAST methodology was invented and marketed in 1974 by Pharmacia Diagnostics AB, Uppsala, Sweden, and the acronym RAST is actually a brand name. In 1989, Pharmacia Diagnostics AB replaced it with a superior test named the ImmunoCAP Specific IgE blood test, which uses the newer fluorescence-labeled technology.
American College of Allergy Asthma and Immunology (ACAAI) and the American Academy of Allergy Asthma and Immunology (AAAAI) issued the Joint Task Force Report “Pearls and pitfalls of allergy diagnostic testing” in 2008, and is firm in its statement that the term RAST is now obsolete:
The term RAST became a colloquialism for all varieties of (in vitro allergy) tests. This is unfortunate because it is well recognized that there are well-performing tests and some that do not perform so well, yet they are all called RASTs, making it difficult to distinguish which is which. For these reasons, it is now recommended that use of RAST as a generic descriptor of these tests be abandoned.
The new version, the ImmunoCAP Specific IgE blood test, is the only specific IgE assay to receive FDA approval to quantitatively report to its detection limit of 0.1kU/l.
Medical specialty.
An allergist is a physician specially trained to manage and treat allergies, asthma and the other allergic diseases.
In the United States physicians holding certification by the American Board of Allergy and Immunology (ABAI) have successfully completed an accredited educational program and evaluation process, including a proctored examination to demonstrate knowledge, skills, and experience in patient care in allergy and immunology. Becoming an allergist/immunologist requires completion of at least nine years of training. After completing medical school and graduating with a medical degree, a physician will undergo three years of training in internal medicine (to become an internist) or pediatrics (to become a pediatrician). Once physicians have finished training in one of these specialties, they must pass the exam of either the American Board of Pediatrics (ABP), the American Osteopathic Board of Pediatrics (AOBP), the American Board of Internal Medicine (ABIM), or the American Osteopathic Board of Internal Medicine (AOBIM). Internists or pediatricians wishing to focus on the sub-specialty of allergy-immunology then complete at least an additional two years of study, called a fellowship, in an allergy/immunology training program. Allergist/immunologists listed as ABAI-certified have successfully passed the certifying examination of the ABAI following their fellowship.
In the United Kingdom, allergy is a subspecialty of general medicine or pediatrics. After obtaining postgraduate exams (MRCP or MRCPCH), a doctor works for several years as a specialist registrar before qualifying for the General Medical Council specialist register. Allergy services may also be delivered by immunologists. A 2003 Royal College of Physicians report presented a case for improvement of what were felt to be inadequate allergy services in the UK. In 2006, the House of Lords convened a subcommittee. It concluded likewise in 2007 that allergy services were insufficient to deal with what the Lords referred to as an "allergy epidemic" and its social cost; it made several recommendations.

</doc>
<doc id="55314" url="http://en.wikipedia.org/wiki?curid=55314" title="Deacon">
Deacon

Deacon is a ministry in the Christian Church that is generally associated with service of some kind, but which varies among theological and denominational traditions. In many traditions the "diaconate" (or deaconate), the term for a deacon's office, is a clerical office; in others it is for laity.
The word "deacon" is derived from the Greek word "diákonos" (διάκονος), which is a standard ancient Greek word meaning "servant", "waiting-man", "minister", or "messenger". One commonly promulgated speculation as to its etymology is that it literally means "through the dust", referring to the dust raised by the busy servant or messenger.
It is believed that the only person in Scripture to hold the title "deaconess" is Phoebe, described in passing in Romans 16:1–2 as a deacon ("diakonos") of the church in Cenchreae, without specific duties or authority defined in the position she held. It is likely that her ministry has something to do with her activity in spreading the Gospel, and Paul was speaking of Phoebe as a female minister who was associated with the congregation in Cenchreae (). It is generally believed that the office of deacon originated in the selection of seven men by the apostles, among them Stephen, to assist with the charitable work of the early church as recorded in . Female deacons are mentioned by Pliny the Younger in a letter to Trajan dated "c". 112. The exact relationship between male and female deacons varies. In some traditions a female deacon is simply a member of the order of deacons; in others, deaconesses constitute a separate order; in others, the title "deaconess" was also given to the wife of a deacon.
A biblical description of the qualities required of a deacon, and of his household, can be found in .
Among the more prominent deacons in history are Stephen, the first Christian martyr (the "protomartyr"); Philip, whose baptism of the Ethiopian eunuch is recounted in ; Saint Lawrence, an early Roman martyr; Saint Vincent of Saragossa, protomartyr of Spain; Saint Francis of Assisi, founder of the mendicant Franciscans; Saint Ephrem the Syrian and Saint Romanos the Melodist, a prominent early hymnographer. Prominent historical figures who played major roles as deacons and went on to higher office include Saint Athanasius of Alexandria, Thomas Becket and Reginald Pole. On June 8, 536 a serving Roman deacon was raised to Pope, Silverius. His father, Pope Agapetus, had died and the office had been vacant for over a month.
The title is also used for the president, chairperson, or head of a trades guild in Scotland; and likewise to two officers of a Masonic Lodge.
Catholicism, Orthodoxy, Anglicanism.
The diaconate is one of the major orders in the Catholic, Anglican, Eastern Orthodox, and Oriental Orthodox churches. The other major orders are those of bishop and presbyter (priest).
While the diaconate as a permanent order was maintained from earliest Apostolic times to the present in the Eastern churches (Orthodox and Catholic), it mostly disappeared in the Western church (with a few notable exceptions such as St Francis of Assisi) during the first millennium, with Western churches retaining deacons attached to diocesan cathedrals. The diaconate continued in a vestigial form as a temporary, final step along the course to ordination to the Roman Catholic priesthood. In the 20th century, the diaconate was restored as a permanent order in many Western churches, most notably in the Latin Rite of the Catholic Church, the Anglican Communion, and the United Methodist Church.
In Catholic, Orthodox, and Anglican churches, deacons assist priests in their pastoral and administrative duties, but often report indirectly to the bishops of their diocese. They have a distinctive role in the liturgy, their main tasks being to proclaim the Gospel, preach, assist in the administration of the Eucharist and to serve the poor and outcast.
Roman Catholicism.
Beginning around the fifth century, there was a gradual decline in the permanent diaconate in the Latin church. It has however remained a vital part of the Eastern Catholic Churches. From that time until the years just prior to the Second Vatican Council, the only men ordained as deacons were seminarians who were completing the last year or so of graduate theological training, who received the order several months before priestly ordination.
Following the recommendations of the council (in "Lumen gentium", 29), in 1967 Pope Paul VI issued the motu proprio "Sacrum Diaconatus Ordinem", restoring the ancient practice of ordaining to the diaconate men who were not candidates for priestly ordination. These men are known as "permanent deacons" in contrast to those continuing their formation, who were then called "transitional deacons". There is no sacramental difference between the two, however, as there is only one order of deacons.
The permanent diaconate formation period in the Roman Catholic Church varies from diocese to diocese as it is determined by the local ordinary. But it usually entails a year of prayerful preparation, a four- or five-year training period that resembles a collegiate course of study, and a year of post-ordination formation as well as the need for lifelong continuing education credits. Diaconal candidates receive instruction in philosophy, theology, study of the Holy Scriptures (the Bible), homiletics, sacramental studies, evangelization, ecclesiology, counseling, and pastoral care and ministry before ordination. Although they are assigned to work in a parish by the diocesan bishop, once assigned, deacons are under the supervision of the parish pastor. Unlike most clerics, permanent deacons who also have a secular profession have no right to receive a salary for their ministry, but many dioceses opt to remunerate them anyway.
The ministry of the deacon in the Roman Catholic Church is described as one of service in three areas: the Word, the Liturgy and Charity. The deacon's ministry of the Word includes proclaiming the Gospel during the Mass, preaching and teaching. The deacon's liturgical ministry includes various parts of the Mass proper to the deacon, including being an ordinary minister of Holy Communion and the proper minister of the chalice when Holy Communion is administered under both kinds. The ministry of charity involves service to the poor and marginalized and working with parishioners to help them become more involved in such ministry. As clerics, deacons are required to recite the Liturgy of the Hours. Deacons, like priests and bishops, are ordinary ministers of the sacrament of Baptism and can serve as the church's witness at the sacrament of Holy Matrimony, which the bride and groom administer to each other (though if the exchange of vows takes place in a wedding Mass, or Nuptial Mass, the Mass is celebrated by the priest and the deacon acts as another witness). Deacons may preside at funeral rites not involving a Mass (e.g., the final commendation at the gravesite or the reception of the body at a service in the funeral home), and may assist the priest at the Requiem Mass. They can preside over various services such as Benediction of the Blessed Sacrament, and they may give certain blessings. They cannot hear confession and give absolution, anoint the sick, or celebrate Mass.
At Mass, the deacon is the ordinary minister of the proclamation of the Gospel (in fact, a priest, bishop, or even the Pope should not proclaim the Gospel if a deacon is present) and of Holy Communion (primarily, of the Precious Blood). As ordained clerics, and if granted faculties by their bishops, deacons may preach the homily at a public Mass, unless the priest celebrant retains that ministry to himself at a given Mass.
The vestments most particularly associated with the Western Rite Catholic deacon are the alb, stole and dalmatic. Deacons, like priests and bishops, must wear their albs and stoles; deacons place the stole over their left shoulder and it hangs across to their right side, while priests and bishops wear it around their necks. The dalmatic, a vestment especially associated with the deacon, is worn during the celebration of the Mass and other liturgical functions; its use is more liberally applied than the corresponding vestment of the priest, the chasuble. At certain major celebrations, such as ordinations, the diocesan bishop wears a dalmatic under his chasuble.
Permanent deacons often serve in parish or other ministry as their time permits, since they typically have other full-time employment. They may also act as parish administrators (C. 217 of the Code of Canon Law). With the passage of time, more and more deacons are serving in full-time ministries in parishes, hospitals, prisons, and in diocesan positions. Deacons often work directly in ministry to the marginalized inside and outside the church: the poor, the sick, the hungry, the imprisoned.
The transitional diaconate is to be conferred on seminarians (continuing to the priesthood) no sooner than 23 years of age (C. 1031 of the Code of Canon Law). The permanent diaconate can be conferred on single men 25 or older, and on married men 35 or older, but an older age can be required by the episcopal conference. If a married deacon is widowed, he must maintain the celibate state. Under some very rare circumstances, however, deacons who have been widowed can receive permission to remarry. This is most commonly done when the deacon is left as a single father. In some cases, a widowed deacon will seek priestly ordination, especially if his children are grown. ("See also clerical celibacy.") The wife of a permanent deacon may be sometimes considered a partner in his ordained ministry. In many dioceses, the wife of the diaconal candidate undertakes the same education and training her husband does.
A permanent deacon is not styled "Father" as a priest would be, but as "Deacon", abbreviated variously as "Dn." or "Dcn." This preferred method of address is stated in the 2005 document of the United States Conference of Catholic Bishops, National Directory for the Formation, Ministry and Life of Permanent Deacons in the United States. The proper address in written correspondence for all Deacons of the Latin (Roman Rite) Catholic Church is "Rev. Mr.". "Rev. Mr.", however, is more often used to indicate a transitional deacon (i.e., preparing for ordination to the priesthood) or one who belongs to a religious institute, while "Deacon" is used as the honorific for permanent deacons (e.g. Deacon John Smith, or Deacon Smith). The decision as to whether deacons wear the Roman collar as street attire is left to the discretion of each bishop for his own diocese. Where clerical garb is approved by the bishop, the deacon can choose to wear or not wear the "collar". Where it is not permitted, the deacon must wear secular clothing. It is becoming more common to see deacons wearing a clerical suit.
Deacons, like seminarians, religious, and the two other orders, Bishops and priests, recite the Liturgy of the Hours; however, deacons, if they are obliged to do so, are usually only required to participate in Morning and Evening Prayer.
In solemn Masses today and more so in older Rites of the Mass, one deacon will serve as the Deacon of the Word (proclaiming the Gospel and the Kyrie, and some other parts), and the Deacon of the Eucharist, who assists the priest during the Liturgy of the Eucharist.
Eastern Orthodoxy and Eastern Catholicism.
In addition to reading the Gospel and assisting in the administration of Holy Communion, the deacon censes the icons and people, calls the people to prayer, leads the litanies, and has a role in the dialogue of the Anaphora. In keeping with Eastern tradition he is not permitted to perform any Sacred Mysteries (sacraments) on his own, except for Baptism "in extremis" (in danger of death), conditions under which anyone, including the laity, may baptize. When assisting at a normal baptism, it is often the deacon who goes down into the water with the one being baptized (). In contrast to the Roman Catholic Church, deacons in the Eastern Churches may not preside at the celebration of marriages, as in Eastern theology the sacrament is conferred by the nuptial blessing of a priest.
Diaconal vestments are the sticharion (dalmatic), the orarion (deacon's stole), and the epimanikia (cuffs). The last are worn under his sticharion, not over it as does a priest or bishop. The deacon usually wears a simple orarion which is only draped over the left shoulder but, if elevated to the rank of archdeacon, he wears the "doubled-orarion", meaning it is passed over the left shoulder, under the right arm, and then crossed over the left shoulder (see photograph, right). In modern Greek practice, a deacon wears this doubled orarion from the time of his ordination. Also, in the Greek practice, he wears the clerical kamilavka (cylindrical head covering) with a rim at the top. In Slavic practice, a hierodeacon (monastic deacon) wears the simple black kamilavka of a monk (without the rim), but he removes the monastic veil (see klobuk) when he is vested; a married deacon would not wear a kamilavka unless it is given to him by the bishop as an ecclesiastical award; the honorary kamilavka is purple in colour, and may be awarded to either married or monastic clergy.
As far as street clothing is concerned, immediately following his ordination the deacon receives a blessing to wear the "Exorasson" (Arabic: "Jib'be", Slavonic: "Riassa"), an outer cassock with wide sleeves, in addition to the "Anterion" (Slavonic: "Podraznik"), the inner cassock worn by all orders of clergy. In the Slavic practice, married clergy may wear any of a number of colours, but most often grey, while monastic clergy always wear black. In certain jurisdtictions in North America and Western Europe, a Roman collar is often worn, although this is not a traditional or widespread practice.
A "protodeacon" (Greek: πρωτοδιάκονος: "protodiakonos", "first deacon") is a distinction of honor awarded to senior deacons, usually serving on the staff of the diocesan bishop. An "archdeacon" is similar, but is among the monastic clergy. Protodeacons and archdeacons use a double-length orarion even if it is not the local tradition for all deacons to use it. In the Slavic tradition a deacon may be awarded the doubled-orarion even if he is not a protodeacon or archdeacon.
According to the practice of the Greek Orthodox Church of America, in keeping with the tradition of the Ecumenical Patriarchate, the most common way to address a deacon is "Father". Depending on local tradition, deacons are addressed as either "Father", "Father Deacon", "Deacon Father", or, if addressed by a Bishop, simply as "Deacon".
The tradition of kissing the hands of ordained clergy extends to the diaconate as well. This practice is rooted in the Holy Eucharist and is in acknowledgement and respect of the eucharistic role members of the clergy play in preparing, handling and disbursing the sacrament during the Divine Liturgy, and in building and serving the church as the Body of Christ.
Anciently, the Eastern churches blessed though never consecrated deaconesses. This practice fell into desuetude in the second millennium, but has been revived in some schismatic churches. Saint Nectarios of Pentapolis blessed a number of nuns as deaconesses in convents. Deaconesses would assist in anointing and baptising women, and in ministering to the spiritual needs of the women of the community, but would not serve within the holy altar. As churches discontinued blessing women as deaconesses, these duties largely fell to the nuns and to the priests' wives.
Anglicanism.
In Anglican churches, deacons often work directly in ministry to the marginalized inside and outside the church: the poor, the sick, the hungry, the imprisoned. Unlike Orthodox and Catholic deacons who may be married only before ordination, Anglican deacons are permitted to marry freely before or after ordination, as are Anglican priests. Most deacons are preparing for priesthood and are usually ordained as priests about a year after their diaconal ordination. However, there are some deacons who do not go on to receive priestly ordination. Many provinces of the Anglican Communion ordain both women and men as deacons. Many of those provinces that ordain women to the priesthood previously allowed them to be ordained only to the diaconate. The effect of this was the creation of a large and overwhelmingly female diaconate for a time, as most men proceeded to be ordained priests after a short time as a deacon.
Anglican deacons may baptize and in some dioceses are granted licences to solemnize matrimony, usually under the instruction of their parish priest and bishop. Deacons are not able to preside at the Eucharist (but can lead worship with the distribution of already-consecrated communion elements where this is permitted), nor can they pronounce God's absolution of sin or pronounce the Trinitiarian blessing. In most cases, deacons minister alongside other clergy.
An Anglican deacon wears an identical choir dress to an Anglican priest: cassock, surplice, tippet and academic hood. However, liturgically, deacons usually wear a stole over their left shoulder and fastened on the right side of their waist. This is worn both over the surplice and the alb. A deacon might also wear a dalmatic.
Deaconesses.
The title "women deacon" or "deaconess" appears in many documents from the early Church period, particularly in the East. Their duties were often different from that of male deacons; women deacons prepared adult women for baptism and they had a general apostolate to female Christians and catechumens (typically for the sake of modesty). Women appear to have been ordained as deacons to serve the larger community until about the 6th century in the West and in the East until modern times.
Liturgies for the ordination of women deacons are quite similar to those for male deacons and the ancient ordination rites have been noted by groups like Womenpriests. Although it is sometimes argued that women deacons of history were not sacramentally ordained in the full sense used in the present day in Canons 1008 and 1009 of the Code of Canon Law, some modern scholars argue that the ordination of women deacons would have been equally sacramental to that of male deacons.
Currently, the Catholic Church has not restored women to the diaconate, although Vatican statements have declined to state that this is not possible, as they have in the case of priestly ordination. The Russian Orthodox Church had a female diaconate into the 20th century. The Holy Synod of the Orthodox Church of Greece restored a monastic female diaconate in 2004.
Lutheran churches.
Lutheran Church–Missouri Synod (USA).
The Lutheran Church–Missouri Synod (LCMS) in the United States has special training and certification programs for deaconesses. LC-MS deaconesses are trained at Concordia University - Chicago or one of their two seminaries (St. Louis, MO or Fort Wayne, IN). Internet based classes are also available through the Mission Training Center (MTC).
Deaconesses assist pastors in human care ministry and other roles with the goals of caring for those in need and freeing pastors to focus on word and sacrament ministry. Acts chapter 6, verse 2 describes the function of deacons (servants) then and now, "So the Twelve gathered all the disciples together and said, 'It would not be right for us to neglect the ministry of the word of God in order to wait on tables.'"
Deaconesses are installed, not ordained, and remain lay persons. The word "ordain" is to be reserved for the pastoral office.
A professional Deaconess (trained at the seminaries or Concordia University-Chicago) does not ordinarily preach and only ordained pastors may administer the sacraments; although she may perform baptism in cases of emergency.
The Atlantic District of the Lutheran Church–Missouri Synod has a Deacon Training program that prepares men and women for ministries of Word and Service in the local congregation. Students take the following courses of study at the Large Catechism level over the course of two years: Christian Doctrine Summary; Interpreting the Bible in Translation; Lutheran Worship I; OT Bible; Fundamental Pastoral Care; Basic Preaching; NT Bible; Teaching the Faith; Mission Outreach in Context; and Church History I (Christ to 1500 A.D.).
Atlantic District deacon students (male and female) who wish to seek commissioning as a Deacon in a local congregation must complete a pre-internship interview, 200 intern hours and their status as Deacon is under the authority of the local Pastoral Office. and a post-internship interview. Students are commissioned for the local congregation According to guidelines, Deacons shall be reviewed tri-annually by the Pastoral Office and the congregational President of the local congregation where the Deacon serves. Other Districts also train laymen and laywomen for service but the nomenclature varies by District.
Deacons, both the professional Deaconesses and the congregational and District trained Deacons (male and female) are considered to hold ministries of Word and Service (as opposed to Word and Sacrament).
Some with the nomenclature of "Deacon" are those training for ordination, although the terms "seminarian" or "vicar" are preferred. Special exceptions may be made for these Deacons who are vicars (training to become pastors) but must be given by the appropriate District president in writing. (A vicar in the LCMS is a third year seminarian who is doing an internship under a pastor. It should not be confused with the same term in Anglican and other church traditions.)
Evangelical Lutheran Church in America.
Deaconess Community (ELCA/ELCIC).
The Deaconess Community, a community of women serving in the Evangelical Lutheran Church in America (ELCA) and the Evangelical Lutheran Church in Canada (ELCIC) was formed in 1884. These women, who bear the title of 'Sister', proclaim the gospel through ministries of mercy and servant leadership on behalf of both churches for the sake of the world. Since the 1970s the sisters have been allowed to marry.
Diaconal Ministers/Associates in Ministry (ELCA/ELCIC).
The diaconate was recognized and rostered by the ELCA in 1993, creating a fourth 'roster' of recognized ministers (the other three being ordained, associates in ministry, and deaconess) in the churchwide body. The community is still young and as such is still being formed as to what styles and forms of ministry a diaconal minister pursues, as well as practices and traditions of the same.
As in the Anglican Communion, Lutheran diaconal ministers are allowed to wear a stole draped sideways from one shoulder and tied off at the waist, usually with some material left hanging below. Diaconal ministers (the term "deacon" is used occasionally but not officially) are involved in preaching, assisting in worship, leading worship in lieu of an ordained pastor and other congregational duties; they are, however, primarily called to service outside the church, in fields such as campus ministry, chaplaincy, congregational ministry, counseling, social service agency work, spiritual direction, parish and community nursing and a range of other avenues. A diaconal minister is "consecrated", rather than "ordained". This ceremony is usually presided over by a bishop.
Also of note are the 'associates in ministry (AIM), a rostered position within the ELCA consisting of laypersons commissioned into positions of service within the church, most often as educators, musicians, and worship leaders. While there is a trend towards combining the diaconal and associate ministries, the 'AIM' program continues in its own right, and associates are spread across the entirety of the churchwide body. AIMs are "commissioned" in the church and the hierarchy for service.
The ELCA is currently reviewing these rosters and working to identify how the institution can better answer the call to Word and Service ministries.
Calvinistic churches.
Church of Scotland.
There are two distinct offices of Deacon in the Church of Scotland. The best known form of diaconate are trained, paid pastoral workers, often working in parishes with considerable social and economic deprivation. The permanent diaconate was formerly exclusively female, and it was in the centenary year of the Diaconate (1988) that men were admitted to the office of Deacon. Women could not be ordained as Ministers until 1968. The offices of Deacon and Minister are now both open to both women and men; Deacons are now ordained (they were previously "commissioned").
The other office of Deacon can be found in congregations formerly belonging to the pre-1900 Free Church of Scotland, with a "Deacons' Court" having responsibility for financial and administrative oversight of congregations. Only a few congregations still retain this constitutional model, with most having since adopted the Church of Scotland's "Model Constitution" (with a Kirk Session and Congregational Board) or "Unitary Congregation" (with just a Kirk Session). Most of the Free Church congregations united with the United Presbyterian Church of Scotland in 1900 creating the United Free Church of Scotland, which itself united with the Church of Scotland in 1929.
The congregations of the Free Church of Scotland (post 1900) which did not join the UF Church in 1900 continue to have Deacons.
Presbyterian Church (USA/PCA).
Individual congregations of these church denominations also elect deacons, along with elders. However, in some churches the property-functions of the diaconate and session of elders is commended to an independent board of trustees. John Calvin's legacy of restoring a servant-ministry diaconate lives on in the Presbyterian churches. Deacons are specially charged with ministries of mercy, especially toward the sick and the poor.
Dutch protestant churches.
In many Dutch Protestant churches deacons are charged with ministries of mercy. As such, the deacons are also a member of the local church council. A special feature of the Dutch churches is the fact that the Diaconate of each local church is an own legal entity with own financial means, separated from the church itself, and governed by the deacons.
Methodist churches.
Methodist Church of Great Britain.
Methodist Church of Great Britain has a Permanent Diaconate, based on an understanding of the New Testament that Deacons have an equal, but distinct ministry from Presbyters. The original Wesleyan Deaconess Order was founded by Rev Thomas Bowman Stephenson in 1890, following observation of new ministries in urban areas in the previous years. The order continued as the Wesley Deaconess Order following Methodist Union in 1932, but, following the admission of women to "The Ministry" (as presbyteral ministry is commonly termed in the Methodist Church), a number of Deaconesses transferred and recruitment for the WDO ceased from 1978. The 1986 Methodist Conference re-opened The Order to both men and women and the first Ordinations to the renewed order occurred during the 1990 Conference in Cardiff, which coincided with celebrations of 100 years of diaconal service in British Methodism; deaconesses had previously been ordained at their annual convocation.
The United Methodist Church.
In U.S. Methodism, the deacon began as a transitional order before a clergy person was ordained elder. In 1996, The United Methodist Church ended the transitional deacon and established a new Order of Deacons to be equal in status with the Order of Elders. Both men and women may be ordained as deacons. Deacons serve in a variety of specialized ministries including, but not limited to, Christian education, music, communications and ministries of justice and advocacy. Unlike United Methodist elders, deacons must find their own place of service. Nevertheless, the bishop does officially approve and appoint deacons to their selected ministry. Deacons may assist the elder in the administration of Sacraments, but must receive special approval from a bishop before presiding over Baptism and Holy Communion.
Other traditions.
Deacons are also appointed or elected in other Protestant denominations, though this is less commonly seen as a step towards the clerical ministry. The role of deacon in these denominations varies greatly from denomination to denomination; often, there will be more emphasis on administrative duties than on pastoral or liturgical duties. In some denominations, deacons' duties are only financial management and practical aid and relief. Elders handle pastoral and other administrative duties.
Amish.
The Amish have deacons, but they are elected by a council and receive no formal training.
Baptists.
Baptists have traditionally followed the principle of the autonomy of the local church congregation, giving each church the ability to discern for themselves the interpretation of scripture. Thus, the views among Baptist churches as to who becomes a deacon and when, as well as what they do and how they go about doing it, vary greatly. Baptists recognize two ordained positions in the church as Elders (Pastors) and Deacons, as per 1 Timothy, third chapter.
There are Baptist churches where the deacons decide many of the church affairs. There are churches where deacons serve in a family ministry only. There are Baptist churches (especially in the United Kingdom, but also in the U.S. and elsewhere) where women are allowed to be deacons; while many Baptist churches do not allow women to serve as deacons. Many Baptists also interpret Scripture as prohibiting divorced men from serving as deacons.
In the General Association of Regular Baptist Churches, deacons can be any adult male member of the congregation who is in good standing.
In some African American Missionary Baptist churches and in churches affiliated with the National Baptist Convention, USA, Inc. male and female deacons serve as one board. Other churches may have two separate boards of deacons and deaconesses. Most often the deacon or deacon candidate is a long-standing member of the church, being middle aged, but younger deacons may be selected from among members of a family that has had several generations in the same church. They are elected by quorum vote annually. Their roles are semi-pastoral in that they fill in for the pastor on occasion, or support the pastor vocally during his sermon. They may also lead a special prayer service, generally known as "The Deacon's Prayer." Their other roles are to accompany the pastor during Communion by handing out the remembrances of bread and wine (or grape juice) and to set a good example for others to follow. Their administrative duties sometimes include oversight of the treasury, Sunday school curriculum, transportation, and various outreach ministries.
See Baptist Distinctives for a more detailed treatment of Deacons in churches in other Associations, particularly the UK.
Uniting Church in Australia.
In the Uniting Church in Australia, the diaconate is one of two offices of ordained ministry. The other is Minister of the Word.
Deacons in the Uniting Church are called to minister to those on the fringes of the church and be involved in ministry in the community. A deacon is a pathfinder. They go where others have not gone before and light the way for the church to respond to where people in the community are hurting, disadvantaged and oppressed. A deacon is a community builder. Not always having congregation, they begin with scattered people and shape them into a community. A deacon is an evangelist. They share the good news of the gospel with people in the community.
Due to shortages in diaconal placements in the Uniting Church, deacons are often serving in similar or exact placements to ministers of the Word.
It is also acknowledged that both the "Ministry of the Word" and the "Ministry of Deacon" have a number of overlaps and not one particular ministry is restricted to any particular ordination.
In the Uniting Church both ministers of the word and deacons are styled "The Reverend".
The Church of Jesus Christ of Latter-day Saints.
The office of Deacon is generally open to all 12- and 13-year-old male members of the church; all are encouraged to become Deacons. Duties include:
Church of Christ.
The role of deacons in this church is also widely varied. Generally they are put in control of various programs of a congregation. They are servants, as the etymology indicates, of the church. They are under the subjection of the elders, as is the rest of the congregation. Their qualifications are found in the New Testament, in 1 Timothy 3:8-13 (Waddey, John; et al. 1981).
New Apostolic Church.
In the New Apostolic Church, the deacon ministry is a local ministry. A deacon mostly works in his home congregation to support the priests. If a priest is unavailable, a deacon will hold a divine service, without the act of communion (Only Priests and up can consecrate Holy Communion).
Jehovah's Witnesses.
Deacons among Jehovah's Witnesses are referred to as ministerial servants, claiming it preferable to translate the descriptive Greek term used in the Bible rather than merely transliterate it as though it were a title. Appointed ministerial servants aid elders in congregational duties. Like the elders, they are adult baptized males and serve voluntarily.
Cognates.
The Greek word "diakonos" (διάκονος) gave rise to the following terms from the history of Russia, not to be confused with each other: "dyak", "podyachy", "dyachok", in addition to "deacon" and "protodeacon".
Scots usage.
In Scots language, the title "deacon" is used for a head-workman, a master or chairman of a trade guild, or one who is adept, expert and proficient. The term "deaconry" refers to the office of a "deacon" or the trade guild under a "deacon".
The most famous holder of this title was Deacon Brodie who was a cabinet-maker and president of the Incorporation of Wrights and Masons as well as being a Burgh councillor of Edinburgh, but at night led a double life as a burglar. He is thought to have inspired the story of "The Strange Case of Dr Jekyll and Mr Hyde".

</doc>
<doc id="55315" url="http://en.wikipedia.org/wiki?curid=55315" title="USS Voyager (Star Trek)">
USS Voyager (Star Trek)

The fictional "Intrepid"-class starship USS "Voyager" is the primary setting of the science fiction television series "". It is commanded by Captain Kathryn Janeway.
"Voyager" was designed by "Star Trek: Voyager" production designer Richard D. James and illustrator Rick Sternbach. Most of the ship's on-screen appearances result from computer-generated imagery, although Tony Meinenger built a model used in the series. The principal model of "Voyager" used for filming sold at Christie's auction in 2006 for USD $132,000.
Mission.
"Voyager" was launched in 2371. The crew's first orders were to track down a Maquis ship in the Badlands. An alien force called the transports both "Voyager" and the Maquis vessel across 70,000 light-years to the Delta Quadrant, damaging "Voyager" and killing several crewmembers (including first officer Lt. Cmdr. Cavit, the ship's chief medical officer and the rest of the medical staff, helm officer Stadi, and the chief engineer). To prevent a genocide of the Ocampans, Janeway orders the destruction of a device that could transport "Voyager" and the Maquis vessel home. Stranded, and with the Maquis ship also destroyed, both crews must integrate and work together for the anticipated 75-year journey home.
Starfleet Command eventually becomes aware of the ship's presence in the Delta Quadrant and is later able to establish regular communication. After a seven-year journey, the ship returns to the Alpha Quadrant via a Borg transwarp conduit with the aid of the time-traveling Admiral Kathryn Janeway (former Captain of "Voyager") from an alternate future.
The ship's motto, as engraved on its dedication plaque, is a quote from the poem "Locksley Hall" by Alfred, Lord Tennyson: "For I dipt in to the future, far as human eye could see; Saw the vision of the world, and all the wonder that would be."
Crew.
USS "Voyager" was launched with 141 crew on board. After the starship was flung to the Delta Quadrant, several crew members were killed, including several senior officers, namely, Lt. Commander Cavit, the first officer; Chief Medical Officer Fitzgerald; Lieutenant Stadi, the Betazoid Helm Officer; and the chief engineer.
As of 2377, the crew complement was at 146, having gained some crew from the Maquis, the "Equinox", Samantha Wildman's child Naomi, and several liberated Borg drones, including Seven of Nine and Icheb.
The senior staff of "Voyager" include Captain Kathryn Janeway (Kate Mulgrew), who commands the ship; Commander (field commission) Chakotay (Robert Beltran), her first officer, who joined from the Maquis; Lt. Tuvok (Tim Russ) (later rising to the rank of Lieutenant Commander), the Security/Tactical officer; Ensign Harry Kim (Garrett Wang), the Operations Officer; Lt JG (junior grade) (field commission) B'Elanna Torres (Roxann Dawson), Chief Engineer; Lt. JG (falling to ensign as a result of demotion but later rising in rank to full lieutenant) Tom Paris (Robert Duncan McNeill), Helm Officer; as well as several noncommissioned personnel, the Emergency Medical Hologram (Robert Picardo) as the Chief Medical Officer, Neelix (Ethan Phillips) as the ship's cook and later Voyager's ambassador, Kes (Jennifer Lien) as the EMH's Medical Assistant, and Seven of Nine (Jeri Ryan), who plays several roles, generally in Astrometrics or Engineering.
Several recurring characters include Naomi Wildman, the "Captain's Assistant" and daughter of Ensign Samantha Wildman, and the Borg children, Icheb, Mezoti, Azan, and Rebi.
Design and capabilities.
The 15-deck (257 rooms), 700,000 metric-tonne "Voyager" was built at the Utopia Planitia Fleet Yards and launched from Earth Station McKinley.
"Voyager" is equipped with bio-neural gel packs, designed to increase processing speed and better organize processed information, that supplement the ship's isolinear optical chips. The ship had 47 spare gel packs and could not replicate additional packs. The ship also has two holodecks. "Voyager" was the first ship to be equipped with a class-9 warp drive, which was intended to be tested in deep space, allowing for a maximum sustainable speed of Warp 9.975. Variable geometry pylons allow Voyager and other Intrepid class ships to exceed warp 5 without damaging subspace. "Voyager" is capable of planetary landings. The ship isn't able to make the saucer separation in case of emergency, instead, it simply ejects the warp core.
"Voyager" includes an Emergency Medical Hologram programmed with a library of more than 5 million different medical treatments from 2,000 medical references and 47 physicians. The hologram itself is generated by a series of holographic emitters installed in sickbay. "Voyager"'s EMH is eventually able to leave sickbay due to a piece of 29th century technology commonly referred to as the "mobile emitter".
The ship is initially equipped with 42 photon torpedoes with type VI warheads and two tricobalt devices, both of which are used to destroy the Caretaker's array. Quantum torpedoes were also compatible with Voyager's launchers, with some modification. Voyager housed four standard torpedo launchers (two fore, two aft.) able to fire up to four torpedoes per launcher at once. An alternate future Kathryn Janeway equips the ship with transphasic torpedoes and ablative hull armor.
During the years in the Delta Quadrant, the ship is augmented with custom, non-spec upgrades and modifications, some of which are modified from technology of other cultures, an example being Seven of Nine's alcoves and the Delta Flyer which both utilize modified Borg technology. Several pieces of technology from the future were also installed, courtesy of Admiral Janeway who went back in time to bring "Voyager" home. Some of the adaptive solutions are to compensate for the disadvantages of being 70,000 light years from port, such as the airponics bay and the transformation of the Captain's dining room to a galley, and the acquisition of enhancements from aliens in the Void that massively increases replicator efficiency.
The Borg are a major source of technological upgrades conducted on "Voyager". Cargo Bay 2 is equipped with several Borg alcoves when Captain Janeway forms an alliance with the Borg and several Borg are forced to work aboard "Voyager" during the alliance. Seven of Nine and Harry Kim build an astrometrics lab from scratch with Borg-enhanced sensors, knowledge of which Seven of Nine retained from the Borg. Additionally, the crew design and build the "Delta Flyer" support craft at the behest of Tom Paris.

</doc>
<doc id="55317" url="http://en.wikipedia.org/wiki?curid=55317" title="First day of issue">
First day of issue

A first day of issue cover or first day cover (FDC) is a postage stamp on a cover, postal card or stamped envelope franked on the first day the issue is authorized for use within the country or territory of the stamp-issuing authority. Sometimes the issue is made from a temporary or permanent foreign or overseas office. There will usually be a first day of issue postmark, frequently a pictorial cancellation, indicating the city and date where the item was first issued, and "first day of issue" is often used to refer to this postmark. Depending on the policy of the nation issuing the stamp, official first day postmarks may sometimes be applied to covers weeks or months after the date indicated.
Postal authorities may hold a first day ceremony to generate publicity for the new issue, with postal officials revealing the stamp, and with connected persons in attendance, such as descendants of the person being honored by the stamp. The ceremony may also be held in a location that has a special connection with the stamp's subject, such as the birthplace of a social movement, or at a stamp show.
Other types of first day covers.
Computer vended postage stamps issued by Neopost had first-day-of-issue ceremonies sponsored by the company, not by an official stamp-issuing entity. Personalised postage stamps of different designs are sometimes also given first-day-of-issue ceremonies and cancellations by the private designer. The stamps issued by private local posts can also have first days of issue, as can artistamps.
Event covers.
Event covers, instead of marking the issuance of a stamp, commemorate events. A design on the left side of the envelope (a "cachet") explains the event or anniversary being celebrated. Ideally the stamp or stamps affixed relate to the event. Cancels are obtained either from the location (e.g., Cape Canaveral, Anytown) or, in the case of the United States, from the Postal Service's Cancellation Services unit in Kansas City.
Earliest known use.
The earliest known use (EKU) of a stamp may or may not be the same as the first day of issue. This can occur if:
The search for EKUs of both old and new stamps is an active area of philately, and new discoveries are regularly announced.
Philatelic covers.
As the collecting of first day covers became more popular they began to appear on prepared envelopes, often with an illustration (commonly referred to by collectors as a cachet) that corresponded with the theme of the stamp. Several printing companies began producing such envelopes and often hired free lance illustrators to design their cachets such as Charles R. Chickering who in his earlier years designed postage stamps for the U.S. Post Office.
See also.
Earliest Reported Postmark on stamped envelopes.

</doc>
<doc id="55319" url="http://en.wikipedia.org/wiki?curid=55319" title="Ambient music">
Ambient music

Ambient music is a genre of music that puts an emphasis on tone and atmosphere over traditional musical structure or rhythm. Ambient music is said to evoke an "atmospheric", "visual" or "unobtrusive" quality. According to one of its pioneers Brian Eno, "Ambient music must be able to accommodate many levels of listening attention without enforcing one in particular; it must be as ignorable as it is interesting."
As a genre it originated in the United Kingdom at a time when new sound-making devices such as the synthesizer, were being introduced to a wider market. Ambient developed in the 1970s from the experimental and synthesizer-oriented styles of the period. Mike Oldfield, Jean Michel Jarre and Vangelis were all influences on the emergence of ambient. Robert Fripp and Brian Eno popularized ambient music in 1972 while experimenting with tape loop techniques. The Orb and Aphex Twin gained commercial success with ambient tracks in the early 1990s. Ambient compositions are often quite lengthy, much longer than more popular, commercial forms of music. Some pieces can reach a half an hour or more in length.
Ambient had a revival towards the late 1980s with the prominence of house and techno music. Eventually, ambient grew a cult following in the 1990s. By the early 1990s artists such as Aphex Twin were being called ambient house, ambient techno, IDM or "ambient" by the media. Genre offshoots include dark ambient, ambient house, ambient industrial, ambient dub, psybient and ambient trance.
History.
Developing in the 1970s, ambient stemmed from the experimental and synthesizer-oriented styles of the period. Although German bands such as Popol Vuh and Tangerine Dream predate him in the creation of Ambient music, Brian Eno played a key role in its development and popularization and is often erroneously cited as ambient's founder. The concept of background or furniture music had already existed some time before, but only in the 70s was ambient music first created, which incorporated New Age ideals with the newly invented modular synthesizer.
As a genre, ambient music usually focuses on creating a mood or atmosphere through synthesizers and timbral qualities. It often lacks the presence of any net composition, beat, or structured melody. Due to its relatively open style, ambient music often takes influences from many other genres, ranging from house, dub, industrial and new age, amongst several others.
Ambient did not achieve large commercial success, being criticized as having a "boring" and "over-intellectual" sound. Nevertheless, it has also attained a certain degree of acclaim throughout the years. It had its first wave of popularity in the 1970s, yet saw a revival towards the late 1980s with the prominence of house and techno music, growing a cult following by the 1990s.
As an early 20th-century French composer, Erik Satie used such Dadaist-inspired explorations to create an early form of ambient / background music that he labeled "furniture music" ("Musique d'ameublement"). This he described as being the sort of music that could be played during a dinner to create a background atmosphere for that activity, rather than serving as the focus of attention.
Brian Eno is generally credited with coining the term "Ambient Music" in the mid-1970s to refer to music that, as he stated, can be either "actively listened to with attention or as easily ignored, depending on the choice of the listener", and that exists on the "cusp between melody and texture". Eno, who describes himself as a "non-musician", termed his experiments in sound as "treatments" rather than as traditional performances. Eno used the word "ambient" to describe music that creates an atmosphere that puts the listener into a different state of mind; having chosen the word based on the Latin term "ambire", "to surround".
The album notes accompanying Eno's 1978 release "" include a manifesto describing the philosophy behind his ambient music: "Ambient Music must be able to accommodate many levels of listening attention without enforcing one in particular; it must be as ignorable as it is interesting."
Eno has acknowledged the influence of Erik Satie and John Cage. In particular, Eno was aware of Cage's use of chance such as throwing the "I Ching" to directly affect the creation of a musical composition. Eno then utilised a similar method of weaving randomness into his compositional structures. This approach was manifested in Eno's creation of Oblique Strategies, where he used a set of specially designed cards to create various sound dilemmas that in turn, were resolved by exploring various open ended paths, until a resolution to the musical composition revealed itself. Eno also acknowledged influences of the drone music of La Monte Young (of whom he said, "La Monte Young is the daddy of us all") and of the mood music of Miles Davis and Teo Macero, especially their 1974 epic piece, "He Loved Him Madly" (from "Get Up with It"), about which Eno wrote, "that piece seemed to have the 'spacious' quality that I was after...it became a touchstone to which I returned frequently."
Beyond the major influence of Brian Eno, other musicians and bands added to the growing nucleus of music that evolved around the development of "Ambient Music". While not an exhaustive list, one cannot ignore the parallel influences of Wendy Carlos, who produced the original music piece called "Timesteps" which was then used as the filmscore to "A Clockwork Orange", as well as her later work "Sonic Seasonings". Other significant artists such as Mike Oldfield, Jean Michel Jarre and Vangelis, also Russian electronic music pioneer , have all added to or directly influenced the evolution of ambient music. Adding to these individual artists, works by groups such as Pink Floyd, through their album " Endless River". The Yellow Magic Orchestra developed a distinct style of ambient electronic music that would later be developed into ambient house music.
1990s developments.
By the early 1990s artists such as The Orb, Aphex Twin, Seefeel, the Irresistible Force, Geir Jenssen's Biosphere, and the Higher Intelligence Agency were being referred to by the popular music press as ambient house, ambient techno, IDM or simply "ambient" according to the liner notes of Brian Eno's "":
So-called 'Chillout' began as a term deriving from British ecstasy culture which was originally applied in relaxed downtempo 'chillout rooms' outside of the main dance floor where ambient, dub and downtempo beats were played to ease the tripping mind.
The London scene artists, such as Aphex Twin (specifically: "Selected Ambient Works Volume II", 1994), Global Communication ("", 1994), FSOL The Future Sound of London ("Lifeforms", "ISDN"), The Black Dog ("Temple of Transparent Balls", 1993), Another Green World ("Invisible Landscape",1996), Autechre ("Incunabula", 1993, "Amber"), Boards of Canada, and The KLF's seminal "Chill Out", 1990, all took a part in popularising and diversifying ambient music where it was used as a calming respite from the intensity of the hardcore and techno popular at that time.
Related and derivative genres.
Dark ambient.
Brian Eno's original vision of ambient music as unobtrusive musical wallpaper, later fused with warm house rhythms and given playful qualities by the Orb in the 1990s, found its opposite in the style known as dark ambient. Populated by a wide assortment of personalities—ranging from aging industrial and metal experimentalists (Scorn's Mick Harris, Current 93's David Tibet, Nurse with Wound's Steven Stapleton) to electronic boffins (Kim Cascone/PGR, Psychick Warriors Ov Gaia), Japanese noise artists (K.K. Null, Merzbow), and latter-day indie rockers (Main, Bark Psychosis) -- dark ambient features toned-down or entirely missing beats with unsettling passages of keyboards, eerie samples, and treated guitar effects. Like most styles related in some way to electronic/dance music of the '90s, it's a very nebulous term; many artists enter or leave the style with each successive release. Related styles include ambient industrial and isolationist ambient.
Ambient house.
Ambient house is a musical category founded in the late 1980s that is used to describe acid house featuring ambient music elements and atmospheres. Tracks in the ambient house genre typically feature four-on-the-floor beats, synth pads, and vocal samples integrated in an atmospheric style. Ambient house tracks generally lack a diatonic center and feature much atonality along with synthesized chords. Illbient is another form of ambient house music.
Ambient industrial.
Ambient industrial is a hybrid genre of ambient and industrial music; the term industrial being used in the original experimental sense, rather than in the sense of industrial metal. A "typical" ambient industrial work (if there is such a thing) might consist of evolving dissonant harmonies of metallic drones and resonances, extreme low frequency rumbles and machine noises, perhaps supplemented by gongs, percussive rhythms, bullroarers, distorted voices or anything else the artist might care to sample (often processed to the point where the original sample is no longer recognizable). Entire works may be based on radio telescope recordings, the babbling of newborn babies, or sounds recorded through contact microphones on telegraph wires.
Space music.
Space music, also spelled spacemusic, includes music from the ambient genre as well as a broad range of other genres with certain characteristics in common to create the experience of contemplative spaciousness.
Many of the earliest performers were associated with the Berlin School of electronic music, which continues to inspire the genre.
Space music ranges from simple to complex sonic textures sometimes lacking conventional melodic, rhythmic, or vocal components, generally evoking a sense of "continuum of spatial imagery and emotion", beneficial introspection, deep listening and sensations of floating, cruising or flying.
Space music is used by individuals for both background enhancement and foreground listening, often with headphones, to stimulate relaxation, contemplation, inspiration and generally peaceful expansive moods and soundscapes. Space music is also a component of many film soundtracks and is commonly used in planetariums, as a relaxation aid and for meditation.
Ambient dub.
Ambient dub involves the genre melding of dub styles. It was pioneered by King Tubby and other Jamaican sound artists, who used DJ-inspired ambient electronica, complete with all the inherent drop-outs, echo, equalization and psychedelic electronic effects. It often features layering techniques and incorporates elements of world music, deep bass lines and harmonic sounds. According to David Toop, "Dub music is like a long echo delay, looping through time...turning the rational order of musical sequences into an ocean of sensation." Notable artists within the genre include Dreadzone, Higher Intelligence Agency, The Orb, Loop Guru, Woob and Transglobal Underground as well as Banco de Gaia.

</doc>
<doc id="55321" url="http://en.wikipedia.org/wiki?curid=55321" title="Postcard">
Postcard

A postcard or post card is a rectangular piece of thick paper or thin cardboard intended for writing and mailing without an envelope. Other shapes than rectangular may also be used. There are novelty exceptions, such as wood postcards, made of thin wood, and copper postcards sold in the Copper Country of the U.S. state of Michigan, and coconut "postcards" from tropical islands.
In some places, it is possible to send them for a lower fee than for a letter. Stamp collectors distinguish between postcards (which require a stamp) and postal cards (which have the postage pre-printed on them). While a postcard is usually printed by a private company, individual or organization, a postal card is issued by the relevant postal authority.
The world's oldest postcard was sent in 1840 to the writer Theodore Hook from Fulham in London, England. The study and collecting of postcards is termed "deltiology".
Early history of postcards.
Cards with messages had been sporadically created and posted by individuals since the creation of postal services. The earliest known picture postcard was a hand-painted design on card, posted in Fulham in London to the writer Theodore Hook in 1840 bearing a penny black stamp. He probably created and posted the card to himself as a practical joke on the postal service, since the image is a caricature of workers in the post office. In 2002 the postcard sold for a record £31,750.
In the United States, a picture or blank card stock that held a message and sent through the mail at letter rate first began when a card postmarked in December 1848 contained printed advertising on it. The first commercially produced card was created in 1861 by John P. Charlton of Philadelphia, who patented a postal card, selling the rights to Hymen Lipman, whose postcards, complete with a decorated border, were labeled "Lipman's postal card." These cards had no images.
In Britain, postcards without images were issued by Post Office, and were printed with a stamp as part of the design, which was included in the price of purchase. The first known printed picture postcard, with an image on one side, was created in France in 1870 at Camp Conlie by Léon Besnardeau (1829–1914). Conlie was a training camp for soldiers in the Franco-Prussian war. They had a lithographed design printed on them containing emblematic images of piles of armaments on either side of a scroll topped by the arms of the Duchy of Brittany and the inscription "War of 1870. Camp Conlie. Souvenir of the National Defence. Army of Brittany". While these are certainly the first known picture postcards, there was no space for stamps and no evidence that they were ever posted without envelopes.
In the following year the first known picture postcard in which the image functioned as a souvenir was sent from Vienna. The first advertising card appeared in 1872 in Great Britain and the first German card appeared in 1874. Cards showing images increased in number during the 1880s. Images of the newly built Eiffel Tower in 1889 and 1890 gave impetus to the postcard, leading to the so-called "golden age" of the picture postcard in years following the mid-1890s. Early postcards often showcased photography of nude women. These were commonly known as French postcards, due to the large number of them produced in France.
Early US postcards.
The first American postcard was developed in 1873 by the Morgan Envelope Factory of Springfield, Massachusetts. These first postcards depicted the "Interstate Industrial Exposition" that took place in Chicago. Later in 1873, Post Master John Creswell introduced the first pre-stamped "Postal Cards," often called "penny postcards". Postcards were made because people were looking for an easier way to send quick notes. The first postcard to be printed as a souvenir in the United States was created in 1893 to advertise the World's Columbian Exposition in Chicago.
The Post Office was the only establishment allowed to print postcards, and it held its monopoly until May 19, 1898, when Congress passed the Private Mailing Card Act, which allowed private publishers and printers to produce postcards. Initially, the United States government prohibited private companies from calling their cards "postcards", so they were known as "souvenir cards". These cards had to be labeled "Private Mailing Cards". This prohibition was rescinded on December 24, 1901, when private companies could use the word "postcard". Postcards were not allowed to have a divided back and correspondents could only write on the front of the postcard. This was known as the "undivided back" era of postcards. On March 1, 1907 the Post Office allowed private citizens to write on the address side of a postcard. It was on this date that postcards were allowed to have a "divided back".
On these cards the back is divided into two sections, the left section being used for the message and the right for the address. Thus began the Golden Age of American postcards, which peaked in 1910 with the introduction of tariffs on German-printed postcards, and ended by 1915, when World War I ultimately disrupted the printing and import of the fine German-printed cards. The postcard craze between 1907 and 1910 was particularly popular among rural and small-town women in Northern U.S. states.
Postcards, in the form of government postal cards and privately printed souvenir cards, became very popular as a result of the Columbian Exposition, held in Chicago in 1893, after postcards featuring buildings were distributed at the fair. In 1908, more than 677 million postcards were mailed.
The "white border" era, named for obvious reasons, lasted from about 1916 to 1930.
Mid-20th century US postcards.
Linen postcards were produced in great quantity from 1931 to 1959. Contrary to the name, linen postcards were not produced on a linen fabric, but used newer printing processes that used an inexpensive card stock with a high rag content, and then finished with a pattern which resembled linen. The face of the cards is distinguished by a textured cloth appearance which makes them easily recognizable. The reverse of the card is smooth as in earlier postcards. The rag content in the card stock allowed for a much more colorful and vibrant image to be printed than the earlier “white border” style. Due to the inexpensive production and bright realistic images they became popular.
One of the better known linen era postcard manufacturers was Curt Teich and Company, who first produced the immensely popular “ large letter linen” postcards (among many others). The card design featured a large letter spelling of a state or place with smaller photos inside the letters. The design can still be found in many places today. Other manufacturers include Tichnor and Company, Haynes, Stanley Piltz, E.C Kropp, and the Asheville Postcard Company.
By the late 1920s new colorants had been developed that were very enticing to the printing industry. Though they were best used as dyes to show off their brightness, this proved to be problematic. Where traditional pigment based inks would lie on a paper’s surface, these thinner watery dyes had a tendency to be absorbed into a paper’s fibers where it lost its advantage of higher color density leaving behind a dull blurry finish. To experience the rich colors of dyes light must be able to pass through them to excite their electrons. A parcial solution was to combine these dyes with petroleum distillates leading to faster drying heatset inks. But it was Curt Teich who finally solved the problem by embossing paper with a linen texture before printing. The embossing created more surface area, which allowed the new heatset inks to dry even faster. The quicker drying time allowed these dyes to remain on the paper’s surface thus retaining their superior strength, which give Linens their telltale bright colors. In addition to printing with the usual CYMK colors, a lighter blue was sometimes used to give the images extra punch. Higher speed presses could also accommodate this method leading to its widespread use. Although first introduced in 1931, their growing popularity was interrupted by the onset of war. They were not to be printed in numbers again until the later 1940s when the war effort ceased consuming most of the country’s resources. Even though the images on linen cards were based on photographs, they contained much handwork of the artists who brought them into production. There is of course nothing new in this; what it notable is that they were to be the last postcards to show any touch of the human hand on them. In their last days, many were published to look more like photo based chrome cards that began to dominate the market. Textured papers for postcards had been manufactured ever since the turn of the century. But since this procedure was not then a necessary step in aiding card production, its added cost kept the process limited to a handful of publishers. Its original use most likely came from attempts to simulate the texture of canvas, thus relating the postcard to a painted work of fine art.
The United States Postal Service defines a postcard as: rectangular, at least 3+1/2 in high × 5 in long × 0.007 in thick and no more than 4+1/4 in high × 6 in long × 0.016 in thick. However, some postcards have deviated from this (for example, shaped postcards).
Contemporary postcards.
The last and current postcard era, which began about 1939, is the "chrome" era, however these types of cards did not begin to dominate until about 1950. The images on these cards are generally based on colored photographs, and are readily identified by the glossy appearance given by the paper's coating. 'These still photographs made the invisible visible, the unnoticed noticed, the complex simple and the simple complex. The power of the still photograph forms symbolic structures and make the image a reality', as Elizabeth Edwards wrote in her book: The Tourist Image: Myths and Myth Making in Tourism.
In 1973 the British Post Office introduced a new type of card, PHQ Cards, popular with collectors, especially when they have the appropriate stamp affixed and a First day of issue postmark obtained.
Postcards in British India.
In July 1879, the Post Office of India introduced a 1/4 anna postcard that provided postage from one place to another within British India. This was the cheapest form of post provided to the Indian people to date and proved a huge success. The establishment of a large postal system spanning India resulted in unprecedented postal access where a message on a postcard could be sent from one part of the country to another part (often to a physical address without a nearby post office) without additional postage affixed. This was followed in April 1880 by postcards meant specifically for government use and by reply post cards in 1890.:423–424 The postcard facility continues to this date in independent India.
British seaside postcards.
In 1894, British publishers were given permission by the Royal Mail to manufacture and distribute picture postcards, which could be sent through the post. It was originally thought that the first UK postcards were produced by printing firm Stewarts of Edinburgh but later research published in Picture Postcard Monthly in 1991, has shown that the first GB picture card was published by ETW Dennis of Scarborough. Two postmarked examples of the September 1894 E T W Dennis card have survived but no cards of Stewart dated 1894 have been found. Early postcards were pictures of landmarks, scenic views, photographs or drawings of celebrities and so on. With steam locomotives providing fast and affordable travel, the seaside became a popular tourist destination, and generated its own souvenir-industry. 
In the early 1930s, cartoon-style saucy postcards became widespread, and at the peak of their popularity the sale of saucy postcards reached a massive 16 million a year. They were often bawdy in nature, making use of innuendo and double entendres and traditionally featured stereotypical characters such as vicars, large ladies and put-upon husbands, in the same vein as the "Carry On" films. In the early 1950s, the newly elected Conservative government were concerned at the apparent deterioration of morals in Britain and decided on a crackdown on these postcards. The main target on their hit list was the postcard artist Donald McGill. In the more liberal 1960s, the saucy postcard was revived and later became to be considered, by some as an art form. However, during the 1970s and 1980s, the quality of the artwork and humour started to deteriorate and, with changing attitudes towards the cards' content, the demise of the saucy postcard occurred. Original postcards are now highly sought after, and rare examples can command high prices at auction. The best-known saucy seaside postcards were created by a publishing company called Bamforths, based in the town of Holmfirth, West Yorkshire, England.
Despite the decline in popularity of postcards that are overtly 'saucy', postcards continue to be a significant economic and cultural aspect of British seaside tourism. Sold by newsagents and street vendors, as well as by specialist souvenir shops, modern seaside postcards often feature multiple depictions of the resort in unusually favourable weather conditions. John Hinde, the British photographer, used saturated colour and meticulously planned his photographs, which made his postcards of the later twentieth century become collected and admired as kitsch. Such cards are also respected as important documents of social history, and have been influential on the work of Martin Parr.
Japan.
In Japan, official postcards have one side dedicated exclusively to the address, and the other side for the content, though commemorative picture postcards and private picture postcards also exist. In Japan today, two particular idiosyncratic postcard customs exist: New Year's Day postcards (年賀状, nengajō) and return postcards (往復はがき, ōfuku-hagaki). New Year's Day postcards serve as greeting cards, similar to Western Christmas cards, while return postcards function similarly to a self-addressed stamped envelope, allowing one to receive a reply without burdening the addressee with postage fees. Return postcards consist of a single double-size sheet, and cost double the price of a usual postcard – one addresses and writes one half as a usual postcard, writes one's own address on the return card, leaving the other side blank for the reply, then folds and sends. Return postcards are most frequently encountered by non-Japanese in the context of making reservations at certain locations that only accept reservations by return postcard, notably at Saihō-ji (moss temple). For overseas purposes, an international reply coupon is used instead.
In Japan, official postcards were introduced in December 1873, shortly after stamps were introduced to Japan. Return postcards were introduced in 1885, sealed postcards in 1900, and private postcards were allowed from 1900.
Russia.
In the State Standard of the Russian Federation "GOST 51507-99. Postal cards. Technical requirements. Methods of Control" (2000) gives the following definition:
Post Card is a standard rectangular form of a paper for public postings. According to the same state standards, cards are classified according to the type and kind.
Depending on whether or not the image on the card printing postage stamp cards are divided into two types:
Depending on whether or not the card illustrations, cards are divided into two types:
Cards, depending on the location of illustrations divided into:
Depending on the walking area cards subdivided into:
Free postcards.
Specialist marketing companies in many countries produce and distribute advertising postcards which are available for free. These are normally offered on wire rack displays in plazas, coffee shops and other commercial locations, usually not intended to be mailed.
Controversy.
The initial appearance of picture postcards (and the enthusiasm with which the new medium was embraced) raised some legal issues. Picture postcards allowed and encouraged many individuals to send images across national borders, and the legal availability of a postcard image in one country did not guarantee that the card would be considered "proper" in the destination country, or in the intermediate countries that the card would have to pass through. Some countries might refuse to handle postcards containing sexual references (in seaside postcards) or images of full or partial nudity (for instance, in images of classical statuary or paintings).
In response to this new phenomenon, the Ottoman Empire banned the sale or importation of some materials relating to the Islamic prophet Muhammad in 1900. Affected postcards that were successfully sent through the Ottoman Empire before this date (and are postmarked accordingly) have a high rarity value and are considered valuable by collectors.

</doc>
<doc id="55325" url="http://en.wikipedia.org/wiki?curid=55325" title="Total Recall (1990 film)">
Total Recall (1990 film)

Total Recall is a 1990 American science fiction action film directed by Paul Verhoeven, starring Arnold Schwarzenegger, Rachel Ticotin, and Sharon Stone. The film is loosely based on the Philip K. Dick story "We Can Remember It for You Wholesale". It tells the story of a construction worker who is having troubling dreams about Mars and a mysterious woman there. It was written by Ronald Shusett, Dan O'Bannon, Jon Povill, and Gary Goldman, and won a Special Achievement Academy Award for its visual effects. The original score composed by Jerry Goldsmith won the BMI Film Music Award.
The film was one of the most expensive films made at the time of its release, although estimates of its exact production budget vary and it is not certain whether it ever actually held the record. "Rambo III", "Who Framed Roger Rabbit" and "Die Hard 2: Die Harder" are considered the most expensive films released within the production period and year of release of "Total Recall".
Plot.
In 2048, Earthbound construction worker Douglas Quaid is having troubling dreams about Mars and a mysterious woman there. His wife Lori dismisses the dreams and discourages him from thinking about Mars, where the governor, Vilos Cohaagen, is fighting rebels while searching for a rumored alien artifact located in the mines. At "Rekall", a company that provides memory implants of vacations, Quaid opts for a memory trip to Mars as a secret agent fantasy. However, during the procedure, before the memory is implanted, something goes wrong, and the story diverges between the question of what is real and what is hallucination. Apparently, Quaid starts revealing previously suppressed memories of actually being a secret agent. The company sedates him, wipes his memory of the visit, and sends him home. On the way home, Quaid is attacked by his friend Harry and some construction coworkers; he is forced to kill them, revealing elite fighting-skills. He is then attacked in his apartment by Lori, who reveals that she was never his wife; their marriage was just a false memory implant and Cohaagen sent her as an agent to monitor Quaid. He is then attacked and pursued by armed thugs led by Richter, Lori's real husband and Cohaagen's operative.
After evading his attackers, Quaid is given a suitcase containing money, gadgets, fake IDs, a disguise, and a video recording. The video is of Quaid himself, who identifies himself as "Hauser" and explains that he used to work for Cohaagen, but learned about the artifact and underwent the memory wipe to protect himself. "Hauser" instructs Quaid to remove a tracking device located inside his skull before ordering him to go to Mars and check into the Hilton with a fake ID. Quaid makes his way to Mars and follows clues to Venusville, the colony's red-light district populated by a people mutated as a result of poor radiation shielding. He meets Benny, a taxi driver, and Melina, the woman from his dreams; but she spurns him, believing that Quaid is still working for Cohaagen.
Quaid later encounters Dr. Edgemar and Lori, who claim Quaid has suffered a "schizoid embolism" and is trapped in a fantasy based on the implanted memories. Edgemar warns that Quaid is headed for lunacy and a lobotomy if he does not return to reality, then offers Quaid a pill that would waken him from the dream. Quaid puts the pill in his mouth, but after seeing Edgemar sweating in fear, he kills Edgemar and spits out the pill instead of swallowing it. Lori alerts Richter's forces, who burst into the room and capture Quaid, but Melina rescues him, with Quaid killing Lori in the process. The two race back to the Venusville bar and escape into the tunnels with Benny. Unable to locate Quaid, Cohaagen shuts down the ventilation to Venusville, slowly asphyxiating its citizens. Quaid, Melina, and Benny are taken to a resistance base, and Quaid is introduced to Kuato, a parasitic twin conjoined to his brother's stomach. Kuato reads Quaid's mind and tells him that the alien artifact is a turbinium reactor that will create a breathable atmosphere for Mars when activated, eliminating Cohaagen's abusive monopoly on breathable air. Cohaagen's forces burst in and kill most of the resistance, including Kuato, who instructs Quaid to start the reactor. Benny reveals that he is also working for Cohaagen.
Quaid and Melina are taken to Cohaagen, who reveals the Quaid persona was a ploy by Hauser to infiltrate the mutants and lead Cohaagen to Kuato, thereby wiping out the resistance. Cohaagen orders Hauser's memory to be re-implanted in Quaid and Melina programmed as Hauser's slave, but Quaid and Melina escape into the mines where the reactor is located. They work their way to the control room of the reactor, and Benny attacks them in an excavation machine. Quaid kills Benny, then confronts Richter and his men, killing them too.
Quaid reaches the reactor control room, where Cohaagen is waiting with a bomb. During the ensuing struggle, Cohaagen triggers the bomb, but Quaid throws it away, blowing out one of the walls of the control room and causing an explosive decompression. While reaching for the reactor controls, Quaid knocks out Cohaagen, which causes him to be sucked out onto the Martian surface, killing him. Quaid manages to activate the reactor before he and Melina are also pulled out. The reactor releases air into the Martian atmosphere, saving Quaid, Melina and the rest of Mars' population. As humans walk onto the surface of the planet in its new atmosphere, Quaid momentarily pauses to wonder whether he is dreaming before turning to kiss Melina.
Production.
The original screenplay was written by Dan O'Bannon and Ronald Shusett, the writers of "Alien", who had bought the rights to Philip K. Dick's short story "We Can Remember It for You Wholesale" while Dick was still alive. They were unable to find a backer for the project and it drifted into development hell, passing from studio to studio.
In the mid-1980s, producer Dino De Laurentiis took on the project with Richard Dreyfuss attached to star. Patrick Swayze, who had recently starred in "Dirty Dancing", was also considered for the role. In 1987, it was announced that De Laurentiis would make the film as the first production for his DEL company at the new De Laurentiis film studios on the Gold Coast, with Bruce Beresford to direct from a screenplay by O'Bannon and Shusett. This film did not eventuate.
David Cronenberg was attached to direct but wanted to cast William Hurt in the lead role. Cronenberg described his work on the project and eventual falling out with Shusett: "I worked on it for a year and did about 12 drafts. Eventually we got to a point where Ron Shusett said, 'You know what you've done? You've done the Philip K. Dick version.' I said, 'Isn't that what we're supposed to be doing?' He said, 'No, no, we want to do "Raiders of the Lost Ark Go to Mars".'". When the adaptation of "Dune" flopped at the box office, De Laurentiis similarly lost enthusiasm for the project. Although he went uncredited in the final version of the film, Cronenberg originated the idea of mutants on Mars, including the character of Kuato (spelled Quato in his screenplay).
The collapse of De Laurentiis' company provided an opening for Schwarzenegger, who had unsuccessfully approached the producer about starring in the film. He persuaded Carolco to buy the rights to the film for a comparatively cheap $3 million and negotiated a salary of $10–11 million (plus 15% of the profits) to star, with an unusually broad degree of control over the production. He obtained veto power over the producer, director, screenplay, co-stars and promotion. The first thing Schwarzenegger did was personally recruit Paul Verhoeven to direct the film, having been impressed by the Dutch director's "RoboCop" (for which Schwarzenegger was considered for the title role). By this time the script had been through forty-two drafts but it still lacked a third act. Gary Goldman was therefore brought in by Paul Verhoeven to work with Ronald Shusett to develop the final draft of the screenplay. The director also brought in many of his collaborators on "RoboCop", including actor Ronny Cox, cinematographer Jost Vacano, production designer William Sandell, editor Frank J. Urioste, and special effects designer Rob Bottin.
Filming.
Much of the filming took place on location in Mexico City and at Estudios Churubusco. The futuristic subway station and vehicles are actually part of the Mexican public transportation system, with the subway cars painted gray and television monitors added.
Rating.
The film was initially given an X rating. Violence was trimmed and different camera angles were used in the over-the-top scenes for an R rating. It was one of the last major Hollywood blockbusters to make large-scale use of miniature effects rather than computer generated imagery. Five different companies were brought in to handle "Total Recall"‍ '​s effects. The only CGI sequence in the entire film was a 42-second sequence, produced by MetroLight Studios, showing the X-rayed skeletons of commuters and their concealed weapons. Only a year later, James Cameron's "", also starring Schwarzenegger, prompted a revolution in special effects with its extensive use of CGI.
Soundtrack.
The score was composed and conducted by Jerry Goldsmith, and 40 minutes of it was released by the Varèse Sarabande label in 1990. Ten years later, the same label released a "Deluxe Edition," in chronological order with additional cues that were left out, totaling 74 minutes. As with several Goldsmith scores, the music was performed by the National Philharmonic Orchestra.
The main title theme features a metal percussion pattern that bears similarities to a drum pattern from "Anvil of Crom". The score has been hailed as one of Goldsmith's best, especially as heard in the deluxe edition, and commended for its blend of electronic and orchestral elements.
Reception.
Critical response.
"Total Recall" debuted at number one at the box office. The film grossed $261,299,840 worldwide, a box office success. Critical reaction to "Total Recall" has been mostly positive. It currently holds an 84% positive rating on Rotten Tomatoes, based on 51 reviews. Metacritic rated it 57 out of 100 based on 17 reviews.
Roger Ebert awarded the film three and a half stars (out of four), calling it "one of the most complex and visually interesting science fiction movies in a long time." Owen Gleiberman of "Entertainment Weekly" gave it a score of "B+" and said that it "starts out as mind-bending futuristic satire and then turns relentless [and] becomes a violent, post-punk version of an Indiana Jones cliff-hanger." Film scholar William Buckland considers it one of the more "sublime" Philip K. Dick adaptations, contrasting it with films like "Impostor" and "Paycheck", which he considered "ridiculous".
Mick LaSalle of the "San Francisco Chronicle" said the film is not a classic, "but it's still solid and entertaining." James Berardinelli gave the film two and a half stars (out of four), saying that "neither Schwarzenegger nor Verhoeven have stretched their talents here," but added, "with a script that's occasionally as smart as it is energetic, "Total Recall" offers a little more than wholesale carnage."
Some critics, such as Janet Maslin of "The New York Times", considered the film excessively violent. Rita Kempley of "The Washington Post" gave it a negative review, saying that director Paul Verhoeven "disappoints with this appalling onslaught of blood and boredom." Feminist cultural critic Susan Faludi called it one of "an endless stream of war and action movies" in which "women are reduced to mute and incidental characters or banished altogether."
The film ranked number 79 on Rotten Tomatoes’ Journey Through Sci-Fi (100 Best-Reviewed Sci-Fi Movies).
Accolades.
In 2008, "Total Recall" was nominated for AFI's Top 10 Science Fiction Films list.
Legacy.
Novelization.
The film was novelized by Piers Anthony. The novel and film correspond fairly well, although Anthony was evidently working from an earlier script than the one used for the film, and was criticized for the ending of his book which removed the ambiguity whether the events of "Total Recall" are real or a dream. In addition, the novel had a subplot wherein the aliens planted a fail-safe device within their Mars technology, so that if it were misused or destroyed, the local star would go nova and therefore prevent the species from entering the galactic community. It coincided with a comment earlier in the novel that astronomers were noticing an abnormal number of recent supernovae, giving an indication that the aliens seeded their tech as part of a galactic experiment in technological maturity. Instead of mentioning that he dreamt of her earlier in the film, Melina mentions she was once a model, explaining how Quaid could have seen her on the screen at Rekall.
Video game.
A video game was made based on the film, featuring 2D action, platformer scenes and top-down racing scenes; a version was released for popular 8-bit home computers (Commodore 64, ZX Spectrum and Amstrad CPC), and popular 16-bit home computers (Amiga and Atari ST). The game was developed and released by Ocean Software, reaching number 2 in the UK sales charts, behind "Teenage Mutant Ninja Turtles". There was also a NES version which was notably different from the others, being developed by a different team (Interplay), who were subcontracted by Acclaim Entertainment. Interplay defended the changes, however, claiming that their alteration stuck closer to the spirit of the original short story, which they said "read more like a platformer." In a tie-in with the NES game, the August 1990 version of "Nintendo Power" promoted the game for their well-known monthly mail-in contests, under the Rekall hype "Making the Impossible Possible" whereby first prize would be one of the Martian police uniforms along with a videotaped trip to Hollywood with a chance to meet Schwarenegger. Years later, the magazine admitted it felt that was their worst promotion, as "our winner did not get to meet Arnold until late 1991, and even then only for a quick handshake." 
Television series.
A television series called "Total Recall 2070" went into production in 1999. The show was meant to be a sequel; however, it had far more similarities with the "Blade Runner" film (also inspired by a Philip K. Dick story) than Verhoeven's film. The two-hour series pilot, released on VHS and DVD for the North American market, borrowed footage from the film, such as the space cruiser arriving on Mars.
Comic series.
In 2011, a four-issue comic book adaptation was released by Dynamite Entertainment, continuing the story from the film.
Sequel.
Due to the film's success, a sequel was written with the script title "Total Recall 2", and with Schwarzenegger's character still Douglas Quaid, now working as a reformed law enforcer. The sequel was based on another Philip K. Dick short story, "The Minority Report", which hypothesizes about a future where a crime can be solved before it is committed—in the movie, the clairvoyants would be Martian mutants. The sequel was not filmed, but the script survived and it was changed drastically and contained greater elements from the original short story. The story was eventually adapted into the Steven Spielberg sci-fi thriller "Minority Report", which opened in 2002 to commercial success.
Remake.
In February 2009, "The Hollywood Reporter" stated that Neal H. Moritz and Original Film were in negotiations for developing a contemporary version of "Total Recall" for Columbia. In June, 2009, it was announced that Columbia Pictures had hired Kurt Wimmer to write the script for the remake. Over a year later, Len Wiseman was hired to direct.
On January 9, 2011, it was confirmed that Colin Farrell would be starring in the remake and Bryan Cranston would play the villain, with production starting in Toronto on May 15. According to producer Neal Moritz, this version of the film would be closer to Dick's original story. Moritz also stated that the film would not be shot in 3D, saying: "we decided that it would be too much." Kate Beckinsale was cast in the role of agent Lori, while John Cho was cast as McClane, the smooth-talking rep for the memory company.
The film was released on August 3, 2012 and received mixed reviews.

</doc>
<doc id="55329" url="http://en.wikipedia.org/wiki?curid=55329" title="Neturei Karta">
Neturei Karta

Neturei Karta (Jewish Babylonian Aramaic: נטורי קרתא nāṭūrī qarṯā, literally "Guardians of the City") is a Jewish religious group, formally created in Jerusalem, British Mandate of Palestine, in 1938, splitting off from Agudas Yisrael. Neturei Karta opposes Zionism and calls for a dismantling of the State of Israel, in the belief that Jews are forbidden to have their own state until the coming of the Jewish Messiah. They live as a part of larger Haredi communities around the globe.
In Israel some members also pray at affiliated "beit midrash", in Jerusalem's Meah Shearim neighborhood and in Ramat Beit Shemesh Bet. Neturei Karta states that no official count of the number of members exists. The Jewish Virtual Library puts their numbers at 5,000 The Anti-Defamation League estimates that fewer than 100 members of the community take part in anti-Israel activism.
According to Neturei Karta:
"The name Neturei Karta is a name usually given to those people who regularly pray in the Neturei Karta synagogues (Torah Ve'Yirah Jerusalem, Torah U'Tefillah London, Torah U'Tefillah NY, Beis Yehudi Upstate NY, etc.), study in or send their children to educational institutions run by Neturei Karta, or actively participate in activities, assemblies or demonstrations called by the Neturei Karta".
History.
The name "Neturei Karta" literally means "Guardians of the City" in Aramaic and comes from the gemara of the Jerusalem Talmud, "Hagigah", 76c. There it is related that Rabbi Judah haNasi sent two rabbis on a tour of inspection:
In one town they asked to see the "guardians of the city" and the city guard was paraded before them. They said that these were not the guardians of the city but its destroyers, which prompted the citizens to ask who, then, could be considered the guardians. The rabbis answered, "The scribes and the scholars," referring them to "Tehillim" (Psalms) Chapter 127.
It is this role that Neturei Karta see themselves as fulfilling by defending what they believe is "the position of the Torah and authentic unadulterated Judaism." Neturei Karta is sometimes confused with Satmar, due to both being anti-Zionist. They are separate groups and have had disagreements.
For the most part, the members of Neturei Karta are descended from Hungarian Jews who settled in Jerusalem's Old City in the early nineteenth century, and from Lithuanian Jews who were students of the Gaon of Vilna (known as "Perushim"), who had settled earlier. In the late nineteenth century, their ancestors participated in the creation of new neighborhoods outside the city walls to alleviate overcrowding in the Old City, and most are now concentrated in the neighborhood of Batei Ungarin and the larger Meah Shearim neighborhood.
At the time, they were vocal opponents to the new political ideology of Zionism that was attempting to assert Jewish sovereignty in Ottoman-controlled Palestine. They resented the new arrivals, who were predominantly secular and anti-religious, while they asserted that Jewish redemption could only be brought about by the Jewish messiah.
Other Orthodox Jewish movements, including some who oppose Zionism, have denounced the activities of the radical branch of Neturei Karta. According to "The Guardian", "[e]ven among Haredi, or ultra-Orthodox circles, the Neturei Karta are regarded as a wild fringe". Neturei Karta asserts that the mass media deliberately downplays their viewpoint and makes them out to be few in number. Their protests in America are usually attended by, at most, a few dozen people. In Israel, several hundred is typical, depending on the nature of the protest and its location.
In July 2013, the Shabak arrested a 46-year-old Neturei Karta member for attempting to spy on Israel for Iran. As part of a plea deal with prosecutors, the man was sentenced to 41⁄2 years in prison.
Neturei Karta's website states that its members "frequently participate[s] in public burning of the Israeli flag." On the Jewish holiday of Purim, Neturei Karta members have routinely burned Israeli flags in celebrations in cities such as London, Brooklyn and Jerusalem.
While many in Neturei Karta chose to simply ignore the State of Israel, this became more difficult. Some took steps to condemn Israel and bring about its eventual dismantling until the coming of the Messiah. Chief among these was Moshe Hirsch, leader of an activist branch of Neturei Karta, who served in Yasser Arafat's cabinet as Minister for Jewish Affairs.
Beliefs.
Neturei Karta stresses what is said in the mussaf Shemona Esrei of Yom Tov, that because of their sins, the Jewish people went into exile from the Land of Israel ("umipnei chatoeinu golinu meiartzeinu"). Additionally, they maintain the view – based on the Babylonian Talmud – that any form of forceful recapture of the Land of Israel is a violation of divine will. They believe that the restoration of the Land of Israel to the Jews should only happen with the coming of the Messiah, not by self-determination.
Neturei Karta believes that the exile of the Jews can only end with the arrival of the Messiah, and that human attempts to establish Jewish sovereignty over the Land of Israel are sinful. In Neturei Karta's view, Zionism is a presumptuous affront against God. Among their arguments against Zionism was a Talmudic discussion about portions of the Bible regarding a pact known as the Three Oaths made between God, the Jewish people, and the nations of the world, when the Jews were sent into exile. One provision of the pact was that the Jews would not rebel against the non-Jewish world that gave them sanctuary; a second was that they would not immigrate en masse to the Land of Israel. In return the gentile nations promised not to persecute the Jews. By rebelling against this pact, they argued, the Jewish People were engaging in rebellion against God.
The Neturei Karta synagogues follow the customs of the Gaon of Vilna, due to Neturei Karta's origin within the Lithuanian rather than Hasidic branch of ultra-Orthodox Judaism. Neturei Karta is not a Hasidic but a Litvish group, they are often mistaken for Hasidim because their style of dress (including a shtreimel on Shabbos) is very similar to that of Hasidim. This style of dress is not unique to Neturei Karta, but is also the style of other Jerusalem Litvaks, such as Rabbi Yosef Sholom Eliashiv and his followers. Furthermore, Shomer Emunim, a Hasidic group with a similar anti-Zionist ideology, is often bundled together with Neturei Karta. Typically, the Jerusalem Neturei Karta will keep the customs of the "Old "Yishuv"" of the city of Jerusalem even when living outside of Jerusalem or even when living abroad, as a demonstration of their love and connection to the Holy Land.
Factionalism.
In the United States, the Neturei Karta are led by Moshe Ber Beck of Monsey, New York. They affiliate with the radical branch led by Moshe Hirsch. Beck has courted controversy by meeting with Nation of Islam leader Minister Louis Farrakhan, who has been accused of inciting antisemitism and of describing Judaism as a "gutter religion" (although Farrakhan insists his words were misinterpreted ). In addition, after meeting with the representatives from Neturei Karta, Farrakhan indicated he would be more cautious in his choice of words in the future.
Moshe Hirsch faction.
Relations with the Palestinians
After two men associated with the radical branch of Neturei Karta participated in a 2004 prayer vigil for Yasser Arafat outside the Percy Military Hospital in Paris, France, where he lay on his death bed, the radical branch of Neturei Karta was widely condemned by other Orthodox Jewish organizations, including many other anti-Zionist Haredi organizations both in New York and Jerusalem. Rabbi Moshe Hirsch, and what Hirsch's faction described as an "impressive contingent" of other members, attended Arafat's funeral in Ramallah.
Almost a year after the Gaza War a group of Neturei Karta members which crossed into Gaza as part of the Gaza Freedom March to celebrate Jewish "Shabbos" to show support for Palestinians in the Hamas ruled enclave.
Relations with Iran and President Mahmoud Ahmedinejad
In October 2005, Neturei Karta leader Rabbi Yisroel Dovid Weiss issued a statement criticising Jewish attacks on Iranian President Mahmoud Ahmadinejad. Weiss wrote that Ahmadinejad's statements were not "indicative of anti-Jewish sentiments", but rather, "a yearning for a better, more peaceful world", and "re-stating the beliefs and statements of Ayatollah Khomeini, who always emphasized and practiced the respect and protection of Jews and Judaism."
In March 2006, several members of a Neturei Karta's faction visited Iran where they met with Iranian leaders, including the Vice-President, and praised Ahmadinejad for calling for the Zionist regime occupying Jerusalem to vanish from the pages of time. The spokesmen commented that they shared Ahmadinejad's aspiration for "a disintegration of the Israeli government". In an interview with Iranian television reporters, Rabbi Weiss remarked, "The Zionists use the Holocaust issue to their benefit. We, Jews who perished in the Holocaust, do not use it to advance our interests. We stress that there are hundreds of thousands Jews around the world who identify with our opposition to the Zionist ideology and who feel that Zionism is not Jewish, but a political agenda. ... What we want is not a withdrawal to the '67 borders, but to everything included in it, so the country can go back to the Palestinians and we could live with them ..."
Tehran Holocaust Conference<br>
In December 2006, members of Neturei Karta, including Yisroel Dovid Weiss, attended the International Conference to Review the Global Vision of the Holocaust, a controversial conference being held in Tehran, Iran that attracted a number of high-profile Holocaust-deniers.
They praised Iranian president Mahmoud Ahmadinejad, and expressed solidarity with the Iranian position of anti-Zionism. Rabbi Yonah Metzger, the chief Ashkenazi Rabbi of Israel, immediately called for those who went to Tehran to be put into 'cherem', a form of excommunication. Subsequently a group of Rabbis claiming to represent part of the recently split anti-Zionist Satmar Hasidic group called on Jews "to keep away from them and condemn their actions". However the newspaper 'Der Blatt' which represents the largest part of the Satmar group refused to denounce the actions of Neturei Karta. In addition Neturei Karta claim that the late Rabbi Avrohom Leitner, one of the major Poskim (Halcachic decisors) of Brooklyn's large Satmar community publicly supported their activities.
On 21 December, the Edah HaChareidis rabbinical council of Jerusalem also released a statement calling on the public to distance itself from those who went to Iran. The Edah's statement followed, in major lines, the Satmar statement released a few days earlier In January 2007, a group of protesters stood outside the radical Neturei Karta synagogue in Monsey, New York, demanding that they leave Monsey and move to Iran, the Neturei Karta and their sympathisers from Monsey's Orthodox community responded with a counter protest.
2008 Mumbai attack on Nariman House.
One of the targets of the 2008 Mumbai attacks was the Nariman House which was operated by the Jewish Chabad movement. Neturei Karta subsequently issued a leaflet criticising the Chabad movement for its relations with "the filthy, deplorable traitors – the cursed Zionists that are your friends." It added that the Chabad movement has been imbued with "false national sentiment" and criticised the organisation for allowing all Jews to stay in its centres, without differentiating "between good and evil, right and wrong, pure and impure, a Jew and a person who joins another religion, a believer and a heretic." The leaflet also criticised the invitation of Israeli state officials to the funerals of the victims, claiming that they "uttered words of heresy and blasphemy." The leaflet concluded that "the road [Chabad] have taken is the road of death and it leads to doom, assimilation and the uprooting of the Torah."
Sikrikim.
A radical breakaway faction called "Sikrikim" is based in Israel, mainly in Jerusalem and Beit Shemesh. The group's engagement in acts of vandalism, "mafia-like intimidation" and violent protests caused several people, including authority figures, to push for officially labeling them as a terrorist organization, along with Neturei Karta.
Further reading.
Books
978-965-91505-0-2 (Hebrew language)

</doc>
<doc id="55331" url="http://en.wikipedia.org/wiki?curid=55331" title="Westport">
Westport

 "Westport" is the name of several communities around the world. 

</doc>
<doc id="55335" url="http://en.wikipedia.org/wiki?curid=55335" title="Discounting">
Discounting

Discounting is a financial mechanism in which a debtor obtains the right to delay payments to a creditor, for a defined period of time, in exchange for a charge or fee. Essentially, the party that owes money in the present purchases the right to delay the payment until some future date. The discount, or charge, is the difference (expressed as a difference in the same units (absolute) or in percentage terms (relative), or as a ratio) between the original amount owed in the present and the amount that has to be paid in the future to settle the debt.
The discount is usually associated with a "discount rate", which is also called the "discount yield". The discount yield is the proportional share of the initial amount owed (initial liability) that must be paid to delay payment for 1 year. 
It is also the rate at which the amount owed must rise to delay payment for 1 year.
Since a person can earn a return on money invested over some period of time, most economic and financial models assume the discount yield is the same as the rate of return the person could receive by investing this money elsewhere (in assets of similar risk) over the given period of time covered by the delay in payment. The concept is associated with the opportunity cost of not having use of the money for the period of time covered by the delay in payment. The relationship between the discount yield and the rate of return on other financial assets is usually discussed in such economic and financial theories involving the inter-relation between various market prices, and the achievement of Pareto optimality through the operations in the capitalistic price mechanism, as well as in the discussion of the efficient (financial) market hypothesis. The person delaying the payment of the current liability is essentially compensating the person to whom he/she owes money for the lost revenue that could be earned from an investment during the time period covered by the delay in payment. Accordingly, it is the relevant "discount yield" that determines the "discount", and not the other way around.
As indicated, the rate of return is usually calculated in accordance to an annual return on investment. Since an investor earns a return on the original principal amount of the investment as well as on any prior period investment income, investment earnings are "compounded" as time advances. Therefore, considering the fact that the "discount" must match the benefits obtained from a similar investment asset, the "discount yield" must be used within the same compounding mechanism to negotiate an increase in the size of the "discount" whenever the time period of the payment is delayed or extended. The "discount rate" is the rate at which the "discount" must grow as the delay in payment is extended. This fact is directly tied into the time value of money and its calculations.
The "time value of money" indicates there is a difference between the "future value" of a payment and the "present value" of the same payment. The rate of return on investment should be the dominant factor in evaluating the market's assessment of the difference between the future value and the present value of a payment; and it is the market's assessment that counts the most. Therefore, the "discount yield", which is predetermined by a related return on investment that is found in the financial markets, is what is used within the time-value-of-money calculations to determine the "discount" required to delay payment of a financial liability for a given period of time.
Basic calculation.
If we consider the value of the original payment presently due to be "P", and the debtor wants to delay the payment for "t" years, then an "r" Market Rate of Return on a similar Investment Assets means the future value of "P" is formula_1, and the discount would be calculated as
where "r" is also the discount yield.
If "F" is a payment that will be made "t" years in the future, then the "Present Value" of this Payment, also called the "Discounted Value" of the payment, is
To calculate the present value of a single cash flow, it is divided by one plus the interest rate for each period of time that will pass. This is expressed mathematically as raising the divisor to the power of the number of units of time.
Consider the task to find the present value "PV" of $100 that will be received in five years. Or equivalently, to find which amount of money today will grow to $100 in five years when subject to a constant discount rate.
Assuming a 12% per year interest rate, it follows that
Discount rate.
The discount rate which is used in financial calculations is usually chosen to be equal to the cost of capital. The cost of capital, in a financial market equilibrium, will be the same as the market rate of return on the financial asset mixture the firm uses to finance capital investment. Some adjustment may be made to the discount rate to take account of risks associated with uncertain cash flows, with other developments.
The discount rates typically applied to different types of companies show significant differences:
The higher discount rate for start-ups reflects the various disadvantages they face, compared to established companies:
One method that looks into a correct discount rate is the capital asset pricing model. This model takes into account three variables that make up the discount rate:
1. Risk free rate: The percentage of return generated by investing in risk free securities such as government bonds.
2. Beta: The measurement of how a company's stock price reacts to a change in the market. A beta higher than 1 means that a change in share price is exaggerated compared to the rest of shares in the same market. A beta less than 1 means that the share is stable and not very responsive to changes in the market. Less than 0 means that a share is moving in the opposite direction from the rest of the shares in the same market.
3. Equity market risk premium: The return on investment that investors require above the risk free rate.
Discount factor.
The discount factor, "DF(T)", is the factor by which a future cash flow must be multiplied in order to obtain the present value. For a zero-rate (also called spot rate) "r", taken from a yield curve, and a time to cash flow "T" (in years), the discount factor is:
In the case where the only discount rate you have is not a zero-rate (neither taken from a zero-coupon bond nor converted from a swap rate to a zero-rate through bootstrapping) but an annually-compounded rate (for example if your benchmark is a US Treasury bond with annual coupons and you only have its yield to maturity, you would use an annually-compounded discount factor:
However, when operating in a bank, where the amount the bank can lend (and therefore get interest) is linked to the value of its assets (including accrued interest), traders usually use daily compounding to discount cash flows. Indeed, even if the interest of the bonds it holds (for example) is paid semi-annually, the value of its book of bond will increase daily, thanks to accrued interest being accounted for, and therefore the bank will be able to re-invest these daily accrued interest (by lending additional money or buying more financial products). In that case, the discount factor is then (if the usual money market day count convention for the currency is ACT/360, in case of currencies such as United States dollar, euro, Japanese yen), with "r" the zero-rate and "T" the time to cash flow in years:
or, in case the market convention for the currency being discounted is ACT/365 (AUD, CAD, GBP):
Sometimes, for manual calculation, the continuously-compounded hypothesis is a close-enough approximation of the daily-compounding hypothesis, and makes calculation easier (even though it does not have any real application as no financial instrument is continuously compounded). In that case, the discount factor is:
Other discounts.
For discounts in marketing, see discounts and allowances, sales promotion, and pricing. The article on discounted cash flow provides an example about discounting and risks in real estate investments.
References.
Notes

</doc>
<doc id="55336" url="http://en.wikipedia.org/wiki?curid=55336" title="Cash">
Cash

In English vernacular cash refers to money in the physical form of currency, such as banknotes and coins.
In bookkeeping and finance, cash refers to current assets comprising currency or currency equivalents that can be accessed immediately or near-immediately (as in the case of money market accounts). Cash is seen either as a reserve for payments, in case of a structural or incidental negative cash flow or as a way to avoid a downturn on financial markets.
Etymology.
The word is variously attributed. Some claim that the word "cash" comes from the modern French word "caisse", which means "(money) box", from the Provençal word "caissa", from the Italian "cassa", from the Latin "capsa" all meaning "box". In the 18th century, the word passed to refer to the money instead of the actual box containing it. Another claim is that it was derived from Tamil word "kāsu" (Tamil: காசு) or Malayalam word "kāśu" (Malayalam: കാശ്) meaning "a coin", by East India Company.
"Cash" used as a verb means "to convert to cash"; for example in the expression "to cash a cheque".
History.
In Western Europe, after the Collapse of the Western Roman Empire, coins, silver jewelry and hacksilver (silver objects hacked into pieces) were for centuries the only form of money, until Venetian merchants started using silver bars for large transactions in the early Middle Ages. In a separate development, Venetian merchants started using paper bills, instructing their banker to make payments. Similar marked silver bars were in use in lands where the Venetian merchants had established representative offices. The Byzantine empire and several states in the Balkan area and Kievan Rus also used marked silver bars for large payments. As the world economy developed and silver supplies increased, in particular after the colonization of South America, coins became larger and a standard coin for international payment developed from the 15th century: the Spanish and Spanish colonial coin of 8 reales. Its counterpart in gold was the Venetian ducat.
Coin types would compete for markets. By conquering foreign markets, the issuing rulers would enjoy extra income from seigniorage (the difference between the value of the coin and the value of the metal the coin was made of). Successful coin types of high nobility would be copied by lower nobility for seigniorage. Imitations were usually of a lower weight, undermining the popularity of the original. As feudal states coalesced into kingdoms, imitation of silver types abated, but gold coins, in particular the gold ducat and the gold florin were still issued as trade coins: coins without a fixed value, going by weight. Colonial powers also sought to take away market share from Spain by issuing trade coin equivalents of silver Spanish coins, without much success.
In the early part of the 17th century, English East India Company coins were minted in England and shipped to the East. In England over time the word ‘Cash’ was adopted from Sanskrit कर्ष karsa, a weight of gold or silver but akin to Old Persian 𐎣𐎼𐏁 karsha, unit of weight (83.30 grams). East India Company coinage had both Urdu and English writing on it, to facilitate its use within trade. In 1671 the directors of The East India Company ordered a mint to be established at Bombay, known as Bombain. In 1677 this was sanctioned by the Crown, the coins, having received royal sanction were struck as silver Rupees; the inscription runs The Rupee of Bombaim, by authority of Charles II.
At about this time coins were also being produced for The East India Company at the Madras mint. The currency at The Company’s Bombay and Bengal administrative regions was The Rupee. At Madras, however, the Company's accounts were reckoned in “pagodas”, “fractions”, “fanams”, “faluce” and “cash”. This system was maintained until 1818 when the rupee was adopted as the unit of currency for the Company's operations, the relation between the two systems being 1 pagoda = 3-91 rupees and 1 rupee = 12 fanams.
Meanwhile, paper money had been developed. At first, it was thought of for emergency issues, hence were most popular in the colonies of European powers. In the 18th century, important paper issues were made in colonies such as Ceylon and the bordering colonies of Essequibo, Demerara and Berbice. John Law did pioneering work on banknotes with the "Banque Royale". However, the relation between money supply and inflation was still imperfectly understood and the bank went under, while its notes became worthless when they were over-issued. The lessons learned were applied to the Bank of England, which played a crucial role in financing Wellington's Peninsular war, against French troops, hamstrung by a metallic Franc de Germinal.
The ability to create paper money made nation-states responsible for the management of inflation, through control of the money supply. It also made a direct relation between the metal of the coin and its denomination superfluous. From 1816, coins generally became token money, though some large silver and gold coins remained standard coins until 1927. The first world war saw standard coins disappear to a very large extent. Afterwards, standard gold coins, mainly British sovereigns, would still be used in colonies and less developed economies and silver Maria Theresa thalers dated 1780 would be struck as trade coins for countries in East Asia until 1946 and possibly later locally.
Cash has now become a very small part of the money supply. Its remaining role is to provide a form of currency storage and payment for those who do not wish to take part in other systems, and make small payments conveniently and promptly, though this latter role is being replaced more and more frequently by electronic payment systems. Research has found that the demand for cash decreases as debit card usage increases because merchants need to make less change for customer purchases.
Cash is increasing in circulation. The value of the United States dollar in circulation increased by 42% from 2007 to 2012. The value of Pound Sterling banknotes in circulation increased by 29% from 2008 to 2013. The value of the Euro in circulation increased by 34% from August 2008 to August 2013 (2% of the increase was due to the adoption of Euro in Slovakia 2009 and in Estonia 2011).

</doc>
<doc id="55338" url="http://en.wikipedia.org/wiki?curid=55338" title="Rock Ridge">
Rock Ridge

The Rock Ridge Interchange Protocol (RRIP, IEEE P1282) is an extension to the ISO 9660 volume format, commonly used on CD-ROM and DVD media, which adds POSIX file system semantics. The availability of these extension properties allows for better integration with Unix and Unix-like operating systems. 
The standard takes its name from the fictional town "Rock Ridge" in Mel Brooks' film "Blazing Saddles. 
Design and contents.
The RRIP extensions are, briefly:
The RRIP extensions are built upon a related standard System Use Sharing Protocol (SUSP, IEEE P1281). SUSP provides a generic way of including additional properties for any directory entry reachable from the primary volume descriptor (PVD).
In an ISO 9660 volume, every directory entry has an optional "system use area" whose contents are undefined and left to be interpreted by the system. SUSP defines a method to subdivide that area into multiple system use fields, each identified by a two-character signature tag. The idea behind SUSP was that it would enable any number of independent extensions to ISO 9660 (not just RRIP) to be created and included on a volume without conflicting. It also allows for the inclusion of property data that would otherwise be too large to fit within the limits of the system use area.
SUSP defines several common tags and system use fields:
RRIP defines additional SUSP tags for support of POSIX semantics, along with the format and meaning of the corresponding system use fields:
Other known SUSP fields include:
Note that the Apple ISO 9660 Extensions do not technically follow the SUSP standard; however the basic structure of the AA and AB fields defined by Apple are forward compatible with SUSP; so that, with care, a volume can use both Apple extensions as well as RRIP extensions.
Variants.
"Amiga Rock Ridge" is similar to RRIP, except it provides additional properties used by the Amiga operating system. It too is built on the SUSP standard by defining an "AS"-tagged system use field. Thus both Amiga Rock Ridge and the POSIX RRIP may be used simultaneously on the same volume.
Some of the specific properties supported by this extension are the additional Amiga-bits for files. There is support for attribute "P" that stands for "pure" bit (indicating re-entrant command) and attribute "S" for script bit (indicating batch file). This includes the protection flags plus an optional comment field. These extensions were introduced by Angela Schmidt with the help of Andrew Young,
the primary author of the Rock Ridge Interchange Protocol and System Use Sharing Protocol.
The Amiga extensions are recognized by Amiga program MasterISO, and should also be recognized by MakeCD and Frying Pan, but the support by latter two programs is uncredited.
Amiga filesystems supporting the extensions are AmiCDFS, AsimCDFS and CacheCDFS.
Users who want to access comments and protection bits of their Amiga files present on CDs could simply mount some new logical units associated to the same physical unit, but using Amiga CacheCDFS as filesystem.

</doc>
<doc id="55340" url="http://en.wikipedia.org/wiki?curid=55340" title="Song Zheyuan">
Song Zheyuan

Sòng Zhéyuán (宋哲元) (October 30, 1885 – April 5, 1940) was a Chinese general during the Chinese Civil War and Second Sino-Japanese War (1937-1945).
Biography.
Early life and education.
Born in the village of Zhaohong, northwest of the seat of Leling County, Shandong, he was educated under his uncle from his mother's side, a teacher of a traditional Confucian private school in Yanshan County. At the age of 20 (1904) he began studying in the military institute founded by Lu Jianzhang at Beijing and had since become Lu's favorite. In 1912 the troops of Lu and Feng Yuxiang, now subordinates of Yuan Shikai, were regrouped and Feng had then been Song's superior.
Military career.
In 1917, a year after being appointed the head of 1st battalion of Feng's 2nd regiment, his battalion spearheaded the removal of Zhang Xun from his imperial restoration in 1917. As part of the Guominjun he became Governor of Jehol Province in 1926. Following the defeat of the Guominjun in the Anti–Fengtian War Feng Yuxiang participated in the Northern Expedition, Sòng assumed the Chairmanship of Shaanxi province in November 1927 and in April of the same year the head of 4th division under the II Corps of the National Revolutionary Army.
Switching sides to the Kuomintang after the abortive coup d'état in 1930 of Feng against Chiang Kai-shek, his troops were designated as the 29th Army and garrisoned in southern Shanxi province where he was responsible for the frontiers of the Rehe and Chahar provinces against the Japanese in Manchukuo.
Chair of Chahar province.
Song was the chairman of Chahar province when Japan invaded the provinces in the end of year 1932. Though poorly equipped compared to the better armed Japanese, Song led the 29th army to resist the aggression in a war known as the Defense of the Great Wall (熱河長城之戰). Japanese troops then entered the suburbs of Beijing and Tianjin after the predictable victory. Song was relieved from his post but reinstalled as commander after the Ho-Umezu agreement.
Later years.
In the Battle of Lugou Bridge, his 29th Army bore the brunt of the Japanese Guandong Army. His troops were halved after the defeat and chased by the Japanese along the Jinpu Railway into Shandong Province during the Beiping–Hankou Railway Operation. However Han Fuqu, chairman of the province and suspected for his clandestine Japanese liaison, forbade Song to retreat across the Yellow River resulting in the 29th Army being shattered at Shijiazhuang in December 1937 and January 1938. Remaining forces suffered various losses against the Imperial Japanese Army and were delegated to guerrilla combat after retreating into the mountainous regions at the borders of Henan and Shanxi province in February 1938.
He soon suffered various illnesses and died at the age of 56 in Mianyang County, Sichuan province after several unsuccessful medical treatments in Guilin, Chongqing, and Chengdu.

</doc>
<doc id="55344" url="http://en.wikipedia.org/wiki?curid=55344" title="Hermann Huppen">
Hermann Huppen

Hermann Huppen (born 17 July 1938) is a Belgian comic book artist. He is better known under his pen-name Hermann. He is most famous for his post-apocalyptic comic "Jeremiah" which was made into a television series.
Biography.
Hermann was born in 1938 in Bévercé (now a part of Malmedy) in Liège Province. After studying to become a furniture maker and working as interior architect, Hermann made his debut as comic book artist in 1964 in the Franco-Belgian comics magazine "Spirou" with a four page story. Greg noticed his talent and offered him to work for his studio. In 1966, he began illustrating the "Bernard Prince" series written by Greg, published in "Tintin" magazine. In 1969, also in collaboration with Greg, he began the western series "Comanche". This appeared at the same time as other western series such as "Blueberry".
Hermann began writing his own stories in 1977, starting the post-apocalyptic "Jeremiah" series, which is still produced today. In the same period, he also made three albums of "Nick", inspired by "Little Nemo in Slumberland", for "Spirou". In 1983 he began a new series, "Les Tours de Bois-Maury", which is set in the Middle Ages and is less focused on action than his other works.
Hermann has also created many non-series graphic novels sometimes together with his son Yves H. One of them, "Lune de Guerre", with a story by Jean Van Hamme, was later filmed as "The Wedding Party" by Dominique Deruddere.
Hermann is characterized by a realistic style and stories that are both somber and angry, with a sense of disillusion with regards to the human character in general, and current society more specifically.
Selected bibliography.
Most of these comics have been published in French and Dutch: other translations are noted in the "remarks" column.
Sources.
</dl>

</doc>
<doc id="55345" url="http://en.wikipedia.org/wiki?curid=55345" title="Net present value">
Net present value

In finance, the net present value (NPV) or net present worth (NPW) is defined as the sum of the present values (PVs) of incoming and outgoing cash flows over a period of time. Incoming and outgoing cash flows can also be described as benefit and cost cash flows, respectively.
Time value of money dictates that time has an impact on the value of cash flows. Cash flows of "nominal" equal value over a time series result in different "effective" value cash flows that makes future cash flows less valuable over time. If for example there exists a time series of identical cash flows, the cash flow in the present is the most valuable, with each future cash flow becoming less valuable than the previous cash flow. Thus, a cash flow today is more valuable than an identical cash flow in the future. This decrease occurs because the discount factor represents the expected rate of return of each cash flow in a different investment with identical risk. With each additional period, the present value of a subsequent future cash flow decreases.
The NPV of an investment is determined by calculating the present value (PV) of the total benefits and costs which is achieved by discounting the future value of each cash flow (see Formula). NPV is a useful tool to determine whether a project or investment will result in a net profit or a loss because of its simplicity. A positive NPV results in profit, while a negative NPV results in a loss. The NPV measures the excess or shortfall of cash flows, in present value terms, above the cost of funds. In a theoretical situation of unlimited capital budgeting a company should pursue every investment with a positive NPV. However, in practical terms a company's capital constraints limit investments to projects with the highest NPV whose cost cash flows do not exceed the company's capital. NPV is a central tool in discounted cash flow (DCF) analysis and is a standard method for using the time value of money to appraise long-term projects. It is widely used throughout economics, finance, and accounting.
In the case when all future cash flows are incoming (such as the principal and coupon payment of a bond) the only outflow of cash is the purchase price, the NPV is simply the PV of future cash flows minus the purchase price (which is its own PV). NPV can be described as the “difference amount” between the sums of discounted cash inflows and cash outflows. It compares the present value of money today to the present value of money in the future, taking inflation and returns into account.
The NPV of a sequence of cash flows takes as input the cash flows and a discount rate or discount curve and outputs a price. The converse process in DCF analysis — taking a sequence of cash flows and a price as input and inferring as output a discount rate (the discount rate which would yield the given price as NPV) — is called the yield and is more widely used in bond trading.
Formula.
Each cash inflow/outflow is discounted back to its present value (PV). Then they are summed. Therefore NPV is the sum of all terms,
where
The result of this formula is multiplied with the Annual Net cash in-flows and reduced by Initial Cash outlay the present value but in cases where the cash flows are not equal in amount, then the previous formula will be used to determine the present value of each cash flow separately. Any cash flow within 12 months will not be discounted for NPV purpose, nevertheless the usual initial investments during the first year "R"0 are summed up a negative cash flow.
Given the (period, cash flow) pairs (formula_2, formula_4) where formula_8 is the total number of periods, the net present value formula_9 is given by:
Many computer-based spreadsheet programs have built-in formulae for PV and NPV.
The discount rate.
The rate used to discount future cash flows to the present value is a key variable of this process.
A firm's weighted average cost of capital (after tax) is often used, but many people believe that it is appropriate to use higher discount rates to adjust for risk, opportunity cost, or other factors. A variable discount rate with higher rates applied to cash flows occurring further along the time span might be used to reflect the yield curve premium for long-term debt.
Another approach to choosing the discount rate factor is to decide the rate which the capital needed for the project could return if invested in an alternative venture. If, for example, the capital required for Project A can earn 5% elsewhere, use this discount rate in the NPV calculation to allow a direct comparison to be made between Project A and the alternative. Related to this concept is to use the firm's reinvestment rate. Reinvestment rate can be defined as the rate of return for the firm's investments on average. When analyzing projects in a capital constrained environment, it may be appropriate to use the reinvestment rate rather than the firm's weighted average cost of capital as the discount factor. It reflects opportunity cost of investment, rather than the possibly lower cost of capital.
An NPV calculated using variable discount rates (if they are known for the duration of the investment) may better reflect the situation than one calculated from a constant discount rate for the entire investment duration. Refer to the tutorial article written by Samuel Baker for more detailed relationship between the NPV value and the discount rate.
For some professional investors, their investment funds are committed to target a specified rate of return. In such cases, that rate of return should be selected as the discount rate for the NPV calculation. In this way, a direct comparison can be made between the profitability of the project and the desired rate of return.
To some extent, the selection of the discount rate is dependent on the use to which it will be put. If the intent is simply to determine whether a project will add value to the company, using the firm's weighted average cost of capital may be appropriate. If trying to decide between alternative investments in order to maximize the value of the firm, the corporate reinvestment rate would probably be a better choice.
Using variable rates over time, or discounting "guaranteed" cash flows differently from "at risk" cash flows, may be a superior methodology but is seldom used in practice. Using the discount rate to adjust for risk is often difficult to do in practice (especially internationally) and is difficult to do well. An alternative to using discount factor to adjust for risk is to explicitly correct the cash flows for the risk elements using rNPV or a similar method, then discount at the firm's rate.
Use in decision making.
NPV is an indicator of how much value an investment or project adds to the firm. With a particular project, if formula_4 is a positive value, the project is in the status of positive cash inflow in the time of "t". If formula_4 is a negative value, the project is in the status of discounted cash outflow in the time of "t". Appropriately risked projects with a positive NPV could be accepted. This does not necessarily mean that they should be undertaken since NPV at the cost of capital may not account for opportunity cost, "i.e.," comparison with other available investments. In financial theory, if there is a choice between two mutually exclusive alternatives, the one yielding the higher NPV should be selected.
Interpretation as integral transform.
The time-discrete formula of the net present value
can also be written in a continuous variation
where
Net present value can be regarded as Laplace- respectively Z-transformed cash flow with the integral operator including the complex number "s" which resembles to the interest rate "i" from the real number space or more precisely "s" = ln(1 + "i").
From this follow simplifications known from cybernetics, control theory and system dynamics. Imaginary parts of the complex number "s" describe the oscillating behaviour (compare with the pork cycle, cobweb theorem, and phase shift between commodity price and supply offer) whereas real parts are responsible for representing the effect of compound interest (compare with damping).
Example.
A corporation must decide whether to introduce a new product line. The company will have immediate costs of 100,000 at "t = 0". Recall, a cost is a negative or outgoing cash flow, thus this cash flow is represented as -100,000. The company assumes the product will provide equal benefits of 10,000 for each of 12 years beginning at "t = 1". For simplicity, assume the company will have no outgoing cash flows after the initial 100,000 cost. This also makes the simplifying assumption that the net cash received or paid is lumped into a single transaction occurring "on the last day" of each year. At the end of the 12 years the product no longer provides any cash flow and is discontinued without any additional costs. Assume that the effective annual discount rate is 10%.
The present value (value at "t = 0") can be calculated for each year:
The total present value of the incoming cash flows is 68,136.92. The total present value of the outgoing cash flows is simply the 100,000 at time "t = 0". 
Thus:
formula_16
Rearranging the formula:
formula_17
In the above example:
formula_18
formula_19
Observe that as "t" increases the present value of each cash flow at "t" decreases. For example, the final incoming cash flow has a future value of 10,000 at "t = 12" but has a present value (at "t = 0") of 3,186.31. The opposite of discounting is compounding. Taking the example in reverse, it is the equivalent of investing 3,186.31 at "t = 0" (the present value) at an interest rate of 10% compounded for 12 years, which results in a cash flow of 10,000 at "t = 12" (the future value).
The importance of NPV becomes clear in this instance. Although the incoming cash flows (10,000 x 12 = 120,000) appear to exceed the outgoing cash flow (100,000), the future cash flows are not adjusted using the discount rate. Thus, the project appears misleadingly profitable. When the cash flows are discounted however, it indicates the project would result in a net loss of 31,863.08. Thus, the NPV calculation indicates that this project should be disregarded because investing in this project is the equivalent of a loss of 31,863.08 at "t = 0". The concept of time value of money indicates that cash flows in different periods of time cannot be accurately compared unless they have been adjusted to reflect their value at the same period of time (in this instance, "t = 0"). It is the present value of each future cash flow that must be determined in order to provide any meaningful comparison between cash flows at different periods of time. There are a few inherent assumptions in this type of discounted cash flow / net present value type analysis:
More realistic problems would also need to consider other factors, generally including: smaller time buckets, the calculation of taxes (including the cash flow timing), inflation, currency exchange fluctuations, hedged or unhedged commodity costs, risks of technical obsolescence, potential future competitive factors, uneven or unpredictable cash flows, and a more realistic salvage value assumption, as well as many others.
A more simple example of the net present value of incoming cash flow over a set period of time, would be winning a Powerball lottery of $500 million. If one does not select the "CASH" option they will be paid $25,000,000 dollars per year for 20 years, a total of $500,000,000, however, if one does select the "CASH" option, they will receive a one-time lump sum payment of approximately $285 million, the NPV of $500,000,000 paid over time. See "other factors" above that could affect the payment amount. Both scenarios are before taxes.
History.
Net present value as a valuation methodology dates at least to the 19th century. Karl Marx refers to NPV as fictitious capital, and the calculation as "capitalising," writing:
In mainstream neo-classical economics, NPV was formalized and popularized by Irving Fisher, in his 1907 "The Rate of Interest" and became included in textbooks from the 1950s onwards, starting in finance texts.

</doc>
<doc id="55347" url="http://en.wikipedia.org/wiki?curid=55347" title="Hierarchical File System">
Hierarchical File System

Hierarchical File System (HFS) is a proprietary file system developed by Apple Inc. for use in computer systems running Mac OS. Originally designed for use on floppy and hard disks, it can also be found on read-only media such as CD-ROMs. HFS is also referred to as Mac OS Standard (or, erroneously, "HFS Standard"), while its successor, HFS Plus, is also called "Mac OS Extended" (or, erroneously, "HFS Extended"). With the introduction of OS X 10.6, Apple dropped support to format or write HFS disks and images, which are only supported as read-only volumes.
History.
HFS was introduced by Apple in September 1985, specifically to support Apple's first hard disk drive for the Macintosh, replacing the Macintosh File System (MFS), the original file system which had been introduced over a year and a half earlier with the first Macintosh computer. HFS drew heavily upon Apple's first hierarchical SOS operating system for the failed Apple III, which also served as the basis for hierarchical filing systems on the Apple IIe and Apple Lisa. HFS was developed by Patrick Dirks and Bill Bruffey. It shared a number of design features with MFS that were not available in other file systems of the time (such as DOS's FAT). Files could have multiple forks (normally a data and a resource fork), which allowed program code to be stored separately from resources such as icons that might need to be localized. Files were referenced with unique file IDs rather than file names, and file names could be 255 characters long (although the Finder only supported a maximum of 31 characters).
However, MFS had been optimized to be used on very small and slow media, namely floppy disks, so HFS was introduced to overcome some of the performance problems that arrived with the introduction of larger media, notably hard drives. The main concern was the time needed to display the contents of a folder. Under MFS all of the file and directory listing information was stored in a single file, which the system had to search to build a list of the files stored in a particular folder. This worked well with a system with a few hundred kilobytes of storage and perhaps a hundred files, but as the systems grew into megabytes and thousands of files, the performance degraded rapidly.
The solution was to replace MFS's directory structure with one more suitable to larger file systems. HFS replaced the flat table structure with the "Catalog File" which uses a B-tree structure that could be searched very quickly regardless of size. HFS also re-designed various structures to be able to hold larger numbers, 16-bit integers being replaced by 32-bit almost universally. Oddly, one of the few places this "upsizing" did not take place was the file directory itself, which limits HFS to a total of 65,535 files on each logical disk.
While HFS is a proprietary file system-format, it is well-documented, so there are usually solutions available to access HFS formatted disks from most modern operating systems.
Apple introduced HFS out of necessity with its first 20 MB hard disk offering for the Macintosh in September 1985. However, HFS was not widely introduced until System 3.0, which debuted with the Macintosh Plus in January 1986, along with the larger 800 KB floppy disk drive for the Macintosh, which also required HFS support. More importantly, HFS was hard-coded into new Plus' 128K ROM, freeing not only space from the system software disk, but also RAM. However, RAM-based HFS support was also implemented for use with the earlier Macintosh 512K's 64K ROM through the addition of an INIT file on the system disk. The introduction of HFS was the first advancement by Apple to leave a Macintosh computer model behind: the original 128K Macintosh, which lacked sufficient memory to load the HFS code and was promptly discontinued.
In 1998, Apple introduced HFS Plus to address inefficient allocation of disk space in HFS and to add other improvements. HFS is still supported by current versions of Mac OS, but starting with OS X, an HFS volume cannot be used for booting, and beginning with OS X 10.6 (Snow Leopard), HFS volumes are read-only and cannot be created or updated.
Design.
A storage volume is inherently divided into "logical blocks" of 512 bytes. The Hierarchical File System groups these logical blocks into "allocation blocks", which can contain one or more logical blocks, depending on the total size of the volume. HFS uses a 16-bit value to address allocation blocks, limiting the number of allocation blocks to 65,535 (216-1).
Five structures make up an HFS volume:
Limitations.
The Catalog File, which stores all the file and directory records in a single data structure, results in performance problems when the system allows multitasking, as only one program can write to this structure at a time, meaning that many programs may be waiting in queue due to one program "hogging" the system. It is also a serious reliability concern, as damage to this file can destroy the entire file system. This contrasts with other file systems that store file and directory records in separate structures (such as DOS's FAT file system or the Unix File System), where having structure distributed across the disk means that damaging a single directory is generally non-fatal and the data may possibly be re-constructed with data held in the non-damaged portions.
Additionally, the limit of 65,535 allocation blocks resulted in files having a "minimum" size equivalent 1/65,535th the size of the disk. Thus, any given volume, no matter its size, could only store a maximum of 65,535 files. Moreover, any file would be allocated more space than it actually needed, up to the allocation block size. When disks were small, this was of little consequence, because the individual allocation block size was trivial, but as disks started to approach the 1 GB mark, the smallest amount of space that any file could occupy (a single allocation block) became excessively large, wasting significant amounts of disk space. For example, on a 1 GB disk, the allocation block size under HFS is 16 KB, so even a 1 byte file would take up 16 KB of disk space. This situation was less of a problem for users having large files (such as pictures, databases or audio) because these larger files wasted less space as a percentage of their file size. Users with many small files, on the other hand, could lose a copious amount of space due to large allocation block size. This made partitioning disks into smaller logical volumes very appealing for Mac users, because small documents stored on a smaller volume would take up much less space than if they resided on a large partition. The same problem existed in the FAT16 file system.
HFS saves the case of a file that is created or renamed but is case-insensitive in operation.

</doc>
<doc id="55348" url="http://en.wikipedia.org/wiki?curid=55348" title="Elevator music">
Elevator music

Elevator music (also known as Muzak, piped music, weather music, or lift music) refers to a type of popular music, often instrumental, that is commonly played through speakers at shopping malls, grocery stores, department stores, telephone systems (while the caller is on hold), cruise ships, airliners (during take-off and flight), hotels, airports, business offices, hospitals, and elevators, as well as electronic program guides, weather forecasts and test cards. The term is also frequently applied as a generic term for any form of easy listening, smooth jazz, or middle of the road music, or to the type of recordings commonly heard on "beautiful music" radio stations.
Elevator music is typically set to a very simple melody so that it can be unobtrusively looped back to the beginning. The dynamic range is also normally reduced, so that the highs and lows do not distract listeners. In a mall or shopping center, elevator music of a specific type has been found to have a psychological effect: slower, more relaxed music tends to make people slow down and browse longer. Elevator music may also be preferred over broadcast radio stations due to the lack of lyrics and commercial interruptions.
This style of music is sometimes used to comedic effect in mass media such as film, where intense or dramatic scenes may be interrupted or interspersed with such anodyne music while characters use an elevator (e.g. "The Spongebob Squarepants Movie", "The Blues Brothers", "Dawn of the Dead", "Mr. and Mrs. Smith", ', ', "Spider-Man 2", and "Night at the museum"). Some video games have used elevator music for comedic effect, e.g. ' where a few elevator music-themed tracks are accessible on the in-game iPod, as well as ', which occasionally plays elevator music when in elevators during stages.
Muzak was a major supplier of business background music, and was the best known such supplier for years. Ironically, while its name is commonly associated with elevator music in the public mind, that was never one of the company's offerings. Since 1997, Muzak has used original artists for its music source, except on the Environmental channel.

</doc>
<doc id="55349" url="http://en.wikipedia.org/wiki?curid=55349" title="RAM drive">
RAM drive

A RAM drive (also called a RAM disk) is a block of random-access memory (primary storage or volatile memory) that a computer's software is treating as if the memory were a disk drive (secondary storage). It is sometimes referred to as a "virtual RAM drive" or "software RAM drive" to distinguish it from a "hardware RAM drive" that uses separate hardware containing RAM, which is a type of battery backed solid-state drive.
Performance.
The performance of a RAM drive is in general orders of magnitude faster than other forms of storage media, such as an SSD, hard drive, tape drive, or optical drive. This performance gain is due to multiple factors, including access time, maximum throughput and type of file system, as well as others.
File access time is greatly reduced since a RAM drive is solid state (no mechanical parts). A physical hard drive or optical media, such as CD-ROM, DVD, and Blu-ray must move a head or optical eye into position and tape drives must wind or rewind to a particular position on the media before reading or writing can occur. RAM drives can access data with only the memory address of a given file, with no movement, alignment or positioning necessary.
Second, the maximum throughput of a RAM drive is limited by the speed of the RAM, the data bus, and the CPU of the computer. Other forms of storage media are further limited by the speed of the storage bus, such as IDE (PATA), SATA, USB or Firewire. Compounding this limitation is the speed of the actual mechanics of the drive motors, heads and/or eyes.
Third, the file system in use, such as NTFS, HFS, UFS, ext2, etc., uses extra accesses, reads and writes to the drive, which although small, can add up quickly, especially in the event of many small files vs. few larger files (temporary internet folders, web caches, etc.).
Because the storage is in RAM, it is volatile memory, which means it will be lost in the event of power loss, whether intentional (computer reboot or shutdown) or accidental (power failure or system crash). This is, in general, a weakness (the data must periodically be backed up to a persistent-storage medium to avoid loss), but is sometimes desirable: for example, when working with a decrypted copy of an encrypted file.
In many cases, the data stored on the RAM drive is created, for faster access, from data permanently stored elsewhere, and is re-created on the RAM drive when the system reboots.
Apart from the risk of data loss, the major limitation of RAM drives is their limited capacity, which is constrained by the amount of RAM within the machine. Multi-terabyte-capacity persistent storage has become commoditized as of 2012, whereas RAM is still measured in gigabytes.
RAM drives use the normal RAM in main memory as if it were a partition on a hard drive rather than actually accessing the data bus normally used for secondary storage. Though RAM drives can often be supported directly from the operating system via special mechanisms in the operating system kernel, it is possible to also create and manage a RAM drive by an application. Usually no battery backup is needed due to the temporary nature of the information stored in the RAM drive, but an uninterrupted power supply can keep the entire system running during a power outage, if necessary.
Some RAM drives use a compressed file system such as cramfs to allow compressed data to be accessed on the fly, without unprepossessing it first. This is convenient because RAM drives are often small due to the higher price per byte than conventional hard drive storage.
History and operating system specifics.
The first software RAM drive for microcomputers was invented and written by Jerry Karlin in the UK in 1979/80. The software, known as the Silicon Disk System was further developed into a commercial product and marketed by JK Systems Research which became Microcosm Research Ltd when the company was joined by Peter Cheesewright of Microcosm Ltd. The idea was to enable the early microcomputers to use more RAM than the CPU could directly address. Making bank-switched RAM behave like a disk drive was much faster than the disk drives - especially in those days before hard drives were readily available on such machines.
The Silicon Disk was launched in 1980, initially for the CP/M operating system and later for MS-DOS. Due to the limitations in memory addressing on Apple II series and Commodore computers, a RAM drive was also a popular application on Commodore 64 and Commodore 128 systems with RAM Expansion Units and on Apple II series computers with more than 64kB of RAM. Apple Computer supported a software RAM drive natively in ProDOS: on systems with 128kB or more of RAM, ProDOS would automatically allocate a RAM drive named /RAM.
IBM added a RAM drive named VDISK.SYS to PC DOS (version 3.0) in August 1984, which was the first DOS component to use extended memory. VDISK.SYS was not available in Microsoft's MS-DOS as it, unlike most components of early versions of PC DOS, was written by IBM. Microsoft included the similar program RAMDRIVE.SYS in MS-DOS 3.2 (released in 1986), which could also use expanded memory. It was discontinued in Windows 7. DR-DOS and the DR family of multi-user operating systems also came with a RAM disk named VDISK.SYS. In Multiuser DOS, the RAM disk defaults to the drive letter M: (for memory drive). AmigaOS has had a built in RAM drive since the release of version 1.1 in 1985 and still has it in AmigaOS 4.1 (2010). Apple Computer added the functionality to the Apple Macintosh with System 7's Memory control panel in 1991, and kept the feature through the life of Mac OS 9. Mac OS X users can use the hdid, newfs (or newfs hfs) and mount utilities to create, format and mount a RAM drive.
A RAM drive innovation introduced in 1986 but made generally available in 1987 by Perry Kivolowitz for AmigaOS was the ability of the RAM drive to survive most crashes and reboots. Called the ASDG Recoverable Ram Disk, the device survived reboots by allocating memory dynamically in the reverse order of default memory allocation (a feature supported by the underlying OS) so as to reduce memory fragmentation. A "super-block" was written with a unique signature which could be located in memory upon reboot. The super-block, and all other RRD disk "blocks" maintained check sums to enable the invalidation of the disk if corruption was detected. At first, the ASDG RRD was locked to ASDG memory boards and used as a selling feature. Later, the ASDG RRD was made available as shareware carrying a suggested donation of 10 dollars. The shareware version appeared on Fred Fish Disks 58 and 241. AmigaOS itself would gain a Recoverable Ram Disk (called "RAD") in version 1.3.
Many Unix and Unix-like systems provide some form of RAM drive functionality, such as /dev/ram on Linux. RAM drives are particularly useful in high-performance, low-resource applications for which Unix-like operating systems are sometimes configured. There are also a few specialized "ultra-lightweight" Linux distributions which are designed to boot from removable media and stored in a ramdisk for the entire session.

</doc>
<doc id="55351" url="http://en.wikipedia.org/wiki?curid=55351" title="Jeet Kune Do">
Jeet Kune Do

Jeet Kune Do, abbreviated JKD, is an eclectic and hybrid style fighting art heavily influenced by the philosophy of martial artist Bruce Lee, who founded the system in 1967, referred it as "non-classical", suggesting that JKD is a form of Chinese Kung Fu, yet without form.
Unlike more traditional martial arts, Jeet Kune Do is not fixed or patterned, and is a philosophy with guiding thoughts. It was named for the Wing Chun concept of interception or attacking while one's opponent is about to attack.
Jeet Kune Do practitioners believe in minimal movements with maximum effects and extreme speed. 
The system works by using different "tools" for different situations, where the situations are divided into ranges, which is kicking, punching, trapping, and grappling, where martial artists use techniques to flow smoothly between them. 
In the screenplay of the 1973 Warner Brothers film, Enter the Dragon, when Lee is asked; "What's your style?" Lee replied, "My style?...You can call it the art of fighting without fighting." Thus one may also ideologically define JKD simply as the Bruce Lee Kung Fu or the Bruce Lee style of Kung Fu.
The name Jeet Kune Do was often said by Lee to be just a name, and he often referred to it as "the art of expressing the human body" in his writings and in interviews. Through his studies Lee came to believe that styles had become too rigid and unrealistic. He called martial art competitions of the day "dry land swimming". He believed real combat was spontaneous, and a martial artist can not predict it, but, only react to it, and a good martial artist should "be like water" - move fluidly without hesitation.
In 2004, the Bruce Lee Foundation decided to use the name Jun Fan Jeet Kune Do (振藩截拳道) to refer to the martial arts system which Lee founded; "Jun Fan" being Lee's Chinese given name.
System and philosophy.
Lee's philosophy.
Originally, when Lee began researching various fighting styles, he called it Jun Fan Gung Fu. However not wanting to create another style which would share the limitations that all styles had, he instead described the process which he used to create it:
 I have not invented a "new style," composite, modified or otherwise that is set within distinct form as apart from "this" method or "that" method. On the contrary, I hope to free my followers from clinging to styles, patterns, or molds. Remember that Jeet Kune Do is merely a name used, a mirror in which to see "ourselves". . . Jeet Kune Do is not an organized institution that one can be a member of. Either you understand or you don't, and that is that.
There is no mystery about my style. My movements are simple, direct and non-classical. The extraordinary part of it lies in its simplicity. Every movement in Jeet Kune Do is being so of itself. There is nothing artificial about it. I always believe that the easy way is the right way. Jeet Kune Do is simply the direct expression of one's feelings with the minimum of movements and energy. The closer to the true way of Kung Fu, the less wastage of expression there is.
Finally, a Jeet Kune Do man who says Jeet Kune Do is exclusively Jeet Kune Do is simply not with it. He is still hung up on his self-closing resistance, in this case anchored down to reactionary pattern, and naturally is still bound by another modified pattern and can move within its limits. He has not digested the simple fact that truth exists outside all molds; pattern and awareness is never exclusive.
Again let me remind you Jeet Kune Do is just a name used, a boat to get one across, and once across it is to be discarded and not to be carried on one's back.
 — Bruce Lee
JKD as it survives since then — if one views it "refined" as a product, not a process — is what was left at the time of Lee's death. It is the result of the lifelong martial arts development process Lee went through. Lee stated his concept does not add more and more things on top of each other to form a system, but rather selects the best thereof. The metaphor Lee borrowed from Chan Buddhism was of constantly filling a cup with water, and then emptying it, used for describing Lee's philosophy of "casting off what is useless". He used the sculptor's mentality of beginning with a lump of clay and removing the material which constituted the "unessentials"; the end result was what he considered to be the bare combat essentials, or JKD.
The dominant or strongest hand should be in the lead because it would perform a greater percentage of the work. Lee minimized the use of other stances except when circumstances warranted such actions. 
Although the On-Guard position is a formidable overall stance, it is by no means the only one. He acknowledged there were times when other positions should be utilised.
Lee felt the dynamic property of JKD was what enabled its practitioners to adapt to the constant changes and fluctuations of live combat. He believed these decisions should be made within the context of "real combat" and/or "all out sparring" and that it was only in this environment that a practitioner could actually deem a technique worthy of adoption.
Lee believed that real combat was alive and dynamic. Circumstances in a fight change from millisecond to millisecond. Thus, pre-arranged patterns and techniques are not adequate in dealing with such a changing situation. As an antidote to this line of thought, Lee once wrote an epitaph which read: 'In memory of a once fluid man, crammed and distorted by the classical mess.' The "classical mess" in this instance was what Lee thought of the "not too alive way of the classical kung fu styles".
Principles.
The following are principles that Lee incorporated into Jeet Kune Do. Lee felt these were universal combat truths that were self-evident, and would lead to combat success if followed. Familiarity with each of the "Four ranges of combat", in particular, is thought to be instrumental in becoming a "total" martial artist.
JKD teaches that the best defense is a strong offense, hence the principle of an "intercepting fist". For someone to attack another hand-to-hand, the attacker must approach the target. This provides an opportunity for the attacked person to "intercept" the attacking movement. The principle of interception may be applied to more than intercepting physical attacks; non-verbal cues (subtle movements that an opponent may be unaware of) may also be perceived or "intercepted", and thus be used to one's advantage.
The "Five ways of attack", categories which help JKD practitioners organize their fighting repertoire, comprise the offensive teachings of JKD. The concepts of "Stop hits & stop kicks", and "Simultaneous parrying & punching", based on the concept of single fluid motions which attack while defending (in systems such as Épée fencing and Wing Chun), compose the defensive teachings of JKD. These concepts were modified for unarmed combat and implemented into the JKD framework by Lee to complement the principle of interception.
Straight lead.
Lee felt that the straight lead was the most integral part of Jeet Kune Do punching, as he stated, "The leading straight punch is the backbone of all punching in Jeet Kune Do." The straight lead is not a power strike but a strike formulated for speed. The straight lead should always be held loosely with a slight motion, as this adds to its speed and makes it more difficult to see and block. The strike is not only the fastest punch in JKD, but also the most accurate. The speed is attributed to the fact that the fist is held out slightly making it closer to the target and its accuracy is gained from the punch being thrown straight forward from one's centerline. The straight lead should be held and thrown loosely and easily, tightening only upon impact, adding to one's punch. The straight lead punch can be thrown from multiple angles and levels.
Non-telegraphed punch.
Lee felt explosive attacks with no telegraphing signs of intention were best. He argued that the attacks should catch the opponent off-guard, throwing them off their balance and leaving them unable to defend against further attacks. "The concept behind this is that when you initiate your punch without any forewarning, such as tensing your shoulders or moving your foot or body, the opponent will not have enough time to react," Lee wrote. The key is that one must keep one's body and arms loose, weaving one's arms slightly and only becoming tense upon impact. Lee wanted no wind-up movements or "get ready poses" to prelude any JKD attacks. Lee explained that any twitches or slight movements before striking should be avoided as they will give the opponent signs or hints as to what is being planned and then they will be able to strike first while one is preparing an attack. Consequently, non-telegraphed movement is an essential part of Jeet Kune Do philosophy.
"Be Like Water".
Lee emphasized that every situation, in fighting or in everyday life, is varied. To obtain victory, therefore, it is essential not to be rigid, but to be fluid and able to adapt to any situation. He compared it to being like water: "Empty your mind, be formless, shapeless, like water. If you put water into a cup, it becomes the cup. You put water into a bottle and it becomes the bottle. You put it in a teapot it becomes the teapot. That water can flow, or it can crash. Be water, my friend." Lee’s theory behind this was that one must be able to function in any scenario one is thrown into and should react accordingly. One should know when to speed up or slow down, when to expand and when to contract, and when to remain flowing and when to crash. It is the awareness that both life and fighting can be shapeless and ever changing that allows one to be able to adapt to those changes instantaneously and bring forth the appropriate solution. Lee did not believe in "styles" and felt that every person and every situation is different and not everyone fits into a mold; one must remain flexible in order to obtain new knowledge and victory in both life and combat. One must never become stagnant in the mind or method, always evolving and moving towards improving oneself.
Economy of motion.
Jeet Kune Do seeks to waste no time or movement, teaching that the simplest things work best, as in Wing Chun. Economy of motion is the principle by which JKD practitioners achieve:
This is meant to help a practitioner conserve both energy and time, two crucial components in a physical confrontation. Maximized force seeks to end the battle quickly due to the amount of damage inflicted upon the opponent. Rapidity aims to reach the target before the opponent can react, which is half-beat faster timing, as taught in Wing Chun and Western boxing. Learned techniques are utilized in JKD to apply these principles to a variety of situations.
Stop hits.
"When the distance is wide, the attacking opponent requires some sort of preparation. Therefore, attack him on his preparation of attack." "To reach me, you must move to me. Your attack offers me an opportunity to intercept you." This means intercepting an opponent's attack with an attack of one's own instead of simply blocking it. It is for this concept Jeet Kune Do is named. JKD practitioners believe that this is the most difficult defensive skill to develop. This strategy is a feature of some traditional Chinese martial arts as Wing Chun, as well as an essential component of European Épée Fencing. Stop hits and kicks utilize the principle of economy of motion by combining attack and defense into one movement, thus minimizing the "time" element.
Simultaneous parrying and punching.
When confronting an incoming attack, the attack is parried or deflected, and a counterattack is delivered at the same time. This is not as advanced as a stop hit but more effective than blocking and counterattacking in sequence. This is practiced by some Chinese martial arts such as Wing Chun, and it is also known in Krav Maga as "bursting". Simultaneous parrying & punching utilizes the principle of economy of motion by combining attack and defense into one movement, thus minimizing the "time" element and maximising the "energy" element. Efficiency is gained by utilizing a parry rather than a block. By definition a "block" stops an attack, whereas a parry merely re-directs it. Redirection has two advantages, first that it requires less energy to execute and second that it utilizes the opponent's energy against them by creating an imbalance. Efficiency is gained in that the opponent has less time to react to an incoming attack, since they are still nullifying the original attack.
Low kicks.
JKD practitioners believe they should direct their kicks to their opponent's shins, knees, thighs, and midsection, as in Wing Chun. These targets are the closest to the foot, provide more stability and are more difficult to defend against. Maintaining low kicks utilizes the principle of economy of motion by reducing the distance a kick must travel, thus minimizing the "time" element. However, as with all other JKD principles nothing is "written in stone". If a target of opportunity presents itself, even a target above the waist, one could take advantage and not be hampered by this principle.
Four ranges of combat.
Jeet Kune Do students train in each of the aforementioned ranges equally. According to Lee, this range of training serves to differentiate JKD from other martial arts. Lee stated that most but not all traditional martial arts systems specialize in training at one or two ranges. Lee's theories have been especially influential and substantiated in the field of mixed martial arts, as the MMA Phases of Combat are essentially the same concept as the JKD combat ranges. As a historic note, the ranges in JKD have evolved over time. Initially the ranges were categorized as short or close, medium, and long range. These terms proved ambiguous and eventually evolved into their more descriptive forms, although some may still prefer the original three categories.
Five ways of attack.
The original five ways of attack are:
Centerline.
 The centerline is an imaginary line drawn vertically along the center of a standing human body, and refers to the space directly in front of that body. If one draws an isosceles triangle on the floor, for which one's body forms the base, and one's arms form the equal legs of the triangle, then "h" (the height of the triangle) is the centerline. The Wing Chun concept is to exploit, control and dominate an opponent's centerline. All attacks, defenses, and footwork are designed to guard one's own centerline while entering the opponent's centerline space. Lee incorporated this theory into JKD from his Sifu Yip Man's Wing Chun.
The three guidelines for centerline are:
This notion is closely related to maintaining control of the center squares in the strategic game chess. The concept is naturally present in xiangqi (Chinese chess), where an "X" is drawn on the game board, in front of both players' general and advisors.
Combat realism.
One of the premises that Lee incorporated in Jeet Kune Do was "combat realism". He insisted that martial arts techniques should be incorporated based upon their effectiveness in real combat situations. This would differentiate JKD from other systems where there was an emphasis on "flowery technique", as Lee would put it. Lee claimed that flashy "flowery techniques" would arguably "look good" but were often not practical or would prove ineffective in street survival and self-defense situations. This premise would differentiate JKD from other "sport"-oriented martial arts systems that were geared towards "tournament" or "point systems". Lee felt that these systems were "artificial" and fooled their practitioners into a false sense of true martial skill. Lee felt that because these systems favored a "sports" approach they incorporated too many rule sets that would ultimately handicap a practitioner in self-defense situations. He felt that this approach to martial arts became a "game of tag" which would lead to bad habits such as pulling punches and other attacks; this would again lead to disastrous consequences in real world situations.
Another aspect of realistic martial arts training fundamental to JKD is what Lee referred to as "aliveness". This is the concept of training techniques with an unwilling assistant who offers resistance. Lee made a reference to this concept in his famous quote "Boards don't hit back!" Because of this perspective of realism and aliveness, Lee utilized safety gear from various other contact sports to allow him to spar with opponents "full out". This approach to training allowed practitioners to come as close as possible to real combat situations with a high degree of safety.
Branches.
Although Lee officially closed his martial arts schools two years before his death, he allowed his curriculum to be taught privately. Since his death, Jeet Kune Do is argued to have split into different groups. Allegedly they are:
As far as is known, Lee himself authorized only one person to teach Jeet Kune Do: Daniel Inosanto, who achieved 3rd rank in JKD. Ted Wong, Lee's last student, achieved 2nd rank in Jeet Kune Do while training privately with Lee. After this, Lee did away with rankings in his teachings, so Ted was never "ranked" beyond the 2nd. Dan Inosanto, however, fully certified Ted Wong to teach after Lee's death.
Two other people were certified to teach by Lee as well. Taky Kimura and James Yimm Lee were certified to teach Jun Fan Gung Fu (the precursor to JKD), but not Jeet Kune Do itself.
There are essentially two "types" or viewpoints of Jeet Kune Do:
Lee believed that this freedom of adoption was a distinguishing property from traditional martial arts.
There are many who confuse the JKD Framework with a JKD Personal System (i.e. Lee's personal JKD) thinking them to be one and the same. The system that Lee personally expressed was his own personal JKD; that is, tailored for himself. Before he could do this, however, he needed to first develop the "JKD Framework" process. Many of the systems that Lee studied were not to develop his "Personal JKD" but rather to gather the "principles" for incorporation in the JKD Framework approach. The uniqueness of JKD to Lee is that it was a "process", not a "product", and thus not a "style" but a system, concept, or approach. Traditional martial arts styles are essentially a product that is given to a student with little provision for change. These traditional styles are usually fixed and not tailored for individuals. Lee said there were inherent problems with this approach and established a "process" based system rather than a fixed style which a student could then utilize to make a "tailored" or "personal" product of their own. To use an analogy: traditional martial arts give students fish to eat (a product), but Lee believed that a martial art should just teach the student to fish (a process) and gain the food directly.
The two branches of JKD differ in what should be incorporated or offered within the "JKD Framework". The Original (or Jun Fan) JKD branch believes that the original principles before Lee died are all that is needed for the construction of personalized systems. The JKD Concepts branch believe that there are further principles that can be added to construct personalized systems. The value of each Branch can be determined by individual practitioners based on whatever merits they deem important.
Original JKD is further divided into two points of view, which both hold Wing Chun, Western boxing, and Fencing as the cornerstones of Lee's JKD.

</doc>
<doc id="55356" url="http://en.wikipedia.org/wiki?curid=55356" title="Asian">
Asian

Asian refers to anything related to the continent of Asia, especially Asian people. 
Asian may also refer to:

</doc>
<doc id="55357" url="http://en.wikipedia.org/wiki?curid=55357" title="Dragon (magazine)">
Dragon (magazine)

Dragon is one of the two official magazines for source material for the "Dungeons & Dragons" role-playing game and associated products; "Dungeon" is the other. TSR, Inc. originally launched the monthly printed magazine in 1976 to succeed the company's earlier publication, "The Strategic Review". The final printed issue was #359 in September 2007. Shortly after the last print issue shipped in mid-August 2007, Wizards of the Coast (part of Hasbro, Inc.), the publication's current intellectual property rightsholder, relaunched "Dragon" as an online magazine, continuing on the numbering of the print edition. The last published issue was No. 430 in December 2013. 
History.
In 1975, TSR, Inc. began publishing "The Strategic Review". At the time, roleplaying games were still seen as a subgenre of the wargaming industry, and the magazine was designed not only to support "Dungeons & Dragons" and TSR's other games, but also to cover wargaming in general. In short order, however, the popularity and growth of "Dungeons & Dragons" made it clear that the game had not only separated itself from its wargaming origins, but had launched an entirely new industry unto itself.
TSR canceled "The Strategic Review" after only seven issues the following year, and replaced it with two magazines, "Little Wars", which covered miniature wargaming, and "The Dragon", which covered role playing games. After twelve issues, "Little Wars" ceased independent publication and issue 13 was published as part of "The Dragon" issue 22. "The Dragon" later changed its name to "Dragon Magazine" and finally simply "Dragon".
"The Dragon" debuted in June 1976. TSR co-founder Gary Gygax commented years later: "When I decided that "The Strategic Review" was not the right vehicle, hired Tim Kask as a magazine editor for Tactical Studies Rules, and named the new publication he was to produce "The Dragon", I thought we would eventually have a great periodical to serve gaming enthusiasts worldwide... At no time did I ever contemplate so great a success or so long a lifespan."
"Dragon" was the launching point for a number of rules, spells, monsters, magic items, and other ideas that were incorporated into later official products of the "Dungeons & Dragons" game. A prime example is the Forgotten Realms campaign setting, which first became known through a series of "Dragon" articles in the 1980s by its creator Ed Greenwood. It subsequently went on to become one of the primary campaign 'worlds' for official "Dungeons and Dragons" products, starting in 1987.
Wizards of the Coast purchased TSR and its intellectual properties, including "Dragon" in 1997. Production was then transferred from Wisconsin to Washington state. In 1999, Wizards of the Coast was itself purchased by Hasbro, Inc.
In 1999 a compilation of the first 250 issues was released in PDF format with a special viewer including an article and keyword search in CD-ROM format. Also included were the 7 issues of "The Strategic Review". This compilation is known as the software title "Dragon Magazine Archive". Because of a conflict regarding the reprint rights for the "Knights of the Dinner Table" comic strips printed in "Dragon" for many years, the "Dragon Magazine Archive" is out of print and very hard to find.
In 2002, Paizo Publishing acquired the rights to publish both "Dragon" and "Dungeon" under license from Wizards of the Coast. It tied "Dragon" more closely to "Dungeon" by including articles supporting and promoting its major multi-issue adventures such as the "Age of Worms" and "Savage Tide". "Class Acts", monthly one or two-page articles offering ideas for developing specific character classes, were also introduced by Paizo.
On April 18, 2007, Wizards of the Coast announced that it would not be renewing Paizo's licenses for "Dragon" and "Dungeon". Scott Rouse, Senior Brand Manager of "Dungeons & Dragons" at Wizards of the Coast stated, "Today the internet is where people go to get this kind of information. By moving to an online model we are using a delivery system that broadens our reach to fans around the world." Paizo published the last print editions of "Dragon" and "Dungeon" magazines for September 2007.
In August 2007, Wizards of the Coast announced plans for the 4th edition of the "Dungeons & Dragons" game. Part of this announcement was that D&D Insider subscriber content would include the new, online versions of both "Dungeon" and "Dragon" magazines along with tools for building campaigns, managing character sheets and other features. In its online form, "Dragon" continues to publish articles aimed at "Dungeons & Dragons" players, with rules data from these articles feeding the D&D Character Builder and other online tools.
In the September 2013 issue of "Dragon" (#427) an article by Wizards of the Coast game designer and editor Chris Perkins announced that both "Dragon" and its sibling publication "Dungeon" would be going on hiatus starting January 2014 pending the release of "Dungeons & Dragons" 5th edition.
Content.
Although "Dragon" provided coverage of fantasy and roleplaying games in general, it became primarily a house organ for role-playing games produced by TSR (or more recently Wizards of the Coast), with a particular focus on "D&D". Its coverage of games created by other companies is often peripheral.
Most of the magazine's articles provided supplementary material for "D&D" including new prestige classes, races, monsters and many other subjects that could be used to enhance a "Dungeons & Dragons" game. A popular long-running column "Sage Advice" offered official answers to "Dungeons & Dragons" questions submitted by players. Other articles provided tips and suggestions for both players and Dungeon Masters (DMs). It sometimes discussed "meta-gaming" issues, such as getting along with fellow players. At the end of its print run, the magazine also featured four comics; Nodwick, Dork Tower, Zogonia and a specialized version of the webcomic "The Order of the Stick". Previous popular gamer-oriented comic strips include Knights of the Dinner Table, "Fineous Fingers", "What's New with Phil & Dixie", "Wormy", "Yamara" and "SnarfQuest".
Many of the gaming world's most famous writers, game designers and artists have published work in the magazine. Through most of its run the magazine frequently published fantasy fiction, either short stories or novel excerpts. After the 1990s, the appearance of fiction stories became relatively rare. One late example was issue #305's featured excerpt from George R.R. Martin's later Hugo-nominated novel "A Feast for Crows". It also featured book reviews of fantasy and science fiction novels, and occasionally of films of particular interest (such as the TV movie of "Mazes and Monsters).
During various Dungeons & Dragons controversies, "Dragon" featured occasional articles from TSR spokespeople discussing issues from their point of view.
A regular feature of "Dragon" for many years was its "Ecology of ..." articles as sometimes discussed by the fictional sage Elminster, in which a particular D&D monster received an in-depth review, explaining how it found food, reproduced, and so forth. Under Paizo's tenure such ecology articles became heavier in "crunch" (game mechanics) as opposed to "fluff" (narrative and description) than previously. The "Dragon" submissions guidelines explicitly stated that Ecology articles "should have a hunter’s guidebook approach, although it should not be written 'in voice'" and further call out the exact format of Ecology articles, leaving less room for artistic license by the author.
In the early 1980s, almost every issue of "Dragon" would contain a role playing adventure, a simple board game, or some kind of special game supplement (such as a cardboard cut-out castle). For instance, Tom Wham's "Snit's Revenge", "The Awful Green Things From Outer Space" and "File 13" all started as supplements within "The Dragon". These bonus features become infrequent after the 1986 launch of "Dungeon" magazine, which published several new "Dungeons & Dragons" adventures in each issue.
During the 1980s, after TSR had purchased Simulations Publications Inc., the magazine had a subsection called "Ares Magazine", based on SPI's magazine of that name, specializing in science fiction and superhero role playing games, with pages marked by a gray border. The content included write-ups for various characters of the Marvel Universe for TSR's "Marvel Super-Heroes".
Special issues.
As noted above "The Dragon" was preceded by seven issues of "The Strategic Review". In the magazine's early years it also published five "Best of" issues, reprinting highly regarded articles from "The Strategic Review" and "The Dragon".
From 1996 to 2001, "Dragon Magazine" published the "Dragon Annual," a thirteenth issue of all new content.
Other releases.
A collection of Dragon was released as the "Dragon Magazine Archive" in 1999. It was released as a CD-ROM for Windows 95/98 or WindowsNT with files in Adobe's PDF format. The "Dragon Magazine Archive" was directed by Rob Voce, and published by TSR/Wizards of the Coast. It was reviewed by the online version of "Pyramid" on November 25, 1999. The reviewer felt that the archive was "worth the price", but noted that it was not Macintosh compatible: "This product fails pretty badly in the Mac world. Because the actual archive is in Adobe's PDF format, the files can be read by anyone with a Macintosh and Adobe Acrobat. Unfortunately, the search utilities that make the archive accessible are not available to Mac users."

</doc>
<doc id="55359" url="http://en.wikipedia.org/wiki?curid=55359" title="SIMD">
SIMD

Single instruction, multiple data (SIMD), is a class of parallel computers in Flynn's taxonomy. It describes computers with multiple processing elements that perform the same operation on multiple data points simultaneously. Thus, such machines exploit data level parallelism, but not concurrency: there are simultaneous (parallel) computations, but only a single process (instruction) at a given moment. SIMD is particularly applicable to common tasks like adjusting the contrast in a digital image or adjusting the volume of digital audio. Most modern CPU designs include SIMD instructions in order to improve the performance of multimedia use.
History.
The first use of SIMD instructions was in vector supercomputers of the early 1970s such as the CDC Star-100 and the Texas Instruments ASC, which could operate on a "vector" of data with a single instruction. Vector processing was especially popularized by Cray in the 1970s and 1980s. Vector-processing architectures are now considered separate from SIMD machines, based on the fact that vector machines processed the vectors one word at a time through pipelined processors (though still based on a single instruction), whereas modern SIMD machines process all elements of the vector simultaneously.
The first era of modern SIMD machines was characterized by massively parallel processing-style supercomputers such as the Thinking Machines CM-1 and CM-2. These machines had many limited-functionality processors that would work in parallel. For example, each of 64,000 processors in a Thinking Machines CM-2 would execute the same instruction at the same time, allowing, for instance multiplications on 64,000 pairs of numbers at a time. Supercomputing moved away from the SIMD approach when inexpensive scalar MIMD approaches based on commodity processors such as the Intel i860 XP became more powerful, and interest in SIMD waned.
The current era of SIMD processors grew out of the desktop-computer market rather than the supercomputer market. As desktop processors became powerful enough to support real-time gaming and video processing, demand grew for this particular type of computing power, and microprocessor vendors turned to SIMD to meet the demand. Sun Microsystems introduced SIMD integer instructions in its "VIS" instruction set extensions in 1995, in its UltraSPARC I microprocessor. MIPS followed suit with their similar MDMX system.
The first widely-deployed desktop SIMD was with Intel's MMX extensions to the x86 architecture in 1996. This sparked the introduction of the much more powerful AltiVec system in the Motorola PowerPC's and IBM's POWER systems. Intel responded in 1999 by introducing the all-new SSE system. Since then, there have been several extensions to the SIMD instruction sets for both architectures.
All of these developments have been oriented toward support for real-time graphics, and are therefore oriented toward processing in two, three, or four dimensions, usually with vector lengths of between two and sixteen words, depending on data type and architecture. When new SIMD architectures need to be distinguished from older ones, the newer architectures are then considered "short-vector" architectures, as earlier SIMD and vector supercomputers had vector lengths from 64 to 64,000. A modern supercomputer is almost always a cluster of MIMD machines, each of which implements (short-vector) SIMD instructions. A modern desktop computer is often a multiprocessor MIMD machine where each processor can execute short-vector SIMD instructions.
Advantages.
An application that may take advantage of SIMD is one where the same value is being added to (or subtracted from) a large number of data points, a common operation in many multimedia applications. One example would be changing the brightness of an image. Each pixel of an image consists of three values for the brightness of the red (R), green (G) and blue (B) portions of the color. To change the brightness, the R, G and B values are read from memory, a value is added to (or subtracted from) them, and the resulting values are written back out to memory.
With a SIMD processor there are two improvements to this process. For one the data is understood to be in blocks, and a number of values can be loaded all at once. Instead of a series of instructions saying "retrieve this pixel, now retrieve the next pixel", a SIMD processor will have a single instruction that effectively says "retrieve n pixels" (where n is a number that varies from design to design). For a variety of reasons, this can take much less time than retrieving each pixel individually, as with traditional CPU design.
Another advantage is that SIMD systems typically include only those instructions that can be applied to all of the data in one operation. In other words, if the SIMD system works by loading up eight data points at once, the codice_1 operation being applied to the data will happen to all eight values at the same time. Although the same is true for any super-scalar processor design, the level of parallelism in a SIMD system is typically much higher.
Chronology.
Examples of SIMD supercomputers (not including vector processors):
Hardware.
Small-scale (64 or 128 bits) SIMD became popular on general-purpose CPUs in the early 1990s and continued through 1997 and later with Motion Video Instructions (MVI) for Alpha. SIMD instructions can be found, to one degree or another, on most CPUs, including the IBM's AltiVec and SPE for PowerPC, HP's PA-RISC Multimedia Acceleration eXtensions (MAX), Intel's MMX and iwMMXt, SSE, SSE2, SSE3 SSSE3 and SSE4.x, AMD's 3DNow!, ARC's ARC Video subsystem, SPARC's VIS and VIS2, Sun's MAJC, ARM's NEON technology, MIPS' MDMX (MaDMaX) and MIPS-3D. The IBM, Sony, Toshiba co-developed Cell Processor's SPU's instruction set is heavily SIMD based. NXP founded by Philips developed several SIMD processors named Xetal. The Xetal has 320 16bit processor elements especially designed for vision tasks.
Modern graphics processing units (GPUs) are often wide SIMD implementations, capable of branches, loads, and stores on 128 or 256 bits at a time.
Intel's AVX SIMD instructions now process 256 bits of data at once. Intel's Larrabee prototype microarchitecture includes more than two 512-bit SIMD registers on each of its cores (VPU: Wide Vector Processing Units), and this 512-bit SIMD capability is being continued in Intel's future Many Integrated Core Architecture (Intel MIC).
Software.
SIMD instructions are widely used to process 3D graphics, although modern graphics cards with embedded SIMD have largely taken over this task from the CPU. Some systems also include permute functions that re-pack elements inside vectors, making them particularly useful for data processing and compression. They are also used in cryptography. The trend of general-purpose computing on GPUs (GPGPU) may lead to wider use of SIMD in the future.
Adoption of SIMD systems in personal computer software was at first slow, due to a number of problems. One was that many of the early SIMD instruction sets tended to slow overall performance of the system due to the re-use of existing floating point registers. Other systems, like MMX and 3DNow!, offered support for data types that were not interesting to a wide audience and had expensive context switching instructions to switch between using the FPU and MMX registers. Compilers also often lacked support, requiring programmers to resort to assembly language coding.
SIMD on x86 had a slow start. The introduction of 3DNow! by AMD and SSE by Intel confused matters somewhat, but today the system seems to have settled down (after AMD adopted SSE) and newer compilers should result in more SIMD-enabled software. Intel and AMD now both provide optimized math libraries that use SIMD instructions, and open source alternatives like libSIMD, SIMDx86 and SLEEF have started to appear.
Apple Computer had somewhat more success, even though they entered the SIMD market later than the rest. AltiVec offered a rich system and can be programmed using increasingly sophisticated compilers from Motorola, IBM and GNU, therefore assembly language programming is rarely needed. Additionally, many of the systems that would benefit from SIMD were supplied by Apple itself, for example iTunes and QuickTime. However, in 2006, Apple computers moved to Intel x86 processors. Apple's APIs and development tools (XCode) were rewritten to use SSE2 and SSE3 instead of AltiVec. Apple was the dominant purchaser of PowerPC chips from IBM and Freescale Semiconductor and even though they abandoned the platform, further development of AltiVec is continued in several Power Architecture designs from Freescale and IBM.
"SIMD within a register", or SWAR, is a range of techniques and tricks used for performing SIMD in general-purpose registers on hardware that doesn't provide any direct support for SIMD instructions. This can be used to exploit parallelism in certain algorithms even on hardware that does not support SIMD directly.
Microsoft added SIMD to .NET in RyuJIT. Use of the libraries that implement SIMD on .NET are available in NuGet package 
SIMD On The Web.
In 2013 John McCutchan announced that he had created a performant interface to SIMD instruction sets for the Dart programming language, bringing the benefits of SIMD to web programs for the first time. The interface consists of two types:
Instances of these types are immutable and in optimized code are mapped directly to SIMD registers. Operations expressed in Dart typically are compiled into a single instruction with no overhead. This is similar to C and C++ intrinsics. Benchmarks for 4x4 matrix multiplication, 3D vertex transformation, and Mandelbrot set visualization show near 400% speedup compared to scalar code written in Dart.
John's work on Dart has been adopted by ECMAScript and that they are implementing John's specification for both V8 and SpiderMonkey.
Emscripten, Mozilla’s C/C++ to JavaScript compiler, with extensions can enable compilation of C++ programs that make use of SIMD intrinsics or gcc style vector code to SIMD API of JavaScript resulting in equivalent speedups compared to scalar code.
Commercial applications.
Though it has generally proven difficult to find sustainable commercial applications for SIMD-only processors, one that has had some measure of success is the GAPP, which was developed by Lockheed Martin and taken to the commercial sector by their spin-off Teranex. The GAPP's recent incarnations have become a powerful tool in real-time video processing applications like conversion between various video standards and frame rates (NTSC to/from PAL, NTSC to/from HDTV formats, etc.), deinterlacing, image noise reduction, adaptive video compression, and image enhancement.
A more ubiquitous application for SIMD is found in video games: nearly every modern video game console since 1998 has incorporated a SIMD processor somewhere in its architecture. The PlayStation 2 was unusual in that one of its vector-float units could function as an autonomous DSP executing its own instruction stream, or as a coprocessor driven by ordinary CPU instructions. 3D graphics applications tend to lend themselves well to SIMD processing as they rely heavily on operations with 4-dimensional vectors. Microsoft's Direct3D 9.0 now chooses at runtime processor-specific implementations of its own math operations, including the use of SIMD-capable instructions.
One of the recent processors to use vector processing is the Cell Processor developed by IBM in cooperation with Toshiba and Sony. It uses a number of SIMD processors (a NUMA architecture, each with independent local store and controlled by a general purpose CPU) and is geared towards the huge datasets required by 3D and video processing applications.
A recent advancement by Ziilabs was the production of an SIMD type processor which can be used on mobile devices, such as media players and mobile phones.
Larger scale commercial SIMD processors are available from ClearSpeed Technology, Ltd. and Stream Processors, Inc. ClearSpeed's CSX600 (2004) has 96 cores each with 2 double-precision floating point units while the CSX700 (2008) has 192. Stream Processors is headed by computer architect Bill Dally. Their Storm-1 processor (2007) contains 80 SIMD cores controlled by a MIPS CPU.

</doc>
<doc id="55360" url="http://en.wikipedia.org/wiki?curid=55360" title="AltiVec">
AltiVec

AltiVec is a floating point and integer SIMD instruction set designed and owned by Apple, IBM and Freescale Semiconductor, formerly the Semiconductor Products Sector of Motorola, (the AIM alliance), and implemented on versions of the PowerPC including Motorola's G4, IBM's G5 and POWER6 processors, and P.A. Semi's PWRficient PA6T. AltiVec is a trademark owned solely by Freescale, so the system is also referred to as Velocity Engine by Apple and VMX by IBM and P.A. Semi, although IBM has recently begun using AltiVec as well.
While AltiVec refers to an instruction set, the implementations in CPUs produced by IBM and Motorola are separate in terms of logic design. To date, no IBM core has included an AltiVec logic design licensed from Motorola or vice versa.
AltiVec is a standard part of the Power ISA v.2.03 specification. It was never formally a part of the PowerPC architecture until this specification although it used PowerPC instruction formats and syntax and occupied the opcode space expressly allocated for such 
purposes.
Comparison to x86-64 SSE.
Both AltiVec and SSE feature 128-bit vector registers that can represent sixteen 8-bit signed or unsigned chars, eight 16-bit signed or unsigned shorts, four 32-bit ints or four 32-bit floating point variables. Both provide cache-control instructions intended to minimize cache pollution when working on streams of data.
They also exhibit important differences. Unlike SSE2, AltiVec supports a special RGB "pixel" data type, but it does not operate on 64-bit double precision floats, and there is no way to move data directly between scalar and vector registers. In keeping with the "load/store" model of the PowerPC's RISC design, the vector registers, like the scalar registers, can only be loaded from and stored to memory. However, AltiVec provides a much more complete set of "horizontal" operations that work across all the elements of a vector; the allowable combinations of data type and operations are much more complete. Thirty-two 128-bit vector registers are provided, compared to eight for SSE and SSE2 (extended to 16 in x86-64), and most AltiVec instructions take three register operands compared to only two register/register or register/memory operands on IA-32.
AltiVec is also unique in its support for a flexible vector permute instruction, in which each byte of a resulting vector value can be taken from any byte of either of two other vectors, parametrized by yet another vector. This allows for sophisticated manipulations in a single instruction.
Recent versions of the GNU Compiler Collection (GCC), IBM VisualAge compiler and other compilers provide intrinsics to access AltiVec instructions directly from C and C++ programs. As of version 4, the GCC also includes auto-vectorization capabilities that attempt to intelligently create Altivec accelerated binaries without the need for the programmer to use intrinsics directly. The "vector" type keyword is introduced to permit the declaration of native vector types, e.g., "codice_1" declares a 128-bit vector variable named "foo" containing sixteen 8-bit unsigned chars. The full complement of arithmetic and binary operators is defined on vector types so that the normal C expression language can be used to manipulate vector variables. There are also overloaded intrinsic functions such as "codice_2" that emit the appropriate op code based on the type of the elements within the vector, and very strong type checking is enforced. In contrast, the Intel-defined data types for IA-32 SIMD registers declare only the size of the vector register (128 or 64 bits) and in the case of a 128-bit register, whether it contains integers or floating point values. The programmer must select the appropriate intrinsic for the data types in use, e.g., "codice_3" for adding two vectors containing eight 16-bit integers.
Development history.
AltiVec was developed between 1996 and 1998 by a collaborative project between Apple, IBM, and Motorola. 
Apple was the primary customer for AltiVec until Apple switched to Intel-made, x86-based CPUs in 2006. They used it to accelerate multimedia applications such as QuickTime, iTunes and key parts of Apple's Mac OS X including in the Quartz graphics compositor. Other companies such as Adobe used AltiVec to optimize their image-processing programs such as Adobe Photoshop. Motorola was the first to supply AltiVec enabled processors starting with their G4 line. AltiVec was also used in some embedded systems for high-performance digital signal processing.
IBM consistently left VMX out of their earlier POWER microprocessors, which were intended for server applications where it was not very useful. The POWER6 microprocessor, introduced in 2007, implements AltiVec. The implementation is similar to the one in 970 and Cell. The last desktop microprocessor from IBM, the PowerPC 970 (dubbed the "G5" by Apple) also implemented AltiVec with hardware similar to that of the PowerPC 7400.
AltiVec is the standard "Category:Vector" part of the Power ISA v.2.03 specification.
The Cell Broadband Engine, used in (amongst other things) the PlayStation 3, also supports AltiVec in its PPU, with the SPU ISA being enhanced but architecturally similar.
Freescale is bringing an enhanced version of AltiVec to e6500 based QorIQ processors.
VMX128.
IBM enhanced VMX for use in Xenon (Xbox 360) and called this enhancement VMX128. The enhancements comprise new routines targeted at gaming (accelerating 3D graphics and game physics) and a total of 128 registers. VMX128 is not entirely compatible with VMX/Altivec, as a number of integer operations were removed to make space for the larger register file and additional application-specific operations.
VSX.
Power ISA v2.06 introduces the new VSX vector-scalar instructions which extend SIMD processing for the Power ISA to support up to 64 registers, with support for regular floating point, decimal floating point and vector execution. POWER7 is the first Power Architecture processor to implement Power ISA v2.06.
Issues.
In C++, the standard way of accessing AltiVec support is mutually exclusive with use of the Standard Template Library codice_4 class template due to the treatment of "vector" as a reserved word when the compiler does not implement the context sensitive keyword version of vector. However, it may be possible to combine them using compiler-specific workarounds; for instance, in GCC one may do codice_5 to remove the codice_6 keyword, and then use the GCC-specific codice_7 keyword in its place.
Implementations.
The following processors have AltiVec, VMX or VMX128 included

</doc>
<doc id="55362" url="http://en.wikipedia.org/wiki?curid=55362" title="78 BC">
78 BC

Year 78 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Lepidus and Catulus (or, less frequently, year 676 "Ab urbe condita"). The denomination 78 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Roman Republic.
</onlyinclude>

</doc>
<doc id="55363" url="http://en.wikipedia.org/wiki?curid=55363" title="79 BC">
79 BC

Year 79 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Isauricus and Pulcher (or, less frequently, year 675 "Ab urbe condita"). The denomination 79 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Roman republic.
</onlyinclude>

</doc>
<doc id="55364" url="http://en.wikipedia.org/wiki?curid=55364" title="MMX (instruction set)">
MMX (instruction set)

MMX is a single instruction, multiple data (SIMD) instruction set designed by Intel, introduced in 1997 with their P5-based Pentium line of microprocessors, designated as "Pentium with MMX Technology". It developed out of a similar unit introduced on the Intel i860, and earlier the Intel i750 video pixel processor. MMX is a processor supplementary capability that is supported on recent IA-32 processors by Intel and other vendors.
MMX has subsequently been extended by several programs by Intel and others: 3DNow! and ongoing revisions of Streaming SIMD Extensions.
Naming.
MMX is officially a meaningless initialism trademarked by Intel; unofficially, the initials have been variously explained as standing for "MultiMedia eXtension", "Multiple Math eXtension", or "Matrix Math eXtension".
AMD, during one of its numerous court battles with Intel, produced marketing material from Intel indicating that MMX stood for "Matrix Math Extensions". Since an initialism cannot be trademarked, this was an attempt to invalidate Intel's trademark. In 1997, Intel filed suit against AMD and Cyrix Corp. for misuse of its trademark MMX. AMD and Intel settled, with AMD acknowledging MMX as a trademark owned by Intel, and with Intel granting AMD rights to use the MMX trademark as a technology name, but not a processor name.
Technical details.
MMX defined eight registers, known as MM0 through MM7 (henceforth referred to as MMn). To avoid compatibility problems with the context switch mechanisms in existing operating systems, these registers were aliases for the existing x87 FPU stack registers (so no new registers needed to be saved or restored). Hence, anything that was done to the floating point stack would also affect the MMX registers and vice versa. However, unlike the FP stack, the MMn registers are directly addressable (random access).
Each of the MMn registers holds 64 bits (the mantissa-part of a full 80-bit FPU register). The main usage of the MMX instruction set is based on the concept of packed data types, which means that instead of using the whole register for a single 64-bit integer, it is possible to process two 32-bit integers, four 16-bit integers, or eight 8-bit integers concurrently.
The mapping of the MMX registers onto the existing FPU registers made it somewhat difficult to work with floating point and SIMD data in the same application. To maximize performance, programmers often used the processor exclusively in one mode or the other, deferring the relatively slow switch between them as long as possible.
Because the FPU stack registers are 80 bits wide, the upper 16 bits of the stack registers go unused in MMX, and these bits are all set to ones, making them NaNs or infinities in the floating point representation. This can be used to decide whether a particular register's content is intended as floating point or SIMD data.
MMX provides only integer operations. When originally developed, for the Intel i860, the use of integer math made sense (both 2D and 3D calculations required it), but as graphics cards that did much of this became common, integer SIMD in the CPU became somewhat redundant for graphical applications. On the other hand, the saturation arithmetic operations in MMX could significantly speed up some digital signal processing applications.
Successor.
AMD, a competing x86 microprocessor vendor, enhanced Intel's MMX with their own 3DNow! instruction set. 3DNow is best known for adding single-precision (32-bit) floating-point support to the SIMD instruction-set, among other integer and more general enhancements.
Following MMX, Intel's next major x86 extension was the SSE, introduced with the Pentium-III family (roughly a year after AMD's 3DNow! was introduced.)
SSE addressed the core shortcomings of MMX (inability to mix integer-SIMD ops with any floating-point ops) by creating a new 128-bit wide register file (XMM0–XMM7) and new SIMD instructions for it. Like 3DNow!, SSE focused exclusively on single-precision floating-point operations (32-bit); integer SIMD operations were still performed using the MMX register and instruction set. However, the new XMM register-file allowed SSE SIMD-operations to be freely mixed with either MMX or x87 FPU ops.
SSE2, introduced with the Pentium 4, further extended the x86 SIMD instruction set with integer (8/16/32 bit) and double-precision floating-point data support for the XMM register file. SSE2 also allowed the MMX opcodes to use XMM register operands, but ended this support with SSE4 (and recently with SSE4.2, introduced in the Core microarchitecture). However, since processor support for any SSE revision also implies support for MMX, the removal does not limit the data types usable by x86 SIMD.
MMX in embedded applications.
Intel's and Marvell's XScale microprocessor core starting with PXA270 include an SIMD instruction set extension to the ARM core called iwMMXt whose functions are similar to those of the IA-32 MMX extension. iwMMXt stands for "Intel Wireless MMX Technology". It provides arithmetic and logic operations on 64-bit integer numbers (the software may choose to instead perform two 32-bit, four 16-bit or eight 8-bit operations in a single instruction). The extension contains 16 data registers of 64-bits and eight control registers of 32-bits. All registers are accessed through standard ARM architecture coprocessor mapping mechanism. iwMMXt occupies coprocessors 0 and 1 space, and some of its opcodes clash with the opcodes of the earlier floating-point extension, FPA.
Later versions of Marvell's ARM processors supports both WMMX (Wireless MMX) and WMMX2 (Wireless MMX2) support.

</doc>
<doc id="55365" url="http://en.wikipedia.org/wiki?curid=55365" title="Streaming SIMD Extensions">
Streaming SIMD Extensions

In computing, Streaming SIMD Extensions (SSE) is a SIMD instruction set extension to the x86 architecture, designed by Intel and introduced in 1999 in their Pentium III series processors as a reply to AMD's 3DNow!. SSE contains 70 new instructions, most of which work on single precision floating point data. SIMD instructions can greatly increase performance when exactly the same operations are to be performed on multiple data objects. Typical applications are digital signal processing and graphics processing.
Intel's first IA-32 SIMD effort was the MMX instruction set. MMX had two main problems: it re-used existing floating point registers making the CPU unable to work on both floating point and SIMD data at the same time, and it only worked on integers. SSE floating point instructions operate on a new independent register set (the XMM registers), and it adds a few integer instructions that work on MMX registers.
SSE was subsequently expanded by Intel to SSE2, SSE3, SSSE3, and SSE4. Because it supports floating point math, it had a wider application than MMX and became more popular. The addition of integer support in SSE2 made MMX largely redundant, though further performance increases can be attained in some situations by using MMX in parallel with SSE operations.
SSE was originally called Katmai New Instructions (KNI), Katmai being the code name for the first Pentium III core revision. During the Katmai project Intel sought to distinguish it from their earlier product line, particularly their flagship Pentium II. It was later renamed Intel Streaming SIMD Extensions (ISSE), then SSE. AMD eventually added support for SSE instructions, starting with its Athlon XP and Duron (Morgan core) processors.
Registers.
SSE originally added eight new 128-bit registers known as XMM0 through XMM7. The AMD64 extensions from AMD (originally called "x86-64") added a further eight registers XMM8 through XMM15, and this extension is duplicated in the Intel 64 architecture. There is also a new 32-bit control/status register, MXCSR. The registers XMM8 through XMM15 are accessible only in 64-bit operating mode.
SSE used only a single data type for XMM registers:
SSE2 would later expand the usage of the XMM registers to include:
Because these 128-bit registers are additional machine states that the operating system must preserve across task switches, they are disabled by default until the operating system explicitly enables them. This means that the OS must know how to use the FXSAVE and FXRSTOR instructions, which is the extended pair of instructions which can save all x86 and SSE register states all at once. This support was quickly added to all major IA-32 operating systems.
The first CPU to support SSE, the Pentium III, shared execution resources between SSE and the FPU. While a compiled application can interleave FPU and SSE instructions side-by-side, the Pentium III will not issue an FPU and an SSE instruction in the same clock cycle. This limitation reduces the effectiveness of pipelining, but the separate XMM registers do allow SIMD and scalar floating point operations to be mixed without the performance hit from explicit MMX/floating point mode switching.
SSE instructions.
SSE introduced both scalar and packed floating point instructions.
Example.
The following simple example demonstrates the advantage of using SSE. Consider an operation like vector addition, which is used very often in computer graphics applications. To add two single precision, four-component vectors together using x86 requires four floating-point addition instructions
 vec_res.x = v1.x + v2.x;
 vec_res.y = v1.y + v2.y;
 vec_res.z = v1.z + v2.z;
 vec_res.w = v1.w + v2.w;
This would correspond to four x86 FADD instructions in the object code. On the other hand, as the following pseudo-code shows, a single 128-bit 'packed-add' instruction can replace the four scalar addition instructions.
Software and hardware issues.
With all x86 instruction set extensions, it is up to the BIOS, operating system and application programmer to test and detect their existence and proper operation.
User application uptake of the x86 extensions has been slow with even bare minimum baseline MMX and SSE support (in some cases) not being supported by applications some 10 years after these extensions became commonly available. Distributed computing has accelerated the use of these extensions in the scientific community—and many scientific applications refuse to run unless the CPU supports SSE2 or SSE3.
The use of multiple revisions of an application to cope with the many different sets of extensions available is the simplest way around the x86 extension optimization problem. Software libraries and some applications have begun to support multiple extension types hinting that full use of available x86 instructions may finally become common some 5 to 15 years after the instructions were initially introduced.
Identifying.
Processor ID applications

</doc>
<doc id="55366" url="http://en.wikipedia.org/wiki?curid=55366" title="Port Arthur">
Port Arthur

Port Arthur may refer to: 
In China:
In Australia:
In the United States:
In Canada:
In Finland:
In other uses:

</doc>
<doc id="55374" url="http://en.wikipedia.org/wiki?curid=55374" title="Hercules (emulator)">
Hercules (emulator)

Hercules is a computer emulator allowing software written for IBM mainframe computers (System/370, System/390, and zSeries/System z) and for plug compatible mainframes (such as Amdahl machines) to run on other types of computer hardware, notably on low-cost personal computers.
Although there are other mainframe emulators performing a similar function, Hercules is significant in enabling private individuals to run mainframe computer software on their own personal computers.
Hercules runs under multiple parent operating systems including Linux, MS Windows, FreeBSD, Oracle Solaris, and Apple Mac OS X and is released under the open source software license QPL. It is analogous to Bochs and QEMU in that it emulates CPU instructions and select peripheral devices only. A vendor (or distributor) must still provide an operating system, and the user must install it. Hercules was notably the first mainframe emulator to incorporate 64-bit z/Architecture support, beating out commercial offerings.
Roger Bowler, a mainframe systems programmer, started development of the Hercules emulator in 1999. Jay Maynard currently maintains and hosts the project.
Design.
The emulator is written almost entirely in C. Its developers ruled out using machine-specific assembly code to avoid problems with portability even though such code could significantly improve performance. There are two exceptions: Hercules uses hardware assists to provide inter-processor consistency when emulating multiple CPUs on SMP host systems, and Hercules uses assembler assists to convert between little-endian and big-endian data on platforms where the operating system provides such services and on x86/x86-64 processors.
Operating systems status.
Hercules is technically compatible with all IBM mainframe operating systems, even older versions which no longer run on newer mainframes. However, many mainframe operating systems require vendor licenses to run legally. Newer licensed operating systems, such as OS/390, z/OS, VSE/ESA, z/VSE, VM/ESA, z/VM, TPF/ESA, and z/TPF are technically compatible but cannot legally run on the Hercules emulator except in very limited circumstances, and they must always be licensed from IBM. IBM's Coupling Facility control code, which enables Parallel Sysplex, and UTS also require licenses to run.
Operating systems which may legally be run without license from IBM on Hercules include:
Usage.
Hercules can be used as a development environment to verify that code is portable (across Linux processor architectures, for example), supports symmetric multiprocessing (SMP), and is 64-bit "clean."
There is also a large community of current and former mainframe operators and programmers, as well as those with no prior experience, who use Hercules and the public domain IBM operating systems as a hobby and for learning purposes. Most of the skills acquired when exploring classic IBM mainframe operating system versions are still relevant when transitioning to licensed IBM machines running the latest versions.
The open source nature of Hercules means that anyone can produce their own customized version of the emulator. For example, a group of developers independent of the Hercules project implemented a hybrid mainframe architecture which they dubbed "S/380" using modifications to both Hercules and to freely available classic versions of MVS, enhancing the latter with some degree of 31-bit binary compatibility with later operating system versions.
Performance.
It is difficult to determine exactly how Hercules emulation performance corresponds to real mainframe hardware, but the performance characteristics are understandably quite different. This is partially due to the difficulty of comparing real mainframe hardware to other PCs and servers as well as the lack of concrete, controlled performance comparisons. Performance comparisons are likely legally impossible for licensed IBM operating systems, and those operating systems are quite different from other operating systems, such as Linux.
Hercules expresses its processing performance in MIPS. Due to the age of the earlier System/360 and System/370 hardware, it is a relatively safe assumption that Hercules will outperform them when running on moderately powerful hardware, despite the considerable overhead of emulating a computer architecture in software. However, newer, partially or fully configured System z machines outperform Hercules by a wide margin. A relatively fast dual processor X86 machine running Hercules is capable of sustaining about 50 to 60 MIPS for code that utilizes both processors in a realistic environment, with sustained rates rising to a reported 300 MIPS on leading-edge (early 2009) PC-class systems. Hercules can produce peaks of over 1200 MIPS when running in a tight loop, such as in a synthetic instruction benchmark or with other small, compute-intensive programs.
Tom Lehmann, co-founder of TurboHercules, wrote:
...We can run a reasonably sized load (800 MIPS with our standard package). If the machine in question is larger than that, we can scale to 1600 MIPS with our quad Nehalem based package, and we have been promised an 8 way Nehalem EX based machine early next year that should take us to the 3200 MIPS mark. Anything bigger than that is replicated by a collection of systems.
Hercules generally outperforms IBM's PC based mainframes from the mid-1990s, which have an advertised peak performance of around 29 MIPS. Compared to the more powerful but still entry-level IBM Multiprise 2000 and 3000 mainframes (also from the 1990s), Hercules on typical X86 hardware would be considered a mid-range server in performance terms. For every mainframe after the 9672 Generation 1, Hercules would generally be the lowest end system. For comparison, current high-end IBM zEnterprise 196 systems can deliver over 52,000 MIPS per machine, and they have considerable I/O performance advantages. With the same number of emulated Sys Z processors, z/PDT is about 3 times faster than Hercules.
Note that there are other non-functional system attributes beyond performance which are typically relevant to mainframe operators.
TurboHercules.
In 2009, Roger Bowler founded TurboHercules SAS, based in France, to commercialize the Hercules technology. In July, 2009, TurboHercules SAS asked IBM to license z/OS to its customers for use on systems sold by TurboHercules. IBM declined the company's request. In March, 2010, TurboHercules SAS filed a complaint with European Commission regulators, alleging that IBM infringed EU antitrust rules through its alleged tying of mainframe hardware to its mainframe operating system, and the EC opened a preliminary investigation. In November, 2010, TurboHercules announced that it had received an investment from Microsoft Corporation. In September, 2011, EC regulators closed their investigation without action.

</doc>
<doc id="55375" url="http://en.wikipedia.org/wiki?curid=55375" title="Coordination of Information on the Environment">
Coordination of Information on the Environment

Coordination of Information on the Environment (CORINE) is a European programme initiated in 1985 by the European Commission, aimed at gathering information relating to the environment on certain priority topics for the European Union (air, water, soil, land cover, coastal erosion, biotopes, etc.). Since 1994, the European Environment Agency (EEA) integrated CORINE in its work programme. EEA is responsible for providing objective, timely and targeted information on Europe's environment.

</doc>
<doc id="55377" url="http://en.wikipedia.org/wiki?curid=55377" title="ELDIS">
ELDIS

Eldis is a database and email service of information sources on international development. It aims to share the best knowledge on development, policy, practice and research.
Background.
"Eldis" was originally an acronym for "Electronic Development and Environment Information System". It is one of a family of knowledge services produced at the Institute of Development Studies, Sussex, England.
Funding.
Eldis is funded by the UK Department for International Development (DFID), Swedish International Development Cooperation Agency (Sida), the and the .
Database.
The information on Eldis is organised into a number of subject-focused "resource guides" and regional and country "profiles"
External links.
Country profiles.
By region:

</doc>
<doc id="55380" url="http://en.wikipedia.org/wiki?curid=55380" title="Disk partitioning">
Disk partitioning

Disk partitioning is used to mean the "partitioning" or "division" of certain kinds of secondary storage (such as hard disk drives (HDDs)), via the creation of multiple partitions. Partitions are logical containers which are usually used to house filesystems, where operating systems, applications, and data are installed on. A single partition may span the entirety of a physical storage device.
A partition editor software program can be used to create, resize, delete, and manipulate these partitions on the HDD. A partition on a traditional mechanical hard drive consists of a range of cylinders of HDD—i.e. each partition is defined by both a start and end cylinder (the size of cylinders varying from disk to disk).
Benefits of multiple partitions.
Creating more than one partition has the following advantages:
Disadvantages of multiple partitions.
Creating more than one partition has the following disadvantages, as compared to having a single partition spanning the same disk area:
PC partition types.
This section describes the master boot record (MBR) partitioning scheme, as used in DOS, Microsoft Windows and Linux (among others) on PC-compatible computer systems. For examples of partitioning schemes used in other operating systems, see the general article on partition tables.
The total data storage space of a PC HDD can contain at most four "primary partitions", or alternatively three primary partitions and an "extended partition". The "Partition Table", located in the master boot record, contains 16-byte entries, each of which describes a partition.
The "partition type" is identified by a 1-byte code found in its partition table entry. Some of these codes (such as 0x05 and 0x0F) may be used to indicate the presence of an extended partition. Most are used by an operating system's bootloader (that examines partition tables) to decide if a partition contains a file system that can be used to "mount / access" for reading or writing data.
Primary partition.
A primary partition contains one file system. In DOS and all early versions of Microsoft Windows systems, Microsoft required what it called the system partition to be the first partition. All Windows operating systems from Windows 95 onwards can be located on (almost) any partition, but the boot files (codice_1, codice_2, codice_3, etc.) must reside on a primary partition. However, other factors, such as a PC's BIOS (see Boot sequence on standard PC) may also impose specific requirements as to which partition must contain the primary OS.
The partition type "code" for a primary partition can either correspond to a file system contained within (e.g. 0x07 means either an NTFS or an OS/2 HPFS file system) or indicate that the partition has a special use (e.g. code 0x82 usually indicates a Linux "swap" partition). The FAT16 and FAT32 file systems have made use of a number of partition type codes due to the limits of various DOS and Windows OS versions. Though a Linux operating system may recognize a number of different file systems (ext4, ext3, ext2, ReiserFS, etc.), they have all consistently used the same partition type code: 0x83 (Linux native file system).
Extended partition.
An HDD may contain only one extended partition, but that extended partition can be subdivided into multiple logical partitions. DOS/Windows systems may then assign a unique drive letter to each logical partition.
Partitioning schemes.
DOS, Windows, and OS/2.
With DOS, Microsoft Windows, and OS/2, a common practice is to use one primary partition for the active file system that will contain the operating system, the page/swap file, all utilities, applications, and user data. On most Windows consumer computers, the drive letter C: is routinely assigned to this primary partition. Other partitions may exist on the HDD that may or may not be visible as drives, such as recovery partitions or partitions with diagnostic tools or data. (Microsoft drive letters do not correspond to partitions in a one-to-one fashion, so there may be more or fewer drive letters than partitions.)
Microsoft Windows 2000, XP, Vista, and Windows 7 include a 'Disk Management' program which allows for the creation, deletion and resizing of FAT and NTFS partitions. The Windows Disk Manager in Windows Vista and Windows 7 utilizes a new 1 MB partition alignment scheme which is fundamentally incompatible with Windows 2000, XP, OS/2, DOS as well as many other operating systems.
Unix-like systems.
On Unix-based and Unix-like operating systems such as GNU/Linux, OS X, BSD, and Solaris, it is possible to use multiple partitions on a disk device. Each partition can be formatted with a file system or as a swap partition.
Multiple partitions allow directories such as /tmp, /usr, /var, or /home to be allocated their own filesystems. Such a scheme has a number of advantages:
A common configuration for GNU/Linux desktop systems is to use two partitions: one holding a file system mounted on "/" (the root directory) and a swap partition.
By default, OS X systems also use a single partition for the entire filesystem and use a swap file inside the file system (like Windows) rather than a swap partition.
In Solaris, partitions are sometimes known as slices. This is a conceptual reference to the slicing of a cake into several pieces.
The term "slice" is used in the FreeBSD operating system to refer to Master Boot Record partitions, to avoid confusion with FreeBSD's own disklabel-based partitioning scheme. However, GUID Partition Table partitions are referred to as "partition" world-wide.
Multi-boot and mixed-boot systems.
Multi-boot systems are computers where the user can boot into one of two or more distinct operating systems (OS) stored in separate storage devices or in separate partitions of the same storage device. In such systems a menu at startup gives a choice of which OS to boot/start (and only one OS at a time is loaded).
This is distinct from virtual operating systems, in which one operating system is run as a self-contained virtual "program" within another already-running operating system. (An example is a Windows OS "virtual machine" running from within a Linux OS.)
GUID Partition Table.
The GUID Partition Table (Globally Unique IDentifier) is a part of the Unified Extensible Firmware Interface (UEFI) standard for the layout of the partition table on a physical hard disk. Many operating systems now support this standard.
Partition recovery.
When a partition is deleted, its entry is removed from a table and the data is no longer accessible. The data remains on the disk until being overwritten. Specialized recovery utilities, (such as TestDisk, M3 Partition Recovery and gpart), may be able to locate "lost" file systems and recreate a partition table which includes entries for these recovered file systems. Some disk utilities may overwrite a number of beginning sectors of a partition they delete. For example, if Windows Disk Management (Windows 2000/XP, etc.) is used to delete a partition, it will overwrite the first sector (relative sector 0) of the partition before removing it. It still may be possible to restore a FAT or NTFS partition if a backup boot sector is available.
Compressed disks.
HDDs can be compressed to create additional space. In DOS and early Microsoft Windows, programs such as Stacker (DR-DOS except 6.0), SuperStor (DR DOS 6.0), DoubleSpace, or DriveSpace (Windows 95) were used. This compression was done by creating a very large file on the partition, then storing the disk's data in this file. At startup, device drivers opened this file and assigned it a separate letter. Frequently, to avoid confusion, the original partition and the compressed drive had their letters swapped, so that the compressed disk is C:, and the uncompressed area (often containing system files) is given a higher name.
Versions of Windows using the NT kernel, including the most recent versions, XP and Vista, contain intrinsic disk compression capability. The use of separate disk compression utilities has declined sharply.

</doc>
<doc id="55382" url="http://en.wikipedia.org/wiki?curid=55382" title="Supernatural">
Supernatural

The supernatural (Medieval Latin: "supernātūrālis": "supra" "above" + "naturalis" "nature", first used: 1520–30 AD) is that which is not subject to the laws of physics or, more figuratively, that which is said to exist above and beyond nature.
The supernatural is a feature of the philosophical traditions of Neoplatonism and Scholasticism. Most religions include the supernatural, and it is also a feature of the paranormal and occultism.
In Catholicism.
In Catholicism, while the meaning of the term and its antithesis vary, the “Supernatural Order” is the gratuitous production, by God, of the ensemble of miracles for the elevation of man to a state of grace, including the hypostatic union (Incarnation), the beatific vision, and the ministry of angels. Divine operation, “spiritual facts” and “voluntary determinations” are consistently referred to as “supernatural” by those who specifically preclude the “extrinsic concurrence” of God or by those espousing a materialist or determinist worldview that excludes immaterial beings or free will. Barring disingenuous intent, there is no objection to this manner of speaking.
Catholic theologians sometimes call supernatural the miraculous way in which certain effects, in themselves natural, are produced, or certain endowments (like man's immunity from death, suffering, passion, and ignorance) that bring the lower class up to the higher though always within the limits of the created, but they are careful in qualifying the former as accidentally supernatural ("supernaturale per accidens") and the latter as relatively supernatural ("prœternaturale"). For a concept of the substantially and absolutely supernatural, they start from a comprehensive view of the natural order taken, in its amplest acceptation, for the aggregate of all created entities and powers, including the highest natural endowments of which the rational creature is capable, and even such Divine operations as are demanded by the effective carrying out of the cosmic order. The supernatural order is then more than a miraculous way of producing natural effects, or a notion of relative superiority within the created world, or the necessary concurrence of God in the universe; it is an effect or series of effects substantially and absolutely above all nature and, as such, calls for an exceptional intervention and gratuitous bestowal of God and rises in a manner to the Divine order, the only one that transcends the whole created world... It is obvious also that this uplifting of the rational creature to the supernatural order cannot be by way of absorption of the created into the Divine or of fusion of both into a sort of monistic identity, but only by way of union or participation, the two terms remaining perfectly distinct.—Joseph Sollier, The Catholic Encyclopedia. Vol. 14., Supernatural Order
 Divine revelation of the supernatural order is considered to be a matter of fact, contingent upon proper evidence of such, (miracle, prophecy etc.). “The revelation and its evidences are called extrinsic and auxiliary supernatural, the elevation itself retaining the name of intrinsic or, according to some, theological supernatural.” The supernatural order was analyzed primarily by scholastic and post-Tridentine theologians. Theories denying or belittling the supernatural order, are historically classified into three groups:
Rosmini ... unwittingly, [may] have paved the way for them in the following vaguely Subjectivist proposition: “The supernatural order consists in the manifestation of Being in the plenitude of its reality, and the effect of that manifestation is a God-like sentiment, inchoate in this life through the light of faith and grace, consummate in the next through the light of glory”... Preserving the dogmatic formulæ while voiding them of their contents, the Modernists constantly speak of the supernatural, but they understand thereby the advanced stages of an evolutive process of the religious sentiment. There is no room in their system for the objective and revealed supernatural: their Agnosticism declares it unknowable, their Immanentism derives it from our own vitality, their symbolism explains it in term of subjective experience and their criticism declares non-authentic the documents used to prove it. “There is no question now,” says Pius X, in his Encyclical “Pascendi” of 8 Sept., 1907, “of the old error by which a sort of right to the supernatural was claimed for human nature. We have gone far beyond that. We have reached the point where it is affirmed that our most holy religion, in the man Christ as in us, emanated from nature spontaneously and entirely. Than this, there is surely nothing more destructive of the whole supernatural order.”—Joseph Sollier, "Supernatural Order" in "The Catholic Encyclopedia". Vol. 14
Process theology.
Process theology or "process thought" is a school of thought influenced by the metaphysical process philosophy of Alfred North Whitehead (1861–1947) and further developed by Charles Hartshorne (1897–2000).
It is not possible, in process metaphysics, to conceive divine activity as a “supernatural” intervention into the “natural” order of events. Process theists usually regard the distinction between the supernatural and the natural as a by-product of the doctrine of creation "ex nihilo". In process thought, there is no such thing as a realm of the natural in contrast to that which is supernatural. On the other hand, if “the natural” is defined more neutrally as “what is in the nature of things,” then process metaphysics characterizes the natural as the creative activity of actual entities. In Whitehead's words, “It lies in the nature of things that the many enter into complex unity” (Whitehead 1978, 21). It is tempting to emphasize process theism's denial of the supernatural and thereby highlight what the process God cannot do in comparison to what the traditional God can do (that is, to bring something from nothing). In fairness, however, equal stress should be placed on process theism's denial of the natural (as traditionally conceived) so that one may highlight what the creatures cannot do, in traditional theism, in comparison to what they can do in process metaphysics (that is, to be part creators of the world with God).—Donald Viney, "Process Theism" in "The Stanford Encyclopedia of Philosophy"
Contrasting views.
The metaphysical considerations of the existence of the supernatural can be difficult to approach as an exercise in philosophy or theology because any dependencies on its antithesis, the natural, will ultimately have to be inverted or rejected.
One complicating factor is that there is no universal agreement about the definition of "natural" or the limits of naturalism. Concepts in the supernatural domain are closely related to concepts in religious spirituality and occultism or spiritualism. Additionally, by definition anything that exists naturally is not supernatural.
For sometimes we use the word "nature" for that "Author of nature" whom the schoolmen, harshly enough, call "natura naturans", as when it is said that "nature" hath made man partly corporeal and partly immaterial. Sometimes we mean by the "nature" of a thing the "essence", or that which the schoolmen scruple not to call the "quiddity" of a thing, namely, the "attribute" or "attributes" on whose score it is what it is, whether the thing be corporeal or not, as when we attempt to define the "nature" of an "angel", or of a "triangle", or of a "fluid" body, as such. Sometimes we take "nature" for an internal principle of motion, as when we say that a stone let fall in the air is by "nature" carried towards the centre of the earth, and, on the contrary, that fire or flame does "naturally" move upwards toward heaven. Sometimes we understand by "nature" the established course of things, as when we say that "nature" makes the night succeed the day, "nature" hath made respiration necessary to the life of men. Sometimes we take "nature" for an aggregate of powers belonging to a body, especially a living one, as when physicians say that "nature" is strong or weak or spent, or that in such or such diseases "nature" left to herself will do the cure. Sometimes we take nature for the universe, or system of the corporeal works of God, as when it is said of a phoenix, or a chimera, that there is no such thing in "nature", i.e. in the world. And sometimes too, and that most commonly, we would express by "nature" a semi-deity or other strange kind of being, such as this discourse examines the notion of.<br><br>And besides these more absolute acceptions, if I may so call them, of the word "nature", it has divers others (more relative), as "nature" is wont to be set or in opposition or contradistinction to other things, as when we say of a stone when it falls downwards that it does it by a "natural motion", but that if it be thrown upwards its motion that way is "violent". So chemists distinguish vitriol into "natural" and "fictitious", or made by art, i.e. by the intervention of human power or skill; so it is said that water, kept suspended in a sucking pump, is not in its "natural" place, as that is which is stagnant in the well. We say also that wicked men are still in the state of "nature", but the regenerate in a state of "grace"; that cures wrought by medicines are natural operations; but the miraculous ones wrought by Christ and his apostles were "supernatural".
—Robert Boyle, A Free Enquiry into the Vulgarly Received Notion of Nature
In a letter to the Reverend Dr. Richard Bentley in 1692, Isaac Newton wrote: "To your second query I answer that the motions which the planets now have could not spring from any natural cause alone but were impressed by an intelligent agent." This statement is referenced by Intelligent Design advocate Stephen C. Meyer in "The Scientific Status of Intelligent Design", who refers to this statement as "Newton's famous postulation of special divine intervention to stabilize the orbital motion in the solar system" in developing his argument of the methodological equivalence of naturalistic and non-naturalistic (i.e. supernatural) theories.
The term "supernatural" is often used interchangeably with paranormal or preternatural — the latter typically limited to an adjective for describing abilities which appear to exceed the bounds of possibility. Epistemologically, the relationship between the supernatural and the natural is indistinct in terms of natural phenomena that, "ex hypothesi," violate the laws of nature, in so far as such laws are realistically accountable.
Parapsychologists use the term psi to refer to an assumed unitary force underlying the phenomena they study. Psi is defined in the "Journal of Parapsychology" as “a general term used to identify personal factors or processes in nature which transcend accepted laws” (1948: 311) and “which are non-physical in nature” (1962:310), and it is used to cover both extrasensory perception (ESP), an “awareness of or response to an external event or influence not apprehended by sensory means” (1962:309) or inferred from sensory knowledge, and psychokinesis (PK), “the direct influence exerted on a physical system by a subject without any known intermediate energy or instrumentation” (1945:305).—Michael Winkelman, Current Anthropology
Many supporters of supernatural explanations believe that past, present, and future complexities and mysteries of the universe cannot be explained solely by naturalistic means and argue that it is reasonable to assume that a non-natural entity or entities resolve the unexplained. Proponents of supernaturalism regard their belief system as more flexible, allowing more diversity in terms of intuition and epistemology.
Views on the "supernatural" vary, for example it may be seen as:

</doc>
<doc id="55383" url="http://en.wikipedia.org/wiki?curid=55383" title="Hard disk drive platter">
Hard disk drive platter

A hard disk drive platter (or disk) is the circular disk on which magnetic data is stored in a hard disk drive. The rigid nature of the platters in a hard drive is what gives them their name (as opposed to the flexible materials which are used to make floppy disk). Hard drives typically have several platters which are mounted on the same spindle. A platter can store information on both sides, requiring two heads per platter.
Design.
The magnetic surface of each platter is divided into small sub-micrometer-sized magnetic regions, each of which is used to represent a single binary unit of information. A typical magnetic region on a hard-disk platter (as of 2006) is about 200–250 nanometers wide (in the radial direction of the platter) and extends about 25–30 nanometers in the down-track direction (the circumferential direction on the platter), corresponding to about 100 billion bits per square inch of disk area (15.5 Gbit/cm2). The material of the main "magnetic medium" layer is usually a cobalt-based alloy. In today's hard drives each of these magnetic regions is composed of a few hundred magnetic grains, which are the base material that gets magnetized. As a whole, each magnetic region will have a magnetization.
One reason magnetic grains are used as opposed to a continuous magnetic medium is that they reduce the space needed for a magnetic region. In continuous magnetic materials, formations called Neel spikes tend to appear. These are spikes of opposite magnetization, and form for the same reason that bar magnets will tend to align themselves in opposite directions. These cause problems because the spikes cancel each other's magnetic field out, so that at region boundaries, the transition from one magnetization to the other will happen over the length of the Neel spikes. This is called the transition width.
Grains help solve this problem because each grain is in theory a single magnetic domain (though not always in practice). This means that the magnetic domains cannot grow or shrink to form spikes, and therefore the transition width will be on the order of the diameter of the grains. Thus, much of the development in hard drives has been in reduction of grain size.
Manufacture.
Platters are typically made using an aluminium or glass and ceramic substrate. In disk manufacturing, a thin coating is deposited on both sides of the substrate, mostly by a vacuum deposition process called magnetron sputtering. The coating has a complex layered structure consisting of various metallic (mostly non-magnetic) alloys as underlayers, optimized for the control of the crystallographic orientation and the grain size of the actual magnetic media layer on top of them, i.e. the film storing the bits of information. On top of it a protective carbon-based overcoat is deposited in the same sputtering process. In post-processing a nanometer thin polymeric lubricant layer gets deposited on top of the sputtered structure by dipping the disk into a solvent solution, after which the disk is buffed by various processes to eliminate small defects and verified by a special sensor on a flying head for absence of any remaining impurities or other defects (where the size of the bit given above roughly sets the scale for what constitutes a significant defect size). In the hard-disk drive the hard-drive heads fly and move radially over the surface of the spinning platters to read or write the data. Extreme smoothness, durability, and perfection of finish are required properties of a hard-disk platter.
In 2005–06, a major shift in technology of hard-disk drives and of magnetic disks/media began. Originally, in-plane magnetized materials were used to store the bits but has now been replaced by perpendicular recording.
The reason for this transition is the need to continue the trend of increasing storage densities, with perpendicularly oriented media offering a more stable solution for a decreasing bit size. Orienting the magnetization perpendicular to the disk surface has major implications for the disk's deposited structure and the choice of magnetic materials, as well as for some of the other components of the hard-disk drive (such as the head and the electronic channel).

</doc>
<doc id="55386" url="http://en.wikipedia.org/wiki?curid=55386" title="Ashkelon">
Ashkelon

Ashkelon (also Ashqelon and Ascalon; Hebrew: אַשְׁקְלוֹן ; Arabic: عسقلان‎ "ʿAsqalān "; Latin: "Ascalonia "; Akkadian: "Isqalluna "; Ancient Greek: Ἀσκάλων, "Askalon") is a coastal city in the Southern District of Israel on the Mediterranean coast, 50 km south of Tel Aviv, and 13 km north of the border with the Gaza Strip. The ancient seaport of Ashkelon dates back to the Neolithic Age. In the course of its history, it has been ruled by the Canaanites, the Philistines, the Egyptians, the Israelites, the Assyrians, the Babylonians, the Greeks, the Phoenicians, the Hasmoneans, the Romans, the Persians, the Arabs and the Crusaders, until it was destroyed by the Mamluks in 1270.
The Arab village of al-Majdal or al-Majdal Asqalan (Arabic: المجدل‎; Hebrew: אל-מג'דל, מגדל‎), was established a few kilometres inland from the ancient site in the 16th century, under Ottoman rule. In 1918, it became part of the British Occupied Enemy Territory Administration and in 1920 became part of Mandatory Palestine. In the 1948 Arab–Israeli War, al-Majdal was the forward position of the Egyptian Expeditionary Force based in Gaza.
The village was occupied by Israeli forces on 5 November 1948, by which time most of the Arab population of 11,000 were forced to leave. Jews moved into the area later that year. The Israeli town was initially called Migdal Gaza, Migdal Gad and Migdal Ashkelon. In 1953, the nearby neighborhood of Afridar was incorporated and the name "Ashkelon" was readopted to the town. By 1961, Ashkelon was ranked 18th among Israeli urban centers with a population of 24,000. In 2010, the population of Ashkelon was 112,900.
Etymology.
The name Ashkelon is probably western Semitic, and might be connected to the root "š-q-l" ("to weigh" from a Semitic root "ṯql", akin to Hebrew "šāqal" שָקַל or Arabic "θiql" ثِقْل "weight") perhaps attesting to its importance as a center for mercantile activities. "Scallion" and "shallot" are derived from "Ascalonia", the Latin name for Ashkelon.
History.
Ashkelon was the oldest and largest seaport in Canaan, one of the "five cities" of the Philistines, north of Gaza and south of Jaffa (Yafa).
Neolithic era.
The Neolithic site of Ashkelon is located on the Mediterranean coast, 1.5 km north of Tel Ashkelon. It is dated by Radiocarbon dating to ca. 7900 bp (uncalibrated), to the poorly known Pre-Pottery Neolithic C phase of the Neolithic. It was discovered and excavated in 1954 by French archaeologist Jean Perrot. In 1997–1998, a large scale salvage project was conducted at the site by Yosef Garfinkel on behalf of the Hebrew University of Jerusalem and nearly 1,000 sqm were examined. A final excavation report was published in 2008.
In the site over a hundred fireplaces and hearths were found and numerous pits, but no solid architecture, except for one wall. Various phases of occupation were found, one atop the other, with sterile layers of sea sand between them. This indicates that the site was occupied on a seasonal basis.
The main finds were enormous quantities of animal bones (ca. 100,000) and 20,000 flint artifacts. Usually at Neolithic sites flints far outnumber animal bones. The bones belong to domesticated and non-domesticated animals. When all aspects of this site are taken into account, it appears to have been used by pastoral nomads for meat processing. The nearby sea could supply salt necessary for the conservation of meat.
Canaanite settlement.
The city was originally built on a sandstone outcropping and has a good underground water supply. It was relatively large as an ancient city with as many as 15,000 people living inside the walls. Ashkelon was a thriving Middle Bronze Age (2000–1550 BC) city of more than 150 acres. Its commanding ramparts, measuring 1.5 mi long, 50 ft high and 150 ft thick, and even as a ruin they stand two stories high. The thickness of the walls was so great that the mudbrick city gate had a stone-lined, 8 ft wide tunnel-like barrel vault, coated with white plaster, to support the superstructure: it is the oldest such vault ever found. Later Roman and Islamic fortifications, faced with stone, followed the same footprint, a vast semicircle protecting Ashkelon on the land side. On the sea it was defended by a high natural bluff. A roadway more than 20 ft in width ascended the rampart from the harbor and entered a gate at the top.
In 1991 the ruins of a small ceramic tabernacle was found a finely cast bronze statuette of a bull calf, originally silvered, 4 in long. Images of calves and bulls were associated with the worship of the Canaanite gods El and Baal.
Ashkelon is mentioned in the Egyptian Execration Texts of the 11th dynasty as "Asqanu." In the Amarna letters ( 1350 BC), there are seven letters to and from Ashkelon's (Ašqaluna) king Yidya, and the Egyptian pharaoh. One letter from the pharaoh to Yidya was discovered in the early 1900s.
Philistine settlement.
The Philistines conquered Canaanite Ashkelon about 1150 BC. Their earliest pottery, types of structures and inscriptions are similar to the early Greek urbanised centre at Mycenae in mainland Greece, adding weight to the hypothesis that the Philistines were one of the populations among the "Sea Peoples" that upset cultures throughout the eastern Mediterranean at that time.
Ashkelon became one of the five Philistine cities that were constantly warring with the Israelites and the Kingdom of Judah. According to Herodotus, its temple of Venus was the oldest of its kind, imitated even in Cyprus, and he mentions that this temple was pillaged by marauding Scythians during the time of their sway over the Medes (653–625 BC). As it was the last of the Philistine cities to hold out against Babylonian king Nebuchadnezzar II. When it fell in 604 BC, burnt and destroyed and its people taken into exile, the Philistine era was over.
Classical period.
Ashkelon was soon rebuilt. Until the conquest of Alexander the Great, Ashkelon's inhabitants were influenced by the dominant Persian culture. It is in this archaeological layer that excavations have found dog burials. It is believed the dogs may have had a sacred role, however evidence is not conclusive. After the conquest of Alexander in the 4th century BC, Ashkelon was an important free city and Hellenistic seaport.
It had mostly friendly relations with the Hasmonean kingdom and Herodian kingdom of Judea, in the 2nd and 1st centuries BC. In a significant case of an early witch-hunt, during the reign of the Hasmonean queen Salome Alexandra, the court of Simeon ben Shetach sentenced to death eighty women in Ashkelon who had been charged with sorcery. Herod the Great, who became a client king of Rome over Judea and its surrounds in 30 BC, had not received Ashkelon, yet he built monumental buildings there: bath houses, elaborate fountains and large colonnades. A discredited tradition suggests Ashkelon has been his birthplace. In 6 CE, when a Roman imperial province was set in Judea, overseen by a lower-rank governor, Ashkelon was moved directly to the higher jurisdiction of the governor of Syria province.
The city remained loyal to Rome during the Great Revolt, 66–70 AD.
Byzantine period.
The city of Ascalon appears on a fragment of the 6th century AD Madaba Map.
The bishops of Ascalon whose names are known include Sabinus, who was at the First Council of Nicaea in 325, and his immediate successor, Epiphanius. Auxentius took part in the First Council of Constantinople in 381, Jobinus in a synod held in Lydda in 415, Leontius in both the Robber Council of Ephesus in 449 and the Council of Chalcedon in 451. Bishop Dionysius, who represented Ascalon at a synod in Jerusalem in 536, was on another occasion called upon to pronounce on the validity of a baptism with sand in waterless desert and sent the person to be baptized in water.
No longer a residential bishopric, Ascalon is today listed by the Catholic Church as a titular see.
Crusader era.
During the Crusades, Ashkelon (known to the Crusaders as "Ascalon") was an important city due to its location near the coast and between the Crusader States and Egypt. In 1099, shortly after the Siege of Jerusalem (1099), an Egyptian Fatimid army that had been sent to relieve Jerusalem was defeated by a Crusader force at the Battle of Ascalon. The city itself was not captured by the Crusaders because of internal disputes among their leaders. This battle is widely considered to have signified the end of the First Crusade. Until 1153, the Fatimids were able to launch raids into the Kingdom of Jerusalem from Ashkelon, which meant that the southern border of the Crusader States was constantly unstable. In response to these incursions into Outremer, King Fulk of Jerusalem constructed a number of Christian settlements around the city during the 1130s, in order to neutralise the threat of the Muslim garrison. In 1148, during the Second Crusade, the city was unsuccessfully besieged for eight days by a small Crusader army that was not fully supported by the Crusader States.
In 1150, the Fatimids fortified the city with 53 towers, as it was their most important frontier fortress. Three years later, after a five-month siege, the city was captured by a Crusader army led by King Baldwin III of Jerusalem. It was then added to the County of Jaffa to form the County of Jaffa and Ascalon, which became one of the four major seigneuries of the Kingdom of Jerusalem.
After the Crusader conquest of Jerusalem the six elders of the Karaite Jewish community in Ashkelon contributed to the ransoming of captured Jews and holy relics from Jerusalem's new rulers. The Letter of the Karaite elders of Ascalon, which was sent to the Jewish elders of Alexandria, describes their participation in the ransom effort and the ordeals suffered by many of the freed captives.
A few hundred Jews, Karaites and Rabbanites, were living in Ashkelon in the second half of the 12th century, but moved to Jerusalem when the city was destroyed in 1191.
Islamic era.
In 1187, Saladin took Ashkelon as part of his conquest of the Crusader States following the Battle of Hattin. In 1191, during the Third Crusade, Saladin demolished the city because of its potential strategic importance to the Christians, but the leader of the Crusade, King Richard I of England, constructed a citadel upon the ruins. Ashkelon subsequently remained part of the diminished territories of Outremer throughout most of the 13th century and Richard, Earl of Cornwall reconstructed and refortified the citadel during 1240–41, as part of the Crusader policy of improving the defences of coastal sites. The Egyptians retook Ashkelon in 1247 during As-Salih Ayyub's conflict with the Crusader States and the city was returned to Muslim rule. The Mamluk dynasty came into power in Egypt in 1250 and the ancient and medieval history of Ashkelon was brought to an end in 1270, when the Mamluk sultan Baybars ordered the citadel and harbour at the site to be destroyed. As a result of this destruction, the site was abandoned by its inhabitants and fell into disuse.
According to Shiite tradition, the head of Husayn ibn Ali, grandson of Mohammad, was buried in Ashkelon. In the late 11th century it was moved to a new shrine named Mashad Nabi Hussein (or Sabni Hussein) built for the purpose. In 1153, at the time of the Crusaders' conquest of Ashkelon, the head was moved to Fustat (Egypt). The shrine was described as the most magnificent building in Ashkelon. In the British Mandate period it was a "large "maqam" on top of a hill" with no tomb, but a fragment of a pillar showing the place where the head had been buried. In July 1950, the shrine was destroyed at the instructions of Moshe Dayan in accordance with a 1950s Israeli policy of erasing Muslim historical sites within Israel.
Ottoman and Mandate eras.
The Arab village of Majdal was mentioned by historians and tourists at the end of the 15th century. In 1596, Ottoman records showed Majdal to be a large village of 559 Muslim households, making it the 7th most populous locality in Palestine after Safad, Jerusalem, Gaza, Nablus, Hebron and Kafr Kanna.
The census of 1931 found 6,166 Muslims and 41 Christians living there. By 1948, the population had grown to about 11,000.
Majdal was especially known for its weaving industry. The town had around 500 looms in 1909. In 1920 a British Government report estimated that there were 550 cotton looms in the town with an annual output worth 30-40,000,000 Francs. But the industry suffered from imports from Europe and by 1927 only 119 weaving establishments remained. The three major fabrics produced were "malak" (silk), 'ikhdari' (bands of red and green) and 'jiljileh' (dark red bands). These were used for festival dresses throughout Southern Palestine. Many other fabrics were produced, some with poetic names such as "ji'nneh u nar" ("heaven and hell"), "nasheq rohoh" ("breath of the soul") and "abu mitayn" ("father of two hundred").
State of Israel.
During the 1948 war, the Egyptian army occupied a large part of Gaza including Majdal. Over the next few months, the town was subjected to Israeli air-raids and shelling. All but about 1,000 of the town's residents were forced to leave by the time it was captured by Israeli forces as a sequel to Operation Yoav on 4 November 1948. General Yigal Allon ordered the expulsion of the remaining Palestinians but the local commanders did not do so and the Arab population soon recovered to more than 2,500 due mostly to refugees slipping back and also due to the transfer of Palestinians from nearby villages. Most of them were elderly, women, or children. During the next year or so, the Palestinians were held in a confined area surrounded by barbed wire, which became commonly known as the "ghetto". Moshe Dayan and Prime Minister David Ben-Gurion were in favor of expulsion, while Mapam and the Israeli labor union Histadrut objected. The government offered the Palestinians positive inducements to leave, including a favorable currency exchange, but also caused panic through night-time raids. The first group was deported to the Gaza Strip by truck on 17 August 1950 after an expulsion order had been served. The deportation was approved by Ben-Gurion and Dayan over the objections of Pinhas Lavon, secretary-general of the Histadrut, who envisioned the town as a productive example of equal opportunity. By October 1950, 20 Palestinian families remained, most of whom later moved to Lydda or Gaza. According to Israeli records, in total 2,333 Palestinians were transferred to the Gaza Strip, 60 to Jordan, 302 to other towns in Israel, and a small number remained in Ashkelon. Lavon argued that this operation dissipated "the last shred of trust the Arabs had in Israel, the sincerity of the State's declarations on democracy and civil equality, and the last remnant of confidence the Arab workers had in the Histadrut." Acting on an Egyptian complaint, the Egyptian-Israel Mixed Armistice Commission ruled that the Palestinians transferred from Majdal should be returned to Israel, but this was not done.
Re-population of abandoned Arab dwellings by Jews became official policy by December 1948, but the process began slowly. The Israeli national plan of June 1949 designated al-Majdal as the site for a regional urban center of 20,000 people. From July 1949, new immigrants and demobilized soldiers moved to the new town, increasing the Jewish population to 2,500 within six months. During 1949, the town was renamed Migdal Gaza, and then Migdal Gad. Soon afterwards it became Migdal Ashkelon. In 1953 the nearby neighborhood of Afridar was incorporated and the current name Ashkelon was adopted. By 1961, Ashkelon ranked 18th among Israeli urban centers with a population of 24,000.
On 1–2 March 2008, rockets fired by Hamas from the Gaza Strip (some of them Grad rockets) hit Ashkelon, wounding seven, and causing property damage. Mayor Roni Mahatzri stated that "This is a state of war, I know no other definition for it. If it lasts a week or two, we can handle that, but we have no intention of allowing this to become part of our daily routine." On 12 May 2008, a rocket fired from the northern Gazan city of Beit Lahiya hit a shopping mall in southern Ashkelon, causing significant structural damage. According to "The Jerusalem Post", four people were seriously injured and 87 were treated for shock. 15 people suffered minor to moderate injuries as a result of the collapsed structure. Southern District Police chief Uri Bar-Lev believed the Grad-model Katyusha rocket was manufactured in Iran. In March 2008, 230 buildings and 30 cars were damaged by rocket fire on Ashkelon.
In March 2009, a Qassam rocket hit a school, destroying classrooms and injuring two people.
In July 2010, a Grad rocket hit a residential area in Ashkelon, damaging nearby cars and an apartment complex. In November 2014, the mayor, Itamar Shimoni, began a policy of discrimination against Arab workers, refusing to allow them to work on city projects to build bomb shelters for children. His discriminatory actions brought criticism from others, including Israeli Prime Minister Benjamin Netanyahu and Jerusalem mayor Nir Barkat who likened the discrimination to the anti-Semitism experienced by Jews in Europe 70 years earlier.
Panorama of modern Ashkelon
Urban development.
In 1949 and 1950, three immigrant transit camps (ma'abarot) were established alongside Majdal (renamed Migdal) for Jewish refugees from Arab countries, Romania and Poland. Northwest of Migdal and the immigrant camps, on the lands of the abandoned Arab village al-Jura, entrepreneur Zvi Segal, one of the signatories of Israel's Declaration of Independence, established the upscale Barnea neighborhood.
A large tract of land south of Barnea was handed over to the trusteeship of the South African Zionist Federation, which established the neighborhood of Afridar. Plans for the city were drawn up in South Africa according to the garden city model. Migdal was surrounded by a broad ring of orchards. Barnea developed slowly, but Afridar grew rapidly. The first homes, built in 1951, were inhabited by new Jewish immigrants from South Africa and South America, with some native-born Israelis. The first public housing project for residents of the transit camps, the Southern Hills Project (Hageva'ot Hadromiyot) or Zion Hill (Givat Zion), was built in 1952.
Economy.
Ashkelon is the northern terminus for the Trans-Israel pipeline, which brings petroleum products from Eilat to an oil terminal at the port. The Ashkelon seawater reverse osmosis (SWRO) desalination plant is the largest in the world. The project was developed as a BOT (Build-Operate-Transfer) by a consortium of three international companies: Veolia water, IDE Technologies and Elran. In March 2006, it was voted "Desalination Plant of the Year" in the Global Water Awards.
Since 1992, Israel Beer Breweries has been operating in Ashkelon, brewing Carlsberg and Tuborg beer for the Israeli market. The brewery is owned by the Central Bottling Company, which has also held the Israeli franchise for Coca-Cola products since 1968.
"Arak Ashkelon", a local brand of arak, is operating since 1925 and distributed throughout Israel.
Education.
The city has 19 elementary schools, and nine junior high and high schools. The Ashkelon Academic College opened in 1998, and now hosts thousands of students. Harvard University operates an archaeological summer school program in Ashkelon.
Landmarks.
Ashkelon Khan and Museum contains archaeological finds, among them a replica of Ashkelon's Canaanite silver calf, whose discovery was reported on the front page of "The New York Times".
The Outdoor Museum near the municipal cultural center displays two Roman burial coffins made of marble depicting battle and hunting scenes, and famous mythological scenes.
The remains of a 4th-century Byzantine church with marble slab flooring and glass mosaic walls can be seen in the Barnea Quarter. Remains of a synagogue from this period have also been found. A domed structure housing the 13th-century tomb of Sheikh Awad sits atop a hill overlooking Ashkelon's northern beaches. A Roman burial tomb two kilometers north of Ashkelon Park was discovered in 1937. There are two burial tombs, a painted Hellenistic cave and a Roman cave. The Hellenistic cave is decorated with paintings of nymphs, water scenes, mythological figures and animals.
There was an 11th-century mosque, "Maqam al-Nabi Hussein", a site of pilgrimage by both Sunnis and Shiites, which had been built under the Fatimids by Badrul’jamali and where tradition held that the head of Mohammad's grandson Hussein ibn Ali was buried, was blown up by the IDF under instructions from Moshe Dayan as part of a broader programme to destroy mosques in July 1950. The area was subsequently redeveloped for a local Israeli hospital, Barzilai. When his remains were later discovered on the hospital grounds, funds from the Shi'ite Ismaili sect in India were used to construct a marble prayer area, and it is visited by Shiite pilgrims from India and Pakistan.
In 1986 ruins of 4th- to 6th-century baths were found in Ashkelon. The bath houses are believed to have been used for prostitution. The remains of nearly 100 mostly male infants were found in a sewer under the bathhouse, leading to conjectures that prostitutes had discarded their unwanted newborns there. The Ashkelon Marina, located between Delila and Bar Kochba beaches, offers a shipyard and repair services. Ashkeluna is a water-slide park on Ashkelon beach.
Ashkelon National Park.
The ancient site of Ashkelon is now a national park on the city's southern coast. The walls that encircled the city are still visible, as well as Canaanite earth ramparts. The park contains Byzantine, Crusader and Roman ruins. The largest dog cemetery in the ancient world was discovered in Ashkelon.
Health care.
Ashkelon and environs is served by the Barzilai Medical Center, established in 1961. It was built in place of Hussein ibn Ali's 11th-century mosque, a center of Muslim pilgrimages, destroyed by the Israeli army in 1950. Situated six miles (10 km) from Gaza, the hospital has been the target of numerous Qassam rocket attacks, sometimes as many as 140 over one weekend. The hospital plays a vital role in treating wounded soldiers and terror victims. A new rocket and missile-proof emergency room is under construction.
Demographics.
In the early 1950s, many South African Jews settled in Ashkelon, establishing the Afridar neighbourhood. They were followed by an influx of immigrants from the United Kingdom.
Culture and sports.
The Ashkelon Sports Arena opened in 1999. The "Jewish Eye" is a Jewish world film festival
that takes place annually in Ashkelon. The festival marked its seventh year in 2010.
The Breeza Music Festival has been held yearly in and around Ashkelon's amphitheatre since 1992. Most of the musical performances are free. Israel Lacrosse operates substantial youth lacrosse programs in the city and recently hosted the Turkey men's national team in Israel's first home international in 2013.
Im schwarzen Walfisch zu Askalon ("in Ashkelons Black Wale inn") is a traditional German academic commercium song and describing a drinking binge staged in the ancient city.
Twin towns – sister cities.
Ashkelon is twinned with:
 Aix-en-Provence, France

</doc>
