<doc id="48678" url="http://en.wikipedia.org/wiki?curid=48678" title="Mahātmā">
Mahātmā

Mahatma (Mə-HÄT-mə) is Sanskrit for "Great Soul" (महात्मा "mahātmā": महा "mahā" (great) + आत्मं or आत्मन "ātman" [soul]). It is similar in usage to the modern Christian term saint. This epithet is commonly applied to prominent people like Mohandas Karamchand Gandhi, Munshiram (later Swami Shraddhananda), Lalon Shah and Jyotirao Phule. According to some authors Rabindranath Tagore is said to have used on March 6, 1915, this title for Gandhi;. Some claim that he was term Mahatma by the residents of Gurukul Kangadi in April 1915, and he in turn termed the founder Munshiram a Mahatma (who later became Swami Shraddhananda). However a document honoring him on Jan 21, 1915, at Jetpur, Gujarat, termed him Mahatma is preserved. The use of the term Mahatma in Jainism to denote a class of lay priests, has been noted since the 17th century.
Theosophy.
The word, used in a technical sense, was popularized in theosophical literature in the late 19th century, when Madame Helena Blavatsky, one of the founders of the Theosophical Society, claimed that her teachers were adepts (or Mahatmas) who reside in Asia.
According to the Theosophical teachings, the Mahatmas are not disembodied beings, but highly evolved people involved in overseeing the spiritual growth of individuals and the development of civilizations. Blavatsky was the first person in modern times to claim contact with these Adepts, especially the "Masters" Koot Hoomi and Morya. Alvin Boyd Kuhn wrote about mahātmās:
"The Masters whom Theosophy presents to us are simply high-ranking students in life's school of experience. They are members of our own evolutionary group, not visitants from the celestial spheres. They are supermen only in that they have attained knowledge of the laws of life and mastery over its forces with which we are still struggling."
In September and October 1880, Blavatsky visited A. P. Sinnett at Simla in northern India. The serious interest of Sinnett in the Theosophical teachings of Mme. Blavatsky and the work of the Theosophical Society prompted Mme. Blavatsky to establish a contact by correspondence between Sinnett and the two adepts who were sponsoring the society, Koot Hoomi and Morya.
From this correspondence Sinnett wrote "The Occult World" (1881) and "Esoteric Buddhism" (1883), both of which had an enormous influence in generating public interest in theosophy. The replies and explanations given by the Mahatmas to the questions by Sinnett are embodied in their letters from 1880 to 1885, published in London in 1923 as "The Mahatma Letters" to Sinnett. The Mahatmas also corresponded with a number of other persons during the early years of the Theosophical Society. Many of these letters have been published in two volumes titled "Letters from the Masters of the Wisdom", Series 1 and Series 2.
There has been a great deal of controversy concerning the existence of these particular adepts. Blavatsky's critics have doubted the existence of her Masters. See, for example, W.E. Coleman's "exposes". However, more than twenty five individuals testified to having seen and been in contact with these Mahatmas during Blavatsky's lifetime. In recent years, K. Paul Johnson has promoted his controversial theory about the Masters.
After Blavatsky's death in 1891, numerous individuals have claimed to be in contact with her Adept Teachers and have stated that they were new "messengers" of the Masters conveying various esoteric teachings. Currently various New Age, metaphysical, and religious organizations refer to them as Ascended Masters, although their character and teachings are in several respects different from those described by Theosophical writers.
Divine Light Mission.
The Divine Light Mission (DLM) was a Sant Mat-based movement begun in India in the 1930s by Hans Ji Maharaj and formally incorporated in 1960. The DLM had as many as 2,000 Mahatmas, all from India or Tibet, who taught the DLM's secret meditation techniques called "Knowledge". The Mahatmas, called "realised souls", or "apostles", also served as local leaders. After Hans Ji's death in 1966 his youngest son, Prem Rawat (known then as Guru Maharaj Ji or Bagyogeshwar), succeeded him. The young guru appointed some new Mahatmas, including one from the United States. In one incident, a prominent Indian Mahatma nearly beat a man to death in Detroit for throwing a pie at the guru. In the early 1980s, Prem Rawat replaced the Divine Light Mission organization with the Elan Vital and replaced the Mahatmas with initiators. The initiators did not have the revered status of the Mahatmas, and they were drawn mostly from Western followers. In the 2000s, the initiators were replaced by a video in which Rawat teaches the techniques himself.
In popular culture.
W.C. Fields used the pseudonym Mahatma Kane Jeeves when writing the script for "The Bank Dick" (1940), in a play on both the word "Mahatma" and a phrase an aristocrat might use when addressing a servant, before leaving the house: "My hat, my cane, Jeeves".
Jain Mahatmas.
Among the Jains the term Mahatma is used for class for scholars who are householders. See Yati for details.
Mahatma Hirananda of Mewad.
The Mewad Ramayana described as "one of the most beautiful manuscripts in the world" has been digitally reunited after being split between organisations in the UK and India for over 150 years, by the British Library and CSMVS Museum in Mumbai. The colophon states that the text, commissioned by Acarya Jasvant for the library of Maharana Jagat Singh I of Mewar, was written by the Mahatma Hirananda, was finished on Friday 25 November 1650. Mahatma Hirananda being a Jain scribe, incorporated traditional Jain scribal elements into the manuscript.
Jain Mahatmas in the Dabestan-e Mazaheb.
The famous Dabestan-e Mazaheb often attributed to one Mohsin Fani, written around 1655 CE. is a text written in the Mughal period that describes various religions and philosophies the author encountered. Its Section 11 is dedicated to Jainism. It states: "Similar to the durvishes of both classes (Srivaras and Jatis) is a third sect, called Mahá-átma; they have the dress and appearance of Jatis; only they do not pluck their hair with tweezers, but cut it. They accumulate money, cook their meal in their houses, drink cold water, and take to them a wife." The term Mahatma was thus used for priest/scholars who were not celibate. The present Persian edition of the text by Rezazadeh Malik attributes it to the son and successor of Azar Kayvan, 'Kay Khosrow Esfandiyar'.
Criticism.
K. Paul Johnson in his books speculates that the "Masters" that Blavatsky wrote about and produced letters from were actually idealizations of people who were her mentors. Aryel Sanat, author of "The inner life of Krishnamurti: private passion and perennial wisdom", wrote that Johnson "claims in all of his books that there were no Masters at all in early TS history, & that HPB invented them (as others had claimed she had invented her travels)." Sanat wrote that Johnson "deliberately ignores the main sources of evidence for their real physical existence."

</doc>
<doc id="48680" url="http://en.wikipedia.org/wiki?curid=48680" title="440s BC">
440s BC


</doc>
<doc id="48682" url="http://en.wikipedia.org/wiki?curid=48682" title="Kirkstall Abbey">
Kirkstall Abbey

Kirkstall Abbey is a ruined Cistercian monastery in Kirkstall north-west of Leeds city centre in West Yorkshire, England. It is set in a public park on the north bank of the River Aire. It was founded "c."1152. It was disestablished during the Dissolution of the Monasteries under the auspices of Henry VIII.
The picturesque ruins have been drawn and painted by artists such as J.M.W. Turner, Thomas Girtin and John Sell Cotman.
Kirkstall Abbey was acquired by Leeds Corporation as a gift from Colonel North and opened to the public in the late 19th century. The gatehouse became a museum.
Foundation.
Henry de Lacy (1070, Halton, – 1123), Lord of the manor of Pontefract, 2nd Lord of Bowland, promised to dedicate an abbey to the Virgin Mary should he survive a serious illness. He recovered and agreed to give the Abbot of Fountains Abbey land at Barnoldswick in the West Riding of Yorkshire (now in Lancashire) on which to found a daughter abbey. Abbot Alexander with twelve Cistercian monks from Fountains went to Barnoldswick and after demolishing the existing church attempted to build the abbey on Henry de Lacy's land. They stayed for six years but found the place inhospitable. Abbot Alexander set about finding a more suitable place for the abbey and came across a site in the heavily wooded Aire Valley occupied by hermits.
Alexander sought help from de Lacy who was sympathetic and helped acquire the land from William de Poitou. The monks moved from Barnoldswick to Kirkstall displacing the hermits, some of whom joined the abbey, the rest being paid to move. The buildings were mostly completed between 1152 when the monks arrived in Kirkstall and the end of Alexander's abbacy in 1182. Millstone Grit for building came from Bramley Fall on the opposite side of the river.
The buildings.
The English Cistercian houses, of which there are remains at Fountains, Rievaulx, Kirkstall, Tintern and Netley were mainly arranged after the same plan, with slight local variations. As an example, below is the groundplan of Kirkstall Abbey, one of the best preserved.
The church is of the Cistercian type, with a short chancel (3), and transepts (4) with three eastward chapels to each, divided by solid walls. The building is plain, the windows are unornamented, and the nave (1) has no triforium. The cloister to the south (5) occupies the whole length of the nave. On the east side stands the two-aisled chapter-house (7), between which and the south transept is a small sacristy, and on the other side two small apartments, one of which was probably the parlour (8). Beyond this is the calefactory or day-room of the monks. Above this whole range of building runs the monks' dormitory, opening by stairs into the south transept of the church.
On the south side of the cloister (5) there are the remains of the old refectory, running, as in Benedictine houses, from east to west, and the new refectory (12), which, with the increase of the inmates of the house, superseded it, stretching, as is usual in Cistercian houses, from north to south. Adjacent to this apartment are the remains of the kitchen, pantry and buttery. The arches of the lavatory are to be seen near the refectory entrance. The western side of the cloister is occupied by vaulted cellars, supporting on the upper story the dormitory of the lay brothers (9).
Extending from the south-east angle of the main group of buildings are the walls and foundations of a secondary group of buildings (17, 18). These have been identified as the hospitium or the abbot's house, but they occupy the position in which the infirmary is more usually found. The hall was a very spacious apartment, measuring 83 ft. in length by 48 ft. 9 inches in breadth, which was divided by two rows of columns. The fish-ponds lay between the monastery and the river to the south. The abbey mill was situated about 80 yards to the north-west. The millpool may be distinctly traced, together with the goit or mill stream.
Dissolution and later history.
On 22 November 1539 the abbey was surrendered to Henry VIII's commissioners in the Dissolution of the monasteries. It was awarded to Thomas Cranmer in 1542, but reverted to the crown when Cranmer was executed in 1556. Sir Robert Savile purchased the estate in 1584, and it remained in his family's hands for almost a hundred years. In 1671 it passed into the hands of the Brudenell family, the Earls of Cardigan. Much of the stone was removed for re-use in other buildings in the area, including the steps leading to Leeds Bridge. 
During the 18th century the picturesque ruins attracted artists of the Romantic movement and were painted by artists including J. M. W. Turner, John Sell Cotman and Thomas Girtin. In 1889 the abbey was sold to Colonel John North, who presented it to Leeds City Council. The Council undertook a major restoration project and the abbey was opened to the public in 1895.
The abbey today.
The abbey is a Grade I listed building and Scheduled Ancient Monument. After a £5.5 million renovation programme there is a new visitor centre with interactive exhibits which illustrates the history of the abbey and the lives of the monks. Entry to the Abbey itself is via the visitor centre – free of charge, but with a donation box. Occasionally, guided tours are available (free of charge).
The "Leeds Shakespeare Festival", performed by the British Shakespeare Company, took place annually in the cloisters from 1995 until 2009. The abbey grounds are a public park, and are used for occasional events such as the annual Kirkstall Festival and the Kirkstall Fantasia open-air concerts.
On the other side of the main road, the grade II* listed former abbey gatehouse now forms the Abbey House Museum.
The Abbey was also used on 19 March 2011 for the live BBC Three event Frankenstein's Wedding... Live in Leeds. A live music drama starring Andrew Gower and Lacey Turner as fiancees Victor Frankenstein and Elizabeth Lavenza.
On 10 and 11 September 2011 the Kaiser Chiefs played two concerts at Kirkstall Abbey to a maximum audience of 10,000 on each day.
References.
</dl>

</doc>
<doc id="48684" url="http://en.wikipedia.org/wiki?curid=48684" title="Merostomata">
Merostomata

Merostomata is the name given to a grouping of the extinct Eurypterida (sea scorpions) and the Xiphosura (horseshoe crabs). The term was originally used by James Dwight Dana to refer to Xiphosura only, but was emended by Henry Woodward to cover both groups.
Etymology.
The name "Merostomata" derives from the Greek roots μηρός ("meros", "thigh") and στόμα ("stoma", "mouth"), in reference to the animals' possession of appendages which are mouthparts at their proximal end, but swimming legs at their distal end.
History.
The scientific consensus at the beginning of the 20th century was that these two marine groups were closely related, and only more distantly related to the terrestrial Arachnida. More recent analyses suggest the grouping Merostomata is not monophyletic, with Xiphosura being basal to a clade comprising Eurypterida and Arachnida. The Xiphosura are estimated to have diverged from the Arachnida million years ago.
The shared features of the two groups traditionally grouped in the Merostomata are now thought to be retentions of primitive conditions (symplesiomorphies), thus the name Merostomata has been recommended to be abandoned.
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="48685" url="http://en.wikipedia.org/wiki?curid=48685" title="Quantum suicide and immortality">
Quantum suicide and immortality

In quantum mechanics, quantum suicide is a thought experiment, originally published independently by Hans Moravec in 1987 and Bruno Marchal in 1988, and independently developed further by Max Tegmark in 1998. It attempts to distinguish between the Copenhagen interpretation of quantum mechanics and the Everett many-worlds interpretation by means of a variation of the Schrödinger's cat thought experiment, from the cat's point of view. Quantum immortality refers to the subjective experience of surviving quantum suicide regardless of the odds.
Keith Lynch recalls that Hugh Everett took great delight in paradoxes such as the unexpected hanging. Everett did not mention quantum suicide or quantum immortality in writing, but his work was intended as a solution to the paradoxes of quantum mechanics. Lynch said "Everett firmly believed that his many-worlds theory guaranteed him immortality: His consciousness, he argued, is bound at each branching to follow whatever path does not lead to death", Tegmark explains, however, that life and death situations do not normally hinge upon a sequence of binary quantum events like those in the thought experiment.
Thought experiment.
Unlike the Schrödinger's cat thought experiment which used poison gas and a radioactive decay trigger, this version involves a life-terminating device and a device that measures the spin value of protons. Every 10 seconds, the spin value of a fresh proton is measured. Conditioned upon that quantum bit, the weapon is either deployed, killing the experimenter, or it makes an audible "click" and the experimenter survives.
The theories are distinctive from the point of view of the experimenter only; their predictions are otherwise identical.
The probability of surviving the first iteration of the experiment is 50%, under both interpretations, as given by the squared norm of the wave function. At the start of the second iteration, if the Copenhagen interpretation is true, the wave function has already collapsed, so if the experimenter is already dead, there's a 0% chance of survival. However, if the many-worlds interpretation is true, a superposition of the live experimenter "necessarily" exists, regardless of how many iterations or how improbable the outcome. Barring life after death, it is not possible for the experimenter to experience having been killed, thus the only possible experience is one of having survived every iteration. Although such an experience remains possible in the Copenhagen interpretation, it gets less and less likely to happen as the number of iterations gets bigger, whereas it always is certain in the many-worlds interpretation, no matter the number of iterations.
Max Tegmark's work.
In response to questions about "subjective immortality", Max Tegmark made some brief comments: He acknowledged the argument that "everyone will be immortal" should follow if a survivor outcome is possible for all life-threatening events. The flaw in that argument, he suggests, is that dying is rarely a binary event; it is a progressive process. The quantum suicide thought experiment attempts to isolate all possible outcomes for the duration of the thought experiment. That isolation delays decoherence in such a way that the subjective experience of the superposition is illustrated. It is only within the confines of such an abstract quantum scenario that an observer finds they defy all odds. Another possibility is that although an observer does not die, they nevertheless continue to suffer the effects of aging, bringing to mind the legend of Tithonus.
In fiction.
Authors of science fiction have used themes involving both quantum suicide and immortality. The basic idea is that a person who dies on one world may survive in another world or parallel universe.

</doc>
<doc id="48686" url="http://en.wikipedia.org/wiki?curid=48686" title="Owain Gwynedd">
Owain Gwynedd

 Owain ap Gruffudd (c. 1100 – 23 or 28 November 1170)
was King of Gwynedd, north Wales, from 1137 until his death in 1170, succeeding his father Gruffudd ap Cynan. He was called "Owain the Great" (Welsh: "Owain Fawr") and the first to be styled "Prince of Wales". He is considered to be the most successful of all the North Welsh princes prior to his grandson, Llywelyn the Great. He became known as Owain Gwynedd (Middle Welsh: "Owain Gwyned", "Owain of Gwynedd") to distinguish him from the contemporary king of southern Powys, Owain ap Gruffydd ap Maredudd, who became known as "Owain Cyfeiliog". 
Early life.
Gwynedd was a member of the House of Aberffraw, the senior branch of the dynasty of Rhodri the Great. His father, Gruffudd ap Cynan, was a strong and long-lived ruler who had made the principality of Gwynedd the most influential in Wales during the sixty-two years of his reign, using the island of Anglesey as his power base. His mother, Angharad ferch Owain, was the daughter of Owain ab Edwin of Tegeingl. Gwynedd was the first son of Gruffydd and Angharad.
Owain is thought to have been born on Anglesey about the year 1100. By about 1120 Gruffydd had grown too old to lead his forces in battle and Owain and his brothers Cadwallon and later Cadwaladr led the forces of Gwynedd against the Normans and against other Welsh princes with great success. His elder brother Cadwallon was killed in a battle against the forces of Powys in 1132, leaving Owain as his father's heir. Owain and Cadwaladr, in alliance with Gruffydd ap Rhys of Deheubarth, won a major victory over the Normans at Crug Mawr near Cardigan in 1136 and annexed Ceredigion to their father's realm.
Accession to the throne and early campaigns.
On Gruffydd's death in 1137, therefore, Owain inherited a portion of a well-established kingdom, but had to share it with Cadwaladr. In 1143 Cadwaladr was implicated in the murder of Anarawd ap Gruffydd of Deheubarth, and Owain responded by sending his son Hywel ab Owain Gwynedd to strip him of his lands in the north of Ceredigion. Though Owain was later reconciled with Cadwaladr, from 1143, Owain ruled alone over most of north Wales. In 1155 Cadwaladr was driven into exile.
Owain took advantage of the civil war in England between King Stephen and the Empress Matilda to push Gwynedd's boundaries further east than ever before. In 1146 he captured the castle of Mold and about 1150 captured Rhuddlan and encroached on the borders of Powys. The prince of Powys, Madog ap Maredudd, with assistance from Earl Ranulf of Chester, gave battle at Coleshill, but Owain was victorious.
War with King Henry II.
All went well until the accession of King Henry II of England in 1154. Henry invaded Gwynedd in 1157 with the support of Madog ap Maredudd of Powys and Owain's brother Cadwaladr. The invasion met with mixed fortunes. Henry's forces ravaged eastern Gwynedd and destroyed many churches thus enraging the local population. The two armies met at Ewloe. Owain's men ambushed the royal army in a narrow, wooded valley, routing it completely with King Henry himself narrowly avoiding capture. The fleet accompanying the invasion made a landing on Anglesey where it was defeated. Ultimately, at the end of the campaign, Owain was forced to come to terms with Henry, being obliged to surrender Rhuddlan and other conquests in the east.
Forty years after these events, the scholar, Gerald of Wales, in a rare quote from these times, wrote what Owain Gwynedd said to his troops on the eve of battle:
"My opinion, indeed, by no means agrees with yours, for we ought to rejoice at this conduct of our adversary; for, unless supported by divine assistance, we are far inferior to the English; and they, by their behaviour, have made God their enemy, who is able most powerfully to avenge both himself and us. We therefore most devoutly promise God that we will henceforth pay greater reverence than ever to churches and holy places."
Madog ap Maredudd died in 1160, enabling Owain to regain territory in the east. In 1163 he formed an alliance with Rhys ap Gruffydd of Deheubarth to challenge English rule. King Henry again invaded Gwynedd in 1165, but instead of taking the usual route along the northern coastal plain, the king's army invaded from Oswestry and took a route over the Berwyn hills. The invasion was met by an alliance of all the Welsh princes, with Owain as the undisputed leader. However, apart from a small melee at the Battle of Crogen there was little fighting, for the Welsh weather came to Owain's assistance as torrential rain forced Henry to retreat in disorder. The infuriated Henry mutilated a number of Welsh hostages, including two of Owain's sons.
Henry did not invade Gwynedd again and Owain was able to regain his eastern conquests, recapturing Rhuddlan castle in 1167 after a siege of three months.
Disputes with the church and succession.
The last years of Owain's life were spent in disputes with the Archbishop of Canterbury, Thomas Becket, over the appointment of a new Bishop of Bangor. When the see became vacant Owain had his nominee, Arthur of Bardsey, elected. The archbishop refused to accept this, so Owain had Arthur consecrated in Ireland. The dispute continued, and the see remained officially vacant until well after Owain's death. He was also put under pressure by the Archbishop and the Pope to put aside his second wife, Cristin, who was his first cousin, this relationship making the marriage invalid under church law. Despite being excommunicated for his defiance, Owain steadfastly refused to put Cristin aside. Owain died in 1169, and despite having been excommunicated was buried in Bangor Cathedral by the local clergy. The annalist writing Brut y Tywysogion recorded his death "after innumerable victories, and unconquered from his youth".
He is believed to have commissioned the propaganda text, "The Life of Gruffydd ap Cynan", an account of his father's life. Following his death, civil war broke out between his sons. Owain was married twice, first to Gwladus ferch Llywarch ap Trahaearn, by whom he had two sons, Maelgwn ab Owain Gwynedd and Iorwerth Drwyndwn, the father of Llywelyn the Great, then to Cristin, by whom he had three sons including Dafydd ab Owain Gwynedd and Rhodri ab Owain Gwynedd. He also had a number of illegitimate sons, who by Welsh law had an equal claim on the inheritance if acknowledged by their father.
Heirs and successors.
Owain had originally designated Rhun ab Owain Gwynedd as his successor. Rhun was Owain's favourite son, and his premature death in 1147 plunged his father into a deep melancholy, from which he was only roused by the news that his forces had captured Mold castle. Owain then designated Hywel ab Owain Gwynedd as his successor, but after his death Hywel was first driven to seek refuge in Ireland by Cristina's sons, Dafydd and Rhodri, then killed at the battle of Pentraeth when he returned with an Irish army. Dafydd and Rhodri split Gwynedd between them, but a generation passed before Gwynedd was restored to its former glory under Owain's grandson Llywelyn the Great.
According to legend, one of Owain's sons was Prince Madoc, who is popularly supposed to have fled across the Atlantic and colonised America.
Altogether, the prolific Owain Gwynedd is said to have had the following children from two wives and at least four mistresses:
Fiction.
Owain is a recurring character in the Brother Cadfael series of novels by Ellis Peters, often referred to, and appearing in the novels "Dead Man's Ransom" and "The Summer of the Danes". He acts shrewdly to keep Wales's borders secure, and sometimes to expand them, during the civil war between King Stephen and Matilda, and sometimes acts as an ally to Cadfael and his friend, Sheriff Hugh Beringar. Cadwaladr also appears in both these novels as a source of grief for his brother. Owain appears as a minor character in novels of Sharon Kay Penman concerning Henry II and Eleanor of Aquitaine ("When Christ and His Saints Slept" and "Time and Chance"). Her focus with respect to Owain is on the fluctuating and factious relationship between England and Wales.
He also appears in the Sarah Woodbury 'Gareth and Gwen Medieval Mystery Series' of books.

</doc>
<doc id="48690" url="http://en.wikipedia.org/wiki?curid=48690" title="Caratacus">
Caratacus

Caratacus (Brythonic "*Caratācos", Middle Welsh "Caratawc"; Welsh "Caradog"; Greek "Καράτακος"; variants Latin "Caractacus", Greek "Καρτάκης") was a first-century British chieftain of the Catuvellauni tribe, who led the British resistance to the Roman conquest.
Before the Roman invasion Caratacus is associated with the expansion of his tribe's territory. His apparent success led to Roman invasion, nominally in support of his defeated enemies. He resisted the Romans for almost a decade, mixing guerrilla warfare with set-piece battles, but was unsuccessful in the latter. After his final defeat he fled to the territory of Queen Cartimandua, who captured him and handed him over to the Romans. He was sentenced to death as a military prisoner, but made a speech before his execution that persuaded the Emperor Claudius to spare him.
The legendary Welsh character Caradog ap Bran and the legendary British king Arvirargus may be based upon Caratacus. Caratacus's speech to Claudius has been a common subject in art.
History.
Claudian Invasion.
Caratacus is named by Dio Cassius as a son of the Catuvellaunian king Cunobelinus. Based on coin distribution Caratacus appears to have been the protégé of his uncle Epaticcus, who expanded Catuvellaunian power westwards into the territory of the Atrebates. After Epaticcus died in about 35 A.D., the Atrebates, under Verica, regained some of their territory, but it appears Caratacus completed the conquest, as Dio tells us Verica was ousted, fled to Rome and appealed to the emperor Claudius for help. This was the excuse used by Claudius to launch his invasion of Britain in the summer of 43 AD. The invasion targetted Caratacus' stronghold of Camulodunon (modern Colchester), previously the seat of his father Cunobelin.
Cunobelinus had died some time before the invasion. Caratacus and his brother Togodumnus led the initial defence of the country against Aulus Plautius's four legions, thought to have been around 40,000 men, primarily using guerrilla tactics. They lost much of the south-east after being defeated in two crucial battles, the Battle of the River Medway and River Thames. Togodumnus was killed (although John Hind argues that Dio was mistaken in reporting Togodumnus' death, that he was defeated but survived, and was later appointed by the Romans as a friendly king over a number of territories, becoming the loyal king referred to by Tacitus as Cogidubnus or Togidubnus) and the Catuvellauni's territories were conquered. Their stronghold of Camulodunon was converted into the first Roman colonia in Britain, Colonia Victricensis.
Resistance to Rome.
We next hear of Caratacus in Tacitus's "Annals", leading the Silures and Ordovices of Wales against Plautius' successor as governor, Publius Ostorius Scapula. Finally, in 51, Scapula managed to defeat Caratacus in a set-piece battle somewhere in Ordovician territory (see the Battle of Caer Caradoc), capturing Caratacus' wife and daughter and receiving the surrender of his brothers. Caratacus himself escaped, and fled north to the lands of the Brigantes (modern Yorkshire) where the Brigantian queen, Cartimandua, handed him over to the Romans in chains. This was one of the factors that led to two Brigantian revolts against Cartimandua and her Roman allies, once later in the 50s and once in 69, led by Venutius, who had once been Cartimandua's husband. With the capture of Caratacus, much of southern Britain from the Humber to the Severn was pacified and garrisoned throughout the 50s.
Legends place Caratacus' last stand at either Caer Caradoc near Church Stretton or British Camp in the Malvern Hills, but the description of Tacitus makes either unlikely:
[Caratacus] resorted to the ultimate hazard, adopting a place for battle so that entry, exit, everything would be unfavourable to us and for the better to his own men, with steep mountains all around, and, wherever a gentle access was possible, he strewed rocks in front in the manner of a rampart. And in front too there flowed a stream with an unsure ford, and companies of armed men had taken up position along the defences.
Although the Severn is visible from British Camp, it is nowhere near it, so this battle must have taken place elsewhere. A number of locations have been suggested, including a site near Brampton Bryan. Bari Jones, in "Archaeology Today" in 1998, identified Blodwel Rocks at Llanymynech in Powys as representing a close fit with Tacitus' account.
Captive in Rome.
After his capture, Caratacus was sent to Rome as a war prize, presumably to be killed after a triumphal parade. Although a captive, he was allowed to speak to the Roman senate. Tacitus records a version of his speech in which he says that his stubborn resistance made Rome's glory in defeating him all the greater:
If the degree of my nobility and fortune had been matched by moderation in success, I would have come to this City as a friend rather than a captive, nor would you have disdained to receive with a treaty of peace one sprung from brilliant ancestors and commanding a great many nations. But my present lot, disfiguring as it is for me, is magnificent for you. I had horses, men, arms, and wealth: what wonder if I was unwilling to lose them? If you wish to command everyone, does it really follow that everyone should accept your slavery? If I were now being handed over as one who had surrendered immediately, neither my fortune nor your glory would have achieved brilliance. It is also true that in my case any reprisal will be followed by oblivion. On the other hand, if you preserve me safe and sound, I shall be an eternal example of your clemency.
He made such an impression that he was pardoned and allowed to live in peace in Rome. After his liberation, according to Dio Cassius, Caratacus was so impressed by the city of Rome that he said "And can you, then, who have got such possessions and so many of them, covet our poor tents?"
Caratacus' name.
Caratacus' name appears as both "Caratacus" and "Caractacus" in manuscripts of Tacitus, and as "Καράτακος" and "Καρτάκης" in manuscripts of Dio. Older reference works tend to favour the spelling "Caractacus", but modern scholars agree, based on historical linguistics and source criticism, that the original Brythonic form was "*Caratācos", pronounced ], which gives the attested names "Caradog" in Welsh, "Karadeg" in Breton and "Carthach" in Irish. It is probably cognate to Corocotta, the name of a Cantabrian warrior during the reign of Augustus.
Legend.
Medieval Welsh traditions.
Caratacus' memory may have been preserved in medieval Welsh tradition. A genealogy in the Welsh Harleian MS 3859 (ca. 1100) includes the generations "Caratauc map Cinbelin map Teuhant", corresponding, via established processes of language change, to "Caratacus, son of Cunobelinus, son of Tasciovanus", preserving the names of the three historical figures in correct relationship.
Caratacus does not appear in Geoffrey of Monmouth's "History of the Kings of Britain" (1136), although he appears to correspond to Arviragus, the younger son of Kymbelinus, who continues to resist the Roman invasion after the death of his older brother Guiderius. In Welsh versions his name is Gweirydd, son of Cynfelyn, and his brother is called Gwydyr; the name Arviragus is taken from a poem by Juvenal.
Caradog, son of Bran, who appears in medieval Welsh literature, has also been identified with Caratacus, although nothing in the medieval legend corresponds except his name. He appears in the Mabinogion as a son of Bran the Blessed, who is left in charge of Britain while his father makes war in Ireland, but is overthrown by Caswallawn (the historical Cassivellaunus, who lived a century earlier than Caratacus). The Welsh Triads agree that he was Bran's son, and name two sons, Cawrdaf and Eudaf.
Modern traditions.
Caradog only began to be identified with Caratacus after the rediscovery of the works of Tacitus, and new material appeared based on this identification. An 18th-century tradition, popularised by the Welsh antiquarian and forger Iolo Morganwg, credits Caradog, on his return from imprisonment in Rome, with the introduction of Christianity to Britain. Iolo also makes the legendary king Coel Hen a son of Caradog's son Saint Cyllin. Richard Williams Morgan claimed that a reference to Cyllin as a son of Caratacus was found in the family records of Iestyn ab Gwrgant and used this as evidence of the early entry of Christianity to Britain: "Cyllin ab Caradog, a wise and just king. In his days many of the Cymry embraced the faith in Christ through the teaching of the saints of Cor-Eurgain, and many godly men from the countries of Greece and Rome were in Cambria. He first of the Cymry gave infants names; for before, names were not given except to adults, and then from something characteristic in their bodies, minds, or manners."
Another tradition, which has remained popular among British Israelites and others, makes Caratacus already a Christian before he came to Rome, Christianity having been brought to Britain by either Joseph of Arimathea or St. Paul, and identifies a number of early Christians as his relatives.
One is Pomponia Graecina, wife of Aulus Plautius, the conqueror of Britain, who as Tacitus relates, was accused of following a "foreign superstition", which the tradition considers to be Christianity. Tacitus describes her as the "wife of the Plautius who returned from Britain with an ovation", which led John Lingard (1771–1851) to conclude, in his "History and Antiquities of the Anglo-Saxon Church", that she was British; however, this conclusion is a misinterpretation of what Tacitus wrote. An ovation was a military parade in honour of a victorious general, so the person who "returned from Britain with an ovation" is clearly Plautius, not Pomponia. This has not prevented the error being repeated and disseminated widely.
Another is Claudia Rufina, a historical British woman known to the poet Martial. Martial describes Claudia's marriage to a man named Pudens, almost certainly Aulus Pudens, an Umbrian centurion and friend of the poet who appears regularly in his "Epigrams". It has been argued since the 17th century that this pair may be the same as the Claudia and Pudens mentioned as members of the Roman Christian community in "2 Timothy" in the New Testament. Some go further, claiming that Claudia was Caratacus' daughter, and that the historical Pope Linus, who is described as the "brother of Claudia" in an early church document, was Caratacus' son. Pudens is identified with St. Pudens, and it is claimed that the basilica of Santa Pudenziana in Rome, and with which St. Pudens is associated, was once called the "Palatium Britannicum" and was the home of Caratacus and his family.
This theory was popularised in a 1961 book called "The Drama of the Lost Disciples" by George Jowett, but Jowett did not originate it. He cites renaissance historians such as Archbishop James Ussher, Caesar Baronius and John Hardyng, as well as classical writers like Caesar, Tacitus and Juvenal, although his classical cites at least are wildly inaccurate, many of his assertions are unsourced, and many of his identifications entirely speculative. He also regularly cites "St. Paul in Britain", an 1860 book by R. W. Morgan, and advocates other tenets of British Israelism, in particular that the British are descended from the lost tribes of Israel.

</doc>
<doc id="48691" url="http://en.wikipedia.org/wiki?curid=48691" title="Ernest Jones">
Ernest Jones

Alfred Ernest Jones, FRCP, MRCS (1 January 1879 – 11 February 1958) was a British neurologist and psychoanalyst, and Sigmund Freud's official biographer. Jones was the first English-speaking practitioner of psychoanalysis and became its leading exponent in the English-speaking world. As President of both the British Psycho-Analytical Society and the International Psychoanalytical Association in the 1920s and 1930s, Jones exercised a formative influence in the establishment of its organisations, institutions and publications.
Early life and career.
Alfred Ernest Jones was born in Gowerton (formerly Ffosfelin), Wales, an industrial village on the outskirts of Swansea. He was the son of a colliery engineer and his wife. Jones was educated at Swansea Grammar School, Llandovery College, and Cardiff University in Wales. Jones studied at the University College London (UCL) and meanwhile he obtained the Conjoint diplomas LRCP and MRCS in 1900. A year later, in 1901, he obtained an M.B. degree with honours in medicine and obstetrics. Within five years he received an MD degree and a Membership of the Royal College of Physicians (MRCP) in 1903. He was particularly pleased to receive the University's gold medal in obstetrics from his distinguished fellow-Welshman, Sir John Williams.
After obtaining his medical degrees, Jones specialised in neurology and took a number of posts in London hospitals. It was through his association with the surgeon Wilfred Trotter that Jones first heard of Freud's work. Having worked together as surgeons at University College Hospital, he and Trotter became close friends, with Trotter taking the role of mentor and confidant to his younger colleague. They had in common a wide-ranging interest in philosophy and literature, as well as a growing interest in Continental psychiatric literature and the new forms of clinical therapy it surveyed. By 1905 they were sharing accommodation above Harley Street consulting rooms with Jones' sister, Elizabeth installed as housekeeper. (Trotter and Elizabeth Jones) later married. Ernest Jones, appalled by the treatment of the mentally ill in institutions, began experimenting with hypnotic techniques in his clinical work.
Jones first encountered Freud's writings in 1905 in a German psychiatric journal, in which Freud recounted the Dora case-history (now famous). Jones formed “the deep impression of there being a man in Vienna who actually listened with attention to every word his patients said to him...a revolutionary difference from the attitude of previous physicians...”
Jones’ early attempts to combine his interest in Freud's ideas with his clinical work with children resulted in adverse effects on his career. In 1906 he was arrested and charged with two counts of indecent assault on two adolescent girls whom he had interviewed in his capacity as an inspector of schools for "mentally defective" children. At the court hearing Jones maintained his innocence, claiming the girls were fantasising about any inappropriate actions by him. The magistrate believed that no jury would believe the testimony of such children and Jones was acquitted. In 1908, employed as a pathologist at a London hospital, Jones accepted a colleague’s challenge to demonstrate the repressed sexual memory underlying the hysterical paralysis of a young girl’s arm. Jones duly obliged but prior to conducting the interview, he omitted to inform the girl’s consultant or arrange for a chaperone. Subsequently he faced complaints from the girl’s parents over the nature of the interview, and he was forced to resign his hospital post.
Personal life.
Jones’ first serious relationship was with Loe Kann, a wealthy Dutch émigré referred to him in 1906 after she had become addicted to morphine during treatment for a serious kidney condition. Their relationship lasted until 1913. It ended with Kann in analysis with Freud and Jones, at Freud's behest, undergoing analysis with Sándor Ferenczi.
A tentative romance with Freud's daughter Anna did not survive the disapproval of her father. Before her visit to Britain in the autumn of 1914, which Jones chaperoned, Freud advised: "She does not claim to be treated as a woman, being still far away from sexual longings and rather refusing man. There is an outspoken understanding between me and her that she should not consider marriage or the preliminaries before she gets two or three years older". (Letter of 22 July 1914 (Paskauskas 1993)).
In 1917 Jones married the Welsh composer Morfydd Llwyn Owen. She died eighteen months later following complications from surgery for appendicitis.
Following some inspired matchmaking by his Viennese colleagues, in 1919 Jones met and married Katherine Jokl, a Jewish economics graduate from Moravia. She had been at school in Vienna with Freud's daughters. In what proved to be a long and happy marriage, the couple had four children. Their son Mervyn Jones became a writer.
Psychoanalytical career.
Whilst attending a congress of neurologists in Amsterdam in 1907, Jones met Carl Jung, from whom he received a first-hand account of the work of Freud and his circle in Vienna. Confirmed in his judgement of the importance of Freud's work, Jones joined Jung in Zurich to plan the inaugural Psychoanalytical Congress. This was held in 1908 in Salzburg, where Jones met Freud for the first time. Jones travelled to Vienna for further discussions with Freud and introductions to the members of the Vienna Psychoanalytic Society. Thus began a personal and professional relationship which, to the acknowledged benefit of both, would survive the many dissensions and rivalries which marked the first decades of the psychoanalytic movement, and would last until Freud's death in 1939.
With his career prospects in Britain in serious difficulty, Jones sought refuge in Canada in 1908. He took up teaching duties in the Department of Psychiatry of the University of Toronto (from 1911, as Associate Professor of Psychiatry). In addition to building a private psychoanalytic practice, he worked as pathologist to the Toronto Asylum and Director of its psychiatric outpatient clinic. Following further meetings with Freud in 1909 at Clark University, Massachusetts, where Freud gave a series of lectures on psychoanalysis, and in the Netherlands the following year, Jones set about forging strong working relationships with the nascent American psychoanalytic movement. He gave some 20 papers or addresses to American professional societies at venues ranging from Boston, to Washington and Chicago. In 1910 he co-founded the American Psychopathological Association and the following year the American Psychoanalytic Association, serving as its first Secretary until 1913.
Jones undertook an intensive programme of writing and research, which produced the first of what were to be many significant contributions to psychoanalytic literature, notably monographs on "Hamlet" and "On the Nightmare". A number of these were published in German in the main psychoanalytic periodicals published in Vienna; these secured his status in Freud's inner circle during the period of the latter's increasing estrangement from Jung. In this context in 1912 Jones initiated, with Freud's agreement, the formation of a Committee of loyalists charged with safeguarding the theoretical and institutional legacy of the psychoanalytic movement. This development also served the more immediate purpose of isolating Jung and, with Jones in strategic control, eventually manoeuvring him out of the Presidency of the International Psychoanalytical Association, a post he had held since its inception. When Jung's resignation came in 1914, it was only the outbreak of the Great War that prevented Jones from taking his place.
Returning to London in 1913, Jones set up in practice as a psychoanalyst, founded the London Psychoanalytic Society, and continued to write and lecture on psychoanalytic theory. A collection of his papers was published as "Papers on Psychoanalysis", the first account of psychoanalytic theory and practice by a practising analyst in the English language.
By 1919, the year he founded the British Psychoanalytical Society, Jones could report proudly to Freud that psychoanalysis in Britain "stands in the forefront of medical, literary and psychological interest" (letter 27 January 1919 (Paskauskas 1993)). As President of the Society – a post he would hold until 1944 – Jones secured funding for and supervised the establishment in London of a offering subsidised fees, and an , which provided administrative, publishing and training facilities for the growing network of professional psychoanalysts.
Jones went on to serve two periods as President of the International Psychoanalytic Association from 1920 to 1924 and 1932 to 1949, where he had significant influence. In 1920 he founded the "International Journal of Psychoanalysis", serving as its editor until 1939. The following year he established the International Psychoanalytic Library, which published some 50 books under his editorship. Jones soon obtained from Freud rights to the English translation of his work. In 1924 the first two volumes of Freud's "Collected Papers " was published in translations edited by Jones and supervised by Joan Riviere, his former analysand and, at one stage, ardent suitor. After a period in analysis with Freud, Riviere worked with Jones as the translation editor of the "International Journal of Psychoanalysis." She then was part of a working group Jones set up to plan and deliver James Strachey's translations for the Standard Edition of Freud's work.
Largely through Jones’ energetic advocacy, the British Medical Association officially recognised psychoanalysis in 1929. The BBC subsequently removed him from a list of speakers declared to be dangerous to public morality. In the 1930s Jones and his colleagues made a series of radio broadcasts on psychoanalysis.
After Hitler took power in Germany, Jones helped many displaced and endangered Jewish analysts to resettle in England and other countries. Following the Anschluss of March 1938, Jones flew into Vienna at considerable personal risk to play a crucial role in negotiating and organising the emigration of Freud and his circle to London.
Jones-Freud controversy.
Jones' early published work on psychoanalysis had been devoted to expositions of the fundamentals of Freudian theory, an elaboration of its theory of symbolism, and its application to the analysis of religion, mythology, folklore and literary and artistic works. Under the influence of Melanie Klein, Jones' work took a new direction.
Klein had made an impact in Berlin in the new field of child analysis and had impressed Jones in 1925 when he attended her series of lectures to the British Society in London. At Jones' invitation she moved to London the following year; she soon acquired a number of devoted and influential followers. Her work had a dramatic effect on the British Society, polarising its members into rival factions as it became clear that her approach to child analysis was seriously at odds with that of Anna Freud, as set out in her 1927 book "An Introduction to the Technique of Child Analysis". The disagreement centred around the clinical approach to the pre-Oedipal child; Klein argued for play as an equivalent to free association in adult analyses. Anna Freud opposed any such equivalence, proposing an educative intervention with the child until an appropriate level of ego development was reached at the Oedipal stage. Klein held this to be a collusive inhibition of analytical work with the child.
Influenced by Klein, and initiating what became known as the "Jones-Freud controversy", Jones set out to explore a range of interlinked topics in the theory of early psychic development. These included the structure and genesis of the superego and the nature of the feminine castration complex. He coined the term “phallocentrism” in a critique of Freud’s account of sexual difference. He argued together with Klein and her Berlin colleague, Karen Horney, for a primary femininity, saying that penis envy arose as a defensive formation rather than arising from the fact, or "injury", of biological asymmetry. In a corresponding reformulation of the castration complex, Jones introduced the concept of "aphanisis" to refer to the fear of "the permanent extinction of the capacity (including opportunity) for sexual enjoyment".
These departures from orthodoxy were noted in Vienna and were topics that were featured in the regular Freud-Jones correspondence, the tone of which became increasingly fractious. Faced with accusations from Freud of orchestrating a campaign against him and his daughter, Jones sought to allay Freud's concerns without abandoning his new critical standpoint. Eventually, following a series of exchange lectures between the Vienna and London societies, which Jones arranged with Anna Freud, Freud and Jones resumed their usual cordial exchanges.
With the arrival in Britain of refugee German and Viennese analysts in the 1930s, including Anna Freud in 1938, the hostility between the orthodox Freudians and Kleinians in the British Society grew more intense. Jones chaired a number of "extraordinary business meetings" with the aim of defusing the conflict, and these continued into the war years. The meetings, which became known as the Controversial discussions, were established on a more regular basis from 1942. By that time, Jones had removed himself from direct participation, owing to ill health and the difficulties of war-time travel from his home in Elsted, West Sussex. He resigned from the Presidency of the British Society in 1944, the year in which, under the Presidency of Sylvia Payne, there finally emerged a tripartite compromise agreement; this allowed the Freudians, Klienians and a group of "Independents" to run their own training and accreditation programmes.
Later life and death.
After the end of the war, Jones gradually relinquished his many official posts whilst continuing his psychoanalytic practice, writings and lecturing. The major undertaking of his final years was his monumental account of Freud's life and work, published to widespread acclaim in three volumes between 1953 and 1957. In this he was ably assisted by his German-speaking wife, who translated much of Freud's early correspondence and other archive documentation made available by Anna Freud. His uncompleted autobiography, "Free Associations", was published posthumously in 1959.
Always proud of his Welsh origins, Jones became a member of the Welsh Nationalist Party, Plaid Cymru. He had a particular love of the Gower Peninsula, which he had explored extensively in his youth. Following the purchase of a holiday cottage in Llanmadoc, this area became a regular holiday retreat for the Jones family. He was instrumental in helping secure its status in 1956, as the first region of the UK to be designated an Area of Outstanding Natural Beauty.
Both of Jones' main leisure pursuits resulted in significant publications. A keen ice skater since his schooldays, Jones published an influential textbook on the subject (Jones 1931b). His passion for chess inspired a psychoanalytical study of the life of American chess genius, Paul Morphy.
Jones was made a Fellow of the Royal College of Physicians (FRCP) in 1942, Honorary President of the International Psychoanalytical Association in 1949, and was awarded an Honorary D.Sc. degree at Swansea University (Wales) in 1954.
Jones died in London on 11 February 1958, and was cremated at Golders Green Crematorium. His ashes were buried in the grave of the oldest of his four children in the churchyard of St Cadoc's Cheriton on the Gower Peninsula.
Books by Jones.
Maddox (2006) includes a comprehensive bibliography of Jones' writings.
External links.
! colspan="3" style="border-top: 5px solid lightgreen" | Professional and academic associations
 1920–1924 
 1932–1949 
 1919–1944 

</doc>
<doc id="48694" url="http://en.wikipedia.org/wiki?curid=48694" title="Richard Llewellyn">
Richard Llewellyn

Richard Dafydd Vivian Llewellyn Lloyd (8 December 1906 – 30 November 1983), known by his pen name Richard Llewellyn, was a British novelist.
Biography.
Llewellyn was born Vivian Lloyd of Welsh parents in Hendon, Middlesex, in 1906. Only after his death was it discovered that his claim that he was born in St. Davids, West Wales, was false.
In the U.S., Llewellyn won the National Book Award for favourite novel of 1940, voted by members of the American Booksellers Association.
He lived a peripatetic life, travelling widely throughout his life. Before World War II, he spent periods working in hotels, wrote a play, worked as a coal miner and produced his best-known novel. During World War II, he rose to the rank of Captain in the Welsh Guards. Following the war, he worked as a journalist, covering the Nuremberg Trials, and then as a screenwriter for MGM. Late in his life, he lived in Eilat, Israel.
Llewellyn married twice: his first wife was Nona Sonstenby, whom he married in 1952 and divorced in 1968, and his second wife was Susan Heimann, whom he married in 1974.
Richard Llewellyn died on 30 November 1983.
Themes.
Several of his novels dealt with a Welsh theme, the best-known being "How Green Was My Valley" (1939), which won international acclaim and was made into a classic Hollywood film. It immortalised the way of life of the South Wales Valleys coal mining communities, where Llewellyn spent a small amount of time with his grandfather. Three sequels followed.
Llewellyn's novels often included the recurring element of protagonists who assume new identities (as they are transplanted into foreign cultures), such as the character Edmund Trothe whose adventures extends through several spy adventure books.
Bibliography.
Edmund Trothe series

</doc>
<doc id="48697" url="http://en.wikipedia.org/wiki?curid=48697" title="Richard Hughes (writer)">
Richard Hughes (writer)

Richard Arthur Warren Hughes OBE (19 April 1900 – 28 April 1976) was a British writer of poems, short stories, novels and plays.
He was born in Weybridge, Surrey. His father was a civil servant Arthur Hughes, and his mother Louisa Grace Warren who had been brought up in Jamaica. He was educated at Charterhouse and graduated from Oriel College, Oxford in 1922.
A Charterhouse schoolmaster had sent Hughes's first published work to "The Spectator" in 1917. The article, written as a school essay, was an attack on "The Loom of Youth", by Alec Waugh, a recently published novel which caused a furore for its frank account of homosexual passions between British schoolboys in a public school. At Oxford he met Robert Graves, also an Old Carthusian, and they co-edited a poetry publication, "Oxford Poetry", in 1921. Hughes's short play "The Sisters' Tragedy" was in the West End at the Royal Court Theatre by 1922. He was the author of the world's first radio play, "Danger", commissioned from him for the BBC by Nigel Playfair and broadcast on 15 January 1924.
Hughes was employed as a journalist and travelled widely before he married, in 1932, the painter Frances Bazley. They settled for a period in Norfolk and then in 1934 at Castle House, Laugharne in south Wales. Dylan Thomas stayed with Hughes and wrote his book "Portrait of the Artist as a Young Dog" whilst living at Castle House. Hughes was instrumental in Thomas permanently relocating to the area.
He wrote only four novels, the most famous of which is "The Innocent Voyage" (1929), or "A High Wind in Jamaica", as Hughes renamed it shortly after its initial publication. Set in the 19th century, it explores the events which follow the accidental capture of a group of English children by pirates: the children are revealed as considerably more amoral than the pirates (it was in this novel that Hughes first described the cocktail Hangman's Blood). In 1938, he wrote an allegorical novel "In Hazard" based on the true story of the S.S. "Phemius" that was caught in the Category 5 1932 Cuba hurricane for 4 days during its peak intensity. He wrote volumes of children's stories, including "The Spider's Palace".
During the Second World War, Hughes had a desk job in the Admiralty. He met the architects Jane Drew and Maxwell Fry, and Jane's and Max's children stayed with the Hughes family for much of that time. After the end of the War, he spent ten years writing scripts for Ealing Studios, and published no more novels until 1961. Of the trilogy "The Human Predicament", only the first two volumes, "The Fox in the Attic" (1961) and "The Wooden Shepherdess" (1973), were complete when he died; twelve chapters, under 50 pages, of the final volume are now published. In these he follows the course of European history from the 1920s through the Second World War, including real characters and events—such as Hitler's escape following the abortive Munich putsch—as well as fictional.
Later in life he moved to Ynys in Gwynedd.
Hughes was a Fellow of the Royal Society of Literature and, in the United States, an honorary member of both the National Institute of Arts and Letters and the American Academy of Arts and Letters. He was awarded the OBE (Officer of the Order of the British Empire) in 1946.
Family.
Richard and Frances Hughes had five children: Robert Elyston-Glodrydd (born 1932), Penelope (1934), Lleky Susannah (1936), Catherine Phyllida (1940) and Owain Gardner Collingwood (1943). Catherine married the historian Colin Wells in 1960.

</doc>
<doc id="48698" url="http://en.wikipedia.org/wiki?curid=48698" title="Leptis Magna">
Leptis Magna

Leptis Magna (Arabic: لَبْدَة‎ "Labdah") also known as Lectis Magna (or Lepcis Magna as it is sometimes spelled), also called Lpqy, Neapolis, Lebida or Lebda to modern-day residents of Libya, was a prominent city of the Roman Empire. Its ruins are located in Khoms, Libya, 130 km east of Tripoli, on the coast where the Wadi Lebda meets the sea. The site is one of the most spectacular and unspoiled Roman ruins in the Mediterranean.
History as a city.
The city appears to have been founded by a group of local Berbers (and probably Phoenicians) sometime around 1000 BC, who gave it the Lybico-Berber name "Lpqy".
The town did not achieve prominence until Carthage became a major power in the Mediterranean Sea in the 4th century BC. It nominally remained part of Carthage's dominions until the end of the Third Punic War in 146 BC and then became part of the Roman Republic, although from about 111 BC onward, it was for all intents and purposes an independent city.
During the reign of Augustus, Leptis Magna was classified as a "Civitas libera et immunis", or a free community, over which the governor had an absolute minimum of control. As such Leptis retain its two suphetes at the head of its government, with the mhzm, similar to the Roman aediles, as minor magistrates. In addition there were such sacred officials as the 'addir 'ararim or praefectus sacrorum, the nēquim ēlīm, and probably a sacred college of fifteen members. These offices were still in effective operation when Leptis was made a "Municipium" with a certain degree of Roman rights and privileges at some time between 61 and 68 A.D., during the rule of Nero.
Soon Italian merchants settled in the city and started a profitable commerce with the Libyan interior. The republican Rome sent some colonists together with a small garrison in order to control the city. Since then the city started to grow and was even allowed to create its own money (coins). 
Leptis Magna remained as such until the reign of the Roman emperor Tiberius, when the city and the surrounding area were formally incorporated into the empire as part of the province of Africa. It soon became one of the leading cities of Roman Africa and a major trading post.
Leptis achieved its greatest prominence beginning in 193 AD, when a Berber native son, Lucius Septimius Severus, became emperor. He favored his hometown above all other provincial cities, and the buildings and wealth he lavished on it made Leptis Magna the third-most important city in Africa, rivaling Carthage and Alexandria. In 205 AD, he and the imperial family visited the city and received great honors.
Among the changes that Severus introduced were to create a magnificent new forum and to rebuild the docks. The natural harbour had a tendency to silt up, but the Severan changes made this worse, and the eastern wharves are extremely well preserved, since they were scarcely used.
Leptis over-extended itself at this period. During the Crisis of the 3rd Century, when trade declined precipitously, Leptis Magna's importance also fell into a decline, and by the middle of the 4th century,even before it was completely devastated by the 365 tsunami, large parts of the city had been abandoned. Ammianus Marcellinus recounts that the crisis was worsened by a corrupt Roman governor named Romanus during a major tribal raid who demanded bribes to protect the city. The ruined city could not pay these and complained to the emperor Valentinian. Romanus then bribed people at court and arranged for the Leptan envoys to be punished "for bringing false accusations". It enjoyed a minor renaissance beginning in the reign of the emperor Theodosius I.
In 439 AD, Leptis Magna and the rest of the cities of Tripolitania fell under the control of the Vandals when their king, Gaiseric, captured Carthage from the Romans and made it his capital. Unfortunately for the future of Leptis Magna, Gaiseric ordered the city's walls demolished so as to dissuade its people from rebelling against Vandal rule. The people of Leptis and the Vandals both paid a heavy price for this in 523 AD when a group of Berber raiders sacked the city.
Belisarius recaptured Leptis Magna in the name of Rome ten years later, and in 534 AD, he destroyed the kingdom of the Vandals. Leptis became a provincial capital of the Eastern Roman Empire (see Byzantine Empire) but never recovered from the destruction wreaked upon it by the Berbers. It was the site of a massacre of Berber chiefs of the Leuathae tribal confederation by the Roman authorities in 543 AD. Historian Theodore Mommsen wrote that under Byzantine rule the city was fully Christian. During the decade 565-578 AD Christian missionaries from Leptis Magna even began to move once more among the Amazigh tribes as far south as the Fezzan in the Libyan desert and converted the Garamantes. But the city's decadence - linked even to the Sahara's desertification - continued, even though new churches were built, and by the time of the Arab conquest of Tripolitania in the 650s, the city was nearly abandoned except for a Byzantine garrison force.
The progressive growth of arid land around Leptis damaged its importance and the port become full of sand. As a consequence, when Arabs arrived around 640 AD and later conquered Leptis, they found only a little garrison and a small city of less than 1,000 inhabitants. Under Arab domination Leptis disappeared: by the 10th century the city was forgotten and fully covered by sand.
History as a historical site.
Today, the site of Leptis Magna is the site of some of the most impressive ruins of the Roman period.
Part of an ancient temple was brought from Leptis Magna to the British Museum in 1816 and installed at the Fort Belvedere royal residence in England in 1826. It now lies in part of Windsor Great Park. The ruins are located between the south shore of Virginia Water and Blacknest Road close to the junction with the A30 London Road and Wentworth Drive.
When Italians conquered Italian Libya in the early 20th century, they dedicated huge efforts to the rediscovery of Leptis Magna. In the early 1930s Italian archeological research was able to show again the buried remains of nearly all the city.
2005 discoveries.
In June 2005, it was revealed that archaeologists from the University of Hamburg had been working along the coast of Libya when they uncovered a 30 ft length of five colourful mosaics created during the 1st or 2nd century. The mosaics show with exceptional clarity depictions of a warrior in combat with a deer, four young men wrestling a wild bull to the ground, and a gladiator resting in a state of fatigue and staring at his slain opponent. The mosaics decorated the walls of a cold plunge pool in a bath house within a Roman villa at Wadi Lebda in Leptis Magna. The gladiator mosaic is noted by scholars as one of the finest examples of representational mosaic art ever seen—a "masterpiece comparable in quality with the Alexander Mosaic in Pompeii." The mosaics were originally discovered in the year 2000 but were kept secret in order to avoid looting. They are currently on display in the Leptis Magna Museum.
In the 2011 Revolution.
There were reports that Leptis Magna was used as a cover for tanks and military vehicles by pro-Gaddafi forces during the 2011 Libyan civil war. When asked about the possibility of conducting an air-strike on the historic site, NATO refused to rule out the possibility of such an action saying that it had not been able to confirm the rebels' report that weapons were being hidden at the location.

</doc>
<doc id="48699" url="http://en.wikipedia.org/wiki?curid=48699" title="Jack Jones (novelist)">
Jack Jones (novelist)

Jack Jones CBE (24 November 1884 – 7 May 1970) was a Welsh miner, Trade Union official, politician, novelist and playwright.
Background.
Jack Jones was born in 1884 at Tai-Harri-Blawdd in Merthyr Tydfil, the eldest son of David Jones, a coal miner, and his wife Sarah Ann. He was educated at St David’s Elementary School, Merthyr Tydfil, Glamorgan. He married Laura Grimes Evans in 1908. They had four sons (two died young) and one daughter. 
Career.
In 1896 he joined his father to work in the mines aged 12. In 1901 at the age of 17 he joined the army and was posted to South Africa with his regiment the Militia Battalion of the Welch. However he was very unhappy there and ended up deserting. Once recaptured, he was transferred to India. When he eventually returned to Wales he went back to working in the coal mines. In 1914 Jones was summoned back to his regiment and sent to the front lines in France and later on Belgium. After suffering shrapnel wounds he was invalided home and appointed as recruiting officer for Merthyr Tydfil. He became honorary secretary of his local miners lodge. In 1923 he was appointed as the full-time secretary-representative of the miners at Blaengarw. In 1926 during the General Strike as part of his job as a miners agent, he travelled around South Wales urging miners to continue supporting the strike. In 1927 he resigned from his full-time post with the miners union. In early 1928 he was employed by Liberal Party headquarters as a speaker. He continued in this job until 1930. By 1934 he had started to earn a living as a writer. He went on two lecture tours in America and the European battlefronts during the War of 1939–45.
Politics.
In 1920 Jones became a member of the Communist Party. He attended a convention in Manchester on behalf of his local miners lodge with the purpose of establishing the party; at this meeting he was chosen to be Corresponding Secretary for the South Wales Region. He later founded a branch at Merthyr Tydfil. In 1923 he left the Communist Party and joined the Labour Party. He undertook a number of speaking engagements for the party including speaking in support of Labour leader Ramsay Macdonald at Aberavon. In 1927 he produced his first article for the press entitled "The Need for a Lib-Lab Coalition". He was becoming disenchanted with the Labour party and its support for nationalisation. He was drawn to support the Liberal Party through its new policies on coal and power. and had joined by the beginning of 1928. Liberal leader David Lloyd George who had been impressed by his rhetoric, recruited him to the Liberal headquarters speaking staff. He travelled around Britain speaking in support of the Liberal party's new industrial policies. In May 1928 he was selected by Neath Liberal Association to be their prospective parliamentary candidate. Neath was a safe Labour seat that the Liberals last won in 1918; at the last election in 1924 the Liberals had not run a candidate. He stood as Liberal candidate for Neath at the 1929 General Election and polled nearly 30%;
He did not stand for parliament again. He was retained by Liberal headquarters as a speaker for another 12 months after the election. During the 1930s he made another political change and was a speaker for Oswald Mosley's New Party. In the 1945 election he supported 'National' candidate Sir James Grigg.
Writing career.
During his 20s Jack Jones began to educate himself and develop his love of the theatre and writing, often taking part in local dramatic productions. In 1926 he successfully entered a short play he had written entitled "Dad's Double" into a competition in Manchester.
He began writing seriously during a period of unemployment. His first novel, "Saran", was never published, but a reduced version of it appeared as "Black Parade" (1935). By 1939, he had written the novels "Rhondda Roundabout" (1934), and "Bidden to the Feast" (1938), a play, "Land of my Fathers" (1937), and his first autobiography, "Unfinished Journey" (1937). The London stage version of Rhondda Roundabout was acclaimed. He wrote the dialogue for the film "The Proud Valley", in which he also had a minor acting role. During the Second World War he was a speaker for the Ministry of Information and the National Savings Movement. He wrote "The Man David" (1944), a life of David Lloyd George. 
After the war he wrote two volumes of autobiography, "Me and Mine" (1946) and "Give Me Back My Heart" (1950), three novels, "Off to Philadelphia in the Morning" (1947), "Some Trust in Chariots" (1948), and "River Out of Eden" (1951), and a play "Transatlantic Episode" (1947). His later works, "Lily of the Valley" and "Lucky Year" (1952), "Time and the Business" (1953), "Choral Symphony" (1955) and "Come, Night; End, Day" (1956) were less well received. In 1954, he married his second wife, Gladys Morgan. He was elected first President of the English section of Yr Academi Gymreig. Until his death in May 1970 he continued writing; these works remained unpublished, including a biographical novel, "A Burnt Offering", based on the life of Dr William Price (1800–1893), Llantrisant, pioneer of cremation.
Honors.
In 1948 he was made a Commander of the Order of the British Empire for his services to the community and to literature. In February 1970 he won an award from the Arts Council of Wales for `his distinguished contribution to the literature of Wales'. Jack Jones died on the 7th of May 1970.

</doc>
<doc id="48701" url="http://en.wikipedia.org/wiki?curid=48701" title="Thomas Cromwell">
Thomas Cromwell

 
Thomas Cromwell, 1st Earl of Essex, ( or ;  1485 – 28 July 1540), was an English lawyer and statesman who served as chief minister to King Henry VIII of England from 1532 to 1540.
Cromwell was one of the strongest and most powerful advocates of the English Reformation. He helped to engineer an annulment of the king's marriage to Queen Catherine of Aragon, to allow Henry to marry his mistress Anne Boleyn. After failing in 1534, to obtain the Pope's approval of the request for annulment, Parliament endorsed the King's claim to be head of a breakaway Church of England, thus giving Henry the authority to annul his own marriage. Cromwell subsequently plotted an evangelical, reformist course for the embryonic Church of England from the unique posts of vicegerent in spirituals and vicar-general.
During his rise to power, Cromwell made many enemies, including his former ally Anne Boleyn; he played a prominent role in her downfall. He later fell from power after arranging the King's marriage to a German princess, Anne of Cleves. Cromwell hoped that the marriage would breathe fresh life into the Reformation in England, but it turned into a disaster for Cromwell and ended in an annulment six months later. Cromwell was arraigned under a bill of attainder and executed for treason and heresy on Tower Hill on 28 July 1540. The King later expressed regret at the loss of his chief minister.
Until the 1950s, historians had downplayed Cromwell's role, calling him a doctrinaire hack who was little more than the agent of the despotic King Henry VIII. Geoffrey Elton in "The Tudor Revolution" (1953), however, featured him as the central figure in the Tudor revolution in government. Elton portrayed Cromwell as the presiding genius, much more so than the King, handling the break with Rome, and the laws and administrative procedures that made the English Reformation so important. Elton says that he was responsible for translating Royal supremacy into Parliamentary terms, creating powerful new organs of government to take charge of Church lands and largely removing the medieval features of central government. Subsequent historians have agreed with Cromwell's importance, although downplaying the "revolution" that Elton claimed.
Leithead (2004) says of Cromwell:
Early life.
Thomas Cromwell was born around 1485, in Putney, London, as the son of Walter Cromwell, a blacksmith, fuller and cloth merchant, and owner of both a hostelry and a brewery. Thomas's mother, Katherine, was the aunt of Nicholas Glossop of Wirksworth in Derbyshire. She lived in Putney in the house of a local attorney, John Welbeck, at the time of her marriage to Walter Cromwell in 1474. Cromwell had two sisters: the elder, Katherine, married Morgan Williams, a Welsh lawyer; the younger, Elizabeth, married a farmer, William Wellyfed. Katherine and Morgan's son Richard was employed in his uncle's service and changed his name to Cromwell. Richard's great-grandson was Oliver Cromwell, the Lord Protector.
Little is known about Thomas Cromwell's early life. It is believed that he was born at the top of Putney Hill, on the edge of Putney Heath. In 1878, his birthplace was still of note:
The site of Cromwell's birthplace is still pointed out by tradition and is in some measure confirmed by the survey of Wimbledon Manor, quoted above, for it describes on that spot 'an ancient cottage called the smith's shop, lying west of the highway from Richmond to Wandsworth, being the sign of the Anchor.' The plot of ground here referred to is now covered by the Green Man public house.Putney Heath was a noted haunt of highwaymen and only a few brave souls ventured across it at night.
A successful merchant and lawyer, Thomas Cromwell was a self-made man of relatively humble beginnings whose intelligence and abilities enabled him to rise to become the most powerful man in England next to the king. His own father, Walter Cromwell, had been a jack of all trades—a blacksmith, fuller, and brewer—who had, from time to time, come to the attention of the authorities. Thomas Cromwell was sent to school as a boy, where he learned to read and write and was taught a little Latin.
Cromwell declared to Archbishop of Canterbury Thomas Cranmer that he had been a "ruffian … in his young days". As a youth, he left his family in Putney and crossed the Channel to the continent. Accounts of his activities in France, Italy and the Low Countries are sketchy and contradictory. It is alleged that he first became a mercenary and marched with the French army to Italy, where he fought in the battle of Garigliano on 28 December 1503. While in Italy, he entered service in the household of the Florentine banker Francesco Frescobaldi.
Later, he visited leading mercantile centres in the Low Countries, living among the English merchants and developing a network of contacts while learning several languages. At some point he returned to Italy. The records of the English Hospital in Rome indicate that he stayed there in June 1514, while documents in the Vatican Archives suggest that he was an agent for the Archbishop of York, Cardinal Christopher Bainbridge, and handled English ecclesiastical issues before the Roman Rota.
Marriage and issue.
At some time during these years, Cromwell returned to England, where around 1515 he married Elizabeth Wyckes (1489–1528). She was the widow of Thomas Williams, a Yeoman of the Guard, and the daughter of a Putney shearman, Henry Wykes, who had served as a Gentleman Usher to King Henry VII.
The couple had three children:
Thomas Cromwell also had an illegitimate daughter, Jane (c. 1520/25 – c. 1580), who married William Hough (c.1525–1585), of Leighton in Wirral, Cheshire, sometime between 1535 and 1539.
Cromwell's wife is believed to have died during the epidemic of sweating sickness sweeping across England in 1527–1528, most likely in the summer of 1528. The last reference to his wife was in a letter from Richard Cave, a man who knew him very well, on 18 June 1528. Cromwell's daughters, Anne and Grace, are believed to have died not long after their mother. Provisions made for Anne and Grace in Thomas Cromwell's will, written on 12 July 1529, have been crossed out at a later date.
Jane's early life is a complete mystery. According to Hilary Mantel, "Cromwell had an illegitimate daughter, and beyond the fact that she existed, we know very little about her. She comes briefly into the records, in an incredibly obscure way—she’s in the archives of the county of Chester." Jane married William Hough (c.1525–1585), of Leighton in Wirral, Cheshire, sometime between 1535 and 1539. William Hough was the son of Richard Hough (1508–1573/4) who worked for Thomas Cromwell from 1534 to 1540. Hough was Cromwell's agent in Chester. It is unknown what role Thomas and Gregory Cromwell played in her life. However, it would be safe to say that, after her marriage, Jane did not share her father's religious beliefs. Jane and her husband William Hough were staunch Catholics who, together with their daughter, Alice, her husband, William Whitmore and their children, all came to the attention of the authorities as recusant Catholics in the reign of Elizabeth I.
Early career.
In 1517, and again in 1518, Cromwell led an embassy to Rome to obtain from Pope Leo X a Papal Bull of Indulgence for the town of Boston in Lincolnshire.
By 1520, Cromwell was firmly established in London mercantile and legal circles. In 1523, he obtained a seat in the House of Commons, though the constituency he represented at that time has not been identified. After Parliament had been dissolved, Cromwell wrote a letter to a friend, jesting about the session's lack of productivity:
I amongst other have indured a parlyament which contenwid by the space of xvii hole wekes wher we communyd of warre pease Stryffe contencyon debatte murmure grudge Riches poverte penurye trowth falshode Justyce equyte dicayte [deceit] opprescyon Magnanymyte actyvyte foce [force] attempraunce [moderation] Treason murder Felonye consyli ... [conciliation] and also how a commune welth myght be ediffyed and a[lso] contenewid within our Realme. Howbeyt in conclusyon we have d[one] as our predecessors have been wont to doo that ys to say, as well we myght and lefte wher we begann.
In 1524, he was elected as a member of Gray's Inn.
From around 1516 to 1530, Cromwell was a member of the household of Cardinal Thomas Wolsey, one of his council by 1519 and his secretary by 1529. In the mid-1520s, Cromwell assisted in the dissolution of nearly thirty (30) monasteries to raise funds for Wolsey to found The King's School, Ipswich (1528), and Cardinal College, in Oxford (1529). In 1526, Wolsey appointed Cromwell a member of his council; by 1529, Cromwell was one of Wolsey's most senior and trusted advisers. But, by the end of October of that year, Wolsey had fallen from power. Cromwell had made enemies by aiding Wolsey to suppress the monasteries, but was determined not to fall out with his master, as he told George Cavendish, then a Gentleman Usher and later Wolsey's biographer:
I do entend (god wyllyng) this after none, whan my lord hathe dyned to ride to london and so to the Court, where I wyll other make or marre, or ere [before] I come agayn, I wyll put my self in the prese [press] to se what any man is Able to lay to my charge of ontrouthe or mysdemeanor.
Privy Councillor.
Cromwell's efforts to overcome the shadow cast over his career by Wolsey's downfall were successful. By November 1529, he had secured a seat in Parliament as a member for Taunton and was reported to be in favour with the King. At some point, during the closing weeks of 1530, the King appointed him to the Privy Council.
During his career in the King's service, Cromwell held numerous offices, which included:
as well as numerous minor offices.
Anne Boleyn.
From 1527, Henry VIII had sought to have his marriage to Queen Katherine annulled so that he could marry Anne Boleyn. At the centre of the campaign to secure the divorce was the emerging doctrine of royal supremacy over the church.
By the autumn of 1531, Cromwell had taken control of the supervision of the King's legal and parliamentary affairs, working closely with Thomas Audley, and had joined the inner circle of the Council. By the following spring, he had begun to exert influence over elections to the House of Commons. He was a modest man, not fond of flattery.
The third session of what is now known as the Reformation Parliament had been scheduled for October 1531, but was postponed until 15 January 1532 because of government indecision as to the best way to proceed. Cromwell now favoured the assertion of royal supremacy and manipulated the Commons by resurrecting anti-clerical grievances expressed earlier in the session of 1529. On 18 March 1532, the Commons delivered a supplication to the King denouncing clerical abuses and the power of the ecclesiastical courts and describing Henry as "the only head, sovereign lord, protector and defender" of the Church. The clergy resisted at first, but capitulated when faced with the threat of Parliamentary reprisal. On 14 May 1532, Parliament was prorogued. Two days later, Sir Thomas More resigned as Lord Chancellor, realising that the battle to save the marriage was lost. More's resignation from the Council represented a triumph for Cromwell and the pro-Reformation faction at court.
The King's gratitude to Cromwell was expressed in a grant of the lordship of Romney in Newport in Wales and appointment to three relatively minor offices: Master of the Jewels on 14 April 1532, Clerk of the Hanaper on 16 July, and Chancellor of the Exchequer on 12 April 1533. None of these offices afforded much income, but the appointments were an indication of royal favour and gave Cromwell a position in three major institutions of government: the royal household, the Chancery and the Exchequer.
By January 1533, Anne Boleyn was pregnant and marriage could no longer be delayed. The date of the wedding is unclear. It may have taken place when Anne was with the King in Calais in November 1532, but it seems more likely that it took place at a secret ceremony on 25 January 1533. Parliament was immediately recalled to pass the necessary legislation. On 26 January 1533, Audley was appointed Lord Chancellor, and Cromwell increased his control over the Commons through his management of by-elections.
The parliamentary session began on 4 February and Cromwell introduced a new bill restricting the right to make appeals to Rome. On 30 March, Cranmer was consecrated Archbishop of Canterbury and Convocation immediately declared the King's marriage to Katherine unlawful. In the first week of April 1533, Parliament passed the Bill into law as the Act in Restraint of Appeals, ensuring that any verdict concerning the King's marriage could not be challenged in Rome. On 11 April, Archbishop Cranmer sent the King a pro forma challenge to the validity of his marriage to Queen Katherine. A formal trial began on 10 May 1533 in Dunstable and on 23 May the Archbishop pronounced sentence, declaring the marriage illegal. Five days later he pronounced the King's marriage to Anne to be lawful, and on 1 June, she was crowned queen.
In December, the King authorised Cromwell to discredit the papacy and the Pope was attacked throughout the nation in sermons and pamphlets. In 1534, a new Parliament was summoned, again under Cromwell's supervision, to enact the legislation necessary to make a formal break of England's remaining ties with Rome. Archbishop Cranmer's sentence took statutory form as the Act of Succession, the Dispensations Act reiterated royal supremacy and the Act for the Submission of the Clergy incorporated into law the clergy's surrender in 1532. On 30 March 1534, Audley gave royal assent to the legislation in the presence of the King.
King's chief minister.
In April 1534, Henry confirmed Cromwell as his principal secretary and chief minister, a position he had held in all but name for some time. Cromwell immediately took steps to enforce the legislation just passed by Parliament. Before the members of both houses returned home on 30 March, they were required to swear an oath accepting the Act of Succession and all the King's subjects were now required to swear to the legitimacy of the marriage and, by implication, to acceptance of the King's new powers and the break from Rome. On 13 April, the London clergy accepted the oath. On the same day, the commissioners offered it to Sir Thomas More and John Fisher, Bishop of Rochester, both of whom refused it. More was taken into custody on the same day and was moved to the Tower of London on 17 April. Fisher joined him there four days later. On 18 April, an order was issued that all citizens of London were to swear. Similar orders were issued throughout the country. When Parliament reconvened in November, Cromwell brought in the most significant revision of the treason laws since 1352, making it treasonous to speak rebellious words against the Royal Family, to deny their titles or to call the King a heretic, tyrant, infidel or usurper. The Act of Supremacy also clarified the King's position as head of the church and the Act for Payment of First Fruits and Tenths substantially increased clerical taxes. Cromwell also strengthened his own control over the Church. On 21 January 1535, the King appointed him Royal Vicegerent and Vicar-General, and commissioned him to organise visitations of all the country's churches, monasteries and clergy. In this capacity, Cromwell conducted a census in 1535 to enable the government to tax church property more effectively.
Fall of Anne Boleyn.
The final session of the Reformation Parliament began on 4 February 1536. By 18 March, an Act for the Suppression of the Lesser Monasteries, those with a gross income of less than £200 per annum, had passed both houses. This caused a clash with Anne Boleyn, formerly one of Cromwell's strongest allies, who wanted the proceeds of the dissolution used for educational and charitable purposes, not paid into the King's coffers.
Anne instructed her chaplains to preach against the Vicegerent and in a blistering sermon on Passion Sunday, 2 April 1536, her almoner, John Skip, denounced Cromwell and his fellow Privy Councillors before the entire court. Skip's diatribe was intended to persuade courtiers and Privy Councillors to change the advice they had been giving the King and to reject the temptation of personal gain. Skip was called before the Council and accused of malice, slander, presumption, lack of charity, sedition, treason, disobedience to the gospel, attacking 'the great posts, pillars and columns sustaining and holding up the commonwealth' and inviting anarchy.
Anne, who had many enemies at court, had never been popular with the people and had so far failed to produce a male heir. The King was growing impatient, having become enamoured of the young Jane Seymour and, encouraged by Anne's enemies, particularly Nicholas Carew and the Seymours. In circumstances that have divided historians, Anne was accused of adultery with Mark Smeaton, a musician of the royal household, Henry Norris, the King's groom of the stool and one of his closest friends, Sir Francis Weston, William Brereton and her brother, Viscount Rochford. The Imperial Ambassador, Eustace Chapuys, wrote to Charles V that: he himself [Cromwell] has been authorised and commissioned by the king to prosecute and bring to an end the mistress's trial, to do which he had taken considerable trouble ... He set himself to devise and conspire the said affair. Regardless of the role Cromwell played in Anne Boleyn's fall, it is clear from Chapuys's letter that he was acting with the King's authority.
The Queen and her brother stood trial on Monday 15 May, while the four others accused with them were condemned on the Friday beforehand. The men were executed on 17 May and, on the same day, Cranmer declared Henry's marriage to Anne invalid, a ruling that bastardised their daughter, Princess Elizabeth. Two days later, Anne herself was executed. On 30 May, the King married Jane Seymour. On 8 June, a new Parliament passed the second Act of Succession, securing the rights of Queen Jane's heirs to the throne.
Baron Cromwell and Lord Privy Seal.
Cromwell's position was now stronger than ever. He succeeded Anne Boleyn's father, Thomas Boleyn, 1st Earl of Wiltshire, as Lord Privy Seal on 2 July 1536, resigning the office of Master of the Rolls, which he had held since 8 October 1534. On 8 July 1536, he was raised to the peerage as Baron Cromwell of Wimbledon.
Religious reform.
In July 1536, the first attempt was made to clarify religious doctrine after the break with Rome. Bishop Edward Foxe, with strong backing from Cromwell and Cranmer, tabled proposals in Convocation, which the King later endorsed as the Ten Articles and which were printed in August 1536. Cromwell circulated injunctions for their enforcement that went beyond the Articles themselves, provoking opposition in September and October in Lincolnshire and then throughout the six northern counties. These widespread popular and clerical uprisings, which found support among the gentry and even the nobility, were collectively known as the Pilgrimage of Grace. Although the grievances of the rebels were wide-ranging, the most significant was the suppression of the monasteries, blamed on the King's "evil counsellors", principally Cromwell and Cranmer. One of the leaders of the rebellion, Thomas Darcy, 1st Baron Darcy of Darcy, before his execution gave Cromwell the prophetic warning "others that have been in such favour with kings as you now enjoy have come to the same fate you bring me to".
The suppression of the risings spurred further Reformation measures. In February 1537, Cromwell convened a vicegerential synod of bishops and doctors. By July, the synod, co-ordinated by Cranmer and Foxe, had prepared a draft document, "The Institution of a Christian Man", more commonly known as the Bishops' Book. By October, it was in circulation, although the King had not yet given it his full assent. However Cromwell's success in Church politics was offset by the fact that his political influence had been weakened by the emergence of a Privy Council, a body of nobles and office-holders that first came together to suppress the Pilgrimage of Grace. The King confirmed his support of Cromwell by appointing him to the Order of the Garter on 5 August 1537, but Cromwell was nonetheless forced to accept the existence of an executive body dominated by his conservative opponents.
In January 1538, Cromwell pursued an extensive campaign against what was termed "idolatry" by the followers of the new religion. Statues, rood screens and images were attacked, culminating in September with the dismantling of the shrine of St Thomas Becket at Canterbury. Early in September, Cromwell also completed a new set of vicegerential injunctions declaring open war on "pilgrimages, feigned relics or images, or any such superstitions" and commanding that "one book of the whole Bible in English" be set up in every church. Moreover, following the "voluntary" surrender of the remaining smaller monasteries during the previous year, the larger monasteries were now also "invited" to surrender throughout 1538, a process legitimised in the 1539 session of Parliament and completed in the following year.
Resistance to further religious reform.
The King was becoming increasingly unhappy about the extent of religious changes and the conservative faction at court was gaining strength. Cromwell took the initiative against his enemies. In November 1538, using evidence acquired from Sir Geoffrey Pole under interrogation in the Tower, he imprisoned the Marquess of Exeter, Sir Edward Neville, and Sir Nicholas Carew on charges of treason; all were executed in the following months.
On 17 December 1538, the Inquisitor-General of France forbade the printing of Miles Coverdale's Great Bible. Cromwell persuaded the King of France to release the unfinished books so that printing could continue in England. In April 1539, the first edition was finally available. The publication of the Great Bible, the first authoritative version in English, was one of Cromwell's principal achievements.
The King, however, continued to resist further Reformation measures. A Parliamentary committee was established to examine doctrine and on 16 May 1539 the Duke of Norfolk presented six questions for the House to consider, which were duly passed as the Act of Six Articles shortly before the session ended on 28 June. The Six Articles reaffirmed a traditional view of the Mass, the Sacraments and the priesthood.
Anne of Cleves.
Queen Jane had died in 1537, less than two weeks after the birth of her only child, the future Edward VI. In early October 1539, the King finally accepted Cromwell's suggestion that he should marry Anne, the sister of Duke Wilhelm, of Cleves. On 27 December, Anne arrived at Dover. On New Year's Day 1540, the King met her at Rochester. The wedding ceremony took place on 6 January at Greenwich, but the marriage was not consummated.
Earl of Essex.
On 18 April 1540, Henry granted Cromwell the earldom of Essex and the senior Court office of Lord Great Chamberlain. Despite these signs of royal favour, Cromwell's tenure as the King's chief minister was almost over. The King's anger at being forced to marry Anne of Cleves was the opportunity Cromwell's conservative opponents, most notably the Duke of Norfolk, needed to topple him.
Downfall and execution.
During 1536 Cromwell had proven himself an adept political survivor. However, the gradual slide towards Protestantism at home and the King's ill-starred marriage to Anne of Cleves, which Cromwell engineered in January 1540, proved costly. The Franco-Imperial alliance had failed to materialise and Henry had therefore been subjected to an unnecessary conjugal difficulty which loosened his Principal Secretary's control of events. In early 1540, Cromwell's conservative, aristocratic enemies, headed by the Duke of Norfolk and assisted by Bishop Gardiner (colloquially known as 'Wily Winchester'), saw an opportunity to displace their foe, in the form of Catherine Howard.
Cromwell was arrested at a Council meeting on 10 June 1540 and imprisoned in the Tower. A Bill of Attainder containing a long list of indictments, including supporting Anabaptists, protecting Protestants accused of heresy and thus failing to enforce the Act of Six Articles, and plotting to marry Lady Mary Tudor, was introduced into the House of Lords a week later and passed on 29 June 1540. He was also connected with 'sacramentarians' (those who denied transubstantiation) in Calais. All Cromwell's honours were forfeited and it was publicly proclaimed that he could only be called "Thomas Cromwell, cloth carder". The King deferred the execution until his marriage to Anne of Cleves could be annulled. Hoping for clemency, Cromwell wrote in support of the annulment, in his last personal address to the King. He ended it with the plea "Most gracious Prince, I cry for mercy, mercy, mercy."
Cromwell was condemned to death without trial and beheaded on Tower Hill on 28 July 1540, the day of the King's marriage to Catherine Howard. After the killing, his head was set on a spike on London Bridge. Edward Hall, a contemporary chronicler, records that Cromwell made a speech on the scaffold, professing to die, "in the traditional faith" and then "so patiently suffered the stroke of the axe, by a ragged and Boocherly miser, whiche very ungoodly perfourmed the Office".
Hall said of Cromwell's downfall:
Many lamented but more rejoiced, and specially such as either had been religious men, or favoured religious persons; for they banqueted and triumphed together that night, many wishing that that day had been seven years before; and some fearing lest he should escape, although he were imprisoned, could not be merry. Others who knew nothing but truth by him both lamented him and heartily prayed for him. But this is true that of certain of the clergy he was detestably hated, & specially of such as had borne swynge [beaten hard], and by his means was put from it; for in deed he was a man that in all his doings seemed not to favour any kind of Popery, nor could not abide the snoffyng pride of some prelates, which undoubtedly, whatsoever else was the cause of his death, did shorten his life and procured the end that he was brought unto.
Henry came to regret Cromwell's killing and later accused his ministers of bringing about Cromwell's downfall by false charges. On 3 March 1541, the French Ambassador, Charles de Marillac, reported in a letter that the King was now said to be lamenting that: under pretext of some slight offences which he had committed, they had brought several accusations against him, on the strength of which he had put to death the most faithful servant he ever had.
There remains an element of what G.R. Elton describes as 'mystery' about Cromwell's demise. In April 1540, just three months before he went to the block, he was created Earl of Essex and Lord Great Chamberlain. The arbitrary and unpredictable streak in the King's personality, which more than once exercised influence during his reign, surfaced again and washed Cromwell away in its wake.
His effectiveness and creativity as a royal minister cannot be denied; neither can his loyalty to the King. During Cromwell's years in power, he skilfully managed Crown finances and extended royal authority. In 1536, he established the Court of Augmentations to handle the massive windfall to the royal coffers from the Dissolution of the Monasteries. Two other important financial institutions, the Court of Wards and the Court of First Fruits and Tenths, owed their existence to him, although they were not set up until after his death. He strengthened royal authority in the north of England, through reform of the Council of the North, extended royal power and introduced Protestantism in Ireland, and was the architect of the Laws in Wales Acts 1535 and 1542, which promoted stability and gained acceptance for the royal supremacy in Wales. He also introduced important social and economic reforms in England in the 1530s, including action against enclosures, the promotion of English cloth exports, and the poor relief legislation of 1536.
Personal religious beliefs.
Although Cromwell always maintained a primarily political outlook on general affairs, there is consensus among scholars that he was a committed Protestant. For him, the Henrician Reformation was certainly more than a jurisdictional revolution masquerading in religious garb. For instance, in the mid-1530s, he promoted Protestant ideas to forge an alliance with German Lutheran states, but his overall support for the Protestant cause is too general to be accurately explained in narrow political terms.
In 1535, Cromwell succeeded in having clearly identified reformers, such as Hugh Latimer, Edward Foxe and Nicholas Shaxton, appointed to the episcopacy. Also, he encouraged and supported the work of reformers, such as Robert Barnes. And it was Cromwell who provided the significant funding for the publication of the English translation of the Bible, known as the Matthew Bible.
Indeed, when Cromwell fell in 1540, his support for Anabaptism was cited by his accusers as a main charge against him. Although the charge was actually completely spurious, the fact that it was levelled at all demonstrates the reputation for evangelical sympathies Cromwell had developed.
Descendants.
Thomas Cromwell's son Gregory Cromwell, 1st Baron Cromwell, married Elizabeth Seymour, the sister of Queen Jane Seymour and widow of Sir Anthony Ughtred. They had five children:
Cromwell's illegitimate daughter Jane married William Hough, of Leighton in Wirrall, Cheshire, and had a daughter, Alice. Jane and her husband were staunch Catholics who, with their daughter, Alice, her husband, William Whitmore and their children, came to the attention of the authorities as recusant Catholics during the reign of Elizabeth I.
The Puritan leader Oliver Cromwell descended from Katherine Cromwell (m. Williams), Thomas Cromwell's elder sister.
Hans Holbein portraits.
Thomas Cromwell was a patron of Hans Holbein the Younger, as were Sir Thomas More and Anne Boleyn. In the New York Frick Collection, two portraits by Holbein hang facing each other on the same wall of the Living Hall, one depicting Thomas Cromwell, the other Thomas More, whose execution he had procured.
Fictional portrayals.
Cromwell has been portrayed in a number of plays, feature films, and television miniseries, usually as a villainous character. More recently, however, Hilary Mantel's two Man Booker Prize-winning novels "Wolf Hall" (2009) and "Bring up the Bodies" (2012) have sought to show him in a more sympathetic light, stressing his family affections, genuine respect for Cardinal Wolsey, zeal for the Reformation and support for a limited degree of social reform.

</doc>
<doc id="48706" url="http://en.wikipedia.org/wiki?curid=48706" title="Rancid (band)">
Rancid (band)

Rancid is an American punk rock band formed in Berkeley, California, in 1991. Founded by 1980s punk veterans Tim Armstrong and Matt Freeman, who previously played in the highly influential ska punk band Operation Ivy, Rancid is often credited—along with Green Day and The Offspring—for reviving mainstream interest in punk rock in the United States during the mid-1990s. Unlike many of their contemporaries, however, Rancid remained signed to an independent record label and retained much of its original fan-base, most of which was connected to its underground roots.
Rancid includes Tim Armstrong on guitar and vocals, Freeman on bass and vocals, Lars Frederiksen on guitar and vocals, and Branden Steineckert on drums. The band was formed by Armstrong, Freeman, and former drummer Brett Reed, who left the band in 2006 and was replaced by Steineckert. Frederiksen joined Rancid in 1993 when the band was searching for a second guitar player.
To date, Rancid has released eight studio albums, one split album, one compilation, two extended plays, and a series of live online-only albums, and has been featured on a number of compilation albums. The band has independently sold over four million records worldwide, making it one of the most successful independent punk groups of all time. The band rose to fame in 1994 with its second studio album, "Let's Go", featuring the single "Salvation". In the following year, Rancid released its highly successful album "...And Out Come the Wolves", which produced its best-known songs "Roots Radicals", "Ruby Soho" and "Time Bomb", and was certified gold and platinum by the RIAA, selling over one million copies in the United States alone. Its next four albums — "Life Won't Wait" (1998), "Rancid" (2000), "Indestructible" (2003) and "Let the Dominoes Fall" (2009) — were also critically acclaimed, though not as successful as "...And Out Come the Wolves". Rancid released "...Honor Is All We Know", their first studio album in five years, on October 27, 2014.
History.
Early history (pre-1993).
Childhood friends Tim Armstrong and Matt Freeman grew up together in the small, working-class town of Albany, California, near Berkeley. The two had been playing together in the influential ska punk band Operation Ivy from 1987 to 1989. The band became popular in the punk scene at 924 Gilman Street, a club and concert venue featuring Bay Area punk bands. When Operation Ivy broke up, Armstrong and Freeman decided to form a new band, and formed a ska punk band called Downfall, which disbanded after a few months. They then started a hardcore punk band called Generator, which also disbanded shortly after. They also started the ska influenced Dance Hall Crashers, though left the band shortly after it was formed. During this time, Armstrong was struggling with alcoholism, and to keep him focused on other interests, Freeman suggested they form a new band. In 1991, they recruited Armstrong's roommate Brett Reed as their drummer and formed Rancid.
A few months after the band's inception, Rancid began performing around the Berkeley area, and quickly developed a fan following. Rancid's first recorded release was a 1992 EP for Operation Ivy's old label Lookout! Records. Shortly after releasing the extended play, the band left Lookout! and was signed to Bad Religion guitarist Brett Gurewitz's record label, Epitaph Records. Rancid released its self-titled debut album through Epitaph in 1993.
Breakthrough success (1994–1996).
While Rancid was writing for a follow-up album, Billie Joe Armstrong joined them to co-write the song "Radio", which resulted in Armstrong playing a live performance with Rancid. Tim had previously asked Lars Frederiksen to be Rancid's second guitarist, but he turned down the request initially as he was playing with the UK Subs at the time. After Billie Joe turned down the request, Frederiksen changed his mind and joined Rancid.
Frederiksen played with the band on its second studio album "Let's Go" (1994). That year, its then-label-mates, The Offspring, experienced huge success with its album "Smash". Rancid supported The Offspring's 1994 tour, which helped "Let's Go" reach number 97 on "Billboard"'s Heatseekers and the "Billboard" 200 charts, respectively. The album also provided its first widespread exposure when MTV broadcast the video for the single "Salvation." "Let's Go" was certified gold on July 7, 2000, and with the success of the album the band was pursued by a number of major record labels, including Madonna's label Maverick Records. Many rumors circulated during this time period. Some of the rumors were Epitaph employees were not allowed to discuss matters with the press, Rancid convinced an A&R man from Epic to shave a blue mohawk, and Madonna sent the band nude pictures of herself.
The band eventually decided to remain signed to Epitaph, and the next year released its third album "...And Out Come the Wolves" on August 22, 1995. The album quickly surpassed "Let's Go" in terms of success, and reached number 45 on the "Billboard" 200 album chart. on January 22, 1996, the album was certified gold. The album received positive reviews, Stephen Thomas Erlewine of AllMusic described the album as having "classic moments of revivalist punk". Erlewine praised the music and claims the album "doesn't mark an isolationist retreat into didactic, defiantly underground punk rock". Three of the album's singles, "Roots Radicals", "Time Bomb", and "Ruby Soho" all charted on the "Billboard" Modern Rock Tracks, and earned Rancid its heaviest airplay on MTV and radio stations to date. The band also performed "Roots Radicals" and "Ruby Soho" on "Saturday Night Live".
Middle years (1997–2003).
After two years of touring for "...And Out Come the Wolves", Rancid returned to the studio in 1997 to begin recording its fourth studio album, "Life Won't Wait", which was released on June 30, 1998. The album branched out from Rancid's previous musical styles, and combined punk rock with elements of roots reggae, rockabilly, dub, hip-hop, and funk, drawing comparisons to The Clash's "Sandinista!". Though the album did not achieve the success of "...And Out Come the Wolves", but has since garnered a strong cult following in recent times. In 1999, Rancid decided to end its seven-year relationship with Epitaph and signed with Armstrong's founded Hellcat Records (which is a sub-label Epitaph).
A second self-titled album was released on August 1, 2000 and would be its first album released through Hellcat. The album failed to achieve the success of Rancid's previous three albums and reached number 68 on the "Billboard" charts. On the album, the group largely abandoned its ska-punk influences, recording a more hardcore-influenced album.
The three original members of Rancid released three songs under the name Devil's Brigade in 2002, one on the "Give 'Em the Boot III" compilation album, and two on a 12-inch vinyl record. In March of the same year, a split album with NOFX titled "BYO Split Series Volume III" was released, in which Rancid covered NOFX songs and NOFX covered Rancid songs.
After a break from touring in 2001, Rancid returned to the studio with Gurewitz in 2002 to record its sixth studio album, "Indestructible", which was released on August 19, 2003 and was their highest-charting album to date, reaching number 15. Unlike the band's previous albums, "Indestructible" was distributed by not only Epitaph/Hellcat but major record label, Warner Bros. Records, a move that received some backlash from the band's fans who questioned their loyalty to the independent scene. When released, the album didn't feature the Warner logo anywhere on the packaging, a move to ease tension among fans. The album was warmly received by most critics however met with mixed reviews from fans, some of which felt the album contained a "poppier" sound (some accusing Warner of having an influence on the music) while others felt it was a mixture of "..And Out Come the Wolves" and "Life Won't Wait". The album's music video for the first single, "Fall Back Down" was also met with some criticism from fans due to members of Good Charlotte and Kelly Osbourne making appearances.
Hiatus (2004–2005).
In 2004, after a tour for "Indestructible", Rancid went on an extensive hiatus. The band members worked with side projects, although it had not officially disbanded. Armstrong continued to play with his side project the Transplants, who released their second album, "Haunted Cities", in 2005. He also contributed guitar and backing vocals on Cypress Hill's song "What's Your Number?" from its tenth album "Till Death Do Us Part". Armstrong also released a solo album, "A Poet's Life" in May 2007. Frederiksen continued working with his side-project Lars Frederiksen and the Bastards and released their second studio album, "Viking", in 2004, the album was co-written and co-produced by Armstrong. Freeman briefly toured with Social Distortion in 2004 as John Maurer's replacement until the band found its current bassist Brent Harding. Freeman and Frederiksen both had children during this time as well—Freeman had two, and Frederiksen had one.
Reformation and "Let the Dominoes Fall" (2006–2010).
In early 2006, Rancid reformed to embark on a successful tour and played a number of acoustic performances as part of Hellcat Records' Hellcat Nights concert series at The Echo. It was the band's first live performance since its hiatus. On April 13, 2006, Rancid announced plans for a worldwide tour beginning in July 2006, and the release of a DVD consisting of 31 of its music videos, as well as a tentative release date of Spring 2007 for a new as-yet-unnamed studio album.
Similar to a number of other bands signed to Lookout! Records, in September 2006, Rancid had taken its self-titled extended play from the label's catalog.
On November 3, 2006, Reed left Rancid and was replaced by Branden Steineckert, formerly a member of The Used.
Rancid released a compilation album, "B Sides and C Sides", on December 11, 2007. The album consists of various b-sides, "c-sides", and songs from other compilations.
Rancid toured Japan in April 2008 for a number of shows following its two days headlining the Punkspring 2008 festival. Following the Japanese tour, Rancid embarked on a full tour of the United States during the summer and a tour of the United Kingdom in the winter.
Rancid used to host a one-hour once a week XM radio show. The show was called Rancid Radio and was on "Fungus" channel 53 Saturday at midnight. However, the show was cancelled due to Fungus 53 being taken from XM's programming.
Although plans for a follow-up to "Indestructible" had been mentioned during 2005, 2006 and 2007, it would not materialize until January 2008, when Rancid entered Skywalker Sound to record it. The resulting seventh studio album, "Let the Dominoes Fall", was released on June 2, 2009. In late May, the full album was streamed from the band's MySpace page. It was Rancid's first album without its "classic" line-up, with Branden Steineckert replacing Brett Reed on drums in 2006. The album was written at Branden's Unknown Studios in Utah and was recorded at Skywalker Sound in Nicasio, California. Music legend Booker T. Jones performed organ on one song. A deluxe version of the album included the CD, some of the songs recorded acoustically on another CD, and a making of the album DVD. Rancid toured North America in the summer of 2009 in support of "Let the Dominoes Fall", with Rise Against, Riverboat Gamblers, and Billy Talent as its opening bands. The tour began on June 4, 2009 in Vancouver, British Columbia and ended in Toronto, Ontario on July 31.
On June 10, 2009, the band appeared as the musical guest on "The Tonight Show with Conan O'Brien" playing "Last One to Die" from "Let the Dominoes Fall".
"...Honor Is All We Know" (2011–present).
Rancid did a small US tour supporting Blink-182 from August 25 through September 4, 2011, to enable them to warm up before entering the studio in September 2011 to record their eighth album with Brett Gurewitz. A 2012 release date was expected. The band also mentioned that a 20th anniversary world tour would accompany the album. They were announced to headline one of the biggest punk festivals in the world, Groezrock in Belgium. According to Tim Armstrong, Rancid's new album will arrive after the Transplants release their new one.
In March 2012, Rancid played some shows with Cock Sparrer. It was the 40th anniversary show for Cock Sparrer, and the 20th anniversary for Rancid. To accompany the shows, Pirates Press Records released a split 7-inch between the bands which featured "East Bay Night" from Rancid. As part of their 20th anniversary, they headlined the Rebellion Festival in Blackpool along with Public Image Limited, Social Distortion and Buzzcocks sharing headline slots, and on December 8, 2012 played in Birmingham along with Cock Sparrer as part of Rebellion Festival.
In December 2012, Rancid released their first new studio song in three years, titled "Fuck You", which they made available for free download on their website. The song was included on "Oi! This is Streetpunk, Volume Two", which was released on December 12, 2012.
In 2012, Rancid released "Rancid Essentials", an online-exclusive massive box set celebrating the band's 20th anniversary through Pirates Press Records. The box set features all of Rancid's officially released albums and compilations including their debut self-titled EP from 1992 through 2009. 92 sides of music on 46 re-mastered 45 rpm 7-inches housed in a leather box. The albums were released each on their own on 7-inch.
On February 6, 2013, Rancid uploaded a picture to their Facebook page of the band in the studio with the caption, "Recording has begun."
In a December 2013 interview on Reddit, Rancid drummer Branden Steineckert revealed that the new album was called "...Honor Is All We Know" and it would be released in 2014.
On September 28, 2014, Rancid revealed the artwork and track listing for "...Honor Is All We Know". On the day after, they announced that the album would be released on October 27, 2014. On September 30, 2014, the band released a video of them performing three of the album's tracks.

</doc>
<doc id="48707" url="http://en.wikipedia.org/wiki?curid=48707" title="GNU Octave">
GNU Octave

GNU Octave is a high-level programming language, primarily intended for numerical computations. It provides a command-line interface for solving linear and nonlinear problems numerically, and for performing other numerical experiments using a language that is mostly compatible with MATLAB. It may also be used as a batch-oriented language.
As part of the GNU Project, it is free software under the terms of the GNU General Public License.
Octave is one of the two major open-source alternatives to MATLAB, the other one being Scilab. Scilab however puts less emphasis on (bidirectional) syntactic compatibility with MATLAB than Octave does.
History.
The project was conceived around 1988. At first it was intended to be a companion to a chemical reactor design course. Real development was started by John W. Eaton in 1992. The first alpha release dates back to January 4, 1993 and on February 17, 1994 version 1.0 was released. Version 3.0 was released on December 21, 2007.
The program is named after Octave Levenspiel, a former professor of the principal author. Levenspiel is known for his ability to perform quick back-of-the-envelope calculations.
Developments.
In addition to use on desktops for personal scientific computing, Octave is used in academia and industry. For example, Octave was used on a massive parallel computer at Pittsburgh supercomputing center to find vulnerabilities related to guessing social security numbers.
Octave, the language.
The Octave language is an interpreted programming language. It is a structured programming language (similar to C) and supports many common C standard library functions, and also certain UNIX system calls and functions. However, it does not support passing arguments by reference.
Octave programs consist of a list of function calls or a script. The syntax is matrix-based and provides various functions for matrix operations. It supports various data structures and allows object-oriented programming.
Its syntax is very similar to MATLAB, and careful programming of a script will allow it to run on both Octave and MATLAB.
Because Octave is made available under the GNU General Public License, it may be freely changed, copied and used. The program runs on Microsoft Windows and most Unix and Unix-like operating systems, including Mac OS X.
Notable features.
Command and variable name completion.
Typing a TAB character on the command line causes Octave to attempt to complete variable, function, and file names (similar to Bash's tab completion). Octave uses the text before the cursor as the initial portion of the name to complete.
Command history.
When running interactively, Octave saves the commands typed in an internal buffer so that they can be recalled and edited.
Data structures.
Octave includes a limited amount of support for organizing data in structures. In this example, we see a structure "x" with elements "a", "b", and "c", (an integer, an array, and a string, respectively):
Short-circuit boolean operators.
Octave's 'codice_1' and 'codice_2' logical operators are evaluated in a short-circuit fashion (like the corresponding operators in the C language), in contrast to the element-by-element operators 'codice_3' and 'codice_4'.
Increment and decrement operators.
Octave includes the C-like increment and decrement operators 'codice_5' and 'codice_6' in both their prefix and postfix forms.
Also augmented assignment.
Unwind-protect.
Octave supports a limited form of exception handling modelled after the of Lisp. The general form of an unwind_protect block looks like this:
As a general rule, GNU Octave recognizes as termination of a given 'codice_7' either the keyword 'codice_8' (which is compatible with the MATLAB language) or a more specific keyword 'codice_9'. As a consequence, an 'codice_10' block can be terminated either with the keyword 'codice_11' as in the example, or with the more portable keyword 'codice_8'.
The "cleanup" part of the block is always executed. In case an exception is raised by the "body" part, "cleanup" is executed immediately before propagating the exception outside the block 'codice_10'.
GNU Octave also supports another form of exception handling (compatible with the MATLAB language):
This latter form differs from an 'codice_10' block in two ways. First, "exception_handling" is only executed when an exception is raised by "body". Second, after the execution of "exception_handling" the exception is not propagated outside the block (unless a 'codice_15' statement is purposely inserted within the "exception_handling" code).
Variable-length argument lists.
Octave has a mechanism for handling functions that take an unspecified number of arguments without explicit upper limit. To specify a list of zero or more arguments, use the special argument codice_16 as the last (or only) argument in the list.
Variable-length return lists.
A function can be set up to return any number of values by using the special return value codice_17. For example:
C++ integration.
It is also possible to execute Octave code directly in a C++ program. For example, here is a code snippet for calling codice_18:
C and C++ code can be integrated into GNU Octave by creating oct files, or using the Matlab compatible MEX files.
MATLAB compatibility.
Octave has been built with MATLAB compatibility in mind, and shares many features with MATLAB:
In fact, Octave treats incompatibility with MATLAB as a bug, therefore it can be considered a software clone, which doesn't infringe software copyright as per Lotus v. Borland court case.
Syntax compatibility.
There are a few purposeful, albeit minor, :
Function compatibility.
Many of the numerous MATLAB functions are available in GNU Octave, some of them are accessible through packages via Octave-forge, but not all of MATLAB functions are available in GNU Octave. List of unavailable functions exists in Octave, and developers are seeking for help to implement them. Looking for function __unimplemented.m__, leads to the .
Unimplemented functions are also categorized in , , , , and packages.
When an unimplemented function is called the following error message is shown:
 octave:1> quad2d
 warning: quad2d is not implemented. Consider using dblquad.
 Please read <http://www.octave.org/missing.html> to learn how you can
 contribute missing functionality.
 warning: called from
 __unimplemented__ at line 523 column 5
 error: 'quad2d' undefined near line 1 column 1
User interfaces.
Until version 3.8, Octave did not come with a graphical user interface (GUI)/integrated development environment (IDE) by default. However, an official graphical interface based on Qt has now been migrated to the main source repository and is available with Octave 3.8, but not as the default interface. It will become the default interface with the release of Octave 4.0. Several 3rd-party graphical front-ends have been developed.
External links.
Numerical packages and libraries interfacing with GNU Octave.
GNU Octave is also powered by third-party tools and libraries, mostly providing general or domain-specific abstractions for scientific computing. Those tools may be categorized according whether their contributions are more oriented toward computational modelling or toward enhancing visual analysis.

</doc>
<doc id="48711" url="http://en.wikipedia.org/wiki?curid=48711" title="Oregon Trail">
Oregon Trail

The Oregon Trail is a 2200 mi historic east-west large wheeled wagon route and emigrant trail that connected the Missouri River to valleys in Oregon. The eastern part of the Oregon Trail spanned part of the future state of Kansas and nearly all of what are now the states of Nebraska and Wyoming. The western half of the trail spanned most of the future states of Idaho and Oregon.
The Oregon Trail was laid by fur trappers and traders from about 1811 to 1840 and was only passable on foot or by horseback. By 1836, when the first migrant wagon train was organized in Independence, Missouri, a wagon trail had been cleared to Fort Hall, Idaho. Wagon trails were cleared further and further west, eventually reaching all the way to the Willamette Valley in Oregon. What came to be called the Oregon Trail was complete, even as improved roads, cutoffs, ferries and bridges made the trip faster and safer almost every year. From various starting points in Missouri, Iowa or Nebraska Territory, the routes converged along the lower Platte River Valley near Fort Kearny, Nebraska Territory and led to rich farmlands west of the Rocky Mountains.
From the early to mid-1830s (and particularly through the epoch years, 1846–1869) the Oregon Trail and its many offshoots were used by about 400,000 settlers, ranchers, farmers, miners, and businessmen and their families. The eastern half of the trail was also used by travelers on the California Trail (from 1843), Bozeman Trail (from 1863), and Mormon Trail (from 1847) before turning off to their separate destinations. Use of the trail declined as the first transcontinental railroad was completed in 1869, making the trip west substantially faster, cheaper, and safer. Today, modern highways such as Interstate 80 follow the same course westward and pass through towns originally established to serve those using the Oregon Trail.
History.
Lewis and Clark Expedition.
In 1803, President Thomas Jefferson issued the following instructions to Meriwether Lewis: "The object of your mission is to explore the Missouri river, & such principal stream of it, as, by its course & communication with the waters of the Pacific Ocean, whether the Columbia, Oregon, Colorado and/or other river may offer the most direct & practicable water communication across this continent, for the purposes of commerce." Although Lewis and William Clark found a path to the Pacific Ocean, it was not until 1859 that a direct and practicable route, the Mullan Road, connected the Missouri River to the Columbia River.
The first land route across what is now the United States was partially mapped by the Lewis and Clark Expedition between 1804 and 1806. Lewis and Clark initially believed they had found a practical overland route to the west coast; however, the two passes they found going through the Rocky Mountains, Lemhi Pass and Lolo Pass, turned out to be much too difficult for wagons to pass through without considerable road work. On the return trip in 1806 they traveled from the Columbia River to the Snake River and the Clearwater River over Lolo pass again. They then traveled overland up the Blackfoot River and crossed the Continental Divide at Lewis and Clark Pass and on to the head of the Missouri River. This was ultimately a shorter and faster route than the one they followed west. This route had the disadvantages of being much too rough for wagons and controlled by the Blackfoot Indians. Even though Lewis and Clark had only traveled a narrow portion of the upper Missouri River drainage and part of the Columbia River drainage, these were considered the two major rivers draining most of the Rocky Mountains, and the expedition confirmed that there was no "easy" route through the northern Rocky Mountains as Jefferson had hoped. Nonetheless, this famous expedition had mapped both the eastern and western river valleys (Platte and Snake Rivers) that bookend the route of the Oregon Trail (and other emigrant trails) across the continental divide—they just had not located the South Pass or some of the interconnecting valleys later used in the high country. They did show the way for the mountain men, who within a decade would find a better way across, even if it was not to be an easy way.
Astorians.
In 1810, fur trader, entrepreneur, and one of the wealthiest men in the U.S., John Jacob Astor
of the American Fur Company, outfitted an expedition (known as the Astor Expedition or "Astorians") under Wilson Price Hunt to find a possible overland supply route and trapping territory for fur trading posts. Fearing attack by the Blackfoot Indians, the overland expedition veered south of Lewis and Clark's route into what is now Wyoming and in the process passed across Union Pass and into Jackson Hole, Wyoming. From there they went over the Teton Range via Teton Pass and then down to the Snake River in Idaho. They abandoned their horses at the Snake River, made dugout canoes, and attempted to use the river for transport. After a few days' travel they soon discovered that steep canyons, waterfalls and impassable rapids made travel by river impossible. Too far from their horses to retrieve them, they had to cache most of their goods and walk the rest of the way to the Columbia River where they made new boats and traveled to the newly established Fort Astoria. The expedition demonstrated that much of the route along the Snake River plain and across to the Columbia was passable by pack train or with minimal improvements, even wagons. This knowledge would be incorporated into the concatenated trail segments as the Oregon Trail took its early shape.
In early 1811, the supply ship "Tonquin" left supplies and men to establish Fort Astoria, at the mouth of the Columbia River, and Fort Okanogan at the confluence of the Okanogan and Columbia rivers. The Tonquin then went up the coast to Clayoquot Sound for a trading expedition. There it was attacked and overwhelmed by the indigenous Nuu-chah-nulth before being blown up, killing all the crew and many natives.
American Fur Company partner Robert Stuart led a small group of men back east to report to Astor. The group planned to retrace the path followed by the overland expedition back up to the east following the Columbia and Snake rivers.
Fear of Indian attack near Union Pass in Wyoming forced the group further south where they luckily discovered South Pass, a wide and easy pass over the Continental Divide. The party continued east via the Sweetwater River, North Platte River (where they spent the winter of 1812–1813) and Platte River to the Missouri River, finally arriving in St. Louis in the spring of 1813. The route they had used appeared to potentially be a practical wagon route, requiring minimal improvements, and Stuart's journals provided a meticulous account of most of the route. Because of the War of 1812 and the lack of U.S. fur trading posts in the Oregon Country, most of the route was unused for more than 10 years.
The North West Company and Hudson's Bay Company.
In August 1811, three months after Fort Astor was established, David Thompson and his team of British North West Company explorers came floating down the Columbia to Fort Astoria. He had just completed a journey through much of western Canada and most of the Columbia River drainage system. He was mapping the country for possible fur trading posts. Along the way he camped at the confluence of the Columbia and Snake rivers and posted a notice claiming the land for Britain and stating the intention of the North West Company to build a fort on the site (Fort Nez Perces was later established there). Astor, pressured by potential confiscation by the British navy of their forts and supplies in the War of 1812, sold to the North West Company in 1812 their forts, supplies and furs on the Columbia and Snake River. The North West Company started establishing more forts and trading posts of their own.
By 1821, when armed hostilities broke out with their Hudson's Bay Company (HBC) rivals, the North West Company was pressured by the British government to merge with the HBC. The HBC had nearly a complete monopoly on trading (and most governing issues) in the Columbia District, or Oregon Country as it was referred to by the Americans, and also in Rupert's Land (western Canada). That year the British parliament passed a statute applying the laws of Upper Canada to the district and giving the HBC power to enforce those laws.
From 1812 to 1840 the British through the HBC had nearly complete control of the Pacific Northwest and the western half of the Oregon Trail. In theory, the Treaty of Ghent ending the War of 1812 restored the U.S. back to its possessions in Oregon territory. "Joint occupation" of the region was formally established by the Anglo-American Convention of 1818. The British through the HBC tried to discourage any U.S. trappers, traders and settlers from doing any significant trapping, trading or settling in the Pacific Northwest.
By overland travel, American missionaries and early settlers (initially mostly ex-trappers) started showing up in Oregon around 1824. Although officially the HBC discouraged settlement because it interfered with their lucrative fur trade, their Chief Factor at Fort Vancouver, John McLoughlin, gave substantial help including employment until they could get established. In the early 1840s thousands of American settlers arrived and soon greatly outnumbered the British settlers in Oregon. McLoughlin, despite working for the HBC, gave help in the form of loans, medical care, shelter, clothing, food, supplies and seed to U.S. emigrants. These new emigrants often arrived in Oregon tired, worn out, nearly penniless, with insufficient food or supplies just as winter was coming on. McLoughlin would later be hailed as the Father of Oregon.
The York Factory Express, establishing another route to the Oregon territory, evolved from an earlier express brigade used by the North West Company between Fort Astoria and Fort William, Ontario on Lake Superior. By 1825 the HBC started using two brigades, each setting out from opposite ends of the express route—one from Fort Vancouver on the Columbia River and the other from York Factory on Hudson Bay—in spring and passing each other in the middle of the continent. This established a "quick"—about 100 days for 2600 mi one way—to resupply their forts and fur trading centers as well as collecting the furs the posts had bought and transmitting messages between Fort Vancouver and York Factory on Hudson Bay.
The HBC built a new much larger Fort Vancouver in 1824 slightly upstream of Fort Astoria on the north side of the Columbia River (they were hoping the Columbia would be the future Canada–U.S. border). The fort quickly became the center of activity in the Pacific Northwest. Every year ships would come from London to the Pacific (via Cape Horn) to drop off supplies and trade goods in their trading posts in the Pacific Northwest and pick up the accumulated furs used to pay for these supplies. It was the nexus for the fur trade on the Pacific Coast; its influence reached from the Rocky Mountains to the Hawaiian Islands, and from Russian Alaska into Mexican-controlled California. At its pinnacle in about 1840, Fort Vancouver and its Factor (manager) watched over 34 outposts, 24 ports, 6 ships, and about 600 employees.
When American emigration over the Oregon Trail began in earnest in the early 1840s, for many settlers the fort became the last stop on the Oregon Trail where they could get supplies, aid and help before starting their homestead. Fort Vancouver was the main re-supply point for nearly all Oregon trail travelers until U.S. towns could be established. The HBC established Fort Colville in 1825 on the Columbia River near Kettle Falls as a good site to collect furs and control the upper Columbia River fur trade. Fort Nisqually was built near the present town of DuPont, Washington and was the first HBC fort on Puget Sound. Fort Victoria was erected in 1843 and became the headquarters of operations in British Columbia, eventually growing into modern-day Victoria, the capital city of British Columbia.
By 1840 the HBC had three forts: Fort Hall (purchased from Nathaniel Jarvis Wyeth in 1837), Fort Boise and Fort Nez Perce on the western end of the Oregon Trail route as well as Fort Vancouver near its terminus in the Willamette Valley. With minor exceptions they all gave substantial and often desperately needed aid to the early Oregon Trail pioneers.
When the fur trade slowed in 1840 because of fashion changes in men's hats, the value of the Pacific Northwest to the British was seriously diminished. Canada had few potential settlers who were willing to move more than 2500 mi to the Pacific Northwest, although several hundred ex-trappers, British and American, and their families did start settling in Oregon, Washington and California. They used most of the York Express route through northern Canada. In 1841 James Sinclair, on orders from Sir George Simpson, guided nearly 200 settlers from the Red River Colony (located at the junction of the Assiniboine River and Red River near present Winnipeg, Canada) into the Oregon territory. This attempt at settlement failed when most of the families joined the settlers in the Willamette Valley, with their promise of free land and HBC-free government.
In 1846 the Oregon Treaty ending the Oregon boundary dispute was signed with Britain. The British lost the land north of the Columbia River they had so long controlled. The new Canada–United States border was established much further north at the 49th parallel. The treaty granted the HBC navigation rights on the Columbia River for supplying their fur posts, clear titles to their trading post properties allowing them to be sold later if they wanted, and left the British with good anchorages at Vancouver and Victoria. It gave the United States what it mostly wanted, a "reasonable" boundary and a good anchorage on the West Coast in Puget Sound. While there were almost no United States settlers in the future state of Washington in 1846, the United States had already demonstrated it could induce thousands of settlers to go to the Oregon Territory, and it would be only a short time before they would vastly outnumber the few hundred HBC employees and retirees living in Washington.
Great American Desert.
Reports from expeditions in 1806 by Lieutenant Zebulon Pike and in 1819 by Major Stephen Long described the Great Plains as "unfit for human habitation" and as "The Great American Desert". These descriptions were mainly based on the relative lack of timber and surface water. The images of sandy wastelands conjured up by terms like "desert" were tempered by the many reports of vast herds of millions of Plains Bison that somehow managed to live in this "desert". In the 1840s, the Great Plains appeared to be unattractive for settlement and were illegal for homesteading until well after 1846—initially it was set aside by the U.S. government for Indian settlements. The next available land for general settlement, Oregon, appeared to be free for the taking and had fertile lands, disease free climate (yellow fever and malaria were prevalent in much of the Missouri and Mississippi River drainage then), extensive uncut, unclaimed forests, big rivers, potential seaports, and only a few nominally British settlers.
Fur traders, trappers and explorers.
Fur trappers, often working for fur traders, followed nearly all possible streams looking for beaver in the years (1812–1840) the fur trade was active. Fur traders included Manuel Lisa, Robert Stuart, William Henry Ashley, Jedediah Smith, William Sublette, Andrew Henry, Thomas Fitzpatrick, Kit Carson, Jim Bridger, Peter Skene Ogden, David Thompson, James Douglas, Donald Mackenzie, Alexander Ross, James Sinclair, and other mountain men. Besides discovering and naming many of the rivers and mountains in the Intermountain West and Pacific Northwest they often kept diaries of their travels and were available as guides and consultants when the trail started to become open for general travel. The fur trade business wound down to a very low level just as the Oregon trail traffic seriously began around 1840.
In fall of 1823, Jedediah Smith and Thomas Fitzpatrick led their trapping crew south from the Yellowstone River to the Sweetwater River. They were looking for a safe location to spend the winter. Smith reasoned since the Sweetwater flowed east it must eventually run into the Missouri River. Trying to transport their extensive fur collection down the Sweetwater and North Platte River, they found after a near disastrous canoe crash that the rivers were too swift and rough for water passage. On July 4, 1824, they cached their furs under a dome of rock they named Independence Rock and started their long trek on foot to the Missouri River. Upon arriving back in a settled area they bought pack horses (on credit) and retrieved their furs. They had re-discovered the route that Robert Stuart had taken in 1813—eleven years before. Thomas Fitzpatrick was often hired as a guide when the fur trade dwindled in 1840. Jedediah Smith was killed by Indians around 1831.
Up to 3,000 mountain men were trappers and explorers, employed by various British and United States fur companies or working as free trappers, who roamed the North American Rocky Mountains from about 1810 to the early 1840s. They usually traveled in small groups for mutual support and protection. Trapping took place in the fall when the fur became prime. Mountain men primarily trapped beaver and sold the skins. A good beaver skin could bring up to $4 at a time when a man's wage was often $1 per day. Some were more interested in exploring the West. In 1825, the first significant American Rendezvous occurred on the Henry's Fork of the Green River. The trading supplies were brought in by a large party using pack trains originating on the Missouri River. These pack trains were then used to haul out the fur bales. They normally used the north side of the Platte River—the same route used 20 years later by the Mormon Trail. For the next 15 years the American rendezvous was an annual event moving to different locations, usually somewhere on the Green River in the future state of Wyoming. Each rendezvous, occurring during the slack summer period, allowed the fur traders to trade for and collect the furs from the trappers and their Indian allies without having the expense of building or maintaining a fort or wintering over in the cold Rockies. In only a few weeks at a rendezvous a year's worth of trading and celebrating would take place as the traders took their furs and remaining supplies back east for the winter and the trappers faced another fall and winter with new supplies. Trapper Jim Beckwourth described the scene as one of "Mirth, songs, dancing, shouting, trading, running, jumping, singing, racing, target-shooting, yarns, frolic, with all sorts of extravagances that white men or Indians could invent." In 1830, William Sublette brought the first wagons carrying his trading goods up the Platte, North Platte, and Sweetwater rivers before crossing over South Pass to a fur trade rendezvous on the Green River near the future town of Big Piney, Wyoming. He had a crew that dug out the gullies and river crossings and cleared the brush where needed. This established that the eastern part of most of the Oregon Trail was passable by wagons. In the late 1830s the HBC instituted a policy intended to destroy or weaken the American fur trade companies. The HBC's annual collection and re-supply Snake River Expedition was transformed to a trading enterprise. Beginning in 1834, it visited the American Rendezvous to undersell the American traders—losing money but undercutting the American fur traders. By 1840 the fashion in Europe and Britain shifted away from the formerly very popular beaver felt hats and prices for furs rapidly declined and the trapping almost ceased.
Fur traders tried to use the Platte River, the main route of the eastern Oregon Trail, for transport but soon gave up in frustration as its many channels and islands combined with its muddy waters were too shallow, crooked and unpredictable to use for water transport. The Platte proved to be unnavigable. The Platte River and North Platte River Valley, however, became an easy roadway for wagons, with its nearly flat plain sloping easily up and heading almost due west.
There were several U.S. government-sponsored explorers who explored part of the Oregon Trail and wrote extensively about their explorations. Captain Benjamin Bonneville on his expedition of 1832 to 1834 explored much of the Oregon trail and brought wagons up the Platte, North Platte, Sweetwater route across South Pass to the Green River in Wyoming. He explored most of Idaho and the Oregon Trail to the Columbia. The account of his explorations in the west was published by Washington Irving in 1838.). John C. Frémont of the U.S. Army's Corps of Topographical Engineers and his guide Kit Carson led three expeditions from 1842 to 1846 over parts of California and Oregon. His explorations were written up by him and his wife Jessie Benton Frémont and were widely published. The first "decent" map of California and Oregon were drawn by Frémont and his topographers and cartographers in about 1848.
Missionaries.
In 1834, The Dalles Methodist Mission was founded by Reverend Jason Lee just east of Mount Hood on the Columbia River. In 1836, Henry H. Spalding and Marcus Whitman traveled west to establish the Whitman Mission near modern day Walla Walla, Washington. The party included the wives of the two men, Narcissa Whitman and Eliza Hart Spalding, who became the first European-American women to cross the Rocky Mountains. En route, the party accompanied American fur traders going to the 1836 rendezvous on the Green River in Wyoming and then joined Hudson's Bay Company fur traders traveling west to Fort Nez Perce (also called Fort Walla Walla). The group was the first to travel in wagons all the way to Fort Hall, where the wagons were abandoned at the urging of their guides. They used pack animals for the rest of the trip to Fort Walla Walla and then floated by boat to Fort Vancouver to get supplies before returning to start their missions. Other missionaries, mostly husband and wife teams using wagon and pack trains, established missions in the Willamette Valley, as well as various locations in the future states of Washington, Oregon, and Idaho.
Early emigrants.
On May 1, 1839, a group of eighteen men from Peoria, Illinois, set out with the intention of colonizing the Oregon country on behalf of the United States of America and drive out the HBC operating there. The men of the Peoria Party were among the first pioneers to traverse most of the Oregon Trail. The men were initially led by Thomas J. Farnham and called themselves the Oregon Dragoons. They carried a large flag emblazoned with their motto "Oregon Or The Grave". Although the group split up near Bent's Fort on the South Platte and Farnham was deposed as leader, nine of their members eventually did reach Oregon.
In September 1840, Robert Newell, Joseph L. Meek, and their families reached Fort Walla Walla with three wagons that they had driven from Fort Hall. Their wagons were the first to reach the Columbia River over land, and they opened the final leg of Oregon Trail to wagon traffic.
In 1841 the Bartleson-Bidwell Party was the first emigrant group credited with using the Oregon Trail to emigrate west. The group set out for California, but about half the party left the original group at Soda Springs, Idaho, and proceeded to the Willamette Valley in Oregon, leaving their wagons at Fort Hall.
On May 16, 1842, the second organized wagon train set out from Elm Grove, Missouri, with more than 100 pioneers. The party was led by Elijah White. The group broke up after passing Fort Hall with most of the single men hurrying ahead and the families following later.
Great Migration of 1843.
In what was dubbed "The Great Migration of 1843" or the "Wagon Train of 1843", an estimated 700 to 1,000 emigrants left for Oregon. They were led initially by John Gantt, a former U.S. Army Captain and fur trader who was contracted to guide the train to Fort Hall for $1 per person. The winter before, Marcus Whitman had made a brutal mid-winter trip from Oregon to St. Louis to appeal a decision by his Mission backers to abandon several of the Oregon missions. He joined the wagon train at the Platte River for the return trip. When the pioneers were told at Fort Hall by agents from the Hudson's Bay Company that they should abandon their wagons there and use pack animals the rest of the way, Whitman disagreed and volunteered to lead the wagons to Oregon. He believed the wagon trains were large enough that they could build whatever road improvements they needed to make the trip with their wagons. The biggest obstacle they faced was in the Blue Mountains of Oregon where they had to cut and clear a trail through heavy timber. The wagons were stopped at The Dalles, Oregon by the lack of a road around Mount Hood. The wagons had to be disassembled and floated down the treacherous Columbia River and the animals herded over the rough Lolo trail to get by Mt. Hood. Nearly all of the settlers in the 1843 wagon trains arrived in the Willamette Valley by early October. A passable wagon trail now existed from the Missouri River to The Dalles. In 1846, the Barlow Road was completed around Mount Hood, providing a rough but completely passable wagon trail from the Missouri River to the Willamette Valley: about 2000 mi.
Oregon Country.
In 1843, settlers of the Willamette Valley drafted the Organic Laws of Oregon organizing land claims within the Oregon Country. Married couples were granted at no cost (except for the requirement to work and improve the land) up to 640 acre (a section or square mile), and unmarried settlers could claim 320 acre. As the group was a provisional government with no authority, these claims were not valid under United States or British law, but they were eventually honored by the United States in the Donation Land Act of 1850. The Donation Land Act provided for married settlers to be granted 320 acre and unmarried settlers 160 acre. Following the expiration of the act in 1854 the land was no longer free but cost $1.25 per acre ($3.09/hectare) with a limit of 320 acre—the same as most other unimproved government land.
Mormon emigration.
Following persecution and mob action in Missouri, Illinois, and other states, and the assassination of their prophet Joseph Smith in 1844, Mormon leader Brigham Young was chosen by the leaders of the Latter Day Saints (LDS) church to lead the LDS settlers west. He chose to lead his people to the Salt Lake Valley in present day Utah. In 1847 Young led a small, especially picked fast-moving group of men and women from their Winter Quarters encampments near Omaha, Nebraska, and their approximately 50 temporary settlements on the Missouri River in Iowa including Council Bluffs. About 2,200 LDS pioneers went that first year as they filtered in from Mississippi, Colorado, California, and several other states. The initial pioneers were charged with establishing farms, growing crops, building fences and herds, and establishing preliminary settlements to feed and support the many thousands of emigrants expected in the coming years. After ferrying across the Missouri River and establishing wagon trains near what became Omaha, the Mormons followed the northern bank of the Platte River in Nebraska to Fort Laramie in present day Wyoming. They initially started out in 1848 with trains of several thousand emigrants, which were rapidly split into smaller groups to be more easily accommodated at the limited springs and acceptable camping places on the trail. Organized as a complete evacuation from their previous homes, farms, and cities in Illinois, Missouri, and Iowa, this group consisted of entire families with no one left behind. The much larger presence of women and children meant these wagon trains did not try to cover as much ground in a single day as Oregon and California bound emigrants did, typically taking about 100 days to cover the 1000 mi trip to Salt Lake City. (The Oregon and California emigrants typically averaged about 15 mi per day.) In Wyoming the Mormon emigrants followed the main Oregon/California/Mormon Trail through Wyoming to Fort Bridger, where they split from the main trail and followed (and improved) the crude path established by the ill-fated Donner Party of 1846 into Utah and the Salt Lake Valley.
Between 1847 and 1860 over 43,000 Mormon settlers and tens of thousands of travelers on the California Trail and Oregon Trail followed Young to Utah. After 1848, the travelers headed to California or Oregon resupplied at the Salt Lake Valley, and then went back over the Salt Lake Cutoff, rejoining the trail near the future Idaho–Utah border at the City of Rocks in Idaho.
Starting in 1855, many of the poorer Mormon travelers made the trek with hand built handcarts and fewer wagons. Guided by experienced guides, handcarts—pulled and pushed by two to four people—were as fast as ox-drawn wagons and allowed them to bring 75 to of possessions plus some food, bedding, and tents to Utah. Accompanying wagons carried more food and supplies. Upon arrival in Utah, the handcart pioneers were given or found jobs and accommodations by individual Mormon families for the winter until they could become established. About 3,000 out of over 60,000 Mormon pioneers came across with handcarts.
Along the Mormon Trail, the Mormon pioneers established a number of ferries and made trail improvements to help later travelers and earn much needed money. One of the better known ferries was the Mormon Ferry across the North Platte near the future site of Fort Caspar in Wyoming which operated between 1848 and 1852 and the Green River ferry near Fort Bridger which operated from 1847 to 1856. The ferries were free for Mormon settlers while all others were charged a toll of from $3 to $8.
California Gold Rush.
In January 1848, James Marshall found gold in the Sierra Nevada portion of the American River, sparking the California Gold Rush. It is estimated that about two-thirds of the male population in Oregon went to California in 1848 to cash in on the opportunity. To get there, they helped build the Lassen Branch of the Applegate-Lassen Trail by cutting a wagon road through extensive forests. Many returned with significant gold which helped jump-start the Oregon economy. Over the next decade, gold seekers from the Midwestern United States and East Coast of the United States dramatically increased traffic on the Oregon and California Trails. The "forty-niners" often chose speed over safety and opted to use shortcuts such as the Sublette-Greenwood Cutoff in Wyoming which reduced travel time by almost seven days but spanned nearly 45 mi of desert without water, grass, or fuel for fires. 1849 was the first year of large scale cholera epidemics in the United States, and thousands are thought to have died along the trail on their way to California—most buried in unmarked graves in Kansas and Nebraska. The "adjusted" 1850 U.S. Census of California showed this rush was overwhelmingly male with about 112,000 males to 8,000 females (with about 5,500 women over age 15). Women were significantly underrepresented in the California Gold Rush, and sex ratios did not reach essential equality in California (and other western states) until about 1950. The relative scarcity of women gave them many opportunities to do many more things that were not "normally" considered "women's work" of this era. After 1849 the California Gold Rush continued for several years as the miners continued to find about $50,000,000 worth of gold per year at $21 per ounce. Once California was established as a prosperous state many thousands more emigrated there each year for the opportunities.
Later emigration and uses of the trail.
The trail was still in use during the Civil War, but traffic declined after 1855 when the Panama Railroad across the Isthmus of Panama was completed. Paddle wheel steamships and sailing ships, often heavily subsidized to carry the mail, provided rapid transport to and from the east coast and New Orleans, Louisiana, to and from Panama to ports in California and Oregon.
Over the years many ferries were established to help get across the many rivers on the path of the Oregon Trail. Multiple ferries were established on the Missouri River, Kansas River, Little Blue River, Elkhorn River, Loup River, Platte River, South Platte River, North Platte River, Laramie River, Green River, Bear River, two crossings of the Snake River, John Day River, Deschutes River, Columbia River, as well as many other smaller streams. During peak immigration periods several ferries on any given river often competed for pioneer dollars. These ferries significantly increased speed and safety for Oregon Trail travelers. They increased the cost of traveling the trail by roughly $30 per wagon but increased the speed of the transit from about 160 to 170 days in 1843 to 120 to 140 days in 1860. Ferries also helped prevent death by drowning at river crossings.
In April 1859, an expedition of U.S. Corps of Topographical Engineers led by Captain James H. Simpson left Camp Floyd, Utah, to establish an army supply route across the Great Basin to the eastern slope of the Sierras. Upon return in early August, Simpson reported that he had surveyed the Central Overland Route from Camp Floyd to Genoa, Nevada. This route went through central Nevada (roughly where U.S. Route 50 goes today) and was about 280 mi shorter than the "standard" Humboldt River California trail route.
The Army improved the trail for use by wagons and stagecoaches in 1859 and 1860. Starting in 1860, the American Civil War closed the heavily subsidized Butterfield Overland Mail stage Southern Route through the deserts of the American Southwest.
In 1860–61 the Pony Express, employing riders traveling on horseback day and night with relay stations about every 10 mi to supply fresh horses, was established from St. Joseph, Missouri, to Sacramento, California. The Pony Express built many of their eastern stations along the Oregon/California/Mormon/Bozeman trails and many of their western stations along the very sparsely settled Central Route across Utah and Nevada. The Pony Express delivered mail summer and winter in roughly 10 days from the midwest to California.
In 1861 John Butterfield, who since 1858 had been using the Butterfield Overland Mail, also switched to the Central Route to avoid traveling through hostile territories during the American Civil War. George Chorpenning immediately realized the value of this more direct route, and shifted his existing mail and passenger line along with their stations from the "Northern Route" (California Trail) along the Humboldt River. In 1861 the First Transcontinental Telegraph also laid its lines alongside the Central Overland Route. Several stage lines were set up carrying mail and passengers that traversed much of the route of the original Oregon Trail to Fort Bridger and from there over the Central Overland Route to California. By traveling day and night with many stations and changes of teams (and extensive mail subsidies) these stages could get passengers and mail from the midwest to California in about 25 to 28 days. These combined stage and Pony Express stations along the Oregon Trail and Central Route across Utah and Nevada were joined by the First Transcontinental Telegraph stations and telegraph line, which followed much the same route in 1861 from Carson City, Nevada to Salt Lake City. The Pony Express folded in 1861 as they failed to receive an expected mail contract from the U.S. government and the telegraph filled the need for rapid east–west communication. This combination wagon/stagecoach/pony express/telegraph line route is labeled the "Pony Express National Historic Trail" on the National Trail Map. From Salt Lake City the telegraph line followed much of the Mormon/California/Oregon trails to Omaha, Nebraska.
After the First Transcontinental Railroad was completed in 1869, telegraph lines usually followed the railroad tracks as the required relay stations and telegraph lines were much easier to maintain alongside the tracks. Telegraph lines to unpopulated areas were largely abandoned.
As the years passed, the Oregon Trail became a heavily used corridor from the Missouri River to the Columbia River. Offshoots of the trail continued to grow as gold and silver discoveries, farming, lumbering, ranching, and business opportunities resulted in much more traffic to many areas. Traffic became two-directional as towns were established along the trail. By 1870 the population in the states served by the Oregon Trail and its offshoots increased by about 350,000 over their 1860 census levels. With the exception of most of the 180,000 population increase in California, most of these people living away from the coast traveled over parts of the Oregon Trail and its many extensions and cutoffs to get to their new residences.
Even before the famous Texas cattle drives after the Civil War, the trail was being used to drive herds of thousands of cattle, horses, sheep, and goats from the midwest to various towns and cities along the trails. According to studies by trail historian John Unruh the livestock may have been as plentiful or more plentiful than the immigrants in many years. In 1852 there were even records of a 1,500-turkey drive from Illinois to California. The main reason for this livestock traffic was the large cost discrepancy between livestock in the midwest and at the end of the trail in California, Oregon, or Montana. They could often be bought in the midwest for about 1/3 to 1/10 what they would fetch at the end of the trail. Large losses could occur and the drovers would still make significant profit. As the emigrant travel on the trail declined in later years and after livestock ranches were established at many places along the trail large herds of animals often were driven along part of the trail to get to and from markets.
Trail decline.
The first transcontinental railroad was completed in 1869, providing faster, safer, and usually cheaper travel east and west (the journey took seven days and cost as little as $65). Some emigrants continued to use the trail well into the 1890s, and modern highways and railroads eventually paralleled large portions of the trail, including U.S. Highway 26, Interstate 84 in Oregon and Idaho and Interstate 80 in Nebraska. Contemporary interest in the overland trek has prompted the states and federal government to preserve landmarks on the trail including wagon ruts, buildings, and "registers" where emigrants carved their names. Throughout the 20th and 21st centuries there have been a number of re-enactments of the trek with participants wearing period garments and traveling by wagon.
Routes.
As the trail developed it became marked by many cutoffs and shortcuts from Missouri to Oregon. The basic route follows river valleys as grass and water were absolutely necessary.
While the first few parties organized and departed from Elm Grove, the Oregon Trail's primary starting point was Independence, Missouri, or Westport on the Missouri River. Later, several feeder trails led across Kansas, and some towns became starting points, including Weston, Fort Leavenworth, Atchison, St. Joseph, and Omaha.
The Oregon Trail's nominal termination point was Oregon City, at the time the proposed capital of the Oregon Territory. However, many settlers branched off or stopped short of this goal and settled at convenient or promising locations along the trail. Commerce with pioneers going further west helped establish these early settlements and launched local economies critical to their prosperity.
At dangerous or difficult river crossings, ferries or toll bridges were set up and bad places on the trail were either repaired or bypassed. Several toll roads were constructed. Gradually the trail became easier with the average trip (as recorded in numerous diaries) dropping from about 160 days in 1849 to 140 days 10 years later.
Many other trails followed the Oregon Trail for much of its length, including the Mormon Trail from Illinois to Utah; the California Trail to the gold fields of California; and the Bozeman Trail to Montana. Because it was more a network of trails than a single trail, there were numerous variations with other trails eventually established on both sides of the Platte, North Platte, Snake, and Columbia rivers. With literally thousands of people and thousands of livestock traveling in a fairly small time slot the travelers had to spread out to find clean water, wood, good campsites, and grass. The dust kicked up by the many travelers was a constant complaint, and where the terrain would allow it there may be between 20 to 50 wagons traveling abreast.
Remnants of the trail in Kansas, Nebraska, Wyoming, Idaho, and Oregon have been listed on the National Register of Historic Places, and the entire trail is a designated National Historic Trail (listed as the Oregon National Historic Trail).
Missouri.
Initially, the main "jumping off point" was the common head of the Santa Fe Trail and Oregon trail—Independence, and Kansas City. Travelers starting in Independence had to ferry across the Missouri River. After following the Santa Fe trail to near present day Topeka, they ferried across the Kansas River to start the trek across Kansas and points west. Another busy "jumping off point" was St. Joseph—established in 1843. In its early days, St. Joseph was a bustling outpost and rough frontier town, serving as one of the last supply points before heading over the Missouri River to the frontier. St. Joseph had good steamboat connections to St. Louis and other ports on the combined Ohio, Missouri, and Mississippi River systems. During the busy season there were several ferry boats and steamboats available to transport travelers to the Kansas shore where they started their travels westward. Before the Union Pacific Railroad was started in 1865, St. Joseph was the westernmost point in the United States accessible by rail. Other towns used as supply points in Missouri included Old Franklin, Arrow Rock, and Fort Osage.
Iowa.
In 1803 President Thomas Jefferson obtained from France the Louisiana Purchase for $15 million (equivalent to about $230 million today) which included all the land drained by the Missouri River and roughly doubled the size of U.S. territory. The future states of Iowa and Missouri, located west of the Mississippi River and east of Missouri River, were part of this purchase. The Lewis and Clark Expedition stopped several times in the future state of Iowa on their 1805–1806 expedition to the west coast. A disputed 1804 treaty between Quashquame and William Henry Harrison (future ninth President of the U.S.) that surrendered much of the future state of Illinois to the U.S. enraged many Sauk (Sac) Indians and led to the 1832 Black Hawk War. As punishment for the uprising, and as part of a larger settlement strategy, treaties were subsequently designed to remove all Indians from Iowa Territory. Some settlers started drifting into Iowa in 1833. President Martin Van Buren on July 4, 1838, signed the U.S. Congress laws establishing the Territory of Iowa. Iowa was located opposite the junction of the Platte and Missouri rivers and was used by some of the fur trapper rendezvous traders as a starting point for their supply expeditions. In 1846 the Mormons, expelled from Nauvoo, Illinois, traversed Iowa (on part of the Mormon Trail) and settled temporarily in significant numbers on the Missouri River in Iowa and the future state of Nebraska at their Winter Quarters near the future city of Omaha, Nebraska. (See: Missouri River settlements (1846–1854)) The Mormons established about 50 temporary towns including the town of Kanesville, Iowa (renamed Council Bluffs in 1852) on the east bank of the Missouri River opposite the mouth of the Platte River. For those travelers to Oregon, California, and Utah who were bringing their teams to the Platte River junction Kanesville and other towns became major "jumping off places" and supply points. In 1847 the Mormons established three ferries across the Missouri River and others established even more ferries for the spring start on the trail. In the 1850 census there were about 8,000 mostly Mormons tabulated in the large Pottawattamie County, Iowa District 21. (The original Pottawattamie County was subsequently made into five counties and parts of several more.) By 1854 most of the Mormon towns, farms and villages were largely taken over by non-Mormons as they abandoned them or sold them for not much and continued their migration to Utah. After 1846 the towns of Council Bluffs, Iowa, Omaha (est. 1852) and other Missouri River towns became major supply points and "jumping off places" for travelers on the Mormon, California, Oregon, and other trails west.
Kansas.
Starting initially in Independence, Missouri, or Kansas City in Missouri, the initial trail follows the Santa Fe Trail into Kansas south of the Wakarusa River. After crossing Mount Oread at Lawrence, the trail crosses the Kansas River by ferry or boats near Topeka and crossed the Wakarusa and Vermillion rivers by ferries. After the Vermillion River the trail angles northwest to Nebraska paralleling the Little Blue River until reaching the south side of the Platte River. Travel by wagon over the gently rolling Kansas countryside was usually unimpeded except where streams had cut steep banks. There a passage could be made with a lot of shovel work to cut down the banks or the travelers could find an already established crossing.
Nebraska.
Those emigrants on the eastern side of the Missouri River in Missouri or Iowa used ferries and steamboats (fitted out for ferry duty) to cross into towns in Nebraska. Several towns in Nebraska were used as "jumping off places" with Omaha eventually becoming a favorite after about 1855. Fort Kearny (est. 1848) is about 200 mi from the Missouri River, and the trail and its many offshoots nearly all converged close to Fort Kearny as they followed the Platte River west. The army maintained fort was the first chance on the trail to buy emergency supplies, do repairs, get medical aid, or mail a letter. Those on the north side of the Platte could usually wade the shallow river if they needed to visit the fort.
The Platte River and the North Platte River in the future states of Nebraska and Wyoming typically had many channels and islands and were too shallow, crooked, muddy and unpredictable for travel even by canoe. The Platte as it pursued its braided paths to the Missouri River was "too thin to plow and too thick to drink". While unusable for transportation, the Platte River and North Platte River valleys provided an easily passable wagon corridor going almost due west with access to water, grass, buffalo, and buffalo chips for fuel. The trails gradually got rougher as it progressed up the North Platte. There were trails on both sides of the muddy rivers. The Platte was about 1 mi wide and 2 to deep. The water was silty and bad tasting but it could be used if no other water was available. Letting it sit in a bucket for an hour or so or stirring in a 1/4 cup of cornmeal allowed most of the silt to settle out. Those traveling south of the Platte crossed the South Platte River with its muddy and treacherous crossings using one of about three ferries (in dry years it could sometimes be forded without a ferry) before continuing up the North Platte River Valley to Fort Laramie in present-day Wyoming. After crossing over the South Platte the travelers encountered Ash Hollow with its steep descent down Windlass Hill.
In the spring in Nebraska and Wyoming the travelers often encountered fierce wind, rain and lightning storms. Until about 1870 travelers encountered hundreds of thousands of bison migrating through Nebraska on both sides of the Platte River, and most travelers killed several for fresh meat and to build up their supplies of dried jerky for the rest of the journey. The prairie grass in many places was several feet high with only the hat of a traveler on horseback showing as they passed through the prairie grass. In many years the Indians fired much of the dry grass on the prairie every fall so the only trees or bushes available for firewood were on islands in the Platte River. Travelers gathered and ignited dried cow dung to cook their meals. These burned fast in a breeze, and it could take two or more bushels of chips to get one meal prepared. Those traveling south of the Platte crossed the South Platte fork at one of about three ferries (in dry years it could be forded without a ferry) before continuing up the North Platte River Valley into present-day Wyoming heading to Fort Laramie. Before 1852 those on the north side of the Platte crossed the North Platte to the south side at Fort Laramie. After 1852 they used Child's Cutoff to stay on the north side to about the present day town of Casper, Wyoming, where they crossed over to the south side.
Notable landmarks in Nebraska include Courthouse and Jail Rocks, Chimney Rock, Scotts Bluff, and Ash Hollow State Historical Park.
Today much of the Oregon Trail follows roughly along Interstate 80 from Wyoming to Grand Island, Nebraska. From there U.S. Highway 30 which follows the Platte River is a better approximate path for those traveling the north side of the Platte. The National Park Service (NPS) gives traveling advice for those who want to follow other branches of the trail.
Cholera on the Platte River.
Because of the Platte's brackish water, the preferred camping spots were along one of the many fresh water streams draining into the Platte or the occasional fresh water spring found along the way. These preferred camping spots became sources of cholera in the epidemic years (1849–1855) as many thousands of people used the same camping spots with essentially no sewage facilities or adequate sewage treatment. One of the side effects of cholera is acute diarrhea which helps contaminate even more water unless it is isolated and/or treated. The cause of cholera, ingesting the "Vibrio cholerae" bacterium from contaminated water, and the best treatment for cholera infections were unknown in this era. Thousands of travelers on the combined California, Oregon, and Mormon trails succumbed to cholera between 1849 and 1855. Most were buried in unmarked graves in Kansas, Nebraska and Wyoming. There are many cases cited involving people who were alive and apparently healthy in the morning and dead by nightfall.
Colorado.
A branch of the Oregon trail crossed the very northeast corner of Colorado if they followed the South Platte River to one of its last crossings. This branch of the trail passed through present day Julesburg before entering Wyoming. Later settlers followed the Platte and South Platte Rivers into their settlements there (much of which became the state of Colorado).
Wyoming.
After crossing the South Platte River the Oregon Trail follows the North Platte River out of Nebraska into Wyoming. Fort Laramie, at the confluence of the Laramie and North Platte rivers, was a major stopping point. Fort Laramie was a former fur trading outpost originally named Fort John that was purchased in 1848 by the U.S. Army to protect travelers on the trails. It was the last army outpost till travelers reached the coast.
Fort Laramie was the end of most cholera outbreaks which killed thousands along the lower Platte and North Platte from 1849 to 1855. Spread by cholera bacteria in fecal contaminated water, cholera caused massive diarrhea, leading to dehydration and death. In those days its cause and treatment were unknown, and it was often fatal—up to 30 percent of infected people died. It is believed that the swifter flowing rivers in Wyoming helped prevent the germs from spreading.
After crossing the South Platte the trail continues up the North Platte River, crossing many small swift-flowing creeks. As the North Platte veers to the south, the trail crosses the North Platte to the Sweetwater River Valley, which heads almost due west. Independence Rock is on the Sweetwater River. The Sweetwater would have to be crossed up to nine times before the trail crosses over the Continental Divide at South Pass, Wyoming. From South Pass the trail continues southwest crossing Big Sandy Creek—about 10 ft wide and 1 foot deep—before hitting the Green River. Three to five ferries were in use on the Green during peak travel periods. The deep, wide, swift and treacherous Green River, which eventually empties into the Colorado River, was usually at high water in July and August, and it was a dangerous crossing. After crossing the Green the main trail continues on in an approximate southwest direction until it encounters the Blacks Fork of the Green River and Fort Bridger. From Fort Bridger the Mormon Trail continued southwest following the upgraded Hastings Cutoff through the Wasatch Mountains. From Fort Bridger, the main trail, comprising several variants, veered northwest over the Bear River Divide and descended to the Bear River Valley. The trail turned north following the Bear River past the terminus of the Sublette-Greenwood Cutoff at Smiths Fork and on to the Thomas Fork Valley at the present Wyoming–Idaho border.
Over time, two major heavily used cutoffs were established in Wyoming. The Sublette-Greenwood Cutoff was established in 1844 and cut about 70 mi off the main route. It leaves the main trail about 10 mi west of South Pass and heads almost due west crossing Big Sandy Creek and then about 45 mi of waterless, very dusty desert before reaching the Green River near the present town of La Barge. Ferries here transferred them across the Green River. From there the Sublette-Greenwood Cutoff trail had to cross a mountain range to connect with the main trail near Cokeville in the Bear River Valley.
The Lander Road, formally the Fort Kearney, South Pass, and Honey Lake Wagon Road, was established and built by U.S. government contractors in 1858–59. It was about 80 mi shorter than the main trail through Fort Bridger with good grass, water, firewood and fishing but it was a much steeper and rougher route, crossing three mountain ranges. In 1859, 13,000 of the 19,000 emigrants traveling to California and Oregon used the Lander Road. The traffic in later years is undocumented.
The Lander Road departs the main trail at Burnt Ranch near South Pass, crosses the Continental Divide north of South Pass and reaches the Green River near the present town of Big Piney, Wyoming. From there the trail followed Big Piney Creek west before passing over the 8800 ft Thompson Pass in the Wyoming Range. It then crosses over the Smith Fork of the Bear River before ascending and crossing another 8200 ft pass on the Salt River Range of mountains and then descending into Star Valley. It exited the mountains near the present Smith Fork road about 6 mi south of the town of Smoot. The road continued almost due north along the present day Wyoming–Idaho western border through Star Valley. To avoid crossing the Salt River (which drains into the Snake River) which runs down Star Valley the Lander Road crossed the river when it was small and stayed west of the Salt River. After traveling down the Salt River Valley (Star Valley) about 20 mi north the road turned almost due west near the present town of Auburn, and entered into the present state of Idaho along Stump Creek. In Idaho it followed the Stump Creek valley northwest until it crossed the Caribou Mountains and proceeded past the south end of Grays Lake. The trail then proceeded almost due west to meet the main trail at Fort Hall; alternatively, a branch trail headed almost due south to meet the main trail near the present town of Soda Springs.
Numerous landmarks are located along the trail in Wyoming including Independence Rock, Ayres Natural Bridge and Register Cliff.
Utah.
In 1847, Brigham Young and the Mormon pioneers departed from the Oregon Trail at Fort Bridger in Wyoming and followed (and much improved) the rough trail originally recommended by Lansford Hastings to the Donner Party in 1846 through the Wasatch Mountains into Utah. After getting into Utah they immediately started setting up irrigated farms and cities—including Salt Lake City. In 1848, the Salt Lake Cutoff was established by Sam Hensley, and returning members of the Mormon Battalion providing a path north of the Great Salt Lake from Salt Lake City back to the California and Oregon trails. This cutoff rejoined the Oregon and California Trails near the City of Rocks near the Utah–Idaho border and could be used by both California and Oregon bound travelers. Located about half way on both the California and Oregon trails many thousands of later travelers used Salt Lake City and other Utah cities as an intermediate stop for selling or trading excess goods or tired livestock for fresh livestock, repairs, supplies or fresh vegetables. The Mormons looked on these travelers as a welcome bonanza as setting up new communities from scratch required nearly everything the travelers could afford to part with. The overall distance to California or Oregon was very close to the same whether one "detoured" to Salt Lake City or not. For their own use and to encourage California and Oregon bound travelers the Mormons improved the Mormon Trail from Fort Bridger and the Salt Lake Cutoff trail. To raise much needed money and facilitate travel on the Salt Lake Cutoff they set up several ferries across the Weber, Bear, and Malad rivers, which were used mostly by travelers bound for Oregon or California.
Idaho.
The main Oregon and California Trail went almost due north from Fort Bridger to the Little Muddy Creek where it passed over the Bear River Mountains to the Bear River Valley, which it followed northwest into the Thomas Fork area, where the trail crossed over the present day Wyoming line into Idaho. In the Eastern Sheep Creek Hills in the Thomas Fork valley the emigrants encountered Big Hill. Big Hill was a detour caused by a then-impassable cut the Bear River made through the mountains and had a tough ascent often requiring doubling up of teams and a very steep and dangerous descent. (Much later, U.S. Highway 30, using modern explosives and equipment, was built through this cut). In 1852 Eliza Ann McAuley found and with help developed the McAuley Cutoff which bypassed much of the difficult climb and descent of Big Hill. About 5 mi on they passed present-day Montpelier, Idaho, which is now the site of the National Oregon-California Trail Center. The trail follows the Bear River northwest to present-day Soda Springs. The springs here were a favorite attraction of the pioneers who marveled at the hot carbonated water and chugging "steamboat" springs. Many stopped and did their laundry in the hot water as there was usually plenty of good grass and fresh water available. Just west of Soda Springs the Bear River turns southwest as it heads for the Great Salt Lake, and the main trail turns northwest to follow the Portneuf River valley to Fort Hall, Idaho. Fort Hall was an old fur trading post located on the Snake River. It was established in 1832 by Nathaniel Jarvis Wyeth and company and later sold in 1837 to the Hudson's Bay Company. At Fort Hall nearly all travelers were given some aid and supplies if they were available and needed. Mosquitoes were constant pests, and travelers often mention that their animals were covered with blood from the bites. The route from Fort Bridger to Fort Hall is about 210 mi, taking nine to twelve days.
At Soda Springs was one branch of Lander Road (established and built with government contractors in 1858), which had gone west from near South Pass, over the Salt River Mountains and down Star Valley before turning west near present-day Auburn, Wyoming, and entering Idaho. From there it proceeded northwest into Idaho up Stump Creek canyon for about 10 mi. One branch turned almost 90 degrees and proceeded southwest to Soda Springs. Another branch headed almost due west past Gray's Lake to rejoin the main trail about 10 mi west of Fort Hall.
On the main trail about 5 mi west of Soda Springs Hudspeth's Cutoff (established 1849 and used mostly by California trail users) took off from the main trail heading almost due west, bypassing Fort Hall. It rejoined the California Trail at Cassia Creek near the City of Rocks. Hudspeth's Cutoff had five mountain ranges to cross and took about the same amount of time as the main route to Fort Hall, but many took it thinking it was shorter. Its main advantage was that it helped spread out the traffic during peak periods, making more grass available.
West of Fort Hall the main trail traveled about 40 mi on the south side of the Snake River southwest past American Falls, Massacre Rocks, Register Rock, and Coldwater Hill near present-day Pocatello, Idaho. Near the junction of the Raft River and Snake River the California Trail diverged from the Oregon Trail at another Parting of the Ways junction. Travellers left the Snake River and followed Raft River about 65 mi southwest past present day Almo. This trail then passed through the City of Rocks and over Granite Pass where it went southwest along Goose Creek, Little Goose Creek, and Rock Spring Creek. It went about 95 mi through Thousand Springs Valley, West Brush Creek, and Willow Creek, before arriving at the Humboldt River in northeastern Nevada near present-day Wells. The California Trail proceeded west down the Humboldt before reaching and crossing the Sierra Nevadas.
There were only a few places where the Snake River was not buried deep in a canyon, and few spots where the river slowed down enough to make a crossing possible. Two of these fords were near Fort Hall, where travelers on the Oregon Trail North Side Alternate (established about 1852) and Goodale's Cutoff (established 1862) crossed the Snake to travel on the north side. Nathaniel Wyeth, the original founder of Fort Hall in 1834, writes in his diary that they found a ford across the Snake River 4 mi southwest of where he founded Fort Hall. Another possible crossing was a few miles upstream of Salmon Falls where some intrepid travelers floated their wagons and swam their stock across to join the north side trail. Some lost their wagons and teams over the falls. The trails on the north side joined the trail from Three Island Crossing about 17 mi west of Glenns Ferry on the north side of the Snake River.
Goodale's Cutoff, established in 1862 on the north side of the Snake River, formed a spur of the Oregon Trail. This cutoff had been used as a pack trail by Indians and fur traders, and emigrant wagons traversed parts of the eastern section as early as 1852. After crossing the Snake River the 230 mi cutoff headed north from Fort Hall toward Big Southern Butte following the Lost River part of the way. It passed near the present-day town of Arco, and wound through the northern part of what is now Craters of the Moon National Monument. From there it went southwest to Camas Prairie and ended at Old Fort Boise on the Boise River. This journey typically took two to three weeks and was noted for its very rough lava terrain and extremely dry climate, which tended to dry the wooden wheels on the wagons, causing the iron rims to fall off the wheels. Loss of wheels caused many wagons to be abandoned along the route. It rejoined the main trail east of Boise. Goodale's Cutoff is visible at many points along U.S. Highway 20, U.S. Highway 26, and U.S. Highway 93 between Craters of the Moon National Monument and Carey.
From the present site of Pocatello, the trail proceeded almost due west on the south side of the Snake River for about 180 mi. This route passed Cauldron Linn rapids, Shoshone Falls, two falls near the present city of Twin Falls, and Upper Salmon Falls on the Snake River. At Salmon Falls there were often a hundred or more Indians fishing who would trade for their salmon, a welcome treat.
The trail continued west to Three Island Crossing (near present-day Glenns Ferry.) Here most emigrants used the divisions of the river caused by three islands to cross the difficult and swift Snake River by ferry or by driving or sometimes floating their wagons and swimming their teams across. The crossings were doubly treacherous because there were often hidden holes in the river bottom which could overturn the wagon or entangle the team, sometimes with fatal consequences. Before ferries were established there were several drownings here nearly every year.
The north side of the Snake had better water and grass than the south. The trail from Three Island Crossing to Old Fort Boise was about 130 mi long. The usually lush Boise River Valley was a welcome relief. The next crossing of the Snake River was near Old Fort Boise. This last crossing of the Snake could be done on bull boats while swimming the stock across. Others would chain a large string of wagons and teams together. The theory was that the front teams, usually oxen, would get out of water first and with good footing help pull the whole string of wagons and teams across. How well this worked in practice is not stated. Often young Indian boys were hired to drive and ride the stock across the river—they knew how to swim, unlike many pioneers. Today's Idaho Interstate 84 roughly follows the Oregon Trail until it leaves the Snake River near Burley. From there Interstate 86 to Pocatello roughly approximates the trail. Highway 30 roughly follows the path of the Oregon Trail from there to Montpelier.
Starting in about 1848 the South Alternate of Oregon Trail (also called the Snake River Cutoff) was developed as a spur off the main trail. It bypassed the Three Island Crossing and continued traveling down the south side of the Snake River. It rejoined the trail near present-day Ontario, Oregon. It hugged the southern edge of the Snake River canyon and was a much rougher trail with poorer water and grass, requiring occasional steep descents and ascents with the animals down into the Snake River canyon to get water. Travellers on this route avoided two dangerous crossings of the Snake River. Today's Idaho State Route 78 roughly follows the path of the South Alternate route of the Oregon Trail.
In 1869 the Central Pacific established Kelton, Utah as a railhead and the terminus of the western mail was moved from Salt Lake City. The Kelton Road became important as a communication and transportation road to the Boise Basin.
Boise has 21 monuments in the shape of obelisks along its portion of the Oregon Trail.
Oregon.
Once across the Snake River ford near Old Fort Boise the weary travelers traveled across what would become the state of Oregon. The trail then went to the Malheur River and then past Farewell Bend on the Snake River, up the Burnt River canyon and northwest to the Grande Ronde Valley near present day La Grande before coming to the Blue Mountains. In 1843 settlers cut a wagon road over these mountains making them passable for the first time to wagons. The trail went to the Whitman Mission near Fort Nez Perces in Washington until 1847 when the Whitmans were killed by Native Americans. At Fort Nez Perce some built rafts or hired boats and started down the Columbia; others continued west in their wagons until they reached The Dalles. After 1847 the trail bypassed the closed mission and headed almost due west to present day Pendleton, Oregon, crossing the Umatilla River, John Day River, and Deschutes River before arriving at The Dalles. Interstate 84 in Oregon roughly follows the original Oregon Trail from Idaho to The Dalles.
Arriving at the Columbia at The Dalles and stopped by the Cascade Mountains and Mount Hood, some gave up their wagons or disassembled them and put them on boats or rafts for a trip down the Columbia River. Once they transited the Cascade's Columbia River Gorge with its multiple rapids and treacherous winds they would have to make the 1.6 mi portage around the Cascade Rapids before coming out near the Willamette River where Oregon City was located. The pioneer's livestock could be driven around Mount Hood on the narrow, crooked and rough Lolo Pass.
Several Oregon Trail branches and route variations led to the Willamette Valley. The most popular was the Barlow Road, which was carved though the forest around Mount Hood from The Dalles in 1846 as a toll road at $5 per wagon and 10 cents per head of livestock. It was rough and steep with poor grass but still cheaper and safer than floating goods, wagons and family down the dangerous Columbia River.
In Central Oregon, there was the Santiam Wagon Road (established 1861), which roughly parallels Oregon Highway 20 to the Willamette Valley. The Applegate Trail (established 1846), cutting off the California Trail from the Humboldt River in Nevada, crossed part of California before cutting north to the south end of the Willamette Valley. U.S. Route 99 and Interstate 5 through Oregon roughly follow the original Applegate Trail.
Travel equipment.
Wagons.
Oregon Trail was too long and arduous for the standard Conestoga wagons commonly used at that time in the eastern United States and on the Santa Fe Trail. Their 6000 lb freight capacity was larger than needed, and the large teams (8 to 10 animals) these wagons required could not navigate the tight corners often found on the Oregon Trail.
This led to the rapid development of prairie schooners. This wagon was approximately half the size of the larger Conestoga, weighed about 1300 lb empty, with about 2500 lb of capacity and about 88 ft3 of storage space in a box 11 ft long, 4 ft wide, and 2 ft high. These wagons could easily be pulled by four to six oxen or six to ten mules. Extra animals were often recommended because animals could stray or become injured or die on the trip. Often late in the trip mixed teams that included dairy cows and riding ponies were sometimes hitched up to make a usable team. The wagons were manufactured in quantity by companies like Studebaker, with new wagons costing between $85 and $170. The cotton canvas covers of the wagons were doubled and treated with linseed oil to help keep out the rain, dust and wind, though the covers tended to leak rain and dust eventually. The typical wagon with 40 to diameter wheels could easily move over rough ground and rocks without high centering and even over most tree stumps if required. The wooden wheels were protected with an iron rim typically about 1.5 in wide. These iron tires were installed hot so they would shrink tightly onto the wood wheel when they cooled. Nevertheless it was advisable to soak the wheel in water periodically as the desert air could dry the wheel so much that the iron tire would fall off. In practice, it was found that the standard farm wagon built by a company or wagon maker (wainwright) of good reputation usually worked almost as well as prairie schooners and had only to be fitted with wooden bows and a canvas cover to be ready. Wagons were generally reliable if maintained, but they sometimes broke down and had to be repaired or abandoned along the way. Broken axles and broken wagon tongues were two of the most common problems. Due to space constraints (maximum of three each of wagon tongues, wheels, and axles per wagon,) replacements were often created out of whatever wood was available. Abandoned wagons were typically scavenged for needed parts. One wagon could carry enough food for six months' travel for four or five travelers as well as a short list of household and luxury items including clothing and ammunition.
It is estimated that about 70 percent (or more) of the wagons traveling west were pulled by oxen; mule teams were a strong second choice at 20 to 30 percent, and initially there were almost no horse-pulled wagons. This was true for many reasons. An ox team was about 10 percent slower than a mule or horse-pulled wagon—about 2 to. However, they were cheaper to buy ($25 to $85 per yoke versus up to $600 or more for six horses), easier to train, could pull more, survived better on the sparse grass often found along the trail, did not require oats or grain, and were often tamer and easier to handle after they were trained. Novices could usually learn to handle a trained ox team in about a week. Oxen could usually be turned loose at night and easily rounded up in the mornings. Mules and horses typically required herding day and night and often had to be staked out on a rope or hobbled. Oxen were usually easier to find and catch, and the Indians were usually less interested in stealing them. Mules were the second choice. They were about as fast as a horse and could survive well on the grazing found along the way, and worked well when trained. Trained mules were hard to find, and mules were difficult to handle until trained by an experienced mule skinner, which could take two months. In later years, horses were chosen more often because they were about 10% faster and the oats and grain required to keep them fit for months of continuous work could by then be bought along the way.
The ox drivers walked alongside the left side of their oxen team and used the voice commands "gee" (right) and "haw" (left) and a whip to guide them. Mules were often guided by riding one that was hooked to the wagon (typically the left hand wheel mule) and handling the reins from there. Whips were seldom used to actually whip the animals but were used to get the animal's attention by snapping them in the air.
Food.
The recommended amount of food to take per person was 150 lb of flour, 20 lb of corn meal, 50 lb of bacon, 40 lb of sugar, 10 lb of coffee, 15 lb of dried fruit, 5 lb of salt, half a pound (0.25 kg) of saleratus (baking soda, baking powder leavening mix), 2 lb of tea, 5 lb of rice, and 15 lb of beans. These provisions were usually kept in water-tight containers or barrels to minimize spoilage. The usual meal for breakfast, lunch and dinner along the trail was bacon, beans, and coffee, with biscuits or bread. The typical cost of food for four people for six months was about $150.
The amount of food required was lessened if beef cattle, calves or sheep were taken for a walking food supply. Before the 1870s, there were vast herds of buffalo in Nebraska, which provided fresh meat and jerky for the trip. In general, wild game could not be depended on for a regular source of food, but when found it was relished as a welcome change in a monotonous diet. Travelers could hunt antelope, buffalo, sage hens, trout, and occasionally elk, bear, duck, geese, salmon and deer along the trail. Most travelers carried a rifle or shotgun and ammunition for hunting game and for protection against snakes and Indian attacks. When they got to the Snake River and Columbia River areas they would often trade with the Indians for salmon. The Indians in Oregon traded potatoes and other vegetables they had learned to grow from the missionaries. Some families took along milk cows, goats, and chickens (penned in crates tied to the wagons). Additional food like pickles, canned butter, cheese or pickled eggs were occasionally carried, but canned goods were expensive and food preservation was primitive, so few items could be safely kept for the four- to six-month duration of the trip.
Cooking along the trail was done over a campfire. Fuels used were wood, buffalo chips, willow or sagebrush. Flint and steel were used to start fires. Some carried matches in water-tight containers. Fire was borrowed from a neighbor for ease of starting. Cooking required simple cooking utensils such as butcher knives, large spoons, spatulas, ladles, Dutch ovens, pots and pans, grills, spits, coffee pots and an iron tripod to suspend the pans and pots over the fire. Some brought small stoves, but these were often jettisoned along the way as being too heavy and unnecessary. Wooden or canvas buckets were brought for carrying water, and most travelers carried canteens or water bags for daily use. A ten-gallon water barrel was needed, but it was usually kept nearly empty to minimize weight (some water had to be kept in it to prevent it from drying out and losing its water tightness). It was only filled for long waterless stretches. Some brought a new invention: an India Rubber combination mattress and water carrier.
Clothing and equipment.
Tobacco was popular, both for personal use and for trading with Indians and other pioneers. Each person brought at least two changes of clothes and multiple pairs of boots (two to three pairs often wore out on the trip). About 25 pounds of soap was recommended for a party of four for bathing and washing clothes. A washboard and tub was usually brought for washing clothes. Wash days typically occurred once or twice a month or less, depending on availability of good grass, water and fuel. Most wagons carried tents for sleeping, though in good weather most would sleep outside. A thin fold-up mattress, blankets, pillows, canvas or rubber gutta percha ground covers were used for sleeping. Sometimes an unfolded feather bed mattress was brought for the wagon if there were pregnant women or very young children along. The wagons had no springs, and the ride along the trail was very rough. Despite modern depictions, almost nobody actually rode in the wagons; it was too dusty, too rough, and too hard on the livestock.
Travelers brought books, Bibles, trail guides, and writing quills, ink and paper for letters (about one in 200 kept a diary).
Belt and folding knives were carried by nearly all men and boys. Awls, scissors, pins, needles and thread for mending were required. Spare leather was used for repairs to shoes, harnesses, and other equipment. Some used goggles to keep dust out of the eyes. Storage boxes were ideally the same height so they could be arranged to give a flat surface inside the wagon for a sleeping platform.
Saddles, bridles, hobbles, and ropes were needed if the party had a horse or riding mule, and many men did. Extra harnesses and spare wagon parts were often carried. Most carried steel shoes for oxen, mules or horses. Tar was carried to help repair an ox's injured hoof.
Goods, supplies and equipment were often shared by fellow travelers. Items that were forgotten, broken or worn out could be bought from a fellow traveler, post or fort along the way. New iron shoes for horses, mules and oxen were put on by blacksmiths found along the way. Equipment repairs and other goods could be procured from blacksmith shops established at some forts and some ferries. Emergency supplies, repairs and livestock were often provided by local residents in Oregon, California, and Utah for late travelers on the trail who were hurrying to beat the snow.
Non-essential items were often abandoned to lighten the load, or in case of emergency. Many travelers would salvage discarded items, picking up essentials or leaving behind their lower quality item when a better one was found abandoned along the road. Some profited by collecting discarded items and hauling them back to jumping off places and reselling them. In the early years, Mormons sent scavenging parties back along the trail to salvage as much iron and other supplies as possible and haul it to Salt Lake City, where supplies of all kinds were needed. Others would use discarded wagons, wheels and furniture as firewood. During the 1849 gold rush, Fort Laramie was known as "Camp Sacrifice" because of the large amounts merchandise discarded nearby. Travelers had pushed along the relatively easy path to Fort Laramie with their luxury items, but discarded them before the difficult mountain crossing ahead and after discovering that many items could be purchased at the forts or located for free along the way. Some travelers carried their excess goods to Salt Lake City to be sold.
Professional tools used by blacksmiths, carpenters, and farmers were carried by nearly all. Shovels, crow bars, picks, hoes, mattocks, saws, hammers, axes and hatchets were used to clear or make a road through trees or brush, cut down the banks to cross a wash or steep banked stream, build a raft or bridge, or repair the wagon. In general, as little road work as possible was done. Travel was often along the top of ridges to avoid the brush and washes common in many valleys.
Statistics.
Overall, some 268,000 pioneers used the Oregon Trail and its three primary off-shoots, the California, Bozeman, and Mormon trails to reach the West Coast, 1840-60. Another 48,000 headed to Utah. There is no estimate on how many used it to return East.
Emigrants.
Some of the trail statistics for the early years were recorded by the U.S. Army at Fort Laramie, Wyoming, from about 1849 to 1855. None of these original statistical records have been found—the Army lost them or destroyed them. There are only some partial written copies of the Army records and notes recorded in several diaries. Emigration to California spiked considerably with the 1849 gold rush. Following the discovery of gold, California remained the destination of choice for most emigrants on the trail up to 1860, with almost 200,000 people traveling there between 1849 and 1860.
Travel diminished after 1860 as the Civil War caused considerable disruptions on the trail. Many of the people on the trail in 1861–1863 were fleeing the war and its attendant drafts in both the south and the north. Trail historian Merrill J. Mattes has estimated the number of emigrants for 1861–1867 given in the total column of the above table. But these estimates may well be low since they only amount to an extra 125,000 people, and the 1870 census shows that over 200,000 additional people (ignoring most of California's population increase which had an excellent sea and rail connections across Panama by then) showed up in all the states served by the California/Oregon/Mormon/Bozeman Trail(s) and its offshoots. Mormon emigration records after 1860 are reasonably accurate as newspaper and other accounts in Salt Lake City give most of the names of emigrants arriving each year from 1847 to 1868. Gold and silver strikes in Colorado, Oregon, Idaho, Nevada and Montana caused a considerable increase in people using the trails, often in directions different from the original trail users.
Though the numbers are significant in the context of the times, far more people chose to remain at home in the 31 states. Between 1840 and 1860, the population of the United States rose by 14 million, yet only about 300,000 decided to make the trip. Many that went were between the ages 12 and 24. Between 1860 and 1870, the U.S. population increased by seven million, with about 350,000 of this increase being in the Western states. Many were discouraged by the cost, effort and danger of the trip. Western scout Kit Carson is thought to have said, "The cowards never started and the weak died on the way", though the general saying was written by Joaquin Miller, in reference to the California gold rush. According to several sources, 3 to 10 percent of the emigrants are estimated to have perished on the way west.
Western census data.
These census numbers show a 363,000 population increase in the western states and territories between 1860 and 1870. Some of this increase is because of a high birth rate in the western states and territories but most is from emigrants moving from the east to the west and new immigration from Europe. Much of the increase in California and Oregon is from emigration by ship as there were fast and reasonably low cost transportation via east and west coast steamships and the Panama Railroad after 1855. The census numbers imply at least 200,000 emigrants (or more) used some variation of the California/Oregon/Mormon/Bozeman trails to get to their new homes between 1860 and 1870.
Costs.
The cost of traveling over the Oregon Trail and its extensions varied from nothing to a few hundred dollars per person. Women seldom went alone. The cheapest way was to hire on to help drive the wagons or herds, allowing one to make the trip for nearly nothing or even make a small profit. Those with capital could often buy livestock in the midwest and drive the stock to California or Oregon for profit. About 60 to 80 percent of the travelers were farmers and as such already owned a wagon, livestock team, and many of the necessary supplies. This lowered the cost of the trip to about $50 per person for food and other items. Families planned the trip months in advance and made many of the extra clothing and other items needed. Individuals buying most of the needed items would end up spending between $150–$200 per person. As the trail matured, additional costs for ferries and toll roads were thought to have been about $30 per car.
Deaths.
The route west was arduous and with many dangers, but the number of deaths on the trail is not known with any precision; there are only wildly varying estimates. Estimating is difficult because of the common practice of burying people in unmarked graves that were intentionally disguised to avoid them being dug up by animals or Indians. Graves were often put in the middle of a trail and then run over by the livestock to make them difficult to find. Disease was the main killer of trail travelers; cholera killed up to 3 percent of all travelers in the epidemic years from 1849 to 1855.
Indian attacks increased significantly after 1860 when most of the army troops were withdrawn and miners and ranchers began fanning out all over the country, often encroaching on Indian territory. Increased attacks along the Humboldt led to most travelers taking the Central Nevada Route. The Goodall cutoff was developed in Idaho in 1862 which kept Oregon bound travelers away from much of the Indian trouble nearer the Snake River. Other trails were developed that traveled further along the South Platte to avoid local Indian hot spots.
Other common causes of death included hypothermia, drowning in river crossings, getting run over by wagons, and accidental gun deaths. Later, more family groups started traveling as well as many more ferries and bridges were being put in, and fording a dangerous river became much less common and dangerous. Surprisingly few people were taught to swim in this era. Being run over was a major cause of death, despite the wagons only averaging 2–3 miles per hour. The wagons could not easily be stopped, and people, particularly children, were often trying to get on and off the wagons while they were moving—not always successfully. Another hazard was a dress getting caught in the wheels and pulling the person under. Accidental shootings declined significantly after Fort Laramie as people became more familiar with their weapons and often just left them in their wagons. Carrying around a ten-pound rifle all day soon became tedious and usually unnecessary as the perceived Indian threat faded and hunting opportunities receded.
A significant number of travelers were suffering from scurvy by the end of their trips. Their typical flour and salted pork/bacon diet had very little vitamin C in it. The diet in the mining camps was also typically low in fresh vegetables and fruit, which indirectly led to early deaths of many of the inhabitants. Some believe that scurvy deaths may have rivaled cholera as a killer, with most deaths occurring after the victim reached California.
Miscellaneous deaths included deaths by homicides, lightning strikes, childbirth, stampedes, snake bites, flash floods, falling trees, and kicks by animals. According to an evaluation by John Unruh, a 4 percent death rate or 16,000 out of 400,000 total pioneers on all trails may have died on the trail.
Other trails west.
There were other possible migration paths for early settlers, miners, or travelers to California or Oregon besides the Oregon trail prior to the establishment of the transcontinental railroads.
From 1821–1846, the Hudson's Bay Company twice annually used the York Factory Express overland trade route from Fort Vancouver to Hudson Bay then on to London. James Sinclair led a large party of nearly 200 settlers from the Red River Colony in 1841. These northern routes were largely abandoned after Britain ceded its claim to the southern Columbia River basin by way of the Oregon Treaty of 1846.
The longest trip was the voyage of about 13600 to on an uncomfortable sailing ship rounding the treacherous, cold, and dangerous Cape Horn between Antarctica and South America and then sailing on to California or Oregon. This trip typically took four to seven months (120 to 210 days) and cost about $350 to $500. The cost could be reduced to zero if you signed on as a crewman and worked as a common seaman. The hundreds of abandoned ships, whose crews had deserted in San Francisco Bay in 1849–50, showed many thousands chose to do this.
Other routes involved taking a ship to Colón, Panama (then called Aspinwall) and a strenuous, disease ridden, five- to seven-day trip by canoe and mule over the Isthmus of Panama before catching a ship from Panama City, Panama to Oregon or California. This trip could be done from the east coast theoretically in less than two months if all ship connections were made without waits and typically cost about $450/person. Catching a fatal disease was a distinct possibility as Ulysses S. Grant in 1852 learned when his unit of about 600 soldiers and some of their dependents traversed the Isthmus and lost about 120 men, women, and children. This passage was considerably sped up and made safer in 1855 when the Panama Railroad was completed at terrible cost in money and life across the Isthmus. The once treacherous 50 mi trip could be done in less than a day. The time and the cost for transit dropped as regular paddle wheel steamships and sailing ships went from ports on the east coast and New Orleans, Louisiana, to Colón, Panama ($80–$100), across the Isthmus of Panama by railroad ($25) and by paddle wheel steamships and sailing ships to ports in California and Oregon ($100–$150).
Another route established by Cornelius Vanderbilt in 1849 was across Nicaragua. The 120 mi long San Juan River to the Atlantic Ocean helps drain the 100 mi long Lake Nicaragua. From the western shore of Lake Nicaragua it is only about 12 mi to the Pacific Ocean. Vanderbilt decided to use paddle wheel steam ships from the U.S. to the San Juan River, small paddle wheel steam launches on the San Juan River, boats across Lake Nicaragua, and a stage coach to the Pacific where connections could be made with another ship headed to California, Oregon, etc.. Vanderbilt, by undercutting fares to the Isthmus of Panama and stealing many of the Panama Railroad workers, managed to attract roughly 30% of the California bound steam boat traffic. All his connections in Nicaragua were never completely worked out before the Panama Railroad's completion in 1855. Civil strife in Nicaragua and a payment to Cornelius Vanderbilt of a "non-compete" payment (bribe) of $56,000 per year killed the whole project in 1855.
Another possible route consisted of taking a ship to Mexico traversing the country and then catching another ship out of Acapulco, Mexico to California etc. This route was used by some adventurous travelers but was not too popular because of the difficulties of making connections and the often hostile population along the way.
The Gila Trail going along the Gila River in Arizona, across the Colorado River and then across the Sonora Desert in California was scouted by Stephen Kearny's troops and later by Captain Philip St. George Cooke's Mormon Battalion in 1846 who were the first to take a wagon the whole way. This route was used by many gold hungry miners in 1849 and later but suffered from the disadvantage that you had to find a way across the very wide and very dry Sonora Desert. It was used by many in 1849 and later as a winter crossing to California, despite its many disadvantages.
Running from 1857 to 1861, the Butterfield Stage Line won the $600,000/yr. U.S. mail contract to deliver mail to San Francisco, California. As dictated by southern Congressional members, the 2800 mi route ran from St. Louis, Missouri through Arkansas, Oklahoma Indian Territory, Texas, New Mexico Territory, and across the Sonora Desert before ending in San Francisco, California. Employing over 800 at its peak, it used 250 Concord Stagecoaches seating 12 very crowded passengers in three rows. It used 1,800 head of stock, horses and mules and 139 relay stations to ensure the stages ran day and night. A one way fare of $200 delivered a very thrashed and tired passenger into San Francisco in 25 to 28 days. After traveling the route, "New York Herald" reporter Waterman Ormsby said, "I now know what Hell is like. I've just had 24 days of it."
Other ways to get to Oregon were: using the York Factory Express route across Canada, and down the Columbia River; ships from Hawaii, San Francisco, or other ports that stopped in Oregon; emigrants trailing up from California, etc. All provided a trickle of emigrants, but they were soon overwhelmed in numbers by the emigrants coming over the Oregon Trail.
The ultimate competitor arrived in 1868, the First Transcontinental Railroad, which cut travel time to about seven days at a low fare (economy) of about $60 (economy)
Legacy.
One of the enduring legacies of the Oregon Trail is the expansion of the United States territory to the West Coast. Without the many thousands of United States settlers in Oregon and California and thousands more on their way each year, it is highly unlikely that this would have occurred. The western expansion and the Oregon Trail in particular inspired many songs that told of the settlers' experiences. "Uncle Sam's Farm" encouraged east-coast dwellers to "Come right away. Our lands they are broad enough, so don't be alarmed. Uncle Sam is rich enough to give us all a farm." In "Western Country", the singer exhorts that "if I had no horse at all, I'd still be a hauling, far across those Rocky Mountains, goin' away to Oregon."
The story of the Oregon Trail inspired a popular educational computer game of the same name, "The Oregon Trail". The game became widely popular in the 1980s and early 1990s. Several sequels to the game were also released, such as "The Oregon Trail II", "The Yukon Trail", and "The Amazon Trail".
"The Oregon Trail" was a television series that ran from September 22 through October 26, 1977, on NBC. The show starred Rod Taylor, Tony Becker, Darleen Carr, Charles Napier, and Ken Swofford. Although the show was canceled after six episodes, the remaining seven episodes were later aired on BBC 2 in the United Kingdom.
In June 2010, the entire series was released on DVD in the USA by Timeless.

</doc>
<doc id="48713" url="http://en.wikipedia.org/wiki?curid=48713" title="St John's College, Cambridge">
St John's College, Cambridge

St John's College is a constituent college of the University of Cambridge. (The full formal name of the college is "The Master, Fellows and Scholars of the College of St John the Evangelist in the University of Cambridge".) The college was founded by Lady Margaret Beaufort. In constitutional terms, the college is an eleemosynary corporation established by Charter dated 9 April 1511. The aims of the college, as specified by its Statutes, are the promotion of education, religion, learning and research.
The college's alumni include nine Nobel Prize winners, six prime ministers of various countries, three archbishops, at least two princes, and three Saints.
St John's College is well known for its choir, for its members' participation in a wide variety of inter-collegiate sporting competitions, and for its annual May Ball.
In 2011 the college celebrated its quincentenary, an event marked by a visit of HM Queen Elizabeth II and HRH Prince Philip, Duke of Edinburgh.
History.
The college was founded on the site of the 13th century Hospital of St John in Cambridge at the suggestion of Saint John Fisher, Bishop of Rochester and chaplain to Lady Margaret. However, Lady Margaret died without having mentioned the foundation of St John's in her will, and it was largely the work of Fisher that ensured that the college was founded. He had to obtain the approval of King Henry VIII of England, the Pope through the intermediary Polydore Vergil, and the Bishop of Ely to suppress the religious hospital and convert it to a college. The college received its charter on 9 April 1511. Further complications arose in obtaining money from the estate of Lady Margaret to pay for the foundation and it was not until 22 October 1512 that a codicil was obtained in the court of the Archbishop of Canterbury. In November 1512 the Court of Chancery allowed Lady Margaret's executors to pay for the foundation of the college from her estates. When Lady Margaret's executors took over they found most of the old Hospital buildings beyond repair, but repaired and incorporated the Chapel into the new college. A kitchen and hall were added, and an imposing gate tower was constructed for the College Treasury. The doors were to be closed each day at dusk, sealing the monastic community from the outside world.
Over the course of the following five hundred years, the college expanded westwards towards the River Cam, and now has eleven courts, the most of any Oxford or Cambridge College. The first three courts are arranged in enfilade.
St John's College first admitted women in October 1981, when K. M. Wheeler was admitted to the fellowship, along with nine female graduate students. The first women undergraduates arrived a year later.
Buildings and grounds.
The Great Gate.
St John's distinctive Great Gate follows the standard contemporary pattern employed previously at Christ's College and Queens' College. The gatehouse is crenelated and adorned with the arms of the foundress Lady Margaret Beaufort. Above these are displayed her ensigns, the Red Rose of Lancaster and Portcullis. The college arms are flanked by curious creatures known as yales, mythical beasts with elephants' tails, antelopes' bodies, goats' heads, and swivelling horns. Above them is a tabernacle containing a socle figure of St John the Evangelist, an Eagle at his feet and symbolic, poisoned chalice in his hands. The fan vaulting above is contemporary with tower, and may have been designed by William Swayne, a master mason of King's College Chapel.
First Court.
First Court is entered via the Great Gate, and is highly architecturally varied. First Court was converted from the hospital on the foundation of the college, and constructed between 1511 and 1520. Though it has since been gradually changed, the front (east) range is still much as it appeared when first erected in the 16th-century. The south range was refaced between 1772–6 in the Georgian style by the local architect James Essex, as part of an abortive attempt to modernise the entire court in the same fashion. The most dramatic alteration to the original, Tudor court however remains the Victorian amendment of the north range, which involved the demolition of the original mediaeval chapel and the construction of a new, far larger set of buildings in the 1860s. These included the Chapel, designed by Sir George Gilbert Scott, which includes in its interior some pieces saved from the original chapel. It is the tallest building in Cambridge. The alteration of the north range necessitated the restructuring of the connective sections of First Court; another bay window was added to enlarge the college's hall, and a new building constructed to the north of Great Gate. Parts of First Court were used as a prison in 1643 during the English Civil War. In April 2011, Queen Elizabeth II visited St John's college to inaugurate a new pathway in First Court, which passes close to the ruins of the Old Chapel.
Dining Hall.
The college's Hall has a fine hammerbeam-roof, painted in black and gold and decorated with the armorial devices of its benefactors. The hall is lined to cill-level with linenfold panelling which dates from 1528–9, and has a five-bay screen, surmounted by the Royal Arms. Above is a hexagonal louvre, dating to 1703. The room was extended from five to eight bays according to designs by Sir George Gilbert Scott in 1863. It has two bay windows, containing heraldic glass dating from the fifteenth to nineteenth centuries. In 1564, Queen Elizabeth rode into the college's Hall on horseback, during a state visit to Cambridge.
Second Court.
Second Court, built from 1598 to 1602, has been described as 'the finest Tudor court in England'. Built atop the demolished foundations of an earlier, far smaller court, Second Court was begun in 1598 to the plans of Ralph Symons of Westminster, and Gilbert Wigge of Cambridge. Their original architectural drawings are housed in the college's library, and are the oldest surviving plans for an Oxford or Cambridge college building. It was financed by the Countess of Shrewsbury, whose arms and statue stand above the court's western gatehouse. The court's Oriel windows are perhaps its most striking feature, though the dominating Shrewsbury Tower to the west is undoubtedly the most imposing. This gatehouse, built as a mirror image of the college's Great Gate, contains a statue of the benefactress Mary Talbot, Countess of Shrewsbury, added in 1671. Behind the Oriel window of the north range lies the Long Gallery, a promenading room that was, prior to its segmentation, 148 feet long. In this room, the treaty between England and France was signed that established the marriage of King Charles I of England to Queen Henrietta Maria. In the 1940s, parts of the D-day landings were planned there. Second Court is also home to the college's famous 'triple set', K6.
The College Library.
The Old Library was built in 1624, largely with funds donated by John Williams, Bishop of Lincoln. Hearing of the college's urgent need for greater library space, Williams donated £1,200 anonymously, later revealing his identity and donating a total of £2,011 towards the library's total cost of £3,000. The Library's fine bay window overlooks the River Cam, and bears the letters ILCS on it, standing for Iohannes Lincolniensis Custos Sigilli, or John of Lincoln, Keeper of the Seal. The original intention of the college had been to construct an elegant, classical building supported by pillared porticos, but Bishop William insisted on a more traditional design. Thus, though the college lays claim to few examples of neo-classical design, the college Library stands as one of the earliest examples of English neo-Gothic architecture.
Third Court.
Third Court is entered through Shrewsbury Tower, which from 1765 to 1859 housed an observatory. Each of its ranges was built in a different style. Following the completion of the college library in 1624, the final sides of Third Court were added between 1669 and 1672, after the college had recovered from the trauma of the English Civil War. The additions included a fine set of Dutch-gabled buildings backing onto the River Cam, and a 'window-with-nothing-behind-it' that was designed to solve the problem of connecting the windowed library with the remainder of the court.
Kitchen "or" Wren Bridge.
This was the first stone bridge erected at St John's college, continuing on from Kitchen lane. The crossing's chief distinction is the use of illusory intaglio; Wren's bridge is carved from a limestone monolith incised to give the appearance of masonry. The crossing lies south of the Bridge of Sighs, and was a replacement for a wooden bridge that had stood on the site since the foundation's early days as a hospital. Though Sir Christopher Wren submitted designs for the bridge, it was eventually built on a different site by a local mason, Robert Grumbold, who also built Trinity College Library. As with the Library, Grumbold's work was based on Wren's designs, and the bridge has become known more famously as 'the Wren Bridge'.
Kitchen Court.
This tiny court, formed within the walls of the old Kitchen Lane, is used as an outdoor dining area.
The Bridge of Sighs.
Though it bears little resemblance to its namesake in Venice, the bridge connecting Third Court to New Court, originally known as New Bridge, is now commonly known as the Bridge of Sighs. It is one of the most photographed buildings in Cambridge, and was described by the visiting Queen Victoria as "so pretty and picturesque". It is a single-span bridge of stone with highly decorative Neo-Gothic covered footwalk over with traceried openings. There is a three bay arcade at the East end of the bridge. The architect was Henry Hutchinson.
New Court.
The 19th century neo-Gothic New Court, probably one of the best known buildings in Cambridge, was the first major building built by any of the colleges on the west side of the river. Designed by Thomas Rickman and Henry Hutchinson, New Court was built between 1826 and 1831 to accommodate the college's rapidly increasing numbers of students. Despite the college's original intention to get the architects to build another copy of Second Court, plans were eventually accepted for a fashionably romantic building in the 'Gothic' style. It is a three-sided court of tall Gothic Revival buildings, closed on the fourth side by an open, seven-bayed cross-vaulted cloister and gateway. It is four storeys high, has battlements and is pinnacled. The main portal has a fan vault with a large octagonal pendant, and the interior of the main building retains many of its original features including ribbed plaster ceilings in the mock-Gothic style. Its prominent location (especially when seen from the river) and flamboyant design have led it to be nicknamed "The Wedding Cake". Hutchinson was suitably proud of his creation, and it is said that he once dashed up a staircase to reprimand an undergraduate for spoiling its symmetry by sitting too near one of its windows.
College Chapel.
The Chapel of St John's College is entered by the north west-corner of First Court, and was constructed between 1866 and 1869 to replace the smaller, mediaeval chapel which dated back to the 13th century. When in 1861 the college's administration decided that a new building was needed, Sir George Gilbert Scott was selected as architect. He had recently finished work on the chapel at Exeter College, Oxford, and went about constructing the chapel of St John's College along similar lines, drawing inspiration from the Church of Saint Chapelle in Paris.
The benefactor Henry Hoare offered a downpayment of £3000 to finance the chapel's construction, in addition to which he promised to pay £1000 a year if a tower were added to Scott's original plans, which had included only a small fleche. Work began, but Mr Hoare's death in a railway accident left the college £3000 short of his expected benefaction. The tower was completed, replete with louvres but left without bells. It is based on Pershore Abbey. The tower is 50 metres high, and is the tallest structure in Cambridge (followed by the Cambridge University Library and King's College Chapel). The Chapel's antechamber contains statues of Margaret Beaufort and John Fisher. Inside the building is a stone-vaulted antechapel, at the end of which hangs a 'Deposition of the Cross' by Anton Rafael Mengs, completed around 1777. The misericordes and panelling date from 1516, and were salvaged from the old chapel. The chapel contains some fifteenth-century glass, but most was cast by Clayton and Bell, Hardman, and Wailes, in around 1869. Freestanding statues and plaques commemorate college benefactors such as James Wood, Master 1815–39, as well as alumni including William Wilberforce, Thomas Clarkson and William Gilbert. The college tower can be climbed, and is accessed via a small door on First Court.
The Chapel is surrounded on three sides by large tabernacles which form part of the external buttresses. Each contains a statue of a prominent college alumnus, alumna or benefactor. The persons commemorated are, beginning with the buttress next to the transept on the south side:
The Master's Lodge and Garden.
St John's Master's lodge is located in a grassy clearing to the north of Third Court. It was built at the same time as the new Chapel was being constructed, and has Tudor fittings, wainscot, portraits and other relics from the demolished north wing of First Court. It has a large garden, and in the winter its westmost rooms have excellent views of the college's old library, the River Cam, and the Bridge of Sighs. The architect was Sir George Gilbert Scott.
Chapel Court.
Located to the west of the Chapel tower. Together with North Court and Forecourt built c1938 to increase the size of the college significantly. An alternative plan was submitted by Maufe which involved demolition of the Master's Lodge, replacing it with a new Court and resiting a new Lodge on the location of the garden.
North Court and Forecourt.
North Court is located to the north of Chapel Court. Forecourt is situated to the east of Chapel court, facing St John's Street. It is used partly as a car park for fellows, and also as a night entrance to the college.
Cripps Building.
This buildings, behind New Court, was built in 1966–67 to meet a post-1945 expansion in the numbers of students. It has two courts, and was designed by architects Philip Powell and Hidalgo Moya. The building was Grade II* listed after receiving an award from the British Architectural Institution, and is considered an exemplar of the later 20th-century architectural style. It is named after its benefactor, Sir Humphrey Cripps. The Cripps Building forms two courts, Upper River Court and Lower River Court.
The School of Pythagoras.
The School of Pythagoras was built around 1200, predating the foundation of the college (1511). It is the oldest secular building in Cambridge and is said to be the oldest building continuously in use by a university in Britain. It was originally the private house of the Merton Family. The School of Pythagoras is now used to store the College's archive collection.
Merton Hall and Merton Court.
Merton Hall is so called because from 1266 until 1959 both the School of Pythagoras and Merton Hall were property of Merton College, Oxford. Merton Court is the college's eleventh and westernmost court.
The Fisher Building.
The Fisher Building was named after John Fisher and was designed by Peter Boston and completed in 1987.
All Saints' Yard.
All Saints' Yard is located directly opposite the college's Great Gate. The complex is formed from the buildings of the so-called 'Triangle Site', a collection of structures owned by the college. An extensive renovation project finished in Michaelmas Term 2012 had a budget of approximately £9.75 million. The centrepiece of the Yard is Corfield Court, named after the project's chief benefactor, Charles Corfield. The site can be entered through one of two card-activated gates, or through the School of Divinity. The School of Divinity is the largest building on the site, and was built between 1878–1879 by Basil Champneys for the University of Cambridge's Divinity Faculty on land leased by St John's College. Control of the building reverted to St John's when the Faculty of Divinity moved to a new building on the Sidgwick site in 2000.
Brickwork.
At least three different brick bonds make up the walls of the college structures: Running bond, Flemish bond, and English bond.
The Second Court of St John's College
Choir.
St John's College Choir has a tradition of religious music and has sung the daily services in the College Chapel since the 1670s. The services follow the cathedral tradition of the Church of England, Evensong being sung during Term six days a week and Sung Eucharist on Sunday mornings. The Choir is currently directed by Mr Andrew Nethsingha, who has previously been Director of Music at Gloucester and Truro Cathedrals. The boys of the choir are all educated at the St John's College School. During university vacations the choir carries out engagements elsewhere. Recent tours have taken it to places including the Netherlands, the USA and France. The choir has made a large number of recordings.
The Choir has an extensive discography dating back to the 1950s, when it was signed to the Decca/Argo label under George Guest. More recently, the Choir has completed a sequence of recordings of English 20th century choral for Naxos, which sold over 200,000 copies. The Choir now records with Hyperion Records, and has released four discs to date with the label: one of the music of Mendelssohn, a collection of music for Advent, Christmas and Epiphany, Christmas at St John's, a recording of the choral and vocal music of Jongen and Peeters and most recently, a collection of the music of Bairstow. The Choir has received invitations to perform throughout the world, recently touring in France, Austria, the Netherlands, Estonia, Hungary and America.
The men of the choir, or choral scholars, also form their own close harmony group, The Gentlemen of St John's. Their repertoire spans the 15th century through to the modern day, and concert tours have taken them to Europe, the USA and Japan. They provide a mixture of classical a capella music and folksongs, as well as covers of recently chart hits and light-hearted entertainment.
Traditions and legends.
Eating Swan.
Fellows of St John's College are the only people outside the Royal Family legally allowed to eat unmarked mute swans. Swan traps were originally built into the walls of the college alongside the river, but these are no longer used. The Crown (the British monarch) retains the right to ownership of all unmarked mute swans in open water, but the Queen only exercises her ownership on certain stretches of the Thames and its surrounding tributaries. This ownership is shared with the Vintners' and Dyers' Companies, who were granted rights of ownership by the Crown in the fifteenth century, and was extended to the college via ancient Royalist ties.
Ghosts.
According to popular legend, St John's College is inhabited by a number of ghosts. In 1706, four fellows exorcised some ghosts from a house opposite the college by the simple method of threatening to fire their pistols at the positions the moans and groans were coming from. Second court is apparently still haunted by the ghost of the former undergraduate, James Wood. Wood was so poor that he could not afford to light his room, and would often do his work in the well-lit stairway.
New Court's Clock Tower.
New Court's central cupola has four blank clock-faces. These are subject to various apocryphal explanations. One legend maintains that a statute limiting the number of chiming clocks in Cambridge rendered the addition of a mechanism illegal. No such limitation is known to exist. More likely explanations include Hutchinson's fear that the installation of a clockface would spoil the building's symmetry, and that the college's financial situation in the early nineteenth century made completion impossible.
Other legends explaining the absence of clockfaces claim that St John's College and its neighbour, Trinity College, were engaged in a race to build the final (or tallest) clocktower in Cambridge. Supposedly, whichever was finished first (or was tallest) would be permitted to house the 'final' chiming clock in Cambridge. Trinity's Tower was finished first (or, in another version of the same story, was made taller overnight by the addition of a wooden cupola), and its clock was allowed to remain.
In truth, the completion of New Court and Trinity's Clock (which is in King Edward's Tower) was separated by nearly two centuries. Trinity's famous double-striking was installed in the seventeenth century by its then-Master, Richard Bentley, a former student of St John's, who dictated that the clock chime once for Trinity, and once for his alma mater, St John's.
College rivalry.
The college remains a great rival of Trinity which is its main competitor in sports and academia (Trinity is situated next to John's). This has given rise to a number of anecdotes and myths. It is often cited as the reason why the older courts of Trinity generally have no J staircases, despite including other letters in alphabetical order. A far more likely reason remains the absence of the letter J in the Latin alphabet, and it should be noted that St John's College's older courts also lack J staircases. There are also two small muzzle-loading cannons on Trinity's bowling green pointing in the direction of John's, though this orientation may be coincidental. Generally the colleges maintain a cordial relationship with one other; compatriotism led famously to the splitting of the atomic nucleus in 1932 by Ernest Walton and John Cockcroft, of Trinity and St John's respectively.
Shield and Arms.
St John's College and Christ's College, Cambridge both bear the arms of the Lady Margaret Beaufort, Countess of Richmond and Derby, mother of Henry VII. These arms are recorded in the College of Arms as being borne by right, and are described as: "Quarterly: 1 and 4 azure three fleurs-de-lis gold (France, Modern); 2 and 3 gules three lions passant gardant or (England); all within a border compony silver and azure". In addition, both foundations use the Beaufort crest, "an eagle displayed arising out of a coronet of roses and fleurs-de-lis all gold", but their title to this is more doubtful. When displayed in their full achievement, the arms are flanked by mythical yales.
Motto.
The college motto is "souvent me souvient", supplied by Lady Margaret Beaufort, and written in Mediaeval French. It is inscribed over gates, lintels and within tympana throughout the college, functioning as a triple pun. It means 'often I remember', 'think of me often' and, when spoken (exploiting the homonym "souvent me sous vient"), 'I often pass beneath it' (referring to the inscriptions). The college shares its motto with Christ's College, Cambridge and Lady Margaret Hall, Oxford.
College Grace.
The College Grace is customarily said before and after dinner in Hall. The reading of Grace before dinner (ante prandium) is usually the duty of a Scholar of the College; Grace after dinner (post prandium) is said by the President or the Senior Fellow dining. The Graces used in St John's have been in continuous use for some centuries and it is known that the Ante Prandium is based upon mediaeval monastic models. The Grace is said shortly after the fellows enter the Hall, signalled by the sounding of a Gong, and accompanied by the ringing of the college's Grace Bell. The Ante Prandium is read after the Fellows have entered, the Post Prandium after they have finished dining:
Student life.
The buildings of St John's College include the Chapel, the Hall, two libraries, a bar, and common rooms for fellows, graduates and undergraduates. There are also extensive gardens, lawns, a neighbouring sportsground, College School and boat-house. On-site accommodation is provided for all undergraduate and most graduate students. This is generally spacious, and some undergraduate rooms comprise 'sets' of living and sleeping rooms. Members of the college can choose to dine either in the Hall, where silver service three-course meals are served, or in the buttery, where food can be purchased from a cafeteria-style buffet. College catering is organised by Michelin Star Chef Bill Brogan, overseer of the intercollegiate Stewards' Cup.
The college maintains an extensive library, which supplements the university libraries. Most undergraduate supervisions are carried out in the college, though for some specialist subjects undergraduates may be sent to tutors in other colleges.
The college has two official combination rooms for junior members, which represent the interests of students in college and are responsible for social aspects of college life. Undergraduates are members of the Junior Combination Room (JCR). Graduate students have membership to the JCR, but also belong to the Samuel Butler Room, which is the name of the Middle Combination Room (MCR) of St John's College.
The fleet of punts is kept in a purpose-built punt pool behind the Cripps Building. St John's tends to be ranked near the middle of the Tompkins Table of undergraduate degree results, with an average position of 12.8 since 1997.
Sports.
The college has a rich sporting history, enjoying much success in most of the major sports on offer in Cambridge.
The Red Boys, St John's College Rugby Club, won the Division One League title for the nine years in a row, before finally losing to Jesus in 2010–11, and the cuppers trophy for 6 years in a row from 2006-2011, making it one of the most successful collegiate sports teams in Cambridge's history. The rugby club has produced several notable alumni including RFU executive Francis Baron, former Newcastle and England fly-half and current RFU Director of Elite Rugby Rob Andrew, and Battlestar Galactica actor Jamie Bamber.
The college rowing club, the Lady Margaret Boat Club (LMBC), is the oldest in the University, and was founded in 1825. Despite many gruesome rumours concerning the name of the club, it was merely the most successful of the many boat clubs established in the college in the 19th century. In a similar fashion the traditional rival of the LMBC, the Boat Club of Trinity College, is known as 'First and Third' in a reference to its formation from two original clubs.
Scholarships and prizes.
Every year the college awards scholarships to a handful of graduate students under the Benefactors' and Scholarships Scheme. The most generous of all the early benefactors of St John's College was Dr Roger Lupton (d. 1540), Provost of Eton and chaplain to Henry VIII. Lupton had amassed immense wealth through a lifetime of royal service and ecclesiastical pluralism and his scholarships exist today as the Lupton and Hebblethwaite Exhibitions. Other scholarships include the Craik Scholarship, the J.C. Hall Scholarship, the Luisa Aldobrandini Studentship Competition, the Paskin Scholarship and the Pelling Scholarship. Competition for these scholarships is very fierce as students from any country reading for any graduate degree—not only members of the college—can apply. There is also the famous Adams Prize in mathematics, named after the mathematician (and alumnus of St John's) John Couch Adams for his discovery of Neptune – it is an annual competition and can be awarded to any mathematician resident in the UK, with an age limit of under 40. The college is also associated with the Dr Manmohan Singh Scholarship, first awarded in 2008.
May Ball.
St John's hosts a large and typically spectacular May Ball, which is traditionally held on the Tuesday of May Week. In recent years, tickets have only been available to Johnians and their guests. Highlights include an extravagant fireworks display and a variety of musical acts.
People associated with the college.
"See also ", ".
Notable Johnian's include former Heads of State, politicians, academics, Nobel laureates, poets and writers. Over 1000 former members of St John's College appear in the Oxford Dictionary of National Biography.
In politics and law, alumni include; the heads of government F. J. Robinson, 1st Viscount Goderich, Prime Minister of the United Kingdom, 1827–28, George Hamilton-Gordon, 4th Earl of Aberdeen, Prime Minister of the United Kingdom, 1852–55, Henry John Temple, 3rd Viscount Palmerston, Prime Minister of the United Kingdom, 1855–58 & 1859–65, Alfred Domett, Prime Minister of New Zealand, 1862–63, Sir Francis Bell, Prime Minister of New Zealand, 1925, Manmohan Singh, Prime Minister of India, 2004–2014.
Other alumni in politics and law include Roger Ascham, tutor of Elizabeth I and advisor to Edward VI and Mary I, William Cecil, 1st Baron Burghley, Lord High Treasurer, 1572–98, and chief advisor to Elizabeth I, Robert Cecil, 1st Earl of Salisbury, Lord High Treasurer, 1608–12, and spymaster for James I, John Williams, Lord Keeper of the Great Seal, 1621–1625, and Archbishop of York, 1641–1650, Thomas Fairfax, 3rd Lord Fairfax of Cameron, parliamentary general and commander-in-chief in the English Civil War, Thomas Wentworth, 1st Earl of Strafford, Lord Deputy of Ireland and leading advisor to Charles I, William Cavendish, 1st Duke of Newcastle, general and major supporter of Charles I in the English Civil War, Thomas Clarkson, abolitionist and a leading campaigner against the slave trade in the British Empire, Dudley Ryder, 1st Earl of Harrowby, Secretary of State for Foreign Affairs, 1804-5, William Wilberforce, Member of Parliament and a leader of the movement to abolish the slave trade, Robert Stewart, Viscount Castlereagh, Secretary of State for Foreign Affairs, 1812–22, George Villiers, 4th Earl of Clarendon, Secretary of State for Foreign Affairs, 1853-8, Suematsu Kenchō, historian and Japanese Minister of Home Affairs, 1900-1, Kikuchi Dairoku, President of Tokyo Imperial University and Japanese Minister of Education, 1901-3, Wee Chong Jin, Chief Justice of Singapore, 1963–90, Nigel Dodds, Democratic Unionist Party MP, Sarah Teather, Liberal Democrat MP for Brent East.
Nobel Prize winners from the college include Sir Edward Appleton, for discovering the Appleton layer, Sir John Cockcroft KCB, physicist who first split the atom, Allan Cormack, for the invention of the CAT scan, Paul Dirac, one of the founders of quantum mechanics, Sir Nevill Francis Mott, for work on the behaviour of electrons in magnetic solids, Abdus Salam, for unifying the electromagnetic force and the weak force, Frederick Sanger, molecular biologist and Maurice Wilkins, awarded Nobel prize for Medicine or Physiology with Watson and Crick for discovering the structure of DNA.
St John's and the abolition of the British slave trade.
Several of St John's graduates were deeply involved in the efforts to abolish the British Slave Trade which culminated in the Act of 1807. In particular, Thomas Clarkson, William Wilberforce, Thomas Gisborne and Thomas Babington were active in the Committee for the Abolition of the Slave Trade and other abolitionist efforts.
As part of the commemoration of the bicentenary of the 1807 Act, and as a representative of one of the Ivy League universities offering American historical perspective on the Triangular Trade, President Ruth J. Simmons of Brown University (herself a direct descendant of American slaves) gave a public lecture at St John's College entitled "Hidden in Plain Sight: Slavery and Justice in Rhode Island" on 16 February 2007. St John's College hosted some of the key events relating to the commemoration, including an academic conference and a Gospel Mass in the College Chapel with the London Adventist Chorale.
Royal Medal Winners.
Three Royal Medals, known also as the Queen's Medals, are awarded annually by the Sovereign upon the recommendation of the Council of the Royal Society, “two for the most important contributions to the advancement of Natural Knowledge (one in the physical and one in the biological sciences) and the other for distinguished contributions in the applied sciences”. The first Royal Medal was awarded in 1826 and previous recipients include thirty-eight Johnians.
Masters.
The current Master of St John's is Chris Dobson, John Humphrey Plummer Professor of Chemical and Structural Biology at the university.

</doc>
<doc id="48716" url="http://en.wikipedia.org/wiki?curid=48716" title="Pulitzer Prize for Drama">
Pulitzer Prize for Drama

The Pulitzer Prize for Drama is one of the seven American Pulitzer Prizes that are annually awarded for Letters, Drama, and Music. It is one of the original Pulitzers, for the program was inaugurated in 1917 with seven prizes, four of which were awarded that year. (No Drama prize was given, however, so that one was inaugurated 1918 in a sense). It recognizes a theatrical work staged in the U.S. during the preceding calendar year.
Through 2006 the Drama Prize was unlike the majority of the other Pulitzer Prizes: during these years, the eligibility period for the drama prize ran from March 2 to March 1, to reflect the Broadway 'season' rather than the calendar year. The decision was made, however, that the 2007 Prize would consider works staged during an eligibility period of January 1 to December 31, 2006—thus bringing the schedule for the Drama Prize in line with those of the other prizes.
The drama jury, which consists of one academic and four critics, attends plays in New York and in regional theaters. The Pulitzer board has the authority to overrule the jury's choice, however, as happened in 1986 when the jury chose "" to receive the prize, but due to the board's opposition no award was given.
In 1955, Joseph Pulitzer, Jr. pressured the prize jury into presenting the Prize to "Cat on a Hot Tin Roof", which the jury considered the weakest of the five shortlisted nominees ("amateurishly constructed... from the stylistic points of view annoyingly pretentious"), instead of Clifford Odets' "The Flowering Peach" (their preferred choice) or "The Bad Seed", their second choice. Edward Albee's "Who's Afraid of Virginia Woolf?" was selected for the 1963 Pulitzer Prize for Drama by that award's committee. However, the committee's selection was overruled by the award's advisory board, the trustees of Columbia University, because of the play's then-controversial use of profanity and sexual themes. Had Albee been awarded, he would be tied with Eugene O'Neill for the most Pulitzer Prizes for Drama (four).
Awards and nominations.
In its first 98 years to 2013, the Drama Pulitzer was awarded 82 times; none was given in 15 years and it was never split. Many of the prizes were won by multiple people for their collaboration, as many as five in 1976.
Musicals.
Eight musicals have won the Pulitzer Prize for Drama—-roughly one per decade from the 1930s to the 2000s. They are: George and Ira Gershwin's "Of Thee I Sing" (1932)¹, Rodgers and Hammerstein's "South Pacific" (1950), Bock & Harnick's "Fiorello!" (1960), Frank Loesser's "How to Succeed in Business Without Really Trying" (1962), Marvin Hamlisch, Ed Kleban, James Kirkwood, and Nicholas Dante's "A Chorus Line" (1976), Stephen Sondheim's and James Lapine's "Sunday in the Park with George" (1985), Jonathan Larson's "Rent" (1996), and Brian Yorkey and Tom Kitt's "Next to Normal" (2010).
"Of Thee I Sing", "Sunday in the Park with George", and "Next to Normal" are the only musicals that won the Pulitzer Prize and did not win the Tony Award for Best Musical. However, "Of Thee I Sing" opened when the Tony Awards did not exist, and "Next to Normal" won the Tony Award for Best Original Score and the Tony Award for Best Orchestrations.
The award goes to the playwright, although production of the play is also taken into account. In the case of a musical being awarded the prize, the composer, lyricist and book writer are generally the recipients. An exception to this was the first Pulitzer ever awarded to a musical: when "Of Thee I Sing" won in 1932, book authors George S. Kaufman and Morrie Ryskind, as well as lyricist Ira Gershwin, were cited as the winners, while composer George Gershwin's contribution was overlooked by the committee. The reason given was that the Pulitzer Prize for Drama is a "dramatic" award, and not a "musical" one. However, by 1950 the Pulitzer committee included composer Richard Rodgers as a recipient when "South Pacific" won the award, in recognition of music as an integral and important part of the theatrical experience.
Additionally, two musicals have been nominated for the Pulitzer Prize for Drama. They are: Lin-Manuel Miranda and Quiara Alegría Hudes' "In the Heights" (2009) and Jeanine Tesori and Lisa Kron's "Fun Home" (2014).
¹All dates are Prize years. Ordinarily the musical opened in New York during the preceding calendar year.
Repeat winners.
Eugene O'Neill won the Pulitzer for Drama four times, three in the 1920s. Several people have won two or three.
The prize has been shared by as many as five people, in 1976 for the musical "A Chorus Line".

</doc>
<doc id="48726" url="http://en.wikipedia.org/wiki?curid=48726" title="Occupational therapist">
Occupational therapist

The role of an occupational therapist is to work with a client to help them achieve a fulfilled and satisfied state in life through the use of "purposeful activity or interventions designed to achieve functional outcomes which promote health, prevent injury or disability and which develop, improve, sustain or restore the highest possible level of independence." 
A practical definition for OT can also be illustrated with the use of models such as the Occupational Performance Model (Australia), known as the OPM(A). At the core of this approach is the ideology that occupational therapists are concerned with the occupations of people and how these contribute to health. Specifically it is a person's occupational performance that influences their health and personal satisfaction of their individual needs. The OPM(A) is constructed on the following definition of Occupational Performance:
The ability to perceive, desire, recall, plan and carry out roles, routines, tasks and sub-tasks for the purpose of self-maintenance, productivity, leisure and rest in response to demands of the internal and/or external environment.
It can be seen that occupational performance, the roles it creates for a client, and the areas it can encompass are so far-reaching that an occupational therapist can work with a wide range of clients of various limitations who are being cared for in an array of settings. Occupational therapy is about helping people do the day-to-day tasks that "occupy" their time, sustain themselves, and enable them to contribute to the wider community. It is these opportunities to "do" that occupational therapy provides that prove important and meaningful to the health of people.
Role.
Occupational therapists (OTs) help people of all ages to improve their ability to perform tasks in their daily living and working environments. They work with individuals who have conditions that are mentally, physically, developmentally, socially or emotionally disabling. They also help them to develop, recover, or maintain daily living and work skills. Occupational therapists help clients not only to improve their basic motor functions and reasoning abilities, but also to compensate for permanent loss of function. Occupational therapists assist clients in performing activities of all types, ranging from using a computer to caring for daily needs such as dressing, cooking, and eating. Physical exercises may be used to increase strength and dexterity, while other activities may be chosen to improve visual acuity and the ability to discern patterns. For example, a client with short-term memory loss might be encouraged to make lists to aid recall, and a person with coordination problems might be assigned exercises to improve hand-eye coordination. Occupational therapists also use computer programs to help clients improve decision-making, abstract-reasoning, problem solving, and perceptual skills, as well as memory, sequencing, and coordination —- all of which are important for independent living. Occupational therapists are often skilled in psychological strategies such as cognitive behavioral therapy and Acceptance and Commitment Therapy, and may use cognitive therapy especially when introducing people to new strategies for carrying out daily activities such as activity pacing or using effective communication strategies.
Clients with permanent disabilities.
Therapists instruct those with permanent disabilities, such as spinal cord injuries, cerebral palsy, or muscular dystrophy, in the use of adaptive equipment, including wheelchairs, orthotics, and aids for eating and dressing. They also design or make special equipment needed at home or at work. Therapists develop computer-aided adaptive equipment and teach clients with severe limitations how to use that equipment in order to communicate better and control various aspects of their environment.
Work-related therapy.
Some occupational therapists treat individuals whose ability to function in a work environment has been impaired. These practitioners arrange employment, evaluate the work environment, plan work activities, and assess the client's progress. Therapists also may collaborate with the client and the employer to modify the work environment so that the work can be successfully completed.
With children.
Occupational therapists may work exclusively with individuals in a particular age group or with particular disabilities. In schools, for example, they evaluate children's abilities, recommend and provide therapy, modify classroom equipment, and help children participate as fully as possible in school programs and activities. A therapist may work with children individually, lead small groups in the classroom, consult with a teacher, or serve on a curriculum or other administrative committee. Early intervention therapy services are provided to infants and toddlers who have, or are at the risk of having, developmental delays. Specific therapies may include facilitating the use of the hands, promoting skills for listening and following directions, fostering social skills, or teaching dressing and grooming skills.
With the elderly.
Occupational therapy is very beneficial to the elderly population. Therapists help the elderly lead more productive, active, and independent lives through a variety of methods, including the use of adaptive equipment. Occupational therapists work with the elderly in many varied environments, such as in their homes in the community, in hospital, and in residential care facilities to name a few. In the home environment, occupational therapists may work with the client to assess for hazards and to identify environmental factors that contribute to falls. Occupational therapists are often instrumental in assessing for appropriate wheelchairs for the elderly. In addition, therapists with specialized training in driver rehabilitation assess an individual's ability to drive using both clinical and on-the-road tests. The evaluations allow the therapist to make recommendations for adaptive equipment, training to prolong driving independence, and alternative transport options.
Mental health.
Occupational therapists also work with people who have mental health problems and learning disabilities. In this work, therapists choose activities that help people learn to engage in and cope with daily life. Activities include time management skills, budgeting, shopping, homemaking, and the use of public transportation. Occupational therapists also may work with individuals who are dealing with alcoholism, drug abuse, depression, eating disorders, or stress-related disorders. The ultimate aim would be to help people to engage in a personally satisfying and socially adaptive range of occupations.
With terminally ill patients.
Occupational therapists work with patients with terminal illness like cancer, Muscular dystrophy,etc. All performance areas including work, play and leisure are widely affected in these sets of patients. An occupational therapist provides various means to these patients to restore or maintain their deteriorating performance components by using their residual capacities and capabilities to give them a sense of self-importance and a measure of confidence.
With people experiencing chronic pain.
Occupational therapists often work within interdisciplinary or multidisciplinary teams (professionals such as nurses and doctors) to help individuals with chronic pain develop active self-management strategies. An area of specific concern to occupational therapists is the use of time but it is also common for occupational therapists to help people return to work, and to return to leisure and family activities. Occupational therapists may use a variety of interventions including biofeedback, relaxation, goal setting, problem solving, planning, and carry this out within both group and individual settings. Therapists may work within a clinic setting, or in the community including the workplace, school, home and health care centres. Occupational therapists may assess occupational performance before and after intervention, as a measure of effectiveness and reduction in disability.
Assessment.
Assessing and recording a client's activities and progress is an important part of an occupational therapist's job. Accurate records are essential for evaluating clients, for billing, and for reporting to physicians and other health care providers.
Thorough and accurate assessment ensures that Occupational Therapists select appropriate and effective interventions for their clients. Assessment in Occupational Therapy is complex and multifaceted, and is an essential component of the Occupational Therapy Process. Assessment occurs at the beginning of the Process (providing the foundation for effective treatment), at the end (evaluation). Reassessment also occurs throughout intervention.
Hand therapy.
Occupational Therapy also plays a major role in the rehabilitation and recovery of patients who have hand or upper extremity injuries. They play a significant role in liaising with Hand Surgeon/Orthopeadic Surgeon and patients employers or case managers in providing the best client centered rehabilitation program. Occupational Therapist treats conditions ranging from soft tissue injuries such as Tennis Elbows to nerve neuropathies such as Cubital Tunnel Syndrome/ Carpal Tunnel Syndrome. An Array of Upper Limb assessment are utilised to provide a treatment care that is effective and appropriate. Treatment modalities such as orthosis/splints, soft braces and education are some of the common treatment tool that an occupational therapist will use during treatment. Hand Therapy is a specialised field of occupational therapy and it requires therapist to be highly skilled and knowledgeable in upper limb anatomy to be able to work in this area. It is definitely an area where Occupational Therapy is famous for due to the therapeutic models that the profession practices which focus on occupation as means and ends and their aim of returning patients to them performing their daily functions.

</doc>
<doc id="48727" url="http://en.wikipedia.org/wiki?curid=48727" title="Corpus Juris Civilis">
Corpus Juris Civilis

The Corpus Juris (or Iuris) Civilis ("Body of Civil Law") is the modern name for a collection of fundamental works in jurisprudence, issued from 529 to 534 by order of Justinian I, Eastern Roman Emperor. It is also sometimes referred to as the Code of Justinian, although this name belongs more properly to the part titled "Codex Justinianus".
The work as planned had three parts: the "Code" ("Codex") is a compilation, by selection and extraction, of imperial enactments to date; the "Digest" or "Pandects" (the Latin title contains both "Digesta" and "Pandectae") is an encyclopedia composed of mostly brief extracts from the writings of Roman jurists; and the "Institutes" ("Institutiones") is a student textbook, mainly introducing the "Code" although it has important conceptual elements that are less developed in the "Code" or the "Digest". All three parts, even the textbook, were given force of law. They were intended to be, together, the sole source of law; reference to any other source, including the original texts from which the "Code" and the "Digest" had been taken, was forbidden. Nonetheless, Justinian found himself having to enact further laws and today these are counted as a fourth part of the Corpus, the "Novellae Constitutiones" ("Novels", literally "New Laws").
The work was directed by Tribonian, an official in Justinian's court. His team was authorized to edit what they included. How far they made amendments is not recorded and, in the main, cannot be known because most of the originals have not survived. The text was composed and distributed almost entirely in Latin, which was still the official language of the government of the Empire in 529–534, whereas the prevalent language of merchants, farmers, seamen, and other citizens was Greek. By the early 7th century, the official government language had become Greek during the lengthy reign of Heraclius (610–641).
How far the Corpus Iuris Civilis or any of its parts was effective, whether in the east or (with reconquest) in the west, is unknown. However, it was not in general use during the Early Middle Ages. After the Early Middle Ages, interest in it revived. It was "received" or imitated as private law and its public-law content was quarried for arguments by both secular and ecclesiastical authorities. This revived Roman law, in turn, became the foundation of law in all civil law jurisdictions. The provisions of the Corpus Juris Civilis also influenced the Canon Law of the church: it was said that "ecclesia vivit lege romana" — the church lives by Roman law. Influence on the common-law systems has been much smaller, although some basic concepts from the Corpus have survived through Norman law - such as the contrast, especially in the "Institutes", between "law and custom ("lex et consuetudo")". The Corpus continues to have a major influence on public international law. Its four parts thus constitute the foundation documents of the Western legal tradition.
The four parts.
Codex.
The "Codex" was the first part to be finished, on 7 April 529. It contained in Latin most of the existing imperial "constitutiones" (imperial pronouncements having force of law), back to the time of Hadrian. It used both the "Codex Theodosianus" and the fourth-century collections embodied in the "Codex Gregorianus" and "Codex Hermogenianus", which provided the model for division into books that were themselves divided into titles. These works had developed authoritative standing. This first edition is now lost; a second edition was issued in 534 and is the text that has survived. At least the second edition contained some of Justinian's own legislation, including some legislation in Greek. It is not known whether he intended there to be further editions, although he did envisage translation of Latin enactments into Greek.
Legislation about religion.
Numerous provisions served to secure the status of Christianity as the state religion of the empire, uniting Church and state, and making anyone who was not connected to the Christian church a non-citizen.
The very first law in the Codex requires all persons under the jurisdiction of the Empire to hold the Christian faith. This was primarily aimed against heresies such as Nestorianism. This text later became the springboard for discussions of international law, especially the question of just what persons are under the jurisdiction of a given state or legal system.
Other laws, while not aimed at pagan belief as such, forbid particular pagan practices. For example, it is provided that all persons present at a pagan sacrifice may be indicted as if for murder.
Digesta.
The "Digesta" or "Pandectae", completed in 533, is a collection of juristic writings, mostly dating back to the second and third centuries. Fragments were taken out of various legal treatises and opinions and inserted in the Digest. In their original context, the statements of the law contained in these fragments were just private opinions of legal scholars - although some juristic writings had been privileged by Theodosius II's Law of Citations in 426. The Digest, however, was given complete force of law.
Institutiones.
As the "Digest" neared completion, Tribonian and two professors, Theophilus and Dorotheus, made a student textbook, called the "Institutions" or "Elements". As there were four elements, the manual consists of four books. The "Institutiones" are largely based on the "Institutiones" of Gaius. Two thirds of the "Institutiones" of Justinian consists of literal quotes from Gaius. The new "Institutiones" were used as a manual for jurists in training from 21 November 533 and were given the authority of law on 30 December 533 along with the "Digest".
Novellae.
The Novellae consisted of new laws that were passed after 534. They were later re-worked into the "Syntagma", a practical lawyer's edition, by Athanasios of Emesa during the years 572–77.
Continuation in the East.
The Byzantine Empire was the medieval continuation of the Roman Empire in the east, and continued to practice Roman Law as collected in the Corpus Juris Civilis. This law was modified to be adequate for the new social relationships in the Middle ages. Thus the Byzantine law was created. New legal codes, based on Corpus Juris Civilis, were enacted. The most known are: Ecloga (740)—enacted by emperor Leo the Isaurian, Proheiron (c. 879)—enacted by emperor Basil the Macedonian and Basilika (late 9th century)—started by Basil the Macedonian and finished by his son Leo the Wise. The last one was a complete adaptation of Justinian's codification. At 60 volumes it proved to be difficult for judges and lawyers to use. There was need for a short and handy version. It was finally made by Constantine Harmenopoulos, a judge from Thesaloniki, in 1345. He made a short version of Basilika in six books, called "Hexabiblos". Serbian state, law and culture was built on the foundations of Rome and Byzantium. Therefore, the most important Serbian legal codes: Zakonopravilo (1219) and Dušan's Code (1349 and 1354), transplanted Roman-Byzantine Law included in Corpus Juris Civilis, Prohiron and Basilika. These Serbian codes were practised until the Serbian Despotate fell to the Turkish Ottoman Empire in 1459. After the liberation from the Turks in the Serbian Revolution, Serbs remained to practise Roman Law by enacting Serbian civil code in 1844. It was a short version of Austrian civil code (called "Allgemeines bürgerliches Gesetzbuch"), which was made on the basis of Corpus Juris Civilis.
Recovery in the West.
Justinian's "Corpus Juris Civilis" was distributed in the West but was lost sight of; it was scarcely needed in the comparatively primitive conditions that followed the loss of the Exarchate of Ravenna by the Byzantine empire in the 8th century. A two-volume edition of the Digest was published in Paris in 1549 and 1550, translated by Antonio Augustini, Bishop of Tarragona, who was well known for other legal works. The full title of the Digest was "Digestorum Seu Pandectarum tomus alter", and it was published by "Apud Carolam Guillards". Vol. 1 of the Digest has 2934 pages, while Vol. 2 has 2754 pages. The only western province where the Justinianic code was effectively introduced was Italy, following its recovery by Byzantine armies (Pragmatic Sanction of 554), but a continuous tradition of Roman law in medieval Italy has not been proven. Historians disagree on the precise way it was recovered in Northern Italy about 1070: legal studies were undertaken on behalf of papal authority central to the Gregorian Reform of Pope Gregory VII, which may have led to its accidental rediscovery. Aside from the Littera Florentina (a 6th-century codex of the Pandects that was preserved at Pisa) there may have been other manuscript sources for the text that began to be taught at Bologna, by Pepo and then by Irnerius. Irnerius' technique was to read a passage aloud, which permitted his students to copy it, then to deliver an excursus explaining and illuminating Justinian's text, in the form of glosses. Irnerius' pupils, the so-called Four Doctors of Bologna, were among the first of the "glossators" who established the curriculum of medieval Roman law. The tradition was carried on by French lawyers, known as the Ultramontani, in the 13th century.
The merchant classes of Italian communes required law with a concept of equity, and law that covered situations inherent in urban life better than the primitive Germanic oral traditions. The provenance of the Code appealed to scholars who saw in the Holy Roman Empire a revival of venerable precedents from the classical heritage. The new class of lawyers staffed the bureaucracies that were beginning to be required by the princes of Europe. The University of Bologna, where Justinian's Code was first taught, remained the dominant centre for the study of law through the High Middle Ages.
Referring to Justinian's Code as "Corpus Juris Civilis" was only adopted in the 16th century, when it was printed in 1583 by Dionysius Gothofredus under this title. The legal thinking behind the "Corpus Juris Civilis" served as the backbone of the single largest legal reform of the modern age, the Napoleonic Code, which marked the abolition of feudalism.
The "Corpus Juris Civilis" was translated into French, German, Italian, and Spanish in the 19th century. However, no English translation of the entire "Corpus Juris Civilis "existed until 1932 when Samuel Parsons Scott published his version . Unfortunately, Scott did not base his translation on the best available Latin versions, and his work was severely criticized. Fortunately, Fred. H. Blume did use the best-regarded Latin editions for his translations of and of .

</doc>
<doc id="48728" url="http://en.wikipedia.org/wiki?curid=48728" title="Social Security (United States)">
Social Security (United States)

In the United States, Social Security is primarily the Old-Age, Survivors, and Disability Insurance (OASDI) federal program.
The original Social Security Act (1935) and the current version of the Act, as amended, encompass several social welfare and social insurance programs. Social Security is funded through payroll taxes called Federal Insurance Contributions Act tax (FICA) or Self Employed Contributions Act Tax (SECA). Tax deposits are collected by the Internal Revenue Service (IRS) and are formally entrusted to the Federal Old-Age and Survivors Insurance Trust Fund, the Federal Disability Insurance Trust Fund, the Federal Hospital Insurance Trust Fund, or the Federal Supplementary Medical Insurance Trust Fund which make up the Social Security Trust Funds. With a few exceptions, all salaried income, up to an amount specifically determined by law (see tax rate table below) has an FICA or SECA tax collected on it. All income over said amount is not taxed, for 2014 the maximum amount of taxable earnings is $117,000.
With few exceptions, all legal residents working in the United States now have an individual Social Security number. Indeed nearly all working (and many non-working) residents since Social Security's 1935 inception have had a Social Security number, because it is required to do a wide range of things including paying the IRS and getting a job.
In 2013, the total Social Security expenditures were $1.3 trillion, 8.4% of the $16.3 trillion GNP (2013) and 37% of the Federal expenditures of $3.684 trillion. Income derived from Social Security is currently estimated to keep roughly 20% of all Americans, age 65 or older, above the Federally defined poverty level. The Social Security Administration is headquartered in Woodlawn, Maryland, just west of Baltimore. On February 16, 2015, the Walla Walla Union-Bulletin reported that $100 billion in Social Security payroll taxes is collected from illegal immigrants, even though few will ever be able to collect benefits, according to Stephen Goss, Social Security’s chief actuary.
History.
Social Security Timeline
A limited form of the Social Security program began, during President Franklin D. Roosevelt's first term, as a measure to implement "social insurance" during the Great Depression of the 1930s, when poverty rates among senior citizens exceeded 50 percent. The Act was an attempt to limit unforeseen and unprepared for dangers in the modern life: including old age, disability, poverty, unemployment, and the burdens of widow(er)s with and without children.
Opponents, however, decried the proposal as socialism. In a Senate Finance Committee hearing, the Democratic Oklahoma Senator Thomas Gore asked Secretary of Labor Frances Perkins, "Isn't this socialism?" She said that it was not, but he continued, "Isn't this a teeny-weeny bit of socialism?"
The provisions of Social Security have been changing since the 1930s, shifting in response to economic worries as well as coverage for the poor, dependent children, spouses, survivors and the disabled. By 1950, debates moved away from which occupational groups should be included to get enough taxpayers to fund Social Security to how to provide more benefits. Changes in Social Security have reflected a balance between promoting "equality" and efforts to provide "adequate" and affordable protection for low wage workers.
Major Social Security Programs.
The larger and better known programs under the Social Security Administration, SSA, are:
Benefits.
Social Security Benefits and Income 2012.
The largest component of OASDI is the payment of retirement benefits. These retirement benefits are a form of social insurance that is heavily biased toward lower paid workers to make sure they do not have to retire in relative poverty. With few exceptions, throughout a worker's career, the Social Security Administration and the Internal Revenue Service, IRS, keeps track of his or her earnings and requires Federal Insurance Contribution Act, FICA or Self Employed Contribution Act, SECA, taxes to be paid on the earnings. The OASI accounts plus trust funds are the only Social Security funding source that brings in more than it sends out.
Social Security revenues exceeded expenditures, between 1983 and 2009.
The disability insurance (DI) taxes of 1.4% are included in the OASDI rate of 6.2% for workers and employers or 12.4% for the self-employed. Outgo of $140.3 billion while having income of only $109.1 billion means the disability trust fund is rapidly being depleted and may require either revisions on what "disabilities" are included/allowed/defined as, fraud minimization or tax increases.
The Medicare hospital insurance, HI, (Part A: Hospital Insurance, inpatient care, skilled nursing facility care, home health care, and hospice care) expenditure rate of $266.8 billion in 2012 while bringing in only $243.0 billion means that the medicare HI trust funds are being seriously depleted and increased taxes or reduced coverage will be required. The additional retirees expected under the "baby boom bulge" will hasten this trust fund depletion. Medicare expenses, tied to medical costs growth rates, have traditionally increased much faster than GDP growth rates.
The Supplementary Medical Insurance, SMI, (otherwise known as Medicare Part B & D) expenditure rate of $307.4 billion in 2012 while bringing in only $293.9 billion means that the Supplementary Medical Insurance trust funds are also being seriously depleted and increased tax rates or reduced coverage will be required. The additional retirees expected under the "baby boom bulge" will hasten this trust fund depletion as well as legislation to end the Medicare Part D medical prescription drug funding "donut hole" are all tied to medical costs growth rates, which have traditionally increased much faster than GDP growth rates.
For workers the Social Security tax rate is 6.2% on income under $118,500 through the end of 2015. The worker Medicare tax rate is 1.45% of all income—employers pay another 1.45%. Employers pay 6.2% up to the wage ceiling and the Medicare tax of 1.45 percent on all income. Workers defined as "self employed" pay 12.4% on income under $113,700 and a 2.9% Medicare tax on all income.
The amount of the monthly Social Security benefit to which a worker is entitled depends upon the earnings record they have paid FICA or SECA taxes on and upon the age at which the retiree chooses to begin receiving benefits.
Primary Insurance Amount and benefit calculations.
All workers paying FICA (Federal Insurance Contributions Act) and SECA (Self Employed Contributions Act) taxes for forty quarters of credit (QC) or more on a specified minimum income or more are "fully insured" and eligible to retire at age 62 with reduced benefits and higher benefits at full retirement ages of 65, 66 or 67 depending on birth date. Retirement benefits depend upon the "adjusted" average wage you or your spouse have earned in the last 35 years and your respective ages. Wages of earlier years are "adjusted" before averaging by multiplying each annual salary by an annual adjusted wage index factor, AWI, for earlier salaries. Adjusted wages for 35 years are always used to compute the 35 year "average" indexed monthly salary. Only wages lower than the "ceiling" income are considered in calculating the adjusted average wage. If the worker has fewer than 35 years of covered earnings these non-contributory years are years are assigned zero earnings. If there are more than 35 years of covered earnings only the highest 35 are considered. The sum of the 35 adjusted salaries (or less if worker has less than 35 years of covered income) times its inflation index, AWI divided by 420 (35 yrs x 12 months/yr) gives the 35 year covered Average Indexed Monthly salary, AIME.
To calculate your Average Indexed Monthly salary (AIME) earnings, the records of your covered salaries may be obtained from the Social Security Administration by applying for them and paying a fee ranging from $15.00 for one year's covered wages to $80.00 for 40 years of wages. The adjusted wage indexes are available at Social Security's "Benefit Calculation Examples For Workers Retiring In 2013". The data from this site can be copied and pasted directly into a spreadsheet. For earlier AWI factors see:. By erasing the example salary data in the spreadsheet and substituting your own salaries and deleting the example indexed salaries and calculating your own Indexed salary (Salary*Index = Indexed salary) you can get your indexed salaries. In the spreadsheet the adjusted salaries can easily be summed and divided by 420 to find your adjusted indexed monthly salary, AIME. Presumably, Social Security can also do this calculation for you — possibly at no charge.
To calculate the total benefits a retiree is eligible for the average indexed monthly salary (AIME) is then divided into three separate salary brackets which are each multiplied by a different benefit percentage for each bracket. The benefits you can receive (the so-called Primary Insurance Amount, PIA) are the sum of the salary in each bracket times the benefit percentages that apply to each bracket. The benefit percentages are set by Congress and so can easily change in the future. The "bendpoints", where the brackets change, are adjusted for inflation each year by Social Security. For example, in 2013 the first bracket runs from $1.00 to $791.00/month and is multiplied by the benefit percentage of 90%, the second salary bracket extends from $791.00 to $4781.00/month is multiplied by 32%, the third salary bracket of more than $4781.00/month is multiplied by 15%. Any higher incomes than the ceiling income are not FICA covered and are not considered in the benefits calculation or in determining the average indexed monthly salary, AIME. At full retirement age the projected retirement income amount (PIA) is the sum of these three brackets of income multiplied by the appropriate benefit percentages—90%, 32% and 15%. Unlike income tax brackets, the Social Security benefits are heavily biased towards lower salaried workers. Social Security has always been primarily a retirement, disability and spousal insurance policy for low wage workers and a very poor retirement plan for higher salaried workers who hopefully have a supplemental retirement plan unless they want to live on significantly less after retirement than they used to earn.
Full retirement age spouses and divorced spouses (married over 10 years before divorce) are entitled to the higher of 50% of the wage earners benefits or their own earned benefits. A low salary worker and his full retirement age spouse making less than or equal to $791/month with 40 quarters of employment credit and at full retirement age (65 if born before 1938, 66 if born from 1938 to 1954 and 67 if born after 1960) could retire with 135% of his indexed average salary. A full retirement age worker and his full retirement age spouse making the ceiling income or more would be eligible for 43% of the ceiling FICA salary (29% if single) and even less if making more than the ceiling income—a supplemental retirement income is highly recommended.
During working years, the low wage worker is eligible for the Earned Income Tax Credit (FICA refunds) and Federal child credits and may pay little or no FICA tax or Income tax. By Congressional Budget Office (CBO) calculations the lowest income quintile (0-20%) and second quintile (21-40%) of households in the U.S. pay an average income tax of -9.3% and -2.6% and Social Security taxes of 8.3% and 7.9% respectively. By CBO calculations the household incomes in the first quintile and second quintile have an average Total Federal Tax rate of 1.0% and 3.8% respectively. Higher income retirees will have to pay income taxes on 85% of their Social Security benefits and 100% on all other retirement benefits they may have.
All workers paying FICA and SECA taxes for forty quarters of credit (QC) or more on a specified minimum income is "fully insured" and eligible to retire at age 62 with reduced benefits. In general the Social Security Administration tries to limit the projected life time benefits to the same amounts of retirement income the recipient would receive if retiring at full retirement age. If a recipient retires earlier he/she draws a lower Social Security benefit income for a longer prospective lifetime after retirement. The basic correction of benefits are age 62 retirees can only draw 75% of what they would draw at full retirement age with higher percentages at different ages more than 62 and less than full retirement age.
Similar computations based on career average adjusted earnings and age of recipient determine disability and survivor benefits. Federal, state and local employees who have elected (when they could) NOT to pay FICA taxes are eligible for a reduced FICA benefits and full Medicare coverage if they have more than forty quarters of qualifying Social Security covered work. To minimize the Social Security payments to those who have not contributed to FICA for 35+ years and are eligible for Federal, State and local benefits, which are usually much more generous, Congress passed the Windfall Elimination Provision, WEP. The WEP provision will not eliminate all Social Security or Medicare eligibility if the worker has 40 quarters of qualifying income, but calculates the benefit payments by reducing the 90% multiplier in the first salary bracket to 40-85% depending on age etc.
For those few cases where workers with very low earnings over a long working lifetime that were too low to receive full retirement credits and the recipients would receive a very small Social Security retirement benefit a "special minimum benefit" (special minimum PIA) provides a "minimum" of $804 per month in Social Security benefits in 2013. To be eligible the recipient along with their auxiliaries and survivors must have very low assets and not be eligible for other retirement system benefits. About 75,000 people in 2013 receive this benefit.
The benefits someone is eligible for are potentially so complicated that potential retirees should consult the Social Security Administration directly for advice. Many questions are addressed and at least partially answered on many online publications and online calculators.
Online Social Security benefits estimate.
On July 22, 2008, the Social Security Administration introduced a new online benefits estimator. A worker who has enough Social Security credits to qualify for benefits, but who is not currently receiving benefits on his or her own Social Security record and who is not a Medicare beneficiary, can obtain an estimate of the retirement benefit that will be provided, for different assumptions about age at retirement. This process is done by opening a secure online account called "my Social Security." For retirees who have non FICA or SECA taxed wages the rules get complicated and probably require additional help.
Normal retirement age.
The earliest age at which (reduced) benefits are payable is 62. Full retirement benefits depend on a retiree's year of birth.
This table was copied in November 2011 from the Social Security Administration web site cited above and referenced in the footnotes. There are different rules for widows and widowers. Also from that site, come the following two notes:
Notes:
1. Persons born on January 1 of any year should refer to the normal retirement age for the previous year.
2. For the purpose of determining benefit reductions for early retirement, widows and widowers whose entitlement is based on having attained age 60 should add 2 years to the year of birth shown in the table.
Those born before 1938 have a normal retirement age of 65. Normal retirement age increases by two months for each ensuing year of birth until 1943, when it reaches 66 and stays at 66 until 1955. Thereafter the normal retirement age increases again by two months for each year until 1960, when normal retirement age is 67 and remains 67 for all individuals born thereafter.
A worker who starts benefits before normal retirement age has their benefit reduced based on the number of months before normal retirement age they start benefits. This reduction is 5/9 of 1% for each month up to 36 and then 5/12 of 1% for each additional month. This formula gives an 80% benefit at age 62 for a worker with a normal retirement age of 65, a 75% benefit at age 62 for a worker with a normal retirement age of 66, and a 70% benefit at age 62 for a worker with a normal retirement age of 67. The 2008–2012 global recession has resulted in an increase in long-term unemployment and an increase in workers taking early retirement.
A worker who delays starting retirement benefits past normal retirement age earns delayed retirement credits that increase their benefit until they reach age 70. These credits are also applied to their widow(er)'s benefit. Children and spouse benefits are not affected by these credits.
The normal retirement age for widow(er) benefits shifts the year-of-birth schedule upward by two years, so that those widow(er)s born before 1940 have age 65 as their normal retirement age.
Spouse's benefit and Government pension offsets.
The spousal "retirement benefit" is "one-half" the "PIA" benefit amount of their spouse or their own earned benefits whichever are higher if they both retire at "normal" retirement ages. Only after the working spouse applies for retirement benefits may the non-working spouse apply for spousal retirement benefits. The spousal benefit is the PIA times an "early-retirement factor" if the spouse is younger than the "normal" full retirement age. The early-retirement factor is 50% minus 25/36 of 1% per month for the first 36 months and 5/12 of 1% for each additional month earlier than the "normal" full retirement date. This typically works out to between 50% and 32.5% of the primary workers PIA benefit. There is no increase for starting spousal benefits "after" normal retirement age. This can occur if there is a married couple in which the younger person is the only worker and is more than 5 years younger. Any current spouse is eligible, and divorced or former spouses are eligible for spousal benefits if the marriage lasted for at least 10 years. It is arithmetically possible for one worker to generate spousal benefits for up to five of his/her spouses that he/she may have, each must be in succession after a proper divorce for each after a marriage that lasted at least ten years each. The spousal "survivor benefit" is the full PIA benefit of the working spouse or their own benefits, whichever are higher.
There is a Social Security Government pension offset that will reduce or eliminate any spousal (or ex-spouse) or widow(er)'s benefits if the spouse or widow(er) is also receiving a government (Federal, State or Local) pension that did not require paying Social Security taxes. The basic "rule" is that Social Security benefits will be reduced by 2/3's of the spouse or widow(er)'s non-FICA taxed government pension. If the spouse's or widow(er)'s government (non-FICA paying) pension exceeds 150% of the "normal" spousal or widow(er)'s benefit the spousal benefit is eliminated. For example a "normal" spousal or widow(er)'s benefit of $1,000/month would be reduced to $0.00 if the spouse or widow(er)'s if already drawing a non-FICA taxed government pension of $1,500/month or more per month. Pensions not based on income does not reduce Social Security spousal or widow(er)'s benefits—how to get a pension not based on income is a mystery.
The passage of the Senior Citizens' Freedom to Work Act, in 2000, allows the worker to earn unlimited outside income without offsets in the year after they reach full retirement. It also allows the spouse and children of a worker who has reached normal full retirement age to receive benefits under some circumstances while he/she does not. The full retirement age worker has to have begun the receipt of benefits, to allow the spousal/children's benefits to begin, and then subsequently suspended his/her own benefits in order to continue the postponement of benefits in exchange for an increased benefit amount (5.5-8.0%/yr increase) up to the age of 70. Thus a worker can delay retirement up to age seventy without affecting spousal or children's benefits.
Delayed Social Security Benefits.
If a worker delays receiving Social Security retirement benefits until after they reach full retirement age there is a Social Security benefits increase by a certain percentage—depending on date of birth. After age 70 there are no more increases in retirement benefits allowed. Social Security uses an "average" survival rate at your full retirement age to prorate the increase in the amount of benefit increase so that the total benefits are roughly the same whenever you retire. Women may benefit more than men from this delayed benefit increase since the "average" survival rates are based on both men and women and women live approximately three years longer than men. The other consideration is that workers only have a limited number of years of "good" health left after they reach full retirement age and unless they enjoy their job they may be passing up an opportunity to do something else they may enjoy doing while they are still relatively healthy.
Social Security Benefits while continuing work.
Due to changing needs or personal preferences, a person may go back to work after retiring. In this case, it is possible to get Social Security retirement or survivors benefits and work at the same time. A worker who is of full retirement age or older may (with spouse) keep all benefits, after taxes, regardless of earnings. But, if this worker or the worker's spouse are younger than full retirement age and receiving benefits and earn “too much”, the benefits will be reduced. If working under full retirement age for the entire year and receiving benefits, Social Security deducts $1 from the worker's benefit payments for every $2 earned above the annual limit of $15,120 (2013). Deductions cease when the benefits have been reduced to zero and the worker will get one more year of income and age credit, slightly increasing future benefits at retirement. For example, if you were receiving benefits of $1,230/month (the average benefit paid) or $14,760 a year and have an income of $29,520/year above the $15,120 limit ($44,640/year) you would lose all ($14,760) of your benefits. If you made $1,000 more than $15,200/year you would "only lose" $500 in benefits. You would get no benefits for the months you work until the $1 deduction for $2 income "squeeze" is satisfied. Your first social security check will be delayed for several months—the first check may only be a fraction of the "full" amount. The benefit deductions change in the year you reach full retirement age and are still working—Social Security only deducts $1 in benefits for every $3 you earn above $40,080 in 2013 for that year and has no deduction thereafter. The income limits change (presumably for inflation) year by year.
Widow(er) benefits.
If a worker covered by Social Security dies, a surviving spouse can receive survivors' benefits. In some instances, survivors' benefits are available even to a divorced spouse. A father or mother with minor or disabled children in his or her care can receive benefits which are not actuarially reduced. The earliest age for a non-disabled widow(er)'s benefit is age 60. The benefit is equal to the worker's full retirement benefit for spouses who are at, or older than, normal retirement age. If the surviving spouse starts benefits before normal retirement age, there is an actuarial reduction. If the worker earned delayed retirement credits by waiting to start benefits after their normal retirement age, the surviving spouse will have those credits applied to their benefit.
Children's benefits.
Children of a retired, disabled or deceased worker receive benefits as a "dependent" or "survivor" if they are under the age of 18, or as long as attending primary or secondary school up to age 19 years, 2 months; or are over the age of 18 and were disabled before the age of 22.
In "Astrue v. Capato" (2012), the Supreme Court unanimously held that children conceived after a parent's death (by in vitro fertilization procedure) are not entitled to Social Security survivors' benefits if the laws of the state in which the parent's will was signed do not provide for such benefits.
Disability.
A worker who has worked long enough and recently enough (based on "quarters of coverage" within the recent past) to be covered "can" receive disability benefits. These benefits start after five full calendar months of disability, regardless of his or her age. The eligibility formula requires a certain number of credits (based on earnings) to have been earned overall, and a certain number within the ten years immediately preceding the disability, but with more-lenient provisions for younger workers who become disabled before having had a chance to compile a long earnings history.
The worker must be unable to continue in his or her previous job and unable to adjust to other work, with age, education, and work experience taken into account; furthermore, the disability must be long-term, lasting 12 months, expected to last 12 months, resulting in death, or expected to result in death. As with the retirement benefit, the amount of the disability benefit payable depends on the worker's age and record of covered earnings.
Supplemental Security Income (SSI) uses the same disability criteria as the insured social security disability program, but SSI is not based upon insurance coverage. Instead, a system of means-testing is used to determine whether the claimants' income and net worth fall below certain income and asset thresholds.
Severely disabled children may qualify for SSI. Standards for child disability are different from those for adults.
Disability determination at the Social Security Administration has created the largest system of administrative courts in the United States. Depending on the state of residence, a claimant whose initial application for benefits is denied can request "reconsideration" or a "hearing before an Administrative Law Judge (ALJ)". Such hearings sometimes involve participation of an independent vocational expert (VE) or medical expert (ME), as called upon by the ALJ.
Reconsideration involves a re-examination of the evidence and, in some cases, the opportunity for a hearing before a (non-attorney) disability hearing officer. The hearing officer then issues a decision in writing, providing justification for his/her finding. If the claimant is denied at the reconsideration stage, (s)he may request a hearing before an Administrative Law Judge. In some states, SSA has implemented a pilot program that eliminates the reconsideration step and allows claimants to appeal an initial denial directly to an Administrative Law Judge.
Because the number of applications for Social Security disability is very large (approximately 650,000 applications per year), the number of hearings requested by claimants often exceeds the capacity of Administrative Law Judges. The number of hearings requested and availability of Administrative Law Judges varies geographically across the United States. In some areas of the country, it is possible for a claimant to have a hearing with an Administrative Law Judge within 90 days of his/her request. In other areas, waiting times of 18 months are not uncommon.
After the hearing, the Administrative Law Judge (ALJ) issues a decision in writing. The decision can be "Fully Favorable" (the ALJ finds the claimant disabled as of the date that (s) he alleges in the application through the present), "Partially Favorable" (the ALJ finds the claimant disabled at some point, but not as of the date alleged in the application; OR the ALJ finds that the claimant "was" disabled but has improved), or "Unfavorable" (the ALJ finds that the claimant was not disabled at all). Claimants can appeal decisions to Social Security's Appeals Council, which is in Virginia. The Appeals Council does not hold hearings; it accepts written briefs. Response time from the Appeals Council can range from 12 weeks to more than 3 years.
If the claimant disagrees with the Appeals Council's decision, (s)he can appeal the case in the federal district court for his/her jurisdiction. As in most federal court cases, an unfavorable district court decision can be appealed to the appropriate United States Court of Appeals, and an unfavorable appellate court decision can be appealed to the United States Supreme Court.
The Social Security Administration has maintained its goal for judges to resolve 500-700 cases per year but an Administrative Law Judge on the average nationwide disposes of approximately 400 cases per year. The debate about the social security system in the United States has been ongoing for decades and there is much concern about its sustainability.
Current operation.
Joining and quitting.
Obtaining a Social Security number for a child is voluntary. Further, there is no general legal requirement that individuals join the Social Security program unless they want or have to work. Under normal circumstances, FICA taxes or SECA taxes will be collected on all wages. About the only way to avoid paying either FICA or SECA taxes are to join a "Religion" that doesn't believe in insurance, such as the Amish, Christian Science or a religion whose members have taken a vow of poverty (see IRS publication 517 and 4361). Federal workers employed before 1987, various state and local workers including those in some school districts who had their own retirement and disability programs were given the one-time option of joining Social Security. Many employees and retirement and disability systems opted to keep out of the Social Security system because of the cost and the limited benefits. It was often much cheaper to obtain much higher retirement and disability benefits by staying in their original retirement and disability plans. Now only a few of these plans allow new hires to join their existing plans without also joining Social Security. In 2004, the Social Security Administration estimated that 96% of all U.S. workers were covered by the system with the remaining 4% mostly a minority of government employees enrolled in public employee pensions and not subject to Social Security taxes due to historical exemptions.
If you went to work for a railroad it is possible to get a "coordinated" retirement and disability benefits. The U.S. 'Railroad Retirement Board (or 'RRB') is an independent agency in the executive branch of the United States government created in 1935 to administer a social insurance program providing retirement benefits to the country's railroad workers. Railroad retirement Tier I payroll taxes are coordinated with social security taxes so that employees and employers pay Tier I taxes at the same rate as social security taxes and have the same benefits. In addition, both workers and employers pay Tier II taxes (about 6.2% in 2005) which are used to finance railroad retirement and disability benefit payments that are over and above social security levels. Tier 2 benefits are a supplemental retirement and disability benefit system which pays 0.875% times years of service times average highest five years of employment salary, in addition to Social Security benefits.
The FICA taxes are imposed on nearly all workers and self-employed persons. Employers are required to report wages for covered employment to Social Security for processing Forms W-2 and W-3. There are some specific wages which are not a part of the Social Security program (discussed below). Internal Revenue Code provisions section 3101 imposes payroll taxes on individuals and employer matching taxes. Section 3102 mandates that employers deduct these payroll taxes from workers' wages before they are paid. Generally, the payroll tax is imposed on everyone in employment earning "wages" as defined in 3121 of the Internal Revenue Code. and also taxes net earnings from self-employment.
Trust fund.
Social Security taxes are paid into the Social Security Trust Fund maintained by the U.S. Treasury (technically, the "Federal Old-Age and Survivors Insurance Trust Fund", as established by  ). Current year expenses are paid from current Social Security tax revenues. When revenues exceed expenditures, as they did between 1983 and 2009, the excess is invested in special series, non-marketable U.S. Government bonds. Thus, the Social Security Trust Fund indirectly finances the federal government's general purpose deficit spending. In 2007, the cumulative excess of Social Security taxes and interest received over benefits paid out stood at $2.2 trillion. The Trust Fund is regarded by some as an accounting construct which holds no economic significance. Others argue that it has specific legal significance because the Treasury securities it holds are backed by the "full faith and credit" of the U.S. government, which has an obligation to repay its debt.
The Social Security Administration's authority to make benefit payments as granted by Congress extends only to its current revenues and existing Trust Fund balance, "i.e.", redemption of its holdings of Treasury securities. Therefore, Social Security's ability to make full payments once annual benefits exceed revenues depends in part on the federal government's ability to make good on the bonds that it has issued to the Social Security trust funds. As with any other federal obligation, the federal government's ability to repay Social Security is based on its power to tax and borrow and the commitment of Congress to meet its obligations.
In 2009 the Office of the Chief Actuary of the Social Security Administration calculated an unfunded obligation of $15.1 trillion for the Social Security program. The unfunded obligation is the difference between the future cost of Social Security (based on several demographic assumptions such as mortality, work force participation, immigration, and age expectancy) and total assets in the Trust Fund given the expected contribution rate through the current scheduled payroll tax. This unfunded obligation is expressed in present value dollars and is a part of the Fund's long-range actuarial estimates, not necessarily a certainty of what will occur in the long run. An Actuarial Note to the calculation says that "The term obligation is used in lieu of the term liability, because liability generally indicates a contractual obligation (as in the case of private pensions and insurance) that cannot be altered by the plan sponsor without the agreement of the plan participants."
Office of Disability Adjudication and Review (ODAR).
The Office of Disability Adjudication and Review (ODAR), known before 2006 as the Office of Hearings and Appeals (OHA), administers the hearings and appeals program for the Social Security Administration (SSA). Administrative Law Judges (ALJs) conduct hearings and issue decisions. The Appeals Council considers appeals from hearing decisions, and acts as the final level of administrative review for the Social Security Administration.
Social Security Benefit payout comparisons.
Some Federal, state, local and education government employees pay no Social Security but have their own retirement, disability systems that nearly always pay much better retirement and disability benefits than Social Security. These plans typically requires vesting—working for 5–10 years for the same employer before becoming eligible for retirement. But their retirement typically only depends on the average of the best 3–10 years salaries times some retirement factor (typically 0.875%-3.0%) times years employed. This retirement benefit can be a "reasonably good" (75%-85% of salary) retirement at close to the monthly salary they were last employed at. For example, if a person joined the University of California retirement system at age 25 and worked for 35 years they could receive 87.5% (2.5% x 35) of their average highest three year salary with full medical coverage at age 60. Police and firemen who joined at 25 and worked for 30 years could receive 90% (3.0% x 30) of their average salary and full medical coverage at age 55. These retirements have cost of living adjustments (COLA) applied each year but are limited to a maximum average income of $350,000/year or less. Spousal survivor benefits are available at 100%-67% of the primary benefits rate for 8.7% to 6.7% reduction in retirement benefits, respectively. UCRP retirement and disability plan benefits are funded by contributions from both members and the University (typically 5% of salary each) and by the compounded investment earnings of the accumulated totals. These contributions and earnings are held in a trust fund that is invested. The retirement benefits are much more generous than Social Security but are believed to be actuarially sound. The main difference between state, local government sponsored retirement systems and Social Security is the state and local retirement systems use compounded investments that are usually heavily weighted in the stock market securities which historically have returned more than 7.0%/year on average despite some years with losses. Short term federal government investments may be "more" secure but pay much lower average percentages. Nearly all other federal, state and local retirement systems work in a similar fashion with different benefit retirement ratios. Some plans are now combined with Social Security and are “piggy backed” on top of Social Security benefits. For example, the current Federal Employees Retirement System, which covers the vast majority of federal civil service employees hired after 1986, combines Social Security, a modest defined-benefit pension (1.1% per year of service) and the defined-contribution Thrift Savings Plan.
The current Social Security formula used in calculating the benefit level (primary insurance amount or PIA) is very biased towards lower average salaries. Anyone who worked in OASDI covered employment and other retirement would be entitled to both the alternative non OASDI pension and an Old Age retirement benefit from Social Security. Because of their limited time working in OASDI covered employment the sum of their covered salaries times inflation factor divided by 420 months yields a low adjusted indexed monthly salary over 35 years, AIME. The low wage bias of the PIA formula would in effect allow these workers to also get a slightly higher Social Security Benefit percentage on this low average salary. Congress passed in 1983 the Windfall Elimination Provision to minimize Social Security benefits for these recipients. The basic provision is that the first salary bracket, 0-$791/month (2013) has its normal benefit percentage of 90% reduced to 40-90%--see Social Security for the exact percentage. The reduction is limited to roughly 50% of what you would be eligible for if you had always worked under OASDI taxes. The 90% benefit percentage factor is not reduced if you have 30 or more years of “substantial” earnings.
Social Security should only be the “minimum” retirement that one has as the retirement benefits are relatively small for workers earning even average and higher salaries. The average Social Security payment of $1,230/month ($14,760/year) in 2013 is only slightly above the Federal poverty level for one--$11,420/yr and below the poverty guideline of $15,500/yr for two. One “good” supplemental retirement plan option is an employer sponsored 401(K) (or 403(B)) plan when they are offered by your employer. Many employers will match a portion of your savings $1.00 for $1.00 up to some percentage of your salary. Even without employer matches, Individual retirement accounts (IRAs) are portable, self-directed, tax deferred retirement accounts that offer the potential to substantially increase retirement savings. Their main limitations are their requirements of self-discipline to allot from an early age the required percentage of salary into "good" investment account(s), and the self–discipline needed to leave it there to earn compound interest until needed after retirement. Long term investment horizons should be used as historically short term investment losses "self correct" and most investments continue to deliver good average investment returns. Living on less than you earn and investing the rest over a long period of time is still the "best" retirement plan--compounded earnings steadily accumulate if given enough time. The IRS has tax penalties for withdrawals from IRAs, 401(K)s, etc. before 59½ and require mandatory withdrawals once the retiree reaches 70 and other restrictions on the amount of tax deferred income one can put in the account(s). (see: ). Self-directed retirement savings plans have the potential to match or even exceed the benefits earned by federal, state and local government retirement plans.
International agreements.
People sometimes relocate from one country to another, either permanently or on a limited-time basis. This presents challenges to businesses, governments, and individuals seeking to ensure future benefits or having to deal with taxation authorities in multiple countries. To that end, the Social Security Administration has signed treaties, often referred to as "Totalization Agreements," with other social insurance programs in various foreign countries.
Overall, these agreements serve two main purposes. First, they eliminate dual Social Security taxation, the situation that occurs when a worker from one country works in another country and is required to pay Social Security taxes to both countries on the same earnings. Second, the agreements help fill gaps in benefit protection for workers who have divided their careers between the United States and another country.
The following countries have signed totalization agreements with the SSA (and the date the agreement became effective):<ref name="www.ssa.gov/international"></ref>
Social Security number.
A side effect of the Social Security program in the United States has been the near-universal adoption of the program's identification number, the Social Security number, as the "de facto" U.S. national identification number. The social security number, or SSN, is issued pursuant to section 205(c)(2) of the Social Security Act, codified as  . The government originally stated that the SSN would not be a means of identification, but currently a multitude of U.S. entities use the Social Security number as a personal identifier. These include government agencies such as the Internal Revenue Service, the military as well as private agencies such as banks, colleges and universities, health insurance companies, and employers.
Although the Social Security Act itself does not require a person to have a Social Security Number (SSN) to live and work in the United States, the Internal Revenue Code does generally require the use of the social security number by individuals for federal tax purposes:
Importantly, most parents apply for Social Security numbers for their dependent children in order to include them on their income tax returns as a dependent. Everyone filing a tax return, as taxpayer or spouse, must have a Social Security Number or Taxpayer Identification Number (TIN) since the IRS is unable to process returns or post payments for anyone without an SSN or TIN.
The Privacy Act of 1974 was in part intended to limit usage of the Social Security number as a means of identification. Paragraph (1) of subsection (a) of section 7 of the Privacy Act, an uncodified provision, states in part:
However, the Social Security Act provides:
Further, paragraph (2) of subsection (a) of section 7 of the Privacy Act provides in part:
The exceptions under section 7 of the Privacy Act include the Internal Revenue Code requirement that social security numbers be used as taxpayer identification numbers for individuals.
Demographic and revenue projections.
In each year since 1982, OASDI tax receipts, interest payments and other income have exceeded benefit payments and other expenditures, for example by more than $150 billion in 2004. As the "baby boomers" move out of the work force and into retirement, however, expenses will come to exceed tax receipts and then, after several more years, will exceed all OASDI trust income, including interest. At that point the system will begin drawing on its trust fund Treasury Notes, and will continue to pay benefits at the current levels until the Trust Fund is exhausted. In 2013, the OASDI retirement insurance fund collected $731.1 billion and spent $645.5 billion; the disability program (DI) collected $109.1 billion and spent $140.3 billion; Medicare (HI) collected $243.0 and spent $266.8 billion and Supplementary Medical Insurance, SMI, collected $293.9 billion and spent $307.4 billion. In 2013 all Social Security programs except the retirement trust fund (OASDI) spent more than they brought in and relied on significant withdrawals from their respective trust funds to pay their bills. The retirement (OASDI) trust fund of $2,541 billion is expected to be emptied by 2033 by one estimate as new retirees become eligible to join. The disability (DI) trust fund’s $153.9 billion will be exhausted by 2018; the Medicare (HI) trust fund of $244.2 billion will be exhausted by 2023 and the Supplemental Medical Insurance (SMI) trust fund will be exhausted by 2020 if the present rate of withdrawals continues—even sooner if they increase. The total “Social Security” expenditures in 2013 were $1,360 billion dollars which were 8.4% of the $16,200 billion GNP (2013) and 37.0% of the Federal expenditures of $3,684 billion (including a $971.0 billion deficit). All other parts of the Social Security program: medicare (HI), disability (DI) and Supplemental Medical (SMI) trust funds are already drawing down their trust funds and are projected to go into deficit in about 2020 if the present rate of withdrawals continue. As the trust funds are exhausted either benefits will have to be cut, fraud minimized or taxes increased. According to the Center for Economic and Policy Research, upward redistribution of income is responsible for about 43% of the projected Social Security shortfall over the next 75 years.
In 2005, this exhaustion of the OASDI Trust Fund was projected to occur in 2041 by the Social Security Administration or by 2052 by the Congressional Budget Office, CBO. Thereafter, however, the projection for the exhaustion date of this event was moved up slightly after the recession worsened the U.S. economy's financial picture. The 2011 OASDI Trustees Report stated:
"Annual cost exceeded non-interest income in 2010 and is projected to continue to be larger throughout the remainder of the 75-year valuation period. Nevertheless, from 2010 through 2022, total trust fund income, including interest income, is more than is necessary to cover costs, so trust fund assets will continue to grow during that time period. Beginning in 2023, trust fund assets will diminish until they become exhausted in 2036. Non-interest income is projected to be sufficient to support expenditures at a level of 77 percent of scheduled benefits after trust fund exhaustion in 2036, and then to decline to 74 percent of scheduled benefits in 2085."
In 2007, the Social Security Trustees suggested that either the payroll tax could increase to 16.41 percent in 2041 and steadily increased to 17.60 percent in 2081 or a cut in benefits by 25 percent in 2041 and steadily increased to an overall cut of 30 percent in 2081.
The Social Security Administration projects that the demographic situation will stabilize. The cash flow deficit in the Social Security system will have leveled off as a share of the economy. This projection has come into question. Some demographers argue that life expectancy will improve more than projected by the Social Security Trustees, a development that would make solvency worse. Some economists believe future productivity growth will be higher than the current projections by the Social Security Trustees. In this case, the Social Security shortfall would be smaller than currently projected.
Tables published by the government's National Center for Health Statistics show that life expectancy at birth was 47.3 years in 1900, rose to 68.2 by 1950 and reached 77.3 in 2002. The latest annual report of the Social Security Agency (SSA) trustees projects that life expectancy will increase just six years in the next seven decades, to 83 in 2075. A separate set of projections, by the Census Bureau, shows more rapid growth.) The Census Bureau projection is that the longer life spans projected for 2075 by the Social Security Administration will be reached in 2050. Other experts, however, think that the past gains in life expectancy cannot be repeated, and add that the adverse effect on the system's finances may be partly offset if health improvements or reduced retirement benefits induce people to stay in the workforce longer.
Actuarial science, of the kind used to project the future solvency of social security, is by nature subject to uncertainty. The SSA actually makes three predictions: optimistic, midline, and pessimistic (until the late 1980s it made 4 projections). The Social Security crisis that was developing prior to the 1983 reforms resulted from midline projections that turned out to be too optimistic. It has been argued that the overly pessimistic projections of the mid to late 1990s were partly the result of the low economic growth (according to actuary David Langer) assumptions which resulted in the projected exhaustion date being pushed back (from 2028 to 2042) with each successive Trustee's report. During the heavy-boom years of the '90s, the midline projections were too pessimistic. Obviously, projecting out 75 years is a significant challenge and, as such, the actual situation might be much better or much worse than predicted.
The Social Security Advisory Board has on three occasions since 1999 appointed a Technical Advisory Panel to review the methods and assumptions used in the annual projections for the Social Security trust funds. The most recent report of the Technical Advisory Panel, released in June 2008 with a copyright date of October 2007, includes a number of recommendations for improving the Social Security projections.
s of December 2013[ [update]], under current law, the Congressional Budget Office reported that the "Disability Insurance trust fund will be exhausted in fiscal year 2017 and the Old-Age and Survivors Insurance trust fund will be exhausted in 2033".
Increased spending for Social Security will occur at the same time as increases in Medicare, as a result of the aging of the baby boomers. One projection illustrates the relationship between the two programs:
Ways to eliminate the projected Social Security shortfall.
Social Security is predicted to start running out of having enough money to pay all prospective retirees at today's benefit payouts by 2033.
Taxation.
Tax on wages and self-employment income.
Benefits are funded by taxes imposed on wages of employees and self-employed persons. As explained below, in the case of employment, the employer and employee are each responsible for one half of the Social Security tax, with the employee's half being withheld from the employee's pay check. In the case of self-employed persons (i.e., independent contractors), the self-employed person is responsible for the entire amount of Social Security tax.
The portion of taxes collected from the employee for Social Security are referred to as "trust fund taxes" and the employer is required to remit them to the government. These taxes take priority over everything, and represent the only debts of a corporation or LLC that can impose personal liability upon its officers or managers. A sole proprietor and officers of a corporation and managers of an LLC can be held personally liable for non-payment of the income tax and social security taxes whether or not actually collected from the employee.
The "Federal Insurance Contributions Act" (FICA) (codified in the Internal Revenue Code) imposes a Social Security withholding tax equal to 6.20% of the gross wage amount, up to but not exceeding the "Social Security Wage Base" ($97,500 for 2007; $102,000 for 2008; and $106,800 for 2009, 2010, and 2011). The same 6.20% tax is imposed on employers. For 2011 and 2012, the employee's contribution was reduced to 4.2%, while the employer's portion remained at 6.2%. In 2012, the wage base increased to $110,100. In 2013, the wage base increased to $113,700. For each calendar year for which the worker is assessed the FICA contribution, the SSA credits those wages as that year's covered wages. The income cutoff is adjusted yearly for inflation and other factors.
A separate payroll tax of 1.45% of an employee's income is paid directly by the employer, and an additional 1.45% deducted from the employee's paycheck, yielding a total tax rate of 2.90%. There is no maximum limit on this portion of the tax. This portion of the tax is used to fund the Medicare program, which is primarily responsible for providing health benefits to retirees.
The Social Security tax rates from 1937–2010 can be accessed on the Social Security Administration's website.
The combined tax rate of these two federal programs is 15.30% (7.65% paid by the employee and 7.65% paid by the employer). In 2011-2012 it temporarily dropped to 13.30% (5.65% paid by the employee and 7.65% paid by the employer).
For self-employed workers (who technically are not employees and are deemed not to be earning "wages" for Federal tax purposes), the self-employment tax, imposed by the Self-Employment Contributions Act of 1954, codified as Chapter 2 of Subtitle A of the Internal Revenue Code, #redirect , is 15.3% of "net earnings from self-employment." In essence, a self-employed individual pays both the employee and employer share of the tax, although half of the self-employment tax (the "employer share") is deductible when calculating the individual's federal income tax.
If an employee has overpaid payroll taxes by having more than one job or switching jobs during the year, the excess taxes will be refunded when the employee files his federal income tax return. Any excess taxes paid by employers, however, are not refundable to the employers.
Wages not subject to tax.
Workers are not required to pay Social Security taxes on wages from certain types of work:
Federal income taxation of benefits.
Originally the benefits received by retirees were not taxed as income. Beginning in tax year 1984, with the Reagan-era reforms to repair the system's projected insolvency, retirees with incomes over $25,000 (in the case of married persons filing separately who did not live with the spouse at any time during the year, and for persons filing as "single"), or with combined incomes over $32,000 (if married filing jointly) or, in certain cases, any income amount (if married filing separately from the spouse in a year in which the taxpayer lived with the spouse at any time) generally saw part of the retiree benefits subject to Federal income tax. In 1984, the portion of the benefits potentially subject to tax was 50%. The Deficit Reduction Act of 1993 set the portion to 85%.
Criticisms.
Claim that it discriminates against the poor and the middle class.
Workers must pay 12.4 percent, including a 6.2 percent employer contribution, on their wages below the Social Security Wage Base ($110,100 in 2012), but no tax on income in excess of this amount. Therefore, high earners pay a lower percentage of their total income because of the income caps; because of this, and the fact there is no tax on unearned income, social security taxes are often viewed as being regressive. However, benefits are adjusted to be significantly more progressive, even when accounting for differences in life expectancy. According to the non-partisan Congressional Budget Office, for people in the bottom fifth of the earnings distribution, the ratio of benefits to taxes is almost three times as high as it is for those in the top fifth.
Supporters of Social Security say that despite its regressive "tax formula", Social Security benefits are calculated using a progressive "benefit formula" that replaces a much higher percentage of low-income workers' pre-retirement income than that of higher-income workers (although these low-income workers pay a higher percentage of their pre-retirement income). They also point to numerous studies that show that, relative to high-income workers, Social Security disability and survivor benefits paid on behalf of low-income workers more than offset any retirement benefits that may be lost because of shorter life expectancy, but this offset requires an individual to be disabled. Other research asserts that survivor benefits, allegedly an offset, actually exacerbate the problem because survivor benefits are denied to single individuals, including widow(er)s married less than nine months (except in certain situations), divorced widow(er)s married less than 10 years, and co-habiting or same-sex couples, unless they are legally married in their state of residence. Unmarried individuals and minorities tend to be less wealthy.
Social Security's benefit formula provides 90% of average indexed monthly earnings (AIME) below the first "bend point" of $791/month, 32% of AIME between the first and second bend points $791 to $4781/month, and 15% of AIME in excess of the second bend point up to the Ceiling cap of $113,700 in 2013. The low income bias of the benefit calculation means that lower paid worker receives a much higher percentage of his or salary in benefit payments than higher paid workers. Indeed a married low salaried worker can receive over 100% of their salary in benefits after retiring at the full retirement age. High salaried workers receive 43% or less of their salary in benefits despite having paid into the "system" at the same rate--(see benefit calculations above.) To minimize the impact of Social Security taxes on low salaried workers the Earned Income Tax Credit" and the Child Care Tax Credit were passed which largely refund the FICA and or SECA payments of low salaried workers through the income tax system. By Congressional Budget Office (CBO) calculations the lowest income quintile (0-20%) and second quintile (21-40%) of households in the U.S. pay an average Federal Income Tax of -9.3% and -2.6% of income and Social Security taxes of 8.3% and 7.9% of income respectively. By CBO calculations the household incomes in the first quintile and second quintile have an average Total Federal tax rate of 1.0% and 3.8% respectively. However, these groups also have by far the smallest percentage of American household incomes - the first quintile earns just 3.2% of all income, while the second quintile earns only 8.4% of all income. Higher income retirees will have to pay income taxes on 85% of their Social Security benefits and 100% on all other retirement benefits they may have.
Marital Status.
The Social Security Act defines the rules for determining marital relationships for SSI recipients. The act requires that if a man and a woman are found to be "holding out"—that is, presenting themselves to the community as husband and wife—they should be considered married for purposes of the SSI program. Consequently, if the claimant is found disabled and found to be "holding out"; this claimant will be entitled of reduced or no SSI benefits. However, the Social Security Act does not accept that a claimant "holding out as husband or wife" should be entitled of Survivor, Retirement or Widows benefits, when the claimant's "husband or wife" pass away. SSA rules and regulations about marital status either prohibit (SRDI program) or reduce (SSI program) benefits to indigent claimants.
Claim that politicians exempted themselves from the tax.
Critics of Social Security have said that the politicians who created Social Security exempted themselves from having to pay the Social Security tax. When the federal government created Social Security, all federal employees, including the President and members of Congress, were exempt from having to pay the Social Security tax, and they received no Social Security benefits. This law was changed by the Social Security Amendments of 1983, which brought within the Social Security system all members of Congress, the President and the Vice President, federal judges, and certain executive-level political appointees, as well as all federal employees hired in any capacity on or after January 1, 1984. Many state and local government workers, however, are exempt from Social Security taxes because they contribute instead to alternative retirement systems set up by their employers.
Claim that the government lied about the maximum tax.
George Mason University economics professor Walter E. Williams claimed that the federal government has broken its own promise regarding the maximum Social Security tax. Williams used data from the federal government to back up his claim.
According to a 1936 pamphlet on the Social Security website, the federal government promised the following maximum level of taxation for Social Security, "... beginning in 1949, twelve years from now, you and your employer will each pay 3 cents on each dollar you earn, up to $3,000 a year. That is the most you will ever pay."
However, according to the Social Security website, by the year 2008, the tax rate was 6.2% each for the employer and employee, and the maximum income level that was subject to the tax was $102,000 raising the bar to $6,324 maximum contribution by both employee and employer (total $12,648).
In 2005, Dr. Williams wrote, "Had Congress lived up to those promises, where $3,000 was the maximum earnings subject to Social Security tax, controlling for inflation, today's $50,000-a-year wage earner would pay about $700 in Social Security taxes, as opposed to the more than $3,000 that he pays today."
According to the Social Security website, "The tax rate in the original 1935 law was 1% each on the employer and the employee, on the first $3,000 of earnings. This rate was increased on a regular schedule in four steps so that by 1949 the rate would be 3% each on the first $3,000. The figure was never $1,400, and the rate was never fixed for all time at 1%."
Claim that it gives a low rate of return.
Critics of Social Security claim that it gives a low rate of return, compared to what is obtained through private retirement accounts. For example, critics point out that under the Social Security laws as they existed at that time, several thousand employees of Galveston County, Texas were allowed to opt out of the Social Security program in the early 1980s, and have their money placed in a private retirement plan instead. While employees who earned $50,000 per year would have collected $1,302 per month in Social Security benefits, the private plan paid them $6,843 per month. While employees who earned $20,000 per year would have collected $775 per month in Social Security benefits, the private plan paid them $2,740 per month, at interest rates prevailing in 1996. While some advocates of privatization of Social Security point to the Galveston pension plan as a model for Social Security reform, critics point to a GAO report to the House Ways and Means Committee, which indicates that, for low and middle income employees, particularly those with shorter work histories, the outcome may be less favorable.
This claim also discounts the fact that investment in private markets is not risk-free and private investments can and often do lose value. A person whose investments fail for whatever reason may lose everything they invest and enter their retirement years penniless. Therefore, advocates argue, Social Security plays an important role by providing every American worker a guaranteed minimum level of retirement income that cannot be lost to market fluctuations, disappear through business failures or be stolen by fraudulent investment schemes.
Claim that it is a Ponzi scheme.
Critics have drawn parallels between Social Security and Ponzi schemes, e.g.:
...the vast majority of the money you pay in Social Security taxes is not invested in anything. Instead, the money you pay into the system is used to pay benefits to those "early investors" who are retired today. When you retire, you will have to rely on the next generation of workers behind you to pay the taxes that will finance your benefits.
As with Ponzi’s scheme, this turns out to be a very good deal for those who got in early. The very first Social Security recipient, Ida Mae Fuller of Vermont, paid just $44 in Social Security taxes, but the long-lived Mrs. Fuller collected $20,993 in benefits. Such high returns were possible because there were many workers paying into the system and only a few retirees taking benefits out of it. In 1950, for instance, there were 16 workers supporting every retiree. Today, there are just over three. By around 2030, we will be down to just two.
As with Ponzi’s scheme, when the number of new contributors dries up, it will become impossible to continue to pay the promised benefits. Those early windfall returns are long gone. When today’s young workers retire, they will receive returns far below what private investments could provide. Many will be lucky to break even.—Michael Tanner
One criticism of the analogy is that while Ponzi schemes and Social Security have similar "structures" (in particular, a sustainability problem when the number of new people paying in is declining), they have different "transparencies". In the case of a Ponzi scheme, the fact that there is no return-generating mechanism other than contributions from new entrants is obscured whereas Social Security payouts have always been openly underwritten by incoming tax revenue and the interest on the Treasury bonds held by or for the Social Security system. The sudden loss of confidence resulting in a collapse of a conventional Ponzi scheme when the scheme's true nature is revealed is unlikely to occur in the case of the Social Security system. Private sector Ponzi schemes are also vulnerable to collapse because they cannot compel new entrants, whereas participation in the Social Security program is a condition for joining the U.S. labor force. In connection with these and other issues, Robert E. Wright calls Social Security a "quasi" pyramid scheme in his book, "Fubarnomics".
Estimated net Social Security benefits under differing circumstances.
In 2004, Urban Institute economists C. Eugene Steuerle and Adam Carasso created a Web-based Social Security benefits calculator. Using this calculator it is possible to estimate net Social Security benefits (i.e., estimated lifetime benefits minus estimated lifetime FICA taxes paid) for different types of recipients. In the book "Democrats and Republicans – Rhetoric and Reality" Joseph Fried used the calculator to create graphical depictions of the estimated net benefits of men and women who were at different wage levels, single and married (with stay-at-home spouses), and retiring in different years. These graphs vividly show that generalizations about Social Security benefits may be of little predictive value for any given worker, due to the wide disparity of net benefits for people at different income levels and in different demographic groups. For example, the graph below (Figure 168) shows the impact of wage level and retirement date on a male worker. As income goes up, net benefits get smaller – even negative.
However, the impact is much greater for the future retiree (in 2045) than for the current retiree (2005). The male earning $95,000 per year and retiring in 2045 is estimated to lose over $200,000 by participating in the Social Security system.
In the next graph (Figure 165) the depicted net benefits are averaged for people turning age 65 anytime during the years 2005 through 2045. (In other words, the disparities shown are not related to retirement.) However, we do see the impact of gender and wage level. Because women tend to live longer, they generally collect Social Security benefits for a longer time. As a result, they get a higher net benefit, on average, no matter what the wage level.
The next image (Figure 166) shows estimated net benefits for married men and women at different wage levels. In this particular scenario it is assumed that the spouse has little or no earnings and, thus, will be entitled to collect a spousal retirement benefit. According to Fried:
"Two significant factors are evident: First, every column in Figure 166 depicts a net benefit that is higher than any column in Figure 165. In other words, the average married person (with a stay-at-home spouse) gets a greater benefit per FICA tax dollar paid than does the average single person – no matter what the gender or wage level. Second, there is only limited progressivity among married workers with stay-at-home spouses. Review Figure 166 carefully: The net benefits drop as the wage levels increase from $50,000 to $95,000; however, they increase as the wage levels grow from $5,000 to $50,000. In fact, net benefits are lowest for those earning just $5,000 per year."
The last graph shown (Figure 167) is a combination of Figures 165 and 166. In this graph it is very clear why generalizations about the value of Social Security benefits are meaningless. At the $95,000 wage level a married person could be a big winner – getting net benefits of about $165,000. On the other hand, he could lose an estimated $152,000 in net benefits if he remains single. Altogether, there is a "swing" of over $300,000 based upon the marriage decision (and the division of earnings between the spouses). In addition there is a large disparity between the high net benefits of the married person earning $95,000 ($165,152) versus the relatively low net benefits of the man or woman earning just $5,000 ($30,025 or $41,890, depending on gender). In other words, the high earner, in this scenario, gets a far greater return on his FICA tax investment than does the low earner.
In the book "How Social Security Picks Your Pocket" other factors affecting Social Security net benefits are identified: Generally, people who work for more than 35 years get a lower net benefit – all other factors being equal. People who do not live long after retirement age get a much lower net benefit. Finally, people who derive a high percentage of income from non-wage sources get high Social Security net benefits because they appear to be poor, when they are not. The progressive benefit formula for Social Security is blind to the income a worker may have from non-wage sources, such as spousal support, dividends and interest, or rental income.
Current controversies.
Proposals to reform of the Social Security system have led to heated debate, centering around funding of the program. In particular, proposals to "privatize" funding have caused great controversy.
Contrast with private pensions.
Although Social Security is sometimes compared to private pensions, the two systems are different in a number of respects. It has been argued that Social Security is an insurance plan as opposed to a retirement plan. Unlike a pension, for example, Social Security pays disability benefits. A private pension fund accumulates the money paid into it, eventually using those reserves to pay pensions to the workers who contributed to the fund; and a private system is not universal. Social Security cannot "prefund" by investing in marketable assets such as equities, because federal law prohibits it from investing in assets other than those backed by the U.S. government. As a result, its investments to date have been limited to special non-negotiable securities issued by the U.S. Treasury, although some argue that debt issued by the Federal National Mortgage Association and other quasi-governmental organizations could meet legal standards. Social Security cannot by law invest in private equities, although some other countries (such as Canada) and some states permit their pension funds to invest in private equities. As a universal system, Social Security generally operates as a pipeline, through which current tax receipts from workers are used to pay current benefits to retirees, survivors, and the disabled. When there is an excess of taxes withheld over benefits paid, by law this excess is invested in Treasury securities (not in private equities) as described above.
Two broad categories of private pension plans are "defined benefit pension plans" and "defined contribution pension plans." Of these two, Social Security is more similar to a defined benefit pension plan. In a defined benefit pension plan, the benefits ultimately received are based on some sort of pre-determined formula (such as one based on years worked and highest salary earned). Defined benefit pension plans generally do not include separate accounts for each participant. By contrast, in a defined contribution pension plan each participant has a specific account with funds put into that account (by the employer or the participant, or both), and the ultimate benefit is based on the amount in that account at the time of retirement. Some have proposed that the Social Security system be modified to provide for the option of individual accounts (in effect, to make the system, at least in part, more like a defined contribution pension plan). Specifically, on February 2, 2005, President George W. Bush made Social Security a prominent theme of his State of the Union Address. He described the Social Security system as "headed for bankruptcy", and outlined, in general terms, a proposal based on partial privatization. Critics responded that privatization would require huge new government borrowing to fund benefit payments during the transition years. See Social Security debate (United States).
Both "defined benefit" and "defined contribution" private pension plans are governed by the Employee Retirement Income Security Act (ERISA), which requires employers to provide minimum levels of funding to support "defined benefits" pensions. The purpose is to protect the workers from corporate mismanagement and outright bankruptcy, although in practice many private pension funds have fallen short in recent years. In terms of financial structure, the current Social Security system is analogous to an underfunded "defined benefit" pension ("underfunded" meaning not that it is in trouble, but that its savings are not enough to pay future benefits without collecting future tax revenues).
Contrast with insurance.
Besides the argument over whether the returns on Social Security contributions should or can be compared to returns on private investment instruments, there is the question of whether the contributions are nonetheless analogous to pooled insurance premiums charged by for-profit commercial insurance companies to maintain and generate a return on a "risk pool of funds". Like any insurance program, Social Security "spreads risk" as the program protects workers and covered family members against loss of income from the wage earner's retirement, disability, or death. For example, a worker who becomes disabled at a young age could receive a large return relative to the amount they contributed in FICA before becoming disabled, since disability benefits can continue for life. As in private insurance plans, everyone in the particular insurance pool is insured against the same risks, but not everyone will benefit to the same extent.
The analogy to insurance, however, is limited by the fact that paying FICA taxes creates no legal right to benefits and by the extent to which Social Security is, in fact, funded by FICA taxes. During 2011 and 2012, for example, FICA tax revenue was insufficient to maintain Social Security's solvency without transfers from general revenues. These transfers added to the general budget deficit like general program spending.
Private retirement savings crisis.
While inflation-adjusted stock market values generally rose from 1978 to 1997, from 1998 through 2007 they were higher than in March 2013. This has caused workers' supplemental retirement plans such as 401(k)s to perform substantially more poorly than expected when current retirees were investing the bulk of their savings in them. In 2010, the median household retirement account balance for workers aged 55 to 64 was $120,000, which will provide only a trivial supplement to Social Security benefits, but about a third of households had no retirement savings at all. 75% of Americans nearing retirement age had less than $30,000 in their retirement accounts, which "Forbes" called "the greatest retirement crisis in American history."
Court interpretation of the Act to provide benefits.
The United States Court of Appeals for the Seventh Circuit has indicated that the Social Security Act has a moral purpose and should be liberally interpreted in favor of claimants when deciding what counted as covered wages for purposes of meeting the quarters of coverage requirement to make a worker eligible for benefits. That court has also stated: ". . . [T]he regulations should be liberally applied in favor of beneficiaries" when deciding a case in favor of a felon who had his disability payments retroactively terminated upon incarceration. According to the court, that the Social Security Act "should be liberally construed in favor of those seeking its benefits can not be doubted." “The hope behind this statute is to save men and women from the rigors of the poor house as well as from the haunting fear that such a lot awaits them when journey's end is near.”
Constitutionality.
The constitutionality of Social Security is intricately linked to the evolving nature of Supreme Court jurisprudence on federal power (the 20th century saw a dramatic increase in allowed congressional action). When Social Security was first passed, there were significant questions over its constitutionality as the Court had found another pension scheme, the original Railroad Retirement Act, to violate the due process clause of the Fifth Amendment. Some, such as University of Chicago law professor Richard Epstein and Harvard University professor Robert Nozick, have argued that Social Security should be unconstitutional.
In the 1937 U.S. Supreme Court case of "Helvering v. Davis", the Court examined the constitutionality of Social Security when George Davis of the Edison Electric Illuminating Company of Boston sued in connection with the Social Security tax. The U.S. District Court for the District of Massachusetts first upheld the tax. The District Court judgment was reversed by the Circuit Court of Appeals. Commissioner Guy Helvering of the Bureau of Internal Revenue (now the Internal Revenue Service) took the case to the Supreme Court, and the Court upheld the validity of the tax.
During the 1930s President Franklin Delano Roosevelt was in the midst of promoting the passage of a large number of social welfare programs under the New Deal and the High Court struck down many of those programs (such as the Railroad Retirement Act and the National Recovery Act) as unconstitutional. Modified versions of the affected programs were afterwards approved by the Court, including Social Security.
When "Helvering v. Davis" was argued before the Court, the larger issue of constitutionality of the old-age insurance portion of Social Security was not decided. The case was limited to whether the payroll tax was a suitable use of Congress's taxing power. Despite this, no serious challenges regarding the system's constitutionality are now being litigated, and Congress's spending power may be more coextensive, as shown in cases like "South Dakota v. Dole" during the Reagan Administration.
Fraud and abuse.
Social security number theft.
Because Social Security Numbers have become useful in identity theft and other forms of crime, various schemes have been perpetrated to acquire valid Social Security Numbers and related identity information.
In February 2006, the Social Security Administration received several reports of an email message being circulated addressed to “Dear Social Security Number And Card owner” and purporting to be from the Social Security Administration. The message informs the reader “that someone illegally is using your Social Security number and assuming your identity” and directs the reader to a website designed to look like Social Security’s Internet website.
“I am outraged that someone would target an unsuspecting public in this manner,” said Commissioner Jo Anne B. Barnhart. “I have asked the Inspector General to use all the resources at his command to find and prosecute whoever is perpetrating this fraud.”
Once directed to the phony website, the individual is reportedly asked to confirm his or her identity with “Social Security and bank information.” Specific information about the individual’s credit card number, expiration date and PIN is then requested. “Whether on our online website or by phone, Social Security will never ask you for your credit card information or your PIN,” Commissioner Jo Anne B. Barnhart reported.
Social Security Administration Inspector General O’Carroll recommended people always take precautions when giving out personal information. “You should never provide your Social Security number or other personal information over the Internet or by telephone unless you are extremely confident of the source to whom you are providing the information,” O’Carroll said. See .
Fraud in the acquisition and use of benefits.
Given the vast size of the program, fraud occurs. The Social Security Administration has its own investigatory group, Continuing Disability Investigations (CDI). In addition, the Social Security Administration may request investigatory assistance from other federal law enforcement agencies including the Office of the Inspector General and the FBI.
Restrictions on potentially deceptive communications.
Because of the importance of Social Security to millions of Americans, many direct-mail marketers packaged their mailings to resemble official communications from the Social Security Administration, hoping that recipients would be more likely to open them. In response, Congress amended the Social Security Act in 1988 to prohibit the private use of the phrase "Social Security" and several related terms in any way that would convey a false impression of approval from the Social Security Administration. The constitutionality of this law (#redirect ) was upheld in "United Seniors Association, Inc. v. Social Security Administration", 423 F.3d 397 (4th Cir. 2005), cert den 547 U.S. 1162; 126 S.Ct. 2346 (2006) (text at Findlaw).
Public economics.
Current recipients.
The 2011 annual report by the program's Board of Trustees noted the following: in 2010, 54 million people were receiving Social Security benefits, while 157 million people were paying into the fund; of those receiving benefits, 44 million were receiving retirement benefits and 10 million disability benefits. In 2011, there will be 56 million beneficiaries and 158 million workers paying in. In 2010, total income was $781.1 billion and expenditures were $712.5 billion, which meant a total net increase in assets of $68.6 billion. Assets in 2010 were $2.6 trillion, an amount that is expected to be adequate to cover the next 10 years. In 2023, total income and interest earned on assets are projected to no longer cover expenditures for Social Security, as demographic shifts burden the system. By 2035, the ratio of potential retirees to working age persons will be 37 percent — there will be less than three potential income earners for every retiree in the population. At this rate the Social Security Trust Fund would be exhausted by 2036.
Saving behavior.
Social Security affects the saving behavior of the people in three different ways. The wealth substitution effect occurs when a person saving for retirement recognizes that the Social Security system will take care of him and decreases his expectations about how much he needs to personally save. The retirement effect occurs when a taxpayer saves more each year in an effort to reduce the total number of years he must work to accumulate enough savings before retirement. The bequest effect occurs when a taxpayer recognizes a decrease in resources stemming from the Social Security tax and compensates by increasing personal savings to cover future expected costs of having children.
Reducing cost of living adjustment (COLA).
At present, a retiree's benefit is annually adjusted for inflation to reflect changes in the consumer price index. Some economists argue that the consumer price index overestimates price increases in the economy and therefore is not a suitable metric for adjusting benefits, while others argue that the CPI underestimates the effect of inflation on what retired people actually need to buy to live.
The current cost of living adjustment is based on the consumer price index for Urban Wage Earners and Clerical Workers (CPI-W). The Bureau of Labor Statistics routinely checks the prices of 211 different categories of consumption items in 38 geographical areas to compute 8,018 item-area indices. Many other indices are computed as weighted averages of these base indices. CPI-W is based on a market basket of goods and services consumed by urban wage earners and clerical workers. The weights for that index are updated in January of every even-numbered year. People who say that the CPI-W overestimates inflation recommend updating the weights each month; this produces the Chained Consumer Price Index for all urban consumers (C-CPI-U). People who say that the C-CPI-U [or the unchained CPI for All Urban Consumers (CPI-U)] disadvantages the elderly point out that seniors consume more medical care than younger people, and that the costs of medical care have been rising faster than inflation in other parts of the economy. According to this view, the costs of the things the elderly buy have been rising faster than the market basket averaged to obtain CPI-W, CPI-U or C-CPI-U. Some have recommended fixing this by using a CPI for the Elderly (CPI-E).
In 2003 economics researchers Hobijn and Lagakos estimated that the social security trust fund would run out of money in 40 years using CPI-W and in 35 years using CPI-E.
References.
Works referenced
</dl>
Further reading.
</dl>

</doc>
<doc id="48732" url="http://en.wikipedia.org/wiki?curid=48732" title="Pyramid scheme">
Pyramid scheme

A pyramid scheme is an unsustainable business model that involves promising participants payment or services, primarily for enrolling other people into the scheme, rather than supplying any real investment or sale of products or services to the public.
Various forms of pyramid schemes are illegal in many countries including Albania, Australia, Austria, Belgium, Brazil, Canada, China, Colombia, Denmark, the Dominican Republic, Estonia, France, Germany, Hong Kong, Hungary, Iceland, Iran, Italy, Japan, Malaysia, Mexico, Nepal, the Netherlands, New Zealand, Norway, the Philippines, Poland, Portugal, Romania, Russian Federation, South Africa, Spain, Sri Lanka, Sweden, Switzerland, Taiwan, Thailand, Turkey, Ukraine, the United Kingdom, and the United States.
These types of schemes have existed for at least a century, some with variations to hide their true nature. Some multilevel marketing plans have also been classified as pyramid schemes.
Concept and basic models.
In a pyramid scheme, an organization compels individuals to make a payment and join. In exchange, the organization promises its new members a share of the money taken from every additional member that they recruit. The directors of the organization (those at the top of the pyramid) also receive a share of these payments. For the directors, the scheme is potentially lucrative—whether or not they do any work, the organization's membership has a strong incentive to continue recruiting and funneling money to the top of the pyramid.
Such organizations seldom involve sales of products or services with real value. Without creating any goods or services, the only ways for a pyramid scheme to generate revenue are to recruit more members or solicit more money from current members. Eventually, recruiting is no longer possible and the plurality of members are unable to profit from the scheme.
The "Eight-Ball" model.
Many pyramids are more sophisticated than the simple model. These recognize that recruiting a large number of others into a scheme can be difficult so a seemingly simpler model is used. In this model each person must recruit two others, but the ease of achieving this is offset because the depth required to recoup any money also increases. The scheme requires a person to recruit two others, who must each recruit two others, who must each recruit two others.
Prior instances of this scheme have been called the "Airplane Game" and the four tiers labelled as "captain", "co-pilot", "crew", and "passenger" to denote a person's level. Another instance was called the "Original Dinner Party" which labeled the tiers as "dessert", "main course", "side salad", and "appetizer". A person on the "dessert" course is the one at the top of the tree. Another variant, "Treasure Traders", variously used gemology terms such as "polishers", "stone cutters", etc. or gems like "rubies", "sapphires", "diamonds", etc.
Such schemes may try to downplay their pyramid nature by referring to themselves as "gifting circles" with money being "gifted". Popular schemes such as "Women Empowering Women" do exactly this.
Whichever euphemism is used, there are 15 total people in four tiers (1 + 2 + 4 + 8) in the scheme—with the Airplane Game as the example, the person at the top of this tree is the "captain", the two below are "co-pilots", the four below are "crew," and the bottom eight joiners are the "passengers".
The eight passengers must each pay (or "gift") a sum (e.g., $5,000) to join the scheme. This sum (e.g., $40,000) goes to the captain who leaves, with everyone remaining moving up one tier. There are now two new captains so the group splits in two with each group requiring eight new passengers. A person who joins the scheme as a passenger will not see a return until they advance through the crew and co-pilot tiers and exit the scheme as a captain. Therefore, the participants in the bottom three tiers of the pyramid lose their money if the scheme collapses.
If a person is using this model as a scam, the confidence trickster would take the majority of the money. They would do this by filling in the first three tiers (with one, two, and four people) with phony names, ensuring they get the first seven payouts, at eight times the buy-in sum, without paying a single penny themselves. So if the buy-in were $5,000, they would receive $40,000, paid for by the first eight investors. They would continue to buy in underneath the real investors, and promote and prolong the scheme for as long as possible to allow them to skim even more from it before it collapses.
Although the "captain" is the person at the top of the tree, having received the payment from the eight paying passengers, once they leave the scheme they are able to re-enter the pyramid as a "passenger" and hopefully recruit enough to reach captain again, thereby earning a second payout.
Matrix schemes.
Matrix schemes use the same fraudulent non-sustainable system as a pyramid; here, the participants pay to join a waiting list for a desirable product which only a fraction of them can ever receive. Since matrix schemes follow the same laws of geometric progression as pyramids, they are subsequently as doomed to collapse. Such schemes operate as a queue, where the person at head of the queue receives an item such as a television, games console, digital camcorder, etc. when a certain number of new people join the end of the queue. For example, ten joiners may be required for the person at the front to receive their item and leave the queue. Each joiner is required to buy an expensive but potentially worthless item, such as an e-book, for their position in the queue. The scheme organizer profits because the income from joiners far exceeds the cost of sending out the item to the person at the front. Organizers can further profit by starting a scheme with a queue with "shill" names that must be cleared out before genuine people get to the front. The scheme collapses when no more people are willing to join the queue. Schemes may not reveal, or may attempt to exaggerate, a prospective joiner's queue position which essentially means the scheme is a lottery. Some countries have ruled that matrix schemes are illegal on that basis.
Connection to multi-level marketing.
The network marketing or multi-level marketing (MLM) business has become associated with pyramid schemes. According to the U.S. Federal Trade Commission, many MLM schemes "simply use the product to hide their pyramid structure". While some people call MLMs in general "pyramid selling," others use the term to denote an illegal pyramid scheme masquerading as an MLM.
The Federal Trade Commission warns, "It’s best not to get involved in plans where the money you make is based primarily on the number of distributors you recruit and your sales to them, rather than on your sales to people outside the plan who intend to use the products." It states that research is your best tool and gives eight steps to follow:
Some authorities contend that MLMs in general are nothing more than legalized pyramid schemes.
Connection to franchise fraud.
Franchise fraud (or "franchise churning") is defined by the U.S. Federal Bureau of Investigation as a pyramid scheme. The FBI website states:
Pyramid schemes—also referred to as franchise fraud or chain referral schemes—are marketing and investment frauds in which an individual is offered a distributorship or franchise to market a particular product. The real profit is earned, not by the sale of the product, but by the sale of new distributorships. Emphasis on selling franchises rather than the product eventually leads to a point where the supply of potential investors is exhausted and the pyramid collapses.
Notable recent cases.
Internet.
In 2003, the United States Federal Trade Commission (FTC) disclosed what it called an Internet-based "pyramid scam." Its complaint states that customers would pay a registration fee to join a program that called itself an "internet mall" and purchase a package of goods and services such as internet mail, and that the company offered "significant commissions" to consumers who purchased and resold the package. The FTC alleged that the company's program was instead and in reality a pyramid scheme that did not disclose that most consumers' money would be kept, and that it gave affiliates material that allowed them to scam others.
WinCapita was a scheme run by Finnish criminals that involved about €100 million.
Others.
The 1997 rebellion in Albania was partially motivated by the collapse of Ponzi schemes; however, they were widely referred to as pyramid schemes due to their prevalence in Albanian society.
In early 2006, Ireland was hit by a wave of schemes with major activity in Cork and Galway. Participants were asked to contribute €20,000 each to a "Liberty" scheme which followed the classic eight-ball model. Payments were made in Munich, Germany to skirt Irish tax laws concerning gifts. Spin-off schemes called "Speedball" and "People in Profit" prompted a number of violent incidents and calls were made by politicians to tighten existing legislation. Ireland has launched a website to better educate consumers to pyramid schemes and other scams.
On 12 November 2008, riots broke out in the municipalities of Pasto, Tumaco, Popayan and Santander de Quilichao, Colombia after the collapse of several pyramid schemes. Thousands of victims had invested their money in pyramids that promised them extraordinary interest rates. The lack of regulation laws allowed those pyramids to grow excessively during several years. Finally, after the riots, the Colombian government was forced to declare the country in a state of economic emergency to seize and stop those schemes. Several of the pyramid's managers were arrested, and are being prosecuted for the crime of "illegal massive money reception."
The "Kyiv Post" reported on 26 November 2008 that American citizen Robert Fletcher (Robert T. Fletcher III; aka "Rob") was arrested by the SBU (Ukraine State Police) after being accused by Ukrainian investors of running a Ponzi scheme and associated pyramid scam netting US$20 million. (The "Kiev Post" also reports that some estimates are as high as US$150M.)
Throughout 2010 and 2011 a number of authorities around the world including the Australian Competition and Consumer Commission, the Bank of Namibia and the Central Bank of Lesotho have declared TVI Express to be a pyramid scheme. TVI Express, operated by Tarun Trikha from India has apparently recruited hundreds of thousands of "investors", very few of whom, it is reported, have recouped any of their investment. In 2013, Tarun Trikha was arrested at the IGI Airport in New Delhi.

</doc>
<doc id="48733" url="http://en.wikipedia.org/wiki?curid=48733" title="Ragnar Lodbrok">
Ragnar Lodbrok

Ragnar Lodbrok or Lothbrok (Old Norse: "Ragnarr Loðbrók", "Ragnar Hairy Breeches") was a legendary Norse ruler, king, and hero from the Viking Age described in Old Norse poetry and several sagas. In this tradition, Ragnar was the scourge of France and England and the father of many renowned sons, including Ivar the Boneless, Björn Ironside, Halfdan Ragnarsson, Sigurd Snake-in-the-Eye, and Ubba. While these men are historical figures, it is uncertain whether Ragnar himself existed or really fathered them. Many of the tales about him appear to originate with the deeds of several historical Viking heroes and rulers.
According to legend, Ragnar was thrice married: to the shieldmaiden Lagertha, to the noblewoman Þóra Borgarhjǫrtr, and to Aslaug. Said to have been a relative of the Danish king Gudfred and son of the Swedish king Sigurd Hring, he became king himself and distinguished himself by many raids and conquests until he was eventually seized by his foe, King Ælla of Northumbria, and killed by being thrown into a pit of snakes. His sons bloodily avenged him by invading England with the Great Heathen Army.
Historicity.
As a figure of legend whose life only partially took place in times and places covered by written sources, the extent of Ragnar's historicity is not quite clear.
In her commentary on Saxo's "Gesta Danorum", Hilda Ellis Davidson notes that Saxo's coverage of Ragnar's legend in book IX of the "Gesta" appears to be an attempt to consolidate many of the confusing and contradictory events and stories known to the chronicler into the reign of one king, Ragnar. That is why many acts ascribed to Ragnar in the "Gesta" can be associated, through other sources, with various figures, some of which are more historically certain. These candidates for the "historical Ragnar" include:
So far, attempts to firmly link the legendary Ragnar with one or several of those men have failed because of the difficulty in reconciling the various accounts and their chronology. Nonetheless, the core tradition of a Viking hero named Ragnar (or similar) who wreaked havoc in mid-ninth-century Europe and who fathered many famous sons is remarkably persistent, and some aspects of it are covered by relatively reliable sources, such as the "Anglo-Saxon Chronicle". According to Davidson, writing in 1979, "certain scholars in recent years have come to accept at least part of Ragnar's story as based on historical fact". Katherine Holman, on the other hand, concludes that "although his sons are historical figures, there is no evidence that Ragnar himself ever lived, and he seems to be an amalgam of several different historical figures and pure literary invention."
Sources.
The medieval sources that cover Ragnar include:

</doc>
<doc id="48738" url="http://en.wikipedia.org/wiki?curid=48738" title="Revolution of 1905">
Revolution of 1905

The Revolution of 1905 was a wave of mass political and social unrest that spread through vast areas of the Russian Empire. Some of it was directed against the government, while some was undirected. It included worker strikes, peasant unrest, and military mutinies. It led to Constitutional Reform including the establishment of the State Duma of the Russian Empire, the multi-party system, and the Russian Constitution of 1906.
Causes.
According to the author Sidney Harcave, who wrote "The Russian Revolution of 1905", there were four problems in Russian society at the time that had led to the revolution. These are the agrarian problem, the nationality problem, the labor problem, and the educated class as a problem. While individually these may have not made a difference, the combination of these problems created the conditions for a potential revolution. "At the turn of the century, discontent with the Tsar’s dictatorship was manifested not only through the growth of political parties dedicated to the overthrow of the monarchy but also through industrial strikes for better wages and working conditions, protests and riots among peasants, university demonstrations, and the assassination of government officials, often done by Socialist Revolutionaries."
The government finally recognized these problems, albeit in a shortsighted and narrow-minded way. The minister of interior Plehve stated in 1903 that, after the agrarian problem, the most serious ones plaguing the country were those of the Jews, the schools, and the workers—in that order. The Russian economy was tied to European finances so when the western money markets contracted in 1899-1900, Russian industry plunged into a crisis deeper and more prolonged than that which concurrently struck western European industry. This setback aggravated discontent throughout society in the five years preceding the revolution of 1905.
Agrarian problem.
Every year thousands of nobles who found themselves in debt either mortgaged their estates to the noble land bank or sold their land to municipalities, merchants, or peasants. The nobility had sold off one-third of its land holding and mortgaged the third that remained. The peasants had become emancipated from serfdom. The government had hoped to make them a politically conservative land holding class. The government issued laws providing the peasant would purchase certain land owned by nobility and would pay for it through redemption dues over decades.
The land, known as “allotment land”, would not be owned by individual peasants, but would be owned by the community of peasants; individual peasants would have rights to strips of land that were assigned to them under the open field system. Unfortunately a peasant was unable to sell or mortgage his piece of land so in practice he could not renounce his rights to his land and thus he would be required to pay his share of redemption dues to the village commune. The government had created this plan to ensure the proletarization of the peasants would never happen, but the peasants were not given enough land to provide for their needs. "Their earnings were often so small that they could neither buy the food they needed nor keep up the payment of taxes and redemption dues they owed the government for their land allotments. By the tenth year of Nicholas II's reign, their total arrears in payments of taxes and dues was 118 million rubles." As time went on, the situation grew worse. Masses of hungry peasants roamed the countryside looking for work and would sometimes walk hundreds of miles to find it. Desperate peasants proved capable of violence. "In the provinces of Kharkov and Poltava in 1902, thousands of them, ignoring restraints and authority, burst out in a rebellious fury that led to extensive destruction of property and looting of noble homes before troops could be brought to subdue and punish them." These violent outbreaks caught the attention of the government, so they created numerous committees to investigate the causes of these violent outbursts from the peasants.
The results of their investigation found that there was no part of the countryside that was prosperous; some parts, especially the fertile areas known as "black-soil region", were in a state of decline. Although cultivated acreage had increased in the last half century, the increase had not been proportionate to the growth of the peasant populations, which had doubled during that time. "There was general agreement at the turn of the century that Russia faced a grave and intensifying agrarian crisis due mainly to rural overpopulation with an annual excess of fifteen to eighteen live births over deaths per 1,000 inhabitants." The investigations revealed many difficulties; however, they could not find remedies that were both sensible and "acceptable" to the government.
Nationality problem.
For generations, Russian Jews had been considered a special problem. "The official view had come to be that they were enemies of Christianity, exploiters of the peasantry, and the fountain head of the revolutionary movement."<ref name="Harcave 1970, 21"/ Jews constituted only about 6 percent of the population, but were concentrated in the western borderlands. Like other minorities in Russia, the Jews lived in "miserable and circumscribed lives, forbidden to settle or acquire land outside the cities and towns, legally limited in attendance at secondary school and higher schools, virtually barred from legal professions, denied the right to vote for municipal councilors, and excluded from services in the Navy or the Guards." 
The government's treatment of Jews, although considered its own issue, was similar to the government's policies in dealing with all national and religious minorities. "Russian administrators, who never succeeded in coming up with a legal definition of "Pole", despite the decades of restrictions on that ethnic group, regularly spoke of individuals 'of Polish descent' or, alternatively, 'of Russian descent,' making identity a function of birth." This policy only succeeded in producing or aggravating feelings of disloyalty. There was growing impatience with their inferior status and resentment against "Russification". Russification is cultural assimilation "according to Benjamin Nathans, is definable as 'a process culminating in the disappearance of a given group as a recognizable distinct element within a larger society.' " Russia was a multiethnic empire. Nineteenth century Russians saw cultures and religions in a clear hierarchy. Non-Russian cultures were tolerated in the empire but were not necessarily respected. "European civilization was valued over Asian or African culture, and Christianity was on the whole considered more progressive and 'true' than other religions."
Besides the imposition of a uniform Russian culture throughout the empire, the government's pursuit of Russification, especially during the second half of the nineteenth century, had many other motives. After the emancipation of the serfs in 1861, the Russian state was compelled to take into account the public, but the government failed to gain the public's support. Another reason was the Polish uprising of 1863. Unlike other minority nationalities, the Poles, in the eyes of the Tsar, were a direct threat to the empire's stability. After the rebellion was crushed, the government implemented policies to reduce Polish cultural influences. In the 1870s the government began to distrust German elements on the western border. The Russian government felt that the unification of Germany would upset the power balance among the great powers of Europe and that Germany would use its strength against Russia. The government thought that the borders would be defended better if the borderland were more Russian in character.
Labor problem.
The economic situation in Russia seemed doomed. They had experimented with "laissez faire" capitalist policies, but they hadn't worked out until the 1890s. "Meanwhile agricultural productivity stagnated, while international prices for grain dropped, and Russia’s foreign debt and need for imports grew. War and military preparations continued to consume government revenues. At the same time, the peasant taxpayers' ability to pay was strained to the utmost, leading to widespread famine in 1891."
In the 1890s, under the minister of finance Sergei Witte, a crash governmental program was proposed to promote industrialization. His policies included heavy government expenditures for railroad building and operations, subsidies and supporting services for private industrialists, high protective tariffs for Russian industries especially heavy industry, increased exports, stable currency, and encouragement of foreign investments. His plan was successful and during the "1890s Russian industrial growth averaged 8 percent per year. Railroad mileage grew from a very substantial base by 40 percent between 1892 and 1902." His success in implementing this program helped spur the 1905 revolution and eventually the 1917 revolution because it created new classes that exacerbated social tensions. "Besides dangerously concentrating a proletariat, a professional and a rebellious student body in centers of political power, industrialization infuriated both these new forces and the traditional rural classes." The government policy of financing industrialization through taxing peasants forced millions of peasants to work in towns. The "peasant worker" saw his labor in the factory as the means to consolidate his family's economic position in the village and played a role in determining the social consciousness of the urban proletariat, but also in spreading urban ideas to the countryside. This improvement in communications helped break down the isolation of the peasants in their communes.
Industrial workers began to feel dissatisfaction with the Tsarist government despite the protective laws that the government had decreed. Some of those laws included the prohibition of children under 12 from working with the exception of night work in glass factories, limited employment of those who were between the ages of 12 and 15 and wouldn’t allow them to work on Sundays and holidays, prohibited charging workers for the cost of lighting of the shops and plants, required workers be paid in cash at least once a month, and limited the size and bases of fines for workers who were tardy. Despite all of this, the workers believed that the laws hadn't done enough to free them from unfair and inhumane practices. Some of those were being forced to work beyond the maximum eleven and a half hours; they were still subject to arbitrary and excessive fines for tardiness, mistakes in their work, or absence. In addition to these problems, they were the lowest wage-workers in Europe. Although the cost of living in Russia was low, "the average worker's 16 rubles per month could not buy the equal of what the French worker's 110 francs would buy for him." Furthermore, the government's "protective" labor laws prohibited organization of trade unions and strikes. The situation was turning the workers' dissatisfaction into desperation, which made them more sympathetic to radical ideas. This change was revealed when some workers defied authority by participating in illegal strikes and by joining revolutionary groups.
The government dealt with this problem in the only way that they knew how: by arresting labor agitators and by enacting more of paternalistic legislation. A new method that the government used to combat these unions and strikes that was introduced in 1900 by Sergei Zubatov, head of the Moscow security department, was called "police socialism." The plan was to form workers' societies with police approval to "provide healthful, fraternal activities and opportunities for cooperative self-help together with "protection" against influences that might have inimical effect on loyalty to job or country." Some of these groups organized in Moscow, Odessa, Kiev, Nikolayev (Ukraine), and Kharkov, but these groups and the idea of police socialism failed.
In 1900–1903, there was a period of industrial depression. Many firms went bankrupt and employment was cut. Employees were restive: they would join legal organizations but turn the organizations toward an end that the organizations' sponsors didn't intend. Workers used them to organize strikes or to draw support for striking workers outside these groups. A strike that was begun in 1902 by workers in the railroad shops in Vladikavkaz and Rostov-on-Don created such a huge response that by the next summer, 225,000 in various industries in southern Russia and Transcaucasia were on strike. These weren't the first illegal strikes in the county's history; however, the strikers' aims, political awareness, and support among non-workers and workers made them more troubling to the government than other strikes before. The government responded by closing all legal organizations by the end of 1903.
Educated class as a problem.
The minister of the interior, Plehve, designated the schools as a pressing problem for the government, but he failed to realize it was only a symptom of antigovernment feelings among the educated class. Students of universities, other schools of higher learning, and occasionally those of the secondary schools and theological seminaries were part of this group. They were taking up problems that were unrelated to their "proper employment", and were taking part in open disorderly displays of defiance and radicalism. To express their feelings, students boycotted examinations, rioted, arranged marches in sympathy with the strikers or political prisoners, circulated petitions, or wrote antigovernment propaganda.
This was originally perceived by the government as lack of proper training in patriotism and religion. The government was disturbed by the widespread behavior but felt it could be fixed. Some believed the curriculum should be toughened up, but there was little improvement after the implementation of measures to emphasize classical language and math in secondary schools. Expulsion, exile, or forced military service were also tried by the government, but these measures were unsuccessful in stopping students. "In fact, when the official decision to overhaul the whole educational system was finally made, in 1904, and to that end Vladimir Glazov, head of General Staff Academy, was selected as Minister of Education, the students had grown bolder and more resistant than ever."
Student radicalism began around the time Tsar Alexander II came to power. While also abolishing serfdom, he enacted fundamental reforms in the legal, administrative, and structure of the Russian empire, which were revolutionary for the time. The Tsar lifted many restrictions placed on universities and abolished obligatory uniforms and military discipline. This ushered in a new freedom in the content and reading lists of academic courses. In turn that created student subcultures, as youth were willing to live in poverty in order to receive an education. As universities expanded, there was a rapid growth of newspapers, journals, and an organization of public lectures and professional societies. The 1860s was a time when the emergence of a new public sphere was created in social life and professional groups. This created the idea of their right to have an independent opinion.
The government looked at these communities with alarm, and in 1861 it created stricter restrictions on admission and prohibited student organizations that resulted in the first ever student demonstration held in St. Petersburg, which led to a two-year closure of the university. The consequent conflict with the state is an important factor in the chronic student protests over subsequent decades. The political engagement carried out by students outside of the universities became a tenet of student radicalism by the 1870s which originated in the atmosphere of the early 1860s. Student radicals described "the special duty and mission of the student as such to spread the new word of liberty. Students were called upon to extend their freedoms into society, to repay the privilege of learning by serving the people, and to become in Nikolai Ogarev's phrase 'apostles of knowledge.' " During the next two decades universities produced a significant share of Russia's revolutionaries. Prosecution records from the 1860s and 1870s show that more than one-half of all political offenses were committed by students despite their minute number in the population as a whole. "The tactics of the left-wing students proved to be remarkably effective, far beyond anyone's dreams. Sensing that neither the university administrations nor the government any longer possessed the will or authority to enforce regulations, radicals simply went ahead with their plans to turn the schools into centers of political activity for students and non students alike."
The combination of all four problems created the conditions for the uprising. Most of the country's population were peasants, so when they were emancipated from serfdom, the government hoped to turn them into a conservative land holding class. This failed mainly because peasants were forced to keep their land and weren't allowed to sell or mortgage it. Their earnings were too small for the peasants to earn a living. Desperate, they began revolting against the government. The nationality problem was important because the "Russification" of its minorities created resentment. Not only were they treated differently in social life; they were banned by the government from voting or from serving in the Guard or Navy, and were allowed limited attendance in schools. Instead of creating loyalty with these groups, the government created hostility. The labor problem began with the industrialization of Russia. Workers felt that, although the government had created reforms that were meant to protect them, the government wasn't doing enough for them. They were hostile towards the government because the government banned strikes and the organization of labor unions. The government's harsh reaction to their strikes made more people receptive to radical ideas. Finally, the educated class as a problem was important because the student movement constituted so large a share of the revolutionary movement. After Tsar Nicholas II relaxed the discipline in Russia's universities, the universities became lax; this gave rise to a new consciousness among students, who then wanted to bring freedom into society. All of these problems contributed to the popular uprising in Russia in 1905.
Rise of the opposition.
The events of 1905 were preceded by a Progressive and academic agitation for more political democracy and limits to Tsarist rule in Russia; plus an increase in strikes by workers against employers for radical economic demands and union recognition, especially in southern Russia. Many socialists view this as a period when the rising revolutionary movement was met with rising reactionary movements. As Rosa Luxemburg stated in "The Mass Strike", when collective strike activity was met with what is perceived as repression from an autocratic state, economic and political demands grew into and reinforced each other.
At the start of the 20th century, Russian progressives formed the Union of Zemstvo Constitutionalists (1903) and the Union of Liberation (1904) which called for a constitutional monarchy. Russian socialists formed two major groups: the Socialist-Revolutionary Party, following the Russian populist tradition, and the Marxist Russian Social Democratic Labour Party.
In the autumn of 1904, liberals started a series of banquets celebrating the 40th anniversary of the liberal court statutes and calling for political reforms and establishment of a constitution. On 13 December [O.S. 30 November] 1904, the Moscow City Duma passed a resolution, demanding establishment of an elected national legislature, full freedom of the press, and freedom of religion. Similar resolutions and appeals from other city dumas and zemstvo councils followed.
Tsar Nicholas II made a move to fulfill many of these demands, appointing liberal Pyotr Dmitrievich Sviatopolk-Mirskii Minister of the Interior after the assassination of Vyacheslav von Plehve. On 25 December [O.S. 12 December] 1904, the Tsar issued a manifesto promising the broadening of the Zemstvo and local municipal councils' authority, insurance for industrial workers, the emancipation of Inorodtsy, and the abolition of censorship. However, the crucial point of representative national legislature was missing in the manifesto.
At the start of the 20th century the Russian industrial worker worked on average an 11-hour day (10 hours on Saturday), factory conditions were perceived as grueling and often unsafe, and attempts at independent unions were often not accepted. 
In 1902, strikes in the Caucasus broke out in March, and strikes on the Railway originating from pay disputes took on other issues, and drew in other industries, culminating in a general strike at Rostov-on-Don in November. Daily meetings of 15,000 to 20,000 heard openly revolutionary appeals for the first time, before a massacre defeated the strikes. But reaction to the massacres brought political demands to purely economic ones. In 1903 "the whole of South Russia in May, June and July was aflame", including Baku where separate wage struggles culminated in a city-wide general strike, and Tiflis, where commercial workers gained a reduction in the working day, and were joined by factory workers. In 1904, massive strike waves broke out in Odessa in the spring, Kiev in July, and Baku in December. This all set the stage for the strikes in St. Petersburg in December 1904 to January 1905 seen as the first step in the 1905 revolution. 
Start of the revolution.
In December 1904, a strike occurred at the Putilov plant (a railway and artillery supplier) in St. Petersburg. Sympathy strikes in other parts of the city raised the number of strikers up to 150,000 workers in 382 factories. By 21 January [O.S. 8 January] 1905, the city had no electricity and no newspapers whatsoever. All public areas were declared closed.
Controversial Orthodox priest Georgy Gapon, who headed a police-sponsored workers' association, led a huge workers' procession to the Winter Palace to deliver a petition to the Tsar on Sunday, 22 January [O.S. 9 January] 1905. The troops guarding the Winter Palace who had been ordered to tell the demonstrators not to pass a certain point, according to Sergei Witte, opened fire on them, which resulted in more than 200 (according to Witte) to 1000 deaths. The event became known as Bloody Sunday, and is usually considered the start of the active phase of the revolution.
The events in St. Petersburg provoked public indignation and a series of massive strikes that spread quickly throughout the industrial centres of the Russian Empire. Polish socialists — both the PPS and the SDKPiL — called for a general strike. By the end of January 1905, over 400,000 workers in Russian Poland were on strike (see Revolution in the Kingdom of Poland (1905–1907)). Half of European Russia's industrial workers went on strike in 1905, 93.2% in Poland. There were also strikes in Finland and the Baltic coast. In Riga, 80 protesters were killed on 26 January [O.S. 13 January] 1905, and in Warsaw a few days later over 100 strikers were shot on the streets. By February, there were strikes in the Caucasus, and by April, in the Urals and beyond. In March, all higher academic institutions were forcibly closed for the remainder of the year, adding radical students to the striking workers. A strike by railway workers on 21 October [O.S. 8 October] 1905 quickly developed into a general strike in Saint Petersburg and Moscow. This prompted the setting up of the short-lived Saint Petersburg Soviet of Workers' Delegates, an admixture of Bolsheviks and Mensheviks headed by Khrustalev-Nossar and despite the Iskra split would see the likes of Julius Martov and Georgi Plekhanov spar with Lenin. Leon Trotsky, who felt a strong connection to the Bolsheviki, but had not given up a compromise meanwhile spearheaded strike action in over 200 factories. By 26 October [O.S. 13 October] 1905, over 2 million workers were on strike and there were almost no active railways in all of Russia. Growing inter-ethnic confrontation throughout the Caucasus resulted in Armenian-Tatar massacres, heavily damaging the cities and the Baku oilfields.
With the unsuccessful and bloody Russo-Japanese War (1904–1905) there was unrest in army reserve units. On 2 January 1905 Port Arthur was lost, and the Russian Baltic Fleet was defeated at Tsushima; in February 1905, the Russian army was defeated at Mukden, losing almost 80,000 men in the process. Witte was dispatched to make peace, negotiating the Treaty of Portsmouth (signed 5 September [O.S. 23 August] 1905). In 1905, there were naval mutinies at Sevastopol (see Sevastopol Uprising), Vladivostok, and Kronstadt, peaking in June with the mutiny aboard the "battleship Potemkin" — some sources claim over 2,000 sailors died in the restoration of order. The mutinies were disorganised and quickly crushed. Despite these mutinies, the armed forces were largely apolitical and remained mostly loyal, if dissatisfied — and were widely used by the government to control the 1905 unrest.
Nationalist groups had been angered by the Russification undertaken since Alexander II. The Poles, Finns, and the Baltic provinces all sought autonomy, and also freedom to use their national languages and promote their own culture. Muslim groups were also active — the First Congress of the Muslim Union took place in August 1905. Certain groups took the opportunity to settle differences with each other rather than the government. Some nationalists undertook anti-Jewish pogroms, possibly with government aid, and in total over 3,000 Jews were killed.
The number of prisoners throughout the Russian Empire, which had peaked at 116,376 in 1893, fell by over a third to a record low of 75,009 in January 1905, chiefly because of several mass amnesties granted by the Tsar; the historian S G Wheatcroft has wondered what role these released criminals played in the 1905–6 social unrest.
Government response.
On 12 January the Tsar appointed Dmitri Feodorovich Trepov as governor in St Petersburg and dismissed the Minister of the Interior, Pyotr Sviatopolk-Mirskii, on 18 February [O.S. 5 February] 1905. He appointed a government commission "to enquire without delay into the causes of discontent among the workers in the city of St Petersburg and its suburbs" in view of the strike movement. The commission was headed by Senator NV Shidlovsky, a member of the State Council, and included officials, chiefs of government factories, and private factory owners. It was also meant to have included workers’ delegates elected according to a two-stage system. Elections of the workers delegates were, however, blocked by the socialists who wanted to divert the workers from the elections to the armed struggle. On 5 March [O.S. 20 February] 1905, the Commission was dissolved without having started work.
Following the assassination of his uncle, the Grand Duke Sergei Aleksandrovich, on 17 February [O.S. 4 February] 1905, the Tsar agreed to give new concessions. On 18 February [O.S. 5 February] 1905 he published the "Bulygin Rescript", which promised the formation of a consultative assembly, religious tolerance, freedom of speech (in the form of language rights for the Polish minority) and a reduction in the peasants' redemption payments.
On 24 and 25 May [O.S. 11 and 12 May] 1905, about 300 Zemstvo and municipal representatives held three meetings in Moscow, which passed a resolution, asking for popular representation at the national level. On 6 June [O.S. 24 May] 1905, Nicholas II had received a Zemstvo deputation. Responding to speeches by Prince Sergei Trubetskoi and Mr Fyodrov, the Tsar confirmed his promise to convene an assembly of people’s representatives.
Height of the revolution.
Tsar Nicholas II agreed on 18 February [O.S. 5 February] to the creation of a State Duma of the Russian Empire but with consultative powers only. When its slight powers and limits on the electorate were revealed, unrest redoubled. The Saint Petersburg Soviet was formed and called for a general strike in October, refusal to pay taxes, and the withdrawal of bank deposits.
In June and July 1905, there were many peasant uprisings in which peasants seized land and tools. Disturbances in the Russian-controlled Congress Poland culminated in June 1905 in the Łódź insurrection.
Surprisingly, only one landlord was recorded as killed. Far more violence was inflicted on peasants outside the commune: 50 deaths were recorded.
The October Manifesto, written by Sergei Witte and Alexis Obolenskii, was presented to the Tsar on 14 October [O.S. 1 October]. It closely followed the demands of the Zemstvo Congress in September, granting basic civil rights, allowing the formation of political parties, extending the franchise towards universal suffrage, and establishing the Duma as the central legislative body. The Tsar waited and argued for three days, but finally signed the manifesto on 30 October [O.S. 17 October] 1905, owing to his desire to avoid a massacre, and a realisation that there was insufficient military force available to do otherwise. He regretted signing the document, saying that he felt "sick with shame at this betrayal of the dynasty ... the betrayal was complete".
When the manifesto was proclaimed there were spontaneous demonstrations of support in all the major cities. The strikes in Saint Petersburg and elsewhere officially ended or quickly collapsed. A political amnesty was also offered. The concessions came hand-in-hand with renewed, and brutal, action against the unrest. There was also a backlash from the conservative elements of society, with right-wing attacks on strikers, left-wingers, and Jews.
While the Russian liberals were satisfied by the October Manifesto and took preparations for upcoming Dumas elections, radical socialists and revolutionaries denounced the elections and called for an armed uprising to destroy the Empire.
Some of the November uprising of 1905 in Sevastopol, headed by retired naval Lieutenant Pyotr Schmidt, was directed against the government, while some was undirected. It included terrorism, worker strikes, peasant unrest, and military mutinies and was only suppressed after a fierce battle. The Trans-Baikal railroad fell into the hands of striker committees and demobilised soldiers returning from Manchuria after the Russo–Japanese War. The Tsar had to send a special detachment of loyal troops along the Trans-Siberian Railway to restore order.
Between 5 and 7 December [O.S. 22 and 24 November], there was a general strike by Russian workers. The government sent in troops on 7 December, and a bitter street-by-street fight began. A week later the Semyonovsky Regiment was deployed, and used artillery to break-up demonstrations and to shell workers' districts. On 18 December [O.S. 5 December], with around a thousand people dead and parts of the city in ruins, the workers surrendered. After a final spasm in Moscow, the uprisings ended in December 1905.
According to figures presented in the Duma by Professor Maksim Kovalevsky, by April 1906, more than 14,000 people had been executed and 75,000 imprisoned.
The historian Brian Taylor states the number of deaths in the 1905 Revolution was in the "thousands", and notes the existence of one source that puts the figure at over 13,000 deaths.
Results.
Following the Revolution of 1905, the Tsar made last effort attempts to keep his regime from being toppled, and offered reforms similar to most rulers when pressured by a revolutionary movement. The military remained loyal throughout the Revolution of 1905, shown through their shooting of revolutionaries ordered by the Tsar, signifying a would-be difficult overthrow. These reforms were outlined under a precursor to the Constitution of 1906 known as the October Manifesto which created the Imperial Duma. The Russian Constitution of 1906, also known as the Fundamental Laws, set up a multiparty system and a limited constitutional monarchy. The revolutionaries were quelled and satisfied with the reforms, but it wasn’t enough to prevent the 1917 revolution that would later topple the Tsar's regime.
Creation of Duma and Stolypin.
The creation of the Duma and the beginning of the Revolution of 1905 was supported and sprung from Russia’s loss in the Russo-Japanese War, where Sergei Witte gained political notoriety. On 17 October 1905, the October Manifesto was signed by Tsar Nicholas II guaranteeing civil liberties to all citizens and the creation of the First Duma. The First Duma was created to be the lower house, with the upper house being the Council of State which was appointed. The composition of the First Duma was 48.1% peasants and 36.7% nobles. Peasants had more representation than the nobility, and were able to use that to their advantage to show discontent and be problematic towards authority. Both houses were needed to meet and agree on laws before they could go to the Tsar, and the Tsar had absolute veto power. Not only did the Tsar retain absolute veto power, there were certain powers left to only the Tsar, in particular control over the army, which hindered the Duma’s ability to truly be a representative and fully powerful legislative body. Among the political parties formed, or made legal, were the liberal-intelligentsia Constitutional Democratic party (the Kadets), the peasant leaders' Labour Group (Trudoviks), the less liberal Union of 17 October (the Octobrists), and the reactionary Union of Land-Owners. The electoral laws were promulgated in December 1905—franchise to male citizens over 25 years of age, electing through four electoral colleges. This was a weighted electoral system where the votes of some sections of society were worth more than others. For example, the vote of a landowner was worth 45 times more than the vote of an industrial worker. The first elections to the Duma took place in March 1906 and were boycotted by the socialists, the SRs and the Bolsheviks. In the First Duma, there were 170 Kadets, 90 Trudoviks, 100 non-aligned peasant representatives, 63 nationalists of various hues, and 16 Octobrists. The Duma's framework and power as controlled by the government issued Fundamental Law (Constitution of 1906), which retained most of the important functions of government to the Tsar. The Duma proved to be an ineffective institution with the Tsar still always having an upper hand in control and power, and it could be dissolved and recalled by the Tsar at any time, both of which were often executed. "The First and Second Dumas were dissolved before having time to enact into laws their comprehensive schemes of constructive reforms." Demanding further liberalisation and acting as a platform for "agitators", the First Duma was dissolved by the Tsar in July 1906. Despite the hopes of the Kadets and the fears of the government, there was no widespread popular reaction to this. However, an assassination attempt on Pyotr Stolypin led to the establishment of field trials for terrorists, and over the next eight months more than a thousand people were hanged. In 1907, another manifesto was written by the tsar in 1907 to revise election law in the Duma, allowing for extreme manipulation by the tsar to determine who was in the Duma. While the election framework of the Duma is one that allowed for gerrymandering, the Duma was used as a platform for those elected to have an opinion and have some ground in Russia’s bureaucracy that the autocratic Tsar had to deal with. The Third and Fourth Dumas also had very little to say for themselves, and the Duma existed until 1917 with the end of the Russian empire.
October Manifesto.
The October Manifesto served as a precursor to the Constitution of 1906. It was reluctantly put into place by Tsar Nicholas II who was convinced by Sergei Witte (the man who negotiated the end of the Russo-Japanese War) that it was necessary. Not only was it necessary, but it is what finally stopped the 1905 Revolution and kept Nicholas II in power for 12 more years. The opposition against the Tsar government was far too strong to not have a manifesto written to attempt to quell uprising. Witte was a strong proponent of a constitutional monarchy, an elected parliament, and enumerated rights and freedoms within the constitution. The main provisions and reforms of the manifesto were :
The manifesto was discussed by the Tsar and Witte till its signing by Nicholas II on 17 October 1905. Witte, the author of the October Manifesto, was also named chair of the new Council of Ministers under the new government. The strikes ended on 19 October, and singing, cheering, and other forms of demonstration followed the signing of the October Manifesto as it were unexpected that such concessions expressed by the constitution were to be made by the government. Main problems with the manifesto were that nowhere in the document does the word "constitution" come up, and Nicholas II never felt much pressure to follow to provisions of the manifesto as he for that moment regained the support of his people. However, the rights listed coupled with the institution of the Duma seemed to signal an end to authoritarian rule, and the October Manifesto failed to pacify liberals and isolate the left in Russia. Many groups either saw the manifesto as a movement towards democracy (while still holding doubts about its implementation), while other groups saw it as an incentive to keep moving forward with the Revolution. Kadets were in favor of the document's freedom of speech and other rights, while Marxists felt it wasn't enough of a concession and not a true signifier of democracy. While the October Manifesto outlined civil liberties and the establishment of a two-house Parliament, none of these measures came into effect until the Constitution of 1906.
Russian Constitution of 1906.
The Russian Constitution of 1906 was published on the eve of the convocation of the First Duma. The new Fundamental Law was enacted to institute promises of the October Manifesto as well as add new reforms. The Tsar was confirmed as absolute leader, with complete control of the executive, foreign policy, church, and the armed forces. The structure of the Duma was changed, becoming a lower chamber below the Council of Ministers, and was half-elected, half-appointed by the Tsar. Legislation had to be approved by the Duma, the Council, and the Tsar to become law. The Fundamental State Laws were the "culmination of the whole sequence of events set in motion in October 1905 and which consolidated the new status quo". The introduction of The Russian Constitution of 1906 isn't simply an institution of the October Manifesto. The introduction of the constitution states (and thus emphasizes) this:
Through the Constitution’s introduction, it makes no mention of any of the provisions of the October Manifesto. While it did enact the provisions laid out previously, its sole purpose seems again to be to propaganda for the monarchy and to simply not fall back on prior promises. The Constitution lasted until the fall of the empire in 1917, and the provisions coupled with the autocratic rule of the Tsar even under the new constitutional monarchy were never enough for Russians and Lenin.
Rise of terrorism.
The years 1904 and 1907 were a time of decline for the mass movements, such as strikes and political demonstrations, but also a time of rising political terrorism. SR Combat Organization and other combat groups carried out numerous assassinations targeting civil servants and police, and robberies. Between 1906 and 1909, revolutionaries killed 7,293 people, of whom 2,640 were officials, and wounded 8,061.
Notable victims of assassins included:
Repression.
The years of revolution were marked by a dramatic rise in the numbers of death sentences and executions. Different figures on the number of executions were compared by Senator Nikolai Tagantsev, and are listed in the table.
These numbers reflect only executions of civilians, and do not include a large number of summary executions by punitive army detachments and executions of military personnel that mutineed.
Anarchist thinker Peter Kropotkin also noted that official statistics did not include executions during punitive expeditions, especially in Siberia, the Caucasus, and the Baltic provinces.
By 1906 there were 4,509 political prisoners in Russian Poland, 20% of the empire's total.
Ivanovo Soviet.
Ivanovo Voznesensk was known as the 'Russian Manchester' for its textile mills. In 1905 its local revolutionaries were overwhelmingly Bolshevik. It was the first Bolshevik branch where workers outnumbered intellectuals.
11 May 1905: The 'Group', the revolutionary leadership, called for all the textile mills to strike.
12 May: The strike begins. Strike leaders meet in the local woods.
13 May: 40,000 workers assemble before the Administration Building to give Svirskii, the regional factory inspector a list of demands.
14 May: Workers' delegates elected at the suggestion of Svirskii. He wants people to negotiate with. A mass meeting is held in Administration Square. Svirskii tells them the mill owners won't meet their demands but will negotiate with elected mill delegates who will be immune to prosecution according to the governor.
15 May: Svirskii tells the strikers they can only negotiate over each factory in turn but they can hold elections wherever. The strikers elect delegates by mill right there in the surrounding boulevards. Later the delegates elect a chairman.
17 May: the meetings are moved to the bank of the Talka on the police chief's suggestion.
27 May: The delegates' meeting house is closed.
3 June: Cossacks break up a workers meeting, arresting over 20. Workers start sabotaging telephone wires and burn down a mill.
9 June: The police chief resigns.
12 June: all prisoners released. Mill owners mostly flee to Moscow. Neither side gives in.
27 June: workers agree to stop striking 1 July.
Finland.
In the Grand Duchy of Finland, the Social Democrats organised the general strike of 1905 (12–19 November [O.S. 30 October – 6 November]). The Red Guards were formed, led by captain Johan Kock. During the general strike, the "Red Declaration", written by Finnish politician and journalist Yrjö Mäkelin, was published in Tampere, demanding dissolution of the Senate of Finland, universal suffrage, political freedoms, and abolition of censorship. Leader of the constitutionalists, Leo Mechelin crafted the "November Manifesto" that led to the abolition of the Diet of Finland and of the four Estates, and to the creation of the modern Parliament of Finland. It also resulted in a temporary halt to the Russification policy started in 1899.
On 12 August [O.S. 30 July] 1906, Russian artillerymen and military engineers rose to rebellion in the fortress of Sveaborg (later called Suomenlinna), Helsinki. The Finnish Red Guards supported the Sveaborg Rebellion with a general strike, but the mutiny was quelled by loyal troops and ships of the Baltic Fleet within sixty hours.
Estonia.
In the Governorate of Estonia, Estonians called for freedom of the press and assembly, for universal suffrage, and for national autonomy. On 29 October [O.S. 16 October], the Russian army opened fire in a meeting on a street market in Tallinn, killing 94 and injuring over 200. The October Manifesto was supported in Estonia and the Estonian flag was displayed publicly for the first time. Jaan Tõnisson used the new political freedoms to widen the rights of Estonians by establishing the first Estonian political party - "National Progress Party".
Another, more radical political organisation, the "Estonian Social Democratic Workers' Union" was founded as well. The moderate supporters of Tõnisson and the more radical supporters of Jaan Teemant could not reach a consensus about how to continue with the revolution, only that they both wanted to limit the rights of Baltic Germans and to end Russification. The radical views were publicly welcomed and in December 1905, martial law was declared in Tallinn. A total of 160 manors were looted, resulting in ca. 400 workers and peasants being killed by the army. Estonian gains from the revolution were minimal, but the tense stability that prevailed between 1905 and 1917 allowed Estonians to advance the aspiration of national statehood.
Latvia.
Following the shooting of demonstrators in St. Petersburg a wide-scale general strike began in Riga. On 26 January [O.S. 13 January], Russian army troops opened fire on demonstrators killing 73 and injuring 200 people. During the summer of 1905, the focus of revolutionary events moved to the countryside with mass meetings and demonstrations. 470 new parish administrative bodies were elected in 94% of the parishes in Latvia. The Congress of Parish Representatives was held in Riga in November. In autumn 1905, armed conflict between the Baltic German nobility and the Latvian peasants begun in the rural areas of Livland and Courland. In Courland, the peasants seized or surrounded several towns. In Livland, the fighters controlled the Rūjiena-Pärnu railway line. Martial law was declared in Courland in August 1905, and in Livland in late November. Special punitive expeditions were dispatched in mid-December to suppress the movement. They executed 1170 people without trial or investigation and burned 300 peasant homes. Thousands were exiled to Siberia. Many Latvian intellectuals only escaped by fleeing to Western Europe or USA. In 1906, the revolutionary movement gradually subsided.
References.
</dl>

</doc>
<doc id="48740" url="http://en.wikipedia.org/wiki?curid=48740" title="Henri Poincaré">
Henri Poincaré

Jules Henri Poincaré (]; 29 April 1854 – 17 July 1912) was a French mathematician, theoretical physicist, engineer, and a philosopher of science. He is often described as a polymath, and in mathematics as "The Last Universalist" by Eric Temple Bell, since he excelled in all fields of the discipline as it existed during his lifetime.
As a mathematician and physicist, he made many original fundamental contributions to pure and applied mathematics, mathematical physics, and celestial mechanics. He was responsible for formulating the Poincaré conjecture, which was one of the most famous unsolved problems in mathematics until it was solved in 2002–2003. In his research on the three-body problem, Poincaré became the first person to discover a chaotic deterministic system which laid the foundations of modern chaos theory. He is also considered to be one of the founders of the field of topology.
Poincaré made clear the importance of paying attention to the invariance of laws of physics under different transformations, and was the first to present the Lorentz transformations in their modern symmetrical form. Poincaré discovered the remaining relativistic velocity transformations and recorded them in a letter to Dutch physicist Hendrik Lorentz (1853–1928) in 1905. Thus he obtained perfect invariance of all of Maxwell's equations, an important step in the formulation of the theory of special relativity.
The Poincaré group used in physics and mathematics was named after him.
Life.
Poincaré was born on 29 April 1854 in Cité Ducale neighborhood, Nancy, Meurthe-et-Moselle into an influential family. His father Leon Poincaré (1828–1892) was a professor of medicine at the University of Nancy. His adored younger sister Aline married the spiritual philosopher Emile Boutroux. Another notable member of Henri's family was his cousin, Raymond Poincaré, who would become the President of France, 1913 to 1920, and a fellow member of the Académie française. He was raised in the Roman Catholic faith. However, he later on became an agnostic and criticized religious dogmas particularly with respect to the mixing of theology and science.
Education.
During his childhood he was seriously ill for a time with diphtheria and received special instruction from his mother, Eugénie Launois (1830–1897).
In 1862, Henri entered the Lycée in Nancy (now renamed the Lycée Henri Poincaré in his honour, along with the University of Nancy). He spent eleven years at the Lycée and during this time he proved to be one of the top students in every topic he studied. He excelled in written composition. His mathematics teacher described him as a "monster of mathematics" and he won first prizes in the concours général, a competition between the top pupils from all the Lycées across France. His poorest subjects were music and physical education, where he was described as "average at best". However, poor eyesight and a tendency towards absentmindedness may explain these difficulties. He graduated from the Lycée in 1871 with a bachelor's degree in letters and sciences.
During the Franco-Prussian War of 1870, he served alongside his father in the Ambulance Corps.
Poincaré entered the École Polytechnique in 1873 and graduated in 1875. There he studied mathematics as a student of Charles Hermite, continuing to excel and publishing his first paper ("Démonstration nouvelle des propriétés de l'indicatrice d'une surface") in 1874. From November 1875 to June 1878 he studied at the École des Mines, while continuing the study of mathematics in addition to the mining engineering syllabus, and received the degree of ordinary mining engineer in March 1879.
As a graduate of the École des Mines, he joined the Corps des Mines as an inspector for the Vesoul region in northeast France. He was on the scene of a mining disaster at Magny in August 1879 in which 18 miners died. He carried out the official investigation into the accident in a characteristically thorough and humane way.
At the same time, Poincaré was preparing for his doctorate in sciences in mathematics under the supervision of Charles Hermite. His doctoral thesis was in the field of differential equations. It was named "Sur les propriétés des fonctions définies par les équations différences". Poincaré devised a new way of studying the properties of these equations. He not only faced the question of determining the integral of such equations, but also was the first person to study their general geometric properties. He realised that they could be used to model the behaviour of multiple bodies in free motion within the solar system. Poincaré graduated from the University of Paris in 1879.
The first scientific achievements.
After receiving his degree, Poincaré began teaching as junior lecturer in mathematics at the University of Caen in Normandy (in December 1879). At the same time he published his first major article concerning the treatment of a class of automorphic functions.
There, in Caen, he met his future wife, Louise Poulin d'Andesi (Louise Poulain d'Andecy) and on 20 April 1881, they married. Together they had four children: Jeanne (born 1887), Yvonne (born 1889), Henriette (born 1891), and Léon (born 1893).
Poincaré immediately established himself among the greatest mathematicians of Europe, attracting the attention of many prominent mathematicians. In 1881 Poincaré was invited to take a teaching position at the Faculty of Sciences of the University of Paris; he accepted the invitation. During the years of 1883 to 1897, he taught mathematical analysis in École Polytechnique.
In 1881–1882, Poincaré created a new branch of mathematics: the qualitative theory of differential equations. He showed how it is possible to derive the most important information about the behavior of a family of solutions without having to solve the equation (since this may not always be possible). He successfully used this approach to problems in celestial mechanics and mathematical physics.
Career.
He never fully abandoned his mining career to mathematics. He worked at the Ministry of Public Services as an engineer in charge of northern railway development from 1881 to 1885. He eventually became chief engineer of the Corps de Mines in 1893 and inspector general in 1910.
Beginning in 1881 and for the rest of his career, he taught at the University of Paris (the Sorbonne). He was initially appointed as the "maître de conférences d'analyse" (associate professor of analysis). Eventually, he held the chairs of Physical and Experimental Mechanics, Mathematical Physics and Theory of Probability, and Celestial Mechanics and Astronomy.
In 1887, at the young age of 32, Poincaré was elected to the French Academy of Sciences. He became its president in 1906, and was elected to the Académie française in 1909.
In 1887, he won Oscar II, King of Sweden's mathematical competition for a resolution of the three-body problem concerning the free motion of multiple orbiting bodies. (See #The three-body problem section below)
In 1893, Poincaré joined the French Bureau des Longitudes, which engaged him in the synchronisation of time around the world. In 1897 Poincaré backed an unsuccessful proposal for the decimalisation of circular measure, and hence time and longitude. It was this post which led him to consider the question of establishing international time zones and the synchronisation of time between bodies in relative motion. (See #Work on relativity section below)
In 1899, and again more successfully in 1904, he intervened in the trials of Alfred Dreyfus. He attacked the spurious scientific claims of some of the evidence brought against Dreyfus, who was a Jewish officer in the French army charged with treason by colleagues.
In 1912, Poincaré underwent surgery for a prostate problem and subsequently died from an embolism on 17 July 1912, in Paris. He was 58 years of age. He is buried in the Poincaré family vault in the Cemetery of Montparnasse, Paris.
A former French Minister of Education, Claude Allègre, has recently (2004) proposed that Poincaré be reburied in the Panthéon in Paris, which is reserved for French citizens only of the highest honour.
Students.
Poincaré had two notable doctoral students at the University of Paris, Louis Bachelier (1900) and Dimitrie Pompeiu (1905).
Work.
Summary.
Poincaré made many contributions to different fields of pure and applied mathematics such as: celestial mechanics, fluid mechanics, optics, electricity, telegraphy, capillarity, elasticity, thermodynamics, potential theory, quantum theory, theory of relativity and physical cosmology.
He was also a populariser of mathematics and physics and wrote several books for the lay public.
Among the specific topics he contributed to are the following:
The three-body problem.
The problem of finding the general solution to the motion of more than two orbiting bodies in the solar system had eluded mathematicians since Newton's time. This was known originally as the three-body problem and later the "n"-body problem, where "n" is any number of more than two orbiting bodies. The "n"-body solution was considered very important and challenging at the close of the 19th century. Indeed in 1887, in honour of his 60th birthday, Oscar II, King of Sweden, advised by Gösta Mittag-Leffler, established a prize for anyone who could find the solution to the problem. The announcement was quite specific:
In case the problem could not be solved, any other important contribution to classical mechanics would then be considered to be prizeworthy. The prize was finally awarded to Poincaré, even though he did not solve the original problem.
One of the judges, the distinguished Karl Weierstrass, said, "This work cannot indeed be considered as furnishing the complete solution of the question proposed, but that it is nevertheless of such importance that its publication will inaugurate a new era in the history of celestial mechanics."
(The first version of his contribution even contained a serious error; for details see the article by Diacu). The version finally printed contained many important ideas which led to the theory of chaos. The problem as stated originally was finally solved by Karl F. Sundman for "n" = 3 in 1912 and was generalised to the case of "n" > 3 bodies by Qiudong Wang in the 1990s.
Work on relativity.
Local time.
Poincaré's work at the Bureau des Longitudes on establishing international time zones led him to consider how clocks at rest on the Earth, which would be moving at different speeds relative to absolute space (or the "luminiferous aether"), could be synchronised. At the same time Dutch theorist Hendrik Lorentz was developing Maxwell's theory into a theory of the motion of charged particles ("electrons" or "ions"), and their interaction with radiation. In 1895 Lorentz had introduced an auxiliary quantity (without physical interpretation) called "local time" formula_1
and introduced the hypothesis of length contraction to explain the failure of optical and electrical experiments to detect motion relative to the aether (see Michelson–Morley experiment).
Poincaré was a constant interpreter (and sometimes friendly critic) of Lorentz's theory. Poincaré as a philosopher was interested in the "deeper meaning". Thus he interpreted Lorentz's theory and in so doing he came up with many insights that are now associated with special relativity. In (1898), Poincaré said, "
A little reflection is sufficient to understand that all these affirmations have by themselves no meaning. They can have one only as the result of a convention." He also argued that scientists have to set the constancy of the speed of light as a postulate to give physical theories the simplest form.
Based on these assumptions he discussed in 1900 Lorentz's "wonderful invention" of local time and remarked that it arose when moving clocks are synchronised by exchanging light signals assumed to travel with the same speed in both directions in a moving frame.
Principle of relativity and Lorentz transformations.
He discussed the "principle of relative motion" in two papers in 1900
and named it the principle of relativity in 1904, according to which no physical experiment can discriminate between a state of uniform motion and a state of rest.
In 1905 Poincaré wrote to Lorentz about Lorentz's paper of 1904, which Poincaré described as a "paper of supreme importance." In this letter he pointed out an error Lorentz had made when he had applied his transformation to one of Maxwell's equations, that for charge-occupied space, and also questioned the time dilation factor given by Lorentz.
In a second letter to Lorentz, Poincaré gave his own reason why Lorentz's time dilation factor was indeed correct after all: it was necessary to make the Lorentz transformation form a group and gave what is now known as the relativistic velocity-addition law.
Poincaré later delivered a paper at the meeting of the Academy of Sciences in Paris on 5 June 1905 in which these issues were addressed. In the published version of that he wrote:
and showed that the arbitrary function formula_2 must be unity for all formula_3 (Lorentz had set formula_4 by a different argument) to make the transformations form a group. In an enlarged version of the paper that appeared in 1906 Poincaré pointed out that the combination formula_5 is invariant. He noted that a Lorentz transformation is merely a rotation in four-dimensional space about the origin by introducing formula_6 as a fourth imaginary coordinate, and he used an early form of four-vectors. Poincaré expressed a disinterest in a four-dimensional reformulation of his new mechanics in 1907, because in his opinion the translation of physics into the language of four-dimensional geometry would entail too much effort for limited profit. So it was Hermann Minkowski who worked out the consequences of this notion in 1907.
Mass–energy relation.
Like others before, Poincaré (1900) discovered a relation between mass and electromagnetic energy. While studying the conflict between the action/reaction principle and Lorentz ether theory, he tried to determine whether the center of gravity still moves with a uniform velocity when electromagnetic fields are included. He noticed that the action/reaction principle does not hold for matter alone, but that the electromagnetic field has its own momentum. Poincaré concluded that the electromagnetic field energy of an electromagnetic wave behaves like a fictitious fluid ("fluide fictif") with a mass density of "E"/"c"2. If the center of mass frame is defined by both the mass of matter "and" the mass of the fictitious fluid, and if the fictitious fluid is indestructible—it's neither created or destroyed—then the motion of the center of mass frame remains uniform. But electromagnetic energy can be converted into other forms of energy. So Poincaré assumed that there exists a non-electric energy fluid at each point of space, into which electromagnetic energy can be transformed and which also carries a mass proportional to the energy. In this way, the motion of the center of mass remains uniform. Poincaré said that one should not be too surprised by these assumptions, since they are only mathematical fictions.
However, Poincaré's resolution led to a paradox when changing frames: if a Hertzian oscillator radiates in a certain direction, it will suffer a recoil from the inertia of the fictitious fluid. Poincaré performed a Lorentz boost (to order "v"/"c") to the frame of the moving source. He noted that energy conservation holds in both frames, but that the law of conservation of momentum is violated. This would allow perpetual motion, a notion which he abhorred. The laws of nature would have to be different in the frames of reference, and the relativity principle would not hold. Therefore he argued that also in this case there has to be another compensating mechanism in the ether.
Poincaré himself came back to this topic in his St. Louis lecture (1904). This time (and later also in 1908) he rejected the possibility that energy carries mass and criticized the ether solution to compensate the above-mentioned problems:
The apparatus will recoil as if it were a cannon and the projected energy a ball, and that contradicts the principle of Newton, since our present projectile has no mass; it is not matter, it is energy. [..] Shall we say that the space which separates the oscillator from the receiver and which the disturbance must traverse in passing from one to the other, is not empty, but is filled not only with ether, but with air, or even in inter-planetary space with some subtile, yet ponderable fluid; that this matter receives the shock, as does the receiver, at the moment the energy reaches it, and recoils, when the disturbance leaves it? That would save Newton's principle, but it is not true. If the energy during its propagation remained always attached to some material substratum, this matter would carry the light along with it and Fizeau has shown, at least for the air, that there is nothing of the kind. Michelson and Morley have since confirmed this. We might also suppose that the motions of matter proper were exactly compensated by those of the ether; but that would lead us to the same considerations as those made a moment ago. The principle, if thus interpreted, could explain anything, since whatever the visible motions we could imagine hypothetical motions to compensate them. But if it can explain anything, it will allow us to foretell nothing; it will not allow us to choose between the various possible hypotheses, since it explains everything in advance. It therefore becomes useless.
He also discussed two other unexplained effects: (1) non-conservation of mass implied by Lorentz's variable mass formula_7, Abraham's theory of variable mass and Kaufmann's experiments on the mass of fast moving electrons and (2) the non-conservation of energy in the radium experiments of Madame Curie.
It was Albert Einstein's concept of mass–energy equivalence (1905) that a body losing energy as radiation or heat was losing mass of amount "m" = "E"/"c"2 that resolved Poincaré's paradox, without using any compensating mechanism within the ether. The Hertzian oscillator loses mass in the emission process, and momentum is conserved in any frame. However, concerning Poincaré's solution of the Center of Gravity problem, Einstein noted that Poincaré's formulation and his own from 1906 were mathematically equivalent.
Poincaré and Einstein.
Einstein's first paper on relativity was published three months after Poincaré's short paper, but before Poincaré's longer version. Einstein relied on the principle of relativity to derive the Lorentz transformations and used a similar clock synchronisation procedure (Einstein synchronisation) to the one that Poincaré (1900) had described, but Einstein's was remarkable in that it contained no references at all. Poincaré never acknowledged Einstein's work on special relativity. However, Einstein expressed sympathy with Poincaré's outlook obliquely in a letter to Hans Vaihinger on 3 May 1919, when Einstein considered Vaihinger's general outlook to be close to his own and Poincaré's to be close to Vaihinger's. In public, Einstein acknowledged Poincaré posthumously in the text of a lecture in 1921 called "Geometrie und Erfahrung" in connection with non-Euclidean geometry, but not in connection with special relativity. A few years before his death, Einstein commented on Poincaré as being one of the pioneers of relativity, saying "Lorentz had already recognised that the transformation named after him is essential for the analysis of Maxwell's equations, and Poincaré deepened this insight still further ..."
Algebra and number theory.
Poincaré introduced group theory to physics, and was the first to study the group of Lorentz transformations. He also made major contributions to the theory of discrete groups and their representations.
Topology.
The subject is clearly defined by Felix Klein in his "Erlangen Program" (1872): the geometry invariants of arbitrary continuous transformation, a kind of geometry. The term "topology" was introduced, as suggested by Johann Benedict Listing, instead of previously used "Analysis situs". Some important concepts were introduced by Enrico Betti and Bernhard Riemann. But the foundation of this science, for a space of any dimension, was created by Poincaré. His first article on this topic appeared in 1894.
His research in geometry led to the abstract topological definition of homotopy and homology. He also first introduced the basic concepts and invariants of combinatorial topology, such as Betti numbers and the fundamental group. Poincaré proved a formula relating the number of edges, vertices and faces of "n"-dimensional polyhedron (the Euler–Poincaré theorem) and gave the first precise formulation of the intuitive notion of dimension.
Astronomy and celestial mechanics.
Poincaré published two now classical monographs, "New Methods of Celestial Mechanics" (1892–1899) and "Lectures on Celestial Mechanics" (1905–1910). In them, he successfully applied the results of their research to the problem of the motion of three bodies and studied in detail the behavior of solutions (frequency, stability, asymptotic, and so on). They introduced the small parameter method, fixed points, integral invariants, variational equations, the convergence of the asymptotic expansions. Generalizing a theory of Bruns (1887), Poincaré showed that the three-body problem is not integrable. In other words, the general solution of the three-body problem can not be expressed in terms of algebraic and transcendental functions through unambiguous coordinates and velocities of the bodies. His work in this area were the first major achievements in celestial mechanics since Isaac Newton.
These include the idea of Poincaré, who later became the base for mathematical "chaos theory" (see, in particular, the Poincaré recurrence theorem) and the general theory of dynamical systems.
Poincaré authored important works on astronomy for the equilibrium figures gravitating rotating fluid. He introduced the important concept of bifurcation points, proved the existence of equilibrium figures of non-ellipsoid, including ring-shaped and pear-shaped figures, their stability. For this discovery, Poincaré received the Gold Medal of the Royal Astronomical Society (1900).
Differential equations and mathematical physics.
After defending his doctoral thesis on the study of singular points of the system of differential equations, Poincaré wrote a series of memoirs under the title "On curves defined by differential equations" (1881–1882). In these articles, he built a new branch of mathematics, called "qualitative theory of differential equations." Poincaré showed that even if the differential equation can not be solved in terms of known functions, yet from the very form of the equation, a wealth of information about the properties and behavior of the solutions can be found. In particular, Poincaré investigated the nature of the trajectories of the integral curves in the plane, gave a classification of singular points (saddle, focus, center, node), introduced the concept of a limit cycle and the loop index, and showed that the number of limit cycles is always finite, except for some special cases. Poincaré also developed a general theory of integral invariants and solutions of the variational equations. For the finite-difference equations, he created a new direction – the asymptotic analysis of the solutions. He applied all these achievements to study practical problems of mathematical physics and celestial mechanics, and the methods used were the basis of its topological works.
Assessments.
Poincaré's work in the development of special relativity is well recognised, though most historians stress that despite many similarities with Einstein's work, the two had very different research agendas and interpretations of the work. Poincaré developed a similar physical interpretation of local time and noticed the connection to signal velocity, but contrary to Einstein he continued to use the ether-concept in his papers and argued that clocks in the ether show the "true" time, and moving clocks show the local time. So Poincaré tried to keep the relativity principle in accordance with classical concepts, while Einstein developed a mathematically equivalent kinematics based on the new physical concepts of the relativity of space and time.
While this is the view of most historians, a minority go much further, such as E. T. Whittaker, who held that Poincaré and Lorentz were the true discoverers of Relativity.
Character.
Poincaré's work habits have been compared to a bee flying from flower to flower. Poincaré was interested in the way his mind worked; he studied his habits and gave a talk about his observations in 1908 at the Institute of General Psychology in Paris. He linked his way of thinking to how he made several discoveries.
The mathematician Darboux claimed he was "un intuitif" (intuitive), arguing that this is demonstrated by the fact that he worked so often by visual representation. He did not care about being rigorous and disliked logic. (Despite this opinion, Jacques Hadamard wrote that Poincaré's research demonstrated marvelous clarity. and Poincaré himself wrote that he believed that logic was not a way to invent but a way to structure ideas and that logic limits ideas.)
Toulouse's characterisation.
Poincaré's mental organisation was not only interesting to Poincaré himself but also to Édouard Toulouse, a psychologist of the Psychology Laboratory of the School of Higher Studies in Paris. Toulouse wrote a book entitled "Henri Poincaré" (1910). In it, he discussed Poincaré's regular schedule:
These abilities were offset to some extent by his shortcomings:
In addition, Toulouse stated that most mathematicians worked from principles already established while Poincaré started from basic principles each time (O'Connor et al., 2002).
His method of thinking is well summarised as:
"Habitué à négliger les détails et à ne regarder que les cimes, il passait de l'une à l'autre avec une promptitude surprenante et les faits qu'il découvrait se groupant d'eux-mêmes autour de leur centre étaient instantanément et automatiquement classés dans sa mémoire." (Accustomed to neglecting details and to looking only at mountain tops, he went from one peak to another with surprising rapidity, and the facts he discovered, clustering around their center, were instantly and automatically pigeonholed in his memory.)—Belliver (1956)
Attitude towards transfinite numbers.
Poincaré was dismayed by Georg Cantor's theory of transfinite numbers, and referred to it as a "disease" from which mathematics would eventually be cured.
Poincaré said, "There is no actual infinite; the Cantorians have forgotten this, and that is why they have fallen into contradiction."
Honours.
Awards
Named after him
Philosophy.
Poincaré had philosophical views opposite to those of Bertrand Russell and Gottlob Frege, who believed that mathematics was a branch of logic. Poincaré strongly disagreed, claiming that intuition was the life of mathematics. Poincaré gives an interesting point of view in his book "Science and Hypothesis":
For a superficial observer, scientific truth is beyond the possibility of doubt; the logic of science is infallible, and if the scientists are sometimes mistaken, this is only from their mistaking its rule.
Poincaré believed that arithmetic is a synthetic science. He argued that Peano's axioms cannot be proven non-circularly with the principle of induction (Murzi, 1998), therefore concluding that arithmetic is "a priori" synthetic and not analytic. Poincaré then went on to say that mathematics cannot be deduced from logic since it is not analytic. His views were similar to those of Immanuel Kant (Kolak, 2001, Folina 1992). He strongly opposed Cantorian set theory, objecting to its use of impredicative definitions.
However, Poincaré did not share Kantian views in all branches of philosophy and mathematics. For example, in geometry, Poincaré believed that the structure of non-Euclidean space can be known analytically. Poincaré held that convention plays an important role in physics. His view (and some later, more extreme versions of it) came to be known as "conventionalism". Poincaré believed that Newton's first law was not empirical but is a conventional framework assumption for mechanics. He also believed that the geometry of physical space is conventional. He considered examples in which either the geometry of the physical fields or gradients of temperature can be changed, either describing a space as non-Euclidean measured by rigid rulers, or as a Euclidean space where the rulers are expanded or shrunk by a variable heat distribution. However, Poincaré thought that we were so accustomed to Euclidean geometry that we would prefer to change the physical laws to save Euclidean geometry rather than shift to a non-Euclidean physical geometry.
Free will.
Poincaré's famous lectures before the Société de Psychologie in Paris (published as "Science and Hypothesis", "The Value of Science", and "Science and Method") were cited by Jacques Hadamard as the source for the idea that creativity and invention consist of two mental stages, first random combinations of possible solutions to a problem, followed by a critical evaluation.
Although he most often spoke of a deterministic universe, Poincaré said that the subconscious generation of new possibilities involves chance.
It is certain that the combinations which present themselves to the mind in a kind of sudden illumination after a somewhat prolonged period of unconscious work are generally useful and fruitful combinations... all the combinations are formed as a result of the automatic action of the subliminal ego, but those only which are interesting find their way into the field of consciousness... A few only are harmonious, and consequently at once useful and beautiful, and they will be capable of affecting the geometrician's special sensibility I have been speaking of; which, once aroused, will direct our attention upon them, and will thus give them the opportunity of becoming conscious... In the subliminal ego, on the contrary, there reigns what I would call liberty, if one could give this name to the mere absence of discipline and to disorder born of chance.
Poincaré's two stages—random combinations followed by selection—became the basis for Daniel Dennett's two-stage model of free will.
References.
"This article incorporates material from on PlanetMath, which is licensed under the ."
Poincaré's writings in English translation.
Popular writings on the philosophy of science:
On algebraic topology:
On celestial mechanics:
On the philosophy of mathematics:
Other:

</doc>
<doc id="48743" url="http://en.wikipedia.org/wiki?curid=48743" title="List of phobias">
List of phobias

The English suffixes -phobia, -phobic, -phobe (from Greek φόβος "phobos", "fear") occur in technical usage in psychiatry to construct words that describe irrational, disabling fear as a mental disorder (e.g. agoraphobia), in chemistry to describe chemical aversions (e.g. hydrophobic), in biology to describe organisms that dislike certain conditions (e.g. acidophobia), and in medicine to describe hypersensitivity to a stimulus, usually sensory (e.g. photophobia). In common usage they also form words that describe dislike or hatred of a particular thing or subject. The suffix is antonymic to -phil-.
For more information on the psychiatric side, including how psychiatry groups phobias such as agoraphobia, social phobia, or simple phobia, see phobia. The following lists include words ending in "-phobia", and include fears that have acquired names. In some cases, the naming of phobias has become a word game, of notable example being a 1998 humorous article published by "BBC News". In some cases a word ending in "-phobia" may have an antonym with the suffix "-phil-", e.g. Germanophobe / Germanophile.
A large number of "-phobia" lists circulate on the Internet, with words collected from indiscriminate sources, often copying each other. Also, a number of psychiatric websites exist that at the first glance cover a huge number of phobias, but in fact use a standard text to fit any phobia and reuse it for all unusual phobias by merely changing the name. Sometimes it leads to bizarre results, such as suggestions to cure "prostitute phobia". Such practice is known as content spamming and is used to attract search engines.
Psychological conditions.
Specialists may prefer to avoid the suffix "-phobia" and use more descriptive terms such as personality disorders, anxiety disorders, and avoidant personality disorder.
Biology, chemistry.
Biologists use a number of "-phobia/-phobic" terms to describe predispositions by plants and animals against certain conditions. For antonyms, see here
Prejudices and discrimination.
The suffix "-phobia" is used to coin terms that denote a particular anti-ethnic or anti-demographic sentiment, such as Americanophobia, Europhobia, Francophobia, Hispanophobia, and Indophobia. Often a synonym with the prefix "anti-" already exists (e.g. Polonophobia vs. anti-Polonism). Anti-religious sentiments are expressed in terms such as Christianophobia and Islamophobia.
Other prejudices include:

</doc>
<doc id="48745" url="http://en.wikipedia.org/wiki?curid=48745" title="400s BC (decade)">
400s BC (decade)


</doc>
<doc id="48748" url="http://en.wikipedia.org/wiki?curid=48748" title="Phil Austin">
Phil Austin

Phil Austin (often credited as Philip; born April 6, 1941) is a comedian and writer. He was born in Denver, Colorado and later grew up in Fresno, California, attending Fresno High School. He attended Bowdoin College and UCLA, joining the staff of KPFK radio in Los Angeles in the late 1960s.
Austin is best known for his work as part of The Firesign Theatre (of which he is the only constant member), where he plays the group's best-known creation, Nick Danger. Other prominent roles are as Harry (Happy) Cox, the narrator of "Everything You Know Is Wrong" and Bebop Loco/Lobo on "Give Me Immortality or Give Me Death". He is also serves as the troupe's musician and record producer. His collection of short stories, "Tales of the Old Detective and Other Big Fat Lies", is published by Audio Editions. Two of his stories appear in the third volume of "Mirth of a Nation".
Austin also wrote a solo work, "Roller Maidens From Outer Space", and directed (and acted in) "Eat Or Be Eaten".
Stage versions of "Don't Crush That Dwarf, Hand Me the Pliers"; "The Further Adventures of Nick Danger, Third Eye"; "Waiting for the Electrician or Someone Like Him"; and "Temporarily Humboldt County" are published Broadway Play Publishing Inc.

</doc>
<doc id="48755" url="http://en.wikipedia.org/wiki?curid=48755" title="Neo-fascism">
Neo-fascism

Neo-fascism is a post–World War II ideology that includes significant elements of fascism. Neo-fascism usually includes ultranationalism, populism, anti-immigration policies or, where relevant, nativism, anti-communism, anti-marxism and opposition to the parliamentary system and liberal democracy. Allegations that a group is neo-fascist may be hotly contested, especially if the term is used as a political epithet. Some post–World War II regimes have been described as neo-fascist due to their authoritarian nature, and sometimes due to their fascination and sympathy towards fascist ideology and rituals.
Post-fascism is a label that has been applied to several European political parties that espouse a modified form of fascism and which partake in constitutional politics.
Bolivia.
The Bolivian Socialist Falange party founded in 1937 played a crucial role in mid-century Bolivian politics. Luis García Meza Tejada's regime took power during the 1980 "Cocaine Coup" in Bolivia with the help of Italian neo-fascist Stefano Delle Chiaie, Nazi war criminal Klaus Barbie and the Buenos Aires junta. That regime has been accused of neo-fascist tendencies and of admiration for Nazi paraphernalia and rituals. Hugo Banzer Suárez, who preceded Tejada, also displayed admiration towards Nazism and fascism.
Greece.
Fascism in Greece has been present in politics since the Greek National Socialist Party of 1932. After World War II, Britain and America supported the Pro-Nazi Fascists in a struggle against the Greek communist KKE movement. In April 1967, a few weeks prior to an election, a military coup d'état took place in Greece and a fascist military government ruled the country from 1967 to 1974. It was called the "Regime of the Colonels", and was headed by Colonel George Papadopoulos. The official reason given for the coup was that a "communist conspiracy" had infiltrated all levels of society.
The contemporary Greek political party Golden Dawn has been described as subscribing to neo-fascist and neo-Nazi beliefs and practices.
Indonesia.
Adolf Hitler's propaganda for the hegemony of "Greater Germany" inspired similar ideas of "Indonesia Mulia" (esteemed Indonesia) and "Indonesia Raya" (great Indonesia) in the former Dutch colony. The first fascist party was the Partai Fasis Indonesia (PFI). Sukarno did admire Hitler's Third Reich and its vision of happiness for all: "It's in the Third Reich that the Germans will see Germany at the apex above other nations in this world," he said in 1963. He stated that Hitler was 'extraordinarily clever' in 'depicting his ideals': he spoke about Hitler's rhetorical skills, but denied any association with Nazism as an ideology, saying that Indonesian nationalism was not as narrow as Nazi nationalism.
Italy.
Italy was broadly divided into two political blocs following World War II, the Christian Democracy, which remained in power until the 1980s, and the Italian Communist Party (PCI), very strong immediately after the war.
With the beginning of Cold War it was feared by British government that the requested extradition of Italian war criminals to Yugoslavia would benefit PCI. Preventing anything like the Nuremberg trial for Italian war crimes, the collective memory of the crimes committed by Italians was expelled from public media, from textbooks in Italian schools, and also from the academic discourse on Western side of the Iron curtain throughout the Cold War. PCI was expulsed from power in May 1947, a month before the Paris Conference on the Marshall Plan, along with the French Communist Party (PCF).
In 1946 a group of Fascist soldiers founded the Italian Social Movement to continue the idea of Benito Mussolini. The leader of the MSI was Giorgio Almirante. who remained at the head of the party until his death in 1988.
Despite attempts in the 1970s towards a "historic compromise" between the PCI and the DC, the PCI didn't take part in the executive power until the 1980s. In December 1970, Junio Valerio Borghese attempted, along with Stefano Delle Chiaie, the "Borghese Coup" which was supposed to install a neo-fascist regime. Neo-fascist groups took part in various false flag terrorist attacks, starting with the December 1969 Piazza Fontana massacre, for which Vincenzo Vinciguerra was convicted, and usually considered to have stopped with the 1980 Bologna railway bombing. A 2000 parliamentary report from the center-left Olive Tree coalition concluded that "the strategy of tension had been supported by the United States in order to impede the PCI, and, in a lesser measure, the PSI from reaching executive power".
Since the 1990s, National Alliance, led by Gianfranco Fini, a former member of Italian Social Movement, has distanced itself from Mussolini and fascism and made efforts to improve relations with Jewish groups, with most die-hards leaving it; it now seeks to present itself as a respectable right-wing party. Fini joined Silvio Berlusconi's government. Neo-fascist parties in Italy are Tricolour Flame ("Fiamma Tricolore"), New Force ("Forza Nuova") and the National Social Front ("fronte sociale nazionale").
Lebanon.
Lebanon (1982–1988) – The far-right wing Christian Phalangist Party "Kataeb" and Lebanese Forces, backed by its own private army and inspired by the Spanish Falangists, was nominally in power in the country during the 1980s but had limited authority over the highly factionalised state, two-thirds of which was controlled by Israeli and Syrian troops.
Mongolia.
With Mongolia located between the larger nations Russia and China, ethnic insecurities have driven many Mongolians to neo-fascism, expressing nationalism centered around Genghis Khan and Adolf Hitler. Groups advocating these ideologies include Blue Mongolia, Dayar Mongol, and Mongolian National Union.
Taiwan.
The National Socialism Association (NSA) is a neo-fascist political organization founded in Taiwan in September 2006 by Hsu Na-chi (許娜琦), a 22-year-old female political science graduate of Soochow University. The NSA views Adolf Hitler as its leader and often uses the slogan "Long live Hitler". This has brought them condemnation from the Simon Wiesenthal Center, an international Jewish human rights center.
Turkey.
Grey Wolves is a Turkish ultra-nationalist and neo-fascist youth organization. It is the "unofficial militant arm" of the Nationalist Movement Party. The Grey Wolves have been accused of terrorism. According to Turkish authorities, the organization carried out 694 murders during the late-1970s political violence in Turkey, between 1974 and 1980.
United Kingdom.
The British National Party are a nationalist party in the United Kingdom who have the ideology of fascism and anti-immigration. Ex-party leader Nick Griffin said in 1998 that he believes the Holocaust "...'extermination' tale is a mixture of Allied wartime propaganda...", although has since retracted this statement.
The UK Independence Party has been accused by political opponents of holding to elements of Fascism e.g. populist nationalist and anti-immigration policies. However, UKIP have denied this, stating that their policies are not anti-immigration but pro-controlled immigration, patriotic not nationalist, in support of British democracy, and for all British citizens without regard to ethnicity or country of birth. Furthermore, they support a small state and economic freedom, which are not typically found within Fascism. A London School of Economics blog examined both UKIP and the BNP and, while it did find similarities in demographic support and a few policies, it failed to conclude any strong ideological links between them. However, it did remark on a coincidental increase in support of UKIP and a decrease in support for the BNP, speculating a possible relationship between them. Other left-wing literature, critical of UKIP, also denies that they are Fascist. 
United States.
Groups identified as neo-fascist in the United States generally include neo-Nazi organizations such as the National Alliance and the American Nazi Party. The Institute for Historical Review publishes negationist historical papers often of an anti-semitic nature.
International networks.
In 1951, the New European Order (NEO) neo-fascist Europe-wide alliance was set up to promote Pan-European nationalism. It was a more radical splinter group of the European Social Movement. The NEO had its origins in the 1951 Malmö conference when a group of rebels led by René Binet and Maurice Bardèche refused to join the European Social Movement as they felt that it did not go far enough in terms of racialism and anti-communism. As a result Binet joined with Gaston-Armand Amaudruz in a second meeting that same year in Zurich to set up a second group pledged to wage war on communists and non-white people.
Several Cold War regimes and international neo-fascist movements collaborated in operations such as assassinations and false flag bombings. Stefano Delle Chiaie, involved in Italy's strategy of tension, took part in Operation Condor; organizing the 1976 assassination attempt of Chilean Christian Democrat Bernardo Leighton. Vincenzo Vinciguerra escaped to Franquist Spain with the help of the SISMI, following the 1972 Peteano attack, for which he was sentenced to life. Along with Delle Chiaie, Vinciguerra testified in Rome in December 1995 before judge Maria Servini de Cubria, stating that Enrique Arancibia Clavel (a former Chilean secret police agent prosecuted for crimes against humanity in 2004) and US expatriate DINA agent Michael Townley were directly involved in General Carlos Prats' assassination. Michael Townley was sentenced in Italy to 15 years of prison for having served as intermediary between the DINA and the Italian neo-fascists.
The regimes of Franquist Spain, Augusto Pinochet's Chile and Alfredo Stroessner's Paraguay participated together in Operation Condor, which targeted political opponents worldwide. During the Cold War, these international operations gave rise to some cooperation between various neo-fascist elements engaged in a "Crusade against Communism". Anti-Fidel Castro terrorist Luis Posada Carriles was condemned for the Cubana Flight 455 bombing on October 6, 1976. According to the "Miami Herald", this bombing was decided on at the same meeting during which it was decided to target Chilean former minister Orlando Letelier, who was assassinated on September 21, 1976. Carriles wrote in his autobiography: "... we the Cubans didn't oppose ourselves to an isolated tyranny, nor to a particular system of our fatherland, but that we had in front of us a colossal enemy, whose main head was in Moscow, with its tentacles dangerously extended on all the planet."

</doc>
<doc id="48756" url="http://en.wikipedia.org/wiki?curid=48756" title="Battle of Lesnaya">
Battle of Lesnaya

The Battle of Lesnaya (Russian: Битва при Лесной "Bitva pri Lesnoy", Swedish: "Slaget vid Lesna"), was one of the major battles of the Great Northern War. It took place on September 28, 1708 (O.S.) / September 29, 1708 (Swedish calendar) / October 9, 1708 (N.S.) between a Russian army of 18,000 regulars and an unknown number of irregulars commanded by the Princes Repnin and Menshikov, and a Swedish force of around 12,000 men,:54 under the command of General Adam Ludwig Lewenhaupt, at the village of Lesnaya, located close to the border between the Polish–Lithuanian Commonwealth and Russia (now the village of Lyasnaya, south-east of Mogilev in Belarus). The Swedes were escorting a supply column of 4,500 wagons, needed by their army in the Ukraine.
Background.
Early Swedish victories at Humlebaek and at the Battle of Narva in 1700 temporarily took both Denmark and Russia out of the war. However, King Charles XII of Sweden proved unable to speedily end the war as it took eight years to deal with the remaining combatant Charles Augustus of Saxony-Poland. Meanwhile, Peter the Great rebuilt his army into modern form, concentrating on infantry trained to use linear tactics and modern firearms properly. He then achieved a stunning victory in Livonia, where he established the city of Saint Petersburg. In retaliation, Charles ordered an attack on the Russian heartland, launching an assault on Moscow from his campaign base in Poland.
Lewenhaupt, one of Sweden's foremost generals, was the commander of one of Sweden's best armies, based at the Baltic Sea port of Riga. In the summer of 1708, King Charles ordered him to march southward with most of his force and link up with the main army of 25,000 men, based in Poland. Lewenhaupt was to bring a fresh supply of ammunition and food to support the Swedish army in a proposed march on the Russian capital of Moscow.
However, Lewenhaupt found that gathering the needed supplies and preparing the army for an overland march took longer than expected and, on September 26, after waiting for Lewenhaupt for weeks, Charles XII abandoned his camps and decided to invade Ukraine, hoping to reach that rich granary before winter. At the time, Lewenhaupt was only about 80 miles from Charles's position.
Having observed these movements, Peter decided to attack Lewenhaupt's smaller force before Charles could support it. Menshikov moved quickly to intercept Lewenhaupt's force and prevented it from crossing the Sozh River to safety.
Lewenhaupt's convoy.
In early April 1708, the governor of Riga, Adam Ludwig Lewenhaupt visited Charles XII at the Royal army's winter quarters in Radoszkowice to discuss strategy and receive orders for the ongoing campaign against Russia. Here he was instructed to obtain a large amount of supplies and wagons that could be sufficient for the main army for about three weeks. Once having collected the supplies Lewenhaupt would assemble as much men as possible from the area, without leaving the garrisons completely stripped. Lewenhaupt would then use these troops to escort the convoy and rendezvous with Charles' main army at Mogilev, in early August.:102 
In May same year, Lewenhaupt returned to Riga in order to complete the task, which proved far from easy. The near lands had suffered many campaigns in the years of the Great Northern War and so much was drained of needed resources. In early June, the column—of which Lewenhaupt was gathering—was ordered to start campaigning to reach Charles XII in Mogilev, according to schedule. However, the convoy was nowhere ready to leave because of the difficulties assembling it. Only in the beginning of July it was "ready", having then suffered three to four weeks behind the schedule:102–105 and a significant shortage of men (20,000 men were expected, however, in reality only 12,000 soldiers were ready to march:54 with a general size of 13,000 soldiers and 1,300 officers):22 proved unable to make it to Charles' army before September.:103
On the march.
The march turned out slower than expected, torrential rain turned the roads into mud, streams became over flooded which turned out to be a major task to cross and so, unfortunately for Lewenhaupt and Charles, the expected time of arrival kept moving back. However, after several weeks of waiting and no words heard from Lewenhaupt, there was a twist to the plans as Charles found his position in Mogilev unsustainable and instead on September 26, decided to abandon his camps and march South towards Severia in Ukraine, hoping to reach that rich granary before winter. During this time Lewenhaupt was about 135 kilometers (90 miles) away from Charles and on September 28, he received new orders to rendezvous at Starodub and started marching south himself. His convoy passed between Mogilev and Gorki heading for Propoisk on the river Sozh.:103 By October 3, Lewenhaupt had crossed the Dnieper and headed south, the crossing itself has to be considered a "military masterpiece".:75 Having observed these movements, Peter I dispatched an army under Boris Sheremetev after Charles and gathered a force of his own to intercept with Lewenhaupt. The Russians made contact with Lewenhaupt's convoy on October 6, and immediately started harassing it, forcing the Swedes to march in defensive formation across difficult terrain while the numbers of shadowing Russian troops steady grew.:103–105
Skirmish at Belitsa.
Peter I, who overestimated the Swedish force being 16,000 men strong, had gathered numbers far superior of those of Lewenhaupt and was eager to catch his convoy while it was still out of reach of Charles' main army and safety.:113 Therefore, he desired to confront and destroy it before its crossing of the river Sozh where it would otherwise reach—as Peter thought—the protection of the main army (the Russians had misleading reports saying Charles was 25 kilometers away from Sozh and not 120 as they had previously presumed).:102 On October 8, the Russians in the area were large enough that they posed a considerable threat to the convoy and so the two sides confronted each other for some time at the village of Belitsa. Subsequently however, Lewenhaupt ordered a cavalry attack consisting of 4,000 men on the equally numbered Russian dragoons who were facing them, the Russian horse did not desire a fight and instead started retreating, persecuted by their enemies for a good four–kilometers step. In this encounter losses amounted to about fifty Russians and three to four wounded Swedes, a real battle did not develop as both sides parted and the confrontation ended with the quick cavalry skirmish.:106
Later the same day, Lewenhaupt reached the small village of Lesnaya and was within a day's march from Propoisk. By now he knew that Peter I was in the area with a fairly large amount of Russian troops. But he did not know exactly how large the Russian army was or if more units were on their way. Once he reached Propoisk he could cross the Sozh river and achieve relative safety in case he was the target of the whole Russian force.:105
Battle.
The Russians gathered their forces to attack the Swedes in the rear as they were crossing the stream of Lesnjanka at the village of Lesnaya, to march south against Propoisk in order to reach safety by crossing the river of Sozh. Thousands of wagons made for slow progress and bottlenecks and the scattered Swedish army was by then very vulnerable to Russian attacks,:137 subsequently Peter I took the advantage and pressed home the assault. His forces included 26,000 Russian regulars (13,000 under his direct command), 5,000 dragoons under Christian Felix Bauer at Berezovka, 8,000 infantry under Werden at Patskovo (infantry which would not participate in the battle):164 and an irregular force of 'estimated' 2,500 to 5,000 Cossacks and Kalmyks:108 in total, Peter had close to 30,000 men against Lewenhaupt. When the attack began, more than half of the Swedish army had crossed the Lesnjanka stream. They numbered about 12,000 men in seventeen battalions, with more than 4,500 wagons in their train.:142–144
Russian advance.
Peter I split his force of 13,000 regulars into two columns, the right (Western, 6,045 men) under himself with Mikhail Mikhailovich Golitsyn assisting (in reality Golitsyn commanded this group and Peter worked as his assistant):107 and the left (Eastern, 6,896 men) under General Aleksandr Menshikov. The two columns marched toward the "Middlefield" between the northern and southern forest fringes. Lewenhaupt's army was behind the southern fringe and Peter I attacked from the north. Menshikov's force traversed two kilometers of road while Peter I struggled to penetrate three kilometers of dense forest.:142–144 Excluding officers, at least 18,000 Russian regulars including Bauer's dragoon force would engage during the battle:229–235 (adding ~10% officers to this number it would reach close to 20,000), with 2,500–5,000 irregulars.:108 In total, 22,000 to 25,000 engaged.
Lewenhaupt initially had around 4,500 men (excluding officers) on the northern side of the Lesnjanka stream to receive the initial attack, his other forces having crossed the stream and marching towards Propoisk.:216 However, as the battle raged, he was able to obtain reinforcements and reach a total fighting force of around 9,000 men (including officers) against the Russians. At least 2,900 men were ordered to protect and maintain the baggage convoy. A Swedish outpost had also been placed on the "Middlefield", consisting of no more than three battalions (reduced to 900 men), to warn of and stall a possible Russian attack. This is where the two columns of Peter I marched.:142–144
Fighting at the outpost.
Around 10:00, October 9, 1708 (N.S.), the battle began. Peter I's column under Menshikov had reached the "Middlefield" from the north-west, finding the 900 Swedes deployed there. Unfortunately for Menshikov, the commander of the heavily outnumbered Swedish outpost, Lieutenant Colonel Freijbourg, seized the initiative and launched a "Carolean-style" surprise attack which threw the Russian column into confusion, while the sudden musket and cannon fire alerted the nearby main Swedish force.:142−144 After this initial success, the Swedes were forced to retreat with many wounded through the southern fringe of the forest, where they were relieved by five fresh battalions under the command of Berndt Otto Stackelberg which had marched from Lesnaya.:147–155
Meanwhile, Tsar Peter's right column had reached the Crossroads and traversed the marshes of Krivl, just south of the "Middlefield", close to where Menshikov's column had been in action. Having Peter to their left flank and Menshikov to the front, Stackelberg's five Swedish battalions were now fighting two Russian columns numbering 13,000 in all. Six other Swedish battalions were on their way to the battle zone. The Russians at the Crossroads under Peter were almost routed by the Swedes and could have faced a crushing defeat, had not the Russian Guards halted their advance. The fighting at the Crossroads surged back and forth. The Russian line was strengthened by six artillery pieces. However, the Swedes who themselves had no artillery in this particular fight, were able to capture four of them and block Peter's progress at the Krivl bridge.:147–155
Swedish withdrawal.
At little past 11:00, Facing the Swedish right flank at the "Middlefield", Menshikov's guardsmen executed a successful flanking maneuver, forcing the five Swedish battalions to retreat into the southern fringe of the forest and prepare to receive the expected Russian onslaught. Their departure left unguarded a bridge near the "Crossroads", leaving it clear for Russian troops to march out and form up "en masse". Thus trapped in a "pincer movement", hemmed in and outnumbered, Stackelberg—against the wishes of Lewenhaupt—ordered an orderly withdrawal. The six Swedish battalions which were yet to arrive on their march through the forest, were also ordered to retreat, an action which isolated and exposed Hälsinge's second battalion which had previously routed the Russians and now came close to being annihilated by them.:147–155
Lewenhaupt (who sought to gather his cavalry to support the Swedish infantry during the fighting at the "Middlefield" and "Crossroads") came under attack by Russian dragoons who swept eastward through the southern fringe of the forest and headed for the Swedish dragoons deployed east of Lesnaya, on the open field. The Russians had some success at first, but as soon as the main bulk of the Swedish cavalry arrived and charged in typical Carolean wedge formation the Russian cavalry was being repulsed and quickly broke.:156–166
Tsar Peter with his Russian infantry and dragoons had now pushed away the last retreating Swedes and had full control over the "southern forest edge". The Russians now strove to reach the "Lesnaya field" between the forest and the village of Lesnaya, to block the bridge over which the Swedes might obtain further reinforcements. (A company of 1,000 cavalry had already managed to get back to assist in the fight at Lesnaya.) A Swedish counter-offensive to push the Russians out of the forest was now ordered by Lewenhaupt, who had been very disappointed by Stackelberg's decision to retreat. The Swedes counter–attacked with the support of 16 artillery pieces from Lesnaya. However the Russian troops, backed by their own 30 cannons, were too strong and the Swedes had to fall back.:156–166
Pause in hostilities.
The Swedes retreated almost to the village of Lesnaya and the Russians followed them to the adjacent open terrain, intending to launch a decisive attack from there. However, both sides being exhausted by the day's intense combat, hostilities were ceased at about 15:00 when, separated by only 150–200 meters, the two sides sank down on the field, facing each other, and rested. During this extraordinary interlude, in which only three Russian cannons sounded off, the two armies distributed food, water and ammunition to their ranks, issued orders and deployed reinforcements in preparation for the final conflict. Somehow during this remarkable phase, the Russian General Friedrich von Hessen-Darmstadt was shot and mortally wounded as he rode back and forth in a provocative manner between the two armies. He died of his wounds four days later. The hour-long pause concluded at about 16:00, with the arrival, after a long march, of Bauer's company of 4,000 Russian dragoons.:167–170
Fighting at Lesnaya.
At a little past 16:00, the Swedes opened fire, with cannons positioned 600 meters from the southern forest edge, on the newly arrived dragoons, who were then attaching themselves to the Russians' left flank. The Russian dragoons under Bauer then—without awaiting orders from Peter I—charged against the Swedes, supported by most of the other Russian troops. The open terrain gave the Swedish army opportunity to closely coordinate its infantry and cavalry, an advantage which they gratefully seized. Repeatedly, Russian front line troops retreated from infantry "Gå–På" shock attacks only to find themselves under immediate attack from the rear by Swedish cavalry.:170–180 However, this could only be a temporary advantage in view of the Russian reserve strength, reportedly three battalions deep by this time, enabling an irresistible grinding advance.:167–170
The Russian right flank under Mikhail Mikhailovich Golitsyn moved to secure the sole bridge across the Lesnjanka in order to prevent the flow of Swedish reinforcements across it, while seeking to trap them with their backs to the river. However, the bridge was ferociously defended and the Russians were beaten off, suffering heavy losses. At this time, both sides were inconvenienced by a snowstorm, a rare event for early–October, even in Russia. At 17:00, Lewenhaupt ordered a concerted attack which, however, was blunted by a tactic of continuous fire which the Russians had devised to counter the Swedish "Gå–På" onslaught. The Swedes took heavy casualties and were driven further back towards the village. Their line was also split in two, one side against the Lesnaya (east of the bridge) and the other against the forest to the west. The all–important bridge was on the brink of being taken when it was saved by the arrival of 900 Swedish dragoons from across the river, whose fierce onslaught drove the Russians back.:170–180 At 19:00 when night fell, the Russians left the field and drew back to the forest fringe. The Swedes stood in their battle formations for several hours, expecting a night attack which did not come.:181–182
Aftermath.
For a few hours the Swedes remained in their positions in case of a renewed attack and to convince the Russians that they intended to stay. Subsequently Lewenhaupt decided to withdraw his army under the cover of the darkness:50 and continue on his march against Propoisk. Each unit slowly made its way across the stream as they were covered by the remaining units. During this progress, a number of wagons broke and partially blocked the road where the Swedish artillery was moving down, so it was decided a number of these would be sunk in the mud (to prevent them falling in Russian hands) as they were hard bringing in the rapid march. Having successfully crossed the stream with all his troops, Lewenhaupt continued towards Propoisk. However, this withdrawal was the beginning of the end for a large part of his army.:116
Swedish disaster.
Despite the difficult condition, having men lost in the woods during the march, the Swedes reached Propoisk, only to find that the town and bridge had been burned down. This was most likely done by Bauer's detachment as they were still blocking the crossing. By now the Swedish army was disintegrating into a mob as fear grew,:50 possibly of being trapped between Peter's army behind them and Bauer's detachment. There were also no suited material for building a bridge. The Swedes saw the risk in having the Russian army pursue them from behind and so Lewenhaupt decided that everything that could be carried be taken from the wagons, subsequently the whole wagon train was burned and the bulk of the essential supplies within.:50 This resulted in that a large part of the army took the opportunity to get drunk:50 and so was left for the enemy to catch, others decided that they were better off surrendering or try to reach home by themselves,:117 in total perhaps 4,000 men went missing after the battle. The next morning the Russians caught of with these deserters and stragglers, about 500 of them were killed at Propoisk. The Russians were content with this and proceeded to round up any deserters they could find, however, they did not attempt to confront the main body of Lewenhaupt's army as they were allowed to withdraw unmolested.:117
The following day Lewenhaupt found a crossing over the river Sozh and over the next two days the soldiers swam across the river to relative safety. By now order had been restored in the Swedish army and all signs of Russian pursuit had gone. The army—now without any artillery or wagon train—made good speed to reach its rendezvous with Charles' army at Starodub. During their way they were attacked by a large detachment of Russians, however, these were soon driven off. On October 23, Lewenhaupt's troops reached the main army at Rukova, having only 6,500 men left in his lines without the sufficient wagon train.:117–118
Modern look.
Lesnaya is often seen as the first great Russian victory of the war and the first indication of the final result of the campaign, in Russia it is said to be the "mother of Poltava". The battle was certainly proclaimed as a Russian triumph at the time, but in modern view, this may not be the case. The victorious Russian army had suffered considerable losses throughout the battle and did not manage to succeed with their goal, to crush the Lewenhaupt's army. Neither did it seriously pursue the retreating Swedish army, instead they contented themselves with catching the stranglers and march in the opposite direction of Lewenhaupt to celebrate their victory. Tsar Peter arranged for the news of the victory to be spread as much as possibly through official declarations and leaflets. At first the Russian version of events claimed they had completely destroyed a superior force, it soon became clear that this was not true so they subsequently modified it down to only equal odds. But the official declarations, leaflets etc., had already been dispatched and still influence the view of the battle today.:119
The two sides were in fact not equal in numbers, they only appear so in many accounts because the numbers given usually only count the initial Russian forces under the Tsar without taking in account the irregulars that accompanied the force or the later arrival of Bauer's command. Sometimes the Swedish units are also assumed to have been at full strength at the battle. Also while the initial Russian forces were about the same strength as the whole Swedish army, they did not all participate in the fighting. The Russians, in fact enjoyed a considerable numerical advantage in all stages of the battle, yet they had not been able to defeat their enemy. Similarly the Swedes were greatly constrained during the battle by the need to protect the vital wagons and their supplies.:119–120
Casualties.
Swedish casualties numbered not much more than 1,000 dead and captured during the battle itself, along with some thousands of wounded, totalling fewer than 3,000, according to Lewenhaupt. 1,000 Swedes were later killed or captured while making their way to the main army; 3,000 went missing (about 1,500 of whom found their way back to Courland). All of the supply wagons were abandoned and destroyed. According to official Russian estimates, the Swedes lost 8,000 killed in the battle and another 1,000 on the march, along with 876 captured, numbers which are questionable.:229–235
Russian casualties amounted to about 1,111 dead and another 2,856 wounded in the battle, according to Russian official claims, figures which are disputed as "incomplete and contradictory", according to Russian historian Pavel Konovaltjuk. The Swedish official reports claimed more than 20,000 Russians died in the battle, again a questionable number. Lewenhaupt initially estimated more than 6,000 dead and wounded Russians during the battle, but later—during captivity in Moscow—discovered that numbers of 9,000 dead and wounded Russians were reported by officers who took part in the battle.:229–235

</doc>
<doc id="48757" url="http://en.wikipedia.org/wiki?curid=48757" title="Great Northern War">
Great Northern War

The Great Northern War (1700–1721) was a conflict in which a coalition led by the Tsardom of Russia successfully contested the supremacy of the Swedish Empire in Central, Northern, and Eastern Europe. The initial leaders of the anti-Swedish alliance were Peter the Great of Russia, Frederick IV of Denmark–Norway and Augustus II the Strong of Saxony-Poland-Lithuania. Frederick IV and Augustus II were forced out of the alliance in 1700 and 1706 respectively, but rejoined it in 1709. George I of Brunswick-Lüneburg (Hanover) joined the coalition in 1714 for Hanover and in 1717 for Britain, and Frederick William I of Brandenburg-Prussia joined it in 1715.
Charles XII led the Swedish army. On the Swedish side were Holstein-Gottorp, several Polish and Lithuanian magnates under Stanisław Leszczyński (1704–10) and Cossacks under the Ukrainian Hetman Ivan Mazepa (1708–10). The Ottoman Empire temporarily hosted Charles XII of Sweden and intervened against Peter I.
The war started when an alliance of Denmark–Norway, Saxony and Russia declared war on the Swedish Empire, launching a threefold attack at Swedish Holstein-Gottorp, Swedish Livonia, and Swedish Ingria, sensing an opportunity as Sweden was ruled by the young Charles XII, who was eighteen years old and inexperienced. Sweden parried the Danish and Russian attacks at Travendal and Narva, and in a counter-offensive pushed August II's forces through Lithuania and Poland to Saxony, dethroning Augustus on the way and forcing him to acknowledge defeat in the Treaty of Altranstädt. The treaty also secured the extradition and execution of Johann Reinhold Patkul, architect of the alliance seven years earlier. Peter I had meanwhile recovered and gained ground in Sweden's Baltic provinces, where he cemented Russia's access to the Baltic Sea by founding Saint Petersburg in 1703. Charles XII moved from Saxony into Russia to confront Peter, but the campaign ended with the destruction of the main Swedish army at the decisive 1709 Battle of Poltava (in present-day Ukraine), and Charles' exile in Ottoman Bender. The Ottoman Empire defeated the Russian-Moldavian army in the Pruth River Campaign, but the peace treaty was in the end without great consequence to Russia's position.
After Poltava, the anti-Swedish coalition was re-established and subsequently joined by Hanover and Prussia. The remaining Swedish forces in plague-stricken areas south and east of the Baltic Sea were evicted, with the last city, Riga, falling in 1710. Most of the Swedish dominions were partitioned among the coalition members, destroying the Swedish "dominium maris baltici". Sweden proper was invaded from the west by Denmark–Norway and from the east by Russia, which had occupied Finland by 1714. The Danish forces were defeated. Charles XII opened up a Norwegian front, but was killed in Fredriksten in 1718.
The war ended with Sweden's defeat, leaving Russia as the new dominant power in the Baltic region and a major force in European politics. The formal conclusion of the war was marked by the Swedish-Hanoverian and Swedish-Prussian Treaties of Stockholm (1719), the Dano-Swedish Treaty of Frederiksborg (1720), and the Russo-Swedish Treaty of Nystad (1721). Therein, Sweden ceded her exemption from the Sound Dues, and lost the Baltic provinces and the southern part of Swedish Pomerania. The peace treaties also ended her alliance with Holstein-Gottorp. Hanover gained Bremen-Verden, Brandenburg-Prussia incorporated the Oder estuary (Stettin Lagoons), Russia secured the Baltic provinces, and Denmark strengthened her position in Schleswig-Holstein. In Sweden, the absolute monarchy had come to an end with the death of Charles XII, and the Age of Liberty began.
Background.
Between 1560 and 1658, Sweden created a Baltic empire centred on the Gulf of Finland and comprising the provinces of Karelia, Ingria, Estonia, and Livonia. During the Thirty Years' War Sweden gained tracts in Germany as well, including Western Pomerania, Wismar, the Duchy of Bremen, and Verden. During the same period Sweden conquered Danish and Norwegian provinces north of the Sound (1645; 1658). These victories may be ascribed to a well-trained army, which despite its comparatively small size, was far more professional than most continental armies, and also to a modernization of administration (both civilian and military) in the course of the 17th century which enabled the monarchy to harness the resources of the country and its empire in an effective way. Fighting in the field, the Swedish army was able, in particular, to make quick, sustained marches across large tracts of land and to maintain a high rate of small arms fire due to proficient military drill.
However, the Swedish state ultimately proved unable to support and maintain its army in a prolonged war. Campaigns on the continent had been proposed on the basis that the army would be financially self-supporting through plunder and taxation of newly gained land, a concept shared by most major powers of the period. The cost of the warfare proved to be much higher than the occupied countries could fund, and Sweden's coffers, and resources in manpower, were eventually drained in the course of long conflicts.
The foreign interventions in Russia during the Time of Troubles resulted in Swedish gains in the Treaty of Stolbovo (1617). The treaty deprived Russia of direct access to the Baltic Sea. Russian fortunes began to reverse in the final years of the 17th century, notably with the rise to power of Peter the Great, who looked to address the earlier losses and re-establish a Baltic presence. In the late 1690s, the adventurer Johann Patkul managed to ally Russia with Denmark and Saxony by the secret Treaty of Preobrazhenskoye and in 1700 the three powers attacked.
Opposing parties.
Swedish camp.
Charles XII of Sweden succeeded Charles XI of Sweden in 1697, aged 14. From his predecessor, he took over the Swedish Empire as an absolute monarch. Charles XI had tried to keep the empire out of wars, and concentrated on inner reforms such as reduction and allotment, which had strengthened the monarch's status and the empire's military abilities. Charles XII refrained from all kinds of luxury and alcohol and usage of the French language, since he considered these things decadent and superfluous. He preferred the life of an ordinary soldier on horseback, not that of contemporary baroque courts. He determinedly pursued his goal of dethroning his adversaries, whom he considered unworthy of their thrones due to broken promises, thereby refusing to take several chances to make peace. During the war, the most important Swedish commanders besides Charles XII were his close friend Carl Gustav Rehnskiöld, also Magnus Stenbock and Adam Ludwig Lewenhaupt.
Charles Frederick, son of Frederick IV, Duke of Holstein-Gottorp (a cousin of Charles XII) and Hedvig Sophia, daughter of Charles XI of Sweden, had been the Swedish heir since 1702. He claimed the throne upon Charles XII's death, but was supplanted by Ulrike Eleonora. Charles Frederick was married to a daughter of Peter I, Anna Petrovna.
Ivan Mazepa was a Ukrainian Cossack hetman who fought for Russia but defected to Charles XII in 1708. Mazepa died in 1710 in Ottoman exile.
Allied camp.
Peter the Great became Tsar in 1682 upon the death of his elder brother Feodor but did not become the actual ruler until 1689. He commenced reforming the country, turning the Russian tsardom into a modernized empire relying on trade and on a strong, professional army and navy. He greatly expanded the size of Russia during his reign while providing access to the Baltic, Black, and Caspian seas. Beside Peter, the principal Russian commanders were Aleksandr Danilovich Menshikov and Boris Sheremetev.
Augustus II the Strong, elector of Saxony and another cousin of Charles XII, gained the Polish crown after the death of Jan Sobieski in 1696. His ambitions to transform the Polish–Lithuanian Commonwealth into an absolute monarchy were not realized. His meeting with Peter the Great in Rawa Ruska in September 1698, where the plans to attack Sweden were made, became legendary for its decadence.
Frederick IV of Denmark-Norway, another cousin of Charles XII, succeeded Christian V in 1699 and continued his anti-Swedish policies. After the setbacks of 1700, he focused on transforming his state, an absolute monarchy, in a manner similar to Charles XI of Sweden. He did not achieve his main goal: to regain the former eastern Danish provinces lost to Sweden in the course of the 17th century. He was not able to keep northern Swedish Pomerania, Danish from 1715 to 1720. He did put an end to the Swedish threat south of Denmark. He ended Sweden's exemption from the Sound Dues (transit taxes/tariffs on cargo moved between the North Sea and the Baltic Sea).
Frederick William I entered the war as elector of Brandenburg and king in Prussia – the royal title had been secured in 1701. He was determined to gain the Oder estuary with its access to the Baltic Sea for the Brandenburgian core areas, which had been a state goal for centuries.
George I of the House of Hanover, elector of Brunswick-Lüneburg and, since 1714, king of Great Britain and of Ireland, took the opportunity to connect his landlocked German electorate to the North Sea.
Army size.
In 1700, Charles XII had a standing army of 77,000 men (based on annual training). By 1707 this number had swollen to at least 120,000 despite casualties.
Russia was able to mobilize a larger army, but could not put all of it into action simultaneously. The Russian mobilization system was ineffective and the expanding nation needed to be defended in many locations. A grand mobilization covering Russia's vast territories would have been unrealistic. Peter I tried to raise his army's morale to Swedish levels. Denmark contributed 20,000 men in their invasion of Holstein-Gottorp and more on other fronts. Poland and Saxony together could mobilize at least 100,000 men.
1700: Denmark, Riga and Narva.
Frederik IV of Denmark–Norway directed his first attack against Sweden's ally Holstein-Gottorp. In March 1700, a Danish army laid siege to Tönning. Simultaneously, Augustus II's forces advanced through Swedish Livonia, captured Dünamünde and laid siege to Riga.
Charles XII of Sweden first focused on attacking Denmark. The Swedish navy was able to outmaneuver the Danish Sound blockade and deploy an army near the Danish capital, Copenhagen. This surprise move and pressure by the Maritime Powers (England and the Dutch Republic) forced Denmark–Norway to withdraw from the war in August 1700 according to the terms of the Peace of Travendal.
Charles XII was now able to speedily deploy his army to the eastern coast of the Baltic Sea and face his remaining enemies: besides the army of Augustus II in Livonia, an army of Russian tsar Peter I was already on its way to invade Swedish Ingria, where it laid siege to Narva in October. In November, the Russian and Swedish armies met at the First Battle of Narva where the Russians suffered a crushing defeat.
After the dissolution of the first coalition through the peace of Travendal and with the victory at Narva; the Swedish chancellor, Benedict Oxenstjerna, attempted to use the bidding for the favour of Sweden by France and the Maritime Powers (then on the eve of the War of the Spanish Succession) to end the war and make Charles an arbiter of Europe.
1701–1706: Poland-Lithuania/Saxony.
Charles XII then turned south to meet his last undefeated opponent: Augustus II, Elector of Saxony, King of Poland and Grand Duke of Lithuania. Poland-Lithuania was formally neutral at this point, as August started the war as an Elector of Saxony. Disregarding Polish negotiation proposals supported by the Swedish parliament, Charles crossed into the Polish–Lithuanian Commonwealth and decisively defeated the Saxe-Polish forces in the Battle of Klissow in 1702 and in the Battle of Pultusk in 1703. This successful invasion enabled Charles XII to dethrone August II and coerce the Polish sejm to replace him with Stanisław Leszczyński in 1704.:694 August II resisted, still possessing control of his native Saxony, but was decisively defeated at the Battle of Fraustadt in 1706, a battle sometimes compared to the Ancient Battle of Cannae due to the Swedish forces' use of double envelopment, with a deadly result for the Saxon army. August II was forced to sign the Treaty of Altranstädt in 1706 in which he made peace with the Swedish Empire,:701 renounced his claims to the Polish–Lithuanian crown, accepted Stanisław Leszczyński as king, and ended his alliance with Russia. Patkul was also extradited and executed by breaking on the wheel in 1707, an incident which given his diplomatic immunity, infuriated opinion against the Swedish king, who then was expected to win the war against the only hostile power remaining, Tsar Peter's Russia.
1702–1710: Russia and the Baltic provinces.
The Battle of Narva dealt a severe setback to Peter the Great, but the shift of Charles XII's army to the Polish-Saxon threat soon afterwards, provided him with an opportunity to regroup and regain territory in the Baltic provinces. Russian victories at Erastfer and Nöteborg (Shlisselburg) provided access to Ingria in 1703, where Peter captured the Swedish fortress of Nyen, guarding the mouth of the River Neva.:691 Thanks to General Adam Ludwig Lewenhaupt, whose outnumbered forces fended the Russians off in the battles of Gemäuerthof and Jakobstadt, Sweden was able to maintain control of most of her Baltic provinces. Before going to war, Peter had made preparations for a navy and a modern-style army, based primarily on infantry drilled in the use of firearms.
The Nyen fortress was soon abandoned and demolished by Peter, who constructed nearby a superior fortress as a beginning to the city of Saint Petersburg. By 1704, other fortresses were situated on the island of Kotlin and the sand flats to its south. These became known as Kronstadt and Kronslot.:691 The Swedes attempted a raid on the Neva fort on 13 July 1704 with ships and landing forces, but the Russian fortifications held. In 1705, repeated Swedish attacks were made against Russian fortifications in the area, to little effect. A major assault on 15 July 1705 resulted in the deaths of more than a third of a 1,500-strong Swedish landing force.
In view of continued failure to check Russian consolidation, and with declining manpower, Sweden opted to blockade Saint Petersburg in 1705. In the summer of 1706, Swedish General Georg Johan Maidel crossed the Neva with 4000 troops and defeated an opposing Russian force, but made no move on Saint Petersburg. Later in the autumn Peter I led an army of 20,000 men in an attempt to take the Swedish town and fortress of Viborg. Unfortunately, bad roads proved impassable to his heavy siege guns. The troops, who arrived on 12 October, therefore had to abandon the siege after only a few days. On 12 May 1708, a Russian galley fleet made a lightning raid on Borgå and managed to return to Kronslot just one day before the Swedish battlefleet returned to the blockade, after being delayed by unfavourable winds.
In August 1708, a Swedish army of 12,000 men under General Georg Henrik Lybecker attacked Ingria, crossing the Neva from the north. They met stubborn resistance, ran out of supplies and, after reaching the Gulf of Finland west of Kronstadt, had to be evacuated by sea between 10–17 October. Over 11,000 men were evacuated but more than 5000 horses were slaughtered, which crippled the mobility and offensive capability of the Swedish army in Finland for several years. Peter I took advantage of this, and was able to redeploy a large number of men from Ingria to the Ukraine.
Charles spent the years 1702–06 in a protracted struggle with August the Strong; he had already inflicted defeat on him at Riga in June 1701 and took Warsaw the following year, but trying to force a decisive defeat proved elusive. Russia withdraws from Poland in the Spring of 1706, abandoning their artillery but escape from the pursuing Swedes who stop at Pinsk.:700 Charles wanted not just to defeat the Commonwealth army but to depose August (see above), whom he regarded as especially treasonous, and have him replaced with someone who would be a Swedish ally, and this goal proved hard to achieve. After years of marches and fighting around Poland he finally had to invade August's hereditary Saxony to bring him out of the war.:701 In the treaty of Altranstädt (1706), August was indeed forced to step down from the Polish throne, but Charles had lost a valuable time advantage over his main enemy in the east, Peter I, who had had the time to recover and build up a new and better army.
At this point, in 1707, Peter offered to retrocede everything he had so far occupied (essentially Ingria) except Saint Petersburg and the line of the Neva, to avoid a full-scale war, but Charles XII refused.:703 Instead he initiated a march from Saxony to invade Russia. Though his primary goal was Moscow, the strength of his forces was sapped by the cold weather (the winter of 1708/09 being one of the most severe in modern European history):707 and Peter's use of scorched earth tactics.:704 When the main army turned south to recover in the Ukraine,:706 the second army with supplies and reinforcements was intercepted and routed at Lesnaya—and so were the supplies and reinforcements of Swedish ally Ivan Mazepa in Baturyn. Charles was crushingly defeated by a larger Russian force under Peter in the Battle of Poltava and fled to the Ottoman Empire while the remains of his army surrendered at Perevolochna.
This shattering defeat in 1709 did not end the war, although it decided it. Denmark and Saxony joined the war again and Augustus the Strong, through the politics of Boris Kurakin, regained the Polish throne.:710 Peter continued his campaigns in the Baltics, and eventually he built up a powerful navy. In 1710 the Russian forces captured Riga,:711 at the time the most populated city in the Swedish realm, and Tallinn, evicting the Swedes from the Baltic provinces, now integrated in the Russian Empire by the capitulation of Estonia and Livonia.
Formation of a new anti-Swedish alliance.
After Poltava, Peter the Great and Augustus the Strong allied again in the Treaty of Thorn (1709); Frederick IV of Denmark-Norway with Augustus the Strong in the Treaty of Dresden (1709); and Russia with Denmark–Norway in the subsequent Treaty of Copenhagen. In the Treaty of Hanover (1710), Brunswick-Lüneburg (Hanover) whose elector was to become George I of Great Britain allied with Russia. In 1713, Brandenburg-Prussia allied with Russia in the Treaty of Schwedt. George I of Great Britain and Hanover concluded three alliances in 1715: the Treaty of Berlin with Denmark–Norway, the Treaty of Stettin with Brandenburg-Prussia, and the Treaty of Greifswald with Russia.
1709–1714: Ottoman Empire.
When his army surrendered, Charles XII of Sweden and a few soldiers escaped to Ottoman territory, founding a colony in front of Bender, Moldova. Peter I demanded Charles's eviction, and when the sultan refused, Peter decided to force it by invading the Ottoman Empire. Peter's army was trapped by an Ottoman army at the Pruth river. Peter managed to negotiate a retreat, making a few territorial concessions and promising to withdraw his forces from the Holy Roman Empire as well as allowing Charles's return to Sweden. These terms were laid out in the Treaty of Adrianople (1713). Charles showed no interest in returning, established a provisional court in his colony, and sought to persuade the sultan to engage in an Ottoman-Swedish assault on Russia. The sultan put an end to the generous hospitality granted and had the king arrested in what became known as the "kalabalik" in 1713. Charles was then confined at Timurtash and Demotika; later he abandoned his hopes for an Ottoman front and returned to Sweden in a 14-day ride.
1710–1716: Sweden and Northern Germany.
In 1710, the Swedish army in Poland retreated to Swedish Pomerania, pursued by the coalition. In 1711, siege was laid to Stralsund. Yet the town could not be taken due to the arrival of a Swedish relief army, which secured the Pomeranian pocket before turning west to defeat an allied army in the Battle of Gadebusch. Pursued by coalition forces, the Swedish army was trapped and surrendered in the Siege of Tönning.
In 1714, Charles XII returned from the Ottoman Empire, arriving in Stralsund in November. In nearby Greifswald, already lost to Sweden, Russian tsar Peter the Great and British king George I, in his position as Elector of Hanover, had just signed an alliance on 17 (OS)/28 (NS) October. Previously a formally neutral party in the Pomeranian campaigns, Brandenburg-Prussia openly joined the coalition by declaring war on Sweden in the summer of 1715. Charles was then at war with much of Northern Europe, and Stralsund was doomed. Charles remained there until December 1715, escaping only days before Stralsund fell. When Wismar surrendered in 1716, all of Sweden's Baltic and German possessions were lost.
1716–1718: Norway.
After Charles XII had returned from the Ottoman Empire and resumed personal control of the war effort, he initiated two Norwegian Campaigns, starting in February 1716, to force Denmark–Norway into a separate peace treaty. Furthermore, he attempted to bar Great Britain access to the Baltic Sea. In search for allies, Charles XII also negotiated with the British Jacobite party. This resulted in Great Britain declaring war on Sweden in 1717. The Norwegian campaigns were halted and the army withdrawn when Charles XII was shot dead while besieging Norwegian Fredriksten on 30 November 1718 (OS). He was succeeded by his sister, Ulrika Eleonora.
1710–1721: Finland.
War between Russia and Sweden continued to rage. After the disaster of Poltava in 1709, the shattered Swedish continental army could provide very little help. Russia captured Viborg (ru. Vyborg) in 1710 and successfully held it against Swedish attempts to retake the town in 1711. In 1712 started first Russian campaign to capture Finland under command of General Admiral Fyodor Apraksin. Apraksin gathered an army of 15,000 men to Vyborg and started the operation in late August. Swedish General Georg Henrik Lybecker chose not to face the Russians with his 7 500 men in the prepared positions close to Vyborg and instead withdrew west of Kymijoki river using scorched earth tactics. Apraksin's forces reached the river but chose not to cross it and instead withdrew back to Vyborg likely due to problems in supply. Swedish efforts to maintain their defences were greatly hampered by the drain of manpower by the continental army and various garrisons around the Baltic Sea as well as by the plague outbreak which struck Finland and Sweden between 1710–1713 which devastated the land killing amongst others over half of the population of Helsingfors (Helsinki).
After the failure of 1712 Peter the Great ordered that further campaigns in war ravaged regions of Finland with poor transportation network were to be performed along the coastline and the seaways near the coast. Alarmed by the Russian preparations Lybecker requested naval units to be brought in as soon as possible in the spring of 1713. However like so often Swedish naval units arrived only after the initial Russian spring campaign had ended. Nominally under command of Fyodor Apraksin, but accompanied by Peter the Great, fleet of coastal ships together with 12,000 men of infantry and artillery started the campaign by sailing from Kronstadt on 2 May 1713, further 4000 cavalry were later sent overland to join up with the army. The fleet had already arrived at Helsingfors (fi. Helsinki) on 8 May and were met by 1,800 Swedish infantry under General Carl Gustaf Armfeldt. Together with rowers from the ships Russians had 20,000 men in their disposal even without the cavalry. Defenders, however, managed to fend off landing attempts by the attackers until Russians landed to their flank at Sandviken which forced Armfelt to retire towards Borgå after setting afire both the town and all the supplies stored there as well as bridges leading north from the town. It was only on 12 May that Swedish squadron under Admiral Erik Johan Lillie made it to Helsinki but there was nothing it could do.
Following this bulk of the Russian forces moved along the coast towards Borgå towards the forces of Lybecker to whom Armfelt had joined. On 21–22 May 1713 Russian force of 10,000 men landed at Pernå (fi. Pernaja) and constructed fortifications there. Large stores of supplies and munitions were transported from Vyborg and Saint Petersburg to the new base of operations. Russian cavalry managed to link up with the rest of the army there as well. Lybecker's army of 7000 infantry and 3000 cavalry avoided contact with the Russians and instead kept withdrawing further inland without even contesting the control of Borgå region or the important coastal road between Helsingfors (Helsinki) and Åbo. This also severed the contact between Swedish fleet and ground forces and prevented Swedish naval units from supplying it. Soldiers in the Swedish army who were mostly Finnish resented being repeatedly ordered to withdraw without even seeing the enemy. Lybecker was soon recalled to Stockholm for a hearing and Armfelt was ordered to the command of the army. Under Armfelt's command Swedish army in Finland stopped to engage the advancing Russians at Pälkäne in October 1713 where Russian flanking manoeuvre forced him to withdraw to avoid getting encircled. Armies met later again at Storkyro (Isokyrö) in February 1714 where Russians won a decisive victory.
In 1714 far greater Swedish naval assets were diverted towards Finland which managed to cut the coastal sea route past Hangö cape already in early May 1714. This caused severe trouble for Russian supply route to Åbo and beyond as supplies had to be carried overland. Russian galley fleet arrived to the area already on 29 June but stayed idle until 26–27 July when under leadership of Peter Russian galleys managed to run the blockade making use of calm weather which immobilized the Swedish battlefleet losing only one galley of his force of roughly 100 galleys. Small hastily assembled Swedish coastal squadron met the Russian galley fleet west of Hangö cape in the battle of Gangut and was overpowered by the Russians who had nearly 10 fold superiority. Russian breach of the blockade at Hangö forced Swedish fleet to withdraw to prevent Russian galley fleet from reaching Sweden itself. The Russian army occupied Finland mostly in 1713–1714, capturing Åland from where population had already fled to Sweden on 13 August 1714. Since Russian galley fleet was not able to raid the Swedish coast, with exception of Umeå which was plundered on 18 September, fleet supported the advance of the Russian army which led to hastily withdrawal of the Swedish army from Brahestad to Torneå. The occupation period of Finland in 1714–1721 is known as the Greater Wrath.
1719–1721: Sweden.
After the death of Charles XII, Sweden still refused to make peace with Russia on Peter's terms. Despite a continued Swedish naval presence and strong patrols to protect the coast since 1715 small Russian raids took place in 1716 at Öregrund while in July 1717 Russian squadron landed troops to Gotland who raided for supplies. To place pressure on Sweden, Russia sent a large fleet in July 1719 to the Swedish east coast. There under protection of the Russian battlefleet the Russian galley fleet was split into three groups. One group headed for coast of Uppland, second to the vicinity of Stockholm and last to coast of Södermanland. Together they carried a landing force of nearly 30,000 men. Raiding continued for a month and devastated amongst others the towns of Norrtälje, Södertälje, Nyköping and Norrköping and almost all buildings in the archipelago of Stockholm were burned. A smaller Russian force advanced on the Swedish capital, but was stopped at the battle of Stäket on 13 August. Swedish and British fleets, now allied with Sweden, sailing from the west coast of Sweden failed to catch the raiders.
Since treaty of Frederiksborg in early 1720 Sweden was no longer in war with Denmark which allowed more forces to be placed against the Russians. This did not prevent Russian galleys from raiding town of Umeå once again. Later in July 1720 a squadron from Swedish battlefleet engaged the Russian galley fleet in battle of Grengam. While the result of the battle is contested it ended Russian galley raids in 1720. As negotiations for peace did not progress the Russian galleys were once again in 1721 sent to raid Swedish coast targeting primarily the Swedish coast between Gävle and Piteå.
Peace.
By the time of Charles XII's death, the anti-Swedish allies became increasingly divided on how to fill the power gap left behind by the defeated and retreating Swedish armies. George I and Frederik IV both coveted hegemony in northern Germany, while August the Strong was concerned about Frederick William I's ambitions on the southeastern Baltic coast. Peter the Great, whose forces were spread all around the Baltic Sea, envisioned hegemony in East Central Europe and sought to establish naval bases as far west as Mecklenburg. In January 1719, George I, August II and emperor Charles VI concluded a treaty in Vienna aimed at the reduction of Russia's frontiers to the pre-war limits.
Hanover-Great Britain and Brandenburg-Prussia thereupon negotiated separate peace treaties with Sweden, the treaties of Stockholm in 1719 and early 1720, which partitioned Sweden's northern German dominions among the parties. The negotiations were mediated by French diplomats, who sought to prevent a complete collapse of Sweden's position on the southern Baltic coast and achieved that Sweden was to retain Wismar and northern Swedish Pomerania. Hanover gained Swedish Bremen-Verden, Brandenburg-Prussia incorporated southern Swedish Pomerania.
In addition to the rivalries in the anti-Swedish coalition, there was an inner-Swedish rivalry between Charles Frederick, Duke of Holstein-Gottorp, and Frederick I of Hesse-Cassel for the Swedish throne. The Gottorp party succumbed and Ulrike Eleonora, wife of Frederick I, transferred power to her husband in May 1720. When peace was concluded with Denmark, the anti-Swedish coalition had already fallen apart, and Denmark was not in a military position to negotiate a return of her former eastern provinces across the sound. Frederick I was however willing to cede the Swedish support for his rival in Holstein-Gottorp, which came under Danish control and the northern part annexed, and furthermore cede the Swedish privilege of exemption from the Sound Dues. A respective treaty was concluded in Frederiksborg in June 1720.
When Sweden finally was at peace with Hanover, Great Britain, Brandenburg-Prussia and Denmark–Norway, she hoped that the anti-Russian sentiments of the Vienna parties and France would culminate in an alliance which would restore to her her Russian-occupied eastern provinces. Yet, primarily due to internal conflicts in Great Britain and France, that did not happen. Therefore, the war was finally concluded by the Treaty of Nystad between Russia and Sweden in Uusikaupunki ("Nystad") on 30 August 1721 (OS). Finland was returned to Sweden, while Swedish Estonia, Livonia, Ingria, Kexholm and the bulk of Karelia were ceded to Russia. Sweden's dissatisfaction with the result led to fruitless attempts at recovering the lost territories in the course of the following century, such as the Russo-Swedish War (1741–1743), and the Russo-Swedish War (1788–1790).
Saxe-Poland-Lithuania and Sweden did not conclude a formal peace treaty, instead, they renewed the Peace of Oliva that had ended the Second Northern War in 1660.
Sweden had lost almost all of its "overseas" holdings gained in the 17th century, and ceased to be a major power. Russia gained its Baltic territories, and became one of the greatest powers in Europe.
Further reading.
</dl>

</doc>
<doc id="48761" url="http://en.wikipedia.org/wiki?curid=48761" title="Denazification">
Denazification

Denazification (German: "Entnazifizierung") was an Allied initiative to rid German and Austrian society, culture, press, economy, judiciary, and politics of any remnants of the National Socialist ideology (Nazism). It was carried out specifically by removing from positions of power and influence those who had been Nazi Party members and by disbanding or rendering impotent the organizations associated with Nazism. The program of denazification was launched after the end of the Second World War and was solidified by the Potsdam Agreement.
The term "denazification" was first coined as a legal term in 1943 in the Pentagon, intended to be applied in a narrow sense with reference to the post-war German legal system. Soon afterward, it took on the more general meaning.
Overview.
Denazification in Germany was attempted through a series of directives issued by the Allied Control Council, seated in Berlin, beginning in January 1946. "Denazification directives" identified specific people and groups and outlined judicial procedures and guidelines for handling them. Though all the occupying forces had agreed on the initiative, the methods used for denazification and the intensity with which they were applied differed between the occupation zones.
Denazification also refers to the removal of the physical symbols of the Nazi regime. For example, in 1957 the West German government re-issued World War II Iron Cross medals, among other decorations, without the swastika in the center.
About 8.5 million Germans, or 10% of the population, had been members of the Nazi Party. Nazi-related organizations also had huge memberships, such as the German Labour Front (25 million), the National Socialists People's Welfare organization (17 million), the League of German Women, Hitler Youth, the Doctors' League, and others. It was through the Party and these organizations that the Nazi state was run, involving as many as 45 million Germans in total. In addition, Nazism found significant support among industrialists, who produced weapons or used slave labour, and large landowners, especially the Junkers in Prussia. Denazification after the surrender of Germany was thus an enormous undertaking, fraught with many difficulties.
The first difficulty was the enormous number of Germans who might have to be first investigated, then penalized if found to have supported the Nazi state to an unacceptable degree. In the early months of denazification there was a great will, especially among the Americans, to be utterly thorough, to investigate everyone and hold every supporter of Nazism to account; however, it turned out that the numbers simply made that goal impractical. It soon became evident, too, that pursuing denazification too scrupulously would make it impossible to create a functioning, democratic society in Germany, one that would be able to support itself economically and not become a burden on the victorious nations. Enforcing the strictest sanctions against lesser offenders would prevent too many talented people from participating in the reconstruction process. The Morgenthau Plan had recommended that the Allies create a post-war Germany with all its industrial capacity destroyed, reduced to a level of subsistence farming; however, that plan was soon abandoned as unrealistic and too likely, because of its punitiveness, to give rise to another round of German anger and aggressiveness. As time went on, another consideration that moderated the denazification effort in the West was the concern to keep enough good will of the German population to prevent the growth of communism.
The denazification process was often completely disregarded by both the Soviets and the Western powers for German rocket scientists and other technical experts, who were taken out of Germany to work on projects in the victor's own country or simply seized in order to prevent the other side from taking them. The U.S. sent 785 scientists and engineers from Germany to America, some of whom formed the backbone of the U.S. space program.
In the case of the top-ranking Nazis, such as Göring, Hess, von Ribbentrop, Streicher, and Speer, the initial plan was to simply arrest them and shoot them, but that course of action was replaced by putting them on trial for war crimes at the Nuremberg Trials in order to publicize their crimes while demonstrating that the trials and the sentences were just, especially to the German people. However, the legal foundations of the trials were sometimes questioned, and the German people were not entirely convinced that the trials were anything more than "victors' justice".
Many refugees from Nazism were Germans and Austrians, and some had fought for Britain in the Second World War. Some were transferred into the Intelligence Corps and sent back to Germany and Austria in British uniform. However, German-speakers were small in number in the British zone, which was hampered by the language deficit. The Americans were able to bring a larger number of German-speakers to the task of working in the Allied Military Government, although many were poorly trained. They were assigned to all aspects of military administration, the interrogation of POWs, collecting evidence for the War Crimes Investigation Unit and the search for war criminals.
Application.
American zone.
The Joint Chiefs of Staff Directive 1067 directed US Army General Dwight D. Eisenhower's policy of denazification. A report of the Institute on Re-education of the Axis Countries in June 1945 recommended: "Only an inflexible long-term occupation authority will be able to lead the Germans to a fundamental revision of their recent political philosophy." The United States military pursued denazification in a zealous, albeit bureaucratic, fashion, especially during the first months of the occupation. It had been agreed among the Allies that denazification would begin by requiring Germans to fill out a questionnaire (German: "Fragebogen") about their activities and memberships during the Third Reich. Five categories were established: "Major Offenders", "Offenders", "Lesser Offenders", "Followers", and "Exonerated Persons". The Americans, unlike the British, French, and Soviets, interpreted this to apply to every German over the age of eighteen in their zone. Eisenhower initially estimated that the denazification process would take 50 years.
When the nearly complete list of Nazi Party memberships was turned over to the Allies (by a German anti-Nazi who had rescued them from destruction in April 1945 as American troops advanced on Munich), it became possible to verify claims about participation or non-participation in the Party. The 1.5 million Germans who had joined before Hitler came to power were deemed to be hard-core Nazis.
Progress was slowed by the overwhelming numbers of Germans to be processed, but also by difficulties such as incompatible power systems and power outages, with the Hollerith IBM data machine that held the American vetting list in Paris. As many as 40,000 forms could arrive in a single day to await processing. By December 1945, even though a full 500,000 forms had been processed, there remained a backlog of 4,000,000 forms from POWs and a potential case load of 7,000,000. The "Fragebogen" were, of course, filled out in German. The number of Americans working on denazification was inadequate to handle the workload, partly as a result of the demand in the U.S. by families to have soldiers returned home. Replacements were mostly unskilled and poorly trained. In addition, there was too much work to be done to complete the process of denazification by 1947, the year American troops were expected to be completely withdrawn from Europe.
Pressure also came from the need to find Germans to run their own country. In January 1946 a directive came from the Control Council entitled "Removal from Office and from Positions of Responsibility of Nazis and Persons Hostile to Allied Purposes." One of the punishments for Nazi involvement was to be barred from public office and/or restricted to manual labour or "simple work". At the end of 1945 3.5 million former Nazis awaited classification, many of them barred from work in the meantime. By the end of the winter of 1945–6 42% of public officials had been dismissed. Malnutrition was widespread, and the economy needed leaders and workers to help clear away debris, rebuild infrastructure, and get foreign exchange to buy food and other essential resources.
Another concern leading to the Americans relinquishing responsibility for denazification and handing it over to the Germans arose from the fact that many of the American denazifiers were German Jews, former refugees returning to administer justice against the tormentors and killers of their relatives. It was felt, both among Germans and top American officials, that their objectivity might be contaminated by a desire for revenge.
As a result of these various pressures, and following a 15 January 1946 a report of the Military Government decrying the efficiency of denazification, saying, "The present procedure fails in practice to reach a substantial number of persons who supported or assisted the Nazis," it was decided to involve Germans in the process. In March 1946 The Law for Liberation from National Socialism and Militarism (German: "Befreiungsgesetz") came into effect, turning over responsibility for denazification to the Germans. Each zone had a Minister of Denazification. On 1 April 1946, a special law established 545 civilian tribunals under German administration (German: "Spruchkammern"), with a staff of 22,000 of mostly lay judges, enough, perhaps, to start to work but too many for all the staff themselves to be thoroughly investigated and cleared. They had a case load of 900,000. Several new regulations came into effect in the setting up of the German-run tribunals, including the idea that the aim of denazification was now rehabilitation rather than merely punishment, and that someone whose guilt might meet the formal criteria could also have their specific actions taken into consideration for mitigation. Efficiency thus improved, while rigor declined.
Many people had to fill in a new background form, called a "Meldebogen" (replacing the widely disliked "Fragebogen"), and were given over to justice under a "Spruchkammer", which assigned them to one of five categories.
Again because the caseload was impossibly large, the German tribunals began to look for ways to speed up the process. Unless their crimes were serious, members of the Nazi Party born after 1919 were exempted on the grounds that they had been brainwashed. Disabled veterans were also exempted. To avoid the necessity of a slow trial in open court, which was required for those belonging to the most serious categories, more than 90% of cases were judged not to belong to the serious categories and therefore were dealt with more quickly. More "efficiencies" followed. The tribunals accepted statements from other people regarding the accused's involvement in National Socialism. These statements earned the nickname of "Persilscheine", after advertisements for the laundry and whitening detergent Persil. There was corruption in the system, with Nazis buying and selling denazification certificates on the black market. Nazis who were found guilty were often punished with fines assessed in deutsche marks, which had become nearly worthless. In Bavaria the Denazification Minister, Anton Pfeiffer, bridled under the "victor's justice", and presided over a system that reinstated 75% of officials the Americans had dismissed and reclassified 60% of senior Nazis. The denazification process lost a great deal of credibility, and there was often local hostility against Germans who helped administer the tribunals.
By early 1947, the Allies held 90,000 Nazis in detention; another 1,900,000 were forbidden to work as anything but manual labourers.
By 1948, the Cold War was clearly in progress and the US began to worry more about a threat from the Eastern Bloc rather than the latent Nazism within occupied Germany. The remaining cases were tried through summary proceedings that left insufficient time to thoroughly investigate the accused, so that many of the judgments of this period have questionable judicial value. For example, by 1952 members of the SS like Otto Skorzeny could be declared formally denazified (German: "entnazifiziert") "in absentia" by a German government arbitration board and without any proof that this was true.
The delicate task of distinguishing those truly complicit in or responsible for Nazi activities from mere "followers" made the work of the courts yet more difficult. US President Harry S. Truman alluded to this problem: "though all Germans might not be guilty for the war, it would be too difficult to try to single out for better treatment those who had nothing to do with the Nazi regime and its crimes." Denazification was from then on supervised by special German ministers, like the Social Democrat Gottlob Kamm in Baden-Württemberg, with the support of the US occupation forces.
Contemporary American critics of denazification denounced it as a "counterproductive witch hunt" and a failure; in 1951 the provisional West German government granted amnesties to lesser offenders and ended the program.
Censorship.
While judicial efforts were handed over to German authorities, the US Army continued its efforts to denazify Germany through control of German media. The Information Control Division of the US Army had by July 1946 taken control of 37 German newspapers, six radio stations, 314 theaters, 642 cinemas, 101 magazines, 237 book publishers, and 7,384 book dealers and printers. Its main mission was democratization but part of the agenda was also the prohibition of any criticism of the Allied occupation forces. In addition, on May 13, 1946 the Allied Control Council issued a directive for the confiscation of all media that could contribute to Nazism or militarism. As a consequence a list was drawn up of over 30,000 book titles, ranging from school textbooks to poetry, which were then banned. All copies of books on the list were confiscated and destroyed; the possession of a book on the list was made a punishable offense. All the millions of copies of these books were to be confiscated and destroyed. The representative of the Military Directorate admitted that the order was in principle no different from the Nazi book burnings.
The censorship in the U.S. zone was regulated by the occupation directive JCS 1067 (valid until July 1947) and in the May 1946 order valid for all zones (rescinded in 1950), Allied Control Authority Order No. 4, "No. 4 – Confiscation of Literature and Material of a Nazi and Militarist Nature". All confiscated literature was reduced to pulp instead of burning. It was also directed by Directive No. 30, "Liquidation of German Military and Nazi Memorials and Museums." An exception was made for tombstones "erected at the places where members of regular formations died on the field of battle."
Artworks were under the same censorship as other media;
The directives were very broadly interpreted, leading to the destruction of thousands of paintings and thousands more were shipped to deposits in the U.S. Those confiscated paintings still surviving in U.S. custody include for example a painting "depicting a couple of middle aged women talking in a sunlit street in a small town". Artists were also restricted in which new art they were allowed to create; "OMGUS was setting explicit political limits on art and representation".
The publication "Der Ruf" ("The Call") was a popular literary magazine first published in 1945 by Alfred Andersch and edited by Hans Werner Richter. "Der Ruf", also called "Independent Pages of the New Generation", claimed to have the aim of educating the German people about democracy. In 1947 its publication was blocked by the American forces for being overly critical of occupational government. Richter attempted to print many of the controversial pieces in a volume entitled "Der Skorpion" ("The Scorpion"). The occupational government blocked publication of "Der Skorpion" before it began, saying that the volume was too "nihilistic".
Publication of "Der Ruf" resumed in 1948 under a new publisher, but "Der Skorpion" was blocked and not widely distributed. Unable to publish his works, Richter founded Group 47.
The Allied costs for occupation were charged to the German people. A newspaper which revealed the charges (including, among other things, thirty thousand bras) was banned by the occupation authorities for revealing this information.
Soviet zone.
From the beginning, denazification in the Soviet zone took on the political tone of class warfare. As they moved into Prussia, amid the invasion, the Soviets expulsed, arrested, or put in internment camps the Junkers and other large landowners, not only for their reputation of being supporters of militarism and Nazism but also in order to seize their lands and redistribute it to small farmers. Many industries were expropriated, with entire factories carted off to Russia, or nationalized.
In July 1945, the Soviets were the first of the Allies to install state ("Länder") governments and the first to allow political parties. These were later either disbanded, or absorbed into the Communist Party, which was then renamed the Socialist Unity Party.
The Soviet secret service, NKVD, set up a number of infamous "special camps" where – among others – alleged Nazis were interned. However, people were sometimes arrested completely arbitrarily and did not receive a fair trial, with some not even receiving any trial at all. At least 43,000 died in the camps. Doing special tasks for the Soviet government could protect Nazi members from prosecution, enabling them to continue working.
 Having special connections with the occupiers in order to have someone vouch for you could also shield you from the denazification laws.
The abandonment of stringent denazification in the West became a major theme of East German government propaganda. Despite the presence of many former Nazis at all levels in the government of East Germany, the Party often claimed that the West German government was nothing but an extension of the old Nazi regime. Such allegations appeared frequently in the official Socialist Unity Party of Germany newspaper, the "Neues Deutschland". The 1953 June 17 riots in Berlin were officially blamed on Nazi "agents provocateurs" from West Berlin, who the "Neues Deutschland" alleged were then working in collaboration with the Western government with the ultimate aim of restoring Nazi rule throughout Germany.
The Berlin Wall was officially called the Anti-Fascist Security Wall (German: "Antifaschistischer Schutzwall") by the East German government, and was ostensibly built to protect East German society from the activities of Nazis in West Berlin.
British zone.
The British prepared a plan from 1942 onwards, assigning a number of quite junior civil servants to head the administration of liberated territory in the rear of the Armies, with draconian powers to remove from their post, in both public and private domains, anyone suspected, usually on behavioural grounds, of harbouring Nazi sympathies. For the British government, the rebuilding of German economic power was more important than the imprisonment of Nazi criminals. Economically hard pressed at home after the war, they did not want the burden of feeding and otherwise administering Germany.
In October 1945, in order to constitute a working legal system, and given that 90% of German lawyers had been members of the Nazi Party, the British decided that 50% of the German Legal Civil Service could be staffed by "nominal" Nazis. Similar pressures caused them to relax the restriction even further in April 1946. In industry, especially in the economically crucial Ruhr area, the British began by being lenient about who owned or operated businesses, turning stricter by autumn of 1945. In order to reduce the power of industrialists, the British expanded the role of trade unions, giving them some decision-making powers.
They were, however, especially zealous during the early months of occupation in bringing to justice anyone, soldiers or civilians, who committed war crimes against POWs or captured Allied aircrew. In June 1945 an 
interrogation centre at Bad Nenndorf was opened, where ex-Nazis and suspected communist agents were tortured with beatings, whippings, thumb-screws, cold, starvation, etc.. A public scandal ensued but only one person was found guilty of neglect.
The British to some extent avoided being overwhelmed by the potential numbers of denazification investigations by requiring that no one need fill out the "Fragebogen" unless they were applying for an official or responsible position. This difference between American and British policy was decried by the Americans and caused some Nazis to seek shelter in the British zone.
In January 1946, the British handed over their denazification panels to the Germans.
French zone.
The French were less vigorous, for a number of reasons, than the Americans, not even using the term "denazification," instead calling it "épuration" (purification). They did not view it as critical to distinguish Nazis from non-Nazis, since in their eyes all Germans were to blame. At the same time, some French occupational commanders had served in the collaborationist Vichy regime during the war where they had formed friendly relationships with Germans. As a result, in the French zone mere membership in the Nazi party was much less important than in the other zones.
Because teachers had been strongly Nazified, the French began by removing three-quarters of all teachers from their jobs. However, finding that the schools could not be run without them, they were soon rehired, although subject to easy dismissal. A similar process governed technical experts. The French were the first to turn over the vetting process to Germans, while maintaining, of course, French power to reverse any German decision. Overall, the business of denazification in the French zone was considered a "golden mean between an excessive degree of severity and an inadequate standard of leniency," laying the groundwork for an enduring reconciliation between France and Germany. In the French zone only thirteen Germans were categorized as "major offenders."
Brown book.
In 1965, the National Front of the German Democratic Republic published what became known as the "Brown Book: War and Nazi Criminals in West Germany: State, Economy, Administration, Army, Justice, Science". As the title would indicate, the presence of former Gestapo members in the "Volkspolizei" and ex-Nazis at all levels of the Socialist Unity Party was not covered. The book, among other things, mentioned 1,800 names of former Nazis who held positions of authority in West Germany. These included 15 ministers and deputy ministers, 100 generals and admirals of the armed forces, 828 senior judges and prosecutors, 245 leading members of the Foreign Ministry, embassies and consulates officials, and 297 senior police officers and Federal Office for the Protection of the Constitution officials. The listing was inaccurate; many of the military names had not been Party members, as the armed forces did not permit its officers to join, while many low level Party members in other groups were overlooked altogether. As revealed by BKA official Dieter Senk in 1989, "today we know that [the] Brown Book didn't contain even approximately all the relevant names [...] For example it mentions only 3 names from the BKA [...]" The book had a controversial impact in West Germany. Reflecting this, a judge ordered the seizure of the volume from the Frankfurt Book Fair in 1967.
Implications.
For future German states.
The culture of denazification strongly influenced the parliamentary council charged with drawing up a constitution for those occupation zones that would become West Germany.
This Constitution (German: "Grundgesetz", Basic Law), was completed on May 8, 1949, ratified on May 23, and came into effect the next day. This date effectively marks the foundation of the Federal Republic of Germany.
For the future of Europe.
The end of denazification saw the "ad hoc" creation initially of the Western Union (not to be confused with the commercial operation of that name) which would be institutionalised as the Western European Union in 1947 and 1955, with a broad socio-economic remit actually implemented in the strict domain of arms control.
Responsibility and collective guilt.
The ideas of collective guilt and collective punishment originated not with the US and British people, but on higher policy levels. Not until late in the war did the U.S. public assign collective responsibility to the German people. The most notable policy document containing elements of collective guilt and collective punishment is JCS 1067 from early 1945. Eventually horrific footage from the concentration camps would serve to harden public opinion and bring it more in line with that of policymakers.
Already in 1944, prominent U.S. opinion makers had initiated a domestic propaganda campaign (which was to continue until 1948) arguing for a harsh peace for Germany, with a particular aim to end the apparent habit in the U.S. of viewing the Nazis and the German people as separate entities.
Statements made by the British and U.S. governments, both before and immediately after Germany's surrender, indicate that the German nation as a whole was to be held responsible for the actions of the Nazi regime, often using the terms "collective guilt" and "collective responsibility".
To that end, as the Allies began their post-war denazification efforts, the Psychological Warfare Division (PWD) of Supreme Headquarters Allied Expeditionary Force undertook a psychological propaganda campaign for the purpose of developing a German sense of collective responsibility.
The Public Relations and Information Services Control Group of the British Element (CCG/BE) of the Allied Control Commission for Germany began in 1945 to issue directives to officers in charge of producing newspapers and radio broadcasts for the German population to emphasize "the moral responsibility of all Germans for Nazi crimes." Similarly, among U.S. authorities, such a sense of collective guilt was "considered a prerequisite to any long-term education of the German people."
Using the German press, which was under Allied control, as well as posters and pamphlets, a program was conducted to acquaint ordinary Germans with what had taken place in the concentration camps. For example using posters with images of concentration camp victims coupled to text such as "YOU ARE GUILTY OF THIS!" or "These atrocities: Your Guilt!!"
A number of films showing the concentration camps were made and screened to the German public, such as "Die Todesmühlen", released in the U.S. zone in January 1946, and "Welt im Film No. 5" in June 1945. A film that was never finished due partly to delays and the existence of the other films was "Memory of the Camps". According to Sidney Bernstein, chief of PWD, the object of the film was to:
... shake and humiliate the Germans and prove to them beyond any possible challenge that these German crimes against humanity were committed and that the German people – and not just the Nazis and SS – bore responsibility.
English writer James Stern recounted an example in a German town soon after the German surrender.
[a] crowd is gathered around a series of photographs which though initially seeming to depict garbage instead reveal dead human bodies. Each photograph has a heading 'WHO IS GUILTY?'. The spectators are silent, appearing hypnotised and eventually retreat one by one. The placards are later replaced with clearer photographs and placards proclaiming 'THIS TOWN IS GUILTY! YOU ARE GUILTY!'
Immediately upon the liberation of the concentration camps, many German civilians were forced to see the conditions in the camps, bury rotting corpses and exhume mass graves. In some instances, civilians were also made to provide items for former concentration camp inmates.
Surveys.
The U.S. conducted opinion surveys in occupied Germany . Tony Judt in his book "Postwar: a History of Europe since 1945" extracted and used some of them.
However, in "Hitler, Germans, and the 'Jewish Question"', Sarah Ann Gordon notes the difficulty of drawing conclusions from the surveys. For example, respondents were given three alternatives from which to choose, as in question 1:
To the question of whether an Aryan who marries a Jew should be condemned, 91% responded "No". To the question of whether "All those who ordered the murder of civilians or participated in the murdering should be made to stand trial," 94% responded "Yes".
Gordon singles out the question "Extermination of the Jews and Poles and other non-Aryans was not necessary for the security of the Germans", which included an implicit double negative to which the response was either yes or no. She concludes that this question was confusingly phrased (given that in the German language the affirmative answer to a question containing a negative statement is "no"): Some interviewees may have responded "no" they did not agree with the statement, when they actually did agree that the extermination was not necessary. She further highlights the discrepancy between the antisemitic implications of the survey results (such as those later identified by Judt) with the 77% percent of interviewees who responded that actions against Jews were in no way justified.
Gordon states that if the 77 percent result is to be believed then an "overwhelming majority" of Germans disapproved of extermination, and if the 37 percent result is believed to be correct then over one third of Germans were willing to exterminate Poles and Jews and others for German security. She concludes that the phrasing of the question on German security lowers the confidence in the later interpretation.
Gordon follows this with another survey where interviewees were asked if Nazism was good or bad (53% chose bad) and reasons for their answer. Among the nine possible choices on why it was bad, 21% chose the effects on the German people before the war, while 3–4 percent chose the answer "race policy, atrocities, pogroms" However, Gordon highlights the issue that it is difficult to pin-down at which point in time respondents became aware of the exterminations, before or after they were interviewed: questionnaire reports indicate that a significant minority had no knowledge until the Nuremberg trials.
She also notes that when confronted with the exterminations there was an element of denial, disbelief, and confusion. Asked about concentration camps, very few Germans associated them with the Jews, leading to the conclusion that they did not understand how they had been used against the Jews during the war and instead continued to think of them as they were before the war, the place where political opponents to the Nazis were kept. "This naivete is only understandable if large numbers of Germans were truly ignorant of the existence of these camps". A British study on the same attitudes concluded thatThose who said National Socialism was a good idea pointed to social welfare plans, the lack of unemployment, the great construction plans of the Nazis ... Nearly all those who thought it a good idea nevertheless rejected Nazi racial theories and disagreed with the inhumanity of the concentration camps and the 'SS'.
Sarah Gordon writes that a majority of Germans appeared to approve of nonviolent removal of Jews from civil service and professions and German life. The German public also accepted the Nuremberg laws because they thought they would act as stabilizers and end violence against Jews. The German public had as a result of the Nazi antisemitic propaganda hardened their attitudes between 1935 and 1938 from the originally favorable stance. By 1938, the propaganda had taken effect and antisemitic policies were accepted, provided no violence was involved. Kristallnacht caused German opposition to antisemitism to peak, with the vast majority of Germans rejecting the violence and destruction, and many Germans aiding the Jews.
The Nazis responded by intimidation in order to discourage opposition, those aiding Jews being victims of large-scale arrests and intimidation. With the start of the war the anti-Semitic minority that approved of restrictions on Jewish domestic activities was growing, but there is no evidence that the general public had any acceptance for labor camps or extermination. As the number of antisemites grew, so too did the number of Germans opposed to racial persecution, and rumors of deportations and shootings in the east led to snowballing criticism of the Nazis. Gordon states that "one can probably conclude that labor camps, concentration camps, and extermination were opposed by a majority of Germans."
Gordon concludes in her analysis on German public opinion based German SD-reports during the war and the Allied questionnaires during the occupation: it would appear that a majority of Germans supported elimination of Jews from the civil service; quotas on Jews in professions, academic institutions, and commercial fields; restrictions on intermarriage; and voluntary emigration of Jews. However, the rabid antisemites' demands for violent boycotts, illegal expropriation, destruction of Jewish property, pogroms, deportation, and extermination were probably rejected by a majority of Germans. They apparently wanted to restrict Jewish rights substantially, but not to annihilate Jews.
End.
The West German political system, as it emerged from the occupation, was increasingly opposed to the Allied denazification policy. As denazification was deemed ineffective and counterproductive by the Americans, they did not oppose the plans of the German chancellor Konrad Adenauer to end the denazification efforts. Adenauer's intention was to switch government policy to reparations and compensation for the victims of NS rule ("Wiedergutmachung"), stating that the main culprits had been persecuted. In 1951 several laws were passed, ending the denazification. Officials were allowed to retake jobs in the civil service, with the exception of people assigned to Group I (Major Offenders) and II (Offenders) during the denazification review process.
Several amnesty laws were also passed which affected about 792,176 people. Those pardoned included people with six-month sentences, 35,000 people with sentences of up to one year and include more than 3,000 functionaries of the SA, the SS, and the Nazi Party who participated in dragging victims to jails and camps; 20,000 other Nazis sentenced for "deeds against life" (presumably murder); 30,000 sentenced for causing bodily injury, and 5,200 who committed "crimes and misdemeanors in office." As a result, several people with a former NS past ended up again in the political apparatus of Western Germany.
Criticism by the Red Army Faction.
Because the Cold War had curtailed the process of denazification in the West, certain radical leftist groups such as the Red Army Faction tried to justify their use of violence against the West German government based on the notion that the West German establishment had benefited from the Nazi period, and that, while having officially renounced the Holocaust and Nazi war crimes, it was still supposedly fascist in outlook in all other aspects. They pointed out that many former Nazis held government posts, while the German Communist Party was illegal. They argued that "What did you do in the war, daddy?" was not a question that many of the leaders of the generation who fought World War II and prospered in the postwar "Wirtschaftswunder" (German Economic Miracle) encouraged their children to ask (the presence of many ex-Nazis in the army, police and government of East Germany was not a part of the argument).
One of the major justifications that the Red Army Faction gave in 1977 for murdering Hanns-Martin Schleyer, President of the Confederation of German Employers' Associations (BDA) and perceived as one of the most powerful industrialists in West Germany, was that as a former member of the SS he was part of an informal network of ex-Nazis who still had great economic power and political influence in West Germany.
Hiding one's Nazi past.
Even today, membership in Nazi organizations is still not an open topic of discussion among most Germans. It was not until 2006 that famous German writer Günter Grass, often viewed as a spokesman of 'the nation's moral conscience', spoke publicly about the fact that he had been a member of the Waffen SS (even though his involvement appears to have been less than criminal; he was conscripted into the Waffen SS while barely seventeen years old and his duties were strictly military in nature). Joseph Ratzinger (later Pope Benedict XVI), on the other hand, has been open about his membership at the age of fourteen in Hitler Youth, when his church youth group was forced to merge with them. Statistically it is likely that there are many more Germans of Grass's generation (also called the "Flakhelfer-Generation") with biographies similar to his.
In other countries.
In practice, denazification was not limited to Germany and Austria; in every European country with a vigorous Nazi or Fascist party measures of denazification were carried out. In France the process was called épuration légale (English: legal cleansing). Prisoners of war held in detention in Allied countries were also subject to denazification qualifications before their repatriation.
Denazification was also practised in many countries which came under German occupation, including Belgium, Norway, Greece and Yugoslavia, because satellite regimes had been established in these countries with the support of local collaborators.
In Greece, for instance, Special Courts of Collaborators were created after 1945 to try former collaborators. The three Greek 'quisling' prime ministers were convicted and sentenced to death or life imprisonment. Other Greek collaborators after German withdrawal underwent repression and public humiliation, besides being tried (mostly on treason charges). In the context of the emerging Greek Civil War however, most wartime figures from the civil service, the Greek Gendarmerie and the notorious Security Battalions were quickly integrated into the strongly anti-Communist postwar establishment.
Further reading.
</dl>

</doc>
<doc id="48764" url="http://en.wikipedia.org/wiki?curid=48764" title="Tocantins">
Tocantins

Tocantins (]) is one of the states of Brazil. (From: Tukã´, "Toucan" + tï, "beak". lit. "Toucan's beak" in Tupi language). It is the newest Brazilian state, formed in 1988 and encompassing what had formerly been the northern two-fifths of the state of Goiás. Construction of its capital, Palmas, began in 1989; most of the other cities in the state date to the Portuguese colonial period. Except for Araguaia, they are minor. The government has invested in a new capital, a major hydropower dam, railroads and related infrastructure to develop this primarily agricultural area.
Tocantins has attracted hundreds of thousands of new residents, primarily to Palmas. It is building on its hydropower resources. The Araguaia and Tocantins rivers drain the largest watershed that lies entirely inside Brazilian territory. The Rio Tocantins has been dammed for hydropower, creating a large reservoir that has become a center of recreation. Because it is in the central zone of the country, Tocantins has characteristics of the Amazon Basin, and also semi-open pastures, known as "cerrado". The Ilha do Bananal, in the southwest of the State, is the largest fluvial island in the world. Tocantins is also home to the Araguaia National Park, the Carajás Indian reservations, and Jalapão state park, which is about 250 km from Palmas. There, the rivers create oases in the dry landscape, attracting many ecotourists to the region.
Geography.
Tocantins geography is varied. It straddles both the Amazon Rainforest and the coastal savanna. Many rivers (including the Tocantins River) traverse the state. Researchers have identified more than 20 archaeologically significant sites related to indigenous cultures.
Tocantins is bordered to the northeast by the states of Maranhão and Piauí, Bahia to the east, Goiás to the south, Mato Grosso to the west, and Pará to the northwest. Tocantins was created from the northern two-fifths of Goiás state in 1989.
Climate.
Most of Tocantins (except the extreme western and northern regions) is situated within a vast Brazilian area known as the cerrado. The cerrado region's typical climate is hot and semi-humid, with pronounced seasonal variation marked by a dry winter from May through October. The annual rainfall is around 800 to 1600 mm. The soils are generally very old, deep, and naturally nutrient-poor.
Vegetation.
The "cerrado" landscape is characterized by extensive savanna formations crossed by gallery forests and stream valleys. Cerrado includes various types of vegetation. Humid fields and "buriti" palm paths are found where the water table is near the surface. Alpine pastures occur at higher altitudes and mesophytic forests on more fertile soils.
The savanna formations are not homogenous. There is great variation between the amount of woody and herbaceous vegetation, forming a gradient from completely open "cerrado" — open fields dominated by grasses — to the closed, forest-like "cerrado" and the "cerradão" ("big cerrado"), a closed canopy forest. Intermediate forms include the dirty field, the "cerrado" field, and the "cerrado" sensu stricto, according to a growing density of trees.
The "cerrado" trees have characteristic twisted trunks covered by a thick bark, and leaves that are usually broad and rigid. Many herbaceous plants have extensive roots to store water and nutrients. The plant's thick bark and roots serve as adaptations for the periodic fires which sweep the cerrado landscape. The adaptations protect the plants from destruction and make them capable of sprouting again after the fire.
As in many savannas in the world, the "cerrado" ecosystems have been coexisting with fire since ancient times. Initially they developed adaptations to natural fires caused by lightning or volcanic activity, and later to those caused by man.
Along the western boundary of the state is the floodplain of the Araguaia River, which includes extensive wetlands and Amazon tropical forest ecosystems. Bananal Island, formed by two branches of the Araguaia, is said to be the largest river island in the world. It consists mostly of marshlands and seasonally flooded savannas, with gallery forest. Where the two branches meet again they form an inland delta called Cantão, a typical Amazonian igapó flooded forest. The Araguaia is also one of the main links between the Amazonian lowlands and the Pantanal wetlands to the south, but the river is not fully navigable.
History.
Portuguese Jesuit missionaries explored what is today Tocantins state about 1625, seeking to convert the Amerindian peoples of the area to Christianity. The area is named after the Tocantins River, whose name is derived from an indigenous language. (From: Tukã´, "Toucan" + tï, "beak". lit. "Toucan's beak" in Tupi language.)
Before 1988 the area made up the northern two-fifths or one-third of Goiás state. Since the 17th century, this area was relatively isolated by rivers navigable only in shorrt portions and mountains, and difficult to access. As a result, the southern area of the state became more developed, particularly after this area was selected in 1956 as the site for the development of the new capital of Brasília and the Federal District. A strong separatist movement developed in the north for independence of its people. 
After the government levied heavy taxes on mining in 1809, local residents began to organize a separatists movement. They made a minor revolt which was quickly crushed by the army. In the 19th century, a string of failed uprisings occurred in the north. Historically the area was inhabited chiefly by Amerindians in some intact indigenous tribes and pardos of Amerindian and Portuguese descent.
In the 1970s, the population of northern Goiás lobbied the government to establish a separate state. In the 1988 Constitution, the State of Tocantins was officially created and admitted as a new Brazilian state.
Since its establishment and investment by the government, as in the new capital of Palmas, Tocantins has been the fastest-growing Brazilian state. Its thriving economy is based on agriculture and agro-industry, attracting thousands of migrants from all over the country. The construction of the long-planned North-South Railway (Brazil) will probably boost economic growth even more. 
Demographics.
According to the IBGE of 2007, there were 1,377,000 people residing in the state. The population density was 4.8 inh./km².
Urbanization: 71.5% (2004); Population growth: 2.6% (1991–2000); Houses: 355,502 (2005).
The last PNAD (National Survey of Households) census revealed the following numbers: 948,000 Pardos (brown, Multiracial) people (68.9%), 330,000 White people (24.0%), 95,000 Black people (6.9%), 2,000 Asian or Amerindian people (0.2%).
Economy.
The service sector is the largest component of GDP at 59.9%, followed by the industrial sector at 27.2%. Agriculture represents 12.9% of GDP (2004). Tocantins exports: soybean 89.2%, beef 10.5% (2002).
Share of the Brazilian economy: 0.4% (2005).
As with much of Brazil, Tocantins' economy is dependent on cattle ranching. The state's pineapple plantations supply much of Brazil with the fruit, as well as many other Mercosul nations. In the state's north, charcoal and oils are extracted from the babaçu palm tree.
Seeking to broaden Tocantins' economic base by funding the construction of a hydroelectric dam in the state, the government allowed a private company to construct a sizable five-turbine hydroelectric dam, blocking the Tocantins River to create a reservoir. This construction displaced some indigenous inhabitants. The dam's economic contribution to the state is large: one turbine provides enough power for the entire state of Tocantins, and the remaining four provide electricity that is sold to other parts of Brazil.
Education.
Portuguese is the official national language, and thus the primary language taught in schools. But English and Spanish are part of the official high school curriculum.
Infrastructure.
Palmas Airport.
The facility occupies one of Brazil’s largest airport sites and has privileged location near the Lajeado Hydroelectric Station.
Designed with a modern concept of visual communication, the new Palmas Airport Complex contains an Aeroshopping area. This is part of a program developed by Infraero, to develop Brazil’s main airports as commercial centers with their own brand and identity.
The passenger terminal has 12.300 square meters of constructed area and capacity to serve up to 370 thousand people a year. It has a food court, cultural space, shops, panoramic deck, elevators, and air conditioning. The runway can receive aircraft the size of a Boeing 767. Three taxiways and aprons are reserved for general aviation, making operations more flexible. The airport's full infrastructure includes a control tower and installations for the Air Navigation Group, fire brigade, a covered equipment parking area, canteen and training rooms, two aircraft fueling stations, a gate with electronic entry control, guard booths, parking and flight protection buildings, besides a 4 km (2.48 mi) access road linking the airport to the Tocantins capital city’s main thoroughfare.
Protected areas.
Araguaia National Park is located on Bananal Island. It borders Cantão State Park, and together, these strictly protected areas form the core of the Araguaia Mosaic of Protected Areas, which consists of over four million hectares of state and federal protected areas and Indian lands along the Araguaia wetlands. The mosaic also extends into the neighboring states of Pará and Mato Grosso.
Nascentes do Rio Parnaiba National Park is located on the opposite corner of the state, in the transition zone between the Cerrado and the semi-arid Caatinga. It also extends into the neighboring states of Maranhão and Piauí.
In addition, the State of Tocantins has established state parks at Jalapão and Serra do Lajeado, protecting two unique samples of the Cerrado. The state parks and protected areas of Tocantins are managed by Naturatins, the state environmental agency.
Flag.
The message of the flag is the phrase "where the sun rises for all". In the middle of the flag is the golden yellow sun, with its rays symbolically targeting to the future of the state. The sun is placed on a white band, where the white color represents peace. The blue in the upper left and the yellow in the bottom right represent the waters and the soil of the state. The colors date back to a flag used by the Autonomous Government of Palmas in the 19th century.
The flag was adopted with the state flag law (law no 094/89) of November 17, 1989.
Represented in popular culture.
"Survivor:" Tocantins — The Brazilian Highlands was the setting for the eighteenth season of the United States reality show "Survivor," filmed in the microregion of Jalapão in Tocantins. The premiere aired February 12, 2009.

</doc>
<doc id="48768" url="http://en.wikipedia.org/wiki?curid=48768" title="Georgia (country)">
Georgia (country)

Georgia (Georgian: საქართველო "Sakartvelo", ]) is a country in the Caucasus region of Eurasia. Located at the crossroads of Western Asia and Eastern Europe, it is bounded to the west by the Black Sea, to the north by Russia, to the south by Turkey and Armenia, and to the southeast by Azerbaijan. The capital and largest city is Tbilisi. Georgia covers a territory of 69700 km², and its population is almost 5 million. Georgia is a unitary, semi-presidential republic, with the government elected through a representative democracy.
During the classical era, several independent kingdoms became established in what is now Georgia. The kingdoms of Colchis and Iberia adopted Christianity in the early 4th century. A unified Kingdom of Georgia reached the peak of its political and economic strength during the reign of King David IV and Queen Tamar in the 11th–12th centuries. Thereafter the area was dominated by various large empires, including the Safavids, Afsharids, and Qajar Persians. In the late 18th century, the kingdom of Kartli-Kakheti forged an alliance with the Russian Empire, and the area was annexed by Russia in 1801. After a brief period of independence following the Russian Revolution of 1917, Georgia was occupied by Soviet Russia in 1921, becoming part of the Soviet Union as the Georgian Soviet Socialist Republic. After independence in 1991, post-communist Georgia suffered from civil unrest and economic crisis for most of the 1990s. This lasted until the Rose Revolution of 2003, after which the new government introduced democratic and economic reforms.
Georgia is a member of the Council of Europe and the GUAM Organization for Democracy and Economic Development. It contains two "de facto" independent regions, Abkhazia and South Ossetia, which gained limited international recognition after the 2008 Russo-Georgian War. Georgia and a major part of the international community consider the regions to be part of Georgia's sovereign territory under Russian military occupation.
Etymology.
The full, official name of the country is "Georgia", as specified in the Georgian constitution. "Georgia" is an exonym, used in the West since the medieval period.
The name was etymologized for the west in honor of Saint George, explicitly so by the end of the 12th century by Jacques de Vitry, due to the Georgians' special reverence for that saint (see Tetri Giorgi). Early modern authors such as Jean Chardin tried to link the name to the literal meaning of the Greek word "γεωργός, geōrgía" ("tiller of the earth; agriculturalist").
The self-designation used by ethnic Georgians is "Kartvelebi" (ქართველები, i.e. "Kartvelians"); the native name of Georgia "Sakartvelo" (საქართველო; "land of Kartvelians"), and the name of the Georgian language "Kartuli" (ქართული). The medieval Georgian Chronicles present an eponymous ancestor of the Kartvelians, Kartlos, a great-grandson of Japheth. The name "Sakartvelo" (საქართველო) consists of two parts. Its root, "kartvel-i" (ქართველ-ი), specifies an inhabitant of the core central-eastern Georgian region of Kartli, or Iberia as it is known in sources of the Eastern Roman Empire. Ancient Greeks (Strabo, Herodotus, Plutarch, Homer, etc.) and Romans (Titus Livius, Tacitus, etc.) referred to early western Georgians as Colchians and eastern Georgians as Iberians ("Iberoi" in some Greek sources).
History.
Prehistory.
The territory of modern-day Georgia was inhabited by "Homo erectus" since the Paleolithic Era. The proto-Georgian tribes first appear in written history in the 12th century BC.
Archaeological finds and references in ancient sources reveal elements of early political and state formations characterized by advanced metallurgy and goldsmith techniques that date back to the 7th century BC and beyond.
Antiquity.
The classical period saw the rise of the early Georgian states Diauehi (13th century BC), Colchis (8th century BC), Sper (7th century BC) and Iberia (6th century BC).
In the 4th century BC, a unified kingdom of Georgia – an early example of advanced state organization under one king and an aristocratic hierarchy – was established. Sargon II (722–705 BC) of the Assyrian empire conquered the Georgian state of Tabal and all of the Hittite kingdoms of the Taurus Mountains.
In Greek mythology, Colchis was the location of the Golden Fleece sought by Jason and the Argonauts in Apollonius Rhodius' epic tale "Argonautica". The incorporation of the Golden Fleece into the myth may have derived from the local practice of using fleeces to sift gold dust from rivers. Known to its natives as Egrisi or Lazica, Colchis was also the battlefield of the Lazic War fought between the Byzantine Empire and Sassanid Persia.
After the Roman Empire completed its conquest of the Caucasus region in 66 BC, the Georgian kingdoms were Roman client states and allies for nearly 400 years. In 337 AD King Mirian III declared Christianity as the state religion, giving a great stimulus to the development of literature, arts, and ultimately playing a key role in the formation of the unified Georgian nation. King Mirian III's acceptance of Christianity effectively tied the kingdom to the neighboring Eastern Roman Empire, which exerted a strong influence on Georgia for nearly a millennium, determining much of its present cultural identity.
Middle Ages.
The early kingdoms disintegrated into various feudal regions by the early Middle Ages. This made it easy for Arabs to conquer most of eastern Georgia in the 7th century. From the 7th century to the 10th century, Georgia was part of the Khazar empire.
The various independent regions would not be united into a single Kingdom of Georgia until the beginning of the 11th century.
Although Arabs captured the capital city of Tbilisi in 645 AD, Kartli-Iberia retained considerable independence under local Arab rulers. The prince Ashot I (r. 813–830) – also known as Ashot Kurapalat – became the first of the Bagrationi family to rule the kingdom. Ashot's reign began a period of nearly 1,000 years during which the Bagrationi, as the noble house was known, ruled at least part of what is now the republic. Bagrat III (r. 1027–1072) united western and eastern Georgia.
The Kingdom of Georgia reached its zenith in the 12th to early 13th centuries. This period during the reigns of David IV (called David the Builder, r. 1089–1125) and his granddaughter Tamar (r. 1184–1213) has been widely termed as Georgia's Golden Age or the Georgian Renaissance This early Georgian renaissance, which preceded its Western European analogue, was characterized by impressive military victories, territorial expansion, and a cultural renaissance in architecture, literature, philosophy and the sciences. The Golden age of Georgia left a legacy of great cathedrals, romantic poetry and literature, and the epic poem "The Knight in the Panther's Skin".
David the Builder initiated the Georgian Golden Age by driving the Seljuk Turks from the country, winning the major Battle of Didgori in 1121, and expanding Georgian cultural and political influence southward into Armenia and eastward to the Caspian Sea.
The 29-year reign of Tamar, the first female ruler of Georgia, is considered the most successful in Georgian history. Tamar was given the title "king of kings" ("mepe mepera"). She succeeded in neutralizing opposition and embarked on an energetic foreign policy aided by the downfall of the rival powers of the Seljuks and Byzantium. Supported by a powerful military élite, Tamar was able to build on the successes of her predecessors to consolidate an empire which dominated the Caucasus, and extended over large parts of present-day Azerbaijan, Armenia, and eastern Turkey, until its collapse under the Mongol attacks within two decades after Tamar's death in 1213.
The revival of the Kingdom of Georgia was set back after Tbilisi was captured and destroyed by the Khwarezmian leader Jalal ad-Din in 1226. The Mongols were expelled by George V of Georgia, son of Demetrius II of Georgia, who was named "Brilliant" for his role in restoring the country's previous strength and Christian culture. George V was the last great king of the unified Georgian state. After his death, different local rulers fought for their independence from central Georgian rule, until the total disintegration of the Kingdom in the 15th century. Georgia was further weakened by several disastrous invasions by Tamerlane. Invasions continued, giving the kingdom no time for restoration, with both Black and White sheep Turkomans constantly raiding its southern provinces. As a result, the Kingdom of Georgia collapsed into anarchy by 1466 and fragmented into three independent kingdoms and five semi-independent principalities. Neighboring empires exploited the internal division of the weakened country, and beginning in the 16th century, the Persian Empire and the Ottoman Empire subjugated the eastern and western regions of Georgia, respectively.
The rulers of regions that remained partly autonomous organized rebellions on various occasions. However, subsequent Persian and Ottoman invasions further weakened local kingdoms and regions. As a result of incessant wars and deportations, the population of Georgia dwindled from 5 million in the 13th century to 250,000 inhabitants at the end of the 18th century. Eastern Georgia, composed of the regions of Kartli and Kakheti, had been under Persian suzerainty since 1555. With the death of Nader Shah in 1747, both kingdoms broke free of Persian control and were reunified through a personal union under the energetic king Heraclius II in 1762.
Georgia in the Russian Empire.
In 1783, Russia and the eastern Georgian Kingdom of Kartli-Kakheti signed the Treaty of Georgievsk, which recognized the bond of Eastern Orthodoxy between the Russian and Georgian people and promised eastern Georgia protection against further Persian attacks, or by other aggressors.
However, despite this commitment to defend Georgia, Russia rendered no assistance when the Turks and Persians invaded in 1785 and in 1795, completely devastating Tbilisi and massacring its inhabitants. This period culminated in the 1801 Russian violation of the Treaty of Georgievsk and annexation of eastern Georgia, followed by the abolishment of the royal Bagrationi dynasty, as well as the autocephaly of the Georgian Orthodox Church. Pyotr Bagration, one of the descendants of the abolished house of Bagrationi, would later join the Russian army and rise to be a general by the Napoleonic wars.
On 22 December 1800, Tsar Paul I of Russia, at the alleged request of the Georgian King George XII, signed the proclamation on the incorporation of Georgia (Kartli-Kakheti) within the Russian Empire, which was finalized by a decree on 8 January 1801, and confirmed by Tsar Alexander I on 12 September 1801. The Georgian envoy in Saint Petersburg reacted with a note of protest that was presented to the Russian vice-chancellor Prince Kurakin. In May 1801, under the oversight of General Carl Heinrich von Knorring, Imperial Russia transferred power in eastern Georgia to the government headed by General Ivan Petrovich Lazarev. The Georgian nobility did not accept the decree until April 1802 when General Knorring compassed the nobility in Tbilisi's Sioni Cathedral and forced them to take an oath on the Imperial Crown of Russia. Those who disagreed were temporarily arrested.
In the summer of 1805, Russian troops on the Askerani River near Zagam defeated the Persian army and saved Tbilisi from reconquest now that it was officially part of the Imperial territories. Russian suzerainty over eastern Georgia was officially finalized with Persia in 1813 following the Treaty of Gulistan.
Following the annexation of eastern Georgia, the western Georgian kingdom of Imereti was annexed by Tsar Alexander I. The last Imeretian king and the last Georgian Bagrationi ruler, Solomon II, died in exile in 1815. From 1803 to 1878, as a result of numerous Russian wars against the Ottoman Empire, several of Georgia's previously lost territories – such as Adjara – were recovered. The principality of Guria was abolished and incorporated into the Empire in 1828, and that of Mingrelia in 1857. The region of Svaneti was gradually annexed in 1857–1859.
Declaration of independence.
After the Russian Revolution of 1917, Georgia declared independence on 26 May 1918, in the midst of the Russian Civil War. The Menshevik Georgian Social-Democratic Party won the parliamentary election. Its leader, Noe Zhordania, became prime minister.
The 1918 Georgian–Armenian War, which erupted over parts of Georgian provinces populated mostly by Armenians, ended because of British intervention. In 1918–1919, Georgian general Giorgi Mazniashvili led an attack against the White Army led by Moiseev and Denikin in order to claim the Black Sea coastline from Tuapse to Sochi and Adler for independent Georgia. The country's independence did not last long. Georgia was under British protection from 1918–1920.
Georgia in the Soviet Union.
In February 1921, Georgia was attacked by the Red Army. The Georgian army was defeated and the Social-Democratic government fled the country. On 25 February 1921, the Red Army entered Tbilisi and installed a communist government loyal to Moscow, led by Georgian Bolshevik Filipp Makharadze.
Nevertheless, there remained significant opposition to the Bolsheviks, and this culminated in the August Uprising of 1924. Soviet rule was firmly established only after this uprising was suppressed. Georgia was incorporated into the Transcaucasian SFSR, which united Georgia, Armenia and Azerbaijan. Later, in 1936, the TSFSR was disaggregated into its component elements and Georgia became the Georgian SSR.
Joseph Stalin, an ethnic Georgian born Ioseb Besarionis Dze Jugashvili (იოსებ ბესარიონის ძე ჯუღაშვილი) in Gori, was prominent among the Bolsheviks. Stalin was to rise to the highest position, leading the Soviet Union from 3 April 1922 until his death on 16 October 1952.
From 1941 to 1945, during World War II, almost 700,000 Georgians fought in the Red Army against Nazi Germany. There were also a few who fought on the German side. About 350,000 Georgians died in the battlefields of the Eastern Front.
On 9 April 1989, a peaceful demonstration in Tbilisi ended with several people being killed by Soviet troops. Before the October 1990 elections to the national assembly, the "Umaghlesi Sabcho" (Supreme Council) – the first polls in the USSR held on a formal multi-party basis – the political landscape was reshaped again. While the more radical groups boycotted the elections and convened an alternative forum (the National Congress) with alleged support of Moscow, another part of the anticommunist opposition united into the Round Table—Free Georgia around the former dissidents like Merab Kostava and Zviad Gamsakhurdia. The latter won the elections by a clear margin, with 155 out of 250 parliamentary seats, whereas the ruling Communist Party (CP) received only 64 seats. All other parties failed to get over the 5 percent threshold and were thus allotted only some single-member constituency seats.
Georgia after restoration of independence.
On 9 April 1991, shortly before the collapse of the Soviet Union, Georgia declared independence. On 26 May 1991, Gamsakhurdia was elected as a first President of independent Georgia. Gamsakhurdia stoked Georgian nationalism and vowed to assert Tbilisi's authority over regions such as Abkhazia and South Ossetia that had been classified as autonomous oblasts under the Soviet Union.
He was soon deposed in a bloody "coup d'état", from 22 December 1991 to 6 January 1992. The coup was instigated by part of the National Guards and a paramilitary organization called "Mkhedrioni" ("horsemen"). The country became embroiled in a bitter civil war, which lasted until nearly 1995. Eduard Shevardnadze (Soviet Minister of Foreign Affairs from 1985 to 1991) returned to Georgia in 1992 and joined the leaders of the coup — Tengiz Kitovani and Jaba Ioseliani — to head a triumvirate called "The State Council".
Simmering disputes within two regions of Georgia, Abkhazia and South Ossetia, between local separatists and the majority Georgian populations, erupted into widespread inter-ethnic violence and wars. Supported by Russia, Abkhazia, and South Ossetia achieved "de facto" independence from Georgia, with Georgia retaining control only in small areas of the disputed territories. In 1995, Shevardnadze was officially elected as president of Georgia.
Roughly 230,000 to 250,000 Georgians were massacred or expelled from Abkhazia by Abkhaz separatists and North Caucasian volunteers (including Chechens) in 1992–1993. Around 23,000 Georgians fled South Ossetia as well, and many Ossetian families were forced to abandon their homes in the Borjomi region and moved to Russia.
In 2003, Shevardnadze (who won re-election in 2000) was deposed by the Rose Revolution, after Georgian opposition and international monitors asserted that the November 2 parliamentary elections were marred by fraud. The revolution was led by Mikheil Saakashvili, Zurab Zhvania and Nino Burjanadze, former members and leaders of Shevardnadze's ruling party. Mikheil Saakashvili was elected as President of Georgia in 2004.
Following the Rose Revolution, a series of reforms were launched to strengthen the country's military and economic capabilities. The new government's efforts to reassert Georgian authority in the southwestern autonomous republic of Ajaria led to a major crisis early in 2004. Success in Ajaria encouraged Saakashvili to intensify his efforts, but without success, in breakaway South Ossetia.
These events, along with accusations of Georgian involvement in the Second Chechen War, resulted in a severe deterioration of relations with Russia, fuelled also by Russia's open assistance and support to the two secessionist areas. Despite these increasingly difficult relations, in May 2005 Georgia and Russia reached a bilateral agreement by which Russian military bases (dating back to the Soviet era) in Batumi and Akhalkalaki were withdrawn. Russia withdrew all personnel and equipment from these sites by December 2007 while failing to withdraw from the Gudauta base in Abkhazia, which it was required to vacate after the adoption of Adapted Conventional Armed Forces in Europe Treaty during the 1999 Istanbul summit.
Russo-Georgian War and since.
Tensions with Russia began escalating in April 2008. South Ossetian separatists committed the first act of violence when they blew up a Georgian military vehicle on 1 August, wounding five Georgian peacekeepers. During the evening, Georgian snipers retaliated by attacking the South Ossetian border checkpoints. Ossetian separatists began shelling Georgian villages on 1 August, with a sporadic response from Georgian peacekeepers and other fighters in the region.
On 7 August, Georgian President Mikheil Saakashvili, ordered a unilateral ceasefire at about 7 p.m. However, Ossetian separatists intensified their attacks on Georgian villages. Georgia launched a large-scale military operation against South Ossetia during the night of 7–8 August 2008. According to the EU fact-finding mission, 10,000–11,000 soldiers took part in the general Georgian offensive in South Ossetia. The official reason given for this was to "restore constitutional order" in the region.
After the heights around Tskhinvali were secured, Georgian troops with tanks and artillery support entered the town. Georgian shelling left parts of Tskhinvali in ruins. According to Russian military commander, over 10 Russian peacekeepers were killed on 8 August. That day Russia officially sent troops across the Georgian border into South Ossetia, claiming to be defending both peacekeepers and South Ossetian civilians. Russia accused Georgia of committing "genocide". Russian authorities claimed that the civilian casualties in Tskhinvali amounted up to 2,000. These high casualty figures were later revised down to 162 casualties.
In five days of fighting, the Russian forces captured Tskhinvali, pushed back Georgian troops, and largely destroyed Georgia’s military infrastructure using airstrikes deep inside Georgia proper. Russian and Abkhaz forces opened a second front by attacking the Kodori Gorge, held by Georgia. After the retreat of the Georgian forces, the Russians temporarily occupied the cities of Poti, Gori, Senaki, and Zugdidi.
Both during and after the war, South Ossetian authorities and irregular militia conducted a campaign of ethnic cleansing against Georgians in South Ossetia, with Georgian villages around Tskhinvali being destroyed after the war had ended. The war displaced 192,000 people, and while many were able to return to their homes after the war, a year later around 30,000 ethnic Georgians remained displaced. In an interview published in “Kommersant”, South Ossetian leader Eduard Kokoity said he would not allow Georgians to return.
Through mediation by President of France Nicolas Sarkozy, the parties reached a ceasefire agreement on 12 August. On 17 August, Dmitry Medvedev announced that Russian forces were to begin withdrawal on the next day. On 8 October, Russian forces withdrew from the buffer zones adjacent to Abkhazia and South Ossetia. The control of the buffer zones was handed over to the EU monitoring mission in Georgia.
Russia recognised Abkhazia and South Ossetia on 26 August 2008. In response, the Georgian government cut diplomatic relations with Russia. Since the war, Georgia has maintained that Abkhazia and South Ossetia are under Russian occupation and remain, legally, part of Georgia.
Government and politics.
Georgia is a representative democratic semi-presidential republic, with the President as the head of state, and Prime Minister as the head of government. The executive branch of power is made up of the President and the Cabinet of Georgia. The Cabinet is composed of ministers, headed by the Prime Minister, and appointed by the President. Notably, the ministers of defense and interior are not members of the Cabinet and are subordinated directly to the President of Georgia. Giorgi Margvelashvili is the current President of Georgia after winning 62.12% of the vote in the 2013 election. Since 2013, Irakli Garibashvili has been the prime minister of Georgia.
Legislative authority is vested in the Parliament of Georgia. It is unicameral and has 150 members, known as deputies, of whom 75 are elected by plurality to represent single-member district, and 75 are chosen to represent parties by proportional representation. Members of parliament are elected for four-year terms. Five parties and electoral blocs had representatives elected to the parliament in the 2008 elections: the United National Movement (governing party), The Joint Opposition, the Christian-Democrats, the Labour Party and Republican Party. On 26 May 2012, Saakashvili inaugurated a new Parliament building in the western city of Kutaisi, in an effort to decentralise power and shift some political control closer to Abkhazia.
Although considerable progress was made since the Rose revolution, former President Mikheil Saakashvili stated in 2008 that Georgia is still not a "full-fledged, very well-formed, crystalized society." The political system remains in the process of transition, with frequent adjustments to the balance of power between the President and Parliament, and opposition proposals ranging from transforming the country into parliamentary republic to re-establishing the monarchy. Observers note the deficit of trust in relations between the Government and the opposition.
Different opinions exist regarding the degree of political freedom in Georgia. Saakashvili believed in 2008 that the country is "on the road to becoming a European democracy." Freedom House lists Georgia as a partly free country.
In preparation for 2012 parliamentary elections, Parliament adopted a new electoral code on 27 December 2011 that incorporated many recommendations from non-governmental organizations (NGOs) and the Venice Commission. However, the new code failed to address the Venice Commission’s primary recommendation to strengthen the equality of the vote by reconstituting single-mandate election districts to be comparable in size. On December 28, Parliament amended the Law on Political Unions to regulate campaign and political party financing. Local and international observers raised concerns about several amendments, including the vagueness of the criteria for determining political bribery and which individuals and organizations would be subject to the law. As of March 2012, Parliament was discussing further amendments to address these concerns.
The elections in October 2012 resulted in the victory for the opposition "Georgian Dream – Democratic Georgia" coalition, which President Saakashvili acknowledged on the following day.
Foreign relations.
Georgia maintains good relations with its direct neighbours (Armenia, Azerbaijan, and Turkey) and is a member of the United Nations, the Council of Europe, the World Trade Organization, the Organization of the Black Sea Economic Cooperation, the Organization for Security and Cooperation in Europe, the Community of Democratic Choice, the GUAM Organization for Democracy and Economic Development, and the Asian Development Bank. Georgia also maintains political, economic, and military relations with Japan, Uruguay, South Korea, Israel, Sri Lanka, Ukraine, and many other countries.
The growing U.S. and European Union influence in Georgia, notably through proposed EU and NATO membership, the U.S. Train and Equip military assistance program, and the construction of the Baku-Tbilisi-Ceyhan pipeline have frequently strained Tbilisi's relations with Moscow. Georgia's decision to boost its presence in the coalition forces in Iraq was an important initiative.
Georgia is currently working to become a full member of NATO. In August 2004, the Individual Partnership Action Plan of Georgia was submitted officially to NATO. On 29 October 2004, the North Atlantic Council of NATO approved the Individual Partnership Action Plan (IPAP) of Georgia, and Georgia moved on to the second stage of Euro-Atlantic Integration. In 2005, by the decision of the President of Georgia, a state commission was set up to implement the Individual Partnership Action Plan, which presents an interdepartmental group headed by the Prime Minister. The Commission was tasked with coordinating and controlling the implementation of the Individual Partnership Action Plan.
On 14 February 2005, the agreement on the appointment of Partnership for Peace (PfP) liaison officer between Georgia and NATO came into force, whereby a liaison officer for the South Caucasus was assigned to Georgia. On March 2, 2005, the agreement was signed on the provision of the host nation support to and transit of NATO forces and NATO personnel. On March 6–9, 2006, the IPAP implementation interim assessment team arrived in Tbilisi. On April 13, 2006, the discussion of the assessment report on implementation of the Individual Partnership Action Plan was held at NATO Headquarters, within 26+1 format. In 2006, the Georgian parliament voted unanimously for the bill which calls for integration of Georgia into NATO. The majority of Georgians and politicians in Georgia support the push for NATO membership.
George W. Bush became the first sitting U.S. president to visit the country. The street leading to Tbilisi International Airport has since been dubbed George W. Bush Avenue. On October 2, 2006, Georgia and the European Union signed a joint statement on the agreed text of the Georgia-European Union Action Plan within the European Neighbourhood Policy (ENP). The Action Plan was formally approved at the EU-Georgia Cooperation Council session on 14 November 2006, in Brussels.
Military.
Georgia's military is organized into land and air forces. They are collectively known as the Georgian Armed Forces (GAF). The mission and functions of the GAF are based on the Constitution of Georgia, Georgia’s Law on Defense and National Military Strategy, and international agreements to which Georgia is signatory. They are performed under the guidance and authority of the Ministry of Defense.
Georgia contributed nearly 1,000 soldiers to the NATO-led International Security Assistance Force in Afghanistan, making it the highest troop contributor per-capita to the mission. As of September 2011, Georgia has suffered 10 deaths and 38 injuries.
Law enforcement.
In Georgia, law enforcement is conducted and provided for by the Ministry of Internal Affairs of Georgia. In recent years, the Patrol Police Department of the Ministry of Internal Affairs of Georgia has undergone a radical transformation, with the police having now absorbed a great many duties previously performed by dedicated independent government agencies. New duties performed by the police include border security and customs functions and contracted security provision; the latter function is performed by the dedicated 'security police'. Intelligence collecting in the interests of national security is now the remit of the Georgian Intelligence Service.
In 2005, President Mikhail Saakashvili fired the entire traffic police force (numbering around 30,000 police officers) of the Georgian National Police due to corruption. A new force was then subsequently built around new recruits. The US State Department's Bureau of International Narcotics and Law-Enforcement Affairs has provided assistance to the training efforts and continues to act in an advisory capacity.
The new "Patruli" force was first introduced in the summer of 2005 to replace the traffic police, a force which was accused of widespread corruption. The police introduced an 022 emergency dispatch service in 2004.
Human rights.
Human rights in Georgia are guaranteed by the country's constitution. There is an independent human rights "public defender" elected by the Parliament of Georgia to ensure such rights are enforced. Georgia has ratified the Framework Convention for the Protection of National Minorities in 2005. NGO "Tolerance", in its alternative report about its implementation, speaks of rapid decreasing of the number of Azerbaijani schools and cases of appointing headmasters to Azerbaijani schools who don't speak the Azerbaijani language.
The government came under criticism for its alleged use of excessive force on 26 May 2011 when it dispersed protesters led by Nino Burjanadze, among others, with tear gas and rubber bullets after they refused to clear Rustaveli avenue for an independence day parade despite the expiration of their demonstration permit and despite being offered to choose an alternative venue. While human rights activists maintained that the protests were peaceful, the government pointed out that many protesters were masked and armed with heavy sticks and molotov cocktails. Georgian opposition leader Nino Burjanadze said the accusations of planning a coup were baseless, and that the protesters' actions were legitimate.
Administrative divisions.
Georgia is divided into 9 regions, 1 city, and 2 autonomous republics. These in turn are subdivided into 69 districts.
Georgia contains two official autonomous regions, of which one has declared independence. In addition, another territory not officially autonomous has also declared independence. Officially autonomous within Georgia, the de facto independent region of Abkhazia declared independence in 1999. South Ossetia is officially known by Georgia as the Tskinvali region, as it views "South Ossetia" as implying political bonds with Russian North Ossetia. It was called South Ossetian Autonomous Oblast when Georgia was part of Soviet Union. Its autonomous status was revoked in 1990. De facto separate since Georgian independence, offers were made to give South Ossetia autonomy again, but in 2006 an unrecognised referendum in the area resulted in a vote for independence.
In both Abkhazia and South Ossetia large numbers of people had been given Russian passports, some through a process of forced passportization by Russian authorities. This was used as a justification for Russian invasion of Georgia during the 2008 South Ossetia war after which Russia recognised the region's independence. Georgia considers the regions as occupied by Russia. Both republics have received minimal international recognition.
Adjara under local strongman Aslan Abashidze maintained close ties with Russia and allowed a Russian military base to be maintained in Batumi. Upon the election of Mikheil Saakashvili in 2004 tensions rose between Adjara and the Georgian government, leading to demonstrations in Adjara and the resignation and flight of Abashidze. The region retains autonomy.
Geography and climate.
Georgia is situated in the South Caucasus, between latitudes 41° and 44° N, and longitudes 40° and 47° E, with an area of 67900 km2. It is a very mountainous country. The Likhi Range divides the country into eastern and western halves. Historically, the western portion of Georgia was known as Colchis while the eastern plateau was called Iberia. Because of a complex geographic setting, mountains also isolate the northern region of Svaneti from the rest of Georgia.
The Greater Caucasus Mountain Range forms the northern border of Georgia. The main roads through the mountain range into Russian territory lead through the Roki Tunnel between South and North Ossetia and the Darial Gorge (in the Georgian region of Khevi). The Roki Tunnel was vital for the Russian military in the 2008 South Ossetia war because it is the only direct route through the Caucasus Mountains. The southern portion of the country is bounded by the Lesser Caucasus Mountains. The Greater Caucasus Mountain Range is much higher in elevation than the Lesser Caucasus Mountains, with the highest peaks rising more than 5000 m above sea level.
The highest mountain in Georgia is Mount Shkhara at 5068 m, and the second highest is Mount Janga (Dzhangi-Tau) at 5059 m above sea level. Other prominent peaks include Mount Kazbek at 5047 m, Shota Rustaveli 4860 m, Tetnuldi 4858 m, Mt. Ushba 4700 m, and Ailama 4547 m. Out of the abovementioned peaks, only Kazbek is of volcanic origin. The region between Kazbek and Shkhara (a distance of about 200 km along the Main Caucasus Range) is dominated by numerous glaciers. Out of the 2,100 glaciers that exist in the Caucasus today, approximately 30% are located within Georgia.
The term Lesser Caucasus Mountains is often used to describe the mountainous (highland) areas of southern Georgia that are connected to the Greater Caucasus Mountain Range by the Likhi Range. The area can be split into two separate sub-regions; the Lesser Caucasus Mountains, which run parallel to the Greater Caucasus Range, and the Southern Georgia Volcanic Highland, which lies immediately to the south of the Lesser Caucasus Mountains.
The overall region can be characterized as being made up of various, interconnected mountain ranges (largely of volcanic origin) and plateaus that do not exceed 3400 m in elevation. Prominent features of the area include the Javakheti Volcanic Plateau, lakes, including Tabatskuri and Paravani, as well as mineral water and hot springs. Two major rivers in Georgia are the Rioni and the Mtkvari. The Southern Georgia Volcanic Highland is a young and unstable geologic region with high seismic activity and has experienced some of the most significant earthquakes that have been recorded in Georgia.
The Krubera Cave is the deepest known cave in the world. It is located in the Arabika Massif of the Gagra Range, in Abkhazia. In 2001, a Russian–Ukrainian team had set the world depth record for a cave at 1710 m. In 2004, the penetrated depth was increased on each of three expeditions, when a Ukrainian team crossed the 2000 m mark for the first time in the history of speleology. In October 2005, an unexplored part was found by the CAVEX team, further increasing the known depth of the cave. This expedition confirmed the known depth of the cave at 2140 m.
Topography.
The landscape within the nation's boundaries is quite varied. Western Georgia's landscape ranges from low-land marsh-forests, swamps, and temperate rainforests to eternal snows and glaciers, while the eastern part of the country even contains a small segment of semi-arid plains. Forests cover around 40% of Georgia's territory while the alpine/subalpine zone accounts for roughly around 10 percent of the land.
Much of the natural habitat in the low-lying areas of western Georgia has disappeared during the past 100 years because of the agricultural development of the land and urbanization. The large majority of the forests that covered the Colchis plain are now virtually non-existent with the exception of the regions that are included in the national parks and reserves (e.g. Lake Paliastomi area). At present, the forest cover generally remains outside of the low-lying areas and is mainly located along the foothills and the mountains. Western Georgia's forests consist mainly of deciduous trees below 600 m above sea level and contain species such as oak, hornbeam, beech, elm, ash, and chestnut. Evergreen species such as box may also be found in many areas. Ca. 1000 of all 4000 higher plants of Georgia are endemic in this country.
The west-central slopes of the Meskheti Range in Ajaria as well as several locations in Samegrelo and Abkhazia are covered by temperate rain forests. Between 600 - above sea level, the deciduous forest becomes mixed with both broad-leaf and coniferous species making up the plant life. The zone is made up mainly of beech, spruce, and fir forests. From 1500 -, the forest becomes largely coniferous. The tree line generally ends at around 1800 m and the alpine zone takes over, which in most areas, extends up to an elevation of 3000 m above sea level. The eternal snow and glacier zone lies above the 3,000 metre line.
Eastern Georgia's landscape (referring to the territory east of the Likhi Range) is considerably different from that of the west, although, much like the Colchis plain in the west, nearly all of the low-lying areas of eastern Georgia including the Mtkvari and Alazani River plains have been deforested for agricultural purposes. In addition, because of the region's relatively drier climate, some of the low-lying plains (especially in Kartli and south-eastern Kakheti) were never covered by forests in the first place.
The general landscape of eastern Georgia comprises numerous valleys and gorges that are separated by mountains. In contrast with western Georgia, nearly 85 percent of the forests of the region are deciduous. Coniferous forests only dominate in the Borjomi Gorge and in the extreme western areas. Out of the deciduous species of trees, beech, oak, and hornbeam dominate. Other deciduous species include several varieties of maple, aspen, ash, and hazelnut. The Upper Alazani River Valley contains yew forests.
At higher elevations above 1000 m above sea level (particularly in the Tusheti, Khevsureti, and Khevi regions), pine and birch forests dominate. In general, the forests in eastern Georgia occur between 500 - above sea level, with the alpine zone extending from 2,000–2,300 to 3,000–3,500 metres (6,562–7,546 to 9,843–11,483 ft). The only remaining large, low-land forests remain in the Alazani Valley of Kakheti. The eternal snow and glacier zone lies above the 3500 m line in most areas of eastern Georgia.
Climate.
The climate of Georgia is extremely diverse, considering the nation's small size. There are two main climatic zones, roughly separating eastern and western parts of the country. The Greater Caucasus Mountain Range plays an important role in moderating Georgia's climate and protects the nation from the penetration of colder air masses from the north. The Lesser Caucasus Mountains partially protect the region from the influence of dry and hot air masses from the south as well.
Much of western Georgia lies within the northern periphery of the humid subtropical zone with annual precipitation ranging from 1000 –. The precipitation tends to be uniformly distributed throughout the year, although the rainfall can be particularly heavy during the Autumn months. The climate of the region varies significantly with elevation and while much of the lowland areas of western Georgia are relatively warm throughout the year, the foothills and mountainous areas (including both the Greater and Lesser Caucasus Mountains) experience cool, wet summers and snowy winters (snow cover often exceeds 2 meters in many regions). Ajaria is the wettest region of the Caucasus, where the Mt. Mtirala rainforest, east of Kobuleti receives around 4500 mm of precipitation per year.
Eastern Georgia has a transitional climate from humid subtropical to continental. The region's weather patterns are influenced both by dry Caspian air masses from the east and humid Black Sea air masses from the west. The penetration of humid air masses from the Black Sea is often blocked by several mountain ranges (Likhi and Meskheti) that separate the eastern and western parts of the nation. Annual precipitation is considerably less than that of western Georgia and ranges from 400 –.
The wettest periods generally occur during spring and autumn, while winter and summer months tend to be the driest. Much of eastern Georgia experiences hot summers (especially in the low-lying areas) and relatively cold winters. As in the western parts of the nation, elevation plays an important role in eastern Georgia where climatic conditions above 1500 m are considerably colder than in the low-lying areas. The regions that lie above 2000 m frequently experience frost even during the summer months.
Biodiversity.
Because of its high landscape diversity and low latitude, Georgia is home to about 1,000 species of vertebrates, (330 birds, 160 fish, 48 reptiles, and 11 amphibians). A number of large carnivores live in the forests, namely Brown bears, wolves, lynxes and Caucasian Leopards. The common pheasant (also known as the Colchian Pheasant) is an endemic bird of Georgia which has been widely introduced throughout the rest of the world as an important game bird. The species number of invertebrates is considered to be very high but data is distributed across a high number of publications. The spider checklist of Georgia, for example, includes 501 species.
Slightly more than 6,500 species of fungi, including lichen-forming species, have been recorded from Georgia, but this number is far from complete. The true total number of fungal species occurring in Georgia, including species not yet recorded, is likely to be far higher, given the generally accepted estimate that only about 7 percent of all fungi worldwide have so far been discovered. Although the amount of available information is still very small, a first effort has been made to estimate the number of fungal species endemic to Georgia, and 2595 species have been tentatively identified as possible endemics of the country. 1729 species of plants have been recorded from Georgia in association with fungi. The true number of plant species occurring in Georgia is likely to be substantially higher.
Economy.
Archaeological research demonstrates that Georgia has been involved in commerce with many lands and empires since the ancient times, largely due its location on the Black Sea and later on the historical Silk Road. Gold, silver, copper and iron have been mined in the Caucasus Mountains. Georgian wine making is a very old tradition and a key branch of the country's economy. The country has sizable hydropower resources. Throughout Georgia's modern history agriculture and tourism have been principal economic sectors, because of the country's climate and topography.
For much of the 20th century, Georgia's economy was within the Soviet model of command economy. Since the fall of the USSR in 1991, Georgia embarked on a major structural reform designed to transition to a free market economy. As with all other post-Soviet states, Georgia faced a severe economic collapse. The civil war and military conflicts in South Ossetia and Abkhazia aggravated the crisis. The agriculture and industry output diminished. By 1994 the gross domestic product had shrunk to a quarter of that of 1989. The first financial help from the West came in 1995, when the World Bank and International Monetary Fund granted Georgia a credit of USD 206 million and Germany granted DM 50 million.
Since the early 21st century visible positive developments have been observed in the economy of Georgia. In 2007, Georgia's real GDP growth rate reached 12 percent making Georgia one of the fastest growing economies in Eastern Europe. The World Bank dubbed Georgia "the number one economic reformer in the world" because it has in one year improved from rank 112th to 18th in terms of ease of doing business. The country has a high unemployment rate of 12.6% and has fairly low median income compared to European countries.
The 2006 ban on imports of Georgian wine to Russia, one of Georgia's biggest trading partners, and break of financial links was described by the IMF Mission as an "external shock". In addition, Russia increased the price of gas for Georgia. This was followed by the spike in the Georgian lari's rate of inflation. The National Bank of Georgia stated that the inflation was mainly triggered by external reasons, including Russia’s economic embargo. The Georgian authorities expected that the current account deficit due to the embargo in 2007 would be financed by "higher foreign exchange proceeds generated by the large inflow of foreign direct investment" and an increase in tourist revenues. The country has also maintained a solid credit in international market securities. Georgia is becoming more integrated into the global trading network: its 2006 imports and exports account for 10% and 18% of GDP respectively. Georgia's main imports are natural gas, oil products, machinery and parts, and transport equipment.
Tourism is an increasingly significant part of the Georgian economy. About a million tourists brought US$313 million to the country in 2006. According to the government, there are 103 resorts in different climatic zones in Georgia. Tourist attractions include more than 2000 mineral springs, over 12,000 historical and cultural monuments, four of which are recognised as UNESCO World Heritage Sites (Bagrati Cathedral in Kutaisi and Gelati Monastery, historical monuments of Mtskheta, and Upper Svaneti).
Georgia is developing into an international transport corridor through Batumi and Poti ports, an oil pipeline from Baku through Tbilisi to Ceyhan, the Baku-Tbilisi-Ceyhan pipeline (BTC) and a parallel gas pipeline, the South Caucasus Pipeline.
Since coming to power Saakashvili administration accomplished a series of reforms aimed at improving tax collection. Among other things a flat income tax was introduced in 2004. As a result budget revenues have increased fourfold and a once large budget deficit has turned into surplus.
As of 2001, 54 percent of the population lived below the national poverty line but by 2006 poverty decreased to 34 percent. In 2005, the average monthly income of a household was GEL 347 (about USD $200). 2013 estimates place Georgia's nominal GDP at US$15.98 billion. Georgia's economy is becoming more devoted to services (now representing 65 percent of GDP), moving away from the agricultural sector (10.9 percent).
In regards to telecommunication infrastructure, Georgia is ranked second to last among its bordering neighbors in the World Economic Forum's Network Readiness Index (NRI) – an indicator for determining the development level of a country’s information and communication technologies. Georgia ranked number 60 overall in the 2014 NRI ranking, up from 65 in 2013.
Transport.
Today transport in Georgia is provided by means of rail, road, shipping and air travel. Positioned in the Caucasus and on the coast of the Black Sea, Georgia is a key country through which energy imports to the European Union from neighbouring Azerbaijan pass. Traditionally the country was located on an important north-south trade route between European Russia and the Near East and Turkey.
In recent years Georgia has invested large amounts of money in the modernisation of its transport networks. The construction of new highways has been prioritised and, as such, major cities like Tbilisi have seen the quality of their roads improve dramatically; despite this however, the quality of inter-city routes remains poor and to date only one motorway-standard road has been constructed - the ს 1.
The Georgian railways represent an important transport artery for the Caucasus as they make up the largest proportion of a route linking the Black and Caspian Seas, this in turn has allowed them to benefit in recent years from increased energy exports from neighbouring Azerbaijan to the European Union, Ukraine and Turkey. Passenger services are operated by the state-owned Georgian Railways whilst freight operations are carried out by a number of licensed operators. Since 2004 the Georgian Railways have been undergoing a rolling program of fleet-renewal and managerial restructuring which is aimed at making the service provided more efficient and comfortable for passengers. Infrastructural development has also been high on the agenda for the railways, with the key Tbilisi railway junction expected to undergo major reorganisation in the near future. Additional projects also include the construction of the economically important Kars–Tbilisi–Baku railway, which for the first time will connect much of the Caucasus with Turkey by standard gauge railway.
Air and maritime transport is developing in Georgia, with the former mainly used by passengers and the latter for transport of freight. Georgia currently has four international airports; the largest of which is by far Tbilisi International Airport, hub for Georgian Airways, which offers connections to many large European cities. Other airports in the country are largely underdeveloped or lack scheduled traffic, although, as of late, efforts have been made to solve both these problems. There are a number of seaports along Georgia's Black Sea coast, the largest and must busy of which is the Port of Batumi; whilst the town is itself a seaside resort, the port is a major cargo terminal in the Caucasus and is often used by neighbouring Azerbaijan as a transit point for making energy deliveries to Europe. Scheduled and chartered passenger ferry services link Georgia with Ukraine and Turkey.
Demographics.
Like most native Caucasian peoples, the Georgians do not fit into any of the main ethnic categories of Europe or Asia. The Georgian language, the most pervasive of the Kartvelian languages, is neither Indo-European, Turkic nor Semitic. The present day Georgian or Kartvelian nation is thought to have resulted from the fusion of aboriginal, autochthonous inhabitants with immigrants who moved into South Caucasus from the direction of Anatolia in remote antiquity. The ancient Jewish chronicle by Josephus mentions Georgians as Iberes who were also called Thobel Tubal.
Ethnic Georgians form about 84 percent of Georgia's current population of 4,661,473 (July 2006 est.). Other ethnic groups include Abkhazians, Ossetians, Armenians, Azerbaijanis, Pontic Greeks (here divided between Caucasus Greeks and Turkish repealing Urums), Jews, Russians. The Georgian Jews are one of the oldest Jewish communities in the world.
The most widespread language group is the Kartvelian family, which includes Georgian, Svan, Mingrelian and Laz. The official languages of Georgia are Georgian, with Abkhaz official within the autonomous region of Abkhazia. Georgian is the primary language of approximately 71 percent of the population, followed by 9 percent speaking Russian, 7 percent Armenian, 6 percent Azerbaijani, and 7 percent other languages.
In the early 1990s, following the dissolution of the Soviet Union, violent separatist conflicts broke out in the autonomous regions of Abkhazia and South Ossetia. Many Ossetians living in Georgia left the country, mainly to Russia's North Ossetia. On the other hand, more than 150,000 Georgians left Abkhazia after the breakout of hostilities in 1993. Of the Meskhetian Turks who were forcibly relocated in 1944 only a tiny fraction returned to Georgia as of 2008.
The 1989 census recorded 341,000 ethnic Russians, or 6.3 percent of the population, 52,000 Ukrainians and 100,000 Greeks in Georgia. Since 1990, 1.5 million Georgian nationals have left. At least 1 million immigrants from Georgia legally or illegally reside in Russia. Georgia's net migration rate is −4.54, excluding Georgian nationals who live abroad. Georgia has nonetheless been inhabited by immigrants from all over the world throughout its independence. According to 2006 statistics, Georgia gets most of its immigrants from Turkey and China.
Today 83.9 percent of the population practices Eastern Orthodoxy, with majority of these adhering to the national Georgian Orthodox Church. Religious minorities include Muslims (9.9 percent), Armenian Apostolic (3.9 percent), and Roman Catholic (0.8 percent). 0.8 percent of those recorded in the 2002 census declared themselves to be adherents of other religions and 0.7 percent declared no religion at all.
Religion.
A large majority of Georgia's population (83.9% in 2002) practices Orthodox Christianity. The Georgian Orthodox Church is one of the world's most ancient Christian Churches, and claims apostolic foundation by Saint Andrew. In the first half of the 4th century, Christianity was adopted as the state religion of Iberia (present-day Kartli, or eastern Georgia), following the missionary work of Saint Nino of Cappadocia. The Church gained autocephaly during the early Middle Ages; it was abolished during the Russian domination of the country, restored in 1917 and fully recognised by the Ecumenical Patriarchate of Constantinople in 1990.
The special status of the Georgian Orthodox Church is officially recognised in the Constitution of Georgia and the Concordat of 2002, although religious institutions are separate from the state, and every citizen has the right of religion.
Religious minorities of Georgia include Armenian Christians (3.9 percent), Muslims (9.9 percent), and Roman Catholics (0.8 percent). Islam is represented by both Azerbaijani Shia Muslims (in the south-east) ethnic Georgian Sunni Muslims in Adjara, and Laz-speaking Sunni Muslims as well as Sunni Meskhetian Turks along the border with Turkey. There are also smaller communities of Greek Muslims (of Pontic Greek origin) and Armenian Muslims, both of whom are descended from Ottoman-era converts to Turkish Islam from Eastern Anatolia who settled in Georgia following the Lala Mustafa Pasha's Caucasian campaign that led to the Ottoman conquest of the country in 1578. Georgian Jews trace the history of their community to the 6th century BC; their numbers have dwindled in the last decades due to strong immigration to Israel.
Despite the long history of religious harmony in Georgia, there have been several instances of religious discrimination and violence against "nontraditional faiths", such as Jehovah's Witnesses, by the followers of the defrocked Orthodox priest Basil Mkalavishvili.
Education.
The education system of Georgia has undergone sweeping modernizing, although controversial, reforms since 2004. Education in Georgia is mandatory for all children aged 6–14. The school system is divided into elementary (six years; age level 6–12), basic (three years; age level 12–15), and secondary (three years; age level 15–18), or alternatively vocational studies (two years). Students with a secondary school certificate have access to higher education. Only the students who have passed the Unified National Examinations may enroll in a state-accredited higher education institution, based on ranking of scores he/she received at the exams.
Most of these institutions offer three levels of study: a Bachelor's Program (three to four years); a Master's Program (two years), and a Doctoral Program (three years). There is also a Certified Specialist's Program that represents a single-level higher education program lasting from three to six years. As of 2008, 20 higher education institutions are accredited by the Ministry of Education and Science of Georgia. Gross primary enrollment ratio was 94 percent for the period of 2001–2006.
Culture.
Georgian culture evolved over thousands of years with its foundations in Iberian and Colchian civilizations, continuing into the rise of the unified Georgian Kingdom under the single monarchy of the Bagrationi. Georgian culture enjoyed a golden age and renaissance of classical literature, arts, philosophy, architecture and science in the 11th century.
The Georgian language, and the Classical Georgian literature of the poet Shota Rustaveli, were revived in the 19th century after a long period of turmoil, laying the foundations of the romantics and novelists of the modern era such as Grigol Orbeliani, Nikoloz Baratashvili, Ilia Chavchavadze, Akaki Tsereteli, Vazha Pshavela, and many others. Georgian culture was influenced by Classical Greece, the Roman Empire, the Byzantine Empire, and later by the Russian Empire.
Georgians have their own unique three alphabets which according to traditional accounts was invented by King Pharnavaz I of Iberia in the 3rd century BC.
Georgia is well known for its rich folklore, unique traditional music, theatre, cinema, and art. Georgians are renowned for their love of music, dance, theatre and cinema. In the 20th century there have been notable Georgian painters such as Niko Pirosmani, Lado Gudiashvili, Elene Akhvlediani; ballet choreographers such as George Balanchine, Vakhtang Chabukiani, and Nino Ananiashvili; poets such as Galaktion Tabidze, Lado Asatiani, and Mukhran Machavariani; and theatre and film directors such as Robert Sturua, Tengiz Abuladze, Giorgi Danelia and Otar Ioseliani.
Architecture and arts.
Georgian architecture has been influenced by many civilizations. There are several different architectural styles for castles, towers, fortifications and churches. The Upper Svaneti fortifications, and the castle town of Shatili in Khevsureti, are some of the finest examples of medieval Georgian castle architecture. Other architectural aspects of Georgia include Rustaveli avenue in Tbilisi in the Hausmann style, and the Old Town District.
Georgian ecclesiastic art is one of the most notable aspects of Georgian Christian architecture, which combines classical dome style with original basilica style forming what is known as the Georgian cross-dome style. Cross-dome architecture developed in Georgia during the 9th century; before that, most Georgian churches were basilicas. Other examples of Georgian ecclesiastic architecture can be found outside Georgia: Bachkovo Monastery in Bulgaria (built in 1083 by the Georgian military commander Grigorii Bakuriani), Iviron monastery in Greece (built by Georgians in the 10th century), and the Monastery of the Cross in Jerusalem (built by Georgians in the 9th century).
The art of Georgia spans the prehistoric, the ancient Greek, Roman, medieval, ecclesiastic, iconic and modern visual arts. One of the most famous late 19th/early 20th century Georgian artists is a primitivist painter Niko Pirosmani.
Music.
Georgia has a rich and vibrant musical tradition, primarily known for its early development of polyphony. Georgian polyphony is based on three vocal parts, a unique tuning system based on perfect fifths, and a harmonic structure rich in parallel fifths and dissonances. Each region in Georgia has its own traditional music with Persian influenced drones and ostinato-like soloists in the east, complex improvised harmonies in the west, and solid moving chords in Svanetie.
Cuisine.
Georgian cuisine and wine have evolved through the centuries, adapting traditions in each era. One of the most unusual traditions of dining is supra, or "Georgian table", which is also a way of socialising with friends and family. The head of "supra" is known as tamada. He also conducts the highly philosophical toasts, and makes sure that everyone is enjoying themselves. Various historical regions of Georgia are known for their particular dishes: for example, khinkali (meat dumplings), from eastern mountainous Georgia, and khachapuri, mainly from Imereti, Samegrelo and Adjara. In addition to traditional Georgian dishes, the foods of other countries have been brought to Georgia by immigrants from Russia, Greece, and recently China.
Sports.
The most popular sports in Georgia are football, basketball, rugby union, wrestling, judo, and weightlifting. Historically, Georgia has been famous for its physical education; it is known that the Romans were fascinated with Georgians' physical qualities after seeing the training techniques of ancient Iberia. Wrestling remains a historically important sport of Georgia, and some historians think that the Greco-Roman style of wrestling incorporates many Georgian elements.
Within Georgia, one of the most popularized styles of wrestling is the Kakhetian style. There were a number of other styles in the past that are not as widely used today. For example, the Khevsureti region of Georgia has three different styles of wrestling. Other popular sports in 19th century Georgia were polo, and Lelo, a traditional Georgian game later replaced by rugby union.
The first and only race circuit in the Caucasian region is located in Georgia. Rustavi International Motorpark originally built in 1978 was re-opened in 2012 after total reconstruction costing $20 million. The track satisfies the FIA Grade 2 requirements and currently hosts the Legends car racing series and Formula Alfa competitions.
Basketball always was one of the notable sports in Georgia, which had a few very famous Soviet Union national team members, such as: Otar Korkia, Mikhail Korkia, Zurab Sakandelidze and Levan Moseshvili. Dinamo Tbilisi won prestigious Euroleague competition in 1962. Georgia totally had five players in the NBA: Vladimir Stepania, Jake Tsakalidis, Nikoloz Tskitishvili, Tornike Shengelia and current Milwaukee Bucks member Zaza Pachulia. Other notable basketball players are: two times Euroleague champion Giorgi Shermadini and Euroleague players Manuchar Markoishvili and Viktor Sanikidze. The sport branch is regaining its popularity in country over the past years. Georgia national basketball team qualified to EuroBasket during last three tournaments since 2011.

</doc>
<doc id="48774" url="http://en.wikipedia.org/wiki?curid=48774" title="Lance">
Lance

The lance is a pole weapon or spear designed to be used by a mounted warrior. During the periods of Classical and Medieval warfare it evolved into being the leading weapon in cavalry charges, and was unsuited for throwing or for repeated thrusting, unlike similar weapons of the spear/javelin/pike family typically used by infantry. Lances were often equipped with a vamplate – a small circular plate to prevent the hand sliding up the shaft upon impact. Though best known as a military and sporting weapon carried by European knights, the use of lances was widespread throughout Asia, the Middle East and North Africa wherever suitable mounts were available. As a secondary weapon, lancers of the Medieval period also bore swords or maces for hand-to-hand combat, since the lance was often a one-use-per-engagement weapon; assuming the lance survived the initial impact intact, it was (depending on the lance) usually too long, heavy and slow to be effectively used against opponents in a melee.
Etymology.
The name is derived from the word "lancea" - the Roman auxiliaries' javelin or throwing knife; although according to the OED, the word may be of Iberian origin. Also compare "longche", a Greek term for lance.
A lance in the original sense is a light throwing spear, or javelin. The English verb "to launch" "fling, hurl, throw" is derived from the term (via Old French "lancier"), as well as the rarer or poetic "to lance". The term from the 17th century came to refer specifically to spears not thrown, used for thrusting by heavy cavalry, and especially in jousting. A thrusting spear which is used by infantry is usually referred to as a pike.
History of use.
Antiquity.
The first use of the lance in this sense was made by the Assyrians, Sarmatian and Parthian cataphracts from around the 3rd century BC. Long thrusting cavalry spears was especially popular among the Hellenistic armies' agema and line cavalry.
One of the most effective ancient lanced cavalry units was Alexander the Great's Companion cavalry, who were successful against both heavy infantry and cavalry units.
The Roman cavalry long thrusting spear was called a "contus" (from the Greek "kontos", barge-pole). It was usually 3 to 4m long, and grasped with both hands. It was employed by "equites contariorum" and "equites cataphractarii", fully armed and armoured cataphracts.
Middle Ages.
The Byzantine cavalry used lances ("kontos" or "kontarion") almost exclusively, often in mixed lancer and mounted archer formations ("cursores et defensores"). The Byzantines used lance both overarm and underarm, couched.
The best known usage of military lances was that of the full-gallop closed-ranks charge of a group of knights with underarm-couched lances, against lines of infantry, archery regiments, defensive embankments, and opposition cavalry. Two variants on the couched lance charge developed, the French method, "en haie", with lancers in a double line and the German method, with lancers drawn up in a deeper formation which was often wedge-shaped. It is commonly believed that this became the dominant European cavalry tactic in the 11th century after the development of the cantled saddle and stirrups (the Great Stirrup Controversy), and of rowel spurs (which enabled better control of the mount). Cavalry thus outfitted and deployed had a tremendous collective force in their charge, and could shatter most contemporary infantry lines. Recent evidence has suggested, however, that the lance charge was effective without the benefit of stirrups.
Because of the extreme stopping power of a thrusting spear, it quickly became a popular weapon of infantry in the Late Middle Ages. These eventually led to the rise of the longest type of spears, the pike. This adaptation of the cavalry lance to infantry use was largely tasked with stopping lance-armed cavalry charges. During the 15th, 16th and 17th centuries, these weapons, both mounted and unmounted, were so effective that lancers and pikemen not only became a staple of every Western army, but also became highly sought-after mercenaries. (However, the pike had already been used by Philip II of Macedon in antiquity to great effect, in the form of the sarissa.)
In Europe, a jousting lance was a variation of the knight's lance which was modified from its original war design. In jousting, the lance tips would usually be blunt, often spread out like a cup or furniture foot, to provide a wider impact surface designed to unseat the opposing rider without spearing him through. The centre of the shaft of such lances could be designed to be hollow, in order for it to break on impact, as a further safeguard against impalement. They were often at least 4m long, and had hand guards built into the lance, often tapering for a considerable portion of the weapon's length. These are the versions that can most often be seen at medieval reenactment festivals. In war, lances were much more like stout spears, long and balanced for one-handed use, and with sharpened tips.
Lance (unit organization).
As a small unit that surrounded a knight when he went into battle during the 14th and 15th centuries, a lance might have consisted of one or two squires, the knight himself, one to three men-at-arms, and possibly an archer. Lances were often combined under the banner of a higher-ranking nobleman to form companies of knights that would act as an ad-hoc unit.
16th century decline in Western Europe.
The advent of wheellock technology spelled the end of the heavy knightly lance in Western Europe, with newer types of heavy cavalry such as reiters and cuirassiers spurning the old one-use weapon and increasingly supplanting the older gendarme type Medieval cavalry. While many Renaissance captains such as Sir Roger Williams continued to espouse the virtues of the lance, many such as François de la Noue openly encouraged its abandonment in the face of the pistol's greater armor piecing power, handiness and greater general utility. At the same time the adoption of pike and shot tactic by most infantry forces would neuter much of the power of the lancer's breakneck charge, making them a non-cost effective type of military unit due to their expensive horses in comparison to cuirassiers and reiters, who usually charging only at a trot could make do with lower quality mounts. After the success of pistol-armed Huguenot heavy horse against their Royalist counterparts during the French Wars of Religion, most Western European powers started rearming their lancers with pistols, initially as an adjunct weapon and eventually as a replacement, with the Spanish retaining the lance the longest.
Only the Polish-Lithuanian Commonwealth with its far greater emphasis on cavalry warfare, large population of Szlachta nobility and general lower military technology level among its foes retained the lance to a large degree, with the famous winged Polish hussars having their glory period during the 16th and 17th century against a wide variety of enemy forces.
18th century.
The mounted lancer experienced a renaissance in the 18th and especially in the 19th century. This followed on the demise of the pike and of body armor during the 17th century, with the reintroduction of lances coming from Poland and Hungary. In both countries formations of lance-armed cavalry has been retained when they disappeared elsewhere in Europe. Lancers became especially prevalent during and after the Napoleonic Wars: a period when almost all the major European powers reintroduced the lance into their respective cavalry arsenals. Formations of uhlans and later other types of cavalry used 2 to 3 m lances as their main weapons. The lance was usually employed in initial charges in close formation, with sabers being used in the melees that followed.
Decline.
The Crimean War saw the use of the lance in the Charge of the Light Brigade. One of the four British regiments involved in the charge, plus the Russian cossacks who counter-attacked, were armed with this weapon.
After the Western introduction of the horse to Native Americans, the Plains Indians also took up the lance, probably independently, as American cavalry of the time were sabre- and pistol-armed, firing forward at full gallop. The natural adaptation of the throwing spear to a stouter thrusting and charging spear appears to be an evolutionary trend in the military use of the horse.
During the War of the Triple Alliance 1864-70, the Paraguayan cavalry made effective use of locally manufactured lances both of conventional design and of an antique pattern used by gauchos for cattle herding.
The Franco-Prussian War of 1870 saw the extensive deployment of cavalry armed with lances on both sides. However the opportunities for using this antique weapon effectively proved infrequent.
During the Second Boer War, British troops successfully used the lance on one occasion - against retreating Boers at the Battle of Elandslaagte (21 October 1899). However the Boers made effective use of trench warfare, field artillery and long range rifles from the beginning of the war. The combined effect was devastating, so that much of the British cavalry was deployed as mounted infantry, dismounting to fight on foot. For some years after the Boer War British lancer regiments carried the lance only for parades. However in 1908 the weapon was readopted for active service.
The Russian cavalry (except for Cossacks) discarded the lance in the late 19th century but in 1907 it was reissued for use by the front line of each squadron when charging in open formation. In its final form the Russian lance took the form of a long metal tube with a steel head and leather arm strap. It was intended as a shock weapon in the charge, to be dropped after impact and replaced by the sword for close combat in a melee. While demoralizing to an opponent, the lance was recognized as being an awkward encumbrance in forested regions.
The relative effectiveness of the lance and the sword as a principal weapon for mounted troops was an issue of dispute in the years immediately preceding World War I. Opponents of the lance argued that the weapon was clumsy, conspicuous, easily deflected and of no use at close quarters in a melee.
World War I.
Lances were still in use by the British, French, Russian, Belgian, Turkish, Italian and German armies at the outbreak of World War I. In initial cavalry skirmishes in France this antique weapon proved ineffective, German uhlans being "hampered by their long lances and a good many threw them away". With the advent of trench warfare, lances and the cavalry that carried them ceased to play a significant role.
Those armies which still retained lances as a service weapon at the end of World War I generally discarded them for all but ceremonial occasions during the 1920s. An exception was the Polish cavalry which retained the lance until 1936 but contrary to popular legend did not make use of it in World War II.
Use as flagstaff.
United States Cavalry and Canadian North-West Mounted Police used a lance-like shaft as a flagstaff. In 1886, the first official musical ride was performed in Regina, with this fine ceremonial lance playing a significant role in the choreography. The world's oldest continuous mounted police unit in the world, being the New South Wales Mounted Police, housed at Redfern Barracks, Sydney, Australia, carries a lance with a navy blue and white pennant in all ceremonial occasions.
Other weapons.
"Lance" is also the name given by some anthropologists to the light flexible javelins (technically, darts) thrown by atlatls (spear-throwing sticks), but these are usually called "atlatl javelins". Some were not much larger than arrows, and were typically feather-fletched like an arrow, and unlike the vast majority of spears and javelins (one exception would be several instances of the many types of ballista bolt, a mechanically-thrown spear).

</doc>
<doc id="48775" url="http://en.wikipedia.org/wiki?curid=48775" title="Halberd">
Halberd

A halberd (also called halbard, halbert or Swiss voulge) is a two-handed pole weapon that came to prominent use during the 14th and 15th centuries. The word "halberd" may come from the German words "Halm" (staff), and "Barte" (axe). In modern-day German, the weapon is called a "Hellebarde". The halberd consists of an axe blade topped with a spike mounted on a long shaft. It always has a hook or thorn on the back side of the axe blade for grappling mounted combatants. It is very similar to certain forms of the voulge in design and usage. The halberd was usually 1.5 to 1.8 metres (5 to 6 feet) long.
History.
The halberd was inexpensive to produce and very versatile in battle. As the halberd was eventually refined, its point was more fully developed to allow it to better deal with spears and pikes (also able to push back approaching horsemen), as was the hook opposite the axe head, which could be used to pull horsemen to the ground. Expert halberdiers were as deadly as any other weapon masters. A Swiss peasant used a halberd to kill Charles the Bold, the Duke of Burgundy—decisively ending the Burgundian Wars, literally in a single stroke. Researchers suspect that a halberd or a bill sliced through the back of King Richard III's skull at the battle of Bosworth.
The halberd was the primary weapon of the early Swiss armies in the 14th and early 15th centuries. Later, the Swiss added the pike to better repel knightly attacks and roll over enemy infantry formations, with the halberd, hand-and-a-half sword, or the dagger known as the "Schweizerdolch" used for closer combat. The German "Landsknechte", who imitated Swiss warfare methods, also used the pike, supplemented by the halberd—but their side arm of choice was a short sword called the "Katzbalger".
As long as pikemen fought other pikemen, the halberd remained a useful supplemental weapon for "push of pike", but when their position became more defensive, to protect the slow-loading arquebusiers and matchlock musketeers from sudden attacks by cavalry, the percentage of halberdiers in the pike units steadily decreased. The halberd all but disappeared as a rank-and-file weapon in these formations by the middle of the sixteenth century.
The halberd has been used as a court bodyguard weapon for centuries, and is still the ceremonial weapon of the Swiss Guard in the Vatican and the Spanish Royal Halberd Guards. The halberd was one of the polearms sometimes carried by lower-ranking officers in European infantry units in the 16th through 18th centuries. In the British army, sergeants continued to carry halberds until 1793, when they were replaced by pikes with cross bars. The 18th century halberd had, however, become simply a symbol of rank with no sharpened edge and insufficient strength to use as a weapon. It did, however, ensure that infantrymen drawn up in ranks stood correctly aligned with each other.

</doc>
<doc id="48778" url="http://en.wikipedia.org/wiki?curid=48778" title="Action theory (philosophy)">
Action theory (philosophy)

Action theory is an area in philosophy concerned with theories about the processes causing willful human bodily movements of a more or less complex kind. This area of thought has attracted the strong interest of philosophers ever since Aristotle's "Nicomachean Ethics" (Third Book). With the advent of psychology and later neuroscience, many theories of action are now subject to empirical testing.
Philosophical action theory, or the philosophy of action, should not be confused with sociological theories of social action, such as the action theory established by Talcott Parsons.
What is left over if I subtract the fact that my arm goes up from the fact that I raise my arm?
Ludwig Wittgenstein, "Philosophical Investigations" §621
Overview.
Basic action theory typically describes action as behavior caused by an "agent" in a particular "situation". The agent's "desires" and "beliefs" (e.g. my wanting a glass of water and believing the clear liquid in the cup in front of me is water) lead to bodily behavior (e.g. reaching over for the glass). In the simple theory (see Donald Davidson), the desire and belief jointly cause the action. Michael Bratman has raised problems for such a view and argued that we should take the concept of intention as basic and not analyzable into beliefs and desires. 
In some theories a desire plus a belief about the means of satisfying that desire are always what is behind an action. Agents aim, in acting, to maximize the satisfaction of their desires. Such a theory of prospective rationality underlies much of economics and other social sciences within the more sophisticated framework of Rational Choice. However, many theories of action argue that rationality extends far beyond calculating the best means to achieve one's ends. For instance, a belief that I ought to do X, in some theories, can directly cause me to do X without my having to want to do X (i.e. have a desire to do X). Rationality, in such theories, also involves responding correctly to the reasons an agent perceives, not just acting on wants. 
While action theorists generally employ the language of causality in their theories of what the nature of action is, the issue of what causal determination comes to has been central to controversies about the nature of free will. 
Conceptual discussions also revolve around a precise definition of action in philosophy. Scholars may disagree on which bodily movements fall under this category, e.g. whether thinking should be analysed as action, and how complex actions involving several steps to be taken and diverse intended consequences are to be summarised or decomposed.
Discussion.
For example, throwing a ball is an instance of action; it involves an intention, a goal, and a bodily movement guided by the agent. On the other hand, catching a cold is not considered an action because it is something which happens "to" a person, not something done "by" one. Generally an agent doesn't intend to catch a cold or engage in bodily movement to do so (though we might be able to conceive of such a case). Other events are less clearly defined as actions or not. For instance, distractedly drumming ones fingers on the table seems to fall somewhere in the middle. Deciding to do something might be considered a mental action by some. However, others think it is not an action unless the decision is carried out. Unsuccessfully trying to do something might also not be considered an action for similar reasons (for e.g. lack of bodily movement). It is contentious whether believing, intending, and thinking are actions since they are mental events. 
Some would prefer to define actions as requiring bodily movement (see behaviorism). The side effects of actions are considered by some to be part of the action; in an example from Anscombe's manuscript Intention, pumping water can also be an instance of poisoning the inhabitants. This introduces a moral dimension to the discussion (see also Moral agency). If the poisoned water resulted in a death, that death might be considered part of the action of the agent that pumped the water. Whether a side effect is considered part of an action is especially unclear in cases in which the agent isn't aware of the possible side effects. For example, an agent that accidentally cures a person by administering a poison he was intending to kill him with. 
A primary concern of the philosophy of action is to analyze the nature of actions and distinguish them from similar phenomena. Other concerns include individuating actions, explaining the relationship between actions and their effects, explaining how an action is related to the beliefs and desires which cause and/or justify it (see practical reason), as well as examining the nature of agency. A primary concern is the nature of free will and whether actions are determined by the mental states that precede them (see determinism). Some philosophers (e.g. Donald Davidson) have argued that the mental states the agent invokes as justifying his action are physical states that cause the action. Problems have been raised for this view because the mental states seem to be reduced to mere physical causes. Their mental properties don't seem to be doing any work. If the reasons an agent cites as justifying his action, however, are not the cause of the action, they must explain the action in some other way or be causally impotent.

</doc>
<doc id="48780" url="http://en.wikipedia.org/wiki?curid=48780" title="Battle of Chancellorsville">
Battle of Chancellorsville

The Battle of Chancellorsville was a major battle of the American Civil War, and the principal engagement of the Chancellorsville Campaign. It was fought from April 30 to May 6, 1863, in Spotsylvania County, Virginia, near the village of Chancellorsville. Two related battles were fought nearby on May 3 in the vicinity of Fredericksburg. The campaign pitted Union Army Maj. Gen. Joseph Hooker's Army of the Potomac against an army less than half its size, Gen. Robert E. Lee's Confederate Army of Northern Virginia. Chancellorsville is known as Lee's "perfect battle" because his risky decision to divide his army in the presence of a much larger enemy force resulted in a significant Confederate victory. The victory, a product of Lee's audacity and Hooker's timid decision making, was tempered by heavy casualties and the mortal wounding of Lt. Gen. Thomas J. "Stonewall" Jackson to friendly fire, a loss that Lee likened to "losing my right arm."
The Chancellorsville Campaign began with the crossing of the Rappahannock River by the Union army on the morning of April 27, 1863. Union cavalry under Maj. Gen. George Stoneman began a long distance raid against Lee's supply lines at about the same time. This operation was completely ineffectual. Crossing the Rapidan River via Germanna and Ely's Fords, the Federal infantry concentrated near Chancellorsville on April 30. Combined with the Union force facing Fredericksburg, Hooker planned a double envelopment, attacking Lee from both his front and rear.
On May 1, Hooker advanced from Chancellorsville toward Lee, but the Confederate general split his army in the face of superior numbers, leaving a small force at Fredericksburg to deter Maj. Gen. John Sedgwick from advancing, while he attacked Hooker's advance with about 4/5ths of his army. Despite the objections of his subordinates, Hooker withdrew his men to the defensive lines around Chancellorsville, ceding the initiative to Lee. On May 2, Lee divided his army again, sending Stonewall Jackson's entire corps on a flanking march that routed the Union XI Corps. While performing a personal reconnaissance in advance of his line, Jackson was wounded by fire from his own men, and Maj. Gen. J.E.B. Stuart temporarily replaced him as corps commander.
The fiercest fighting of the battle—and the second bloodiest day of the Civil War—occurred on May 3 as Lee launched multiple attacks against the Union position at Chancellorsville, resulting in heavy losses on both sides. That same day, Sedgwick advanced across the Rappahannock River, defeated the small Confederate force at Marye's Heights in the Second Battle of Fredericksburg, and then moved to the west. The Confederates fought a successful delaying action at the Battle of Salem Church and by May 4 had driven back Sedgwick's men to Banks's Ford, surrounding them on three sides. Sedgwick withdrew across the ford early on May 5, and Hooker withdrew the remainder of his army across U.S. Ford the night of May 5–6. The campaign ended on May 7 when Stoneman's cavalry reached Union lines east of Richmond.
Background.
Union attempts against Richmond.
In the Eastern Theater of the American Civil War, the basic offensive plan for the Union had been to advance and seize the Confederate capital, Richmond, Virginia. In the first two years of the war, four major attempts had failed: the first foundered just miles away from Washington, D.C., at the First Battle of Bull Run (First Manassas) in July 1861. Maj. Gen. George B. McClellan's Peninsula Campaign took an amphibious approach, landing his Army of the Potomac on the Virginia Peninsula in the spring of 1862 and coming within 6 mi of Richmond before being turned back by Gen. Robert E. Lee in the Seven Days Battles. That summer, Maj. Gen. John Pope's Army of Virginia was defeated at the Second Battle of Bull Run. In December 1862, Maj. Gen. Ambrose Burnside commanded the Army of the Potomac and attempted to reach Richmond by way of Fredericksburg, Virginia, where he was defeated at the Battle of Fredericksburg. This string of Union defeats was interrupted in September 1862 when Lee moved into Maryland and his campaign was turned back by McClellan at the Battle of Antietam, but this represented no threat to Richmond.
Shakeup in the Army of the Potomac.
In January 1863, the Army of the Potomac, following the Battle of Fredericksburg and the humiliating Mud March, suffered from rising desertions and plunging morale. Maj. Gen. Ambrose Burnside decided to conduct a mass purge of the Army of the Potomac's leadership, eliminating a number of generals who he felt were responsible for the disaster at Fredericksburg. In reality, he had no power to dismiss anyone without the approval of Congress. Predictably, Burnside's purge went nowhere, and he offered President Abraham Lincoln his resignation from command of the Army of the Potomac. He even offered to resign entirely from the Army, but the president persuaded him to stay, transferring him to the Western Theater, where he became commander of the Department of the Ohio. Burnside's former command, the IX Corps, was transferred to the Virginia Peninsula, a movement that prompted the Confederates to detach troops from Lee's army under Lt. Gen. James Longstreet, a decision that would be consequential in the upcoming campaign.
Abraham Lincoln had become convinced that the appropriate objective for his Eastern army was the army of Robert E. Lee's, not any geographic features such as a capital city, but he and his generals knew that the most reliable way to bring Lee to a decisive battle was to threaten his capital. Lincoln tried a fifth time with a new general on January 25, 1863—Maj. Gen. Joseph Hooker, a man with a pugnacious reputation who had performed well in previous subordinate commands.
With Burnside's departure, Maj. Gen. William B. Franklin left as well. Franklin had been a staunch supporter of George B. McClellan and refused to serve under Hooker, because he disliked him personally and also because he was senior to Hooker in rank. Maj. Gen. Edwin V. Sumner stepped down due to old age (he was 65) and poor health. He was reassigned to a command in Missouri, but died before he could assume it. Brig. Gen. Daniel Butterfield was reassigned from command of the V Corps to be Hooker's chief of staff.
Hooker embarked on a reorganization of the army, doing away with Burnside's grand division system, which Hooker considered unwieldy; he also no longer had sufficient senior officers on hand that he could trust to command multi-corps operations. He organized the cavalry into a separate corps under the command of Brig. Gen. George Stoneman (who had commanded the III Corps at Fredericksburg). But while he concentrated the cavalry into a single organization, he dispersed his artillery battalions to the control of the infantry division commanders, removing the coordinating influence of the army's artillery chief, Brig. Gen. Henry J. Hunt.
During the spring of 1863, Hooker established a reputation as an outstanding administrator and restored the morale of his soldiers, which had plummeted to a new low under Burnside. Among his changes were fixes to the daily diet of the troops, camp sanitary changes, improvements and accountability of the quartermaster system, addition of and monitoring of company cooks, several hospital reforms, an improved furlough system, orders to stem rising desertion, improved drills, and stronger officer training.
Opposing forces.
The Army of the Potomac, commanded by Maj. Gen. Joseph Hooker, had 133,868 men and 413 guns organized as follows:
Gen. Robert E. Lee's Army of Northern Virginia fielded 60,892 men and 220 guns, organized as follows:
The Chancellorsville Campaign was one of the most lopsided clashes of the war, with the Union's effective fighting force more than twice the Confederates', the greatest imbalance during the war in Virginia. Hooker's army was much better supplied and was well-rested after several months of inactivity. Lee's forces, on the other hand, were poorly provisioned and were scattered all over the state of Virginia. Some 15,000 men of Longstreet's Corps had previously been detached and stationed near Norfolk in order to block a potential threat to Richmond from Federal troops stationed at Fort Monroe and Newport News on the Peninsula, as well as at Norfolk and Suffolk. In light of the continued Federal inactivity, by late March Longstreet's primary assignment became that of requisitioning provisions for Lee's forces from the farmers and planters of North Carolina and Virginia. As a result of this the two divisions of Maj. Gen. John Bell Hood and Maj. Gen. George Pickett were 130 mi away from Lee's army and would take a week or more of marching to reach it in an emergency. After nearly a year of campaigning, allowing these troops to slip away from his immediate control was Lee's gravest miscalculation. Although he hoped to be able to call on them, these men would not arrive in time to aid his outnumbered forces.
Intelligence and strategy.
My plans are perfect. May God have mercy on General Lee for I will have none.
 Maj. Gen. Joseph Hooker
Hooker took advantage of improved military intelligence about the positioning and capabilities of the opposing army, superior to that available to his predecessors in army command. His chief of staff, Butterfield, commissioned Col. George H. Sharpe from the 120th New York regiment to organize a new Bureau of Military Intelligence in the Army of the Potomac, part of the provost marshal function under Brig. Gen. Marsena R. Patrick. Previously, intelligence gatherers, such as Allan Pinkerton and his detective agency, gathered information only by interrogating prisoners, deserters, "contrabands" (slaves), and refugees. The new BMI added other sources including infantry and cavalry reconnaissance, spies, scouts, signal stations, and an aerial balloon corps. As he received the more complete information correlated from these additional sources, Hooker realized that if he were to avoid the bloodbath of direct frontal attacks, which were features of the battles of Antietam and, more recently, Fredericksburg, he could not succeed in his crossing of the Rappahannock "except by stratagem."
Hooker's army faced Lee across the Rappahannock from its winter quarters in Falmouth and around Fredericksburg. Hooker developed a strategy that was, on paper, superior to those of his predecessors. He planned to send his 10,000 cavalrymen under Maj. Gen. George Stoneman to cross the Rappahannock far upstream and raid deep into the Confederate rear areas, destroying crucial supply depots along the railroad from the Confederate capital in Richmond to Fredericksburg, which would cut Lee's lines of communication and supply. Hooker assumed that Lee would react to this threat by abandoning his fortified positions on the Rappahannock and withdrawing toward his capital. At that time, Hooker's infantry would cross the Rappahannock in pursuit, attacking Lee when he was moving and vulnerable. Stoneman attempted to execute this turning movement on April 13, but heavy rains made the river crossing site at Sulphur Spring impassable. President Lincoln lamented, "I greatly fear it is another failure already." Hooker was forced to create a new plan for a meeting with Lincoln, Secretary of War Edwin M. Stanton, and general in chief Henry W. Halleck in Aquia on April 19.
On the whole I think this plan was decidedly the best strategy conceived in any of the campaigns ever set foot against [the Army of Northern Virginia]. And the execution of it was, also, excellently managed, up to the morning of May 1st.
 Confederate artillery officer Edward Porter Alexander
Hooker's second plan was to launch both his cavalry and infantry simultaneously in a bold double envelopment of Lee's army. Stoneman's cavalry would make a second attempt at its deep strategic raid, but at the same time, 42,000 men in three corps (V, XI, XII Corps) would stealthily march to cross the Rappahannock upriver at Kelly's Ford. They would then proceed south and cross the Rapidan at Germanna and Ely's Ford, concentrate at the Chancellorsville crossroads, and attack Lee's army from the west. While they were under way, 10,000 men in two divisions from the II Corps would cross at the U.S. Ford and join with the V Corps in pushing the Confederates away from the river. The second half of the double envelopment was to come from the east: 40,000 men in two corps (I and VI Corps, under the overall command of John Sedgwick) would cross the Rappahannock below Fredericksburg and threatened to attack Stonewall Jackson's position on the Confederate right flank. The remaining 25,000 men (III Corps and one division of the II Corps) would remain visible in their camps at Falmouth to divert Confederate attention from the turning movement. Hooker anticipated that Lee would either be forced to retreat, in which case he would be vigorously pursued, or he would be forced to attack the Union Army on unfavorable terrain.
One of the defining characteristics of the battlefield was a dense woodland south of the Rapidan known locally as the "Wilderness of Spotsylvania". The area had once been an open broadleaf forest, but during colonial times the trees were gradually cut down to make charcoal for local pig iron furnaces. When the supply of wood was exhausted, the furnaces were abandoned and secondary forest growth developed, creating a dense mass of brambles, thickets, vines, and low-lying vegetation. Catharine Furnace, abandoned in the 1840s, had recently been reactivated to produce iron for the Confederate war effort. This area was largely unsuitable for the deployment of artillery and the control of large infantry formations, which would nullify some of the Union advantage in military power. It was important for Hooker's plan that his men move quickly out of this area and attack Lee in the open ground to the east. There were three primary roads available for this west-to-east movement: the Orange Plank Road, the Orange Turnpike, and the River Road.
The Confederate dispositions were as follows: the Rappahannock line at Fredericksburg was occupied by Longstreet's First Corps division of Lafayette McLaws on Marye's Heights, with Jackson's entire Second Corps to their right. Early's division was at Prospect Hill and the divisions of Rodes, Hill, and Colston extended the Confederate right flank along the river almost to Skinker's Neck. The other division present from Longstreet's Corps, Anderson's, guarded the river crossings on the left flank. Stuart's cavalry was largely in Culpeper County near Kelly's Ford, beyond the infantry's left flank.
April 27–30: Movement to battle.
On April 27–28, the initial three corps of the Army of the Potomac began their march under the leadership of Slocum. They crossed the Rappahannock and Rapidan rivers as planned and began to concentrate on April 30 around the hamlet of Chancellorsville, which was little more than a single large, brick mansion at the junction of the Orange Turnpike and Orange Plank Road. Built in the early 19th century, it had been used as an inn on the turnpike for many years, but now served as a home for the Frances Chancellor family. (Some of the family remained in the house during the battle.) Hooker arrived late in the afternoon on April 30 and made the mansion his headquarters. Stoneman's cavalry began on April 30 its second attempt to reach Lee's rear areas. The two II Corps divisions crossed at U.S. Ford on April 30 without opposition. By dawn on April 29, pontoon bridges spanned the Rappahannock south of Fredericksburg and Sedgwick's force began to cross. Pleased with the success of the operation so far, and realizing that the Confederates were not vigorously opposing the river crossings, Hooker ordered Sickles to begin the movement of the III Corps from Falmouth the night of April 30 – May 1. By May 1, Hooker had approximately 70,000 men concentrated in and around Chancellorsville.
In his Fredericksburg headquarters, Lee was initially in the dark about the Union intentions and he suspected that the main column under Slocum was heading towards Gordonsville. Jeb Stuart's cavalry was cut off at first by Stoneman's departure on April 30, but they were soon able to move freely around the army's flanks on their reconnaissance missions after almost all their Union counterparts had left the area. As Stuart's intelligence information about the Union river crossings began to arrive, Lee did not react as Hooker had anticipated. He decided to violate one of the generally accepted principles of war and divide his force in the face of a superior enemy, hoping that aggressive action would allow him to attack and defeat a portion of Hooker's army before it could be fully concentrated against him. He became convinced that Sedgwick's force would demonstrate against him, but not become a serious threat, so he ordered about 4/5 of his army to meet the challenge from Chancellorsville. He left behind a brigade under Brig. Gen. William Barksdale on heavily fortified Marye's Heights behind Fredericksburg and one division under Maj. Gen. Jubal A. Early, on Prospect Hill south of the town. These roughly 11,000 men and 56 guns would attempt to resist any advance by Sedgwick's 40,000. He ordered Stonewall Jackson to march west and link up with Maj. Gen. Richard H. Anderson's division, which had pulled back from the river crossings they were guarding and began digging earthworks on a north-south line between the Zoan and Tabernacle churches. McLaws's division was ordered from Fredericksburg to join Anderson. This would amass 40,000 men to confront Hooker's movement east from Chancellorsville. Fortunately for the Confederates, heavy fog along the Rappahannock masked some of these westward movements and Sedgwick chose to wait until he could determine the enemy's intentions.
Battle.
May 1: Hooker loses his nerve.
Jackson's men began marching west to join with Anderson before dawn on May 1. Jackson himself met with Anderson near Zoan Church at 8 a.m., finding that McLaws's division had already arrived to join the defensive position. But Stonewall Jackson was not in a defensive mood. He ordered an advance at 11 a.m. along two roads toward Chancellorsville: McLaws's division and the brigade of Brig. Gen. William Mahone on the Turnpike, and Anderson's other brigades and Jackson's arriving units on the Plank Road. At about the same time, Hooker ordered his men to advance on three roads to the east: two divisions of Meade's V Corps (Griffin and Humphreys) on the River Road to uncover Banks's Ford, and the remaining division (Sykes) on the Turnpike; and Slocum's XII Corps on the Plank Road, with Howard's XI Corps in close support. Couch's II Corps was placed in reserve, where it would be soon joined by Sickles's III Corps.
The first shots of the Battle of Chancellorsville were fired at 11:20 a.m. as the armies collided. McLaws's initial attack pushed back Sykes's division, but the Union general organized a counterattack that recovered the lost ground. Anderson then sent a brigade under Brig. Gen. Ambrose Wright up an unfinished railroad south of the Plank Road, around the right flank of Slocum's corps. This would normally be a serious problem, but Howard's XI Corps was advancing from the rear and could deal with Wright. Sykes's division had proceeded farther forward than Slocum on his right, leaving him in an exposed position, which forced him to conduct an orderly withdrawal at 2 p.m. to take up a position behind Hancock's division of the II Corps, which was ordered by Hooker to advance and help repulse the Confederate attack. Meade's other two divisions made good progress on the River Road and were approaching their objective, Banks's Ford.
Modern attempts to rehabilitate and fumigate Joe Hooker's reputation usually and remarkably employ special pleading about the difficulties of moving in the Wilderness. Such arguments actually emphasize the salient factor on May 1: Getting out of that wilderness of course was the very essence of the general's needs. When he abandoned the chance to reach that desirable goal, Hooker at once passed the initiative, with all of its advantages, to Lee. The Confederate would make superb use of the opportunity.
Robert K. Krick, "Lee's Greatest Victory"
Despite being in a potentially favorable situation, Hooker halted his brief offensive. His actions may have demonstrated his lack of confidence in handling the complex actions of such a large organization for the first time (he had been an effective and aggressive division and corps commander in previous battles), but he had also decided before beginning the campaign that he would fight the battle defensively, forcing Lee, with his small army, to attack Hooker's larger one. At the [First] Battle of Fredericksburg (December 13, 1862), the Union army had done the attacking and met with a bloody defeat. Hooker knew Lee could not sustain such a defeat and keep an effective army in the field, so he ordered his men to withdraw back into the Wilderness and take a defensive position around Chancellorsville, daring Lee to attack him or retreat with superior forces at his back. He confused matters by issuing a second order to his subordinates to hold their positions until 5 p.m., but by the time it was received, most of the Union units had begun their rearward movements. That evening, Hooker sent a message to his corps commanders, "The major general commanding trusts that a suspension in the attack to-day will embolden the enemy to attack him."
The retrograde movement had prepared me for something of the kind, but to hear from [Hooker's] own lips that the advantages gained by the successful marches of his lieutenants were to culminate in fighting a defensive battle in that nest of thickets was too much, and I retired from his presence with the belief that my commanding general was a whipped man.
 Union Maj. Gen. Darius N. Couch
Hooker's subordinates were surprised and outraged by the change in plans. They saw that the position they were fighting for near the Zoan Church was relatively high ground and offered an opportunity for the infantry and artillery to deploy outside the constraints of the Wilderness. Meade exclaimed, "My God, if we can't hold the top of the hill, we certainly can't hold the bottom of it!" Viewing through the lens of hindsight, some of the participants and many modern historians judged that Hooker effectively lost the campaign on May 1. Stephen W. Sears observed, however, that Hooker's concern was based on more than personal timidity. The ground being disputed was little more than a clearing in the Wilderness, to which access was available by only two narrow roads. The Confederate response had swiftly concentrated the aggressive Stonewall Jackson's corps against his advancing columns such that the Federal army was outnumbered in that area, about 48,000 to 30,000, and would have difficulty maneuvering into effective lines of battle. Meade's two divisions on the River Road were too far separated to support Slocum and Sykes, and reinforcements from the rest of the II Corps and the III Corps would be too slow in arriving.
As the Union troops dug in around Chancellorsville that night, creating log breastworks, faced with abatis, Lee and Stonewall Jackson met at the intersection of the Plank Road and the Furnace Road to plan their next move. Jackson believed that Hooker would retreat across the Rappahannock, but Lee assumed that the Union general had invested too much in the campaign to withdraw so precipitously. If the Federal troops were still in position on May 2, Lee would attack them. As they discussed their options, cavalry commander J.E.B. Stuart arrived with an intelligence report from his subordinate, Brig. Gen. Fitzhugh Lee. Although Hooker's left flank was firmly anchored by Meade's V Corps on the Rappahannock, and his center was strongly fortified, his right flank was "in the air." Howard's XI Corps was camped on the Orange Turnpike, extending past Wilderness Church, and was vulnerable to a flanking attack. Investigations of a route to be used to reach the flank identified the proprietor of Catharine Furnace, Charles C. Wellford, who showed Jackson's cartographer, Jedediah Hotchkiss, a recently constructed road through the forest that would shield marchers from the observation of Union pickets. Lee directed Jackson to make the flanking march, a maneuver similar to the one that had been so successful prior to the Second Battle of Bull Run (Second Manassas). An account by Hotchkiss recalls that Lee asked Jackson how many men he would take on the flanking march and Jackson replied, "with my whole command."
May 2: Jackson's flank attack.
Early on the morning of May 2, Hooker began to realize that Lee's actions on May 1 had not been constrained by the threat of Sedgwick's force at Fredericksburg, so no further deception was needed on that front. He decided to summon the I Corps of Maj. Gen. John F. Reynolds to reinforce his lines at Chancellorsville. His intent was that Reynolds would form up to the right of the XI Corps and anchor the Union right flank on the Rapidan River. Given the communications chaos of May 1, Hooker was under the mistaken impression that Sedgwick had withdrawn back across the Rappahannock and, based on this, that the VI Corps should remain on the north bank of the river across from the town, where it could protect the army's supplies and supply line. (In fact, both Reynolds and Sedgwick were still west of the Rappahannock, south of the town.) Hooker sent his orders at 1:55 a.m., expecting that Reynolds would be able to start marching before daylight, but problems with his telegraph communications delayed the order to Fredericksburg until just before sunrise. Reynolds was forced to make a risky daylight march. By the afternoon of May 2, when he should have been digging in on the Union right at Chancellorsville, he was still marching to the Rappahannock.
Meanwhile, for the second time, Lee was dividing his army. Jackson would lead his Second Corps of 28,000 men around to attack the Union right flank while Lee exercised personal command of the remaining two divisions, about 13,000 men and 24 guns facing the 70,000 Union troops at Chancellorsville. For the plan to work, several things had to happen. First, Jackson had to make a 12-mile (19 km) march via roundabout roads to reach the Union right, and he had to do it undetected. Second, Hooker had to stay tamely on the defensive. Third, Early would have to keep Sedgwick bottled up at Fredericksburg, despite the four-to-one Union advantage there. And when Jackson launched his attack, he had to hope that the Union forces were unprepared.
All of these conditions were met. Confederate cavalry under Stuart kept most Union forces from spotting Jackson on his long flank march, which started between 7 and 8 a.m. and lasted until midafternoon. Several Confederate soldiers saw the Union observation balloon "Eagle" soaring overhead and assumed that they could likewise be seen, but no such report was sent to headquarters. When men of the III Corps spotted a Confederate column moving through the woods, their division commander, Brig. Gen. David B. Birney, ordered his artillery to open fire, but this proved little more than harassment. The corps commander, Sickles, rode to Hazel Grove to see for himself and he reported after the battle that his men observed the Confederates passing for over three hours.
When Hooker received the report about the Confederate movement, he thought that Lee might be starting a retreat, but he also realized that a flanking march might be in progress. He took two actions. First, he sent a message at 9:30 a.m. to Maj. Gen. Oliver O. Howard on his right flank: "We have good reason to suppose the enemy is moving to our right. Please advance your pickets for purposes of observation as far as may be safe in order to obtain timely information of their approach." At 10:50 a.m., Howard replied that he was "taking measures to resist an attack from the west." Hooker's second action was to send orders to Sedgwick ("attack the enemy in his front" at Fredericksburg if "an opportunity presents itself with a reasonable expectation of success") and Sickles ("advance cautiously toward the road followed by the enemy, and harass the movement as much as possible"). Sedgwick did not take action from the discretionary orders. Sickles, however, was enthusiastic when he received the order at noon. He sent Birney's division, flanked by two battalions of Col. Hiram Berdan's U.S. sharpshooters, south from Hazel Grove with orders to pierce the column and gain possession of the road. But the action came too late. Jackson had ordered the 23rd Georgia Infantry to guard the rear of the column and they resisted the advance of Birney and Berdan at Catherine Furnace. The Georgians were driven south and they made a stand at the same unfinished railroad bed used by Wright's Brigade the day before. They were overwhelmed by 5 p.m. and most were captured. Two brigades from A.P. Hill's division turned back from the flanking march and prevented any further damage to Jackson's column, which by now had left the area.
Most of Jackson's men were unaware of the small action at the rear of their column. As they marched north on Brock Road, Jackson was prepared to turn right on the Orange Plank Road, from which his men would attack the Union lines at around Wilderness Church. However, it became apparent that this direction would lead to essentially a frontal assault against Howard's line. Fitzhugh Lee met Jackson and they ascended a hill with a sweeping view of the Union position and Jackson was delighted to see that Howard's men were resting, unaware of the impending Confederate threat. Although by now it was 3 p.m., Jackson decided to march his men two miles farther and turn right on the Turnpike instead, allowing him to strike the unprotected flank directly. The attack formation consisted of two lines—the divisions of Brig. Gens. Robert E. Rodes and Raleigh E. Colston—stretching almost a mile on either side of the turnpike, separated by 200 yards, followed by a partial line with the arriving division of A.P. Hill.
Significant contributors to the impending Union disaster were the nature of the Union XI Corps and the incompetent performance of its commander, Maj. Gen. Oliver O. Howard. Howard failed to make any provision for defending against a surprise attack, even though Hooker had ordered him to do so. The Union right flank was not anchored on any natural obstacle, and the only defenses against a flank attack consisted of two cannons pointing out into the Wilderness. Also, the XI Corps was an organization with poor morale. The corps had originally been commanded by Brig. Gen. Franz Sigel, a political general appointed because of his abolitionist views. Although inept as a commander, he was very popular with the Germans and the immigrant soldiers had a saying "I fights mit Sigel". During the spring of 1862, Sigel's corps was detached from the main Army of the Potomac and placed in the Shenandoah Valley, where it was defeated by Stonewall Jackson's forces at Cross Keys. After the Peninsula Campaign, it was attached to Maj. Gen. John Pope's Army of Virginia where it fared no better, delivering a poor performance at Second Bull Run. The XI Corps did not participate in the Antietam or Fredericksburg campaigns, and after Hooker took command of the army Sigel was dismissed and replaced by Howard. He dismissed a number of popular generals and replaced them with men like Brig. Gen. Francis C. Barlow, a ferocious disciplinarian who was known for swatting stragglers with the blunt end of his sword. Many of the immigrants had poor English language skills and they were subjected to ethnic friction with the rest of the Army of the Potomac, where all non-Irish immigrants were referred to as "Germans". In fact, half the XI Corps consisted of native-born Americans, mostly from the Midwest, but it was the immigrants with whom the corps came to be associated. The corps' readiness was poor as well. Of the 23 regiments, eight had no combat experience, and the remaining 15 had never fought on the winning side of a battle. And although many of the immigrants had served in European armies, they tended to not perform well under the loose discipline of the American volunteer military. Because of these factors, Hooker had placed the XI Corps on his flank and did not have any major plans for it except as a reserve or mopping-up force after the main fighting was over.
Around 5:30 p.m., Jackson's 21,500 men exploded out of the woods screaming the Rebel Yell. Most of the men of the XI Corps were sitting down for supper and had their rifles unloaded and stacked. Their first clue to the impending onslaught was the observation of numerous animals, such as rabbits and foxes, fleeing in their direction. After the division of Brig. Gen. Charles Devens, Jr., collapsed, Maj. Gen. Carl Schurz ordered his division to shift from an east-west alignment to north-south, which they did with amazing precision and speed. However, they were overlapped significantly on both sides by the Confederate onslaught and Schurz ordered a retreat at 6:30 p.m. General Howard partially redeemed his inadequate performance prior to the battle by his personal bravery in attempting to rally the troops. He stood shouting and waving a flag held under the stump of his amputated arm (lost at the Battle of Seven Pines in 1862), ignoring the danger of the heavy rifle fire, but he could only gather small pockets of soldiers to resist before his corps disintegrated. Several thousand of Howard's men gathered at Fairview, a clearing across the road from the Chancellor mansion, where 37 guns of the XII Corps artillery brought Rodes's now-disorganized division to a standstill at 7:15. Hooker urged the III Corps division of Maj. Gen. Hiram G. Berry to defend a line a half mile from Chancellorsville with their bayonets, but by that time, the momentum of the attack had passed.
By nightfall, the Confederate Second Corps had advanced more than 1.25 miles, to within sight of Chancellorsville, but darkness and confusion were taking their toll. The attackers were almost as disorganized as the routed defenders. Although the XI Corps had been defeated, it would be incorrect to characterize the action as thousands of men simply fleeing for their lives. The corps suffered nearly 2,500 casualties (259 killed, 1,173 wounded, and 994 missing or captured), about one quarter of its strength, including 12 of 23 regimental commanders, which suggests that they fought fiercely during their retreat. Jackson's force was now separated from Lee's men only by Sickles's corps, which had been separated from the main body of the army after its foray attacking Jackson's column earlier in the afternoon. By 9 p.m., Sickles's men had struggled back to Hazel Grove, but their day was not finished. Between 11 p.m. and midnight, Sickles organized an assault north from Hazel Grove toward the Plank Road, but called it off when his men began suffering artillery and rifle fire from the XII Corps.
Stonewall Jackson wanted to press his advantage before Hooker and his army could regain their bearings and plan a counterattack, which might still succeed because of the sheer disparity in numbers. He rode out onto the Plank Road that night to determine the feasibility of a night attack by the light of the full moon, traveling beyond the farthest advance of his men. When one of his staff officers warned him about the dangerous position, Jackson replied, "The danger is all over. The enemy is routed. Go back and tell A.P. Hill to press right on." As he and his staff started to return, they were incorrectly identified as Union cavalry by men of the 18th North Carolina Infantry, who hit Jackson with friendly fire. Jackson's three bullet wounds were not in themselves life-threatening, but his left arm was broken and had to be amputated. He contracted pneumonia and died on May 10. His death was a devastating loss for the Confederacy. Some historians and participants—particularly those of the postbellum Lost Cause movement—attribute the Confederate defeat at Gettysburg two months later to Jackson's absence.
May 3: Chancellorsville.
Despite the fame of Stonewall Jackson's victory on May 2, it did not result in a significant military advantage for the Army of Northern Virginia. Howard's XI Corps had been defeated, but the Army of the Potomac remained a potent force and Reynolds's I Corps had arrived overnight, which replaced Howard's losses. About 76,000 Union men faced 43,000 Confederate at the Chancellorsville front. The two halves of Lee's army at Chancellorsville were separated by Sickles's III Corps, which occupied a strong position on high ground at Hazel Grove. Unless Lee could devise a plan to eject Sickles from Hazel Grove and combine the two halves of his army, he would have little chance of success in assaulting the formidable Union earthworks around Chancellorsville. Fortunately for Lee, Joseph Hooker inadvertently cooperated. Early on May 3, Hooker ordered Sickles to move from Hazel Grove to a new position on the Plank Road. As they were withdrawing, the trailing elements of Sickles's corps was attacked by the Confederate brigade of Brig. Gen. James J. Archer, which captured about 100 prisoners and four cannons. Hazel Grove was soon turned into a powerful artillery platform with 30 guns under Col. Porter Alexander.
After Jackson was wounded on May 2, command of the Second Corps fell to his senior division commander, Maj. Gen. A.P. Hill. Hill was soon wounded himself, however. He consulted with Brig. Gen. Robert E. Rodes, the next most senior general in the corps, and Rodes acquiesced in Hill's decision to summon Maj. Gen. J.E.B. Stuart to take command, notifying Lee after the fact. Brig. Gen. Henry Heth replaced Hill in division command. Although cavalryman Stuart had never commanded infantry before, he would turn in a very credible performance at Chancellorsville. By the morning of May 3, the Union line resembled a giant horseshoe. The center was held by the III, XII, and II Corps. On the left were the remnants of the XI Corps, and the right was held by the V and I Corps. On the western side of the Chancellorsville salient, Stuart organized his three divisions to straddle the Plank Road: Heth's in the advance, Colston's 300–500 yards behind, and Rodes's, whose men had done the hardest fighting on May 2, near the Wilderness Church. The attack began about 5:30 a.m. and was aided by the newly installed artillery at Hazel Grove, and by simultaneous attacks by the divisions of Anderson and McLaws from the south and southeast. The Confederates were resisted fiercely by the Union troops behind strong earthworks, and the fighting on May 3 was the heaviest of the campaign. The initial waves of assaults by Heth and Colston gained a little ground, but were beaten back by Union counterattacks.
 At Hazel Grove, in short, the finest artillerists of the Army of Northern Virginia were having their greatest day. They had improved guns, better ammunition and superior organization. With the fire of battle shining through his spectacles, William Pegram rejoiced. "A glorious day, Colonel," he said to Porter Alexander, "a glorious day!"
 Douglas Southall Freeman, "Lee's Lieutenants"
Rodes sent his men in last and this final push, along with the excellent performance of the Confederate artillery, carried the morning battle. Chancellorsville was the only occasion in the war in Virginia in which Confederate gunners held a decided advantage over their Federal counterparts. Confederate guns on Hazel Grove were joined by 20 more on the Plank Road to duel effectively with the Union guns on neighboring Fairview Hill, causing the Federals to withdraw as ammunition ran low and Confederate infantrymen picked off the gun crews. Fairview was evacuated at 9:30 a.m., briefly recaptured in a counterattack, but by 10 a.m. Hooker ordered it abandoned for good. The loss of this artillery platform doomed the Union position at the Chancellorsville crossroads as well, and the Army of the Potomac began a fighting retreat to positions circling United States Ford. The soldiers of the two halves of Lee's army reunited shortly after 10 a.m. before the Chancellor mansion, wildly triumphant as Lee arrived on Traveller to survey the scene of his victory.
 Lee's presence was the signal for one of those uncontrollable bursts of enthusiasm which none can appreciate who has not witnessed them. The fierce soldiers, with their faces blackened with the smoke of battle, the wounded crawling with feeble limbs from the fury of the devouring flames, all seemed possessed with a common impulse. One long unbroken cheer, in which the feeble cry of those who lay helpless on the earth blended with the strong voices of those who still fought, rose high above the roar of battle and hailed the presence of a victorious chief. He sat in the full realization of all that soldiers dream of—triumph; and as I looked at him in the complete fruition of the success which his genius, courage, and confidence in his army had won, I thought that it must have been from some such scene that men in ancient days ascended to the dignity of gods.
 — Charles Marshall, Lee's military secretary, "An Aide-de-Camp to Lee"
At the height of the fighting on May 3, Hooker suffered an injury when at 9:15 a.m. a Confederate cannonball hit a wooden pillar he was leaning against at his headquarters. He later wrote that half of the pillar "violently [struck me] ... in an erect position from my head to my feet." He likely received a concussion, which was sufficiently severe to render him unconscious for over an hour. Although clearly incapacitated after he arose, Hooker refused to turn over command temporarily to his second-in-command, Maj. Gen. Darius N. Couch, and, with Hooker's chief of staff, Maj. Gen. Daniel Butterfield, and Sedgwick out of communication (again due to the failure of the telegraph lines), there was no one at headquarters with sufficient rank or stature to convince Hooker otherwise. This failure affected Union performance over the next day and directly contributed to Hooker's seeming lack of nerve and timid performance throughout the rest of the battle.
May 3: Fredericksburg and Salem Church.
As Lee was savoring his victory at the Chancellorsville crossroads, he received disturbing news: Maj. Gen. John Sedgwick's force had broken through the Confederate lines at Fredericksburg and was headed toward Chancellorsville. On the night of May 2, in the aftermath of Jackson's flank attack, Hooker had ordered Sedgwick to "cross the Rappahannock at Fredericksburg on the receipt of this order, and at once take up your line of march on the Chancellorsville road until you connect with him. You will attack and destroy any force you may fall in with on the road." Lee had left a relatively small force at Fredericksburg, ordering Brig. Gen. Jubal Early to "watch the enemy and try to hold him." If he was attacked in "overwhelming numbers," Early was to retreat to Richmond, but if Sedgwick withdrew from his front, he was to join with Lee at Chancellorsville. On the morning of May 2, Early received a garbled message from Lee's staff that caused him to start marching most of his men toward Chancellorsville, but he quickly returned after a warning from Brig. Gen. William Barksdale of a Union advance against Fredericksburg. At 7 a.m. on May 3, Early was confronted with four Union divisions: Brig. Gen. John Gibbons of the II Corps had crossed the Rappahannock north of town, and three divisions of Sedgwick's VI Corps—Maj. Gen. John Newton and Brig. Gens. Albion P. Howe and William T. H. Brooks—were arrayed in line from the front of the town to Deep Run. Most of Early's combat strength was deployed to the south of town, where Federal troops had achieved their most significant successes during the December battle. Marye's Heights was defended by Barksdale's Mississippi brigade and Early ordered the Louisiana brigade of Brig. Gen. Harry T. Hays from the far right to Barksdale's left.
By midmorning, two Union attacks against the infamous stone wall on Marye's Heights were repulsed with numerous casualties. A Union party under flag of truce was allowed to approach ostensibly to collect the wounded, but while close to the stone wall, they were able to observe how sparsely the Confederate line was manned. A third Union attack was successful in overrunning the Confederate position. Early was able to organize an effective fighting retreat. John Sedgwick's road to Chancellorsville was open, but he wasted time forming a marching column. His men, led by Brooks's division, followed by Newton and Howe, were delayed for several hours by successive actions against the Alabama brigade of Brig. Gen. Cadmus M. Wilcox. His final delaying line was a ridge at Salem church, where he was joined by three brigades from McLaws's division and one from Anderson's, bringing the total Confederate strength to about 10,000 men.
Artillery fire was exchanged by both sides in the afternoon and at 5:30 p.m., two brigades of Brooks's division attacked on both sides of the Plank Road. The advance south of the road reached as far as the churchyard, but was driven back. The attack north of the road could not break the Confederate line. Wilcox described the action as "a bloody repulse to the enemy, rendering entirely useless to him his little success of the morning at Fredericksburg." Hooker expressed his disappointment in Sedgwick: "my object in ordering General Sedgwick forward ... Was to relieve me from the position in which I found myself at Chancellorsville. ... In my judgment General Sedgwick did not obey the spirit of my order, and made no sufficient effort to obey it. ... When he did move it was not with sufficient confidence or ability on his part to manoeuvre his troops."
The fighting on May 3, 1863, was some of the most furious anywhere in the war. The loss of 21,357 men that day in the three battles, divided equally between the two armies, ranks the fighting only behind the Battle of Antietam as the bloodiest day of the war.
May 4–6: Union withdrawals.
On the evening of May 3 and all day May 4, Hooker remained in his defenses north of Chancellorsville. Lee observed that Hooker was threatening no offensive action, so felt comfortable ordering Anderson's division to join the battle against Sedgwick. He sent orders to Early and McLaws to cooperate in a joint attack, but the orders reached his subordinates after dark, so the attack was planned for May 4. By this time Sedgwick had placed his divisions into a strong defensive position with its flanks anchored on the Rappahannock, three sides of a rectangle extending south of the Plank Road. Early's plan was to drive the Union troops off Marye's Heights and the other high ground west of Fredericksburg. Lee ordered McLaws to engage from the west "to prevent [the enemy] concentrating on General Early."
Early reoccupied Marye's Heights on the morning of May 4, cutting Sedgwick off from the town. However, McLaws was reluctant to take any action. Before noon, Lee arrived with Anderson's division, giving him a total of 21,000 men, slightly outnumbering Sedgwick. Despite Lee's presence, McLaws continued his passive role and Anderson's men took a few hours to get into position, a situation that frustrated and angered both Early and Lee, who had been planning on a concentrated assault from three directions. The attack finally began around 6 p.m. Two of Early's brigades (under Brig. Gens. Harry T. Hays and Robert F. Hoke) pushed back Sedgwick's left-center across the Plank Road, but Anderson's effort was a slight one and McLaws once again contributed nothing. Throughout the day on May 4, Hooker provided no assistance or useful guidance to Sedgwick, and Sedgwick thought about little else than protecting his line of retreat.
Sedgwick withdrew across the Rappahannock at Banks's Ford during the pre-dawn hours of May 5. When he learned that Sedgwick had retreated back over the river, Hooker felt he was out of options to save the campaign. He called a council of war and asked his corps commanders to vote about whether to stay and fight or to withdraw. Although a majority voted to fight, Hooker had had enough, and on the night of May 5–6, he withdrew back across the river at U.S. Ford. It was a difficult operation. Hooker and the artillery crossed first, followed by the infantry beginning at 6 a.m. on May 6. Meade's V Corps served as the rear guard. Rains caused the river to rise and threatened to break the pontoon bridges. Couch was in command on the south bank after Hooker departed, but he was left with explicit orders not to continue the battle, which he had been tempted to do. The surprise withdrawal frustrated Lee's plan for one final attack against Chancellorsville. He had issued orders for his artillery to bombard the Union line in preparation for another assault, but by the time they were ready Hooker and his men were gone.
Aftermath.
My God! It is horrible—horrible; and to think of it, 130,000 magnificent soldiers so cut to pieces by less than 60,000 half-starved ragamuffins!
 Horace Greeley, "New York Tribune"
The Union cavalry under Brig. Gen. George Stoneman, after a week of ineffectual raiding in central and southern Virginia in which they failed to attack any of the objectives Hooker established, withdrew into Union lines east of Richmond—the peninsula north of the York River, across from Yorktown—on May 7, ending the campaign.
Casualties.
Lee, despite being outnumbered by a ratio of over two to one, won arguably his greatest victory of the war, sometimes described as his "perfect battle." But he paid a terrible price for it. With only 60,000 men engaged, he suffered 13,303 casualties (1,665 killed, 9,081 wounded, 2,018 missing), losing some 22% of his force in the campaign—men that the Confederacy, with its limited manpower, could not replace. Just as seriously, he lost his most aggressive field commander, Stonewall Jackson. Brig. Gen. Elisha F. Paxton was the other Confederate general killed during the battle. After Longstreet rejoined the main army, he was highly critical of Lee's strategy, saying that battles like Chancellorsville cost the Confederacy more men than it could afford to lose.
Of the 133,000 Union men engaged, 17,197 were casualties (1,606 killed, 9,672 wounded, 5,919 missing), a percentage much lower than Lee's, particularly considering that it includes 4,000 men of the XI Corps who were captured on May 2. When comparing only the killed and wounded, there were almost no differences between the Confederate and Federal losses at Chancellorsville. The Union lost three generals in the campaign: Maj. Gens. Hiram G. Berry and Amiel W. Whipple and Brig. Gen. Edmund Kirby.
Assessment of Hooker.
Lee's Chancellorsville consisted of a pastiche of unbelievably risky gambits that led to a great triumph. Hooker's campaign, after the brilliant opening movements, degenerated into a tale of opportunities missed and troops underutilized.
Robert K. Krick, "Lee's Greatest Victory"
Hooker, who began the campaign believing he had "80 chances in 100 to be successful", lost the battle through miscommunication, the incompetence of some of his leading generals (most notably Howard and Stoneman, but also Sedgwick), but mostly through the collapse of his confidence. Hooker's errors included abandoning his offensive push on May 1 and ordering Sickles to give up Hazel Grove and pull back on May 2. He also erred in his disposition of forces; despite Abraham Lincoln's exhortation, "this time put in "all" your men," some 40,000 men of the Army of the Potomac scarcely fired a shot. When later asked why he had ordered a halt to his advance on May 1, Hooker is reputed to have responded, "For the first time, I lost faith in Hooker." However, Stephen W. Sears has categorized this as a myth:
Nothing has been more damaging to General Joseph Hooker's military reputation than this, from John Bigelow's "The Campaign of Chancellorsville" (1910): "A couple of months later, when Hooker crossed the Rappahannock [actually, the Potomac] with the Army of the Potomac in the Campaign of Gettysburg he was asked by General Doubleday: 'Hooker, what was the matter with you at Chancellorsville? ... Hooker answered frankly ... 'Doubleday ... For once I lost confidence in Hooker'".
Sears's research has shown that Bigelow was quoting from a letter written in 1903 by an E. P. Halstead, who was on the staff of Doubleday's I Corps division. There is no evidence that Hooker and Doubleday ever met during the Gettysburg Campaign, nor was there any chance of them meeting—they were dozens of miles apart. Finally, Doubleday made no mention of such a confession from Hooker in his history of the Chancellorsville Campaign, published in 1882. Sears concludes:
It can only be concluded that forty years after the event, elderly ex-staff officer Halstead was at best retailing some vaguely remembered campfire tale, and at worst manufacturing a role for himself in histories of the campaign ... Whatever Joe Hooker's failings at Chancellorsville, he did not publicly confess them.
Union reaction.
The Union was shocked by the defeat. President Abraham Lincoln was quoted as saying, "My God! My God! What will the country say?" A few generals were career casualties. Hooker relieved Stoneman for incompetence and for years waged a vituperative campaign against Howard, who he blamed for his loss. He wrote in 1876 that Howard was "a hypocrite ... totally incompetent ... a perfect old woman ... a bad man." He labeled Sedgwick as "dilatory." Couch was so disgusted by Hooker's conduct of the battle (and his incessant political maneuvering) that he resigned and was placed in charge of the Department of the Susquehanna, commanding only Pennsylvania militia. President Lincoln chose to retain Hooker in command of the army, but the friction between Lincoln, general in chief Henry W. Halleck, and Hooker became intolerable in the early days of the Gettysburg Campaign and Lincoln relieved Hooker of command on June 28, just before the Battle of Gettysburg. One of the consequences of Chancellorsville at Gettysburg was the conduct of Daniel Sickles, who undoubtedly recalled the terrible consequences of withdrawing from Hazel Grove when he decided to ignore the commands of his general and moved his lines on the second day of battle to ensure that a minor piece of high ground, the Peach Orchard, was not available to the enemy's artillery.
Confederate reaction.
The Confederate public had mixed feelings about the result, joy at Lee's tactical victory tempered by the loss of their most beloved general, Stonewall Jackson. Following the death of Jackson, Lee reorganized the Army of Northern Virginia from two large corps into three, under James Longstreet, Richard S. Ewell, and A.P. Hill. The new assignments for the latter two generals caused some command difficulties in the upcoming Gettysburg Campaign, which began in June. Of more consequence for Gettysburg, however, was the attitude that Lee absorbed from his great victory at Chancellorsville, that his army was virtually invincible and would succeed at anything he asked them to do.
Battlefield preservation.
The battlefield was a scene of widespread destruction, covered with dead men and animals. The Chancellor family, whose house was destroyed during the battle, placed the entire 854-acre property for sale four months after the battle. A smaller version of the house was rebuilt using some of the original materials, which served as a landmark for many of the veteran reunions of the late 19th century. In 1927, the rebuilt house was destroyed by fire. That same year, the United States Congress authorized the Fredericksburg and Spotsylvania National Military Park, which preserves some of the land that saw fighting in the 1862 Battle of Fredericksburg, the Chancellorsville Campaign, the Battle of the Wilderness, and the Battle of Spotsylvania Court House (the latter two being key battles in the 1864 Overland Campaign).
In May 2002, a regional developer announced a plan to build 2,300 houses and 2,000,000 square feet of commercial space on the 790-acre Mullins Farm, site of the first day of fighting at the Battle of Chancellorsville. Soon thereafter, the Civil War Trust formed the Coalition to Save Chancellorsville, a network of national and local preservation groups that waged a vocal campaign against the development.
For nearly a year, the Coalition mobilized local citizens, held candlelight vigils and hearings, and encouraged residents to become more involved in preservation. Public opinion polling conducted by the Coalition found that more than two-thirds of local residents opposed the development. The survey also found that 90 percent of local residents believed their county has a responsibility to protect Chancellorsville and other historic resources.
As a result of these efforts, in March 2003 the Spotsylvania County Board of Supervisors denied the rezoning application that would have allowed for the development of the site. Immediately following the vote, the Civil War Trust and other Coalition members began working to acquire the battlefield. By working with county officials and developers, the Civil War Trust acquired 140 acres in 2004 and another 74 acres in 2006.
In popular media.
The Battle of Chancellorsville was depicted in the 2003 film "Gods and Generals", based on the novel of the same name. The treatment of the battle in both the novel and the movie focused on Jackson's assault on the Union right flank, his wounding, and his subsequent death.
The battle formed the basis for Stephen Crane's 1895 novel "The Red Badge of Courage".
The battle serves as the background for one of F. Scott Fitzgerald's first short stories, published in the February 1935 Esquire Magazine, entitled "The Night at Chancellorsville".

</doc>
<doc id="48781" url="http://en.wikipedia.org/wiki?curid=48781" title="Philosophiæ Naturalis Principia Mathematica">
Philosophiæ Naturalis Principia Mathematica

Philosophiæ Naturalis Principia Mathematica, Latin for "Mathematical Principles of Natural Philosophy", often referred to as simply the Principia, is a work in three books by Sir Isaac Newton, in Latin, first published 5 July 1687. After annotating and correcting his personal copy of the first edition, Newton also published two further editions, in 1713 and 1726. The "Principia" states Newton's laws of motion, forming the foundation of classical mechanics, also Newton's law of universal gravitation, and a derivation of Kepler's laws of planetary motion (which Kepler first obtained empirically). The "Principia" is "justly regarded as one of the most important works in the history of science".
The French mathematical physicist Alexis Clairaut assessed it in 1747: "The famous book of "mathematical Principles of natural Philosophy" marked the epoch of a great revolution in physics. The method followed by its illustrious author Sir Newton ... spread the light of mathematics on a science which up to then had remained in the darkness of conjectures and hypotheses." A more recent assessment has been that while acceptance of Newton's theories was not immediate, by the end of a century after publication in 1687, "no one could deny that" (out of the "Principia") "a science had emerged that, at least in certain respects, so far exceeded anything that had ever gone before that it stood alone as the ultimate exemplar of science generally."
In formulating his physical theories, Newton developed and used mathematical methods now included in the field of calculus. But the language of calculus as we know it was largely absent from the "Principia"; Newton gave many of his proofs in a geometric form of infinitesimal calculus, based on limits of ratios of vanishing small geometric quantities. In a revised conclusion to the "Principia" (see "General Scholium"), Newton used his expression that became famous, "Hypotheses non fingo" ("I contrive no hypotheses").
Contents.
Expressed aim and topics covered.
In the preface of the "Principia", Newton wrote
 [...] Rational Mechanics will be the science of motions resulting from any forces whatsoever, and of the forces required to produce any motions, accurately proposed and demonstrated [...] And therefore we offer this work as mathematical principles of philosophy. For all the difficulty of philosophy seems to consist in this—from the phenomena of motions to investigate the forces of Nature, and then from these forces to demonstrate the other phenomena [...]
The "Principia" deals primarily with massive bodies in motion, initially under a variety of conditions and hypothetical laws of force in both non-resisting and resisting media, thus offering criteria to decide, by observations, which laws of force are operating in phenomena that may be observed. It attempts to cover hypothetical or possible motions both of celestial bodies and of terrestrial projectiles. It explores difficult problems of motions perturbed by multiple attractive forces. Its third and final book deals with the interpretation of observations about the movements of planets and their satellites. It shows how astronomical observations prove the inverse square law of gravitation (to an accuracy that was high by the standards of Newton's time); offers estimates of relative masses for the known giant planets and for the Earth and the Sun; defines the very slow motion of the Sun relative to the solar-system barycenter; shows how the theory of gravity can account for irregularities in the motion of the Moon; identifies the oblateness of the figure of the Earth; accounts approximately for marine tides including phenomena of spring and neap tides by the perturbing (and varying) gravitational attractions of the Sun and Moon on the Earth's waters; explains the precession of the equinoxes as an effect of the gravitational attraction of the Moon on the Earth's equatorial bulge; and gives theoretical basis for numerous phenomena about comets and their elongated, near-parabolic orbits.
The opening sections of the "Principia" contain, in revised and extended form, nearly all of the content of Newton's 1684 tract "De motu corporum in gyrum".
The "Principia" begin with 'Definitions' and 'Axioms or Laws of Motion' and continues in three books:
Book 1, De motu corporum.
Book 1, subtitled "De motu corporum" ("On the motion of bodies") concerns motion in the absence of any resisting medium. It opens with a mathematical exposition of "the method of first and last ratios", a geometrical form of infinitesimal calculus.
The second section establishes relationships between centripetal forces and the law of areas now known as Kepler's second law (Propositions 1–3), and relates circular velocity and radius of path-curvature to radial force (Proposition 4), and relationships between centripetal forces varying as the inverse-square of the distance to the center and orbits of conic-section form (Propositions 5–10).
Propositions 11–31 establish properties of motion in paths of eccentric conic-section form including ellipses, and their relation with inverse-square central forces directed to a focus, and include Newton's theorem about ovals (lemma 28).
Propositions 43–45 are demonstration that in an eccentric orbit under centripetal force where the apse may move, a steady non-moving orientation of the line of apses is an indicator of an inverse-square law of force.
Book 1 contains some proofs with little connection to real-world dynamics. But there are also sections with far-reaching application to the solar system and universe:
Propositions 57–69 deal with the "motion of bodies drawn to one another by centripetal forces." This section is of primary interest for its application to the solar system, and includes Proposition 66 along with its 22 corollaries: here Newton took the first steps in the definition and study of the problem of the movements of three massive bodies subject to their mutually perturbing gravitational attractions, a problem which later gained name and fame (among other reasons, for its great difficulty) as the three-body problem.
Propositions 70–84 deal with the attractive forces of spherical bodies. The section contains Newton's proof that a massive spherically symmetrical body attracts other bodies outside itself as if all its mass were concentrated at its centre. This fundamental result, called the Shell Theorem, enables the inverse square law of gravitation to be applied to the real solar system to a very close degree of approximation.
Book 2.
Part of the contents originally planned for the first book was divided out into a second book, which largely concerns motion through resisting mediums. Just as Newton examined consequences of different conceivable laws of attraction in Book 1, here he examines different conceivable laws of resistance; thus discusses resistance in direct proportion to velocity, and goes on to examine the implications of resistance in proportion to the square of velocity. Book 2 also discusses (in ) hydrostatics and the properties of compressible fluids. The effects of air resistance on pendulums are studied in , along with Newton's account of experiments that he carried out, to try to find out some characteristics of air resistance in reality by observing the motions of pendulums under different conditions. Newton compares the resistance offered by a medium against motions of bodies of different shape, attempts to derive the speed of sound, and gives accounts of experimental tests of the result.
Less of Book 2 has stood the test of time than of Books 1 and 3, and it has been said that Book 2 was largely written on purpose to refute a theory of Descartes which had some wide acceptance before Newton's work (and for some time after). According to this Cartesian theory of vortices, planetary motions were produced by the whirling of fluid vortices that filled interplanetary space and carried the planets along with them. Newton wrote at the end of Book 2 his conclusion that the hypothesis of vortices was completely at odds with the astronomical phenomena, and served not so much to explain as to confuse them.
Book 3, De mundi systemate.
Book 3, subtitled "De mundi systemate" ("On the system of the world") is an exposition of many consequences of universal gravitation, especially its consequences for astronomy. It builds upon the propositions of the previous books, and applies them with further specificity than in Book 1 to the motions observed in the solar system. Here (introduced by Proposition 22, and continuing in Propositions 25–35) are developed several of the features and irregularities of the orbital motion of the Moon, especially the variation. Newton lists the astronomical observations on which he relies, and establishes in a stepwise manner that the inverse square law of mutual gravitation applies to solar system bodies, starting with the satellites of Jupiter and going on by stages to show that the law is of universal application. He also gives starting at Lemma 4 and Proposition 40) the theory of the motions of comets, for which much data came from John Flamsteed and Edmond Halley, and accounts for the tides, attempting quantitative estimates of the contributions of the Sun and Moon to the tidal motions; and offers the first theory of the precession of the equinoxes. Book 3 also considers the harmonic oscillator in three dimensions, and motion in arbitrary force laws.
In Book 3 Newton also made clear his heliocentric view of the solar system, modified in a somewhat modern way, since already in the mid-1680s he recognised the "deviation of the Sun" from the centre of gravity of the solar system. For Newton, "the common centre of gravity of the Earth, the Sun and all the Planets is to be esteem'd the Centre of the World", and that this centre "either is at rest, or moves uniformly forward in a right line". Newton rejected the second alternative after adopting the position that "the centre of the system of the world is immoveable", which "is acknowledg'd by all, while some contend that the Earth, others, that the Sun is fix'd in that centre". Newton estimated the mass ratios Sun:Jupiter and Sun:Saturn, and pointed out that these put the centre of the Sun usually a little way off the common center of gravity, but only a little, the distance at most "would scarcely amount to one diameter of the Sun".
Commentary on the Principia.
The sequence of definitions used in setting up dynamics in the "Principia" is recognisable in many textbooks today. Newton first set out the definition of mass6
 The quantity of matter is that which arises conjointly from its density and magnitude. A body twice as dense in double the space is quadruple in quantity. This quantity I designate by the name of body or of mass.
This was then used to define the "quantity of motion" (today called momentum), and the principle of inertia in which mass replaces the previous Cartesian notion of "intrinsic force". This then set the stage for the introduction of forces through the change in momentum of a body. Curiously, for today's readers, the exposition looks dimensionally incorrect, since Newton does not introduce the dimension of time in rates of changes of quantities.
He defined space and time "not as they are well known to all". Instead, he defined "true" time and space as "absolute" and explained:
 Only I must observe, that the vulgar conceive those quantities under no other notions but from the relation they bear to perceptible objects. And it will be convenient to distinguish them into absolute and relative, true and apparent, mathematical and common. [...] instead of absolute places and motions, we use relative ones; and that without any inconvenience in common affairs; but in philosophical discussions, we ought to step back from our senses, and consider things themselves, distinct from what are only perceptible measures of them.
To some modern readers it can appear that some dynamical quantities recognised today were used in the "Principia" but not named. The mathematical aspects of the first two books were so clearly consistent that they were easily accepted; for example, Locke asked Huygens whether he could trust the mathematical proofs, and was assured about their correctness.
However, the concept of an attractive force acting at a distance received a cooler response. In his notes, Newton wrote that the inverse square law arose naturally due to the structure of matter. However, he retracted this sentence in the published version, where he stated that the motion of planets is consistent with an inverse square law, but refused to speculate on the origin of the law. Huygens and Leibniz noted that the law was incompatible with the notion of the aether. From a Cartesian point of view, therefore, this was a faulty theory. Newton's defence has been adopted since by many famous physicists—he pointed out that the mathematical form of the theory had to be correct since it explained the data, and he refused to speculate further on the basic nature of gravity. The sheer number of phenomena that could be organised by the theory was so impressive that younger "philosophers" soon adopted the methods and language of the "Principia".
Rules of Reasoning in Philosophy.
Perhaps to reduce the risk of public misunderstanding, Newton included at the beginning of Book 3 (in the second (1713) and third (1726) editions) a section entitled "Rules of Reasoning in Philosophy." In the four rules, as they came finally to stand in the 1726 edition, Newton effectively offers a methodology for handling unknown phenomena in nature and reaching towards explanations for them. The four Rules of the 1726 edition run as follows (omitting some explanatory comments that follow each):
Rule 1: "We are to admit no more causes of natural things than such as are both true and sufficient to explain their appearances."
Rule 2: "Therefore to the same natural effects we must, as far as possible, assign the same causes."
Rule 3: "The qualities of bodies, which admit neither intensification nor remission of degrees, and which are found to belong to all bodies within the reach of our experiments, are to be esteemed the universal qualities of all bodies whatsoever."
Rule 4: "In experimental philosophy we are to look upon propositions inferred by general induction from phenomena as accurately or very nearly true, not withstanding any contrary hypothesis that may be imagined, till such time as other phenomena occur, by which they may either be made more accurate, or liable to exceptions."
This section of Rules for philosophy is followed by a listing of 'Phenomena', in which are listed a number of mainly astronomical observations, that Newton used as the basis for inferences later on, as if adopting a consensus set of facts from the astronomers of his time.
Both the 'Rules' and the 'Phenomena' evolved from one edition of the "Principia" to the next. Rule 4 made its appearance in the third (1726) edition; Rules 1–3 were present as 'Rules' in the second (1713) edition, and predecessors of them were also present in the first edition of 1687, but there they had a different heading: they were not given as 'Rules', but rather in the first (1687) edition the predecessors of the three later 'Rules', and of most of the later 'Phenomena', were all lumped together under a single heading 'Hypotheses' (in which the third item was the predecessor of a heavy revision that gave the later Rule 3).
From this textual evolution, it appears that Newton wanted by the later headings 'Rules' and 'Phenomena' to clarify for his readers his view of the roles to be played by these various statements.
In the third (1726) edition of the "Principia", Newton explains each rule in an alternative way and/or gives an example to back up what the rule is claiming. The first rule is explained as a philosophers' principle of economy. The second rule states that if one cause is assigned to a natural effect, then the same cause so far as possible must be assigned to natural effects of the same kind: for example respiration in humans and in animals, fires in the home and in the Sun, or the reflection of light whether it occurs terrestrially or from the planets. An extensive explanation is given of the third rule, concerning the qualities of bodies, and Newton discusses here the generalisation of observational results, with a caution against making up fancies contrary to experiments, and use of the rules to illustrate the observation of gravity and space.
Isaac Newton’s statement of the four rules revolutionised the investigation of phenomena. With these rules, Newton could in principle begin to address all of the world’s present unsolved mysteries. He was able to use his new analytical method to replace that of Aristotle, and he was able to use his method to tweak and update Galileo’s experimental method. The re-creation of Galileo's method has never been significantly changed and in its substance, scientists use it today.
General Scholium.
The "General Scholium" is a concluding essay added to the second edition, 1713 (and amended in the third edition, 1726). It is not to be confused with the "General Scholium" at the end of Book 2, Section 6, which discusses his pendulum experiments and resistance due to air, water, and other fluids.
Here Newton used what became his famous expression Hypotheses non fingo, "I contrive no hypotheses", in response to criticisms of the first edition of the "Principia". ("'Fingo'" is sometimes nowadays translated 'feign' rather than the traditional 'frame'.) Newton's gravitational attraction, an invisible force able to act over vast distances, had led to criticism that he had introduced "occult agencies" into science. Newton firmly rejected such criticisms and wrote that it was enough that the phenomena implied gravitational attraction, as they did; but the phenomena did not so far indicate the cause of this gravity, and it was both unnecessary and improper to frame hypotheses of things not implied by the phenomena: such hypotheses "have no place in experimental philosophy", in contrast to the proper way in which "particular propositions are inferr'd from the phenomena and afterwards rendered general by induction".
Newton also underlined his criticism of the vortex theory of planetary motions, of Descartes, pointing to its incompatibility with the highly eccentric orbits of comets, which carry them "through all parts of the heavens indifferently".
Newton also gave theological argument. From the system of the world, he inferred the existence of a Lord God, along lines similar to what is sometimes called the argument from intelligent or purposive design. It has been suggested that Newton gave "an oblique argument for a unitarian conception of God and an implicit attack on the doctrine of the Trinity", but the General Scholium appears to say nothing specifically about these matters.
Writing and publication.
Halley and Newton's initial stimulus.
In January 1684, Halley, Wren and Hooke had a conversation in which Hooke claimed to not only have derived the inverse-square law, but also all the laws of planetary motion. Wren was unconvinced, Hooke did not produce the claimed derivation although the others gave him time to do it, and Halley, who could derive the inverse-square law for the restricted circular case (by substituting Kepler's relation into Huygens' formula for the centrifugal force) but failed to derive the relation generally, resolved to ask Newton.
Halley's visits to Newton in 1684 thus resulted from Halley's debates about planetary motion with Wren and Hooke, and they seem to have provided Newton with the incentive and spur to develop and write what became "Philosophiae Naturalis Principia Mathematica" ("Mathematical Principles of Natural Philosophy"). Halley was at that time a Fellow and Council member of the Royal Society in London, (positions that in 1686 he resigned to become the Society's paid Clerk). Halley's visit to Newton in Cambridge in 1684 probably occurred in August. When Halley asked Newton's opinion on the problem of planetary motions discussed earlier that year between Halley, Hooke and Wren, Newton surprised Halley by saying that he had already made the derivations some time ago; but that he could not find the papers. (Matching accounts of this meeting come from Halley and Abraham De Moivre to whom Newton confided.) Halley then had to wait for Newton to 'find' the results, but in November 1684 Newton sent Halley an amplified version of whatever previous work Newton had done on the subject. This took the form of a 9-page manuscript, "De motu corporum in gyrum" ("Of the motion of bodies in an orbit"): the title is shown on some surviving copies, although the (lost) original may have been without title.
Newton's tract "De motu corporum in gyrum", which he sent to Halley in late 1684, derived what are now known as the three laws of Kepler, assuming an inverse square law of force, and generalised the result to conic sections. It also extended the methodology by adding the solution of a problem on the motion of a body through a resisting medium. The contents of "De motu" so excited Halley by their mathematical and physical originality and far-reaching implications for astronomical theory, that he immediately went to visit Newton again, in November 1684, to ask Newton to let the Royal Society have more of such work. The results of their meetings clearly helped to stimulate Newton with the enthusiasm needed to take his investigations of mathematical problems much further in this area of physical science, and he did so in a period of highly concentrated work that lasted at least until mid-1686.
Newton's single-minded attention to his work generally, and to his project during this time, is shown by later reminiscences from his secretary and copyist of the period, Humphrey Newton. His account tells of Isaac Newton's absorption in his studies, how he sometimes forgot his food, or his sleep, or the state of his clothes, and how when he took a walk in his garden he would sometimes rush back to his room with some new thought, not even waiting to sit before beginning to write it down. Other evidence also shows Newton's absorption in the "Principia": Newton for years kept up a regular programme of chemical or alchemical experiments, and he normally kept dated notes of them, but for a period from May 1684 to April 1686, Newton's chemical notebooks have no entries at all. So it seems that Newton abandoned pursuits to which he was normally dedicated, and did very little else for well over a year and a half, but concentrated on developing and writing what became his great work.
The first of the three constituent books was sent to Halley for the printer in spring 1686, and the other two books somewhat later. The complete work, published by Halley at his own financial risk, appeared in July 1687. Newton had also communicated "De motu" to Flamsteed, and during the period of composition he exchanged a few letters with Flamsteed about observational data on the planets, eventually acknowledging Flamsteed's contributions in the published version of the "Principia" of 1687.
Preliminary version.
The process of writing that first edition of the "Principia" went through several stages and drafts: some parts of the preliminary materials still survive, others are lost except for fragments and cross-references in other documents.
Surviving preliminary materials show that Newton (up to some time in 1685) conceived his book as a two-volume work: The first volume was to be "De motu corporum, Liber primus", with contents that later appeared in extended form as Book 1 of the "Principia".
A fair-copy draft of Newton's planned second volume "De motu corporum, Liber secundus" still survives, and its completion has been dated to about the summer of 1685. What it covers is the application of the results of "Liber primus" to the Earth, the Moon, the tides, the solar system, and the universe: in this respect it has much the same purpose as the final Book 3 of the "Principia", but it is written much less formally and is more easily read.
It is not known just why Newton changed his mind so radically about the final form of what had been a readable narrative in "De motu corporum, Liber secundus" of 1685, but he largely started afresh in a new, tighter, and less accessible mathematical style, eventually to produce Book 3 of the "Principia" as we know it. Newton frankly admitted that this change of style was deliberate when he wrote that he had (first) composed this book "in a popular method, that it might be read by many", but to "prevent the disputes" by readers who could not "lay aside the[ir] prejudices", he had "reduced" it "into the form of propositions (in the mathematical way) which should be read by those only, who had first made themselves masters of the principles established in the preceding books". The final Book 3 also contained in addition some further important quantitative results arrived at by Newton in the meantime, especially about the theory of the motions of comets, and some of the perturbations of the motions of the Moon.
The result was numbered Book 3 of the "Principia" rather than Book 2, because in the meantime, drafts of "Liber primus" had expanded and Newton had divided it into two books. The new and final Book 2 was concerned largely with the motions of bodies through resisting mediums.
But the "Liber secundus" of 1685 can still be read today. Even after it was superseded by Book 3 of the "Principia", it survived complete, in more than one manuscript. After Newton's death in 1727, the relatively accessible character of its writing encouraged the publication of an English translation in 1728 (by persons still unknown, not authorised by Newton's heirs). It appeared under the English title "A Treatise of the System of the World". This had some amendments relative to Newton's manuscript of 1685, mostly to remove cross-references that used obsolete numbering to cite the propositions of an early draft of Book 1 of the "Principia". Newton's heirs shortly afterwards published the Latin version in their possession, also in 1728, under the (new) title "De Mundi Systemate", amended to update cross-references, citations and diagrams to those of the later editions of the "Principia", making it look superficially as if it had been written by Newton after the "Principia", rather than before. The "System of the World" was sufficiently popular to stimulate two revisions (with similar changes as in the Latin printing), a second edition (1731), and a 'corrected' reprint of the second edition (1740).
Halley's role as publisher.
The text of the first of the three books of the "Principia" was presented to the Royal Society at the close of April 1686. Hooke made some priority claims (but failed to substantiate them), causing some delay. When Hooke's claim was made known to Newton, who hated disputes, Newton threatened to withdraw and suppress Book 3 altogether, but Halley, showing considerable diplomatic skills, tactfully persuaded Newton to withdraw his threat and let it go forward to publication. Samuel Pepys, as President, gave his imprimatur on 30 June 1686, licensing the book for publication. The Society had just spent its book budget on a "History of Fishes", and the cost of publication was borne by Edmund Halley (who was also then acting as publisher of the "Philosophical Transactions of the Royal Society"): the book appeared in summer 1687.
Historical context.
Beginnings of the Scientific Revolution.
Nicolaus Copernicus had moved the Earth away from the center of the universe with the heliocentric theory for which he presented evidence in his book "De revolutionibus orbium coelestium" ("On the revolutions of the heavenly spheres") published in 1543. The structure was completed when Johannes Kepler wrote the book "Astronomia nova" ("A new astronomy") in 1609, setting out the evidence that planets move in elliptical orbits with the sun at one focus, and that planets do not move with constant speed along this orbit. Rather, their speed varies so that the line joining the centres of the sun and a planet sweeps out equal areas in equal times. To these two laws he added a third a decade later, in his book "Harmonices Mundi" ("Harmonies of the world"). This law sets out a proportionality between the third power of the characteristic distance of a planet from the sun and the square of the length of its year.
The foundation of modern dynamics was set out in Galileo's book "Dialogo sopra i due massimi sistemi del mondo" ("Dialogue on the two main world systems") where the notion of inertia was implicit and used. In addition, Galileo's experiments with inclined planes had yielded precise mathematical relations between elapsed time and acceleration, velocity or distance for uniform and uniformly accelerated motion of bodies.
Descartes' book of 1644 "Principia philosophiae" ("Principles of philosophy") stated that bodies can act on each other only through contact: a principle that induced people, among them himself, to hypothesize a universal medium as the carrier of interactions such as light and gravity—the aether. Newton was criticized for apparently introducing forces that acted at distance without any medium. Not until the development of particle theory was Descartes' notion vindicated when it was possible to describe all interactions, like the strong, weak, and electromagnetic fundamental interactions, using mediating gauge bosons and gravity through hypothesized gravitons. Although he was mistaken in his treatment of circular motion, this effort was more fruitful in the short term when it led others to identify circular motion as a problem raised by the principle of inertia. Christiaan Huygens solved this problem in the 1650s and published it much later in 1673 in his book "Horologium oscillatorium sive de motu pendulorum".
Newton's role.
Newton had studied these books, or, in some cases, secondary sources based on them, and taken notes entitled "Quaestiones quaedam philosophicae" ("Questions about philosophy") during his days as an undergraduate. During this period (1664–1666) he created the basis of calculus, and performed the first experiments in the optics of colour. At this time, his proof that white light was a combination of primary colours (found via prismatics) replaced the prevailing theory of colours and received an overwhelmingly favourable response, and occasioned bitter disputes with Robert Hooke and others, which forced him to sharpen his ideas to the point where he already composed sections of his later book "Opticks" by the 1670s in response. Work on calculus is shown in various papers and letters, including two to Leibniz. He became a fellow of the Royal Society and the second Lucasian Professor of Mathematics (succeeding Isaac Barrow) at Trinity College, Cambridge.
Newton's early work on motion.
In the 1660s Newton studied the motion of colliding bodies, and deduced that the centre of mass of two colliding bodies remains in uniform motion. Surviving manuscripts of the 1660s also show Newton's interest in planetary motion and that by 1669 he had shown, for a circular case of planetary motion, that the force he called 'endeavour to recede' (now called centrifugal force) had an inverse-square relation with distance from the center. After his 1679–1680 correspondence with Hooke, described below, Newton adopted the language of inward or centripetal force. According to Newton scholar J Bruce Brackenridge, although much has been made of the change in language and difference of point of view, as between centrifugal or centripetal forces, the actual computations and proofs remained the same either way. They also involved the combination of tangential and radial displacements, which Newton was making in the 1660s. The difference between the centrifugal and centripetal points of view, though a significant change of perspective, did not change the analysis. Newton also clearly expressed the concept of linear inertia in the 1660s: for this Newton was indebted to Descartes' work published 1644.
Controversy with Hooke.
Hooke published his ideas about gravitation in the 1660s and again in 1674. He argued for an attracting principle of gravitation in "Micrographia" of 1665, in a 1666 Royal Society lecture "On gravity", and again in 1674, when he published his ideas about the "System of the World" in somewhat developed form, as an addition to "An Attempt to Prove the Motion of the Earth from Observations". Hooke clearly postulated mutual attractions between the Sun and planets, in a way that increased with nearness to the attracting body, along with a principle of linear inertia. Hooke's statements up to 1674 made no mention, however, that an inverse square law applies or might apply to these attractions. Hooke's gravitation was also not yet universal, though it approached universality more closely than previous hypotheses. Hooke also did not provide accompanying evidence or mathematical demonstration. On these two aspects, Hooke stated in 1674: "Now what these several degrees [of gravitational attraction] are I have not yet experimentally verified" (indicating that he did not yet know what law the gravitation might follow); and as to his whole proposal: "This I only hint at present", "having my self many other things in hand which I would first compleat, and therefore cannot so well attend it" (i.e., "prosecuting this Inquiry").
In November 1679, Hooke began an exchange of letters with Newton, of which the full text is now published. Hooke told Newton that Hooke had been appointed to manage the Royal Society's correspondence, and wished to hear from members about their researches, or their views about the researches of others; and as if to whet Newton's interest, he asked what Newton thought about various matters, giving a whole list, mentioning "compounding the celestial motions of the planets of a direct motion by the tangent and an attractive motion towards the central body", and "my hypothesis of the lawes or causes of springinesse", and then a new hypothesis from Paris about planetary motions (which Hooke described at length), and then efforts to carry out or improve national surveys, the difference of latitude between London and Cambridge, and other items. Newton's reply offered "a fansy of my own" about a terrestrial experiment (not a proposal about celestial motions) which might detect the Earth's motion, by the use of a body first suspended in air and then dropped to let it fall. The main point was to indicate how Newton thought the falling body could experimentally reveal the Earth's motion by its direction of deviation from the vertical, but he went on hypothetically to consider how its motion could continue if the solid Earth had not been in the way (on a spiral path to the centre). Hooke disagreed with Newton's idea of how the body would continue to move. A short further correspondence developed, and towards the end of it Hooke, writing on 6 January 1680 to Newton, communicated his "supposition ... that the Attraction always is in a duplicate proportion to the Distance from the Center Reciprocall, and Consequently that the Velocity will be in a subduplicate proportion to the Attraction and Consequently as Kepler Supposes Reciprocall to the Distance." (Hooke's inference about the velocity was actually incorrect.)
In 1686, when the first book of Newton's "Principia" was presented to the Royal Society, Hooke claimed that Newton had obtained from him the "notion" of "the rule of the decrease of Gravity, being reciprocally as the squares of the distances from the Center". At the same time (according to Edmond Halley's contemporary report) Hooke agreed that "the Demonstration of the Curves generated therby" was wholly Newton's.
A recent assessment about the early history of the inverse square law is that "by the late 1660s," the assumption of an "inverse proportion between gravity and the square of distance was rather common and had been advanced by a number of different people for different reasons". Newton himself had shown in the 1660s that for planetary motion under a circular assumption, force in the radial direction had an inverse-square relation with distance from the center. Newton, faced in May 1686 with Hooke's claim on the inverse square law, denied that Hooke was to be credited as author of the idea, giving reasons including the citation of prior work by others before Hooke. Newton also firmly claimed that even if it had happened that he had first heard of the inverse square proportion from Hooke, which it had not, he would still have some rights to it in view of his mathematical developments and demonstrations, which enabled observations to be relied on as evidence of its accuracy, while Hooke, without mathematical demonstrations and evidence in favour of the supposition, could only guess (according to Newton) that it was approximately valid "at great distances from the center".
The background described above shows there was basis for Newton to deny deriving the inverse square law from Hooke. On the other hand, Newton did accept and acknowledge, in all editions of the "Principia", that Hooke (but not exclusively Hooke) had separately appreciated the inverse square law in the solar system. Newton acknowledged Wren, Hooke and Halley in this connection in the Scholium to Proposition 4 in Book 1. Newton also acknowledged to Halley that his correspondence with Hooke in 1679–80 had reawakened his dormant interest in astronomical matters, but that did not mean, according to Newton, that Hooke had told Newton anything new or original: "yet am I not beholden to him for any light into that business but only for the diversion he gave me from my other studies to think on these things & for his dogmaticalness in writing as if he had found the motion in the Ellipsis, which inclined me to try it ...".) Newton's reawakening interest in astronomy received further stimulus by the appearance of a comet in the winter of 1680/1681, on which he corresponded with John Flamsteed.
In 1759, decades after the deaths of both Newton and Hooke, Alexis Clairaut, mathematical astronomer eminent in his own right in the field of gravitational studies, made his assessment after reviewing what Hooke had published on gravitation. "One must not think that this idea ... of Hooke diminishes Newton's glory", Clairaut wrote; "The example of Hooke" serves "to show what a distance there is between a truth that is glimpsed and a truth that is demonstrated".
Location of copies.
Several national rare-book collections contain original copies of Newton's "Principia Mathematica", including:
A facsimile edition (based on the 3rd edition of 1726 but with variant readings from earlier editions and important annotations) was published in 1972 by Alexandre Koyré and I. Bernard Cohen.
Later editions.
Two later editions were published by Newton:
Second edition, 1713.
Newton had been urged to make a new edition of the "Principia" since the early 1690s, partly because copies of the first edition had already become very rare and expensive within a few years after 1687. Newton referred to his plans for a second edition in correspondence with Flamsteed in November 1694: Newton also maintained annotated copies of the first edition specially bound up with interleaves on which he could note his revisions; two of these copies still survive: but he had not completed the revisions by 1708, and of two would-be editors, Newton had almost severed connections with one, Fatio de Duillier, and the other, David Gregory seems not to have met with Newton's approval and was also terminally ill, dying later in 1708. Nevertheless, reasons were accumulating not to put off the new edition any longer. Richard Bentley, master of Trinity College, persuaded Newton to allow him to undertake a second edition, and in June 1708 Bentley wrote to Newton with a specimen print of the first sheet, at the same time expressing the (unfulfilled) hope that Newton had made progress towards finishing the revisions. It seems that Bentley then realised that the editorship was technically too difficult for him, and with Newton's consent he appointed Roger Cotes, Plumian professor of astronomy at Trinity, to undertake the editorship for him as a kind of deputy (but Bentley still made the publishing arrangements and had the financial responsibility and profit). The correspondence of 1709–1713 shows Cotes reporting to two masters, Bentley and Newton, and managing (and often correcting) a large and important set of revisions to which Newton sometimes could not give his full attention. Under the weight of Cotes' efforts, but impeded by priority disputes between Newton and Leibniz, and by troubles at the Mint, Cotes was able to announce publication to Newton on 30 June 1713. Bentley sent Newton only six presentation copies; Cotes was unpaid; Newton omitted any acknowledgement to Cotes.
Among those who gave Newton corrections for the Second Edition were: Firmin Abauzit, Roger Cotes and David Gregory. However, Newton omitted acknowledgements to some because of the priority disputes. John Flamsteed, the Astronomer Royal, suffered this especially.
Third edition, 1726.
The third edition was published 25 March 1726, under the stewardship of "Henry Pemberton, M.D., a man of the greatest skill in these matters ..."; Pemberton later said that this recognition was worth more to him than the two hundred guinea award from Newton.
Annotated and other editions.
In 1739–42 two French priests, Pères Thomas LeSeur and François Jacquier (of the 'Minim' order, but sometimes erroneously identified as Jesuits) produced with the assistance of J-L Calandrini an extensively annotated version of the 'Principia' in the 3rd edition of 1726. Sometimes this is referred to as the 'Jesuit edition': it was much used, and reprinted more than once in Scotland during the 19th century.
Émilie du Châtelet also made a translation of Newton's Principia into French. Unlike LeSeur and Jacquier's edition, hers was a complete translation of Newton's three books and their prefaces. She also included a Commentary section where she fused the three books into a much clearer and easier to understand summary. She included an analytical section where she applied the new mathematics of calculus to Newton's most controversial theories. Previously, geometry was the standard mathematics used to analyse theories. Du Chatelet's translation is the only complete one to have been done in French and hers remains the standard French translation to this day. See "Translating Newton's 'Principia': The Marquise du Châtelet's Revisions and Additions for a French Audience." Author(s): Judith P. Zinsser Source: Notes and Records of the Royal Society of London, Vol. 55, No. 2 (May 2001), pp. 227–245.
English translations.
Two full English translations of Newton's 'Principia' have appeared, both based on Newton's 3rd edition of 1726.
The first, from 1729, by Andrew Motte, was described by Newton scholar I. Bernard Cohen (in 1968) as "still of enormous value in conveying to us the sense of Newton's words in their own time, and it is generally faithful to the original: clear, and well written". The 1729 version was the basis for several republications, often incorporating revisions, among them a widely used modernised English version of 1934, which appeared under the editorial name of Florian Cajori (though completed and published only some years after his death). Cohen pointed out ways in which the 18th-century terminology and punctuation of the 1729 translation might be confusing to modern readers, but he also made severe criticisms of the 1934 modernised English version, and showed that the revisions had been made without regard to the original, also demonstrating gross errors "that provided the final impetus to our decision to produce a wholly new translation".
The second full English translation, into modern English, is the work that resulted from this decision by collaborating translators I. Bernard Cohen and Anne Whitman; it was published in 1999 with a guide by way of introduction.
William H. Donahue has published a translation of the work's central argument, published in 1996, along with expansion of included proofs and ample commentary. The book was developed as a textbook for classes at St. John's College and the aim of this translation is to be faithful to the Latin text.
Homages.
British astronaut Tim Peake named his 2014 mission to the International Space Station "Principia" after the book, in "honour of Britain's greatest scientist".

</doc>
<doc id="48786" url="http://en.wikipedia.org/wiki?curid=48786" title="Polyvinylpyrrolidone">
Polyvinylpyrrolidone

Polyvinylpyrrolidone (PVP), also commonly called polyvidone or povidone, is a water-soluble polymer made from the monomer "N"-vinylpyrrolidone:
Uses.
Medical.
PVP was used as a plasma volume expander for trauma victims after the 1950s.
It is used as a binder in many pharmaceutical tablets; it simply passes through the body when taken orally. However, autopsies have found that crospovidone (PVPP) contributes to pulmonary vascular injury in substance abusers who have injected pharmaceutical tablets intended for oral consumption. The long-term effects of crospovidone or povidone within the lung are unknown. PVP added to iodine forms a complex called povidone-iodine that possesses disinfectant properties. This complex is used in various products like solutions, ointment, pessaries, liquid soaps and surgical scrubs. It is known under the trade names Betadine and Pyodine among a plethora of others.
It is used in pleurodesis (fusion of the pleura because of incessant pleural effusions). For this purpose, povidone iodine is equally effective and safe as talc, and may be preferred because of easy availability and low cost.
Technical.
PVP is also used in many technical applications:
Other uses.
PVP binds to polar molecules exceptionally well, owing to its polarity. This has led to its application in coatings for photo-quality ink-jet papers and transparencies, as well as in inks for inkjet printers.
PVP is also used in personal care products, such as shampoos and toothpastes, in paints, and adhesives that must be moistened, such as old-style postage stamps and envelopes. It has also been used in contact lens solutions and in steel-quenching solutions. PVP is the basis of the early formulas for hair sprays and hair gels, and still continues to be a component of some.
As a food additive, PVP is a stabilizer and has E number E1201. PVPP (crospovidone) is E1202. It is also used in the wine industry as a fining agent for white wine or some beers. Other references state that polyvinyl pyrrolidone and its derivatives are fully from mineral synthetic origin. Therefore, its use in the production should not be a problem for vegans.
In molecular biology, PVP can be used as a blocking agent during Southern blot analysis as a component of Denhardt's buffer. It is also exceptionally good at absorbing polyphenols during DNA purification. Polyphenols are common in many plant tissues and can deactivate proteins if not removed and therefore inhibit many downstream reactions like PCR.
In microscopy, PVP is useful for making an aqueous mounting medium.
Safety.
The U.S. Food and Drug Administration (FDA) has approved this chemical for many uses, and it is generally considered safe. However, there have been documented cases of allergic reactions to PVP/povidone, particularly regarding subcutaneous (applied under the skin) use and situations where the PVP has come in contact with autologous serum (internal blood fluids) and mucous membranes. For example, a boy having an anaphylactic response after application of PVP-Iodine for treatment of impetigo was found to be allergic to the PVP component of the solution. A woman, who had previously experienced urticaria (hives) from various hair products, later found to contain PVP, had an anaphylactic response after povidone-iodine solution was applied internally. She was found to be allergic to PVP. In another case, a man experiencing anaphylaxis after taking acetaminophen tablets orally was found to be allergic to PVP.
Povidone is commonly used in conjunction with other chemicals. Some of these, such as iodine, are blamed for allergic responses, although testing results in some patients show no signs of allergy to the suspect chemical. Allergies attributed to these other chemicals may possibly be caused by the PVP instead.
Properties.
PVP is soluble in water and other polar solvents. When dry it is a light flaky hygroscopic powder, readily absorbing up to 40% of its weight in atmospheric water. In solution, it has excellent wetting properties and readily forms films. This makes it good as a coating or an additive to coatings.
History.
PVP was first synthesized by Walter Reppe and a patent was filed in 1939 for one of the most interesting derivatives of acetylene chemistry. PVP was initially used as a blood plasma substitute and later in a wide variety of applications in medicine, pharmacy, cosmetics and industrial production.

</doc>
<doc id="48787" url="http://en.wikipedia.org/wiki?curid=48787" title="Advanced Maryland Automatic Network Disk Archiver">
Advanced Maryland Automatic Network Disk Archiver

Amanda, previously known as Advanced Maryland Automatic Network Disk Archiver is an open source computer archiving tool that is able to back up data residing on multiple computers on a network. It uses a client–server model, where the server contacts each client to perform a backup at a scheduled time.
Amanda was initially developed at the University of Maryland and is released under a BSD-style license. Amanda is available both as a free community edition and fully supported enterprise edition. Amanda runs on almost any Unix or Unix-like systems. Amanda supports Windows systems using Samba or a native Win32 client with support for open files.
Amanda supports both tape-based and disk-based backup, and provides some useful functionality not available in other backup products. Amanda supports tape-spanning - i.e. if a backup set does not fit in one tape, it will be split into multiple tapes.
Among its key features is an intelligent scheduler which optimizes use of computing resources across backup runs.
Major releases.
The most recent stable release is version 3.3.7p1, which was released on February 15 2015.
Enterprise edition.
Amanda Enterprise Edition is a commercial version of Amanda which has been developed by Zmanda. It includes a management GUI—Zmanda Management Console (ZMC)—and other features such as scheduler, plugin framework, and also an optional cloud backup service support. The cloud backup option uses the Amazon S3 service from Amazon Web Services as the cloud storage provider and enables safe offsite storage of the Amanda backup data. The plugin framework allows for application-specific backups and is used by Amanda Enterprise to support applications such as Oracle database, Samba network share, NDMP, etc. Amanda Enterprise also supports image-level backup of live VMs running on VMware infrastructure.

</doc>
<doc id="48791" url="http://en.wikipedia.org/wiki?curid=48791" title="Pathology">
Pathology

Pathology (from the Ancient Greek roots of "pathos" (πάθος), meaning "experience" or "suffering", and "-logia" (-λογία), "an account of") is a significant component of the causal study of disease and a major field in modern medicine and diagnosis. The term pathology itself may be used broadly to refer to the study of disease in general, incorporating a wide range of bioscience research fields and medical practices (including plant pathology and veterinary pathology), or more narrowly to describe work within the contemporary medical field of "general pathology," which includes a number of distinct but inter-related medical specialties which diagnose disease mostly through the analysis of tissue, cell, and body fluid samples. Used as a count noun, "a pathology" (plural, "pathologies") can also refer to the predicted or actual progression of particular diseases (as in the statement "the many different forms of cancer have diverse pathologies"), and the affix "path" is sometimes used to indicate a state of disease in cases of both physical ailment (as in cardiomyopathy) and psychological conditions (such as psychopathy). Similarly, a pathological condition is one caused by disease, rather than occurring physiologically. A physician practicing pathology is called a pathologist.
As a field of general inquiry and research, pathology addresses four components of disease: cause/etiology, mechanisms of development (pathogenesis), structural alterations of cells (morphologic changes), and the consequences of changes (clinical manifestations). In common medical practice, general pathology is mostly concerned with analyzing known clinical abnormalities that are markers or precursors for both infectious and non-infectious disease and is conducted by experts in one of two major specialties, anatomical pathology and clinical pathology. Further divisions in specialty exist on the basis of the involved sample types (comparing, for example, cytopathology, hematopathology, and histopathology), organs (as in renal pathology), and physiological systems (oral pathology), as well as on the basis of the focus of the examination (as with forensic pathology).
The sense of the word "pathology" as a synonym of "disease" or "pathosis" is very common in health care. The persistence of this usage despite attempted proscription is discussed elsewhere.
History.
The study of pathology, including the detailed examination of the body, including dissection and inquiry into specific maladies, dates back to antiquity. Rudimentary understanding of many conditions was present in most early societies and is attested to in the records of the earliest historical societies, including those of the Middle East, India, and China. By the Hellenic period of ancient Greece, a concerted causal study of disease was underway (see Medicine in ancient Greece), with many notable early physicians (such as Hippocrates, for whom the modern Hippocratic Oath is named) having developed methods of diagnosis and prognosis for a number of diseases.The medical practices of the Romans and those of the Byzantines continued from these Greek roots, but, as with many areas of scientific inquiry, growth in understanding of medicine stagnated some after the Classical Era, but continued to slowly develop throughout numerous cultures. Notably, many advances were made in the medieval era of Islam (see Medicine in medieval Islam), during which numerous texts of complex pathologies were developed, also based on the Greek tradition. Even so, growth in complex understanding of disease mostly languished until knowledge and experimentation again began to proliferate in the Renaissance, Enlightenment, and Baroque eras, following the resurgence of the empirical method at new centers of scholarship. By the 17th century, the study of micrography was underway and examination of tissues had led British Royal Society member Robert Hooke to coin the word "cell", setting the stage for later germ theory.
 Modern pathology began to develop as a distinct field of inquiry during the 19th Century through natural philosophers and physicians that studied disease and the informal study of what they termed “pathological anatomy” or “morbid anatomy”. However, pathology as a formal area of specialty was not fully developed until the late 19th and early 20th centuries, with the advent of detailed study of microbiology. In the 19th century, physicians had begun to understand that disease-causing pathogens, or "germs" (a catch-all for disease-causing, or pathogenic, microbes, such as bacteria, viruses, fungi, amoebae, molds, protists, and prions) existed and were capable of reproduction and multiplication, replacing earlier beliefs in humors or even spiritual agents, that had dominated for much of the previous 1,500 years in European medicine. With the new understanding of causative agents, physicians began to compare the characteristics of one germ’s symptoms as they developed within an affected individual to another germ’s characteristics and symptoms. This realization led to the foundational understanding that diseases are able to replicate themselves, and that they can have many profound and varied effects on the human host. In order to determine causes of diseases, medical experts used the most common and widely accepted assumptions or symptoms of their times, a general principal of approach that persists into modern medicine.
Modern medicine was particularly advanced by further developments of the microscope to analyze tissues, to which Rudolf Virchow gave a significant contribution, leading to a slew of research developments.
By the late 1920s to early 1930s pathology was deemed a medical specialty. Combined with developments in the understanding of general physiology, by the beginning of the 20th century, the study of pathology had begun to split into a number of rarefied fields and resulting in the development of large number of modern specialties within pathology and related disciplines of diagnostic medicine.
General medical pathology.
The modern practice of pathology is divided into a number of subdisciplines within the discrete but deeply interconnected aims of biological research and medical practice. Biomedical research into disease incorporates the work of vast variety of life science specialists, whereas, in most parts of the world, to be licensed to practice pathology as medical specialty, one has to complete medical school and secure a license to practice medicine. Structurally, the study of disease is divided into many different fields which study or diagnose markers for disease using methods and technologies particular to specific scales, organs and tissue types. The information in this section mostly concerns pathology as it regards common medical practice in these systems, but each of these specialties is also the subject of voluminous pathology research as regards the disease pathways of specific pathogens and disorders that affect the tissues of these discrete organs or structures. (See also Gross pathology).
Anatomical pathology.
Anatomical pathology ("Commonwealth") or anatomic pathology ("United States") is a medical specialty that is concerned with the diagnosis of disease based on the gross, microscopic, chemical, immunologic and molecular examination of organs, tissues, and whole bodies (as in a general examination or an autopsy). Anatomical pathology is itself divided into subfields, the main divisions being surgical pathology, cytopathology, and forensic pathology. Anatomical pathology is one of two main divisions of the medical practice of pathology, the other being clinical pathology, the diagnosis of disease through the laboratory analysis of bodily fluids and tissues. Sometimes, pathologists practice both anatomical and clinical pathology, a combination known as general pathology.
Cytopathology.
Cytopathology (sometimes referred to as "cytology") is a branch of pathology that studies and diagnoses diseases on the cellular level. It is usually used to aid in the diagnosis of cancer, but also helps in the diagnosis of certain infectious diseases and other inflammatory conditions as well as thyroid lesions, diseases involving sterile body cavities (peritoneal, pleural, and cerebrospinal), and a wide range of other body sites. Cytopathology is generally used on samples of free cells or tissue fragments (in contrast to histopathology, which studies whole tissues) and cytopathologic tests are sometimes called smear tests because the samples may be smeared across a glass microscope slide for subsequent staining and microscopic examination. However, cytology samples may be prepared in other ways, including cytocentrifugation.
Dermatopathology.
Dermatopathology is a subspecialty of anatomic pathology that focuses on the skin and the rest of the integumentary system as an organ. It is unique in that there are two routes which a physician can use to obtain thespecialization. All general pathologists and general dermatologists are trained in the pathology of the skin, so the term dermatopathologist denotes either of these who has reached a certainly level accreditation and experience; in the USA, either a general pathologist or a dermatologist can undergo a 1 to 2 year fellowship in the field of dermatopathology. The completion of this fellowship allows one to take a subspecialty board examination, and becomes a board certified dermatopathologist. Dermatologists are able to recognize most skin diseases based on their appearances, anatomic distributions, and behavior. Sometimes, however, those criteria do not allow a conclusive diagnosis to be made, and a skin biopsy is taken to be examined under the microscope using usual histological tests. In some cases, additional specialized testing needs to be performed on biopsies, including immunofluorescence, immunohistochemistry, electron microscopy, flow cytometry, and molecular-pathologic analysis. One of the greatest challenges of dermatopathology is its scope. More than 1500 different disorders of the skin exist, including cutaneous eruptions ("rashes") and neoplasms. Therefore, dermatopathologists must maintain a broad base of knowledge in clinical dermatology, and be familiar with several other specialty areas in Medicine.
Forensic pathology.
Forensic pathology focuses on determining the cause of death by post-mortem examination of a corpse or partial remains. An autopsy is typically performed by a coroner or medical examiner, often during criminal investigations; in this role, Coroners and medical examiners are also frequently asked to confirm the identity of a corpse. The requirements for becoming a licensed practitioner of forensic pathology varies from country to country (and even within a given nation) but typically a minimal requirement is a medical doctorate with a specialty in general or anatomical pathology with subsequent study in forensic medicine. The methods utilized by forensic scientists to determine death include examination of tissue specimens in order to identify the presence or absence of natural disease and other microscopic findings, interpretations of toxicology on body tissues and fluids to determine the chemical cause of overdoses, poisonings or other cases involving toxic agents, and the examinations of physical trauma. Forensic pathology is a major component in the trans-disciplinary field of forensic science.
Histopathology.
Histopathology refers to the microscopic examination of various forms of human tissue. Specifically, in clinical medicine, histopathology refers to the examination of a biopsy or surgical specimen by a pathologist, after the specimen has been processed and histological sections have been placed onto glass slides. This contrasts with the methods of cytopathology which utilizes free cells or tissue fragments. Histopathological examination of tissues starts with surgery, biopsy, or autopsy. The tissue is removed from the body of an organism and then placed in a fixative which stabilizes the tissues to prevent decay. The most common fixative is formalin, although frozen section fixing is also common. To see the tissue under a microscope, the sections are stained with one or more pigments. The aim of staining is to reveal cellular components; counterstains are used to provide contrast. Histochemistry refers to the science of using chemical reactions between laboratory chemicals and components within tissue. The histological slides are then interpreted diagnostically and the resulting pathology report describes the histological findings and the opinion of the pathologist. In the case of cancer, this represents the tissue diagnosis required for most treatment protocols.
Neuropathology.
Neuropathology is the study of disease of nervous system tissue, usually in the form of either surgical biopsies or sometimes whole brains in the case of autopsy. Neuropathology is a subspecialty of anatomic pathology, neurology, and neurosurgery. In many English-speaking countries, neuropathology is considered a subfield of anatomical pathology. A physician who specializes in neuropathology, usually by completing a fellowship after a residency in anatomical or general pathology, is called a neuropathologist. In day-to-day clinical practice, a neuropathologist is a consultant for other physicians. If a disease of the nervous system is suspected, and the diagnosis cannot be made by less invasive methods, a biopsy of nervous tissue is taken from the brain or spinal cord to aid in diagnosis. Biopsy is usually requested after a mass is detected by medical imaging. With autopsies, the principal work of the neuropathologist is to help in the post-mortem diagnosis of various conditions that affect the central nervous system. Biopsies can also consist of the skin. Epidermal nerve fiber density testing (ENFD) is a more recently developed neuropathology test in which a punch skin biopsy is taken to identify small fiber neuropathies by analyzing the nerve fibers of the skin. This test is becoming available in select labs as well as many universities; it replaces the traditional nerve biopsy test as less invasive.
Pulmonary pathology.
Pulmonary pathology is the subspecialty of anatomic (and especially surgical) pathology which deals with the diagnosis and characterization of neoplastic and non-neoplastic diseases of the lungs and thoracic pleura. Diagnostic specimens are often obtained via bronchoscopic transbronchial biopsy, CT-guided percutaneous biopsy, or video-assisted thoracic surgery. These tests can be necessary to diagnose between infection, inflammation, or fibrotic conditions.
Renal pathology.
Renal pathology is a subspecialty of anatomic pathology that deals with the diagnosis and characterization of disease of the kidneys. In a medical setting, renal pathologists work closely with nephrologists and transplant surgeons, who typically obtain diagnostic specimens via percutaneous renal biopsy. The renal pathologist must synthesize findings from traditional microscope histology, electron microscopy, and immunofluorescence to obtain a definitive diagnosis. Medical renal diseases may affect the glomerulus, the tubules and interstitium, the vessels, or a combination of these compartments.
Surgical pathology.
Surgical pathology is one of the primary areas of practice for most anatomical pathologists. Surgical pathology involves the gross and microscopic examination of surgical specimens, as well as biopsies submitted by surgeons and non-surgeons such as general internists, medical subspecialists, dermatologists, and interventional radiologists. Often an excised tissue sample is the best and most definitive evidence of disease (or lack thereof) in cases where tissue is surgically removed from a patient. These determinations are usually accomplished by a combination of gross (i.e., macroscopic) and histologic (i.e., microscopic) examination of the tissue, and may involve evaluations of molecular properties of the tissue by immunohistochemistry or other laboratory tests.
There are two major types of specimens submitted for surgical pathology analysis: biopsies and surgical resections. A biopsy is a small piece of tissue removed primarily for the purposes of surgical pathology analysis, most often in order to render a definitive diagnosis. Types of biopsies include core biopsies, which are obtained through the use of large-bore needles, sometimes under the guidance of radiological techniques such as ultrasound, CT scan, or magnetic resonance imaging. Incisional biopsies are obtained through diagnostic surgical procedures that remove part of a suspicious lesion, whereas excisional biopsies remove the entire lesion, and are similar to therapeutic surgical resections. Excisional biopsies of skin lesions and gastrointestinal polyps are very common. The pathologist's interpretation of a biopsy is critical to establishing the diagnosis of a benign or malignant tumor, and can differentiate between different types and grades of cancer, as well as determining the activity of specific molecular pathways in the tumor. Surgical resection specimens are obtained by the therapeutic surgical removal of an entire diseased area or organ (and occasionally multiple organs). These procedures are often intended as definitive surgical treatment of a disease in which the diagnosis is already known or strongly suspected, but pathological analysis of these specimens remains important in confirming the previous diagnosis.
Clinical pathology.
Clinical pathology is a medical specialty that is concerned with the diagnosis of disease based on the laboratory analysis of bodily fluids such as blood and urine, as well as tissues, using the tools of chemistry, clinical microbiology, hematology and molecular pathology. Clinical pathologists work in close collaboration with medical technologists, hospital administrations, and referring physicians. Clinical pathologist learn to administer a number of visual and microscopic tests and an especially large variety of tests of the biophysical properties of tissue samples involving Automated analysers and cultures. Sometimes the general term "laboratory medicine specialist" is used to refer to those working in clinical pathology, including medical doctors, Ph.D.s and doctors of pharmacology. Immunopathology, the study of an organism's immune response to infection, is sometimes considered to fall within the domain of clinical pathology.
Hematopathology.
Hematopathology is the study of diseases of blood cells (including constituents such as white blood cells, red blood cells, and platelets) and the tissues, and organs comprising 
the hematopoietic system. The term hematopoietic system refers to tissues and organs that produce and/or primarily host hematopoietic cells and includes bone marrow, the lymph nodes, thymus, spleen, and other lymphoid tissues. In the United States, hematopathology is a board certified subspecialty (licensed under the American Board of Pathology) practiced by those physicians who have completed a general pathology residency (anatomic, clinical, or combined) and an additional year of fellowship training in hematology. The hematopathologist reviews biopsies of lymph nodes, bone marrows and other tissues involved by an infiltrate of cells of the hematopoietic system. In addition, the hematopathologist may be in charge of flow cytometric and/or molecular hematopathology studies.
Molecular pathology.
Molecular pathology is focused upon the study and diagnosis of disease through the examination of molecules within organs, tissues or bodily fluids. Molecular pathology is multidisciplinary by nature and shares some aspects of practice with both anatomic pathology and clinical pathology, molecular biology, biochemistry, proteomics and genetics. It is often applied in a context that is as much scientific as directly medical and encompasses the development of molecular and genetic approaches to the diagnosis and classification of human diseases, the design and validation of predictive biomarkers for treatment response and disease progression, and the susceptibility of individuals of different genetic constitution to particular disorders. The crossover between molecular pathology and epidemiology is represented by a related field "molecular pathological epidemiology".
 Molecular pathology is commonly used in diagnosis of cancer and infectious diseases. Techniques are numerous but include quantitative polymerase chain reaction (qPCR), multiplex PCR, DNA microarray, in situ hybridization, DNA sequencing, antibody based immunofluorescence tissue assays, molecular profiling of pathogens, and analysis of bacterial genes for antimicrobial resistance.
Oral and maxillofacial pathology.
Oral and Maxillofacial Pathology is one of nine dental specialties recognized by the American Dental Association, and is sometimes considered a specialty of both dentistry and pathology. Oral Pathologists must complete three years of post doctoral training in an accredited program and subsequently obtain diplomate status from the American Board of Oral and Maxillofacial Pathology. The specialty focuses on the diagnosis, clinical management and investigation of diseases that affect the oral cavity and surrounding maxillofacial structures including but not limited to odontogenic, infectious, epithelial, salivary gland, bone and soft tissue pathologies. It also significantly intersects with the field of dental pathology. Although concerned with a broad variety of diseases of the oral cavity, they have roles distinct from otorhinolaryngologists ("ear, nose, and throat" specialists), and speech pathologists, the latter of which helps diagnose many neurological or neuromuscular conditions relevant to speech phonology or swallowing. Owing to the availability of the oral cavity to non-invasive examination, many conditions in the study of oral disease can be diagnosed, or at least suspected, from gross examination, but biopsies, cell smears, and other tissue analysis remain important diagnostic tools in oral pathology.
Medical training and accreditation.
Individual nations vary some in the medical licensing required of pathologists. In the United States, pathologists are physicians (D.O. or M.D.) that have completed a four-year undergraduate program, four years of medical school training, and three to four years of postgraduate training in the form of a pathology residency. Training may be within two primary specialties, as recognized by the American Board of Pathology: anatomical Pathology and clinical Pathology, each of which requires separate board certification. The American Osteopathic Board of Pathology also recognizes four primary specialties: anatomic pathology, dermatopathology, forensic pathology, and laboratory medicine. Pathologists may pursue specialised fellowship training within one or more subspecialties of either anatomical or clinical pathology. Some of these subspecialties permit additional board certification, while others do not.
In the United Kingdom, pathologists are physicians licensed by the UK General Medical Council. The training to become a pathologist is under the oversight of the Royal College of Pathologists. After four to six years of undergraduate medical study, trainees proceed to a two-year foundation program. Full-time training in histopathology currently lasts between five and five and a half years and includes specialist training in surgical pathology, cytopathology, and autopsy pathology. It is also possible to take a Royal College of Pathologists diploma in forensic pathology, dermatopathology, or cytopathology, recognising additional specialist training and expertise and to get specialist accreditation in forensic pathology, pediatric pathology, and neuropathology. All postgraduate medical training and education in the UK is overseen by the General Medical Council.
In France, Pathology is separate in two distinct specialties, anatomical pathology and clinical pathology. Residencies for both lasts four years. Residency in anatomical pathology is open to physicians only, while clinical pathology is open to both physicians and pharmacists. Anatomical pathology in France is integrated in the internal medicine specialty track. At the end of the second year of clinical pathology residency, residents can choose between general clinical pathology and a specialization in one of the disciplines, but they can not practice anatomical pathology, nor can anatomical pathology residents can not practice clinical pathology.
Overlap with other diagnostic medicine.
Although separate fields in terms of medical practice, there are a number of areas of inquiry in medicine and
medical science which either overlap greatly with general pathology, work in tandem with it, or which contribute significantly to the understanding of the pathology of a given disease or its course in an individual. As a significant portion of all general pathology practice is concerned with cancer, the practice of oncology is deeply tied to, and dependent upon, the work of both anatomical and clinical pathologists. Biopsy, resection and blood tests are all examples of pathology work that is essential for the diagnoses of many kinds of cancer and for the staging of cancerous masses. In a similar fashion, the tissue and blood analysis techniques of general pathology are of central significance to the investigation of serious infectious disease and as such inform significantly upon the fields of epidemiology, etiology, immunology, and parasitology. General pathology methods are of great importance to biomedical research into disease, wherein they are sometimes referred to as "experimental" or "investigative" pathology.
Medical imaging is the process of generating visual representations of the interior of a body for clinical analysis and medical intervention, for the purpose of revealing details internal physiology in order to plan appropriate treatments for tissue infection and trauma. Medical imaging is also central in supplying the biometric data necessary to establish baseline features of anatomy and physiology so as to increase the accuracy with which early or fine-detail abnormalities are detected. These diagnostic techniques are often performed in combination with general pathology procedures and are themselves often essential to developing new understanding of the pathogenesis of a given disease and tracking the progress of disease in specific medical cases. Examples of important subdivisions in medical imaging include radiology (which uses the imaging technologies of X-ray radiography) magnetic resonance imaging, medical ultrasonography (or ultrasound), endoscopy, elastography, tactile imaging, thermography, medical photography, nuclear medicine and functional imaging techniques such as positron emission tomography. Though they do not strictly relay images, readings from diagnostics tests involving electroencephalography, magnetoencephalography, and electrocardiography often give hints as to the state and function of certain tissues in the brain and heart respectively.
Psychopathology.
Psychopathology is the study of mental illness, particularly of severe disorders. Informed heavily by both psychology and neurology, its purpose is to classify mental illness, elucidate its underlying causes, and guide clinical psychiatric treatment accordingly. Although diagnosis and classification of mental norms and disorders is largely the purview of psychiatry—the results of which are guidelines such as the Diagnostic and Statistical Manual of Mental Disorders, which attempt to classify mental disease mostly on behavioural evidence, though not without controversy—the field is also heavily, and increasingly, informed upon by neuroscience and other of the biological cognitive sciences. Mental or social disorders or behaviours which are seen to be generally unhealthy or excessive in a given individual to the point where they cause harm or severe disruption to the sufferer's lifestyle are often given the handle of "pathological" (e.g. pathological gambling or pathological liar).
Study of pathology in non-humans.
Although the vast majority of lab work and research in pathology concerns the development of disease in humans, pathology is of significance throughout the biological sciences. Two main catch-all fields exist to represent most complex organisms capable of serving as host to a pathogen or other form of disease: veterinary pathology (concerned with all non-human species of kingdom of Animalia) and phytopathology, which studies disease in plants.
Veterinary pathology.
Veterinary pathology covers a vast array of species, but with a significantly smaller number of practitioners, so understanding of disease in non-human animals, especially as regards veterinary practice, varies considerably by species. Nonetheless, significant amounts of pathology research are conducted on animals, for two primary reasons: 1) The origins of diseases are typically zoonotic in nature, and many infectious pathogens have animal vectors and, as such, understanding the mechanisms of action for these pathogens in non-human hosts is essential to the understanding and application of epidemiology and 2) those animals which share physiological and genetic traits with humans can be used as surrogates for the study of the disease and potential treatments as well as the effects of various synthetic products. For this reason, as well as their roles as livestock and companion animals, mammals generally have the largest body of research in veterinary pathology. Animal testing remains a controversial practice, even in cases where it is used to research treatment for human disease. As in human medical pathology, the practice of veterinary pathology is customarily divided into the two main fields of anatomical and clinical pathology.
Phytopathology.
Although the pathogens and their mechanics differ greatly from those of animals, plants are subject to a wide variety of diseases, including those caused by fungi, oomycetes, bacteria, viruses, viroids, virus-like organisms, phytoplasmas, protozoa, nematodes and parasitic plants. Damage caused by insects, mites, vertebrate, and other small herbivores is not considered a part of the domain of plant pathology. The field is deeply connected to plant disease epidemiology and the horticulture of species that are of high importance to the human diet or other uses.

</doc>
<doc id="48803" url="http://en.wikipedia.org/wiki?curid=48803" title="Gamma-ray burst">
Gamma-ray burst

Gamma-ray bursts (GRBs) are flashes of gamma rays associated with extremely energetic explosions that have been observed in distant galaxies. They are the brightest electromagnetic events known to occur in the universe. Bursts can last from ten milliseconds to several hours. The initial burst is usually followed by a longer-lived "afterglow" emitted at longer wavelengths (X-ray, ultraviolet, optical, infrared, microwave and radio).
Most observed GRBs are believed to consist of a narrow beam of intense radiation released during a supernova or hypernova as a rapidly rotating, high-mass star collapses to form a neutron star, quark star, or black hole. A subclass of GRBs (the "short" bursts) appear to originate from a different process – this may be due to the merger of binary neutron stars. The cause of the precursor burst observed in some of these short events may be due to the development of a resonance between the crust and core of such stars as a result of the massive tidal forces experienced in the seconds leading up to their collision, causing the entire crust of the star to shatter.
The sources of most GRBs are billions of light years away from Earth, implying that the explosions are both extremely energetic (a typical burst releases as much energy in a few seconds as the Sun will in its entire 10-billion-year lifetime) and extremely rare (a few per galaxy per million years). All observed GRBs have originated from outside the Milky Way galaxy, although a related class of phenomena, soft gamma repeater flares, are associated with magnetars within the Milky Way. It has been hypothesized that a gamma-ray burst in the Milky Way, pointing directly towards the Earth, could cause a mass extinction event.
GRBs were first detected in 1967 by the Vela satellites, a series of satellites designed to detect covert nuclear weapons tests. Hundreds of theoretical models were proposed to explain these bursts in the years following their discovery, such as collisions between comets and neutron stars. Little information was available to verify these models until the 1997 detection of the first X-ray and optical afterglows and direct measurement of their redshifts using optical spectroscopy, and thus their distances and energy outputs. These discoveries, and subsequent studies of the galaxies and supernovae associated with the bursts, clarified the distance and luminosity of GRBs. These facts definitively placed them in distant galaxies and also connected long GRBs with the explosion of massive stars, the only possible source for the energy outputs observed.
History.
Gamma-ray bursts were first observed in the late 1960s by the U.S. Vela satellites, which were built to detect gamma radiation pulses emitted by nuclear weapons tested in space. The United States suspected that the USSR might attempt to conduct secret nuclear tests after signing the Nuclear Test Ban Treaty in 1963. On July 2, 1967, at 14:19 UTC, the Vela 4 and Vela 3 satellites detected a flash of gamma radiation unlike any known nuclear weapons signature. Uncertain what had happened but not considering the matter particularly urgent, the team at the Los Alamos Scientific Laboratory, led by Ray Klebesadel, filed the data away for investigation. As additional Vela satellites were launched with better instruments, the Los Alamos team continued to find inexplicable gamma-ray bursts in their data. By analyzing the different arrival times of the bursts as detected by different satellites, the team was able to determine rough estimates for the sky positions of sixteen bursts and definitively rule out a terrestrial or solar origin. The discovery was declassified and published in 1973 as an "Astrophysical Journal" article entitled "Observations of Gamma-Ray Bursts of Cosmic Origin".
Many theories were advanced to explain these bursts, most of which posited nearby sources within the Milky Way Galaxy. Little progress was made until the 1991 launch of the Compton Gamma Ray Observatory and its Burst and Transient Source Explorer (BATSE) instrument, an extremely sensitive gamma-ray detector. This instrument provided crucial data that showed the distribution of GRBs is isotropic—not biased towards any particular direction in space, such as toward the galactic plane or the galactic center. Because of the flattened shape of the Milky Way Galaxy, if the sources were from within our own galaxy they would be strongly concentrated in or near the galactic plane. The absence of any such pattern in the case of GRBs provided strong evidence that gamma-ray bursts must come from beyond the Milky Way. However, some Milky Way models are still consistent with an isotropic distribution.
Counterpart objects as candidate sources.
For decades after the discovery of GRBs, astronomers searched for a counterpart at other wavelengths: i.e., any astronomical object in positional coincidence with a recently observed burst. Astronomers considered many distinct classes of objects, including white dwarfs, pulsars, supernovae, globular clusters, quasars, Seyfert galaxies, and BL Lac objects. All such searches were unsuccessful, and in a few cases particularly well-localized bursts (those whose positions were determined with what was then a high degree of accuracy) could be clearly shown to have no bright objects of any nature consistent with the position derived from the detecting satellites. This suggested an origin of either very faint stars or extremely distant galaxies. Even the most accurate positions contained numerous faint stars and galaxies, and it was widely agreed that final resolution of the origins of cosmic gamma-ray bursts would require both new satellites and faster communication.
Afterglow.
Several models for the origin of gamma-ray bursts postulated that the initial burst of gamma rays should be followed by slowly fading emission at longer wavelengths created by collisions between the burst ejecta and interstellar gas. This fading emission would be called the "afterglow." Early searches for this afterglow were unsuccessful, largely due to the difficulties in observing a burst's position at longer wavelengths immediately after the initial burst. The breakthrough came in February 1997 when the satellite BeppoSAX detected a gamma-ray burst (GRB 970228) and when the X-ray camera was pointed towards the direction from which the burst had originated, it detected fading X-ray emission. The William Herschel Telescope identified a fading optical counterpart 20 hours after the burst. Once the GRB faded, deep imaging was able to identify a faint, distant host galaxy at the location of the GRB as pinpointed by the optical afterglow.
Because of the very faint luminosity of this galaxy, its exact distance was not measured for several years. Well before then, another major breakthrough occurred with the next event registered by BeppoSAX, GRB 970508. This event was localized within four hours of its discovery, allowing research teams to begin making observations much sooner than any previous burst. The spectrum of the object revealed a redshift of "z" = 0.835, placing the burst at a distance of roughly 6 billion light years from Earth. This was the first accurate determination of the distance to a GRB, and together with the discovery of the host galaxy of 970228 proved that GRBs occur in extremely distant galaxies. Within a few months, the controversy about the distance scale ended: GRBs were extragalactic events originating within faint galaxies at enormous distances. The following year, GRB 980425 was followed within a day by a coincident bright supernova (SN 1998bw), indicating a clear connection between GRBs and the deaths of very massive stars. This burst provided the first strong clue about the nature of the systems that produce GRBs.
BeppoSAX functioned until 2002 and CGRO (with BATSE) was deorbited in 2000. However, the revolution in the study of gamma-ray bursts motivated the development of a number of additional instruments designed specifically to explore the nature of GRBs, especially in the earliest moments following the explosion. The first such mission, HETE-2, launched in 2000 and functioned until 2006, providing most of the major discoveries during this period. One of the most successful space missions to date, Swift, was launched in 2004 and as of 2014 is still operational. Swift is equipped with a very sensitive gamma ray detector as well as on-board X-ray and optical telescopes, which can be rapidly and automatically slewed to observe afterglow emission following a burst. More recently, the Fermi mission was launched carrying the Gamma-Ray Burst Monitor, which detects bursts at a rate of several hundred per year, some of which are bright enough to be observed at extremely high energies with Fermi's Large Area Telescope. Meanwhile, on the ground, numerous optical telescopes have been built or modified to incorporate robotic control software that responds immediately to signals sent through the Gamma-ray Burst Coordinates Network. This allows the telescopes to rapidly repoint towards a GRB, often within seconds of receiving the signal and while the gamma-ray emission itself is still ongoing.
New developments over the past few years include the recognition of short gamma-ray bursts as a separate class (likely due to merging neutron stars and not associated with supernovae), the discovery of extended, erratic flaring activity at X-ray wavelengths lasting for many minutes after most GRBs, and the discovery of the most luminous (GRB 080319B) and the former most distant (GRB 090423) objects in the universe. The most distant known GRB, GRB 090429B, is now the most distant known object in the universe.
Classification.
The light curves of gamma-ray bursts are extremely diverse and complex. No two gamma-ray burst light curves are identical, with large variation observed in almost every property: the duration of observable emission can vary from milliseconds to tens of minutes, there can be a single peak or several individual subpulses, and individual peaks can be symmetric or with fast brightening and very slow fading. Some bursts are preceded by a "precursor" event, a weak burst that is then followed (after seconds to minutes of no emission at all) by the much more intense "true" bursting episode. The light curves of some events have extremely chaotic and complicated profiles with almost no discernible patterns.
Although some light curves can be roughly reproduced using certain simplified models, little progress has been made in understanding the full diversity observed. Many classification schemes have been proposed, but these are often based solely on differences in the appearance of light curves and may not always reflect a true physical difference in the progenitors of the explosions. However, plots of the distribution of the observed duration for a large number of gamma-ray bursts show a clear bimodality, suggesting the existence of two separate populations: a "short" population with an average duration of about 0.3 seconds and a "long" population with an average duration of about 30 seconds. Both distributions are very broad with a significant overlap region in which the identity of a given event is not clear from duration alone. Additional classes beyond this two-tiered system have been proposed on both observational and theoretical grounds.
Short gamma-ray bursts.
Events with a duration of less than about two seconds are classified as short gamma-ray bursts. These account for about 30% of gamma-ray bursts, but until 2005, no afterglow had been successfully detected from any short event and little was known about their origins. Since then, several dozen short gamma-ray burst afterglows have been detected and localized, several of which are associated with regions of little or no star formation, such as large elliptical galaxies and the central regions of large galaxy clusters. This rules out a link to massive stars, confirming that short events are physically distinct from long events. In addition, there has been no association with supernovae.
The true nature of these objects (or even whether the current classification scheme is accurate) remains unknown, although the leading hypothesis is that they originate from the mergers of binary neutron stars or a neutron star with a black hole. Such mergers might also be expected to produce kilonovae, and evidence for a kilonova associated with GRB 130603B has been seen. The mean duration of these events of 0.2 seconds suggests a source of very small physical diameter in stellar terms; less than 0.2 light-seconds (about 37,000 miles—four times the Earth's diameter). This further suggests a very compact object as the source. The observation of minutes to hours of X-ray flashes after a short gamma-ray burst is consistent with small particles of a primary object like a neutron star initially swallowed by a black hole in less than two seconds, followed by some hours of lesser energy events, as remaining fragments of tidally disrupted neutron star material (no longer neutronium) remain in orbit to spiral into the black hole, over a longer period of time. A small fraction of short gamma-ray bursts are probably produced by giant flares from soft gamma repeaters in nearby galaxies.
Long gamma-ray bursts.
Most observed events (70%) have a duration of greater than two seconds and are classified as long gamma-ray bursts. Because these events constitute the majority of the population and because they tend to have the brightest afterglows, they have been studied in much greater detail than their short counterparts. Almost every well-studied long gamma-ray burst has been linked to a galaxy with rapid star formation, and in many cases to a core-collapse supernova as well, unambiguously associating long GRBs with the deaths of massive stars. Long GRB afterglow observations, at high redshift, are also consistent with the GRB having originated in star-forming regions.
Ultra-long gamma-ray bursts.
These events are at the tail end of the long GRB duration distribution, lasting more than 10,000 seconds. They have been proposed to form a separate class, possibly the result of the collapse of a blue supergiant star. Only a small number have been identified to date, their primary characteristic being their gamma ray emission duration. So far, the known and well established ultra long GRBs are GRB 091024A, GRB 101225A, and GRB 111209A. A recent study, on the other hand, shows that the existing evidence for a separate ultra-long GRB population with a new type of progenitor is inconclusive, and further multi-wavelength observations are needed to draw a firmer conclusion.
Tidal disruption events.
This new class of GRB-like events was first discovered through the detection of GRB 110328A by the Swift Gamma-Ray Burst Mission on 28 March 2011. This event had a gamma-ray duration of about 2 days, much longer than even ultra-long GRBs, and was detected in X-rays for many months. It occurred at the center of a small elliptical galaxy at redshift z = 0.3534. There is an ongoing debate as to whether the explosion was the result of stellar collapse or a tidal disruption event accompanied by a relativistic jet, although the latter explanation has become widely favoured.
A tidal disruption event of this sort is when a star interacts with a supermassive black hole shredding the star, and in some cases creating a relativistic jet which produces bright emission of gamma ray radiation. The event GRB 110328A (also denoted Swift J1644+57) was initially argued to be produced by the disruption of main sequence star by a black hole of several million times the mass of the Sun, although it has subsequently been argued that the disruption of a white dwarf by a black hole of mass about 10 thousand times the Sun may be more likely.
Energetics and beaming.
Gamma-ray bursts are very bright as observed from Earth despite their typically immense distances. An average long GRB has a bolometric flux comparable to a bright star of our galaxy despite a distance of billions of light years (compared to a few tens of light years for most visible stars). Most of this energy is released in gamma rays, although some GRBs have extremely luminous optical counterparts as well. GRB 080319B, for example, was accompanied by an optical counterpart that peaked at a visible magnitude of 5.8, comparable to that of the dimmest naked-eye stars despite the burst's distance of 7.5 billion light years. This combination of brightness and distance implies an extremely energetic source. Assuming the gamma-ray explosion to be spherical, the energy output of GRB 080319B would be within a factor of two of the rest-mass energy of the Sun (the energy which would be released were the Sun to be converted entirely into radiation).
No known process in the Universe can produce this much energy in such a short time. Rather, gamma-ray bursts are thought to be highly focused explosions, with most of the explosion energy collimated into a narrow jet traveling at speeds exceeding 99.995% of the speed of light. The approximate angular width of the jet (that is, the degree of spread of the beam) can be estimated directly by observing the achromatic "jet breaks" in afterglow light curves: a time after which the slowly decaying afterglow begins to fade rapidly as the jet slows and can no longer beam its radiation as effectively. Observations suggest significant variation in the jet angle from between 2 and 20 degrees.
Because their energy is strongly focused, the gamma rays emitted by most bursts are expected to miss the Earth and never be detected. When a gamma-ray burst is pointed towards Earth, the focusing of its energy along a relatively narrow beam causes the burst to appear much brighter than it would have been were its energy emitted spherically. When this effect is taken into account, typical gamma-ray bursts are observed to have a true energy release of about 1044 J, or about 1/2000 of a Solar mass (M☉) energy equivalent—which is still many times the mass-energy equivalent of the Earth (about 5.5 × 1041 J). This is comparable to the energy released in a bright type Ib/c supernova and within the range of theoretical models. Very bright supernovae have been observed to accompany several of the nearest GRBs. Additional support for focusing of the output of GRBs has come from observations of strong asymmetries in the spectra of nearby type Ic supernova and from radio observations taken long after bursts when their jets are no longer relativistic.
Short (time duration) GRBs appear to come from a lower-redshift (i.e. less distant) population and are less luminous than long GRBs. The degree of beaming in short bursts has not been accurately measured, but as a population they are likely less collimated than long GRBs or possibly not collimated at all in some cases.
Progenitors.
Because of the immense distances of most gamma-ray burst sources from Earth, identification of the progenitors, the systems that produce these explosions, is particularly challenging. The association of some long GRBs with supernovae and the fact that their host galaxies are rapidly star-forming offer very strong evidence that long gamma-ray bursts are associated with massive stars. The most widely accepted mechanism for the origin of long-duration GRBs is the collapsar model, in which the core of an extremely massive, low-metallicity, rapidly rotating star collapses into a black hole in the final stages of its evolution. Matter near the star's core rains down towards the center and swirls into a high-density accretion disk. The infall of this material into a black hole drives a pair of relativistic jets out along the rotational axis, which pummel through the stellar envelope and eventually break through the stellar surface and radiate as gamma rays. Some alternative models replace the black hole with a newly formed magnetar, although most other aspects of the model (the collapse of the core of a massive star and the formation of relativistic jets) are the same.
The closest analogs within the Milky Way galaxy of the stars producing long gamma-ray bursts are likely the Wolf–Rayet stars, extremely hot and massive stars which have shed most or all of their hydrogen due to radiation pressure. Eta Carinae and WR 104 have been cited as possible future gamma-ray burst progenitors. It is unclear if any star in the Milky Way has the appropriate characteristics to produce a gamma-ray burst.
The massive-star model probably does not explain all types of gamma-ray burst. There is strong evidence that some short-duration gamma-ray bursts occur in systems with no star formation and where no massive stars are present, such as elliptical galaxies and galaxy halos. The favored theory for the origin of most short gamma-ray bursts is the merger of a binary system consisting of two neutron stars. According to this model, the two stars in a binary slowly spiral towards each other due to the release of energy via gravitational radiation until the neutron stars suddenly rip each other apart due to tidal forces and collapse into a single black hole. The infall of matter into the new black hole produces an accretion disk and releases a burst of energy, analogous to the collapsar model. Numerous other models have also been proposed to explain short gamma-ray bursts, including the merger of a neutron star and a black hole, the accretion-induced collapse of a neutron star, or the evaporation of primordial black holes.
An alternative explanation proposed by Friedwardt Winterberg is that in the course of a gravitational collapse and in reaching the event horizon of a black hole, all matter disintegrates into a burst of gamma radiation.
Emission mechanisms.
The means by which gamma-ray bursts convert energy into radiation remains poorly understood, and as of 2010 there was still no generally accepted model for how this process occurs. Any successful model of GRB emission must explain the physical process for generating gamma-ray emission that matches the observed diversity of light curves, spectra, and other characteristics. Particularly challenging is the need to explain the very high efficiencies that are inferred from some explosions: some gamma-ray bursts may convert as much as half (or more) of the explosion energy into gamma-rays. Recent observations of the bright optical counterpart of GRB 080319B, whose light curve was correlated with the gamma-ray light curve, has suggested that inverse Compton may be the dominant process in some events. In this model, pre-existing low-energy photons are scattered by relativistic electrons within the explosion, augmenting their energy by a large factor and transforming them into gamma-rays.
The nature of the longer-wavelength afterglow emission (ranging from X-ray through radio) that follows gamma-ray bursts is better understood. Any energy released by the explosion not radiated away in the burst itself takes the form of matter or energy moving outward at nearly the speed of light. As this matter collides with the surrounding interstellar gas, it creates a relativistic shock wave that then propagates forward into interstellar space. A second shock wave, the reverse shock, may propagate back into the ejected matter. Extremely energetic electrons within the shock wave are accelerated by strong local magnetic fields and radiate as synchrotron emission across most of the electromagnetic spectrum. This model has generally been successful in modeling the behavior of many observed afterglows at late times (generally, hours to days after the explosion), although there are difficulties explaining all features of the afterglow very shortly after the gamma-ray burst has occurred.
Rate of occurrence and potential effects on life on Earth.
All GRBs observed to date have occurred well outside the Milky Way galaxy and have been harmless to Earth. However, if a GRB were to occur within the Milky Way, and its emission were beamed straight towards Earth, the effects could be devastating for the planet. Currently, orbiting satellites detect on average approximately one GRB per day. The closest observed GRB as of March 2014 was GRB 980425, located 40Mpc (130 million light years) away in a (z=0.0085) SBc-type dwarf galaxy. GRB 980425 was far less energetic than the average GRB and was associated with the Type Ib supernova SN 1998bw.
Estimating the exact rate at which GRBs occur is difficult, but for a galaxy of approximately the same size as the Milky Way, the expected rate (for long-duration GRBs) is about one burst every 100,000 to 1,000,000 years. Only a small percentage of these would be beamed towards Earth. Estimates of rate of occurrence of short-duration GRBs are even more uncertain because of the unknown degree of collimation, but are probably comparable.
Since GRBs are thought to involve beamed emission along two jets in opposing directions, only planets in the path of these jets would be subjected to the high energy gamma radiation.
Depending on its distance from Earth, a GRB and its ultraviolet radiation could damage even the most radiation resistant organism known, the bacterium "Deinococcus radiodurans". These bacteria can endure 2,000 times more radiation than humans. Life surviving an initial onslaught, including those located on the side of the earth facing away from the burst, would have to contend with the potentially lethal after-effect of the depletion of the atmosphere's protective ozone layer by the burst.
Hypothetical effects of gamma-ray bursts in the past.
GRBs close enough to affect life in some way might occur once every five million years or so – around a thousand times since life on Earth began.
The major Ordovician–Silurian extinction events of 450 million years ago may have been caused by a GRB. The late Ordovician species of trilobite that spent some of its life in the plankton layer near the ocean surface was much harder hit than deep-water dwellers, which tended to stay put within quite restricted areas. Usually it is the more widely spread species that fare better in extinction, and hence this unusual pattern could be explained by a GRB, which would probably devastate creatures living on land and near the ocean surface, but leave deep-sea creatures relatively unharmed.
A case has been made that the cause of the 774–775 carbon-14 spike was the result of a short GRB.
Hypothetical effects of gamma-ray bursts in the future.
The greatest danger is believed to come from Wolf–Rayet stars, regarded by astronomers as likely GRB candidates. When such stars transition to supernovae, they may emit intense beams of gamma rays, and if Earth were to lie in the beam zone, devastating effects may occur. Gamma rays would not penetrate Earth's atmosphere to impact the surface directly, but they would chemically damage the stratosphere.
For example, if WR 104, at a distance of 8,000 light-years, were to hit Earth with a burst of 10 seconds duration, its gamma rays could deplete about 25 percent of the world's ozone layer. This would result in mass extinction, food chain depletion, and starvation. The side of Earth facing the GRB would receive potentially lethal radiation exposure, which can cause radiation sickness in the short term, and, in the long term, results in serious impacts to life due to ozone layer depletion.
Effects after exposure to the gamma-ray burst on Earth's atmosphere.
Longer-term, gamma ray energy may cause chemical reactions involving oxygen and nitrogen molecules which may create nitrogen oxide then nitrogen dioxide gas, causing photochemical smog. The GRB may produce enough of the gas to cover the sky and darken it. Gas would prevent sunlight from reaching Earth's surface, producing a "cosmic winter" effect – a similar situation to an impact winter, but not caused by an impact. GRB-produced gas could also even further deplete the ozone layer.
References.
</dl>

</doc>
<doc id="48804" url="http://en.wikipedia.org/wiki?curid=48804" title="London Marathon">
London Marathon

The London Marathon (also known as the Virgin Money London Marathon) is a long-distance running event held in London, United Kingdom and is the third largest running event in the UK, after the Great North Run from Newcastle upon Tyne to South Shields and the Great Manchester Run 10,000 metre run around central Manchester, and it is also part of the World Marathon Majors. The event was first run on 29 March 1981 and has been held in the spring of every year since. Since 2010, the race has been sponsored by Virgin Money. The most recent event was the 2015 London Marathon on 26 April 2015.
Overview.
The race was founded by the former Olympic champion and journalist Chris Brasher and athlete John Disley. It is organised by Hugh Brasher (son of Chris) as Race Director and Nick Bitel as Chief Executive. Set over a largely flat course around the River Thames, the race begins at three separate points around Blackheath and finishes in The Mall alongside St. James's Park. Since the first marathon, the course has undergone very few route changes. In 1982, the finishing post was moved from Constitution Hill to Westminster Bridge due to construction works. It remained there for twelve years before moving to its present location at The Mall.
In addition to being one of the top five international marathons run over the distance of 26 miles and 385 yards, the IAAF standard for the marathon established in 1921 and originally used for the 1908 London Olympics, the London Marathon is also a large, celebratory sporting festival, third only to the Great North Run in South Shields and Great Manchester Run in Manchester in terms of the number of participants. The event has raised over £450 million for charity since 1981, and holds the Guinness world record as the largest annual fund raising event in the world, with the 2009 participants raising over £47.2 million for charity. In 2007, 78% of all runners raised money. In 2011 the official charity of the London Marathon was Oxfam. In 2014, the official charity was Anthony Nolan, and in 2015, it will be Cancer Research UK.
History.
The London Marathon was not the first long-distance running event held in the city, which has a long history of marathon events. The Polytechnic Marathon (also known as the "Poly") was first held in 1909.
The current London Marathon was founded in 1981 by former Olympic champion and journalist Chris Brasher and athlete John Disley. Shortly after completing the New York City Marathon in November 1979 Brasher wrote an article for "The Observer" newspaper which began:
To believe this story you must believe that the human race be one joyous family, working together, laughing together, achieving the impossible. Last Sunday, in one of the most trouble-stricken cities in the world, 11,532 men and women from 40 countries in the world, assisted by over a million black, white and yellow people, laughed, cheered and suffered during the greatest folk festival the world has seen.
Inspired by the people of New York coming together for this occasion, he asked "whether London could stage such a festival?" The following year Brasher and Disley made trips to America to study the organisation and finance of big city marathons (such as those in New York and Boston). Brasher signed a contract with Gillette for £50,000, established charitable status and outlined six main aims in the hope to mirror the scenes he witnessed in New York and establish the United Kingdom on the map as a country capable of arranging major events. The London Marathon was born.
The first London Marathon was held on 29 March 1981, more than 20,000 applied to run. 6,747 were accepted and 6,255 crossed the finish line on Constitution Hill. The Marathon's popularity has steadily grown since then. As at 2009, 746,635 people have completed the race since its inception. In 2010, 36,549 people crossed the line, the biggest field since the race began. The first wheelchair marathon race was held in 1983 and the event was credited with reducing the stigma surrounding disabled athletes. In 2013 the IPC Athletics Marathon World Cup was held within the London Marathon featuring athletes of both genders in the T42–T46 and T11–T13 categories. In August 2013 it was announced that the event would be staged in London until 2017 and feature athletes in the T11-T12, T13, T42-T44, T43, T45-46, T51-52 and the T53-54 class.
For many years the London and Polytechnic Marathons competed with each other until, in 1996, the latter folded in due to the popularity of the former. Eleven participants have died since the event began, the most recent being in 2012 when a 30-year-old woman, Claire Squires, collapsed whilst running along Birdcage Walk, near St. James's Park. She had opened a JustGiving page to raise money for the Samaritans; as news of her death circulated, the number of donations increased from £500 to more than £1 million.
Following the Boston Marathon bombings, organisers of the 2013 London Marathon undertook a review of their security arrangements, despite no specific threats against the event. A 30 second silence was held before the start of the marathon to show respect and support to those affected by the tragedy.
Organisation.
The race is currently organised by Hugh Brasher, son of Chris, as Race Director and Nick Bitel as Chief Executive. Previously David Bedford and Bitel had overseen a period of great change for the race, including amendments to the course in 2005 which saw the cobbled section by the Tower of London replaced with a flat stretch along the Highway.
Dr Dan Tunstall-Pedoe, was the medical director of the London Marathon for 25 years between the first one in 1981 until 2005. In 2003, Dr Tunstall-Pedoe was shadowed by Professor Sanjay Sharma from St George's Hospital (University of London) who took over the role in its entirety in 2006. Medical cover is provided by 150 highly experienced doctors in internal medicine, intensive care, sports medicine, orthopaedics and anaesthetics. The doctors are assisted by more than 1,500 volunteers of St. John Ambulance, who organise over 50 first aid posts along the route, and three field hospitals at the finish. St John Ambulance also provide a large number of Healthcare Professionals for the event, including a vast number of Nurses and Paramedics. They also provide a large number of Ambulances and Ambulance Crews for use at the event and also across London to support the NHS Ambulance Service.
The BBC covers the event, devoting rolling coverage for most of the morning. The theme music associated with this coverage, and with the event itself, is called "Main Titles to The Trap", composed by Ron Goodwin for the film "The Trap".
There are three separate groups of starters: Elite Women, Wheelchair (Men and Women), and Elite Men followed by Mass Race.
Course.
The marathon is run over a largely flat course around the River Thames, and spans 42.195 kilometres (26 miles and 385 yards).
The route has markers at one mile intervals. Although the race publicity (athlete advice, timing charts and so on) is mile-oriented, the individual timing splits that are available to competitors after the event are kilometre-oriented.
The course begins at three separate points: the 'red start' in southern Greenwich Park on Charlton Way, the 'green start' in St John's Park, and the 'blue start' on Shooter's Hill Road. From these points around Blackheath at 35 m above sea level, south of the River Thames, the route heads east through Charlton. The three courses converge after 4.5 km in Woolwich, close to the Royal Artillery Barracks.
As the runners reach the 10 km, they pass by the Old Royal Naval College and head towards "Cutty Sark" drydocked in Greenwich. Heading next into Surrey Quays in the Docklands, and out towards Bermondsey, competitors race along Jamaica Road before reaching the half-way point as they cross Tower Bridge. Running east again along The Highway through Wapping, competitors head up towards Limehouse and into Mudchute in the Isle of Dogs via Westferry Road, before heading into Canary Wharf.
As the route leads away from Canary Wharf into Poplar, competitors run west down Poplar High Street back towards Limehouse and on through Commercial Road. They then move back onto The Highway, onto Lower and Upper Thames Streets. Heading into the final leg of the race, competitors pass The Tower of London on Tower Hill. In the penultimate mile along The Embankment, the London Eye comes into view, before the athletes turn right into Birdcage Walk to complete the final 352 m, catching the sights of Big Ben and Buckingham Palace, and finishing in The Mall alongside St. James's Palace. This final section of the route formed part of the 2012 Olympic Marathon Course.
Since the first marathon, the course has undergone very few route changes. In the first race, the course took a diversion around Southwark Park before re-joining Jamaica Road on the way to Tower Bridge and was routed through St Katherine Docks past the Tower Hotel, en route to the Tower of London and the cobblestoned stretch of road that in later years was carpeted, to help runners prevent injury on the uneven surface. In 1982, the finishing post was moved from Constitution Hill to Westminster Bridge due to construction works. It remained there for twelve years before moving to its present location at The Mall. In 2005, the route around the Isle of Dogs between 22 and was switched from a clockwise to an anti-clockwise direction, and at 35 km the route was diverted to avoid St Katherine Docks and the cobblestoned area near the Tower of London. In 2008, a suspected gas leak at a pub in Wapping diverted the course, but in 2009 the race followed the same path as in 2007.
Results.
London is one of the top six world marathons that form the World Marathon Majors competition with a million prize purse. The inaugural marathon had 7,741 entrants, 6,255 of whom completed the race. The first Men's Elite Race in 1981 was tied between American Dick Beardsley and Norwegian Inge Simonsen, who crossed the finish line holding hands in 2 hours, 11 minutes, 48 seconds. The first Women's Elite Race, also in 1981, was won by Briton Joyce Smith in 2:29:57. In 1983, the first wheelchair races took place. Organized by the British Sports Association for the Disabled (BASD), 19 people competed and 17 finished. Gordon Perry of the United Kingdom won the Men's Wheelchair Race, coming in at 3:20:07, and Denise Smith, also of the UK, won the Women's Wheelchair Race in 4:29:03.
World records for marathon running have been set four times. Khalid Khannouchi, representing the United States, set the men's world record in 2:05:38 in 2002. The following year, British runner Paula Radcliffe set the women's world record in 2:15:25 (later briefly downgraded to "world best" by the IAAF as it was achieved in a mixed race, but restored to the title of "world Record" shortly thereafter). Radcliffe's time also stands as the current course record in the Women's Elite Race: this followed women's records set in 1983 and 1985 by Grete Waitz and Ingrid Kristiansen respectively, both of Norway. The current men's course record is 2:04:29, set by Kenyan Wilson Kipsang Kiprotich in the 2014 edition. Kurt Fearnley of Australia set the Men's Wheelchair Race course record at 1:28:57 in 2009, and the Women's equivalent was set by American athlete Tatyana McFadden in 2013, with 1:46:02.
Amateur runners.
The race attracts amateur runners who make up the bulk of the thirty thousand or more participants; commonly running in fancy dress for charity causes. In 2002, Lloyd Scott completed the marathon wearing a deep sea diving suit that weighed a total of 110 lb, with each shoe weighing 24 lb; he also set a record for the slowest London Marathon time. On 19 April 2003, former boxer Michael Watson, who had been told he would never be able to walk again after a fight with Chris Eubank, made headlines by finishing the marathon in six days. In 2006, Sir Steve Redgrave (winner of five consecutive Olympic gold medals) set a new Guinness World Record for money raised through a marathon by collecting £1.8 million in sponsorship. This broke the record set the previous year by the founder of the Oasis Trust, Steve Chalke MBE, who had collected over £1.25 million. Steve Chalke recovered the record in 2007, raising £1.86 million. In 2011 Chalke broke the record for a third time, raising £2.32 million. The sum of £500 that Claire Squires collected before the race swelled to £920,000 after she died having collapsed during the 2012 race.
A small number of runners, known as the "Ever Presents", have completed each of the London Marathons since 1981. After 2014 their number has shrunk to 14. At the running of the 2014 event the oldest runner was Kenneth Jones, 80 years old, whilst the youngest runner was 55-year-old Chris Finill. They are all male.
Mini Marathon.
The Virgin Money Giving Mini London Marathon is the sister of The London Marathon. The course is the last three miles of the London Marathon and is aimed at ages 11–17 from all 33 London Boroughs along with 13 teams from ten English regions and three Home Countries: Scotland, Wales and Northern Ireland. There is also a Mini Wheelchair race on the day.
BBC live coverage.
Since 1981, the BBC has broadcast live coverage of the London Marathon. The main presenters on BBC One have been Sue Barker, Jonathan Edwards and Gabby Logan. The highlight presenters on BBC Two have been Jonathan Edwards (2007–12), Sonali Shah (2013), and Helen Skelton (2014–15). The commentators between for the Marathon on the BBC were David Coleman, Ron Pickering, Brendan Foster, Paul Dickinson, Steve Cram, Stuart Storey, Paula Radcliffe, Dame Tanni Grey-Thompson, Liz McColgan and Rob Walker.
Sponsorship and marketing.
On 22 April 2013 the London Marathon renewed its sponsorship deal with Virgin Money for a further five years and the race will change its name to the Virgin Money London Marathon.
On 16 May 2008, London Marathon Limited signed a five-year £17m sponsorship deal with Virgin and Virgin Money. The original sponsors were Gillette, who sponsored the event from 1981 to 1983. The other sponsors have been Mars (1984–1988), ADT (1989–1992), NutraSweet (1993–1995), and Flora (1996–2009). A number of other companies and organisations also use the event for brand identification and marketing, including Adidas, Lucozade Sport, and Fuller's Brewery.

</doc>
<doc id="48808" url="http://en.wikipedia.org/wiki?curid=48808" title="Second Council of Constantinople">
Second Council of Constantinople

The Second Council of Constantinople is the fifth of the first seven ecumenical councils recognized as such by both West and East. Eastern Orthodox, Catholics, and Old Catholics unanimously recognize it. Protestant opinions and recognition of it are varied. Traditional Protestants such as Reformed and Lutheran recognize the first four councils, whereas most High Church Anglicans accept all seven. Constantinople II was convoked by Byzantine Emperor Justinian I under the presidency of Patriarch Eutychius of Constantinople and was held from 5 May to 2 June 553. Participants were overwhelmingly Eastern bishops – only sixteen Western bishops were present, including nine from Illyricum and seven from Africa, but none from Italy – out of the 152 total.
The main work of the council was to confirm the condemnation issued by edict in 551 by the Emperor Justinian against the Three Chapters (cf. Three Chapters controversy and Three Chapters schism). The "Three Chapters" were, one, both the person and writings of Theodore of Mopsuestia (d. 428), two, the attacks on Cyril of Alexandria and the First Council of Ephesus written by Theodoret of Cyrus (d. c. 466), and three, the attacks on Cyril and Ephesus by Ibas of Edessa (d. 457).
The purpose of the condemnation was to make plain that the Imperial, Chalcedonian (that is, recognizing the hypostatic union of Christ as two natures, one divine and one human, united in one person with neither confusion nor division) Church was firmly opposed to all those who had either inspired or assisted Nestorius, the eponymous heresiarch of Nestorianism – the proposition that the Christ and Jesus were two separate persons loosely conjoined, somewhat akin to adoptionism, and that the Virgin Mary could not be called the Mother of God (Gk. "theotokos") but only the mother of Christ (Gk. "Christotokos") – which was condemned at the earlier ecumenical council of Ephesus in 431.
Justinian hoped that this would contribute to a reunion between the Chalcedonians and monophysites in the eastern provinces of the Empire; various attempts at reconciliation between the monophysite and orthodox parties were made by many emperors over the four centuries following the Council of Ephesus, none of them succeeding, and some, attempts at reconciliation, such as this – the condemnation of the Three Chapters – causing further schisms and heresies to arise in the process, such as the aforementioned schism of the Three Chapters, and the heresies of monoenergism and monotheletism – the propositions, respectively, that Christ had only one function, operation, or energy (purposefully formulated in an equivocal and vague manner, and promulgated between 610 and 622 by the Emperor Heraclius under the advice of Patriarch Sergius I of Constantinople) and that Christ only had one will (promulgated in 638 by the same).
Proceedings.
The Council was presided over by Eutychius, Patriarch of Constantinople, assisted by the other three eastern patriarchs or their representatives. Pope Vigilius was also invited; but even though he was at this period resident in Constantinople (to avoid the perils of life in Italy, convulsed by the war against the Ostrogoths), he declined to attend, and even issued a document forbidding the council from preceding without him (his 'First Constitutum'). For more details see Pope Vigilius.
The council, however, proceeded without the pope to condemn the Three Chapters. And during the seventh session of the council, the bishops had Vigilius stricken from the diptychs for his refusal to appear at the council and approve its proceedings, effectively excommunicating him personally but not the rest of the Western Church. Vigilius was then imprisoned in Constantinople by the emperor and his advisors were exiled. After six months, in December 553, he agreed, however, to condemn the Three Chapters, claiming that his hesitation was due to being misled by his advisors. His approval of the council was expressed in two documents, (a letter to Eutychius of Constantinople, 8 Dec., 553, and a second "Constitutum" of 23 February, 554, probably addressed to the Western episcopate), condemning the Three Chapters, on his own authority and without mention of the council.
In Northern Italy the ecclesiastical provinces of Milan and Aquileia broke communion with Rome. Milan accepted the condemnation only toward the end of the sixth century, whereas Aquileia did not do so until about 700 The rest of the Western Church accepted the decrees of the council, though without great enthusiasm. Though ranked as one of the ecumenical councils, it never attained in the West the status of either Nicaea or Chalcedon.
In Visigothic Spain (Reccared having converted a short time prior) the churches never accepted the council; when news of the later Third Council of Constantinople was communicated to them by Rome it was received as the "fifth" ecumenical council, not the sixth. Isidore of Seville, in his "Chronicle" and "De Viris Illustribus", judged Justinian a tyrant and persecutor of the orthodox and an admirer of heresy, contrasting him with Facundus of Hermiane and Victor of Tunnuna, who was considered a martyr.
The unhappy story of the conflict between the council and the pope, and its lack of immediate and obvious fruits in reconciling Chalcedonians and non-Chalcedonians, should not blind us, however, to its weighty theological contribution. The canons condemning the Three Chapters were preceded by ten dogmatic canons which defined Chalcedonian Christology with a new precision, bringing out that God the Word is the one subject of all the operations of Christ, divine and human. The 'two natures' defined at Chalcedon were now clearly interpreted as two sets of attributes possessed by a single person, Christ God, the Second Person of the Trinity, Later Byzantine Christology, as found in Maximus the Confessor and John of Damascus, was built upon this basis. It might have proved sufficient, moreover, to bring about the reunion of Chalcedonians and non-Chalcedonians, had it not been for the severance of connections between the two groups that resulted from the Muslim conquests of the next century.
Acts.
The original Greek acts of the council are lost, but an old Latin version exists, possibly made for Vigilius, of which there is a critical edition and of which there is now an English translation and commentary, it was alleged (probably falsely) that the original Acts of the Fifth Council had been tampered with in favour of Monothelitism. This condemnation was confirmed by Pope Vigilius and the subsequent ecumenical council (third Council of Constantinople) gave its "assent" in its Definition of Faith to the five previous synods, including "... the last, that is the Fifth holy Synod assembled in this place, against Theodore of Mopsuestia, Origen, Didymus, and Evagrius ..."; its full conciliar authority has only been questioned in modern times.
Notes.
 incorporates text from a publication now in the public domain: 

</doc>
<doc id="48814" url="http://en.wikipedia.org/wiki?curid=48814" title="Aerobot">
Aerobot

An aerobot is an aerial robot, usually used in the context of an unmanned space probe or unmanned aerial vehicle.
While work has been done since the 1960s on robot "rovers" to explore the Moon and other worlds in the Solar system, such machines have limitations. They tend to be expensive and have limited range, and due to the communications time lags over interplanetary distances, they have to be smart enough to navigate without disabling themselves. 
For planets with atmospheres of any substance, however, there is an alternative: an autonomous flying robot, or "aerobot". Most aerobot concepts are based on aerostats, primarily balloons, but occasionally airships. Flying above obstructions in the winds, a balloon could explore large regions of a planet in great detail for relatively low cost. Airplanes for planetary exploration have also been proposed.
Basics of balloons.
While the notion of sending a balloon to another planet sounds strange at first, balloons have a number of advantages for planetary exploration. They can be made light in weight and are potentially relatively inexpensive. They can cover a great deal of ground, and their view from a height gives them the ability to examine wide swathes of terrain with far more detail than would be available from an orbiting satellite. For exploratory missions, their relative lack of directional control is not a major obstacle as there is generally no need to direct them to a specific location. 
Balloon designs for possible planetary missions have involved a few unusual concepts. One is the solar, or infrared (IR) Montgolfiere. This is a hot-air balloon where the envelope is made from a material that traps heat from sunlight, or from heat radiated from a planetary surface. Black is the best color for absorbing heat, but other factors are involved and the material may not necessarily be black. 
Solar Montgolfieres have several advantages for planetary exploration, as they can be easier to deploy than a light gas balloon, do not necessarily require a tank of light gas for inflation, and are relatively forgiving of small leaks. They do have the disadvantage that they are only aloft during daylight hours. 
The other is a "reversible fluid" balloon. This type of balloon consists of an envelope connected to a reservoir, with the reservoir containing a fluid that is easily vaporized. The balloon can be made to rise by vaporizing the fluid into gas, and can be made to sink by condensing the gas back into fluid. There are a number of different ways of implementing this scheme, but the physical principle is the same in all cases. 
A balloon designed for planetary exploration will carry a small gondola containing an instrument payload. The gondola will also carry power, control, and communications subsystems. Due to weight and power supply constraints, the communications subsystem will generally be small and low power, and interplanetary communications will be performed through an orbiting planetary probe acting as a relay. 
A solar Montgolfiere will sink at night, and will have a guide rope attached to the bottom of the gondola that will curl up on the ground and anchor the balloon during the darkness hours. The guide rope will be made of low friction materials to keep it from catching or tangling on ground features. 
Alternatively, a balloon may carry a thicker instrumented "snake" in place of the gondola and guiderope, combining the functions of the two. This is a convenient scheme for making direct surface measurements. 
A balloon could also be anchored to stay in one place to make atmospheric observations. Such a static balloon is known as an "aerostat". 
One of the trickier aspects of planetary balloon operations is inserting them into operation. Typically, the balloon enters the planetary atmosphere in an "aeroshell", a heat shield in the shape of a flattened cone. After atmospheric entry, a parachute will extract the balloon assembly from the aeroshell, which falls away. The balloon assembly then deploys and inflates. 
Once operational, the aerobot will be largely on its own and will have to conduct its mission autonomously, accepting only general commands over its long link to Earth. The aerobot will have to navigate in three dimensions, acquire and store science data, perform flight control by varying its altitude, and possibly make landings at specific sites to provide close-up investigation.
The Venus Vega balloons.
The first, and so far only, planetary balloon mission was performed by the Space Research Institute of Soviet Academy of Sciences in cooperation with the French space agency CNES in 1985. A small balloon, similar in appearance to terrestrial weather balloons, was carried on each of the two Soviet Vega Venus probes, launched in 1984.
The first balloon was inserted into the atmosphere of Venus on 11 June 1985, followed by the second balloon on 15 June 1985. The first balloon failed after only 56 minutes, but the second operated for a little under two Earth days until its batteries ran down. 
The Venus Vega balloons were the idea of Jacques Blamont, chief scientist for CNES and the father of planetary balloon exploration. He energetically promoted the concept and enlisted international support for the small project. 
The scientific results of the Venus VEGA probes were modest. More importantly, the clever and simple experiment demonstrated the validity of using balloons for planetary exploration.
The Mars aerobot effort.
After the success of the Venus VEGA balloons, Blamont focused on a more ambitious balloon mission to Mars, to be carried on a Soviet space probe. 
The atmospheric pressure on Mars is about 150 times less than that of Earth. In such a thin atmosphere, a balloon with a volume of 5,000 to 10,000 cubic meters (178,500 to 357,000 cubic feet) could carry a payload of 20 kilograms (44 pounds), while a balloon with a volume of 100,000 cubic meters (3,600,000 cubic feet) could carry 200 kilograms (440 pounds). 
The French had already conducted extensive experiments with solar Montgolfieres, performing over 30 flights from the late 1970s into the early 1990s. The Montgolfieres flew at an altitude of 35 kilometers, where the atmosphere was as thin and cold as it would be on Mars, and one spent 69 days aloft, circling the Earth twice. 
Early concepts for the Mars balloon featured a "dual balloon" system, with a sealed hydrogen or helium-filled balloon tethered to a solar Montgolfiere. The light-gas balloon was designed to keep the Montgolfiere off the ground at night. During the day, the Sun would heat up the Montgolfiere, causing the balloon assembly to rise. 
Eventually, the group decided on a cylindrical sealed helium balloon made of aluminized PET film, and with a volume of 5,500 cubic meters (196,000 cubic feet). The balloon would rise when heated during the day and sink as it cooled at night. 
Total mass of the balloon assembly was 65 kilograms (143 pounds), with a 15 kilogram (33 pound) gondola and a 13.5 kilogram (30 pound) instrumented guiderope. The balloon was expected to operate for ten days. Unfortunately, although considerable development work was performed on the balloon and its subsystems, Russian financial difficulties pushed the Mars probe out from 1992, then to 1994, and then to 1996. The Mars balloon was dropped from the project due to cost.
JPL aerobot experiments.
By this time, the Jet Propulsion Laboratory (JPL) of the US National Aeronautics and Space Administration (NASA) had become interested in the idea of planetary aerobots, and in fact a team under Jim Cutts of JPL had been working on concepts for planetary aerobots for several years, as well as performing experiments to validate aerobot technology. 
The first such experiments focused on a series of reversible-fluid balloons, under the project name ALICE, for "Altitude Control Experiment". The first such balloon, ALICE 1, flew in 1993, with other flights through ALICE 8 in 1997. 
Related work included the characterization of materials for a Venus balloon envelope, and two balloon flights in 1996 to test instrument payloads under the name BARBE, for "Balloon Assisted Radiation Budget Equipment". 
By 1996, JPL was working on a full-fledged aerobot experiment named PAT, for "Planetary Aerobot Testbed", which was intended to demonstrate a complete planetary aerobot through flights into Earth's atmosphere. PAT concepts envisioned a reversible-fluid balloon with a 10-kilogram payload that would include navigation and camera systems, and eventually would operate under autonomous control. The project turned out to be too ambitious, and was cancelled in 1997. JPL continued to work on a more focused, low-cost experiments to lead to a Mars aerobot, under the name MABVAP, for "Mars Aerobot Validation Program". MABVAP experiments included drops of balloon systems from hot-air balloons and helicopters to validate the tricky deployment phase of a planetary aerobot mission, and development of envelopes for superpressure balloons with materials and structures suited to a long-duration Mars mission. 
JPL also provided a set of atmospheric and navigation sensors for the Solo Spirit round-the-world manned balloon flights, both to support the balloon missions and to validate technologies for planetary aerobots. 
While these tests and experiments were going on, JPL performed a number of speculative studies for planetary aerobot missions to Mars, Venus, Saturn's moon Titan, and the outer planets.
Mars.
JPL's MABVAP technology experiments were intended to lead to an actual Mars aerobot mission, named MABTEX, for "Mars Aerobot Technology Experiment". As its name implies, MABTEX was primarily intended to be an operational technology experiment as a precursor to a more ambitious efforts. MABTEX was envisioned as a small superpressure balloon, carried to Mars on a "microprobe" weighing no more than 40 kg. Once inserted, the operational balloon would have a total mass of no more than 10 kg and would remain operational for a week. The small gondola would have navigational and control electronics, along with a stereo imaging system, as well as a spectrometer and magnetometer. 
Plans envisioned a follow-on to MABTEX as a much more sophisticated aerobot named MGA, for "Mars Geoscience Aerobot". Design concepts for MGA envisioned a superpressure balloon system very much like that of MABTEX, but much larger. MGA would carry a payload ten times larger than that of MABTEX, and would remain aloft for up to three months, circling Mars more than 25 times and covering over 500000 km. The payload would include sophisticated equipment, such as an ultrahigh resolution stereo imager, along with oblique imaging capabilities; a radar sounder to search for subsurface water; an infrared spectroscopy system to search for important minerals; a magnetometer; and weather and atmospheric instruments. MABTEX might be followed in turn by a small solar-powered blimp named MASEPA, for "Mars Solar Electric Propelled Aerobot". 
Venus.
JPL has also pursued similar studies on Venus aerobots. A Venus Aerobot Technology Experiment (VEBTEX) has been considered as a technology validation experiment, but the focus appears to have been more on full operational missions. One mission concept, the Venus Aerobot Multisonde (VAMS), envisions an aerobot operating at altitudes above 50 km that would drop surface probes, or "sondes", onto specific surface targets. The balloon would then relay information from the sondes directly to Earth, and would also collect planetary magnetic field data and other information. VAMS would require no fundamentally new technology, and may be appropriate for a NASA low-cost Discovery planetary science mission.
Significant work has been performed on a more ambitious concept, the Venus Geoscience Aerobot (VGA). Designs for the VGA envision a relatively large reversible-fluid balloon, filled with helium and water, that could descend to the surface of Venus to sample surface sites, and then rise again to high altitudes and cool off. 
Developing an aerobot that can withstand the high pressures and temperatures (up to 480 degrees Celsius, or almost 900 degrees Fahrenheit) on the surface of Venus, as well as passage through sulfuric acid clouds, will require new technologies. As of 2002, VGA was not expected to be ready until late in the following decade. Prototype balloon envelopes have been fabricated from polybenzoxazole, a polymer that exhibits high strength, resistance to heat, and low leakage for light gases. A gold coating is applied to allow the polymer film to resist corrosion from acid clouds. 
Work has also been done on a VGA gondola weighing about 30 kg. In this design, most instruments are contained in a spherical pressure vessel with an outer shell of titanium and an inner shell of stainless steel. The vessel contains a solid-state camera and other instruments, as well as communications and flight control systems. The vessel is designed to tolerate pressures of up to a hundred atmospheres and maintain internal temperatures below 30 C even on the surface of Venus. The vessel is set at the bottom of a hexagonal "basket" of solar panels that in turn provide tether connections to the balloon system above, and is surrounded by a ring of pipes acting as a heat exchanger. An S-band communications antenna is mounted on the rim of the basket, and a radar antenna for surface studies extends out of the vessel on a mast.
Titan.
Titan, the largest moon of Saturn, is an attractive target for aerobot exploration, as it has a nitrogen atmosphere five times as dense as that of Earth's that contains a smog of organic photochemicals, hiding the moon's surface from view by visual sensors. An aerobot would be able to penetrate this haze to study the moon's mysterious surface and search for complex organic molecules. NASA has outlined a number of different aerobot mission concepts for Titan, under the general name of Titan Biologic Explorer. 
One concept, known as the Titan Aerobot Multisite mission, involves a reversible-fluid balloon filled with argon that could descend from high altitude to the surface of the moon, perform measurements, and then rise again to high altitude to perform measurements and move to a different site. Another concept, the Titan Aerobot Singlesite mission, would use a superpressure balloon that would select a single site, vent much of its gas, and then survey that site in detail. 
An ingenious variation on this scheme, the Titan Aerover, combines aerobot and rover. This vehicle features a triangular frame that connects three balloons, each about two meters (6.6 ft) in diameter. After entry into Titan's atmosphere, the aerover would float until it found an interesting site, then vent helium to descend to the surface. The three balloons would then serve as floats or wheels as necessary. JPL has built a simple prototype that looks three beachballs on a tubular frame. 
No matter what form the Titan Biologic Explorer mission takes, the system would likely require an atomic-powered radioisotope thermoelectric generator module for power. Solar power would not be possible at Saturn's distance and under Titan's smog, and batteries would not give adequate mission endurance. The aerobot would also carry a miniaturized chemical lab to search for complicated organic chemicals.
Outside of JPL, other mission studies of Titan aerobot concepts have included studies of airships by MIT and NASA Glenn,
Jupiter.
Finally, aerobots might be used to explore the atmosphere of Jupiter and possibly the other gaseous outer planets. As the atmospheres of these planets are largely composed of hydrogen, and since there is no lighter gas than hydrogen, such an aerobot would have to be a Montgolfiere. As sunlight is weak at such distances, the aerobot would obtain most of its heating from infrared energy radiated by the planet below. 
A Jupiter aerobot might operate at altitudes where the air pressure ranges from one to ten atmospheres, occasionally dropping lower for detailed studies. It would make atmospheric measurements and return imagery and remote sensing of weather phenomena, such as Jupiter's Great Red Spot. A Jupiter aerobot might also drop sondes deep into the atmosphere and relay their data back to an orbiter until the sondes are destroyed by temperature and pressure.
Planetary Aircraft.
Winged airplane concepts have been proposed for robotic exploration in the atmosphere of Mars, Venus, Titan, and even Jupiter.
The main technical challenges of flying on Mars include:
An aircraft concept, ARES was selected for a detailed design study as one of the four finalists for the 2007 Mars Scout Program opportunity, but was eventually not selected in favor of the Phoenix mission. In the design study, both half-scale and full-scale aircraft were tested under Mars-atmospheric conditions. "(See also Mars airplane.)"

</doc>
<doc id="48815" url="http://en.wikipedia.org/wiki?curid=48815" title="Fire balloon">
Fire balloon

A fire balloon (風船爆弾, fūsen bakudan, lit. "balloon bomb"), or Fu-Go (ふ号[兵器], fugō [heiki], lit. "Code Fu [Weapon]"), was a weapon launched by Japan during World War II. A hydrogen balloon with a load varying from a 15 kg antipersonnel bomb to one 12 kg incendiary bomb and four 5 kg incendiary devices attached, it was designed as a cheap weapon intended to make use of the jet stream over the Pacific Ocean and drop bombs on American and Canadian cities, forests, and farmland.
The balloons were relatively ineffective as weapons but were used in one of the few attacks on North America during World War II.
Overview.
From late 1944 until early 1945, the Japanese launched over 9,300 fire balloons, of which 300 were found or observed in the U.S. Despite the high hopes of their designers, the balloons were ineffective as weapons: causing only six deaths (from one single incident) and a small amount of damage.
The Japanese designed two types of balloons. The first was called the "Type B Balloon" and was designed by the Japanese Navy. It was 9 m in diameter and consisted of rubberized silk. The type B balloons were sent first and mainly used for meteorological purposes. The Japanese used them to determine the possibility of the bomb-carrying balloons reaching North America. The second type was the bomb-carrying balloon. Japanese bomb-carrying balloons were 10 m in diameter and, when fully inflated, held about 540 m3 of hydrogen. Their launch sites were located on the east coast of the main Japanese island of Honshū.
Japan released the first of these bomb-bearing balloons on November 3, 1944. They were found in Alaska, Washington, Oregon, California, Arizona, Idaho, Montana, Utah, Wyoming, Colorado, Texas, Kansas, Nebraska, South Dakota, North Dakota, Michigan and Iowa, as well as Mexico and Canada.
General Kusaba's men launched over 9,000 balloons throughout the course of the project. The Japanese expected 10% (around 900) of them to reach America, which is also what is currently believed by researchers. About 300 balloon bombs were found or observed in America. It is likely that more balloon bombs landed in unpopulated areas of North America.
The last one was launched in April 1945.
Origins.
The balloon campaign was the fourth attack the Japanese had made on the American mainland. The "fūsen bakudan" campaign was, however, the most earnest of the attacks. The concept was the brainchild of the Imperial Japanese Army's Ninth Army's Number Nine Research Laboratory, under Major General Sueyoshi Kusaba, with work performed by Technical Major Teiji Takada and his colleagues. The balloons were intended to make use of a strong current of winter air that the Japanese had discovered flowing at high altitude and speed over their country, which later became known as the jet stream.
The jet stream reported by Wasaburo Oishi blew at altitudes above 9.15 km (30,000 ft) and could carry a large balloon across the Pacific in three days, over a distance of more than 8000 km. Such balloons could carry incendiary and high-explosive bombs to the United States and drop them there to kill people, destroy buildings, and start forest fires.
The preparations were lengthy because the technological problems were acute. A hydrogen balloon expands when warmed by the sunlight, and rises; then it contracts when cooled at night, and falls. The engineers devised a control system driven by an altimeter to discard ballast. When the balloon descended below 9 km, it electrically fired a charge to cut loose sandbags. The sandbags were carried on a cast-aluminium four-spoked wheel and discarded two at a time to keep the wheel balanced.
Similarly, when the balloon rose above about 11.6 km (38,000 ft), the altimeter activated a valve to vent hydrogen. The hydrogen was also vented if the balloon's pressure reached a critical level.
The control system ran the balloon through three days of flight. At that time, it was likely over the U.S., and its ballast was expended. The final flash of gunpowder released the bombs, also carried on the wheel, and lit a 19.5 m long fuse that hung from the balloon's equator. After 84 minutes, the fuse fired a flash bomb that destroyed the balloon.
The balloon had to carry about 454 kg of gear. At first the balloons were made of conventional rubberized silk, but improved envelopes had less leakage. An order went out for ten thousand balloons made of "washi", a paper derived from mulberry bushes that was impermeable and very tough. It was only available in squares about the size of a road map, so it was glued together in three or four laminations using edible "konnyaku" (devil's tongue) paste. Hungry workers stole the paste and ate it. Many workers were nimble-fingered teenaged school girls. They assembled the paper in many parts of Japan. Large indoor spaces, such as sumo halls, sound stages, and theatres, were required for the envelope assembly.
The bombs most commonly carried by the balloons were:
The Japanese Imperial Army NOBORITO institute cultivated anthrax and Pasteurella pestis, furthermore, it produced 20 tons of cowpox viruses which is quantity to be equivalent to the whole area of the United States.
The plan of deployment of these biological weapon on Fire balloons was planned in 1944.
The Emperor Hirohito did not admit deployment of biological weapon on the occasion of a report of President Staff Officer Umezu on October 25, 1944. Consequently, the biological warfare was not realized.
Similar, but cruder, balloons were also used by Britain to attack Germany between 1942 and 1944.
Offensive.
A balloon launch organization of three battalions was formed. The first battalion included headquarters and three squadrons totaling 1500 men in Ibaraki Prefecture with nine launch stations at Ōtsu. The second battalion of 700 men in three squadrons operated six launch stations at Ichinomiya, Chiba; and the third battalion of 600 men in two squadrons operated six launch stations at Nakoso in Fukushima Prefecture. The Ōtsu site included hydrogen gas generating facilities; but the 2nd and 3rd battalion launch sites used hydrogen manufactured elsewhere. The best time to launch was just after the passing of a high-pressure front, and wind conditions were most suitable for several hours prior to the onshore breezes at sunrise. Suitable launch conditions were expected on only about fifty days through the winter period of maximum jet stream velocity, and the combined launch capacity of all three battalions was about 200 balloons per day.
Initial tests took place in September 1944 and proved satisfactory; however, before preparations were complete, B-29s began their raids on the Japanese home islands. The attacks were somewhat ineffectual at first but still fueled the desire for revenge sparked by the Doolittle Raid.
The first balloon was released on 3 November 1944. Major Takada watched as the balloon flew upward and over the sea: "The figure of the balloon was visible only for several minutes following its release until it faded away as a spot in the blue sky like a daytime star." A few balloons carried radiosonde equipment rather than bombs. These balloons were tracked by direction finding stations in Ichinomiya, Chiba, in Iwanuma, Miyagi, in Misawa, Aomori, and on Sakhalin to estimate progress toward the United States.
The Japanese chose to launch the campaign in November; because the period of maximum jet stream velocity is November through March. This limited the chance of the incendiary bombs causing forest fires, as that time of year produces the maximum North American Pacific coastal precipitation, and forests were generally snow-covered or too damp to catch fire easily. On 4 November 1944 a United States Navy patrol craft discovered one of the first radiosonde balloons floating off San Pedro, Los Angeles. National and state agencies were placed on heightened alert status when balloons were found in Wyoming and Montana before the end of November.
The balloons continued to arrive in Oregon, Kansas, Iowa, British Columbia, Saskatchewan, Manitoba, Alberta, the Yukon, Northwest Territories, Washington, Idaho, South Dakota, Nevada (including one that landed near Yerington that was discovered by cowboys who cut it up and used it as a hay tarp, another by a prospector near Elko who delivered it to local authorities on the back of a donkey, and another was shot down by U.S. Army Air Forces planes near Reno). In all, seven fire balloons were turned in to the Army in Nevada, Colorado, Texas, Northern Mexico, Michigan, and even the outskirts of Detroit. Fighters scrambled to intercept the balloons, but they had little success; the balloons flew very high and surprisingly fast, and fighters destroyed fewer than 20.
American authorities concluded the greatest danger from these balloons would be wildfires in the Pacific coastal forests. The Fourth Air Force, Western Defense Command and Ninth Service Command organized the "Firefly Project" of 2,700 troops including 200 paratroopers of the 555th Parachute Infantry Battalion with Stinson L-5 Sentinel and Douglas C-47 Skytrain aircraft. These men were stationed at critical points for use in fire-fighting missions. The 555th suffered one fatality and 22 injuries fighting fires.
By early 1945, Americans were becoming aware that something strange was going on. Balloons had been sighted and explosions heard, from California to Alaska. Something that appeared to witnesses to be like a parachute descended over Thermopolis, Wyoming. A fragmentation bomb exploded, and shrapnel was found around the crater. A P-38 Lightning shot a balloon down near Santa Rosa, California; another was seen over Santa Monica; and bits of washi were found in the streets of Los Angeles.
In February and March 1945, P-40 fighter pilots from 133 Squadron, Royal Canadian Air Force operating out of RCAF Patricia Bay (Victoria, British Columbia), intercepted and destroyed two fire balloons, On 21 February, Pilot Officer E. E. Maxwell While shot down a balloon, which landed on Sumas Mountain, in Washington State. On 10 March, Pilot Officer J. O. Patten destroyed a balloon near Saltspring Island, British Columbia.
On March 10, 1945, one of the last paper balloons descended in the vicinity of the Manhattan Project's production facility at the Hanford Site. This balloon caused a short circuit in the power lines supplying electricity for the nuclear reactor cooling pumps, but backup safety devices restored power almost immediately.
Two paper balloons were recovered in a single day in Modoc National Forest, east of Mount Shasta. Near Medford, Oregon, a balloon bomb exploded in towering flames. The Navy found balloons in the ocean. Balloon envelopes and apparatus were found in Montana, Arizona, Saskatchewan, in the Northwest Territories, and in the Yukon Territory. Eventually, an Army fighter managed to push one of the balloons around in the air and force it to ground intact, where it was examined and filmed. Japanese propaganda broadcasts announced great fires and an American public in panic, declaring casualties in the thousands.
Hayfork is a small community in northern California about 40 mi west of Redding. On February 1, 1945 a Japanese bombing balloon was spotted by several local residents drifting over the Trinity National Forest area and slowly descending. No one knew what it was, but an alert forest ranger called the military authorities at the Presidio of San Francisco and reported it. Meanwhile the balloon came to rest atop a 60 ft dead fir tree in the forest near a local road. In the next few hours several people gathered in the area to gaze up at the strange object.
Shortly after dark there was a tremendous blast. The balloon's gas bag disappeared in a fireball and the balloon's undercarriage came crashing to the ground. No one was hurt. Forest rangers kept the curious well back from the fallen debris until Army personnel arrived. Upon examination, it was found to be a Japanese bombing balloon with four incendiary bombs and one high explosive bomb still aboard and the bomb releasing mechanism still very much intact. It later proved to be one of the most intact bombing balloons yet to fall into
American hands. As was usual in instances of this sort, the local people were told what it was and were asked to keep secret what they had seen.
Allied investigation.
Despite their low success, the authorities were worried about the balloons. There was the chance that they might get lucky. Much worse, the Americans had some knowledge that the Japanese had been working on biological weapons, most specifically at the infamous Unit 731 site at Pingfan in Manchuria, and a balloon carrying biowarfare agents could be a real threat.
Nobody believed the balloons could have come directly from Japan. It was thought that the balloons must be coming from North American beaches, launched by landing parties from submarines. Wilder theories speculated that they could have been launched from German prisoner of war camps in the U.S., or even from Japanese-American internment centers.
Some of the sandbags dropped by the "fusen bakudan" were taken to the Military Geology Unit of the United States Geological Survey for investigation. Working with Colonel Sidman Poole of U.S. Army Intelligence, the researchers of the Military Geological Unit began microscopic and chemical examination of the sand from the sandbags to determine types and distribution of diatoms and other microscopic sea creatures, and its mineral composition. The sand could not be coming from American beaches, nor from the mid-Pacific. It had to be coming from Japan. The geologists ultimately determined the precise beaches in Japan the sand had been taken from. By this time, it was mostly irrelevant, since by early spring the balloon offensive was almost over.
Despite the military origins of the balloons, the FBI was the primary government agency charged with responding to reports of balloons.
Press coverup.
The bombs caused little damage, but their potential for destruction and fires was large. The bombs also had a potential psychological effect on the American people. The U.S. strategy was to keep the Japanese from knowing of the balloon bombs' effectiveness.
In 1945 "Newsweek" ran an article titled "Balloon Mystery" in their January 1 issue, and a similar story appeared in a newspaper the next day.
The Office of Censorship then sent a message to newspapers and radio stations to ask them to make no mention of balloons and balloon-bomb incidents. They did not want the enemy to get the idea that the balloons might be effective weapons or to have the American people start panicking. Cooperating with the desires of the government, the press did not publish any balloon bomb incidents. Perhaps as a result, the Japanese only learned of one bomb's reaching Wyoming, landing and failing to explode, so they stopped the launches after less than six months.
The press blackout in the U.S. was lifted after the first deaths to ensure that the public was warned, though public knowledge of the threat could have possibly prevented it.
Japanese abandon the project.
With no evidence of any effect, General Kusaba was ordered to cease operations in April 1945, believing that the mission had been a total fiasco. The expense was large, and in the meantime the B-29s had destroyed two of the three hydrogen plants needed by the project.
The last fire balloon was launched in April 1945.
Single lethal attack.
On May 5, 1945, a pregnant woman and five children were killed when they discovered a balloon bomb that had landed in the forest of Gearhart Mountain in Southern Oregon. Archie Mitchell was the pastor of the Bly Christian and Missionary Alliance Church. He and his pregnant wife Elsie drove up to Gearhart Mountain with five of their Sunday school students (aged 11–14) to have a picnic. They had to stop at this spot near Bly, Oregon, due to construction and a road closing. Elsie and the children got out of the car at Bly, while Archie drove on to find a parking spot. As Elsie and the children looked for a good picnic spot, they saw a strange balloon lying on the ground. As the group approached the balloon, a bomb attached to it exploded, killing Elsie and all five children instantly. Archie and several members of the road crew witnessed the explosion and immediately ran to the scene and used their hands to extinguish the fire on Elsie's and the children's clothing, but they could not save them. Two members of the road crew gathered their bearings and headed to Bly. They knew authorities and military personnel were needed. These are the only known deaths caused by the balloon bombs. Aside from the Aleutian Islands Campaign in the Alaska Territory, they are the only known deaths in the contiguous U.S. as the result of enemy action during World War II.
Military personnel arrived on the scene within hours, and saw that the balloon still had snow underneath it, while the surrounding area did not. They concluded that the balloon bomb had drifted to the ground several weeks earlier, and had lain there undisturbed until found by the group.
Elsie Mitchell is buried in the Ocean View Cemetery in Port Angeles, Washington. A memorial, the Mitchell Monument, is located at the point of the explosion, 110 km northeast of Klamath Falls in the Mitchell Recreation Area. It was listed in the National Register of Historic Places in 2001. Several Japanese civilians have visited the monument to offer their apologies for the deaths that took place here, and several cherry trees have been planted around the monument as a symbol of peace.
Post World War II.
The remains of balloons continued to be discovered after the war. Eight were found in the 1940s, three in the 1950s and two in the 1960s. In 1978, a ballast ring, fuses and barometers were found near Agness, Oregon and are now part of the collection of the Coos Historical & Maritime Museum.
The remains of balloon bomb was found in Lumby, British Columbia in October 2014 and detonated by a Royal Canadian Navy ordnance disposal team.
The Canadian War Museum, in Ottawa, Ontario, has a full, intact balloon on display.

</doc>
<doc id="48816" url="http://en.wikipedia.org/wiki?curid=48816" title="Frodo (disambiguation)">
Frodo (disambiguation)

Frodo may mean:

</doc>
<doc id="48819" url="http://en.wikipedia.org/wiki?curid=48819" title="Clash of Civilizations">
Clash of Civilizations

The Clash of Civilizations is a theory that people's cultural and religious identities will be the primary source of conflict in the post-Cold War world. It was proposed by political scientist Samuel P. Huntington in a 1992 lecture at the American Enterprise Institute, which was then developed in a 1993 "Foreign Affairs" article titled "The Clash of Civilizations?", in response to his former student Francis Fukuyama's 1992 book, "The End of History and the Last Man". Huntington later expanded his thesis in a 1996 book "The Clash of Civilizations and the Remaking of World Order".
The phrase itself was earlier used by Bernard Lewis in an article in the September 1990 issue of "The Atlantic Monthly" titled "The Roots of Muslim Rage". Even earlier, the phrase appears in a 1926 book regarding the Middle East by Basil Mathews: "Young Islam on Trek: A Study in the Clash of Civilizations" (p. 196).
This expression derives from clash of cultures, already used during the colonial period and the Belle Époque.
Overview.
Huntington began his thinking by surveying the diverse theories about the nature of global politics in the post-Cold War period. Some theorists and writers argued that human rights, liberal democracy, and capitalist free market economy had become the only remaining ideological alternative for nations in the post-Cold War world. Specifically, Francis Fukuyama argued that the world had reached the 'end of history' in a Hegelian sense.
Huntington believed that while the age of ideology had ended, the world had only reverted to a normal state of affairs characterized by cultural conflict. In his thesis, he argued that the primary axis of conflict in the future will be along cultural and religious lines.
As an extension, he posits that the concept of different civilizations, as the highest rank of cultural identity, will become increasingly useful in analyzing the potential for conflict.
In the 1993 "Foreign Affairs" article, Huntington writes:
It is my hypothesis that the fundamental source of conflict in this new world will not be primarily ideological or primarily economic. The great divisions among humankind and the dominating source of conflict will be cultural. Nation states will remain the most powerful actors in world affairs, but the principal conflicts of global politics will occur between nations and groups of different civilizations. The clash of civilizations will dominate global politics. The fault lines between civilizations will be the battle lines of the future.
In the end of the article, he writes:
This is not to advocate the desirability of conflicts between civilizations. It is to set forth descriptive hypothesis as to what the future may be like.
In addition, the clash of civilizations, for Huntington, represents a development of history. In the old time, the history of international system was mainly about the struggles between monarchs, nations and ideologies. Those conflicts were primarily seen within Western civilization. But after the end of the cold war, world politics had been moved into a new aspect in which non- Western civilizations were no more the exploited recipients of Western civilization but become another important actor joining the West to shape and move the world history.
Major civilizations according to Huntington.
Huntington divided the world into the "major civilizations" in his thesis as such:
Huntington's thesis of civilizational clash.
Russia and India are what Huntington terms 'swing civilizations' and may favor either side. Russia, for example, clashes with the many Muslim ethnic groups on its southern border (such as Chechnya) but—according to Huntington—cooperates with Iran to avoid further Muslim-Orthodox violence in Southern Russia, and to help continue the flow of oil. Huntington argues that a "Sino-Islamic connection" is emerging in which China will cooperate more closely with Iran, Pakistan, and other states to augment its international position.
Huntington also argues that civilizational conflicts are "particularly prevalent between Muslims and non-Muslims", identifying the "bloody borders" between Islamic and non-Islamic civilizations. This conflict dates back as far as the initial thrust of Islam into Europe, its eventual expulsion in the Iberian reconquest and the attacks of the Ottoman Turks on Eastern Europe and Vienna.
Huntington also believes that some of the factors contributing to this conflict are that both Christianity (which has influenced Western civilization) and Islam are:
More recent factors contributing to a Western-Islamic clash, Huntington wrote, are the Islamic Resurgence and demographic explosion in Islam, coupled with the values of Western universalism—that is, the view that all civilizations should adopt Western values—that infuriate Islamic fundamentalists. All these historical and modern factors combined, Huntington wrote briefly in his "Foreign Affairs" article and in much more detail in his 1996 book, would lead to a bloody clash between the Islamic and Western civilizations. The political party Hizb ut-Tahrir also reiterate Huntington's views in their published book, "The Inevitability of Clash of Civilisation".
Why Civilizations will Clash.
Huntington offers six explanations for why civilizations will clash:
The West versus the Rest.
Huntington suggests that in the future the central axis of world politics tends to be the conflict between Western and non-Western civilizations, in Kishore Mahbubani's phrase, the conflict between "the West and the Rest." He offers three forms of general actions that non-Western civilization can take in response to Western countries.
Core state and fault line conflicts.
In Huntington's view, intercivilizational conflict manifests itself in two forms: fault line conflicts and core state conflicts.
"Fault line conflicts" are on a local level and occur between adjacent states belonging to different civilizations or within states that are home to populations from different civilizations.
"Core state conflicts" are on a global level between the major states of different civilizations. Core state conflicts can arise out of fault line conflicts when core states become involved.
These conflicts may result from a number of causes, such as: relative influence or power (military or economic), discrimination against people from a different civilization, intervention to protect kinsmen in a different civilization, or different values and culture, particularly when one civilization attempts to impose its values on people of a different civilization.
Modernization, westernization, and "torn countries".
Critics of Huntington's ideas often extend their criticisms to traditional cultures and internal reformers who wish to modernize without adopting the values and attitudes of Western culture. These critics sometimes claim that to modernize is necessary to become Westernized to a very large extent.
In reply, those who consider the "Clash of Civilizations" thesis accurate often point to the example of Japan, claiming that it is not a Western state at its core. They argue that it adopted much Western technology (also inventing technology of its own in recent times), parliamentary democracy, and free enterprise, but has remained culturally very distinct from the West, particularly in its conceptions of society as strictly hierarchical. Contradictory evidence on a more granular scale in turn comes from empirical evidence that greater exposure to factories, schools and urban living is associated with more 'modern' attitudes to rationality, individual choice and responsibility.
China is also cited by some as a rising non-Western economy. Many also point out the East Asian Tigers or neighboring states as having adapted western economics, while maintaining traditional or authoritarian social government.
Perhaps the ultimate example of non-Western modernization is Russia, the core state of the Orthodox civilization. The variant of this argument that uses Russia as an example relies on the acceptance of a unique non-Western civilization headed by an Orthodox state such as Russia or perhaps an Eastern European country.
Huntington argues that Russia is primarily a non-Western state although he seems to agree that it shares a considerable amount of cultural ancestry with the modern West. Russia was one of the great powers during World War I. It also happened to be a non-Western power.
According to Huntington, the West is distinguished from Orthodox Christian countries by the experience of the Renaissance, Reformation, the Enlightenment, overseas colonialism rather than contiguous expansion and colonialism, and a recent re-infusion of Classical culture through ancient Greece rather than through the continuous trajectory of the Byzantine Empire.
The differences among the modern Slavic states can still be seen today. This issue is also linked to the "universalizing factor" exhibited in some civilizations.
Huntington refers to countries that are seeking to affiliate with another civilization as "torn countries." Turkey, whose political leadership has systematically tried to Westernize the country since the 1920s, is his chief example. Turkey's history, culture, and traditions are derived from Islamic civilization, but Turkey's elite, beginning with Mustafa Kemal Atatürk who took power as first President of the Republic of Turkey in 1923, imposed western institutions and dress, embraced the Latin alphabet, joined NATO, and is seeking to join the European Union.
Mexico and Russia are also considered to be torn by Huntington. He also gives the example of Australia as a country torn between its Western civilizational heritage and its growing economic engagement with Asia.
According to Huntington, a torn country must meet three requirements to redefine its civilizational identity. Its political and economic elite must support the move. Second, the public must be willing to accept the redefinition. Third, the elites of the civilization that the torn country is trying to join must accept the country.
The book claims that to date no torn country has successfully redefined its civilizational identity, this mostly due to the elites of the 'host' civilization refusing to accept the torn country, though if Turkey gained membership in the European Union, it has been noted that many of its people would support Westernization, as in the following quote by EU Minister Egemen Bağış: "This is what Europe needs to do: they need to say that when Turkey fulfills all requirements, Turkey will become a member of the EU on date X. Then, we will regain the Tukish public opinion support in one day.". If this were to happen, it would, according to Huntington, be the first to redefine its civilizational identity.
Criticism.
Huntington has fallen under the stern critique of various academic writers, who have either empirically, historically, logically, or ideologically challenged his claims (Fox, 2005; Mungiu Pippidi & Mindruta, 2002; Henderson & Tucker, 2001; Russett, Oneal, & Cox, 2000; Harvey, 2000).
In an article explicitly referring to Huntington, scholar Amartya Sen (1999) argues that "diversity is a feature of most cultures in the world. Western civilization is no exception. The practice of democracy that has won out in the modern West is largely a result of a consensus that has emerged since the Enlightenment and the Industrial Revolution, and particularly in the last century or so. To read in this a historical commitment of the West—over the millennia—to democracy, and then to contrast it with non-Western traditions (treating each as monolithic) would be a great mistake" (p. 16).
In his 2003 book "Terror and Liberalism", Paul Berman argues that distinct cultural boundaries do not exist in the present day. He argues there is no "Islamic civilization" nor a "Western civilization", and that the evidence for a civilization clash is not convincing, especially when considering relationships such as that between the United States and Saudi Arabia. In addition, he cites the fact that many Islamic extremists spent a significant amount of time living and/or studying in the Western world. According to Berman, conflict arises because of philosophical beliefs various groups share (or do not share), regardless of cultural or religious identity.
Edward Said issued a response to Huntington's thesis in his 2001 article, "The Clash of Ignorance". Said argues that Huntington's categorization of the world's fixed "civilizations" omits the dynamic interdependency and interaction of culture. A longtime critic of the Huntingtonian paradigm, and an outspoken proponent of Arab issues, Edward Said (2004) also argues that the clash of civilizations thesis is an example of "the purest invidious racism, a sort of parody of Hitlerian science directed today against Arabs and Muslims" (p. 293).
Noam Chomsky has criticized the concept of the clash of civilizations as just being a new justification for the United States "for any atrocities that they wanted to carry out", which was required after the Cold War as the Soviet Union was no longer a viable threat.
Opposing concepts.
In recent years, the theory of Dialogue Among Civilizations, a response to Huntington's Clash of Civilizations, has become the center of some international attention. The concept was originally coined by Austrian philosopher Hans Köchler in an essay on cultural identity (1972). In a letter to UNESCO, Köchler had earlier proposed that the cultural organization of the United Nations should take up the issue of a "dialogue between different civilizations" ("dialogue entre les différentes civilisations"). In 2001, Iranian president Mohammad Khatami introduced the concept at the global level. At his initiative, the "dialogue among civilizations" was the basis for United Nations' resolution to name the year 2001 as the Year of Dialogue among Civilizations. The year 2001 was proclaimed as the "United Nations Year of Dialogue among Civilizations".
The Alliance of Civilizations (AOC) initiative was proposed at the 59th General Assembly of the United Nations in 2005 by the President of the Spanish Government, José Luis Rodríguez Zapatero and co-sponsored by the Turkish Prime Minister Recep Tayyip Erdoğan. The initiative is intended to galvanize collective action across diverse societies to combat extremism, to overcome cultural and social barriers between mainly the Western and predominantly Muslim worlds, and to reduce the tensions and polarization between societies which differ in religious and cultural values.
Intermediate Region.
Huntington's geopolitical model, especially the structures for North Africa and Eurasia, is largely derived from the "Intermediate Region" geopolitical model first formulated by Dimitri Kitsikis and published in 1978. The Intermediate Region, which spans the Adriatic Sea and the Indus River, is neither western nor eastern (at least, with respect to the Far East) but is considered distinct.
Concerning this region, Huntington departs from Kitsikis contending that a civilizational fault line exists between the two dominant yet differing religions (Orthodox Christianity and Sunni Islam), hence a dynamic of external conflict. However, Kitsikis establishes an integrated civilization comprising these two peoples along with those belonging to the less dominant religions of Shiite Islam, Alevism, and Judaism. They have a set of mutual cultural, social, economic and political views and norms which radically differ from those in the West and the Far East.
In the Intermediate Region, therefore, one cannot speak of a civilizational clash or external conflict, but rather an internal conflict, not for cultural domination, but for political succession. This has been successfully demonstrated by documenting the rise of Christianity from the hellenized Roman Empire, the rise of the Islamic caliphates from the Christianized Roman Empire and the rise of Ottoman rule from the Islamic caliphates and the Christianized Roman Empire.

</doc>
<doc id="48823" url="http://en.wikipedia.org/wiki?curid=48823" title="Malabo">
Malabo

Malabo is the capital of Equatorial Guinea, located on the northern coast of Bioko Island (formerly Fernando Pó) on the rim of a sunken volcano. With a population of 155,963 (2005) it is also the second largest city in the country, after Bata in Río Muni on the African mainland. Oyala is a planned city currently under construction, designed to replace Malabo as the capital.
History.
The city was first founded by the British in 1827, who leased the island from Spain during the colonial period. Named Port Clarence, it was used as a naval station in the effort to suppress the slave trade. Many newly freed slaves were also settled there, prior to the establishment of Liberia as a colony for freed slaves. While many of them later relocated to Sierra Leone, some of their descendants, called Fernandinos, can still be found in Malabo and the surrounding area, where they constitute a distinct ethnic group, speaking their own Afro-Portuguese pidgin dialect.
When the island reverted to complete Spanish control, Malabo was renamed Santa Isabel. It was chosen to replace the mainland city of Bata as the capital of the country in 1969, and was renamed Malabo in 1973 as part of President Francisco Macías Nguema's campaign to replace European place names with "authentic" African ones.
Climate.
Malabo features a tropical monsoon climate. Malabo sees on average 1,800 mm of rain per year. The city has a pronounced, albeit short dry season from December through February. February is normally its driest with 33 mm (0.2 in) of rain falling on average. It also has a very lengthy wet season that covers the remaining nine months. On average, the months hit hardest by the wet season are from September to October, with 500 mm of rain falling between them.
Daytime temperatures do not vary at all day to day, and only vary a few degrees throughout the entire year. At night, the average low temperature is 21-22 degrees in every month of the year, apart from January when average low is 19 degrees). January has cooler nights and hotter days because it has clearer weather. Nonetheless, Malabo, with only 1,180 hours of sunshine per year, is one of the gloomiest capitals in the world and experiences much fog even when it is not raining.
Layout.
Despite its status as the capital of Equatorial Guinea for several decades, Malabo's street network remains poorly developed. Malabo itself has few paved roads leading into it, and fewer than one hundred paved and developed streets. Many of the street names reflect an African nationalist or anti-colonial theme, with names such as "Independence Avenue" or "Patrice Lumumba Road" being main roads. The few large roads not named for an African nationalist ideal or person are named for cities in Equatorial Guinea or other places or countries in Africa, as well as the road leading to the presidential palace. The palace and grounds consume a substantial part of the eastern side of Malabo, and are off-limits. The heart of the city is the colonial cathedral at Independence Place. Many buildings in the city from the Spanish colonial era are still standing.
The south of Malabo is bordered by the "Rio Consul". Across this lies the hospital to the southeast. To the west is the recently renovated Malabo Airport. The coastal northern region of the city is pierced by headlands and bays. The largest headland is the crescent-shaped "Tip of African Unity" behind the presidential palace. Encompassing the entire eastern side of Malabo Bay, it is almost as long as Malabo is tall. Malabo is part of a wider bay that represents most of the northern coast of Bioko; it stretches from "Europe Point" in the west (home to the airport), to barren lands in the east.
Notable buildings in Malabo include Malabo Cathedral, Malabo Government Building, and the Malabo Court Building. The city is served by Malabo International Airport, while ferries sail from its port to Douala and Bata. The city contains several notable hotels including Sofitel Malabo President Palace, Hotel Ureka, Hotel Bahia and Hotel Impala.
Discovery of oil.
Malabo has been significantly affected by Teodoro Obiang Nguema Mbasogo's growing co-operation with the oil industry. The country's production has reached 360000 oilbbl/d as of 2004, an increase which led to a doubling of the city's population, but for the vast majority, very little of that wealth has been invested in development.
Education.
The Colegio Nacional Enrique Nvó Okenve has campuses here and in Bata.
International schools include:
Transportation.
Malabo International Airport serves Malabo. Several domestic and international carriers fly into Malabo.
International relations.
Twin towns – Sister cities.
Malabo is twinned with:

</doc>
<doc id="48824" url="http://en.wikipedia.org/wiki?curid=48824" title="Gravitational lens">
Gravitational lens

A gravitational lens refers to a distribution of matter (such as a cluster of galaxies) between a distant source and an observer, that is capable of bending the light from the source, as it travels towards the observer. This effect is known as gravitational lensing and is one of the predictions of Albert Einstein's general theory of relativity.
Although Orest Chwolson (1924) or Frantisek Klin (1936) are sometimes credited as being the first ones to discuss the effect in print, the effect is more commonly associated with Einstein, who published a more famous article on the subject in 1936.
Fritz Zwicky posited in 1937 that the effect could allow galaxy clusters to act as gravitational lenses. It was not until 1979 that this effect was confirmed by observation of the so-called "Twin QSO" SBS 0957+561.
Description.
Unlike an optical lens, maximum 'bending' occurs closest to, and minimum 'bending' furthest from, the center of a gravitational lens. Consequently, a gravitational lens has no single focal point, but a focal line instead. If the (light) source, the massive lensing object, and the observer lie in a straight line, the original light source will appear as a ring around the massive lensing object. If there is any misalignment the observer will see an arc segment instead. This phenomenon was first mentioned in 1924 by the St. Petersburg physicist Orest Chwolson, and quantified by Albert Einstein in 1936. It is usually referred to in the literature as an Einstein ring, since Chwolson did not concern himself with the flux or radius of the ring image. More commonly, where the lensing mass is complex (such as a galaxy group or cluster) and does not cause a spherical distortion of space–time, the source will resemble partial arcs scattered around the lens. The observer may then see multiple distorted images of the same source; the number and shape of these depending upon the relative positions of the source, lens, and observer, and the shape of the gravitational well of the lensing object.
There are three classes of gravitational lensing:
1. Strong lensing: where there are easily visible distortions such as the formation of Einstein rings, arcs, and multiple images.
2. Weak lensing: where the distortions of background sources are much smaller and can only be detected by analyzing large numbers of sources to find coherent distortions of only a few percent. The lensing shows up statistically as a preferred stretching of the background objects perpendicular to the direction to the center of the lens.
By measuring the shapes and orientations of large numbers of distant galaxies, their orientations can be averaged to measure the shear of the lensing field in any region. This, in turn, can be used to reconstruct the mass distribution in the area: in particular, the background distribution of dark matter can be reconstructed. Since galaxies are intrinsically elliptical and the weak gravitational lensing signal is small, a very large number of galaxies must be used in these surveys. These weak lensing surveys must carefully avoid a number of important sources of systematic error: the intrinsic shape of galaxies, the tendency of a camera's point spread function to distort the shape of a galaxy and the tendency of atmospheric seeing to distort images must be understood and carefully accounted for. The results of these surveys are important for cosmological parameter estimation, to better understand and improve upon the Lambda-CDM model, and to provide a consistency check on other cosmological observations. They may also provide an important future constraint on dark energy.
3. Microlensing: where no distortion in shape can be seen but the amount of light received from a background object changes in time. The lensing object may be stars in the Milky Way in one typical case, with the background source being stars in a remote galaxy, or, in another case, an even more distant quasar. The effect is small, such that (in the case of strong lensing) even a galaxy with a mass more than 100 billion times that of the Sun will produce multiple images separated by only a few arcseconds. Galaxy clusters can produce separations of several arcminutes. In both cases the galaxies and sources are quite distant, many hundreds of megaparsecs away from our Galaxy.
Gravitational lenses act equally on all kinds of electromagnetic radiation, not just visible light. Weak lensing effects are being studied for the cosmic microwave background as well as galaxy surveys. Strong lenses have been observed in radio and x-ray regimes as well. If a strong lens produces multiple images, there will be a relative time delay between two paths: that is, in one image the lensed object will be observed before the other image.
History.
Spacetime around a massive object (such as a galaxy cluster or a black hole) is curved, and as a result light rays from a background source (such as a galaxy) propagating through spacetime are bent. The lensing effect can magnify and distort the image of the background source.
According to general relativity, mass "warps" space–time to create gravitational fields and therefore bend light as a result. This theory was confirmed in 1919 during a solar eclipse, when Arthur Eddington and Frank Watson Dyson observed the light from stars passing close to the Sun was slightly bent, so that stars appeared slightly out of position. 
Einstein realized that it was also possible for astronomical objects to bend light, and that under the correct conditions, one would observe multiple images of a single source, called a gravitational lens or sometimes a gravitational mirage.
However, as he only considered gravitational lensing by single stars, he concluded that the phenomenon would most likely remain unobserved for the foreseeable future. In 1937, Fritz Zwicky first considered the case where a galaxy could act as a source, something that according to his calculations should be well within the reach of observations.
It was not until 1979 that the first gravitational lens would be discovered. It became known as the "Twin QSO" since it initially looked like two identical quasistellar objects; it is officially named SBS 0957+561. This gravitational lens was discovered by Dennis Walsh, Bob Carswell, and Ray Weymann using the Kitt Peak National Observatory 2.1 meter telescope.
In the 1980s, astronomers realized that the combination of CCD imagers and computers would allow the brightness of millions of stars to be measured each night. In a dense field, such as the galactic center or the Magellanic clouds, many microlensing events per year could potentially be found. This led to efforts such as Optical Gravitational Lensing Experiment, or OGLE, that have characterized hundreds of such events.
Explanation in terms of space–time curvature.
In general relativity, light follows the curvature of spacetime, hence when light passes around a massive object, it is bent. This means that the light from an object on the other side will be bent towards an observer's eye, just like an ordinary lens. Since light always moves at a constant speed, lensing changes the direction of the velocity of the light, but not the magnitude.
Light rays are the boundary between the future, the spacelike, and the past regions. The gravitational attraction can be viewed as the motion of undisturbed objects in a background curved "geometry" or alternatively as the response of objects to a "force" in a flat geometry. The angle of deflection is:
toward the mass "M" at a distance "r" from the affected radiation, where "G" is the universal constant of gravitation and "c" is the speed of light in a vacuum.
Since the Schwarzschild radius formula_2 is defined as formula_3, this can also be expressed in simple form as
Search for gravitational lenses.
Most of the gravitational lenses in the past have been discovered accidentally. A search for gravitational lenses in the northern hemisphere (Cosmic Lens All Sky Survey, CLASS), done in radio frequencies using the Very Large Array (VLA) in New Mexico, led to the discovery of 22 new lensing systems, a major milestone. This has opened a whole new avenue for research ranging from finding very distant objects to finding values for cosmological parameters so we can understand the universe better.
A similar search in the southern hemisphere would be a very good step towards complementing the northern hemisphere search as well as obtaining other objectives for study. If such a search is done using well-calibrated and well-parameterized instrument and data, a result similar to the northern survey can be expected. The use of the Australia Telescope 20 GHz (AT20G) Survey data collected using the Australia Telescope Compact Array (ATCA) stands to be such a collection of data. As the data were collected using the same instrument maintaining a very stringent quality of data we should expect to obtain good results from the search. The AT20G survey is a blind survey at 20 GHz frequency in the radio domain of the electromagnetic spectrum. Due to the high frequency used, the chances finding gravitational lenses increases as the relative number of compact core objects (e.g. Quasars) are higher (Sadler et al. 2006). This is important as the lensing is easier to detect and identify in simple objects compared to objects with complexity in them. This search involves the use of interferometric methods to identify candidates and follow them up at higher resolution to identify them. Full detail of the project is currently under works for publication.
In a 2009 article on Science Daily a team of scientists led by a cosmologist from the U.S. Department of Energy's Lawrence Berkeley National Laboratory has made major progress in extending the use of gravitational lensing to the study of much older and smaller structures than was previously possible by stating that weak gravitational lensing improves measurements of distant galaxies.
Astronomers from the Max Planck Institute for Astronomy in Heidelberg, Germany, the results of which are accepted for publication on Oct 21, 2013 in the Astrophysical Journal Letters (arXiv.org), discovered what at the time was the most distant gravitational lens galaxy termed as J1000+0221 using NASA’s Hubble Space Telescope. While it remains the most distant quad-image lensing galaxy known, an even more distant two-image lensing galaxy was subsequently discovered by an international team of astronomers using a combination of Hubble Space Telescope and Keck telescope imaging and spectroscopy. The discovery and analysis of the IRC 0218 lens was published in the Astrophysical Journal Letters on June 23, 2014.
A research published Sep 30, 2013 in the online edition of Physical Review Letters, led by McGill University in Montreal, Canada, has discovered the B-modes, that are formed due to gravitational lensing effect, using National Science Foundation's South Pole Telescope and with help from the Herschel space observatory. This discovery would open the possibilities of testing the theories of how our universe originated.
Abell 2744 galaxy cluster - extremely distant galaxies revealed by gravitational lensing (16 October 2014).
Solar gravitational lens.
Albert Einstein predicted in 1936 that rays of light from the same direction that skirt the edges of the Sun would converge to a focal point approximately 542 AU from the Sun. Thus, the Sun could act as a gravitational lens for magnifying distant objects in a way that provides some flexibility in aiming unlike the coincidence-based lens usage of more distant objects, such as intermediate galaxies. A probe's location could shift around as needed to select different targets relative to the Sun (acting as a lens).
This distance is far beyond the progress and equipment capabilities of space probes such as Voyager 1, and beyond the known planets and dwarf planets, though over thousands of years 90377 Sedna will move further away on its highly elliptical orbit. The high gain for potentially detecting signals through this lens, such as microwaves at the 21-cm hydrogen line, led to the suggestion by Frank Drake in the early days of SETI that a probe could be sent to this distance. A multipurpose probe SETISAIL and later FOCAL was proposed to the ESA in 1993, but is expected to be a difficult task. If a probe does pass 542 AU, the gain and image-forming capabilities of the lens will continue to improve at further distances as the rays that come to a focus at these distances pass further away from the distortions of the Sun's corona.
Measuring weak lensing.
Kaiser et al. (1995), Luppino & Kaiser (1997) and Hoekstra et al. (1998) prescribed a method to invert the effects of the Point Spread Function (PSF) smearing and shearing, recovering a shear estimator uncontaminated by the systematic distortion of the PSF. This method (KSB+) is the most widely used method in current weak lensing shear measurements. 
Galaxies have random rotations and inclinations. As a result, the shear effects in weak lensing need to be determined by statistically preferred orientations. The primary source of error in lensing measurement is due to the convolution of the PSF with the lensed image. The KSB method measures the ellipticity of a galaxy image. The shear is proportional to the ellipticity. The objects in lensed images are parameterized according to their weighted quadrupole moments. For a perfect ellipse, the weighted quadrupole moments are related to the weighted ellipticity. KSB calculate how a weighted ellipticity measure is related to the shear and use the same formalism to remove the effects of the PSF.
KSB’s primary advantages are its mathematical ease and relatively simple implementation. However, KSB is based on a key assumption that the PSF is circular with an anisotropic distortion. It’s fine for current cosmic shear surveys, but the next generation of surveys (e.g. LSST) may need much better accuracy than KSB can provide. Because during that time, the statistical errors from the data are negligible, the systematic errors will dominate.
Gallery.
Gravitationally-lensed distant star-forming galaxy.

</doc>
<doc id="48827" url="http://en.wikipedia.org/wiki?curid=48827" title="Wittenberg">
Wittenberg

Wittenberg, officially Lutherstadt Wittenberg, is a city in Saxony-Anhalt, Germany. Situated on the river Elbe, it has a population of about 50,000.
The importance of Wittenberg historically was due to its seat of the Elector of Saxony, a dignity held by the dukes of Saxe-Wittenberg and also to its close connection with Martin Luther and the dawn of the Protestant Reformation; several of its buildings are associated with the events of this time. Part of the Augustinian monastery in which Luther dwelt, first as a monk and later as owner with his wife and family, is preserved and considered to be the world's premier museum dedicated to Luther. Various Luther and Melanchthon memorial sites were added to the UNESCO world heritage list in 1996.
History.
A settlement was first mentioned in 1180 as a small village founded by Flemish colonists under the rule of the House of Ascania. In 1260, this village became the residence of the dukes of Saxe-Wittenberg, and in 1293 the settlement was granted its town charter as a free-standing town.
Wittenberg developed into an important trade center during the following several centuries, because of its central location. When the Ascanians died out, the property of Saxe-Wittenberg passed to the House of Wettin. This town became an important regional political and cultural center at the end of the 15th Century, when Frederick III "the Wise", the Elector of Saxony, made his residence in Wittenberg. Several parts of boundaries of the town were extended soon afterward. The second bridge over the Elbe River was built from 1486 through 1490 (two years before Christopher Columbus sailed towards the New World) and the castle church (the "Schlosskirche" in German) was erected from 1496 through 1506. The Elector's palace was rebuilt the same time.
In 1502, the University of Wittenberg was founded, and it gave a home to some important thinkers, such as Martin Luther—a professor of theology beginning in 1508—and Philipp Melanchthon—a professor of Greek starting in 1518.
On 31 October 1517, Luther nailed his 95 theses against the selling of indulgences at the door of the All Saints', the Castle Church, marking the beginning of the Protestant Reformation. The Anabaptist movement also had one of its earliest homes in Wittenberg, when the Zwickau prophets moved there in late 1521, only to be suppressed by Luther when he returned from the Wartburg in spring 1522. The Capitulation of Wittenberg (1547) is the name given to the treaty by which John Frederick the Magnanimous was compelled to resign the electoral dignity and most of his territory to the Albertine branch of the House of Wettin.
In 1760, during the Seven Years' War, the Prussian-occupied town was bombarded by the Austrians. It was occupied by the French in 1806, and refortified in 1813 by command of Napoleon. In 1814, it was stormed by the Prussian Army under Tauentzien, who received the title of "von Wittenberg" as a reward. In 1815, Wittenberg became part of Prussia and was administered within the Province of Saxony. Wittenberg continued to be a fortress of the third class until the reorganisation of German defences after the foundation of the new German Empire led to its being dismantled in 1873.
Unlike many other historic German cities during World War II, Wittenberg's city centre was spared destruction during the war. The Allies agreed not to bomb Wittenberg, though there was fighting in the city, with bullet pock-marks visible on the statues of Luther and Melanchthon at the market square, or so the popular version of the city's history goes. In actuality, the Luther statue was not even present in the city square during much of the war. It was stored at Luther Brunnen, a roadhouse only a few kilometers north of the city.
Wittenberg's reputation as a city protected from Allied bombing is also not historically accurate. On the outskirts of Wittenberg was the "Arado Flugzeugwerke" (the Arado Aircraft Factory), which produced components of airplanes for the Luftwaffe. This factory was staffed by Jews, Russians, Poles, political prisoners and even a few Americans—all prisoners engaging in forced labour. Despite the prisoner status of its workers, American and British planes bombed the factory near the end of the war. One thousand prisoner workers were killed. The recent publication of "...und morgen war Krieg!" by Renate Gruber-Lieblich attempts to document this tragic bombing of Wittenberg.
At the end of the war, Wittenberg was occupied by Soviet forces and became part of East Germany in 1949. During the East German period, it was part of Halle District. By means of the peaceful revolution in 1989, the communist regime was brought down and the city has been governed democratically since 1990.
Main sights.
Wittenberg is home to numerous historical sites, as well as portraits and other paintings by Lucas Cranach the Elder and Younger. On the doors of All Saints' Church, the "Schlosskirche" ("castle church", built in 1496–1506) Luther is said to have nailed his 95 theses in 1517. It was seriously damaged by fire in 1760 during a bombardment by the French during the Seven Years' War, was practically rebuilt, and was later (1885–1892) restored. The wooden doors, burnt in 1760, were replaced in 1858 by bronze doors, bearing the Latin text of the theses. Inside the church are the tombs of Luther and Philipp Melanchthon, and of the electors Frederick the Wise (by Peter Vischer the Younger, 1527) and John the Constant (by Hans Vischer), and portraits of the reformers by Lucas Cranach the Younger.
St. Mary's Church, the parish church in which Luther often preached, was built in the 14th century, but has been much altered since Luther's time. It contains a magnificent painting by Lucas Cranach the Elder, representing the Last Supper (with the faces of Luther and other reformers), Baptism and Confession, also a font by Hermann Vischer the Elder (1457). In addition, there are numerous historic paintings in the church.
The ancient electoral palace is another of the buildings that suffered severely in 1760; it now contains archives.
Martin Luther's home, the Lutherhaus, where he studied and lived both before and after the Reformation, is now a museum containing many artifacts from his life. Melanchthon's house and the house of Lucas Cranach the Elder, mayor of Wittenberg, can also be found here. Statues of Luther (by Schadow), Melanchthon and Bugenhagen embellish the town. The spot outside the Elster Gate where Luther publicly burned the papal bull in 1520 is marked by an oak tree.
Coat of arms.
Wittenberg's civic coat of arms conveys with its various heraldic elements something of the town's history. On 27 June 1293, Wittenberg was granted town rights by Duke Albrecht II. There then arose a mediaeval town whose highest governing body was its council. This council, known to have existed as early as 1317, was given the job of administering the town in its care through law and legislation, and of handling the town's revenue. For documentation, the administration used its own seal.
One version of what is believed to be the town's oldest town seal, which the council used, and which dated from the first half of the 14th century, set the pattern with its elements for various civic coats of arms down to the present day.
The coat of arms symbolizes, with its crenelated wall and the towers within and each side, a town that was already strongly fortified by 1409.
The two shields in the centre form the coat of arms of the Electorate of Saxony with the Saxon arms on the right, whose gold and black stripes recall the Ascanian rulers' house colours with the "Rautenkranz" (literally "lozenge wreath", although it is no such thing, as can be seen at the Saxony article) across them symbolizing the town's founder Duke Albrecht II since 1262, when it appeared in his arms.
The shield on the left is the Wittenberg district's arms. In 1356, Emperor Charles IV bestowed upon the Duke of Saxony-Wittenberg the honour of Elector. Wittenberg became an Electoral residence. The shield with its crossed swords stands for the office of "Arch-Marshal of the Holy Roman Empire" inextricably joined by the Electorate, brought to Wittenberg by Rudolf I. Both coats of arms continued to be used by the Wettins after the Ascanians died out.
The flowing water at the foot of the shield symbolizes Wittenberg's location on the River Elbe.
The fish is a salmon, which were once abundant in the Elbe. The fishermen, like all professions in town, got their own order in 1422, and the fish found its way onto their coat of arms.
Economy and Infrastructure.
The City is an important centre of chemical industry with the SKW Stickstoffwerke Piesteritz GmbH. The whole area of the industrial park covers more than 220 hectars with more than 1,500 workers. Wittenberg is also the headquarters of the eco-friendly web search engine Ecosia. Tourism plays a major role. Wittenberg is one of the top destinations in Saxony-Anhalt.
Lutherstadt Wittenberg station is the main railway station. It connects Wittenberg hourly with Berlin to the north and Leipzig and Halle (Saale) to the south. The station is being rebuilt. Construction should be completed in 2017.
Theatre, culture and education.
Wittenberg has a long tradition of cultural events. The City Theatre ("Mitteldeutsches Landestheater") reached a great importance in GDR times. Since 1996, the City has staged open-air theatre shows based on the Lutheran history still alive in many historical places of the ancient town. As highlights, in 2001 and 2005, Fernando Scarpa became the artistic director of the "Bühne Wittenberg" (Stage Wittenberg), a project for theatre, art and culture in the whole of Germany which attracts to the city plenty of audience and whose success achieves European echo. On 2002 and 2003 Stefano Vagnini, Italian composer and organist created the music for "Thesys" and "Luther Stories". Prince Hamlet is said to have studied in Wittenberg and it was the supposed home of Dr Faustus.
Wittenberg is seat of the Leucorea which is part of the Martin Luther University of Halle-Wittenberg, the largest university in Saxony-Anhalt.
International relations.
Wittenberg is twinned with:

</doc>
<doc id="48830" url="http://en.wikipedia.org/wiki?curid=48830" title="Georgia (U.S. state)">
Georgia (U.S. state)

Georgia ( ) is a state located in the southeastern United States. It was established in 1732, the last of the original Thirteen Colonies. Named after King George II of Great Britain, Georgia was the fourth state to ratify the United States Constitution, on January 2, 1788. It declared its secession from the Union on January 19, 1861, and was one of the original seven Confederate states. It was the last state to be restored to the Union, on July 15, 1870. Georgia is the 24th largest and the 8th most populous of the 50 United States. From 2007 to 2008, 14 of Georgia's counties ranked among the nation's 100 fastest-growing, second only to Texas. Georgia is known as the "Peach State" and the "Empire State of the South". Atlanta is the state's capital and its most populous city.
Georgia is bordered on the south by Florida, on the east by the Atlantic Ocean and South Carolina, on the west by Alabama, and on the north by Tennessee and North Carolina. The northern part of the state is in the Blue Ridge Mountains, part of the Appalachian Mountains system. The Piedmont extends through the central part of the state from the foothills of the Blue Ridge to the Fall Line, where the rivers cascade down in elevation to the coastal plain of the southern part of the state. The highest point in Georgia is Brasstown Bald at 4784 ft above sea level; the lowest point is the Atlantic Ocean. Georgia is the largest state east of the Mississippi River in land area, although it is the fourth largest (after Michigan, Florida, and Wisconsin) in total area, including expanses of water that are part of state territory.
History.
Before settlement by Europeans, Georgia was inhabited by the mound building cultures. The British colony of Georgia was founded by James Oglethorpe on February 12, 1733 (February 1, 1732 O.S.). The colony was administered by the Trustees for the Establishment of the Colony of Georgia in America under a charter issued by (and named for) King George II. The Trustees implemented an elaborate plan for the colony's settlement, known as the Oglethorpe Plan, which envisioned an agrarian society of yeoman farmers and prohibited slavery. In 1742 the colony was invaded by the Spanish during the War of Jenkins' Ear. In 1752, after the government failed to renew subsidies that had helped support the colony, the Trustees turned over control to the crown. Georgia became a crown colony, with a governor appointed by the king.
The Province of Georgia was one of the Thirteen Colonies that revolted against British rule in the American Revolution by signing the 1776 Declaration of Independence. The State of Georgia's first constitution was ratified in February 1777. Georgia was the 10th state to ratify the Articles of Confederation on July 24, 1778, and was the 4th state to ratify the current Constitution on January 2, 1788.
In 1829, gold was discovered in the North Georgia mountains, which led to the Georgia Gold Rush and an established federal mint in Dahlonega, which continued its operation until 1861. The subsequent influx of white settlers put pressure on the government to take land from the Cherokee Nation. In 1830, President Andrew Jackson signed the Indian Removal Act into law, sending many eastern Native American nations to reservations in present-day Oklahoma, including all of Georgia's tribes. Despite the Supreme Court's ruling in Worcester v. Georgia that states were not permitted to redraw the Indian boundaries, President Jackson and the state of Georgia ignored the ruling. In 1838, his successor, Martin Van Buren, dispatched federal troops to gather the Cherokee and deport them west of the Mississippi. This forced relocation, known as the Trail of Tears, led to the death of over 4,000 Cherokees.
In early 1861, Georgia joined the Confederacy and became a major theater of the Civil War. Major battles took place at Chickamauga, Kennesaw Mountain, and Atlanta. In December 1864, a large swath of the state from Atlanta to Savannah was destroyed during General William Tecumseh Sherman's March to the Sea. 18,253 Georgian soldiers died in service, roughly 1 of every 5 who served. In 1870, following reconstruction, Georgia became the last Confederate state restored to the Union.
With white Democrats having regained power in the state legislature, they passed a poll tax in 1877, which disfranchised many poor blacks and whites, preventing them from registering. In 1908, the state established a white primary; with the only competitive contests within the Democratic Party, it was another way to exclude blacks from politics. They constituted 46.7% of the state's population in 1900.<ref name="pop/perc">, accessed March 15, 2008</ref> This disfranchisement persisted through the mid-1960s, until federal legislation with the Voting Rights Act of 1965.
Geography.
Boundaries.
Beginning from the Atlantic Ocean, the state's eastern border with South Carolina runs up the Savannah River, northwest to its origin at the confluence of the Tugaloo and Seneca Rivers. It then continues up the Tugaloo (originally Tugalo) and into the Chattooga River, its most significant tributary. These bounds were decided in the 1797 Treaty of Beaufort, and tested in the U.S. Supreme Court in the two "Georgia v. South Carolina" cases in 1923 and 1989.
The border then takes a sharp turn around the tip of Rabun County, at latitude 35°N, though from this point it diverges slightly south (due to inaccuracies in the original survey). This was originally the Georgia and North Carolina border all the way back to the Mississippi River, until Tennessee was divided from North Carolina, and the Yazoo companies induced the legislature of Georgia to pass an act, approved by the governor in 1795, to sell the greater part of Georgia's territory presently comprising Alabama and Mississippi.
The state's western border then departs in another straight line south-southeastward, at a point southwest of Chattanooga, to meet the westernmost point of the Chattahoochee River near West Point. It continues down to the point where it ends at the Flint River (the confluence of the two forming Florida's Apalachicola River), and goes almost due east and very slightly south, in a straight line to the origin of the St. Mary's River, which then forms the remainder of the boundary back to the ocean.
The water boundaries are still set to be the original thalweg of the rivers. Since then, several have been inundated by lakes created by dams, including the Apalachicola/Chattahoochee/Flint point now under Lake Seminole.
Georgia state legislators have claimed that in an 1818 survey, the state's border with Tennessee was erroneously placed one mile (1.6 km) farther south than intended, and they proposed a correction in 2010. The state was then in the midst of a significant drought, and the new border would allow Georgia access to water from the Tennessee River.
Geology and terrain.
Each region has its own distinctive characteristics. For instance, the Ridge and Valley, which lies in the northwest corner of the state, includes limestone, sandstone, shale and other sedimentary rocks, which have yielded construction-grade limestone, barite, ocher, and small amounts of coal.
Flora.
The state of Georgia has approximately 250 tree species and 58 protected plants. Georgia's native trees include red cedar, a variety of pines, oaks, maples, cypress, sweetgum and scaly-bark and white hickories. Palmettos and other subtropical flora are found in the southern and coastal regions. Yellow jasmine, and mountain laurel make up just a few of the flowering shrubs in the state.
Fauna.
White-tailed (Virginia) deer are in nearly all counties. The northern mockingbird and brown thrasher are among the 160 bird species that live in the state.
Reptiles and amphibians include the eastern diamondback, copperhead, and cottonmouth, salamanders, frogs, alligators and toads. There are about 79 species of reptile and 63 amphibians known to live in Georgia.
The most popular freshwater game fish are trout, bream, bass, and catfish, all but the last of which are produced in state hatcheries for restocking. Popular saltwater game fish include red drum, spotted seatrout, flounder, and tarpon. Porpoises, whales, shrimp, oysters, and blue crabs are found inshore and offshore of the Georgia coast.
Climate.
The majority of the state is primarily a humid subtropical climate. Hot and humid summers are typical, except at the highest elevations. The entire state, including the North Georgia mountains, receives moderate to heavy precipitation, which varies from 45 inches (1143 mm) in central Georgia to approximately 75 inches (1905 mm) around the northeast part of the state. The degree to which the weather of a certain region of Georgia is subtropical depends on the latitude, its proximity to the Atlantic Ocean or Gulf of Mexico, and the elevation. The latter factor is felt chiefly in the mountainous areas of the northern part of the state, which are farther away from the ocean and can be 4500 feet (1350 m) above sea level. The USDA Plant hardiness zones for Georgia range from zone 6b (no colder than -5 F ) in the Blue Ridge Mountains to zone 8b (no colder than 15 F ) along the Atlantic coast and Florida border.
The highest temperature ever recorded is 112 °F (44.4 °C) in Louisville on July 24, 1952, while the lowest is −17 °F (−27.2 °C) in northern Floyd County on January 27, 1940. Georgia is one of the leading states in frequency of tornadoes, though they are rarely stronger than F1. A tornado hit downtown Atlanta on March 14, 2008, causing moderate to severe damage. With a coastline on the Atlantic Ocean, Georgia is also vulnerable to hurricanes, although direct hurricane strikes were rare during the 20th century. Georgia often is affected by hurricanes that strike the Florida panhandle, weaken over land, and bring strong tropical storm winds and heavy rain to the interior, as well as hurricanes that come close to the Georgia coastline, brushing the coast on their way north.
Demographics.
The United States Census Bureau estimates that the population of Georgia was 10,097,343 on July 1, 2014, a 4.23% increase since the 2010 United States Census
In 2014, Georgia had an estimated population of 10,097,343 which was an increase of 105,176 from the previous year, and an increase of 409,690 since 2010. This includes a natural increase since the last census of 438,939 people (that is 849,414 births minus 410,475 deaths) and an increase from net migration of 606,673 people into the state. Immigration resulted in a net increase of 228,415 people, and migration within the country produced a net increase of 378,258 people.
As of 2010, the state has the sixth highest number of illegal immigrants in the country. There were 35,000 in 1990; the count more than doubled from January 2000 to January 2009, at 480,000.
There were 743,000 veterans in 2009.
Race and age.
According to the 2010 United States Census, Georgia had a population of 9,687,653. In terms of race and ethnicity, the state was 59.7% White (55.9% Non-Hispanic White Alone), 30.5% Black or African American, 0.3% American Indian and Alaska Native, 3.2% Asian, 0.1% Native Hawaiian and Other Pacific Islander, 4.0% from Some Other Race, and 2.1% from Two or More Races. Hispanics and Latinos of any race made up 8.8% of the population.
As of 2011, 58.8% of Georgia's population younger than age 1 were minorities.
The largest European ancestry groups are:
In the 1980 census 1,584,303 Georgians claimed English ancestry out of a total state population of 3,994,817, making them 40% of the state, and the largest ethnic group at the time. Today, many of these same people claiming that they are of "American" ancestry are actually of English descent, and some are of Scots-Irish descent; however, their families have lived in the state for so long, in many cases since the colonial period, that they choose to identify simply as having "American" ancestry or do not in fact know their own ancestry. Their ancestry primarily goes back to the original thirteen colonies and for this reason many of them today simply claim "American" ancestry, though they are of predominately English ancestry.
As of 2004, 7.7% of Georgia's population was reported as under 5 years of age, 26.4% under 18, and 9.6% were 65 or older. Also as of 2004, females made up approximately 50.6% of the population and African Americans made up approximately 29.6%.
Historically, about half of Georgia's population was composed of African Americans who, prior to the Civil War, were almost exclusively enslaved. The Great Migration of hundreds of thousands of blacks from the rural South to the industrial North from 1914–70 reduced the African American population.
Georgia had the second-fastest-growing Asian population growth in the U.S. from 1990 to 2000, more than doubling in size during the ten-year period. In addition, according to census estimates, Georgia ranks third among the states in terms of the percent of the total population that is African American (after Mississippi and Louisiana) and third in numerical Black population after New York and Florida. Georgia was the state with the largest numerical increase in the black population from 2006 to 2007 with 84,000.
Georgia is the state with the third-lowest percentage of older people (65 or older), at 10.1 percent (as of 2008).
The colonial settlement of large numbers of Scottish American, English American and Scotch-Irish Americans in the mountains and piedmont, and coastal settlement by some English Americans and African Americans, have strongly influenced the state's culture in food, language and music. The concentration of Africans imported to coastal areas in the 18th century repeatedly from rice-growing regions of West Africa led to the development of Gullah-Geechee language and culture in the Low Country among African Americans. They share a unique heritage in which African traditions of food, religion and culture were continued more than in some other areas. In the creolization of Southern culture, their foodways became an integral part of all Southern cooking in the Low Country.
Languages.
As of 2010, 87.35% (7,666,663) of Georgia residents age 5 and older spoke English at home as a primary language, while 7.42% (651,583) spoke Spanish, 0.51% (44,702) Korean, 0.44% (38,244) Vietnamese, 0.42% (36,679) French, 0.38% (33,009) Chinese (which includes Mandarin,) and German was spoken as a main language by 0.29% (23,351) of the population over the age of five. In total, 12.65% (1,109,888) of Georgia's population age 5 and older spoke a mother language other than English.
Major cities.
"Populations indicated above are the latest 2013 estimates from the US Census Bureau. In 2012, voters in Macon and Bibb County approved the consolidation of the city of Macon and unincorporated Bibb County (the population above is for the totality of Bibb County). Macon joined Columbus, Augusta, and Athens as consolidated cities in Georgia."
Religion.
The composition of religious affiliation in Georgia is 70% Protestant, 12% Catholic, 1% Mormon, 1% Jewish, 0.5% Muslim, 0.5% Buddhist and 0.5% Hindu. Atheists, deists, agnostics and other unaffiliated people make up 13% of the population. The largest Christian denominations by number of adherents in 2010 were the Southern Baptist Convention with 1,759,317; the United Methodist Church with 619,394; and the Catholic Church with 596,384. Non-denominational Evangelical Protestant had 566,782 members, the Church of God (Cleveland, Tennessee) has 175,184 members, and the National Baptist Convention, USA, Inc. has 172,982 members. The PC(USA) has almost 100,000 members and 300 congregations, the Presbyterian Church in America had at its founding date 14 congregations and 2,800 members, in 2010 it had 139 congregations and 32,000 members. Georgia is home to the largest Hindu temple in the United States, the BAPS Shri Swaminarayan Mandir Atlanta. Georgia is home to several historical Jewish Synagogues including The Temple (Atlanta), Congregation Beth Jacob (Atlanta), and Congregation Mickve Israel. Chabad and the Rohr Jewish Learning Institute are also active in the state.
Government.
State government.
As with all other US states and the federal government, Georgia's government is based on the separation of legislative, executive, and judicial power. Executive authority in the state rests with the governor, currently Nathan Deal (Republican). (See List of Governors of Georgia). Both the governor and lieutenant governor are elected on separate ballots to four-year terms of office. Unlike the federal government, but like many other U.S. States, most of the executive officials who comprise the governor's cabinet are elected by the citizens of Georgia rather than appointed by the governor.
Legislative authority resides in the General Assembly, composed of the Senate and House of Representatives. The Lieutenant Governor presides over the Senate, while members of the House of Representatives select their own Speaker. The Georgia Constitution mandates a maximum of 56 senators, elected from single-member districts, and a minimum of 180 representatives, apportioned among representative districts (which sometimes results in more than one representative per district); there are currently 56 senators and 180 representatives. The term of office for senators and representatives is two years. The laws enacted by the General Assembly are codified in the Official Code of Georgia Annotated.
Georgia stands alone as the only U.S. state to enact a full codification of the common law of contracts, torts, property and domestic relations that was completely independent of David Dudley Field II. This resulted from the breakdown in North-South communications preceding the Civil War, which meant that the Georgia codifiers and Field were unaware of each other's work. (The four other common law states that pursued such codification enacted versions of Field's civil code.)
State judicial authority rests with the state Supreme Court and Court of Appeals, which have statewide authority. In addition, there are smaller courts which have more limited geographical jurisdiction, including State Courts, Superior Courts, Magistrate Courts and Probate Courts. Justices of the Supreme Court and judges of the Court of Appeals are elected statewide by the citizens in non-partisan elections to six-year terms. Judges for the smaller courts are elected to four-year terms by the state's citizens who live within that court's jurisdiction.
Local government.
Georgia consists of 159 counties, second only to Texas, with 254. Georgia had 161 counties until the end of 1931, when Milton and Campbell were merged into the existing Fulton. Some counties have been named for prominent figures in both American and Georgian history, and many bear names with Native American origin. Counties in Georgia have their own elected legislative branch, usually called the Board of Commissioners, which usually also has executive authority in the county. Several counties have a sole Commissioner form of government, with legislative and executive authority vested in a single person. Georgia is the only state with Sole Commissioner counties. Georgia's Constitution provides all counties and cities with "home rule" authority. The county commissions have considerable power to pass legislation within their county, as a municipality would.
Georgia recognizes all local units of government as cities, so every incorporated town is legally a city. Georgia does not provide for townships or independent cities, though there is a movement in the Legislature to provide for townships; it does allow consolidated city-county governments by local referendum. All of Georgia's second-tier cities except Savannah have now formed consolidated city-county governments by referendum: Columbus (in 1970), Athens (1990), Augusta (1995), and Macon (2012). (Augusta and Athens have excluded one or more small, incorporated towns within their consolidated boundaries, while Macon has excluded a small unincorporated area; Columbus eventually absorbed all smaller incorporated entities within its consolidated boundaries.) The small town of Cusseta adopted a consolidated city-county government in 2003.
There is no true metropolitan government in Georgia, though the Atlanta Regional Commission (ARC) and Georgia Regional Transportation Authority do provide some services, and the ARC must approve all major land development projects in the Atlanta metropolitan area.
Elections.
Until recently, Georgia's state government had the longest unbroken record of single-party dominance, by the Democratic Party, of any state in the Union. This record was established largely due to the disenfranchisement of most blacks and many poor whites by the state in its constitution and laws in the early 20th century. Some elements, such as requiring payment of poll taxes and passing literacy tests, prevented blacks from registering to vote; their exclusion from the political system lasted into the 1960s and reduced the Republican Party to a non-competitive status in the early 20th century.
White Democrats regained power after Reconstruction through intimidation, violence and fraud. In 1900, shortly before Georgia adopted a disfranchising constitutional amendment in 1908, blacks comprised 47% of the state's population. Thus nearly half the population was excluded when the state excluded blacks from the political system. In the early 20th century, Progressives promoted electoral reform and reducing the power of ward bosses to clean up politics. Rules such as the eight-box law effectively closed out people who were illiterate. White, one-party rule was solidified.
For more than 130 years, from 1872 to 2003, Georgians nominated and elected only white Democratic governors, and white Democrats held the majority of seats in the General Assembly. Most of the Democrats elected throughout these years were Southern Democrats or Dixiecrats, who were fiscally and socially conservative by national standards. This voting pattern continued after the segregationist period.
Segregation was ended by court orders in the 1960s. According to the 1960 census, the proportion of Georgia's population that was African American was 28%; many blacks had left the state in the Great Migration, and new generations of whites had come from migration and immigration. Following support from the national Democratic Party for the civil rights movement and especially civil rights legislation of 1964 and 1965, most African-American voters, as well as other minority voters, have largely supported the Democratic Party in Georgia. In the decades since the late 20th century, the white-majority voters have increasingly supported Republicans for national and state offices.
In 2003, incumbent Governor Roy Barnes was defeated by Republican Sonny Perdue, a state legislator and former Democrat. While Democrats retained control of the State House, they lost their majority in the Senate when four Democrats switched parties. They lost the House in the 2004 election. Republicans now control all three partisan elements of the state government.
Even before 2003, the state had become increasingly supportive of Republicans in Presidential elections. It has supported a Democrat for president only three times since 1960. In 1976 and 1980, native son Jimmy Carter carried the state; in 1992, the former Arkansas governor Bill Clinton narrowly won the state. Generally, Republicans are strongest in the predominantly white suburban (especially the Atlanta suburbs) and rural portions of the state. Many of these areas were represented by conservative Democrats in the state legislature well into the 21st century. One of the most conservative of these was U.S. Congressman Larry McDonald, former head of the John Birch Society, who died when the Soviet Union shot down KAL 007 near Sakhalin Island. Democratic candidates have tended to win a higher percentage of the vote in the areas where black voters are most numerous, as well as in the cities (especially Atlanta and Athens), and the rural Black Belt region that passes through the central and southwestern portion of the state.
The ascendancy of the Republican Party in Georgia and in the South in general also resulted in Georgia U.S. House of Representatives member Newt Gingrich being elected as Speaker of the House following the election of a Republican majority in the House in 1994. Gingrich served as Speaker until 1999, when he resigned in the aftermath of the loss of House seats held by members of the GOP. Gingrich also mounted an unsuccessful bid for President in the 2012 election, but withdrew after winning only the South Carolina and Georgia primaries.
As of the 2010[ [update]] reapportionment, the state has 14 seats in the U.S. House of Representatives. These are held by 10 Republicans and 4 Democrats.
In recent events, Democrat Jim Martin ran against incumbent Republican Senator Saxby Chambliss. Chambliss failed to acquire the necessary 50 percent of votes, a Libertarian Party candidate receiving the remainder of votes. In the runoff election held on December 2, 2008, Chambliss became the second Georgia Republican to be reelected to the U.S. Senate.
Politics.
During the 1960s and 1970s, Georgia made significant changes in civil rights, governance, and economic growth focused on Atlanta. It was a bedrock of the emerging "New South".
In the 21st century, many conservative Democrats, including former U.S. Senator and governor Zell Miller, have decided to support Republicans. The state's socially conservative bent results in wide support for such measures as restrictions on abortion. In 2004, a state constitutional amendment banning same-sex marriages was approved by 76% of voters.
On April 1, 2009, Senate Resolution 632 passed by a vote of 43–1. It reads in part:Any Act by the Congress of the United States, Executive Order of the President of the United States of America or Judicial Order by the Judicatories of the United States of America which assumes a power not delegated to the government of the United States of America by the Constitution for the United States of America and which serves to diminish the liberty of the any of the several States or their citizens shall constitute a nullification of the Constitution for the United States of America by the government of the United States of America. On April 16, Jay Bookman of The Atlanta Journal-Constitution wrote "It wasn't quite the firing on Fort Sumter that launched the Civil War. But on April 1, your Georgia Senate did threaten by a vote of 43–1 to secede from and even disband the United States."
Economy.
Georgia's 2010 total gross state product was $403 billion. Its per capita personal income for 2011 put it 39th in the nation at $35,979. If Georgia were a stand-alone country, it would be the 28th largest economy in the world.
There are 15 Fortune 500 companies and 26 Fortune 1000 companies with headquarters in Georgia, including such names as Home Depot, UPS, Coca-Cola, Delta Air Lines, Aflac, Southern Company, and SunTrust Banks. Hartsfield-Jackson Atlanta International Airport, the world's busiest airport as measured by passenger traffic and aircraft traffic, is located in Georgia. Georgia has over 1,700 internationally headquartered facilities representing 43 countries, employing more than 112,000 Georgians with an estimated capital investment of $23 billion.
Atlanta has a large effect on the state of Georgia and the Southeastern United States. Atlanta has been the site of growth in real estate, service, and the communications industries.
Tourism makes an important contribution to the economy.
Agriculture.
Widespread farms produce peanuts, corn, and soybeans across middle and south Georgia. The state is the number one producer of pecans in the world, with the region around Albany in southwest Georgia being the center of Georgia's pecan production. Gainesville in northeast Georgia touts itself as the Poultry Capital of the World. Georgia is in the top five blueberry producers in the United States.
Georgia's agricultural outputs include poultry and eggs, pecans, peaches, cotton, peanuts, rye, cattle, hogs, dairy products, turfgrass, timber, particularly pine trees, tobacco and vegetables.
Mining.
Major products in the mineral industry include a variety of clays, stones, sands and the clay palygorskite, known as attapulgite.
Industry.
Industry in Georgia is diverse.
The textile industry is located around the cities of Rome, Columbus, Augusta, Macon and along the I-75 corridor between Atlanta and Chattanooga, Tennessee, to include the towns of Cartersville, Calhoun, Ringgold and Dalton (the Carpet Capital of the World).
In November 2009, Kia started production at the first U.S. Kia Motors plant, Kia Motors Manufacturing Georgia in West Point.
Industrial output includes textiles and apparel, transportation equipment, food processing, paper products, chemical products, and electric equipment.
Logistics.
The Georgia Ports Authority owns and operates four ports in the state: Port of Savannah, Port of Brunswick, Port Bainbridge, and Port Columbus. The Port of Savannah is the fourth largest seaport in the United States, importing and exporting a total of 2.3 million TEUs per year. Several major companies including Target, IKEA, and Heineken operate distribution centers in close proximity to the Port of Savannah.
Military.
Georgia has one of the largest military presences in the country. Several US military installations are located in the state including Fort Stewart, Hunter Army Airfield, Naval Submarine Base Kings Bay, Fort Benning, Moody Air Force Base, Robins Air Force Base, Fort Gordon, Marine Corps Logistics Base Albany, Dobbins Air Reserve Base, Coast Guard Air Station Savannah and Coast Guard Station Brunswick.
Energy use and production.
Georgia's electricity generation and consumption are among the highest in the United States, with natural gas being the primary electrical generation fuel, followed by coal. However, the state also has two nuclear power plants which contribute almost one fourth of Georgia's electricity generation. In 2013, the generation mix was 39% gas, 35% coal, 23% nuclear, 3% hydro and other renewable sources. The leading area of energy consumption is the industrial sector because Georgia "is a leader in the energy-intensive wood and paper products industry". Solar generated energy is becoming more in use with solar energy generators currently installed ranking Georgia 15th in the country in installed solar capacity. In 2013, $189 million was invested in Georgia to install solar for home, business and utility use representing a 795% increase over the previous year.
State taxes.
Georgia has a progressive income tax structure with six brackets of state income tax rates that range from 1% to 6%. In 2009, Georgians paid 9% of their income in state and local taxes, compared to the US average of 9.8% of income. This ranks Georgia 25th among the states for total state and local tax burden. The state sales tax in Georgia is 4% with additional percentages added through local options (e.g. Special-purpose local-option sales tax or SPLOST), but there is no sales tax on prescription drugs, certain medical devices, or food items for home consumption.
The state legislature may allow municipalities to institute local sales taxes and special local taxes, such as the 2% SPLOST tax and the 1% sales tax for MARTA serviced counties. Excise taxes are levied on alcohol, tobacco, and motor fuel. Owners of real property in Georgia pay property tax to their county. All taxes are collected by the Georgia Department of Revenue and then properly distributed according to any agreements that each county has with its cities.
Film.
The Georgia Film, Music and Digital Entertainment Office promotes filming in the state. Since 1972, seven hundred film and television projects have been filmed on location in Georgia. In 2008–2009, Georgia's film and television industry created a $1.15 billion economic impact on the state's economy.
Tourism.
In the Atlanta area, World of Coke, Georgia Aquarium, Zoo Atlanta and Stone Mountain are important tourist attractions. Stone Mountain is Georgia's "most popular attraction"; receiving over four million tourists per year. The Georgia Aquarium, in Atlanta, was the largest aquarium in the world in 2010 according to Guinness World Records.
Callaway Gardens, in western Georgia, is a family resort. The area is also popular with golfers.
The Savannah Historic District attracts over eleven million tourists each year.
The Golden Isles are a string of barrier islands off the Atlantic coast of Georgia near Brunswick that include beaches, golf courses and the Cumberland Island National Seashore.
Several sites honor the lives and careers of noted American leaders: the Little White House in Warm Springs, which served as the summer residence of President Franklin Delano Roosevelt while he was being treated for polio; President Jimmy Carter's hometown of Plains and the Carter Presidential Center in Atlanta; the Martin Luther King, Jr., National Historic Site in Atlanta, which is the final resting place of Martin Luther King, Jr. and Coretta Scott King; and Atlanta's Ebenezer Baptist Church, where Dr. King preached.
Cultural.
Fine and performing arts.
Georgia's major fine art museums include the High Museum of Art and the Michael C. Carlos Museum, both in Atlanta; the Georgia Museum of Art on the campus of the University of Georgia in Athens; Telfair Museum of Art in Savannah; and the Morris Museum of Art in Augusta.
The state theatre of Georgia is the Springer Opera House located in Columbus.
The Atlanta Opera brings opera to Georgia stages. The Atlanta Symphony Orchestra is the most widely recognized orchestra and largest arts organization in the southeastern United States.
There are a number of performing arts venues in the state, among the largest are the Fox Theatre, and the Alliance Theatre at the Woodruff Arts Center, both on Peachtree Street in Midtown Atlanta as well as the Cobb Energy Performing Arts Centre, located in Northwest Atlanta.
Literature.
The rich heritage and southern antebellum atmosphere of Georgia has given rise to a great number of works such as Margaret Mitchell's "Gone with the Wind", Olive Ann Burns' "Cold Sassy Tree", and Alice Walker's "The Color Purple".
A number of noted authors, poets and playwrights have lived in Georgia such as James Dickey, Flannery O'Connor, Sidney Lanier, Frank Yerby and Lewis Grizzard.
Television.
Well-known television shows set in Atlanta include, from Tyler Perry Studios, "House of Payne" and "Tyler Perry's Meet the Browns", "The Real Housewives of Atlanta", the CBS sitcom "Designing Women", the popular AMC series "The Walking Dead", Lifetime "Drop Dead Diva", "Rectify" and numerous HGTV original productions.
"The Dukes of Hazzard", a 1980s TV show that spun off several films, was set in the fictional Hazzard County, Georgia. The show was filmed at Warner Bros. back lot in Burbank, California and on location in Conyers and Covington Georgia as well as some locations in Atlanta.
Also filmed in Georgia is "The Vampire Diaries", using Covington as the setting for the fictional Mystic Falls.
Music.
A number of notable musicians in various genres of popular music are from Georgia. Included is Ray Charles (whose many hits include "Georgia on My Mind", now the official state song), and Gladys Knight (known for her Georgia-themed song, "Midnight Train to Georgia").
Rock groups from Georgia include the Atlanta Rhythm Section, The Black Crowes, the Allman Brothers.
The university city of Athens sparked an influential rock music scene in the 1980s and 1990s. Among the groups achieving their initial prominence in that city were R.E.M., Widespread Panic, and the B-52's.
Since the 1990s, various hip-hop and R&B musicians have included top-selling artists such as Outkast, Usher, Ludacris, TLC, B.o.B., and Ciara. Atlanta is mentioned in a number of these artists' tracks, such as Usher's "A-Town Down" reference in his 2004 hit Yeah! (which also features Atlanta artists Lil Jon and Ludacris), Ludacris' "Welcome to Atlanta", Outkast's album "ATLiens", and B.o.B.'s multiple references to Decatur, such as in his hit song "Strange Clouds".
Film.
Films set in Georgia include two pictures both set in Atlanta that were awarded the Oscar for Best Picture:, "Gone with the Wind" (1939) and "Driving Miss Daisy" (1989). Other films set in Georgia include "Deliverance" (1972), which was based on the novel of the same name by James Dickey, and "Parental Guidance" (2012).
Sports.
Sports in Georgia include professional teams in all major sports, Olympic Games contenders and medalists, collegiate teams in major and small-school conferences and associations, and active amateur teams and individual sports. The state of Georgia has a team in three major professional leagues — Atlanta Braves of Major League Baseball, Atlanta Falcons of the National Football League, Atlanta Hawks of the National Basketball Association — and in 2017 is scheduled to land a fourth with an expansion franchise in Major League Soccer.
The Georgia Bulldogs and Georgia Tech Yellow Jackets are two major college football teams in the NCAA Division I FBS, where they have won multiple national championships.
The 1996 Summer Olympics took place in Atlanta. The stadium that was built to host various Olympic events was converted to Turner Field, the home of the Atlanta Braves.
The Masters golf tournament, the first of the PGA tour's four "majors", is held annually the second weekend of April at the Augusta National Golf Club.
The Atlanta Motor Speedway hosts the Dixie 500 NASCAR Cup Series stock car race and Road Atlanta the Petit Le Mans endurance sports car race.
Atlanta's Georgia Dome hosted Super Bowl XXVIII in 1994 and Super Bowl XXXIV in 2000. The Georgia Dome hosted the NCAA Final Four Men's Basketball National Championship in 2002, 2007, and 2013.
It hosted WWE's WrestleMania XXVII in 2011, an event which set an attendance record of 71,617. The dome is also the venue of the annual Chick-fil-A Bowl post-season college football games. Since 2004 the FIRST World Championships have been held there.
Professional baseball's Ty Cobb was the first player inducted into the Baseball Hall of Fame. He was from Narrows and was nicknamed "The Georgia Peach."
Parks and recreation.
There are 63 parks in Georgia, 48 of which are state parks and 15 that are historic sites, and numerous state wildlife preserves, under the supervision of the Georgia Department of Natural Resources. Other historic sites and parks are supervised by the National Park Service and include the Andersonville National Historic Site in Andersonville; Appalachian National Scenic Trail; Chattahoochee River National Recreation Area near Atlanta; Chickamauga and Chattanooga National Military Park at Fort Oglethorpe; Cumberland Island National Seashore near St. Marys; Fort Frederica National Monument on St. Simons Island; Fort Pulaski National Monument in Savannah; Jimmy Carter National Historic Site near Plains; Kennesaw Mountain National Battlefield Park near Kennesaw; Martin Luther King, Jr., National Historic Site in Atlanta; Ocmulgee National Monument at Macon; Trail of Tears National Historic Trail; and the Okefenokee Swamp in Waycross, Georgia
Outdoor recreational activities include hiking along the Appalachian Trail; Civil War Heritage Trails; rock climbing and whitewater paddling. Other outdoor activities include hunting and fishing.
Education.
Georgia county and city public school systems are administered by school boards with members elected at the local level. As of 2013, all but 19 of 181 boards are elected from single-member districts. Residents and activist groups in Fayette County, Georgia sued the board of commissioners and school board for maintaining an election system based on at-large voting, which tended to increase the power of the majority and effectively prevented minority participation on elected local boards for nearly 200 years. A change to single-member districts has resulted in the African-American minority being able to elect representatives of its choice.
Georgia high schools (grades nine through twelve) are required to administer a standardized, multiple choice End of Course Test, or EOCT, in each of eight core subjects including algebra, geometry, U.S. history, economics, biology, physical science, Ninth Grade Literature and composition, and American literature. The official purpose of the tests is to assess "specific content knowledge and skills." Although a minimum test score is not required for the student to receive credit in the course, completion of the test is mandatory. The EOCT score accounts for 15% of a student's grade in the course. The Criterion-Referenced Competency Tests (CRCT) is taken in 1st–8th grade.
High school students must also receive passing scores on four Georgia High School Graduation Tests (GHSGT) and the Georgia High School Writing Assessment in order to receive a diploma. Subjects assessed include Mathematics, Science, Language Arts, and Social Studies. These tests are initially offered during students' eleventh-grade year, allowing for multiple opportunities to pass the tests before graduation at the end of twelfth grade.
Georgia has almost 70 public colleges, universities, and technical colleges in addition to over 45 private institutes of higher learning. Among Georgia's public universities is the flagship research university, University of Georgia, the oldest public university in the United States. The University System of Georgia is the presiding body over public education in the state. The System includes 35 institutions of higher learning. The System is governed by the Georgia Board of Regents.
The HOPE Scholarship, funded by the state lottery, is available to all Georgia residents who have graduated from high school or earned a General Educational Development certificate. The student must maintain a 3.2 or higher grade point average and attend a public college or university in the state.
Media.
The Atlanta metropolitan area is the ninth largest media market in the United States as ranked by Nielsen Media Research. The state's other top markets are Savannah (95th largest), Augusta (115th largest), and Columbus (127th largest).
There are 48 television broadcast stations in Georgia including TBS, TNT, TCM, Cartoon Network, CNN and Headline News, all founded by Notable Georgia Resident Ted Turner.
By far, the largest daily newspaper in Georgia is the Atlanta Journal-Constitution with a daily readership of 195,592 and a Sunday readership of 397,925. Other large dailies include "The Augusta Chronicle", the "Columbus Ledger-Enquirer", "The Telegraph" (formerly "The Macon Telegraph") and the "Savannah Morning News".
WSB-AM in Atlanta was the first licensed radio station in the southeastern United States, signing on in 1922. Georgia Public Radio has been in service since 1984 and, with the exception of Atlanta, it broadcasts daily on several FM (and one AM) stations across the state. Georgia Public Radio reaches nearly all of Georgia (with the exception of the Atlanta area, which is served by WABE).
WSB-TV in Atlanta is the state's oldest television station, having begun operations in 1948. WSB was only the second such operation founded in the Southern U.S., trailing only a station in Richmond, Virginia.
Also the main headquarters of The Weather Channel is in Atlanta.
Infrastructure.
Transportation.
Transportation in Georgia is overseen by the Georgia Department of Transportation, a part of the executive branch of the state government. Georgia's major Interstate Highways are I-75 and I-85. On March 18, 1998, the Georgia House of Representatives passed a resolution naming the portion of Interstate Highway 75, which runs from the Chattahoochee River northward to the Tennessee state line the Larry McDonald Memorial Highway. Larry McDonald, a Democratic member of the House of Representatives, had been on Korean Air Lines Flight 007 when it was shot down by the Soviets on September 1, 1983.
Georgia's primary commercial airport is Hartsfield–Jackson Atlanta International Airport (ATL), and is the world's busiest passenger airport. In addition to Hartsfield-Jackson, there are eight other airports serving major commercial traffic in Georgia. Savannah/Hilton Head International Airport is the second-busiest airport in the state as measured by passengers served, and is the only additional international airport. Other commercial airports (ranked in order of passengers served) are located in Augusta, Columbus, Albany, Macon, Brunswick, Valdosta, and Athens.
The Georgia Ports Authority manages two deepwater seaports, at Savannah and Brunswick, and two river ports, at Bainbridge and Columbus. The Port of Savannah is a major U.S. seaport on the Atlantic coast.
The Metropolitan Atlanta Rapid Transit Authority (MARTA) is the principal rapid transit system in the Atlanta metropolitan area. Formed in 1971 as strictly a bus system, MARTA operates a network of bus routes linked to a rapid transit system consisting of 48 mi of rail track with 38 train stations. MARTA operates almost exclusively in Fulton and DeKalb counties, with bus service to two destinations in Cobb county and the Cumberland Transfer Center next to the Cumberland Mall, and a single rail station in Clayton County at Hartsfield-Jackson Atlanta International Airport. MARTA also operates a separate paratransit service for disabled customers. s of 2009[ [update]], the average total daily ridership for the system (bus and rail) was 482,500 passengers.
Health care.
The state has 151 general hospitals, over 15,000 doctors and almost 6,000 dentists. The state is ranked forty-first in the percentage of residents who engage in regular exercise.
Cities.
Atlanta, located in north-central Georgia at the Eastern Continental Divide, has been Georgia's capital city since 1868. It is the most populous city in Georgia, with just over 420,000 residents in 2010.
The Atlanta metropolitan area is the cultural and economic center of the Southeast, and its population in 2010 was 5,268,860, or 53.6% of Georgia's total. Atlanta is the nation's ninth largest metropolitan area.
The state has fourteen other cities with populations above 50,000 (based on 2012 census estimates). In descending order of size they are Columbus, Augusta, Macon, Savannah, Athens, Sandy Springs, Roswell, Albany, Johns Creek, Warner Robins, Alpharetta, Marietta, Valdosta and Smyrna.
Along with the rest of the Southeast, Georgia's population continues to grow rapidly, with primary gains concentrated in urban areas. The population of the Atlanta metropolitan area added 1.23 million people (24 percent) between 2000 and 2010, and Atlanta rose in rank from the eleventh largest metropolitan area in the United States to the ninth largest.
Notable people.
Jimmy Carter, from Plains, Georgia, was President of the United States from 1977 to 1981.

</doc>
<doc id="48833" url="http://en.wikipedia.org/wiki?curid=48833" title="Battle of Fredericksburg">
Battle of Fredericksburg

The Battle of Fredericksburg was fought December 11–15, 1862, in and around Fredericksburg, Virginia, between General Robert E. Lee's Confederate Army of Northern Virginia and the Union Army of the Potomac, commanded by Major General Ambrose Burnside. The Union Army's futile frontal attacks on December 13 against entrenched Confederate defenders on the heights behind the city is remembered as one of the most one-sided battles of the American Civil War, with Union casualties more than twice as heavy as those suffered by the Confederates.
Burnside's plan was to cross the Rappahannock River at Fredericksburg in mid-November and race to the Confederate capital of Richmond before Lee's army could stop him. Bureaucratic delays prevented Burnside from receiving the necessary pontoon bridges in time and Lee moved his army to block the crossings. When the Union army was finally able to build its bridges and cross under fire, urban combat in the city resulted on December 11–12. Union troops prepared to assault Confederate defensive positions south of the city and on a strongly fortified ridge just west of the city known as Marye's Heights.
On December 13, the "grand division" of Maj. Gen. William B. Franklin was able to pierce the first defensive line of Confederate Lieutenant General Stonewall Jackson to the south, but was finally repulsed. Burnside ordered the grand divisions of Maj. Gens. Edwin V. Sumner and Joseph Hooker to make multiple frontal assaults against Lt. Gen. James Longstreet's position on Marye's Heights, all of which were repulsed with heavy losses. On December 15, Burnside withdrew his army, ending another failed Union campaign in the Eastern Theater.
Background and Burnside's plan.
In November 1862, U.S. President Abraham Lincoln needed to demonstrate the success of the Union war effort before the Northern public lost confidence in his administration. Confederate armies had been on the move earlier in the fall, invading Kentucky and Maryland, and although each had been turned back, those armies remained intact and capable of further action. Lincoln urged Maj. Gen. Ulysses S. Grant to advance against the Confederate stronghold of Vicksburg, Mississippi. He replaced Maj. Gen. Don Carlos Buell with Maj. Gen. William S. Rosecrans, hoping for a more aggressive posture against the Confederates in Tennessee, and on November 5, seeing that his replacement of Buell had not stimulated Maj. Gen. George B. McClellan into action, he issued orders to replace McClellan in command of the Army of the Potomac in Virginia. McClellan had stopped Robert E. Lee at the Battle of Antietam in Maryland, but had not been able to destroy Lee's army, nor did he pursue Lee back into Virginia aggressively enough for Lincoln.
McClellan's replacement was Maj. Gen. Ambrose E. Burnside, the commander of the IX Corps. Burnside had established a reputation as an independent commander, with successful operations earlier that year in coastal North Carolina and, unlike McClellan, had no apparent political ambitions. However, he felt himself unqualified for army-level command and objected when offered the position. He accepted only when it was made clear to him that McClellan would be replaced in any event and that an alternative choice for command was Maj. Gen. Joseph Hooker, whom Burnside disliked and distrusted. Burnside assumed command on November 7.
In response to prodding from Lincoln and general-in-chief Maj. Gen. Henry W. Halleck, Burnside planned a late fall offensive; he communicated his plan to Halleck on November 9. The plan relied on quick movement and deception. He would concentrate his army in a visible fashion near Warrenton, feigning a movement on Culpeper Court House, Orange Court House, or Gordonsville. Then he would rapidly shift his army southeast and cross the Rappahannock River to Fredericksburg, hoping that Robert E. Lee would sit still, unclear as to Burnside's intentions, while the Union Army made a rapid movement against Richmond, south along the Richmond, Fredericksburg and Potomac Railroad from Fredericksburg. Burnside selected this plan because he was concerned that if he were to move directly south from Warrenton, he would be exposed to a flanking attack from Lt. Gen. Thomas J. "Stonewall" Jackson, whose corps was at that time in the Shenandoah Valley south of Winchester. He also believed that the Orange and Alexandria Railroad would be an inadequate supply line. (Burnside was also influenced by plans McClellan began developing just prior to being relieved. Aware that Lee had blocked the O&A, McClellan considered a route through Fredericksburg and ordered a small group of cavalrymen commanded by Capt. Ulric Dahlgren to investigate the condition of the RF&P.) While Burnside began assembling a supply base at Falmouth, near Fredericksburg, the Lincoln administration entertained a lengthy debate about the wisdom of his plan, which differed from the president's preference of a movement south on the O&A and a direct confrontation with Lee's army instead of the movement focused on the city of Richmond. Lincoln reluctantly approved the plan on November 14 but cautioned his general to move with great speed, certainly doubting that Lee would react as Burnside anticipated.
Opposing forces.
Burnside organized his Army of the Potomac into three so-called "grand divisions", organizations that included infantry corps, cavalry, and artillery, comprising 120,000 men, of whom 114,000 would be engaged in the coming battle:
Robert E. Lee's Army of Northern Virginia had nearly 85,000 men, with 72,500 engaged. His organization of the army in corps was approved by an act of the Confederate Congress on November 6, 1862.
The two armies at Fredericksburg represented the largest number of armed men that ever confronted each other for combat during the Civil War.
Movement to battle.
The Union Army began marching on November 15, and the first elements arrived in Falmouth on November 17. Burnside's plan quickly went awry—he had ordered pontoon bridges to be sent to the front and assembled for his quick crossing of the Rappahannock, but because of administrative bungling, the bridges did not arrive on time. Burnside first requisitioned the pontoon bridging (along with many other provisions) on November 7 when he detailed his plan to Halleck. The plan was sent to the attention of Brig. Gen. George Washington Cullum, the chief of staff in Washington (received on November 9). Plans called for both riverine and overland movement of the pontoon trains to Falmouth. On November 14, the 50th New York Engineers reported the pontoons were ready to move, except for a lack of the 270 horses needed to move them. Unknown to Burnside, most of the bridging was still on the upper Potomac. Communications between Burnside's staff engineer Cyrus B. Comstock and the Engineer Brigade commander Daniel P. Woodbury indicate that Burnside had assumed the bridging was en route to Washington based on orders given on November 6.
As Maj. Gen. Edwin V. Sumner arrived, he strongly urged an immediate crossing of the river to scatter the token Confederate force of 500 men in the town and occupying the commanding heights to the west. Burnside became anxious, concerned that the increasing autumn rains would make the fording points unusable and that Sumner might be cut off and destroyed, ordering Sumner to wait in Falmouth.
Lee at first anticipated that Burnside would beat him across the Rappahannock and that to protect Richmond, he would assume the next defensible position to the south, the North Anna River. But when he saw how slowly Burnside was moving (and Confederate President Jefferson Davis expressed reservations about planning for a battle so close to Richmond), he directed all of his army toward Fredericksburg. By November 23, all of Longstreet's corps had arrived and Lee placed them on the ridge known as Marye's Heights to the west of town, with Anderson's division on the far left, McLaws's directly behind the town, and Pickett's and Hood's to the right. He sent for Jackson on November 26, but his Second Corps commander had anticipated the need and began forced-marching his troops from Winchester on November 22, covering as many as 20 miles a day. Jackson arrived at Lee's headquarters on November 29 and his divisions were deployed to prevent Burnside crossing downstream from Fredericksburg: D.H. Hill's division moved to Port Royal, 18 miles down river; Early's 12 miles down river at Skinker's Neck; A.P. Hill's at Thomas Yerby's house, "Belvoir", about 6 miles southeast of town; and Taliaferro's along the RF&P Railroad, 4 miles south at Guinea Station.
The boats and equipment for a single pontoon bridge arrived at Falmouth on November 25, much too late to enable the Army of the Potomac to cross the river without opposition. Burnside still had an opportunity, however, because by then he was facing only half of Lee's army, not yet dug in, and if he acted quickly, he might have been able to attack Longstreet and defeat him before Jackson arrived. Once again he squandered his opportunity. The full complement of bridges arrived at the end of the month, but by this time Jackson was present and Longstreet was preparing strong defenses.
Burnside originally planned to cross his army east of Fredericksburg at Skinker's Neck, but an advance movement by Federal gunboats to there was fired upon and drew Early's and D.H. Hill's divisions into that area, a movement spotted by Union balloon observers. Now assuming that Lee had anticipated his plan, Burnside guessed that the Confederates had weakened their left and center to concentrate against him on their right. So he decided to cross directly at Fredericksburg. On December 9, he wrote to Halleck, "I think now the enemy will be more surprised by a crossing immediately in our front than any other part of the river. ... I'm convinced that a large force of the enemy is now concentrated at Port Royal, its left resting on Fredericksburg, which we hope to turn." In addition to his numerical advantage in troop strength, Burnside also had the advantage of knowing his army could not be attacked effectively. On the other side of the Rappahannock, 220 artillery pieces had been located on the ridge known as Stafford Heights to prevent Lee's army from mounting any major counterattacks.
Battle.
Crossing the Rappahannock, December 11–12.
Union engineers began to assemble six pontoon bridges before dawn on December 11, two just north of the town center, a third on the southern end of town, and three farther south, near the confluence of the Rappahannock and Deep Run. The engineers constructing the bridge directly across from city came under punishing fire from Confederate sharpshooters, primarily from the Mississippi brigade of Brig. Gen. William Barksdale, in command of the town defenses. Union artillery attempted to dislodge the sharpshooters, but their positions in the cellars of houses rendered the fire from 150 guns mostly ineffective. Eventually Burnside's artillery commander, Brig. Gen. Henry J. Hunt, convinced him to send infantry landing parties over in the pontoon boats to secure a small bridgehead and rout the sharpshooters. Col. Norman J. Hall volunteered his brigade for this assignment. Burnside suddenly turned reluctant, lamenting to Hall in front of his men that "the effort meant death to most of those who should undertake the voyage." When his men responded to Hall's request with three cheers, Burnside relented. At 3 p.m., the Union artillery began a preparatory bombardment and 135 infantrymen from the 7th Michigan and the 19th Massachusetts crowded into the small boats, and the 20th Massachusetts followed soon after. They crossed successfully and spread out in a skirmish line to clear the sharpshooters. Although some of the Confederates surrendered, fighting proceeded street by street through the town as the engineers completed the bridges. Sumner's Right Grand Division began crossing at 4:30 p.m., but the bulk of his men did not cross until December 12. Hooker's Center Grand Division crossed on December 13, using both the northern and southern bridges.
The clearing of the city buildings by Sumner's infantry and by artillery fire from across the river began the first major urban combat of the war. Union gunners sent more than 5,000 shells against the town and the ridges to the west. By nightfall, four brigades of Union troops occupied the town, which they looted with a fury that had not been seen in the war up to that point. This behavior enraged Lee, who compared their depredations with those of the ancient Vandals. The destruction also angered the Confederate troops, many of whom were native Virginians. Many on the Union side were also shocked by the destruction inflicted on Fredericksburg. Civilian casualties were unusually sparse in the midst of such widespread violence; George Rable estimates no more than four civilian deaths.
River crossings south of the city by Franklin's Left Grand Division were much less eventful. Both bridges were completed by 11 a.m. on December 11 while five batteries of Union artillery suppressed most sniper fire against the engineers. Franklin was ordered at 4 p.m. to cross his entire command, but only a single brigade was sent out before dark. Crossings resumed at dawn and were completed by 1 p.m. on December 12. Early on December 13, Jackson recalled his divisions under Jubal Early and D.H. Hill from down river positions to join his main defensive lines south of the city.
Burnside's verbal instructions on December 12 outlined a main attack by Franklin, supported by Hooker, on the southern flank, while Sumner made a secondary attack on the northern. His actual orders on December 13 were vague and confusing to his subordinates. At 5 p.m. on December 12, he made a cursory inspection of the southern flank, where Franklin and his subordinates pressed him to give definite orders for a morning attack by the grand division, so they would have adequate time to position their forces overnight. However, Burnside demurred and the order did not reach Franklin until 7:15 or 7:45 a.m. When it arrived, it was not as Franklin expected. Rather than ordering an attack by the entire grand division of almost 60,000 men, Franklin was to keep his men in position, but was to send "a division at least" to seize the high ground (Prospect Hill) around Hamilton's Crossing, Sumner was to send one division through the city and up Telegraph Road, and both flanks were to be prepared to commit their entire commands. Burnside was apparently expecting these weak attacks to intimidate Lee, causing him to withdraw. Franklin, who had originally advocated a vigorous assault, chose to interpret Burnside's order very conservatively. Brig. Gen. James A. Hardie, who delivered the order, did not ensure that Burnside's intentions were understood by Franklin, and map inaccuracies about the road network made those intentions unclear. Furthermore, Burnside's choice of the verb "to seize" was less forceful in 19th century military terminology than an order "to carry" the heights.
South of the city, December 13.
December 13 began cold and overcast. A dense fog blanketed the ground and made it impossible for the armies to see each other. Franklin ordered his I Corps commander, Maj. Gen. John F. Reynolds, to select a division for the attack. Reynolds chose his smallest division, about 4,500 men commanded by Maj. Gen. George G. Meade, and assigned Brig. Gen. John Gibbon's division to support Meade's attack. His reserve division, under Maj. Gen. Abner Doubleday, was to face south and protect the left flank between the Richmond Road and the river. Meade's division began moving out at 8:30 a.m., with Gibbon following behind. At around 10:30, the fog started lifting. They moved parallel to the river initially, turning right to face the Richmond Road, where they began to be struck by enfilading fire from the Virginia Horse Artillery under Major John Pelham. Pelham started with two cannons—a 12-pounder Napoleon smoothbore and a rifled Blakely—but continued with only one after the latter was disabled by counter-battery fire. "Jeb" Stuart sent word to Pelham that he should feel free to withdraw from his dangerous position at any time, to which Pelham responded, "Tell the General I can hold my ground." The Iron Brigade (formerly Gibbon's command, but now led by Brig. Gen. Solomon Meredith) was sent out to deal with the Confederate horse artillery. This action was mainly conducted by the 24th Michigan Infantry, a newly enlisted regiment that had joined the brigade in October. After about an hour, Pelham's ammunition began to run low and he withdrew. General Lee observed the action and commented about Pelham, age 24, "It is glorious to see such courage in one so young." The most prominent victim of Pelham's fire was Brig. Gen. George D. Bayard, a cavalry general mortally wounded by a shell while standing in reserve near Franklin's headquarters. Jackson's main artillery batteries had remained silent in the fog during this exchange, but the Union troops soon began to receive direct fire from Prospect Hill, principally five batteries directed by Lt. Col. Reuben Lindsay Walker, and Meade's attack was stalled about 600 yards from his initial objective for almost two hours by these combined artillery attacks.
The Union artillery fire was lifted as Meade's men moved forward around 1 p.m. Jackson's force of about 35,000 remained concealed on the wooded ridge to Meade's front. His formidable defensive line had an unforeseen flaw. In A.P. Hill's division's line, a triangular patch of the woods that extended beyond the railroad was swampy and covered with thick underbrush and the Confederates had left a 600-yard gap there between the brigades of Brig. Gens. James H. Lane and James J. Archer. Brig. Gen. Maxcy Gregg's brigade stood about a quarter mile behind the gap. Meade's 1st Brigade (Col. William Sinclair) entered the gap, climbed the railroad embankment, and turned right into the underbrush, striking Lane's brigade in the flank. Following immediately behind, his 3rd Brigade (Brig. Gen. Feger Jackson) turned left and hit Archer's flank. The 2nd Brigade (Col. Albert L. Magilton) came up in support and intermixed with the leading brigades. As the gap widened with pressure on the flanks, thousands of Meade's men reached the top of the ridge and ran into Gregg's brigade. Many of these Confederates had stacked arms while taking cover from Union artillery and were not expecting to be attacked at that moment, so were killed or captured unarmed. Gregg at first mistook the Union soldiers for fleeing Confederate troops and ordered his men not to fire on them. While he rode prominently in front of his lines, the partially deaf Gregg could not hear the approaching Federals or their bullets flying around him. He was shot through the spinal cord, dying two days later.
Confederate reserves—the divisions of Brig. Gens. Jubal A. Early and William B. Taliaferro—moved into the fray from behind Gregg's original position. Inspired by their attack, regiments from Lane's and Archer's brigades rallied and formed a new defensive line in the gap. Now Meade's men were receiving fire from three sides and could not withstand the pressure. Feger Jackson attempted to flank a Confederate battery, but after his horse was shot and he began to lead on foot, he was shot in the head by a volley and his brigade fell back, leaderless (Col. Joseph W. Fisher soon replaced Jackson in command).
To Meade's right, Gibbon's division prepared to move forward at 1 p.m. Brig. Gen. Nelson Taylor proposed to Gibbon that they supplement Meade's assault with a bayonet charge against Lane's position. However, Gibbon stated that this would violate his orders, so Taylor's brigade did not move forward until 1:30 p.m. The attack did not have the benefit of a gap to exploit, nor did the Union soldiers have any wooded cover for their advance, so progress was slow under heavy fire from Lane's brigade and Confederate artillery. Immediately following Taylor was the brigade of Col. Peter Lyle, and the advance of the two brigades ground to a halt before they reached the railroad. Committing his reserve at 1:45 p.m., Gibbon sent forward his brigade under Col. Adrian R. Root, which moved through the survivors of the first two brigades, but they were soon brought to a halt as well. Eventually some of the Federals reached the crest of the ridge and had some success during hand-to-hand fighting—men on both sides had depleted their ammunition and resorted to bayonets and rifle butts, and even empty rifles with bayonets thrown like javelins—but they were forced to withdraw back across the railroad embankment along with Meade's men to their left. Gibbon's attack, despite heavy casualties, had failed to support Meade's temporary breakthrough.
My God, General Reynolds, did they think my division could whip Lee's whole army?
Maj. Gen. George G. Meade to Maj. Gen. John F. Reynolds, afternoon of December 13
After the battle Meade complained that some of Gibbon's officers had not charged quickly enough. But his primary frustration was with Brig. Gen. David B. Birney, whose division of the III Corps had been designated to support the attack as well. Birney claimed that his men had been subjected to damaging artillery fire as they formed up, that he had not understood the importance of Meade's attack, and that Reynolds had not ordered his division forward. When Meade galloped to the rear to confront Birney with a string of fierce profanities that, in the words of one staff lieutenant, "almost makes the stones creep," he was finally able to order the brigadier forward under his own responsibility, but harbored resentment for weeks. By this time, however, it was too late to accomplish any further offensive action.
Early's division began a counterattack, led initially by Col. Edmund N. Atkinson's Georgia brigade, which inspired the men from the brigades of Col. Robert Hoke, Brig. Gen. James J. Archer, and Col. John M. Brockenbrough to charge forward out of the railroad ditches, driving Meade's men from the woods in a disorderly retreat, followed closely by Gibbon's. Early's orders to his brigades were to pursue as far as the railroad, but in the chaos many kept up the pressure over the open fields as far as the old Richmond Road. Union artillery crews proceeded to unleash a blast of close-range canister shot, firing as fast as they could load their guns. The Confederates were also struck by the leading brigade of Birney's belated advance, commanded by Brig. Gen. J. H. Hobart Ward. Birney followed up with the brigades of Brig. Gens. Hiram G. Berry and John C. Robinson, which broke the Rebel advance that had threatened to drive the Union into the river. Any further Confederate advance was deterred by the arrival of the III Corps division of Brig. Gen. Daniel E. Sickles on the right. General Burnside, who by this time was focused on his attacks on Marye's Heights, was dismayed that his left flank attack had not achieved the success he assumed earlier in the day. He ordered Franklin to "advance his right and front," but despite repeated entreaties, Franklin refused, claiming that all of his forces had been engaged. This was not true, however, as the entire VI Corps and Brig. Gen. Abner Doubleday's division of the I Corps had been mostly idle, suffering only a few casualties from artillery fire while they waited in reserve.
It is well that war is so terrible, or we should grow too fond of it.
 Gen. Robert E. Lee, watching the carnage of the Confederate counterattack from the center of his line, a position now known as Lee's Hill
The Confederates withdrew back to the safety of the hills south of town. Stonewall Jackson considered mounting a resumed counterattack, but the Federal artillery and impending darkness changed his mind. A fortuitous Union breakthrough had been wasted because Franklin did not reinforce Meade's success with some of the 20,000 men standing in reserve. Neither Franklin nor Reynolds took any personal involvement in the battle, and were unavailable to their subordinates at the critical point. Franklin's losses were about 5,000 casualties in comparison to Stonewall Jackson's 3,400, demonstrating the ferocity of the fighting. Skirmishing and artillery duels continued until dark, but no additional major attacks took place, while the center of the battle moved north to Marye's Heights.
Marye's Heights, December 13.
On the northern end of the battlefield, Brig. Gen. William H. French's division of the II Corps prepared to move forward, subjected to Confederate artillery fire that was descending on the fog-covered city of Fredericksburg. General Burnside's orders to Maj. Gen. Edwin V. Sumner, commander of the Right Grand Division, was to send "a division or more" to seize the high ground to the west of the city, assuming that his assault on the southern end of the Confederate line would be the decisive action of the battle. The avenue of approach was difficult—mostly open fields, but interrupted by scattered houses, fences, and gardens that would restrict the movement of battle lines. A canal stood about 200 yards west of the town, crossed by three narrow bridges, which would require the Union troops to funnel themselves into columns before proceeding. About 600 yards to the west of Fredericksburg was the low ridge known as Marye's Heights, rising 40–50 feet above the plain. (Although popularly known as Marye's Heights, the ridge was composed of several hills separated by ravines, from north to south: Taylor's Hill, Stansbury Hill, Marye's Hill, and Willis Hill.) Near the crest of the portion of the ridge comprising Marye's Hill and Willis Hill, a narrow lane in a slight cut—the Telegraph Road, known after the battle as the Sunken Road—was protected by a 4-foot stone wall, enhanced in places with log breastworks and abatis, making it a perfect infantry defensive position. Confederate Maj. Gen. Lafayette McLaws initially had about 2,000 men on the front line of Marye's Heights and there were an additional 7,000 men in reserve on the crest and behind the ridge. Massed artillery provided almost uninterrupted coverage of the plain below. General Longstreet had been assured by his artillery commander, Lt. Col. Edward Porter Alexander, "General, we cover that ground now so well that we will comb it as with a fine-tooth comb. A chicken could not live on that field when we open on it."
The fog lifted from the town around 10 a.m. and Sumner gave his order to advance an hour later. French's brigade under Brig. Gen. Nathan Kimball began to move around noon. They advanced slowly through heavy artillery fire, crossed the canal in columns over the narrow bridges, and formed in line, with fixed bayonets, behind the protection of a shallow bluff. In perfect line of battle, they advanced up the muddy slope until they were cut down at about 125 yards from the stone wall by repeated rifle volleys. Some soldiers were able to get as close as 40 yards, but having suffered severe casualties from both the artillery and infantry fire, the survivors clung to the ground. Kimball was severely wounded during the assault, and his brigade suffered 25% casualties. French's brigades under Col. John W. Andrews and Col. Oliver H. Palmer followed, with casualty rates of almost 50%.
Sumner's original order called for the division of Brig. Gen. Winfield S. Hancock to support French and Hancock sent forward his brigade under Col. Samuel K. Zook behind Palmer's. They met a similar fate. Next was his Irish Brigade under Brig. Gen. Thomas F. Meagher. By coincidence, they attacked the area defended by fellow Irishmen of Col. Robert McMillan's 24th Georgia Infantry. One Confederate who spotted the green regimental flags approaching cried out, "Oh God, what a pity! Here comes Meagher's fellows." But McMillan exhorted his troops: "Give it to them now, boys! Now's the time! Give it to them!" Hancock's final brigade was led by Brig. Gen. John C. Caldwell. Leading his two regiments on the left, Col. Nelson A. Miles suggested to Caldwell that the practice of marching in formation, firing, and stopping to reload, made the Union soldiers easy targets, and that a concerted bayonet charge might be effective in carrying the works. Caldwell denied permission. Miles was struck by a bullet in the throat as he led his men to within 40 yards of the wall, where they were pinned down as their predecessors had been. Caldwell himself was soon struck by two bullets and put out of action.
The commander of the II Corps, Maj. Gen. Darius N. Couch, was dismayed at the carnage wrought upon his two divisions in the hour of fighting and, like Col. Miles, realized that the tactics were not working. He first considered a massive bayonet charge to overwhelm the defenders, but as he surveyed the front, he quickly realized that French's and Hancock's divisions were in no shape to move forward again. He next planned for his final division, commanded by Maj. Gen. Oliver O. Howard, to swing to the right and attempt to envelop the Confederate left, but upon receiving urgent requests for help from French and Hancock, he sent Howard's men over and around the fallen troops instead. The brigade of Col. Joshua Owen went in first, reinforced by Col. Norman J. Hall's brigade, and then two regiments of Brig. Gen. Alfred Sully's brigade. The other corps in Sumner's grand division was the IX Corps, and he sent in one of its divisions under Brig. Gen. Samuel Sturgis. After two hours of desperate fighting, four Union divisions had failed in the mission Burnside had originally assigned to one. Casualties were heavy: II Corps losses for the afternoon were 4,114, Sturgis's division 1,011.
While the Union Army paused, Longstreet reinforced his line so that there were four ranks of infantrymen behind the stone wall. Brig. Gen. Thomas R. R. Cobb of Georgia, who had commanded the key sector of the line, was mortally wounded by an exploding artillery shell and was replaced by Brig. Gen. Joseph B. Kershaw. General Lee expressed concerns to Longstreet about the massing troops breaking his line, but Longstreet assured his commander, "General, if you put every man on the other side of the Potomac on that field to approach me over the same line, and give me plenty of ammunition, I will kill them all before they reach my line."
By midafternoon, Burnside had failed on both flanks to make progress against the Confederates. Rather than reconsidering his approach in the face of heavy casualties, he stubbornly decided to continue on the same path. He sent orders to Franklin to renew the assault on the left (which, as described earlier, the Left Grand Division commander ignored) and ordered his Center Grand Division, commanded by Maj. Gen. Joseph Hooker, to cross the Rappahannock into Fredericksburg and continue the attack on Marye's Heights. Hooker performed a personal reconnaissance (something that neither Burnside nor Sumner had done, both remaining east of the river during the failed assaults) and returned to Burnside's headquarters to advise against the attack.
Brig. Gen. Daniel Butterfield, commanding Hooker's V Corps, while waiting for Hooker to return from his conference with Burnside, sent his division under Brig. Gen. Charles Griffin to relieve Sturgis's men. By this time, Maj. Gen. George Pickett's Confederate division and one of Maj. Gen. John Bell Hood's brigades had marched north to reinforce Marye's Heights. Griffin smashed his three brigades against the Confederate position, one by one. Also concerned about Sturgis, Couch sent the six guns of Capt. John G. Hazard's Battery B, 1st Rhode Island Light Artillery, to within 150 yards of the Confederate line. They were hit hard by Confederate sharpshooter and artillery fire and provided no effective relief to Sturgis.
A soldier in Hancock's division reported movement in the Confederate line that led some to believe that the enemy might be retreating. Despite the unlikeliness of this supposition, the V Corps division of Brig. Gen. Andrew A. Humphreys was ordered to attack and capitalize on the situation. Humphreys led his first brigade on horseback, with his men moving over and around fallen troops with fixed bayonets and unloaded rifles; some of the fallen men clutched at the passing pant legs, urging their comrades not to go forward, causing the brigade to become disorganized in their advance. The charge reached to within 50 yards before being cut down by concentrated rifle fire. Brig. Gen. George Sykes was ordered to move forward with his V Corps regular army division to support Humphreys's retreat, but his men were caught in a crossfire and pinned down.
By 4 p.m., Hooker had returned from his meeting with Burnside, having failed to convince the commanding general to abandon the attacks. While Humphreys was still attacking, Hooker reluctantly ordered the IX Corps division of Brig. Gen. George W. Getty to attack as well, but this time to the leftmost portion of Marye's Heights, Willis Hill. Col. Rush Hawkins's brigade, followed by Col. Edward Harland's brigade, moved along an unfinished railroad line just north of Hazel Run, approaching close to the Confederate line without detection in the gathering twilight, but they were eventually detected, fired on, and repulsed.
Seven Union divisions had been sent in, generally one brigade at a time, for a total of fourteen individual charges, all of which failed, costing them from 6,000 to 8,000 casualties. Confederate losses at Marye's Heights totaled around 1,200. The falling of darkness and the pleas of Burnside's subordinates were enough to put an end to the attacks. Longstreet later wrote, "The charges had been desperate and bloody, but utterly hopeless." Thousands of Union soldiers spent the cold December night on the fields leading to the heights, unable to move or assist the wounded because of Confederate fire. That night, Burnside attempted to blame his subordinates for the disastrous attacks, but they argued that it was entirely his fault and no one else's.
Lull and withdrawal, December 14–15.
During a dinner meeting the evening of December 13, Burnside dramatically announced that he would personally lead his old IX Corps in one final attack on Marye's Heights, but his generals talked him out of it the following morning. The armies remained in position throughout the day on December 14. That afternoon, Burnside asked Lee for a truce to attend to his wounded, which the latter graciously granted. The next day the Federal forces retreated across the river, and the campaign came to an end.
Testament to the extent of the carnage and suffering during the battle was the story of Richard Rowland Kirkland, a Confederate Army sergeant with Company G, 2nd South Carolina Volunteer Infantry. Stationed at the stone wall by the sunken road below Marye's Heights, Kirkland had a close up view to the suffering and like so many others was appalled at the cries for help of the Union wounded throughout the cold winter night of December 13, 1862. After obtaining permission from his commander, Brig. Gen. Joseph B. Kershaw, Kirkland gathered canteens and in broad daylight, without the benefit of a cease fire or a flag of truce (refused by Kershaw), provided water to numerous Union wounded lying on the field of battle. Union soldiers held their fire as it was obvious what Kirkland's intent was. Kirkland was nicknamed the "Angel of Marye's Heights" for these actions, and is memorialized with a statue by Felix de Weldon on the Fredericksburg and Spotsylvania National Military Park where he carried out his actions. Details of this story (first recorded in 1880) conflict with multiple after-action reports and may have been embellished and personalized for effect.
On the night of December 14, the Aurora Borealis made an appearance unusual for that latitude, presumably caused by a large solar flare. One witness described that "the wonderful spectacle of the Aurora Borealis was seen in the Gulf States. The whole sky was a ruddy glow as if from an enormous conflagration, but marked by the darting rays peculiar to the Northern light." The event was noted in the diaries and letters of many soldiers at Fredericksburg, such as John W. Thompson, Jr., who wrote "Louisiana sent those famous cosmopolitan Zouaves called the Louisiana Tigers, and there were Florida troops who, undismayed in fire, stampeded the night after Fredericksburg, when the Aurora Borealis snapped and crackled over that field of the frozen dead hard by the Rappahannock ..."
Aftermath.
The Union army suffered 12,653 casualties (1,284 killed, 9,600 wounded, 1,769 captured/missing). Two Union generals were mortally wounded: Brig. Gens. George D. Bayard and Conrad F. Jackson. The Confederate army lost 5,377 (608 killed, 4,116 wounded, 653 captured/missing), most of them in the early fighting on Jackson's front. Confederate Brig. Gens. Maxcy Gregg and T. R. R. Cobb were both mortally wounded. The casualties sustained by each army showed clearly how disastrous the Union army's tactics were. Although the fighting on the southern flank produced roughly equal casualties (about 4,000 Confederate, 5,000 Union), the northern flank was completely lopsided, with about eight Union casualties for each Confederate. Burnside's men had suffered considerably more in the attack originally meant as a diversion than in his main effort.
The South erupted in jubilation over their great victory. The Richmond "Examiner" described it as a "stunning defeat to the invader, a splendid victory to the defender of the sacred soil." General Lee, normally reserved, was described by the Charleston "Mercury" as "jubilant, almost off-balance, and seemingly desirous of embracing everyone who calls on him." The newspaper also exclaimed that, "General Lee knows his business and the army has yet known no such word as fail."
Reactions were opposite in the North, and both the Army and President Lincoln came under strong attacks from politicians and the press. The Cincinnati "Commercial" wrote, "It can hardly be in human nature for men to show more valor or generals to manifest less judgment, than were perceptible on our side that day." Senator Zachariah Chandler, a Radical Republican, wrote that, "The President is a weak man, too weak for the occasion, and those fool or traitor generals are wasting time and yet more precious blood in indecisive battles and delays." Pennsylvania Governor Andrew Curtin visited the White House after a trip to the battlefield. He told the president, "It was not a battle, it was a butchery." Curtin reported that the president was "heart-broken at the recital, and soon reached a state of nervous excitement bordering on insanity." Lincoln himself wrote, "If there is a worse place than hell, I am in it." Burnside was relieved of command a month later, following an unsuccessful attempt to purge some of his subordinates from the Army and the humiliating failure of his "Mud March" in January.
Preservation efforts.
In March 2003, the Civil War Trust announced the beginning of a $12 million national campaign to preserve the historic Slaughter Pen Farm, a key part of the Fredericksburg battlefield. The 205 acre farm, known locally as the Pierson Tract, was the scene of bloody struggle on December 13, 1862. Over this ground Federal troops under Maj. Gen. George Meade and Brig. Gen. John Gibbon launched their assault against Lt. Gen. Thomas "Stonewall" Jackson's Confederates holding the southern portion of the Army of Northern Virginia's line at Fredericksburg. Despite suffering enormous casualties the Federal troops under Meade were able to temporarily penetrate the Confederate line and for a time represented the North's best chance of winning the Battle of Fredericksburg. The fighting on this southern portion of the battlefield, later named the Slaughter Pen, produced 5,000 casualties and five Medal of Honor recipients.
The Slaughter Pen Farm was considered to be the largest remaining unprotected part of the Fredericksburg battlefield. It is also the only place on the battlefield where a visitor can still follow the Union assault of December 13 from beginning to end. Nearly all the other land associated with Union attacks at Fredericksburg—either on the southern end of the battlefield or in front of Marye's Heights—has been degraded by development. The $12 million acquisition of the Slaughter Pen Farm at the Fredericksburg battlefield has been called the most ambitious nonprofit battlefield acquisition in American history.
In October 2006, the Department of the Interior awarded a $2 million grant based on the significance of the Slaughter Pen Farm. The money was provided through a U.S. Congressional appropriation from the Land and Water Conservation Fund. The fund supports non-federal efforts to acquire and preserve meaningful American Civil War battlefield lands. The program is administered by the American Battlefield Protection Program, an arm of the National Park Service. In addition, the Central Virginia Battlefields Trust (CVBT) committed $1 million toward the Slaughter Pen Farm fundraising campaign.
In November 2012, during archaeological investigations at the construction site for a new courthouse, remains of Union artifacts were recovered. These included ammunition, smoking pipes, and food tins.
In popular media.
The Battle of Fredericksburg was depicted in the 2003 film "Gods and Generals", based on the novel of the same name, a prequel of "The Killer Angels" from which the earlier film "Gettysburg" was adapted. Both the novel and film focused primarily on the disastrous charges on Marye's Heights, with the movie highlighting the charges of Hancock's division of II Corps, the Irish Brigade, Caldwell's brigade, and Zook's brigade, and the 20th Maine Infantry Regiment (V Corps).
American author Louisa May Alcott fictionalized her experience nursing soldiers injured in the Battle of Fredericksburg in her book "Hospital Sketches" (1863).

</doc>
