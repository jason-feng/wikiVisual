<doc id="45474" url="http://en.wikipedia.org/wiki?curid=45474" title="Grand Teton National Park">
Grand Teton National Park

Grand Teton National Park is a United States National Park in northwestern Wyoming. At approximately 310000 acre, the park includes the major peaks of the 40 mi Teton Range as well as most of the northern sections of the valley known as Jackson Hole. It is only 10 mi south of Yellowstone National Park, to which it is connected by the National Park Service-managed John D. Rockefeller, Jr. Memorial Parkway. Along with surrounding National Forests, these three protected areas constitute the almost 18000000 acre Greater Yellowstone Ecosystem, one of the largest intact mid-latitude temperate ecosystems in the world.
Human history of the Grand Teton region dates back at least 11,000 years, when the first nomadic hunter-gatherer Paleo-Indians began migrating into the region during warmer months pursuing food and supplies. In the early 19th century, the first White explorers encountered the eastern Shoshone natives. Between 1810 and 1840, the region attracted fur trading companies that vied for control of the lucrative beaver pelt trade. U.S. Government expeditions to the region commenced in the mid-19th century as an offshoot of exploration in Yellowstone, with the first permanent white settlers in Jackson Hole arriving in the 1880s. Efforts to preserve the region as a national park commenced in the late 19th century, and in 1929 Grand Teton National Park was established, protecting the major peaks of the Teton Range. The valley of Jackson Hole remained in private ownership until the 1930s, when conservationists led by John D. Rockefeller, Jr. began purchasing land in Jackson Hole to be added to the existing national park. Against public opinion and with repeated Congressional efforts to repeal the measures, much of Jackson Hole was set aside for protection as Jackson Hole National Monument in 1943. The monument was abolished in 1950 and most of the monument land was added to Grand Teton National Park.
Grand Teton National Park is named for Grand Teton, the tallest mountain in the Teton Range. The naming of the mountains is attributed to early 19th-century French-speaking trappers—"les trois tétons" (the three teats) was later anglicized and shortened to "Tetons". At 13775 ft, Grand Teton abruptly rises more than 7000 ft above Jackson Hole, almost 850 ft higher than Mount Owen, the second-highest summit in the range. The park has numerous lakes, including 15 mi Jackson Lake as well as streams of varying length and the upper main stem of the Snake River. Though in a state of recession, a dozen small glaciers persist at the higher elevations near the highest peaks in the range. Some of the rocks in the park are the oldest found in any U.S. National Park and have been dated at nearly 2.7 billion years.
Grand Teton National Park is an almost pristine ecosystem and the same species of flora and fauna that have existed since prehistoric times can still be found there. More than 1,000 species of vascular plants, dozens of species of mammals, 300 species of birds, more than a dozen fish species and a few species of reptiles and amphibians exist. Due to various changes in the ecosystem, some of them human-induced, efforts have been made to provide enhanced protection to some species of native fish and the increasingly threatened whitebark pine.
Grand Teton National Park is a popular destination for mountaineering, hiking, fishing and other forms of recreation. There are more than 1,000 drive-in campsites and over 200 mi of hiking trails that provide access to backcountry camping areas. Noted for world-renowned trout fishing, the park is one of the few places to catch Snake River fine-spotted cutthroat trout. Grand Teton has several National Park Service-run visitor centers, and privately operated concessions for motels, lodges, gas stations and marinas.
Human history.
Paleo-Indians and Native Americans.
Paleo-Indian presence in what is now Grand Teton National Park dates back more than 11,000 years. Jackson Hole valley climate at that time was colder and more alpine than the semi-arid climate found today, and the first humans were migratory hunter-gatherers spending summer months in Jackson Hole and wintering in the valleys west of the Teton Range. Along the shores of Jackson Lake, fire pits, tools and what are thought to have been fishing weights have been discovered. One of the tools found is of a type associated with the Clovis culture, and tools from this cultural period date back at least 11,500 years. Some of the tools are made of obsidian which chemical analysis indicates came from sources near present day Teton Pass, south of Grand Teton National Park. Though obsidian was also available north of Jackson Hole, virtually all the obsidian spear points found are from a source to the south, indicating that the main seasonal migratory route for the Paleo-Indian was from this direction. Elk, which winter on the National Elk Refuge at the southern end of Jackson Hole and northwest into higher altitudes during spring and summer, follow a similar migratory pattern to this day. From 11,000 to about 500 years ago, there is little evidence of change in the migratory patterns amongst the Native American groups in the region and no evidence that indicates any permanent human settlement.
When white American explorers first entered the region in the first decade of the 19th century, they encountered the eastern tribes of the Shoshone people. Most of the Shoshone that lived in the mountain vastness of the greater Yellowstone region continued to be pedestrian while other groups of Shoshone that resided in lower elevations had limited use of horses. The mountain-dwelling Shoshone were known as "Sheep-eaters" or "Tukudika" as they referred to themselves, since a staple of their diet was the Bighorn Sheep. The Shoshones continued to follow the same migratory pattern as their predecessors and have been documented as having a close spiritual relationship with the Teton Range. A number of stone enclosures on some of the peaks, including on the upper slopes of Grand Teton (known simply as "The Enclosure") are thought to have been used by Shoshone during vision quests. The Teton and Yellowstone region Shoshone relocated to the Wind River Indian Reservation after it was established in 1868. The reservation is situated 100 mi southeast of Jackson Hole on land that was selected by Chief Washakie.
Fur trade exploration.
The Lewis and Clark Expedition (1804–1806) passed well north of the Grand Teton region. During their return trip from the Pacific Ocean, expedition member John Colter was given an early discharge so he could join two fur trappers who were heading west in search of beaver pelts. Colter was later hired by Manuel Lisa to lead fur trappers and to explore the region around the Yellowstone River. During the winter of 1807/08 Colter passed through Jackson Hole and was the first Caucasian to see the Teton Range. Lewis and Clark expedition co-leader William Clark produced a map based on the previous expedition and included the explorations of John Colter in 1807, apparently based on discussions between Clark and Colter when the two met in St. Louis, Missouri in 1810. Another map attributed to William Clark indicates John Colter entered Jackson Hole from the northeast, crossing the Continental Divide at either Togwotee Pass or Union Pass and left the region after crossing Teton Pass, following the well established Native American trails. In 1931, the Colter Stone, a rock carved in the shape of a head with the inscription "John Colter" on one side and the year "1808" on the other, was discovered in a field in Tetonia, Idaho, which is west of Teton Pass. The Colter Stone has not been authenticated to have been created by John Colter and may have been the work of later expeditions to the region.
John Colter is widely considered the first mountain man and, like those that came to the Jackson Hole region over the next 30 years, he was there primarily for the profitable fur trapping; the region was rich with the highly sought after pelts of beaver and other fur bearing animals. Between 1810 and 1812, the Astorians traveled through Jackson Hole and crossed Teton Pass as they headed east in 1812. After 1810, American and British fur trading companies were in competition for control of the North American fur trade, and American sovereignty over the region was not secured until the signing of the Oregon Treaty in 1846. One party employed by the British North West Company and led by explorer Donald Mackenzie entered Jackson Hole from the west in 1818 or 1819. The Tetons, as well as the valley west of the Teton Range known today as Pierre's Hole, may have been named by French speaking Iroquois or French Canadian trappers that were part of Mackenzie's party. Earlier parties had referred to the most prominent peaks of the Teton Range as the Pilot Knobs. The French trappers' "les trois tétons" (the three breasts) was later shortened to the Tetons.
Formed in the mid-1820s, the Rocky Mountain Fur Company partnership included Jedediah Smith, William Sublette and David Edward Jackson or "Davey Jackson". Jackson oversaw the trapping operations in the Teton region between 1826 and 1830. Sublette named the valley east of the Teton Range "Jackson's Hole" (later simply Jackson Hole) for Davey Jackson. As the demand for beaver fur declined and the various regions of the American West became depleted of beaver due to over trapping, American fur trading companies folded; however, individual mountain men continued to trap beaver in the region until about 1840. From the mid-1840s until 1860, Jackson Hole and the Teton Range were generally devoid of all but the small populations of Native American tribes that had already been there. Most overland human migration routes such as the Oregon and Mormon Trails crossed over South Pass, well to the south of the Teton Range, and Caucasian influence in the Teton region was minimal until the U.S. Government commenced organized explorations.
Organized exploration and settlement.
The first U.S. Government sponsored expedition to enter Jackson Hole was the 1859–60 Raynolds Expedition. Led by U.S. Army Captain William F. Raynolds and guided by mountain man Jim Bridger, it included naturalist F. V. Hayden, who later led other expeditions to the region. The expedition had been charged with exploring the Yellowstone region, but encountered difficulties crossing mountain passes due to snow. Bridger ended up guiding the expedition south over Union Pass then following the Gros Ventre River drainage to the Snake River and leaving the region over Teton Pass. Organized exploration of the region was halted during the American Civil War but resumed when F. V. Hayden led the well-funded Hayden Geological Survey of 1871. In 1872, Hayden oversaw explorations in Yellowstone, while a branch of his expedition known as the Snake River Division was led by James Stevenson and explored the Teton region. Along with Stevenson was photographer William Henry Jackson who took the first photographs of the Teton Range. The Hayden Geological Survey named many of the mountains and lakes in the region. The explorations by early mountain men and subsequent expeditions failed to identify any sources of economically viable mineral wealth. Nevertheless, small groups of prospectors set up claims and mining operations on several of the creeks and rivers. By 1900 all organized efforts to retrieve minerals had been abandoned.
Though the Teton Range was never permanently inhabited, pioneers began settling the Jackson Hole valley to the east of the range in 1884. These earliest homesteaders were mostly single men who endured long winters, short growing seasons and rocky soils that were hard to cultivate. The region was mostly suited for the cultivation of hay and cattle ranching. By 1890, Jackson Hole had an estimated permanent population of 60. Menor's Ferry was built in 1892 near present day Moose, Wyoming to provide access for wagons to the west side of the Snake River. Ranching increased significantly from 1900 to 1920, but a series of agricultural related economic downturns in the early 1920s left many ranchers destitute. Beginning in the 1920s, the automobile provided faster and easier access to areas of natural beauty and old military roads into Jackson Hole over Teton and Togwotee Passes were improved to accommodate the increased vehicle traffic. In response to the increased tourism, dude ranches were established, some new and some from existing cattle ranches, so urbanized travelers could experience the life of a cattleman.
Establishment of the park.
To the north of Jackson Hole, Yellowstone National Park had been established in 1872, and by the close of the 19th century, conservationists wanted to expand the boundaries of that park to include at least the Teton Range. By 1907, in an effort to regulate water flow for irrigation purposes, the U.S. Bureau of Reclamation had constructed a log crib dam at the Snake River outlet of Jackson Lake. This dam failed in 1910 and a new concrete Jackson Lake Dam replaced it by 1911. The dam was further enlarged in 1916, raising lake waters 39 ft as part of the Minidoka Project, designed to provide irrigation for agriculture in the state of Idaho. Further dam construction plans for other lakes in the Teton Range alarmed Yellowstone National Park superintendent Horace Albright, who sought to block such efforts. Jackson Hole residents were opposed to an expansion of Yellowstone, but were more in favor of the establishment of a separate National Park which would include the Teton Range and six lakes at the base of the mountains. After congressional approval, President Calvin Coolidge signed the executive order establishing the 96000 acre Grand Teton National Park on February 26, 1929.
The valley of Jackson Hole remained primarily in private ownership when John D. Rockefeller, Jr. and his wife visited the region in the late 1920s. Horace Albright and Rockefeller discussed ways to preserve Jackson Hole from commercial exploitation, and in consequence, Rockefeller started buying Jackson Hole properties through the Snake River Land Company for the purpose of later turning them over to the National Park Service. In 1930, this plan was revealed to the residents of the region and was met with strong disapproval. Congressional efforts to prevent the expansion of Grand Teton National Park ended up putting the Snake River Land Company's holdings in limbo. By 1942 Rockefeller had become increasingly impatient that his purchased property might never be added to the park, and wrote to the Secretary of the Interior Harold L. Ickes that he was considering selling the land to another party. Secretary Ickes recommended to President Franklin Roosevelt that the Antiquities Act, which permitted Presidents to set aside land for protection without the approval of Congress, be used to establish a National Monument in Jackson Hole. Roosevelt created the 221000 acre Jackson Hole National Monument in 1943, using the land donated from the Snake River Land Company and adding additional property from Teton National Forest. The monument and park were adjacent to each other and both were administered by the National Park Service, but the monument designation ensured no funding allotment, nor provided a level of resource protection equal to the park. Members of Congress repeatedly attempted to have the new National Monument abolished.
After the end of World War II national public sentiment was in favor of adding the monument to the park, and though there was still much local opposition, the monument and park were combined in 1950. In recognition of John D. Rockefeller, Jr.'s efforts to establish and then expand Grand Teton National Park, a 24000 acre parcel of land between Grand Teton and Yellowstone National Parks was added to the National Park Service in 1972. This land and the road from the southern boundary of the park to West Thumb in Yellowstone National Park was named the John D. Rockefeller, Jr. Memorial Parkway. The Rockefeller family owned the JY Ranch, which bordered Grand Teton National Park to the southwest. In November 2007, the Rockefeller family transferred ownership of the ranch to the park for the establishment of the Laurance S. Rockefeller Preserve, which was dedicated on June 21, 2008.
History of mountaineering.
During the last 25 years of the 19th century, the mountains of the Teton Range became a focal point for explorers wanting to claim first ascents of the peaks. However, white explorers may not have been the first to climb many of the peaks and the earliest first ascent of even the formidable Grand Teton itself might have been achieved long before written history documented it. Native American relics remain including "The Enclosure", an obviously man-made structure that is located about 530 ft below the summit of Grand Teton at a point near the Upper Saddle (13160 ft). Nathaniel P. Langford and James Stevenson, both members of the Hayden Geological Survey of 1872, found The Enclosure during their early attempt to summit Grand Teton. Langford claimed that he and Stevenson climbed Grand Teton, but were vague as to whether they had made it to the summit. Their reported obstacles and sightings were never corroborated by later parties. Langford and Stevenson likely did not get much further than The Enclosure. The first ascent of Grand Teton that is substantiated was made by William O. Owen, Frank Petersen, John Shive and Franklin Spencer Spalding on August 11, 1898. Owen had made two previous attempts on the peak and after publishing several accounts of this first ascent, discredited any claim that Langford and Stevenson had ever reached beyond The Enclosure in 1872. The disagreement over which party first reached the top of Grand Teton may be the greatest controversy in the history of American mountaineering. After 1898 no other ascents of Grand Teton were recorded until 1923.
By the mid-1930s, more than a dozen different climbing routes had been established on Grand Teton including the northeast ridge in 1931 by Glenn Exum. Glenn Exum teamed up with another noted climber named Paul Petzoldt to found the Exum Mountain Guides in 1931. Of the other major peaks on the Teton Range, all were climbed by the late 1930s including Mount Moran in 1922 and Mount Owen in 1930 by Fritiof Fryxell and others after numerous previous attempts had failed. Both Middle and South Teton were first climbed on the same day, August 29, 1923, by a group of climbers led by Albert R. Ellingwood. New routes on the peaks were explored as safety equipment and skills improved and eventually climbs rated at above 5.9 on the Yosemite Decimal System difficulty scale were established on Grand Teton. The classic climb following the route first pioneered by Owen, known as the Owen-Spalding route, is rated at 5.4 due a combination of concerns beyond the gradient alone. Rock climbing and bouldering had become popular in the park by the mid 20th century. In the late 1950s, gymnast John Gill came to the park and started climbing large boulders near Jenny Lake. Gill approached climbing from a gymnastics perspective and while in the Tetons became the first known climber in history to use gymnastic chalk to improve handholds and to keep hands dry while climbing. During the latter decades of the 20th century, extremely difficult cliffs were explored including some in Death Canyon, and by the mid-1990s, 800 different climbing routes had been documented for the various peaks and canyon cliffs.
Park management.
Grand Teton National Park is one of the ten most visited national parks in the U.S., with an average of 2.5 million visitors annually. The National Park Service is a federal agency of the United States Department of the Interior and manages both Grand Teton National Park and the John D. Rockefeller, Jr. Memorial Parkway. Grand Teton National Park has an average of 100 permanent and 180 seasonal employees. The park also manages 27 concession contracts that provide services such as lodging, restaurants, mountaineering guides, dude ranching, fishing and a boat shuttle on Jenny Lake. The National Park Service works closely with other federal agencies such as the U.S. Forest Service, the U.S. Fish and Wildlife Service, the Bureau of Reclamation, and also, in consequence of Jackson Hole Airport's presence in the park, the Federal Aviation Administration. Initial construction of the airstrip north of the town of Jackson was completed in the 1930s. When Jackson Hole National Monument was designated, the airport was inside it. After the monument and park were combined, the Jackson Hole Airport became the only commercial airport within a U.S. National Park. Jackson Hole Airport has some of the strictest noise abatement regulations of any airport in the U.S. The airport has night flight curfews and overflight restrictions, with pilots being expected to approach and depart the airport along the east, south or southwest flight corridors. As of 2010, 110 privately owned property inholdings, many belonging to the state of Wyoming, are located within Grand Teton National Park. Efforts to purchase or trade these inholdings for other federal lands are ongoing and through partnerships with other entities, 10 million dollars is hoped to be raised to acquire private inholdings by 2016. Some monies are allocated from congress via the Land and Water Conservation Fund, but Grand Teton National Park may not get all the money needed from the fund as it is divided up between four different federal agencies. Efforts to exchange federal land from other areas for inholdings were still in the negotiation phase in 2012.
Geography.
Grand Teton National Park is located in the northwestern region of the U.S. state of Wyoming. To the north the park is bordered by the John D. Rockefeller, Jr. Memorial Parkway, which is administered by Grand Teton National Park. The scenic highway with the same name passes from the southern boundary of Grand Teton National Park to West Thumb in Yellowstone National Park. Grand Teton National Park covers approximately 310000 acre, while the John D. Rockefeller, Jr. Memorial Parkway includes 23700 acre. Most of the Jackson Hole valley and virtually all the major mountain peaks of the Teton Range are within the park. The Jedediah Smith Wilderness of Caribou-Targhee National Forest lies along the western boundary and includes the western slopes of the Teton Range. To the northeast and east lie the Teton Wilderness and Gros Ventre Wilderness of Bridger-Teton National Forest. The National Elk Refuge is to the southeast, and migrating herds of elk winter there. Privately owned land borders the park to the south and southwest. Grand Teton National Park, along with Yellowstone National Park, surrounding National Forests and related protected areas constitute the 18000000 acre (28000 sqmi) Greater Yellowstone Ecosystem. The Greater Yellowstone Ecosystem spans across portions of three states and is one of the largest intact mid-latitude ecosystems remaining on Earth. By road, Grand Teton National Park is 290 mi from Salt Lake City, Utah and 550 mi from Denver, Colorado.
Teton Range.
The youngest mountain range in the Rocky Mountains, the Teton Range began forming between 6 to 9 million years ago. It runs roughly north to south and rises from the floor of Jackson Hole without any foothills along a 40 mi long by 7 to wide active fault-block mountain front. The range tilts westward, rising abruptly above Jackson Hole valley which lies to the east but more gradually into Teton Valley to the west. A series of earthquakes along the Teton Fault slowly displaced the western side of the fault upward and the eastern side of the fault downward at an average of 1 ft of displacement every 300–400 years. Most of the displacement of the fault occurred in the last 2 million years. While the fault has experienced up to 7.5–earthquake magnitude events since it formed, it has been relatively quiescent during historical periods, with only a few 5.0–magnitude or greater earthquakes known to have occurred since 1850.
In addition to 13775 ft high Grand Teton, another nine peaks are over 12000 ft above sea level. Eight of these peaks between Avalanche and Cascade Canyons make up the often-photographed Cathedral Group. The most prominent peak north of Cascade Canyon is the monolithic Mount Moran (12605 ft) which rises 5728 ft above Jackson Lake. To the north of Mount Moran, the range eventually merges into the high altitude Yellowstone Plateau. South of the central Cathedral Group the Teton Range tapers off near Teton Pass and blends into the Snake River Range.
West to east trending canyons provide easier access by foot into the heart of the range as no vehicular roads traverse the range except at Teton Pass, which is south of the park. Carved by a combination of glacier activity as well as by numerous streams, the canyons are at their lowest point along the eastern margin of the range at Jackson Hole. Flowing from higher to lower elevations, the glaciers created more than a dozen U-shaped valleys throughout the range. Cascade Canyon is sandwiched between Mount Owen and Teewinot Mountain to the south and Symmetry Spire to the north and is situated immediately west of Jenny Lake. North to south, Webb, Moran, Paintbrush, Cascade, Death and Granite Canyons slice through Teton Range.
Jackson Hole.
Jackson Hole is a 55 mi long by 6 to wide graben valley with an average elevation of 6800 ft, its lowest point is near the southern park boundary at 6350 ft. The valley sits east of the Teton Range and is vertically displaced downward 30000 ft, making the Teton Fault and its parallel twin on the east side of the valley normal faults with the Jackson Hole block being the hanging wall and the Teton Mountain block being the footwall. Grand Teton National Park contains the major part of both blocks. Erosion of the range provided sediment in the valley so the topographic relief is only 7700 ft. Jackson Hole is comparatively flat, with only a modest increase in altitude south to north, however a few isolated buttes such as Blacktail Butte and hills including Signal Mountain dot the valley floor. In addition to a few outcroppings, the Snake River has eroded terraces into Jackson Hole. Southeast of Jackson Lake, glacial depressions known as kettles are numerous. The kettles were formed when ice situated under gravel outwash from ice sheets melted as the glaciers retreated.
Lakes and rivers.
Most of the lakes in the park were formed by glaciers and the largest of these lakes are located at the base of the Teton Range. In the northern section of the park lies Jackson Lake, the largest lake in the park at 15 mi in length, 5 mi wide and 438 ft deep. Though Jackson Lake is natural, the Jackson Lake Dam was constructed at its outlet before the creation of the park and the lake level was raised almost 40 ft consequently. East of the Jackson Lake Lodge lies Emma Matilda and Two Ocean Lakes. South of Jackson Lake, Leigh, Jenny, Bradley, Taggart and Phelps Lakes rest at the outlets of the canyons which lead into the Teton Range. Within the Teton Range, small alpine lakes in cirques are common, and there are more than 100 scattered throughout the high country. Lake Solitude, located at an elevation of 9035 ft, is in a cirque at the head of the North Fork of Cascade Canyon. Other high altitude lakes can be found at over 10000 ft in elevation and a few, such as Icefloe Lake, remain ice clogged for much of the year. The park is not noted for large waterfalls; however, 100 ft high Hidden Falls just west of Jenny Lake is easy to reach after a short hike.
From its headwaters on Two Ocean Plateau in Yellowstone National Park, the Snake River flows north to south through the park, entering Jackson Lake near the boundary of Grand Teton National Park and John D. Rockefeller, Jr. Memorial Parkway. The Snake River then flows through the spillways of the Jackson Lake Dam and from there southward through Jackson Hole, exiting the park just west of the Jackson Hole Airport. The largest lakes in the park all drain either directly or by tributary streams into the Snake River. Major tributaries which flow into the Snake River include Pacific Creek and Buffalo Fork near Moran and the Gros Ventre River at the southern border of the park. Through the comparatively level Jackson Hole valley, the Snake River descends an average of 19 ft/mi, while other streams descending from the mountains to the east and west have higher gradients due to increased slope. The Snake River creates braids and channels in sections where the gradients are lower and in steeper sections, erodes and undercuts the cobblestone terraces once deposited by glaciers.
Glaciation.
The major peaks of the Teton Range were carved into their current shapes by long vanished glaciers. Commencing 250,000–150,000 years ago, the Tetons went through several periods of glaciation with some areas of Jackson Hole covered by glaciers 2000 ft thick. This heavy glaciation is unrelated to the uplift of the range itself and is instead part of a period of global cooling known as the Quaternary glaciation. Beginning with the Buffalo Glaciation and followed by the Bull Lake and then the Pinedale glaciation, which ended roughly 15,000 years ago, the landscape was greatly impacted by glacial activity. During the Pinedale glaciation, the landscape visible today was created as glaciers from the Yellowstone Plateau flowed south and formed Jackson Lake, while smaller glaciers descending from the Teton Range pushed rock moraines out from the canyons and left behind lakes near the base of the mountains. The peaks themselves were carved into horns and arêtes and the canyons were transformed from water-eroded V-shapes to glacier-carved U-shaped valleys. Approximately a dozen glaciers currently exist in the park, but they are not ancient as they were all reestablished sometime between 1400 and 1850 AD during the Little Ice Age. Of these more recent glaciers, the largest is Teton Glacier, which sits below the northeast face of Grand Teton. Teton Glacier is 3500 ft long and 1100 ft wide, and nearly surrounded by the tallest summits in the range. Teton Glacier is also the best studied glacier in the range, and researchers concluded in 2005 that the glacier could disappear in 30 to 75 years. West of the Cathedral Group near Hurricane Pass, Schoolroom Glacier is tiny but has well defined terminal and lateral moraines, a small proglacial lake and other typical glacier features in close proximity to each other.
Geology.
Grand Teton National Park has some of the most ancient rocks found in any U.S. National Park. The oldest rocks dated so far are 2,680 ± 12 million years old, though even older rocks are believed to exist in the park. Formed during the Archean Eon (4 to 2.5 billion years ago), these metamorphic rocks include gneiss, schist and amphibolites. Metamorphic rocks are the most common types found in the northern and southern sections of the Teton Range. 2,545 million years ago, the metamorphic rocks were intruded by igneous granitic rocks, which are now visible in the central Tetons including Grand Teton and the nearby peaks. The light colored granites of the central Teton Range contrast with the darker metamorphic gneiss found on the flanks of Mount Moran to the north. Magma intrusions of diabase rocks 765 million years ago left dikes that can be seen on the east face of Mount Moran and Middle Teton. Granite and pegmatite intrusions also worked their way into fissures in the older gneiss. Precambrian rocks in Jackson Hole are buried deep under comparatively recent Tertiary volcanic and sedimentary deposits, as well as Pleistocene glacial deposits.
By the close of the Precambrian, the region was intermittently submerged under shallow seas, and for 500 million years various types of sedimentary rocks were formed. During the Paleozoic (542 to 251 million years ago) sandstone, shale, limestone and dolomite were deposited. Though most of these sedimentary rocks have since eroded away from the central Teton Range, they are still evident on the northern, southern and western flanks of the range. One notable exception is the sandstone Flathead Formation which continues to cap Mount Moran. Sedimentary layering of rocks in Alaska Basin, which is on the western border of Grand Teton National Park, chronicles a 120 million year period of sedimentary deposition. Fossils found in the sedimentary rocks in the park include algae, brachiopods and trilobites. Sedimentary deposition continued during the Mesozoic (250–66 million years ago) and the coal seams found in the sedimentary rock strata indicate the region was densely forested during that era. Numerous coal seams of 5 to in thickness are interspersed with siltstone, claystone and other sedimentary rocks. During the late Cretaceous, a volcanic arc west of the region deposited fine grained ash that later formed into bentonite, an important mineral resource.
From the end of the Mesozoic to present, the region went through a series of uplifts and erosional sequences. Commencing 66 million years ago the Laramide orogeny was a period of mountain-building and erosion in western North America that created the ancestral Rocky Mountains. This cycle of uplift and erosion left behind one of the most complete non-marine Cenozoic rock sequences found in North America. Conglomerate rocks composed of quartzite and interspersed with mudstone and sandstones were deposited during erosion from a now vanished mountain range that existed to the northwest of the current Teton Range. These deposits also have trace quantities of gold and mercury. During the Eocene and Oligocene, volcanic eruptions from the ancestral Absaroka Range buried the region under various volcanic deposits. Sedimentary basins developed in the region due to drop faulting, creating an ancestral Jackson Hole and by the Pliocene (10 million years ago), an ancestral Jackson Lake known as Teewinot Lake. During the Quaternary, landslides, erosion and glacial activity deposited soils and rock debris throughout the Snake River valley of Jackson Hole and left behind terminal moraines which impound the current lakes. The most recent example of rapid alteration to the landscape occurred in 1925 just east of the park, when the Gros Ventre landslide was triggered by spring melt from a heavy snowpack as well as heavy rain.
Ecology.
Flora.
Grand Teton National Park and the surrounding region host over 1000 species of vascular plants. With an altitude variance of over 7000 ft, the park has a number of different ecological zones including alpine tundra, the Rocky Mountains subalpine zone where spruce-fir forests are dominant, and the valley floor, where a mixed conifer and deciduous forest zone occupies regions with better soils intermixed with sagebrush plains atop alluvial deposits. Additionally, wetlands near some lakes and in the valley floor adjacent to rivers and streams cover large expanses, especially along the Snake River near Oxbow Bend near Moran and Willow Flats near the Jackson Lake Lodge. Altitude, available soils, wildfire incidence, avalanches and human activities have a direct impact on the types of plant species in an immediate area. Where these various niches overlap is known as an ecotone.
The range of altitude in Grand Teton National Park impacts the types of plant species found at various elevations. In the alpine zone above the tree line, which in Grand Teton National Park is at approximately 10000 ft, tundra conditions prevail. In this treeless region, hundreds of species of grass, wildflower, moss and lichen are found. In the subalpine region from the tree line to the base of the mountains, whitebark pine, limber pine, subalpine fir, and Engelmann spruce are dominant. In the valley floor, lodgepole pine is most common but Rocky Mountain Douglas-fir, and blue spruce inhabit drier areas, while aspen, cottonwood, alder, and willow are more commonly found around lakes, streams and wetlands. However, the tablelands above the Snake River channel are mostly sagebrush plains and in terms of acreage is the most widespread habitat in the park. The sagebrush plains or flats have 100 species of grasses and wildflowers. Slightly more elevated sections of the plains of the northern sections of Jackson Hole form forest islands with one such obvious example being Timbered Island. In this ecotone, forested islands surrounded by sagebrush expanses provide shelter for various animal species during the day and nearby grasses for night time foraging.
While the flora of Grand Teton National Park is generally healthy, the whitebark pine, and to a lesser degree the lodgepole pine, are considered at risk. In the case of the whitebark pine, an invasive species of fungus known as white pine blister rust weakens the tree, making it more susceptible to destruction from endemic mountain pine beetles. Whitebark pines generally thrive at elevations above 8000 ft and produce large seeds that are high in fat content and an important food source for various species such as the grizzly bear, red squirrel and Clark's nutcracker. The species is considered to be a keystone and a foundation species; keystone in that its ""ecological role (is) disproportionately large relative to its abundance" and foundation in that it has a paramount role that "defines ecosystem structure, function, and process"". Whitebark pine has generally had a lower incidence of blister rust infection throughout the Greater Yellowstone Ecosystem than in other regions such as Glacier National Park and the Cascade Range. The incidence of blister rust on whitebark pines in Yellowstone National Park is slightly lower than in Grand Teton. Though blister rust is not in itself the cause of increased mortality, its weakening effect on trees allows native pine beetles to more easily infest the trees, increasing mortality. While general practice in National Parks is to allow nature to take its course, the alarming trend of increased disease and mortality of the vital whitebark pine trees has sparked a collaborative effort amongst various government entities to intervene to protect the species.
Fauna.
Sixty-one species of mammals have been recorded in Grand Teton National Park. This includes the gray wolf, which had been extirpated from the region by the early 1900s but migrated into the Grand Teton National Park from adjacent Yellowstone National Park after the species had been reintroduced there. The re-establishment of the wolves has ensured that every indigenous mammal species now exists in the park. In addition to gray wolves, another 17 species of carnivores reside within Grand Teton National Park including grizzlies and the more commonly seen American black bear. Relatively common sightings of coyote, river otter, marten and badger and occasional sightings of mountain lion, lynx and wolverine are reported annually. A number of rodent species exist including yellow-bellied marmot, least chipmunk, muskrat, beaver, Uinta ground squirrel, pika, snowshoe hare, porcupine, and six species of bats.
Of the larger mammals the most common are elk, which exist in the thousands. Their migration route between the National Elk Refuge and Yellowstone National Park is through Grand Teton National Park, so while easily seen anytime of the year, they are most numerous in the spring and fall. Other ungulates include bison and pronghorn—the fastest land mammal in the western hemisphere—which are found throughout Jackson Hole as are moose, which tend to stay near waterways and wetlands. Between 100–125 bighorn sheep dwell in the alpine and rocky zones of the peaks.
Over 300 species of birds have been sighted in the park including the calliope hummingbird, the smallest bird species in North America, as well as trumpeter swans, which is North America's largest waterfowl. In addition to trumpeter swans, another 30 species of waterfowl have been recorded including blue-winged teal, common merganser, American wigeon and the colorful but reclusive harlequin duck which is occasionally spotted in Cascade Canyon. Both bald and golden eagles and other birds of prey such as the osprey, red-tailed hawk, American kestrel and occasional sightings of peregrine falcon have been reported. Of the 14 species of owls reported, the most common is the great horned owl, though the boreal owl and great grey owl are also seen occasionally. A dozen species of woodpeckers have been reported, as have a similar number of species of warblers, plovers and gulls. The vocal and gregarious black-billed magpie frequents campgrounds while Steller's jay and Clark's nutcracker are found in the backcountry. The sage covered plains of Jackson Hole are favored areas for sage grouse, Brewer's sparrow and sage thrashers, while the wetlands are frequented by great blue heron, American white pelican, sandhill crane and on rare occasions it's endangered relative, the whooping crane.
The Snake River fine-spotted cutthroat trout (or "Snake River cutthroat trout") is the only native trout species in Grand Teton National Park. It is also the only subspecies of cutthroat trout that is exclusively native to large streams and rivers. Various researchers have not been able to identify any genetic differences between the Snake River fine-spotted cutthroat trout and the Yellowstone cutthroat trout, though in terms of appearances, the Snake River subspecies has much smaller spots which cover a greater portion of the body, and the two subspecies inhabit different ecological niches. The Snake River fine-spotted cutthroat trout was identified by some researchers as a separate subspecies by the mid-1990s, and is managed as a distinct subspecies by the state of Wyoming, but is not yet recognized as such by the neighboring states of Idaho and Montana. Snake River fine-spotted cutthroat trout is found only in the Snake River and tributaries below the Jackson Lake dam to the Palisades Reservoir in Idaho. Other non-native species of trout such as the rainbow trout and lake trout were introduced by the Wyoming Fish and Game Department or migrated out of Yellowstone. Today five trout species inhabit park waters. Native species of fish include the mountain whitefish, longnose dace, mountain sucker and non-native species include the Utah chub and Arctic grayling.
Only four species of reptiles are documented in the park: three species of snakes which are the wandering garter snake, the less commonly seen valley garter snake and rubber boa, as well as one lizard species, the northern sagebrush lizard, that was first reported in 1992. None of the species are venomous. Six amphibian species have been documented including the Columbia spotted frog, boreal chorus frog, tiger salamander and the increasingly rare boreal toad and northern leopard frog. A sixth amphibian species, the bullfrog, was introduced. An estimated 10,000 insect species frequent the park; they pollinate plants, provide a food source for birds, fish, mammals and other animals, and help in the decomposition of wood. In one example of the importance of insects to the ecosystem, swarms of Army cutworm moths die in huge numbers after mating and provide a high fat and protein diet for bears and other predators. One study concluded that when this moth species is most available, bears consume 40,000 moths per day which is roughly 20,000 kcal/day.
Grand Teton National Park is the only U.S. National Park that permits hunting, but only of elk in an effort to keep the populations of that species regulated. This provision was included in the legislation that combined Jackson Hole National Monument and Grand Teton National Park in 1950. While some National Parks in Alaska permit subsistence hunting by indigenous natives and a few other National Park Service managed areas allow hunting under highly regulated circumstances, hunting in U.S. National Parks is not generally allowed. In Grand Teton National Park, hunters are required to obtain Wyoming hunting licenses and be deputized as park rangers. Hunting is restricted to areas east of the Snake River and north of Moran, Wyoming, the hunt is permitted only east of U.S. Route 89. Proponents of continuing the elk hunt, which occurs in the fall, argue that the elk herd would become overpopulated without it, leading to vegetation degradation from overgrazing elk herds. Opponents cite that there has been an increase of predators such as the wolf and grizzly bear in Grand Teton National Park, rendering the annual hunt unnecessary and exposing hunters to attacks by grizzly bears as they become accustomed to feeding on remains left behind from the hunt.
Fire ecology.
The role of wildfire is an important one for plant and animal species diversity. Many tree species have evolved to mainly germinate after a wildfire. Regions of the park that have experienced wildfire in historical times have greater species diversity after reestablishment than those regions that have not been influenced by fire. Though the Yellowstone fires of 1988 had minimal impact on Grand Teton National Park, studies conducted before and reaffirmed after that event concluded than the suppression of natural wildfires during the middle part of the 20th century decreased plant species diversity and natural regeneration of plant communities. One study conducted 15 years before the 1988 Yellowstone National Park fires concluded that human suppression of wildfire had adversely impacted Aspen tree groves and other forest types. The majority of conifer species in Grand Teton National Park are heavily dependent on wildfire and this is particularly true of the Lodgepole Pine. Though extremely hot canopy or crown fires tend to kill Lodgepole Pine seeds, lower severity surface fires usually result in a higher post wildfire regeneration of this species. In accordance with a better understanding of the role wildfire plays in the environment, the National Park Service and other land management agencies have developed Fire Management Plans which provide a strategy for wildfire management and are expected to best enhance the natural ecosystem.
Climate.
Grand Teton National Park has a semi-arid climate with the wettest months between November and January, mostly in the form of snow. The park averages 450 in of snow in the mountains and 191 in in the valley annually. In January, the daily temperature range averages between 26 °F during the day to 1 °F at night. During the month of July, the daily temperature range is 80 and. The record high is 93 °F and the record low is -66 °F. Temperatures above the valley in the mountains during the summer average 1 F-change cooler for every 1000 ft of altitude gained, so high altitude passes may remain snow-covered until mid-July. Thunderstorms are common during the summer, especially over the mountains; by contrast, no tornados have ever been reported within the park. The F4 Teton-Yellowstone tornado was a high altitude tornado which touched down northeast of the park on the border of the Teton Wilderness of Bridger-Teton National Forest and Yellowstone National Park.
Air and water quality.
Grand Teton National Park is more than 100 mi air distance from any major urban or industrial area, and localized human activities have generally had a very low environmental impact on the surrounding region. However, levels of ammonium and nitrogen have been trending slightly upwards due to deposition from rain and snow that is believed to originate from regional agricultural activities. Additionally, there has also been a slight increase in mercury and pesticides that have been detected in snow and some alpine lakes. Ozone and haze may be impacting overall visibility levels. Grand Teton National Park, in partnership with other agencies, erected the first air quality monitoring station in the park in 2011. The station is designed to check for various pollutants as well as ozone levels and weather.
A 2005 study of the water of Jackson, Jenny and Taggart Lakes indicated that all three of these lakes had virtually pristine water quality. Of the three lakes, only on Taggart Lake are motorized boats prohibited, yet little difference in water quality was detected in the three lakes. In a study published in 2002, the Snake River was found to have better overall water quality than other river systems in Wyoming, and low levels of pollution from anthropogenic sources.
Recreation.
Mountaineering.
Grand Teton National Park is a popular destination for mountain and rock climbers partly because the mountains are easily accessible by road. Trails are well marked and routes to the summits of most peaks are long established, and for the experienced and fit, most peaks can be climbed in one day. The highest maintained trails climb from the floor of Jackson Hole over 4000 ft to mountain passes that are sometimes called saddles or divides. From these passes, the climbs follow routes that require varying skill levels. Climbers do not need a permit but are encouraged to voluntarily register their climbing plans with the National Park Service and inform associates of their itinerary. Any climb requiring an overnight stay in the backcountry does require a permit. Climbers are essentially on their own to determine their own skill levels and are encouraged to not take unnecessary risks. The Exum Mountain Guides, which is considered one of the finest mountaineering guide services in the U.S., as well as the Jackson Hole Mountain Guides, offer instruction and climbing escorts for those who are less experienced or unfamiliar with various routes.
An average of 4,000 climbers per year make an attempt to summit Grand Teton and most ascend up Garnet Canyon to a mountain pass called the Lower Saddle, which is between Grand Teton and Middle Teton. From the Lower Saddle, climbers often follow the Owen-Spalding or Exum Ridge routes to the top of Grand Teton though there are 38 distinct routes to the summit. The north face route to the summit of Grand Teton is a world renowned climb involving a dozen distinct pitches and is rated at grade 5.8 in difficulty for the 3000 ft vertical ascent. On a connecting ridge and just north of Grand Teton lies Mount Owen, and though lower in altitude, this peak is considered more difficult to ascend. Middle Teton is another popular climb that is most easily summited from a saddle between it and South Teton. Well north of Grand Teton lies Mount Moran, which is further from trailheads and more difficult to access and ascend. The Direct South Buttress of Mount Moran provides a vertical mile of climbing that was considered the most difficult climb in the U.S. when first accomplished in 1953. Other popular climbing destinations include Buck Mountain, Symmetry Spire, Mount Saint John, Mount Wister, Teewinot Mountain and Nez Perce Peak and each mountain has at least six established routes to their summits.
Camping and hiking.
Grand Teton National Park has five front-country vehicular access campgrounds. The largest are the Colter Bay and Gros Ventre campgrounds, and each has 350 campsites which can accommodate large recreational vehicles. Lizard Creek and Signal Mountain campgrounds have 60 and 86 campsites respectively, while the smaller Jenny Lake campground has only 49 sites for tent use only. Additionally, full hookups for recreational vehicles are at the concessionaire managed 112 campsites at Colter Bay Village and another 100 at Flagg Ranch in the John D. Rockefeller Memorial Parkway. Though all front-country campgrounds are only open from late spring to late fall, primitive winter camping is permitted at Colter Bay near the visitor center.
All campsites accessible only on foot or by horseback are considered backcountry campsites and they are available by permit only, but camping is allowed in most of these backcountry zones year-round. The National Park Service has a combination of specific sites and zones for backcountry camping with a set carrying capacity of overnight stays per zone to protect the resources from overcrowding. Open fires are not permitted in the backcountry and all food must be stored in an Interagency Grizzly Bear Committee approved bear-resistant container. As of 2012, only four brands of bear-resistant containers had been approved for use in the Grand Teton National Park backcountry. Additionally, hikers may use an approved bear spray to elude aggressive bears.
The park has 200 mi of hiking trails, ranging in difficulty from easy to strenuous. The easiest hiking trails are located in the valley, where the altitude changes are generally minimal. In the vicinity of Colter Bay Village, the Hermitage Point Trail is 9.4 mi long and considered easy. Several other trails link Hermitage Point with Emma Matilda Lake and Two Ocean Lake Trails, also considered to be relatively easy hikes in the Jackson Lake Lodge area. Other easy hikes include the Valley Trail which runs from Trapper Lake in the north to the south park boundary near Teton Village and the Jenny Lake Trail which circles the lake. Ranging from moderate to strenuous in difficulty, trails leading into the canyons are rated based on distance and more importantly on the amount of elevation change. The greatest elevation change is found on the Paintbrush Canyon, Alaska Basin and Garnet Canyon Trails, where elevation increases of over 4000 ft are typical. Horses and pack animals are permitted on almost all trails in the park, however there are only five designated backcountry camping locations for pack animals and these campsites are far from the high mountain passes. Bicycles are limited to vehicle roadways only and the park has widened some roads to provide a safer biking experience. A paved multi-use pathway opened in 2009 and provides non-motorized biking access from the town of Jackson to South Jenny Lake.
Boating and fishing.
Grand Teton National Park allows boating on all the lakes in Jackson Hole, but motorized boats can only be used on Jackson and Jenny Lakes. While there is no maximum horsepower limit on Jackson Lake (though there is a noise restriction), Jenny Lake is restricted to 10 horsepower. Only non–motorized boats are permitted on Bearpaw, Bradley, Emma Matilda, Leigh, Phelps, String, Taggart and Two Ocean Lakes. There are four designated boat launches located on Jackson Lake and one on Jenny Lake. Additionally, sailboats, windsurfers and water skiing are only allowed on Jackson Lake and no jet skis are permitted on any of the park waterways. All boats are required to comply with various safety regulations including personal flotation devices for each passenger. Only non–motorized watercraft are permitted on the Snake River. All other waterways in the park are off limits to boating, and this includes all alpine lakes and tributary streams of the Snake River.
In 2010, Grand Teton National Park started requiring all boats to display an Aquatic Invasive Species decal issued by the Wyoming Game and Fish Department or a Yellowstone National Park boat permit. In an effort to keep the park waterways free of various invasive species such as the Zebra mussel and whirling disease, boaters are expected to abide by certain regulations including displaying a self-certification of compliance on the dashboard of any vehicle attached to an empty boat trailer.
Grand Teton National Park fisheries are managed by the Wyoming Fish and Game Department and a Wyoming state fishing license is required to fish all waterways in Grand Teton National Park. The creel limit for trout is restricted to six per day, including no more than three cutthroat trout with none longer than 12 in, while the maximum length of other trout species may not exceed 20 in, except those taken from Jackson Lake, where the maximum allowable length is 24 in. There are also restrictions as to the seasonal accessibility to certain areas as well as the types of bait and fishing tackle permitted.
Winter activities.
Visitors are allowed to snowshoe and do cross-country skiing and are not restricted to trails. The Teton Park Road between the Taggart Lake trailhead to Signal Mountain Campground is closed to vehicular traffic during the winter and this section of the road is groomed for skiing and snowshoeing traffic. The park service offers guided snowshoe tours daily from the main headquarters located in Moose, Wyoming. Overnight camping is allowed in the winter in the backcountry with a permit and visitors should inquire about avalanche dangers.
The only location in Grand Teton National Park where snowmobiles are permitted is on Jackson Lake. The National Park Service requires that all snowmobiles use "Best Available Technology" (BAT) and lists various models of snowmobiles that are permitted, all of which are deemed to provide the least amount of air pollution and maximize noise abatement. All snowmobiles must be less than 10 years old and have odometer readings of less than 6000 mi. Additionally, snowmobile use is for the purposes of accessing ice fishing locations only. Snowmobile access was permitted between Moran Junction and Flagg Ranch adjacent to the John D. Rockefeller, Jr. Memorial Parkway so that travelers using the Continental Divide Snowmobile Trail could traverse between Bridger-Teton National Forest and Yellowstone National Park. However in 2009, winter use planners closed this since unguided snowmobile access into Yellowstone National Park was also discontinued.
Tourism.
Visitor centers.
The Craig Thomas Discovery and Visitor Center adjacent to the park headquarters at Moose, Wyoming, is open year round. Opened in 2007 to replace an old, inadequate visitor center, the facility is named for the late U.S. Senator Craig Thomas. It was financed with a combination of federal grants and private donations. An adjoining 154-seat auditorium was opened to the public in April 2011. To the north at Colter Bay Village on Jackson Lake, the Colter Bay Visitor Center & Indian Arts Museum is open from the beginning of May to the early October. The Colter Bay Visitor Center & Indian Arts Museum has housed the David T. Vernon Indian Arts Exhibit since 1972. The Colter Bay Visitor Center was built in 1956 and was determined in 2005 to be substandard for the proper care and display of the Indian arts collection. During the winter of 2011–2012, a $150,000 renovation project was completed at the center and a portion of the arts collection was made available for viewing when the center opened for the season in May 2012.
South of Moose on the Moose–Wilson Road, the Laurance S. Rockefeller Preserve Center is located on land that was privately owned by Laurance S. Rockefeller and is situated on Phelps Lake. Donated to Grand Teton National Park and opened to the public in 2008, the property was once part of the JY Ranch, the first dude ranch in Jackson Hole. At Jenny Lake, the Jenny Lake Visitor Center is open from mid–May to mid–September. This visitor center is within the Jenny Lake Ranger Station Historic District and is the same structure photographer Harrison Crandall had constructed as an art studio in the 1920s.
Accommodations.
Contracted through the National Park Service, various concessionaire entities manage lodging facilities inside the park. The largest such facility is the Jackson Lake Lodge, which is managed by the Grand Teton Lodge Company. Located near Jackson Lake Dam, the Jackson Lake Lodge has a total of 385 rooms, meeting facilities, a retail shop and a restaurant. The Grand Teton Lodge Company also manages the Jenny Lake Lodge, which consists of cabins and a restaurant and Colter Bay Village, which has cabins, a restaurant, a grocery store, a laundry and a marina. South of Jackson Lake Dam, the Signal Mountain Lodge is managed by Forever Resorts and provides cabins, a marina, a gas station and a restaurant. The American Alpine Club has hostel dormitory style accommodations primarily reserved for mountain climbers at the Grand Teton Climber's Ranch. Adjacent to the Snake River in Moose, Wyoming, Dornan's is an inholding on private land which has year-round cabin accommodations and related facilities. Lodging is also available at the Triangle X Ranch, another private inholding in the park and the last remaining dude ranch within park boundaries.

</doc>
<doc id="45475" url="http://en.wikipedia.org/wiki?curid=45475" title="Marfan syndrome">
Marfan syndrome

Marfan syndrome (also called Marfan's syndrome) is a genetic disorder of connective tissue. It has a variable clinical presentation, ranging from mild to severe systemic disease. The most serious manifestations involve defects of the heart valves and aorta, which may lead to early death if not properly managed. The syndrome also may affect the lungs, eyes, dural sac surrounding the spinal cord, the skeleton, and the hard palate. People with Marfan syndrome tend to be unusually tall, with long limbs and long, thin fingers and toes.
The syndrome is caused by the misfolding of fibrillin-1, a glycoprotein which forms elastic fibers in connective tissue and contributes to cell signaling activity by binding to and sequestering transforming growth factor beta (TGF-β). The mutated fibrillin binds poorly to TGF-β, which results in an accumulation of excess TGF-β in the lungs, heart valves, and aorta. This in turn causes abnormal structure and function of vascular smooth muscle and reduced integrity of the extracellular matrix, which weaken the tissues and cause the features of Marfan syndrome.
Named after Antoine Marfan, the French pediatrician who first described the condition in 1896, the disease is an autosomal dominant disorder. Management often includes the use of angiotensin II receptor antagonists (ARBs) and beta blockers.
Signs and symptoms.
More than 30 different signs and symptoms are variably associated with Marfan syndrome. The most prominent of these affect the skeletal system and are found in numerous other diseases (see Differential diagnosis, below). Therefore, distinguishing Marfan syndrome from other "marfanoid" syndromes requires the assessment of non-skeletal clinical and laboratory findings—especially of the eyes, aorta, and heart. Complicating the physical assessment of such persons, considerable clinical variability occurs within families carrying an identical DNA variant.
Skeletal system.
Most of the readily visible signs are associated with the skeletal system. Many individuals with Marfan syndrome grow to above-average height. Some have disproportionately long, slender limbs with thin, weak wrists and long fingers and toes. Besides affecting height and limb proportions, people with Marfan syndrome may have abnormal curvature of the spine, abnormal indentation or protrusion of the sternum, abnormal joint flexibility, a high palate, malocclusions, flat feet, hammer toes, stooped shoulders, and unexplained stretch marks on the skin. It can also cause pain in the joints, bones and muscles. Some people with Marfan have speech disorders resulting from symptomatic high palates and small jaws. Early osteoarthritis may occur. Other signs include limited range of motion in the hips due to the femoral head protruding into abnormally deep hip sockets.
Eyes.
In Marfan syndrome the health of the eye can be affected in many ways but the principal change is partial lens dislocation (the lens is shifted out of its normal position). This occurs because of weakness in the ciliary zonules, the connective tissue strands which suspend the lens within the eye. (The lens can be seen as a trampoline surface supported by the zonules as springs). Fibrillin 1 (FBN1) is a component of the zonules and mutations in FBN1 are found in most individuals diagnosed with Marfan syndrome. Those mutations weaken the zonules and cause them to stretch. The inferior zonules are most frequently stretched resulting in the lens shifting upwards and outwards but it can shift in other directions as well. Nearsightedness and blurred vision are common, but farsightedness can also result particularly if the lens is highly subluxed. Subluxation (partial dislocation) of the lens can be detected clinically in 80% of patients by the use of a slit-lamp biomicroscope. If the lens subluxation is subtle then imaging with high resolution ultrasound biomicroscopy might be used. Other problems are a greater risk of retinal detachment, an earlier onset of cataract formation and a higher risk of chronic open angle glaucoma. Rarely, the lens can dislocate through the pupillary opening to precipitate an ocular emergency, pupillary block glaucoma. The latter is rare but in the context of direct ocular trauma in a patient with Marfan syndrome it should be ruled out. 
Cardiovascular system.
The most serious signs and symptoms associated with Marfan syndrome involve the cardiovascular system: undue fatigue, shortness of breath, heart palpitations, racing heartbeats, or chest pain radiating to the back, shoulder, or arm. Cold arms, hands and feet can also be linked to Marfan syndrome because of inadequate circulation. A heart murmur, abnormal reading on an ECG, or symptoms of angina can indicate further investigation. The signs of regurgitation from prolapse of the mitral or aortic valves (which control the flow of blood through the heart) result from cystic medial degeneration of the valves, which is commonly associated with Marfan syndrome (see mitral valve prolapse, aortic regurgitation). However, the major sign that would lead a doctor to consider an underlying condition is a dilated aorta or an aortic aneurysm. Sometimes, no heart problems are apparent until the weakening of the connective tissue (cystic medial degeneration) in the ascending aorta causes an aortic aneurysm or aortic dissection, a surgical emergency. An aortic dissection is most often fatal and presents with pain radiating down the back, giving a tearing sensation.
Because underlying connective tissue abnormalities cause Marfan syndrome, there is an increased incidence of dehiscence of prosthetic mitral valve. Care should be taken to attempt repair of damaged heart valves rather than replacement.
During pregnancy, even in the absence of preconception cardiovascular abnormality, women with Marfan syndrome are at significant risk of aortic dissection, which is often fatal even when rapidly treated. Women with Marfan syndrome, then, should receive a thorough assessment prior to conception, and echocardiography should be performed every six to 10 weeks during pregnancy, to assess the aortic root diameter. For most women, safe vaginal delivery is possible.
Lungs.
Pulmonary symptoms are not a major feature of Marfan syndrome, but spontaneous pneumothorax is common. In spontaneous unilateral pneumothorax, air escapes from a lung and occupies the pleural space between the chest wall and a lung. The lung becomes partially compressed or collapsed. This can cause pain, shortness of breath, cyanosis, and, if not treated, it can cause death. Other possible pulmonary manifestations of Marfan syndrome include sleep apnea and idiopathic obstructive lung disease. Pathologic changes in the lungs have been described such as cystic changes, emphysema, pneumonia, bronchiectasis, bullae, apical fibrosis and congenital malformations such as middle lobe hypoplasia.
Central nervous system.
Dural ectasia, the weakening of the connective tissue of the dural sac encasing the spinal cord, can result in a loss of quality of life. It can be present for a long time without producing any noticeable symptoms. Symptoms that can occur are lower back pain, leg pain, abdominal pain, other neurological symptoms in the lower extremities, or headaches – symptoms which usually diminish when lying flat. On X-ray however dural ectasia is not often visible in the early stages. A worsening of symptoms might warrant an MRI of the lower spine. Dural ectasia that has progressed to this stage would appear in an MRI as a dilated pouch wearing away at the lumbar vertebrae. Other spinal issues associated with Marfan syndrome include degenerative disc disease, spinal cysts and dysfunction of the autonomic nervous system.
Pathogenesis.
Marfan syndrome is caused by mutations in the "FBN1" gene on chromosome 15, which encodes fibrillin-1, a glycoprotein component of the extracellular matrix. Fibrillin-1 is essential for the proper formation of the extracellular matrix, including the biogenesis and maintenance of elastic fibers. The extracellular matrix is critical for both the structural integrity of connective tissue, but also serves as a reservoir for growth factors. Elastic fibers are found throughout the body, but are particularly abundant in the aorta, ligaments and the ciliary zonules of the eye; consequently, these areas are among the worst affected. It can also be caused by a range of intravenous crystal treatments in those susceptible to the disorder.
A transgenic mouse has been created carrying a single copy of a mutant fibrillin-1, a mutation similar to that found in the human gene known to cause Marfan syndrome. This mouse strain recapitulates many of the features of the human disease and promises to provide insights into the pathogenesis of the disease. Reducing the level of normal fibrillin 1 causes a Marfan-related disease in mice.
Transforming growth factor beta (TGF-β) plays an important role in Marfan syndrome. Fibrillin-1 directly binds a latent form of TGF-β, keeping it sequestered and unable to exert its biological activity. The simplest model of Marfan syndrome suggests reduced levels of fibrillin-1 allow TGF-β levels to rise due to inadequate sequestration. Although it is not proven how elevated TGF-β levels are responsible for the specific pathology seen with the disease, an inflammatory reaction releasing proteases that slowly degrade the elastic fibers and other components of the extracellular matrix is known to occur. The importance of the TGF-β pathway was confirmed with the discovery of the similar Loeys-Dietz syndrome involving the "TGFβR2" gene on chromosome 3, a receptor protein of TGF-β. Marfan syndrome has often been confused with Loeys-Dietz syndrome, because of the considerable clinical overlap between the two pathologies.
Diagnosis.
Diagnostic criteria of Marfan syndrome were agreed upon internationally in 1996. A diagnosis of Marfan syndrome is based on family history and a combination of major and minor indicators of the disorder, rare in the general population, that occur in one individual — for example: four skeletal signs with one or more signs in another body system such as ocular and cardiovascular in one individual. The following conditions may result from Marfan syndrome, but may also occur in people without any known underlying disorder.
Revised Ghent Nosology.
According to The Marfan Foundation, in 2010 the Ghent Nosology was revised, and new diagnostic criteria superseded the previous agreement made in 1996. The seven new criteria can lead to a diagnosis:
In the absence of a family history of MFS:
In the presence of a family history of MFS (as defined above):
The thumb sign (Steinberg's sign) is elicited by asking the patient to flex the thumb as far as possible and then close the fingers over it. A positive thumb sign is where part of the thumb is visible beyond the ulnar border of the hand, caused by a combination of hypermobility of the thumb as well as a thumb which is longer than usual.
The wrist sign (Walker's sign) is elicited by asking the patient to curl the thumb and fingers of one hand around the other wrist. A positive wrist sign is where the little finger and the thumb overlap, caused by a combination of thin wrists and long fingers.
Differential diagnosis.
Many disorders have the potential to produce the same type of body habitus (i.e. shape) as Marfan syndrome. Distinguishing among these "marfanoid" disorders can be facilitated by genetic testing, and by evaluating signs and symptoms other than body habitus. Among the disorders capable of producing a marfanoid body habitus are:
Management.
There is no cure for Marfan syndrome, but life expectancy has increased significantly over the last few decades and is now similar to that of the average person. Clinical trials are also under way for promising new treatments. At present (2013), the syndrome is treated by simply addressing each issue as it arises and, in particular, preventative medication even for young children to slow progression of aortic dilation if such exists.
Since angiotensin II receptor antagonists (ARBs) also reduce TGF-β, these drugs have been tested in a small sample of young, severely affected people with Marfan syndrome. In some, the growth of the aorta was reduced. However, a recent study published in NEJM demonstrated similar cardiac outcomes between the ARB, losartan, and the more established beta blocker therapy, atenolol.
Marfan syndrome is expressed dominantly. This means a child with one parent a bearer of the gene has a 50% probability of getting the syndrome. However, as the gene causing Marfan syndrome is known, arduous genetic techniques are able to circumvent this. In 1996, the first preimplantation genetic testing therapy for Marfan was conducted; in essence PGT means conducting a genetic testing on early stage IVF embryo cells and discarding those embryos affected by the Marfan mutation.
Regular checkups by a cardiologist are needed to monitor the health of the heart valves and the aorta. The goal of treatment is to slow the progression of aortic dilation and damage to heart valves by eliminating arrythmias, minimizing the heart rate, and minimizing blood pressure. Beta blockers have been used to control arrythmias and slow the heart rate. Other medications might be needed to further minimize blood pressure without slowing the heart rate, such as ACE inhibitors and ARBs. If the dilation of the aorta progresses to a significant diameter aneurysm, causes a dissection or a rupture, or leads to failure of the aortic or other valve, then surgery (possibly a composite aortic valve graft or valve-sparing aortic root replacement) becomes necessary. Although aortic graft surgery (or any vascular surgery) is a serious undertaking it is generally successful if undertaken on an elective basis. Surgery in the setting of acute aortic dissection or rupture is considerably more problematic. Elective aortic valve/graft surgery is usually considered when aortic root diameter reaches 50 mm, but each case needs to be specifically evaluated by a qualified cardiologist. New valve-sparing surgical techniques are becoming more common. As Marfan patients live longer, other vascular repairs are becoming more common, e.g., repairs of descending thoractic aortic aneurysms and aneurysms of vessels other than the aorta.
The skeletal and ocular manifestations of Marfan syndrome can also be serious, although not life-threatening. These symptoms are usually treated in the typical manner for the appropriate condition, such as with various kinds of pain medication or muscle relaxants. It is also common for patients to receive treatment from a physiotherapist, using TENS therapy, ultrasound and skeletal adjustment. This can also affect height, arm length, and life span. A physiotherapist can also help improve function and prevent injuries in individuals with Marfan syndrome. The Nuss procedure is now being offered to people with Marfan syndrome to correct 'sunken chest' or (pectus excavatum). Because Marfan syndrome may cause asymptomatic spinal abnormalities, any spinal surgery contemplated on a Marfan patient should only follow detailed imaging and careful surgical planning, regardless of the indication for surgery.
Treatment of a spontaneous pneumothorax is dependent on the volume of air in the pleural space and the natural progression of the individual's condition. A small pneumothorax might resolve without active treatment in one to two weeks. Recurrent pneumothoraces might require chest surgery. Moderately sized pneumothoraces might need chest drain management for several days in a hospital. Large pneumothoraces are likely to be medical emergencies requiring emergency decompression.
Research in laboratory mice has suggested the losartan, an ARB which appears to block TGF-beta activity, can slow or halt the formation of aortic aneurysms in Marfan syndrome. A large clinical trial sponsored by the National Institutes of Health comparing the effects of losartan and atenolol on the aortas of Marfan patients was scheduled to begin in early 2007, coordinated by Johns Hopkins.
Prognosis.
Prior to modern cardiovascular surgical techniques and drugs such as losartan and metoprolol, the prognosis of those with Marfan's syndrome was not good: a range of untreatable cardiovascular issues was common. Lifespan was reduced by at least a third, and many died in their teens and twenties due to cardiovascular problems. Today, cardiovascular symptoms of Marfan syndrome are still the most significant issues in diagnosis and management of the disease, but adequate prophylactic monitoring and prophylactic therapy offers something approaching a normal lifespan, and more manifestations of the disease are being discovered as more patients live longer.
Epidemiology.
Marfan syndrome affects males and females equally, and the mutation shows no ethnic or geographical bias. Estimates indicate about one in 3,000 to 5,000 individuals have Marfan syndrome. Each parent with the condition has a 50% risk of passing the genetic defect on to any child due to its autosomal dominant nature. Most individuals with Marfan syndrome have another affected family member. Approximately 15–30% of all cases are due to "de novo" genetic mutations; such spontaneous mutations occur in about one in 20,000 births. Marfan syndrome is also an example of dominant negative mutation and haploinsufficiency. It is associated with variable expressivity; incomplete penetrance has not been definitively documented.
History.
Marfan syndrome is named after Antoine Marfan, the French pediatrician who first described the condition in 1896 after noticing striking features in a five-year-old girl. The gene linked to the disease was first identified by Francesco Ramirez at the Mount Sinai Medical Center in New York City in 1991.
Society and culture.
Contributors to public perception of Marfan syndrome include Flo Hyman, an Olympic silver medalist in women's volleyball (1984) who died suddenly at a match from an aortic dissection; Jonathan Larson, author and composer of "Rent", who also died from aortic dissection the day before the opening of the musical; Vincent Schiavelli, an actor and spokesperson for the The Marfan Foundation (then named the National Marfan Foundation), who had the syndrome but died from an unrelated cause, musician Bradford Cox from the indie band Deerhunter and Isaiah Austin, a basketball player who was diagnosed with it, and forced to give up his dream of an NBA career.
Studies suggest that Akhenaten, a Pharaoh of the eighteenth dynasty of Egypt, may have suffered from Marfan's syndrome. Abraham Lincoln was once thought to have had the disease, but that view has been discounted.

</doc>
<doc id="45479" url="http://en.wikipedia.org/wiki?curid=45479" title="Utility">
Utility

Utility, or usefulness, is the (perceived) ability of something to satisfy needs or wants. Utility is an important concept in economics and game theory, because it represents satisfaction experienced by the consumer of a good. Not coincidentally, a good is something that satisfies human wants and provides utility, for example, to a consumer making a purchase. It was recognized that one can not directly measure benefit, satisfaction or happiness from a good or service, so instead economists have devised ways of representing and measuring utility in terms of economic choices that can be counted. Economists have attempted to perfect highly abstract methods of comparing utilities by observing and calculating economic choices. In the simplest sense, economists consider utility to be revealed in people's willingness to pay different amounts for different goods (i.e. in prices; without conflating the two concepts).
Economic definitions.
In economics, utility is a representation of preferences over some set of goods and services. Preferences have a (continuous) utility representation so long as they are transitive, complete, and continuous.
Utility is usually applied by economists in such constructs as the indifference curve, which plot the combination of commodities that an individual or a society would accept to maintain a given level of satisfaction. Individual utility and social utility can be construed as the value of a utility function and a social welfare function respectively. When coupled with production or commodity constraints, under some assumptions, these functions can be used to analyze Pareto efficiency, such as illustrated by Edgeworth boxes in contract curves. Such efficiency is a central concept in welfare economics.
In finance, utility is applied to generate an individual's price for an asset called the indifference price. Utility functions are also related to risk measures, with the most common example being the entropic risk measure.There has been some controversy over the question whether the utility of a commodity can be measured or not. At one time, it was assumed that the consumer was able to say exactly how much utility he got from the commodity. The economists who made this assumption, belong to the 'Cardinalist School' (of Economics).
Quantifying utility.
It was recognized that utility could not be measured or observed directly, so instead economists devised a way to infer underlying relative utilities from observed choice. These 'revealed preferences', as they were named by Paul Samuelson, were revealed e.g. in people's willingness to pay: Utility is taken to be correlative to Desire or Want. It has been already argued that desires cannot be measured directly, but only indirectly, by the outward phenomena to which they give rise: and that in those cases with which economics is chiefly concerned the measure is found in the price which a person is willing to pay for the fulfilment or satisfaction of his desire.:78
Cardinal and ordinal utility.
Economists distinguish between cardinal utility and ordinal utility. When cardinal utility is used, the magnitude of utility differences is treated as an ethically or behaviorally significant quantity. On the other hand, ordinal utility captures only ranking and not strength of preferences.
Utility functions of both sorts assign a ranking to members of a choice set. For example, suppose a cup of orange juice has utility of 120 utils, a cup of tea has a utility of 80 utils, and a cup of water has a utility of 40 utils. When speaking of cardinal utility, it could be concluded that the cup of orange juice is better than the cup of tea by exactly the same amount by which the cup of tea is better than the cup of water. One is not entitled to conclude, however, that the cup of tea is two thirds as good as the cup of juice, because this conclusion would depend not only on magnitudes of utility differences, but also on the "zero" of utility.
It is tempting when dealing with cardinal utility to aggregate utilities across persons. The argument against this is that interpersonal comparisons of utility are meaningless because there is no simple way to interpret how different people value consumption bundles.
When ordinal utilities are used, differences in utils are treated as ethically or behaviorally meaningless: the utility index encode a full behavioral ordering between members of a choice set, but tells nothing about the related "strength of preferences". In the above example, it would only be possible to say that juice is preferred to tea to water, but no more.
Neoclassical economics has largely retreated from using cardinal utility functions as the basic objects of economic analysis, in favor of considering agent preferences over choice sets. However, preference relations can often be represented by utility functions satisfying several properties.
Ordinal utility functions are unique up to positive monotone transformations. For example, if a function formula_1 is taken as ordinal, it is equivalent to the function formula_2, because taking the 3rd power is a positive monotone transformation. This means that the ordinal preference induced by these functions is the same. In contrast, cardinal utilities are unique up to positive linear transformations, so if formula_1 is taken as cardinal, it is equivalent to formula_2.
Although preferences are the conventional foundation of microeconomics, it is often convenient to represent preferences with a utility function and analyze human behavior indirectly with utility functions. Let "X" be the consumption set, the set of all mutually-exclusive baskets the consumer could conceivably consume. The consumer's utility function formula_5 ranks each package in the consumption set. If the consumer strictly prefers "x" to "y" or is indifferent between them, then formula_6.
For example, suppose a consumer's consumption set is "X" = {nothing, 1 apple,1 orange, 1 apple and 1 orange, 2 apples, 2 oranges}, and its utility function is "u"(nothing) = 0, "u"(1 apple) = 1, "u"(1 orange) = 2, "u"(1 apple and 1 orange) = 4, "u"(2 apples) = 2 and "u"(2 oranges) = 3. Then this consumer prefers 1 orange to 1 apple, but prefers one of each to 2 oranges.
In microeconomic models, there are usually a finite set of L commodities, and a consumer may consume an arbitrary amount of each commodity. This gives a consumption set of formula_7, and each package formula_8 is a vector containing the amounts of each commodity. In the previous example, we might say there are two commodities: apples and oranges. If we say apples is the first commodity, and oranges the second, then the consumption set formula_9 and "u"(0, 0) = 0, "u"(1, 0) = 1, "u"(0, 1) = 2, "u"(1, 1) = 4, "u"(2, 0) = 2, "u"(0, 2) = 3 as before. Note that for "u" to be a utility function on "X", it must be defined for every package in "X".
A utility function formula_5 represents a preference relation formula_11 on X iff for every formula_12, formula_13 implies formula_14. If u represents formula_11, then this implies formula_11 is complete and transitive, and hence rational.
In order to simplify calculations, various assumptions have been made of utility functions.
Most utility functions used in modeling or theory are well-behaved. They are usually monotonic and quasi-concave. However, it is possible for preferences not to be representable by a utility function. An example is lexicographic preferences which are not continuous and cannot be represented by a continuous utility function.
Expected utility.
The expected utility theory deals with the analysis of choices among risky projects with (possibly multidimensional) outcomes.
The expected utility model was first proposed by Nicholas Bernoulli in 1713 and solved by Daniel Bernoulli in 1738 as the St. Petersburg paradox. Bernoulli argued that the paradox could be resolved if decisionmakers displayed risk aversion and argued for a logarithmic cardinal utility function.
The first important use of the expected utility theory was that of John von Neumann and Oskar Morgenstern, who used the assumption of expected utility maximization in their formulation of game theory.
Additive von Neumann–Morgenstern utility.
When comparing objects it makes sense to rank utilities, but older conceptions of utility allowed no way to compare the sizes of utilities — a person may say that a new shirt is preferable to a baloney sandwich, but not that it is twenty times preferable to the sandwich.
The reason is that the utility of twenty sandwiches is not twenty times the utility of one sandwich, by the law of diminishing returns. So it is hard to compare the utility of the shirt with 'twenty times the utility of the sandwich'. But Von Neumann and Morgenstern suggested an unambiguous way of making a comparison like this.
Their method of comparison involves considering probabilities. If a person can choose between various randomized events (lotteries), then it is possible to "additively" compare the shirt and the sandwich. It is possible to compare "a sandwich with probability" 1, to "a shirt with probability p or nothing with probability" 1 − "p". By adjusting "p", the point at which the sandwich becomes preferable defines the ratio of the utilities of the two options.
A notation for a "lottery" is as follows: if options A and B have probability "p" and 1 − "p" in the lottery, write it as a linear combination:
More generally, for a lottery with many possible options:
where formula_19.
By making some reasonable assumptions about the way choices behave, von Neumann and Morgenstern showed that if an agent can choose between the lotteries, then this agent has a utility function which can be added and multiplied by real numbers, which means the utility of an arbitrary lottery can be calculated as a linear combination of the utility of its parts.
This is called the "expected utility theorem". The required assumptions are four axioms about the properties of the agent's preference relation over 'simple lotteries', which are lotteries with just two options. Writing formula_20 to mean 'A is weakly preferred to B' ('A is preferred at least as much as B'), the axioms are:
Axioms 3 and 4 enable us to decide about the relative utilities of two assets or lotteries.
In more formal language: A von Neumann–Morgenstern utility function is a function from choices to the real numbers:
which assigns a real number to every outcome in a way that captures the agent's preferences over simple lotteries. Under the four assumptions mentioned above, the agent will prefer a lottery formula_45 to a lottery formula_46 if and only if the expected utility of formula_45 is greater than the expected utility of formula_46:
Repeating in category language: formula_50 is a morphism between the category of preferences with uncertainty and the category of reals as an additive group.
Of all the axioms, independence is the most often discarded. A variety of generalized expected utility theories have arisen, most of which drop or relax the independence axiom.
Money.
One of the most common uses of a utility function, especially in economics, is the utility of money. The utility function for money is a nonlinear function that is bounded and asymmetric about the origin. These properties can be derived from reasonable assumptions that are generally accepted by economists and decision theorists, especially proponents of rational choice theory. The utility function is concave in the positive region, reflecting the phenomenon of diminishing marginal utility. The boundedness reflects the fact that beyond a certain point money ceases being useful at all, as the size of any economy at any point in time is itself bounded. The asymmetry about the origin reflects the fact that gaining and losing money can have radically different implications both for individuals and businesses. The nonlinearity of the utility function for money has profound implications in decision making processes: in situations where outcomes of choices influence utility through gains or losses of money, which are the norm in most business settings, the optimal choice for a given decision depends on the possible outcomes of all other decisions in the same time-period.
Utility as probability of success.
Castagnoli and LiCalzi and Bordley and LiCalzi (2000) provided another interpretation for Von Neumann and Morgenstern's theory. Specifically for any utility function, there exists a hypothetical reference lottery with the utility of a lottery being its probability of performing no worse than the reference lottery. Suppose success is defined as getting an outcome no worse than the outcome of the reference lottery. Then this mathematical equivalence means that maximizing expected utility is equivalent to maximizing the probability of success. In many contexts, this makes the concept of utility easier to justify and to apply. For example, a firm's utility might be the probability of meeting uncertain future customer expectations.
Discussion and criticism.
Cambridge economist Joan Robinson famously criticized utility for being a circular concept: "Utility is the quality in commodities that makes individuals want to buy them, and the fact that individuals want to buy commodities shows that they have utility":48 Robinson also pointed out that because the theory assumes that preferences are fixed this means that utility is not a testable assumption. This is because if we take changes in peoples' behavior in relation to a change in prices or a change in the underlying budget constraint we can never be sure to what extent the change in behavior was due to the change in price or budget constraint and how much was due to a change in preferences. This criticism is similar to that of the philosopher Hans Albert who argued that the ceteris paribus conditions on which the marginalist theory of demand rested on rendered the theory itself an empty tautology and completely closed to experimental testing. In essence, demand and supply curve (theoretical line of quantity of a product which would have been offered or requested for given price) is purely ontological and could never been demonstrated empirically.
Another criticism comes from the assertion that neither cardinal nor ordinal utility is empirically observable in the real world. In the case of cardinal utility it is impossible to measure the level of satisfaction "quantitatively" when someone consumes or purchases an apple. In case of ordinal utility, it is impossible to determine what choices were made when someone purchases, for example, an orange. Any act would involve preference over a vast set of choices (such as apple, orange juice, other vegetable, vitamin C tablets, exercise, not purchasing, etc.).
Other questions of what arguments ought to enter into a utility function are difficult to answer, yet seem necessary to understanding utility. Whether people gain utility from coherence of wants, beliefs or a sense of duty is key to understanding their behavior in the utility organon. Likewise, choosing between alternatives is itself a process of determining what to consider as alternatives, a question of choice within uncertainty.
An evolutionary psychology perspective is that utility may be better viewed as due to preferences that maximized evolutionary fitness in the ancestral environment but not necessarily in the current one.

</doc>
<doc id="45481" url="http://en.wikipedia.org/wiki?curid=45481" title="Soylent Green">
Soylent Green

Soylent Green is a 1973 American science fiction film directed by Richard Fleischer and starring Charlton Heston, Leigh Taylor-Young, and, in his final film, Edward G. Robinson. The film combines the police procedural and science fiction genres, depicting the investigation into the murder of a wealthy businessman in a dystopian future suffering from pollution, overpopulation, depleted resources, poverty, dying oceans, and all year humidity due to the greenhouse effect. Much of the population survives on processed food rations, including "soylent green".
The film, which is loosely based upon the 1966 science fiction novel "Make Room! Make Room!" by Harry Harrison, won the Nebula Award for Best Dramatic Presentation and the Saturn Award for Best Science Fiction Film in 1973.
Plot.
The 20th century's industrialization has left the world permanently overcrowded, polluted and stagnant by the turn of the 21st century. In 2022, with 40 million people in New York City alone, housing is dilapidated and overcrowded; homeless people fill the streets; about half are unemployed, the few "lucky" ones with jobs are only barely scraping by themselves, and food and working technology is scarce. Most of the population survives on rations produced by the Soylent Corporation, whose newest product is Soylent Green, a green wafer advertised to contain "high-energy plankton" from the world's oceans, more nutritious and palatable than its predecessors "Red" and "Yellow", but in short supply.
New York City Police Department detective Frank Thorn (Charlton Heston) lives with his aged friend Solomon "Sol" Roth (Edward G. Robinson). Due to Roth's advanced age, he remembers life before its current miserable state, and he routinely waxes nostalgic for his youth when the air was clean and the weather wasn't perpetually summer. He was also well educated and has a small library of reference materials which he uses to help Det. Thorn solve crimes (consequently Roth is referred to as a police "book"). While investigating the murder of William R. Simonson (Joseph Cotten), obviously a member of the wealthy elite, Thorn questions Shirl (Leigh Taylor-Young), a concubine (referred to as "furniture"), and Tab Fielding (Chuck Connors), Simonson's bodyguard, who, when the murder took place, was escorting Shirl to a store selling meat "under the counter" for Simonson. Thorn searches Simonson's apartment for clues and enjoys Simonson's luxurious lifestyle like air conditioning and hot running water, and he helps himself to Simonson's real bourbon, fresh vegetables, and a flank steak that Shirl had purchased earlier as a special surprise for Simonson. 
Thorn later gives Roth the classified "Soylent Oceanographic Survey Report, 2015 to 2019" found in Simonson's apartment. Roth's research reveals that Simonson and the current state governor of New York, Joseph Santini (Whit Bissell), were partners in a well-known high-powered law firm, and that Simonson was also a member of the Board of Soylent.
At the police station, Thorn tells his lieutenant, Hatcher (Brock Peters), that he suspects an assassination: nothing was stolen from the apartment, its sophisticated alarm was not working for the first time in two years, and Simonson's bodyguard was conveniently absent. Continuing his investigation, Thorn visits Fielding's apartment and questions Fielding's concubine, Martha (Paula Kelly), helping himself to a teaspoon of strawberry jam, later identified by Roth as too great a luxury for the concubine of a bodyguard to afford.
Under questioning, Shirl reveals that Simonson became troubled in the days before his death. Thorn questions a Catholic priest that Simonson had visited, but the priest at first fails to remember Simonson and is later unable to describe the confession. Fielding later murders the priest to silence him.
Meanwhile, Governor Santini orders the investigation closed, but Thorn disobeys and the Soylent Corporation dispatches Simonson's murderer to kill Thorn. He tracks Thorn to a ration distribution center where police officers are providing security. When the Soylent Green there is exhausted, the crowd riots. The assassin tries to kill Thorn in the confusion, but is crushed by a "scoop" crowd-dispersion vehicle. Thorn then threatens both Fielding and Martha to scare Fielding out of following him and returns to Shirl, telling her that all cities are like theirs and the more valuable, unharmed countryside is guarded to protect the wealthier classes' privileges of better food, water and shelter, leaving the majority of people trapped in the cities with no escape.
Roth takes Soylent's oceanographic reports to a like-minded group of researchers known as the Exchange, who agree that the oceans no longer produce the plankton from which Soylent Green is reputedly made, and infer that it must be made from human remains, as this is the only conceivable supply of protein that matches the known production. Unable to live with this discovery, Roth seeks assisted suicide at a government clinic called "Home."
Thorn rushes to stop him, but arrives too late, and is mesmerized by the euthanasia process's visual and musical montage – a display of forests, wild animals, rivers, and ocean life, now extinct. Under the influence of a lethal drug, Roth tells Thorn his discovery and begs him to expose the truth. To this end, Thorn stows himself aboard a garbage truck to the disposal center, where he sees human corpses converted into Soylent Green. Returning to make his report, he is ambushed by Fielding and others.
He phones his precinct for backup but the precinct is engaged on a priority call. Thorn asks to be connected with Shirl, and to be "cut in" when the precinct is free. Thorn tells Shirl to stay with her apartment's new owner, and Shirl tells Thorn she wants to live with him, but the line is "cut in" and Thorn is connected to Hatcher. Thorn retreats into a cathedral filled with homeless people. In the ensuing fight, he kills Fielding but is seriously injured. When the police arrive, Thorn urges Hatcher to spread the word that "Soylent Green is people!".
Production.
The screenplay was based on Harry Harrison's novel "Make Room! Make Room!" (1966), which is set in the year 1999 with the theme of overpopulation and overuse of resources leading to increasing poverty, food shortages, and social disorder. Harrison was contractually forbidden control over the screenplay and kept from knowing during negotiations that it was MGM buying the film rights. He discussed the adaptation in "Omni's Screen Flights/Screen Fantasies" (1984), noting, the "murder and chase sequences [and] the 'furniture' girls are not what the film is about — and are completely irrelevant", and answered his own question, "Am I pleased with the film? I would say fifty percent".
While the book refers to "soylent steaks", it makes no reference to "Soylent Green", the processed food rations depicted in the film. The book's title was not used for the movie on grounds that it might have confused audiences into thinking it a big-screen version of "Make Room for Daddy".
This was the 101st and last movie in which Edward G. Robinson appeared; he died of cancer twelve days after the filming, on January 26, 1973. Robinson had previously worked with Heston in "The Ten Commandments" (1956) and the make-up tests for "Planet of the Apes" (1968). In his book "The Actor's Life: Journal 1956-1976", Heston wrote "He knew while we were shooting, though we did not, that he was terminally ill. He never missed an hour of work, nor was late to a call. He never was less than the consummate professional he had been all his life. I'm still haunted, though, by the knowledge that the very last scene he played in the picture, which he knew was the last day's acting he would ever do, was his death scene. I know why I was so overwhelmingly moved playing it with him."
The film's opening sequence, depicting America becoming more crowded with a series of archive photographs set to music, was created by filmmaker Charles Braverman. The "going home" score in Roth's death scene was conducted by Gerald Fried and consists of the main themes from Symphony No. 6 ("Pathétique") by Tchaikovsky, Symphony No. 6 ("Pastoral") by Beethoven, and the "Peer Gynt Suite" ("Morning Mood" and "Åse's Death") by Edvard Grieg.
Critical response.
The film was released April 19, 1973. "TIME" called it "intermittently interesting"; they note that "Heston forsak[es] his granite stoicism for once" and assert the film "will be most remembered for the last appearance of Edward G. Robinson... In a rueful irony, his death scene, in which he is hygienically dispatched with the help of piped-in light classical music and movies of rich fields flashed before him on a towering screen, is the best in the film." "New York Times" critic A.H. Weiler wrote ""Soylent Green" projects essentially simple, muscular melodrama a good deal more effectively than it does the potential of man's seemingly witless destruction of the Earth's resources"; Weiler concludes "Richard Fleischer's direction stresses action, not nuances of meaning or characterization. Mr. Robinson is pitiably natural as the realistic, sensitive oldster facing the futility of living in dying surroundings. But Mr. Heston is simply a rough cop chasing standard bad guys. Their 21st-century New York occasionally is frightening but it is rarely convincingly real."
As of August 2013, "Soylent Green" has a 71% rating on Rotten Tomatoes, based on 34 reviews.
Awards and honors.
American Film Institute Lists
Home video.
"Soylent Green" was released on laserdisc by MGM/UA in 1992 (ISBN 0792813995, OCLC 31684584). In November 2007, Warner Home Video released the film on DVD concurrent with the DVD releases of two other science fiction films; "Logan's Run" (1976) and "Outland" (1981). A Blu-ray Disc release followed on March 29, 2011.
References.
Notes

</doc>
<doc id="45488" url="http://en.wikipedia.org/wiki?curid=45488" title="The Last Laugh">
The Last Laugh

The Last Laugh (German: "Der letzte Mann (The Last Man)") is a German 1924 silent film directed by German director F. W. Murnau from a screenplay written by Carl Mayer. The film stars Emil Jannings and Maly Delschaft. It is the most famous example of the short-lived "Kammerspielfilm" or "chamber-drama" genre. It is noted for its near-absence of the intertitles that characterize most silent films; moreover, none of the intertitles in "The Last Laugh" represent spoken dialogue. In 1955 the film was remade starring Hans Albers.
Plot.
Jannings' character, the doorman for a famous hotel, is demoted to washroom (restroom) attendant, as he is considered too old and infirm to be the image of the hotel. He tries to conceal his demotion from his friends and family, but to his shame, he is discovered. His friends, thinking he has lied to them all along about his prestigious job, taunt him mercilessly while his family rejects him out of shame. The man, shocked and in incredible grief, returns to the hotel to sleep in the washroom where he works. The only person to be kind towards him is the night watchman, who covers him with his coat as he falls asleep.
Following this comes the film's only title card, which says: "Here the story should really end, for, in real life, the forlorn old man would have little to look forward to but death. The author took pity on him and has provided a quite improbable epilogue."
At the end, the doorman reads in the newspaper that he inherited a fortune from a Mexican millionaire named U. G. Monen, a patron who died in his arms in the hotel washroom. Jannings returns to the hotel, where he dines happily with the night watchman who showed him kindness. It is this ending that inspires the English language title.
Murnau noted that the story was absurd on the grounds that "everyone knows that a washroom attendant makes more than a doorman."
Production.
Director F. W. Murnau was at the height of his film career in Germany and had high ambitions for his first film with UFA. He stated that "All our efforts must be directed towards abstracting everything that isn't the true domain of the cinema. Everything that is trivial and acquired from other sources, all the tricks, devices and cliches inheirited from the stage and from books." Murnau called screenwriter Carl Mayer someone who worked in "the true domain of the cinema" and agreed to make "The Last Laugh" after Mayer and film director Lupu Pick fought and Pick left the film. The film famously uses no intertitles, which had previously been done by Mayer and Pick on "Scherben" and "Sylvester" several years earlier, as well as by director Arthur Robinson in the film "Schatten" in 1923.
The film was shot entirely at the UFA Studios. Murnau and cinematographer Karl Freund used elaborate camera movements for the film, a technique later called "entfesslte Kamera" ("unchained camera"). In one scene a camera was strapped to Freund's chest as he rode a bicycle into an elevator and onto the street below. In another scene a camera is sent down a wire from a window to the street below, and later reversed in editing. French filmmaker Marcel Carne later said that "The camera...glides, rises, zooms or weaves where the story takes it. It is no longer fixed, but takes part in the action and becomes a character in the drama." Years later Karl Freund dismissed Murnau's contributions to the films that they made together, claiming that Murnau had no interest in lighting and never looked through the camera, and that "Carl Mayer used to take much more interest than he did in framing." The film's set designers Robert Herlth and Walter Rohrig denied this statement and defended Murnau. Murnau described the films cinematography as being "on account of the way...[objects] were placed or photographed, their image is a visual drama. In their relationship with other objects or with the characters, they are units in the symphony of the film."
Reception and legacy.
The film was a major critical and financial success and allowed Murnau to make two big budget films shortly afterwards. Critics praised the film's style and artistic camera movements. Film critic Paul Rotha said that it "definitely established the film as an independent medium of expression...Everything that had to be said...was said entirely through the camera..."The Last Laugh" was cine-fiction in its purest form; exemplary of the rhythmic composition proper to the film." Years later C. A. Lejeune called it "probably the least sensational and certainly the most important of Murnau's films. It gave the camera a new dominion, a new freedom...It influenced the future of motion picture photography...all over the world, and without suggesting any revolution in method, without storming critical opinion as "Caligari" had done, it turned technical attention towards experiment, and stimulated...a new kind of camera-thinking with a definite narrative end. Lotte Eisner praised its "opalescent surfaces streaming with reflections, rain, or light: car windows, the glazed leaves of the revolving door reflecting the silhouette of the doorman dressed in a gleaming black waterproof, the dark moss of houses with lighted windows, wet pavements and shimmering puddles...His camera captures the filtered half-light falling from the street lamps...it seizes railings through basement windows."
The film's story and content were also praised by critics, with Eisner stating that it "is preeminanently a German tragedy, and can only be understood in a country where uniform is king, not to say god. A non-German mind will have difficulty in comprehending all its tragic implications." Siegfried Kracauer pointed out that "all the tenants, in particular the female ones...[revere the uniform] as a symbol of supreme authority and are happy to be allowed to revere it."
In 2000, Roger Ebert included it among his list of Great Movies.

</doc>
<doc id="45491" url="http://en.wikipedia.org/wiki?curid=45491" title="Life After Life (book)">
Life After Life (book)

Life After Life is a 1975 book written by psychiatrist Raymond Moody. It is a report on a qualitative study in which Moody interviewed 150 people who had undergone near-death experiences (NDEs). The book presents the author's composite account of what it is like to die. On the basis of his collection of cases, Moody identified a common set of elements in NDEs:
"Life After Life" sold more than 13 million copies, was translated into a dozen foreign languages and became an international best seller, which made the subject of NDEs popular and opened the way for many other studies.
Reception.
Scientists have written that Moody's alleged evidence for an afterlife is flawed, logically and empirically. The psychologist James Alcock has noted that "[Moody] appears to ignore a great deal of the scientific literature dealing with hallucinatory experiences in general, just as he quickly glosses over the very real limitations of his research method."
The philosopher Paul Kurtz has written that Moody's evidence for the NDE is based on personal interviews and anecdotal accounts and there has been no statistical analyses of his data. According to Kurtz "there is no reliable evidence that people who report such experiences have died
and returned, or that consciousness exists separate from the brain or body."

</doc>
<doc id="45492" url="http://en.wikipedia.org/wiki?curid=45492" title="I Am Legend (novel)">
I Am Legend (novel)

I Am Legend is a 1954 horror fiction novel by American writer Richard Matheson. It was influential in the development of the zombie genre and in popularizing the concept of a worldwide apocalypse due to disease. The novel was a success and was adapted to film as "The Last Man on Earth" in 1964, as "The Omega Man" in 1971, and as "I Am Legend" in 2007, along with a direct-to-video 2007 production capitalizing on that film, "I Am Omega". The novel was also the inspiration behind the 1968 film "Night of the Living Dead".
Plot summary.
Robert Neville is the apparent sole survivor of a pandemic whose symptoms resemble vampirism. It is said that the pandemic was caused by a war, and that it was spread by dust storms in the cities and an explosion in the mosquito population. The narrative details Neville's daily life in Los Angeles as he attempts to comprehend, research, and possibly cure the disease, to which he is immune. Neville's past is revealed through flashbacks; the disease claimed his wife and daughter, and he was forced to kill his wife after she seemingly rose from the dead as a vampire and attacked him.
Neville survives by barricading himself by sunset inside his house, further protected by garlic, mirrors, and crucifixes. Swarms of vampires, led by Neville's neighbor, Ben Cortman, regularly surround his house, trying to find ways to get inside. During the day, he scavenges for supplies and searches out the inactive vampires, driving stakes into their hearts to kill them. He finds brief solace in a stray dog that finds its way to his house. Desperate for company, Neville slowly earns the dog's trust with food and brings it into the house. Despite his efforts, the dog proves to be infected and dies a week later.
After bouts of depression and alcoholism, Neville decides to find out the scientific cause of the pandemic. He obtains books and other research materials from a library, and through painstaking research discovers the root of the disease in a strain of bacteria capable of infecting both deceased and living hosts. He also discovers that the vampires are affected by the garlic, mirrors, and crosses because of "hysterical blindness", the result of previous psychological conditioning of the infected. Driven insane by the disease, the infected now react as they believe they should when confronted with these items. Even then, their reaction is constrained to the beliefs of the particular person; for example, a Christian vampire would fear the cross, but a Jewish vampire would not.
Neville also discovers more efficient means of killing the vampires, other than just driving a stake into their hearts. This includes exposing them to direct sunlight (which kills the bacteria) or inflicting deep wounds on their bodies so that the bacteria switch from being anaerobic symbionts to aerobic parasites, rapidly consuming their hosts when exposed to air. He is now killing such large numbers of vampires in his daily forays that his nightly visitors have diminished significantly.
After three years, Neville sees an apparently uninfected woman, Ruth, abroad in the daylight, and captures her. After some convincing, Ruth tells him her story of how she and her husband survived the pandemic (though her husband was killed two weeks earlier). Neville is puzzled by the fact that she is upset when he speaks of killing vampires; he thinks that if her story of survival was true, she would have become hardened to the act. He attempts to test whether she is a vampire by exposing her to garlic, which causes her to recoil violently. At night Neville is startled awake and finds Ruth fully clothed at the front door of the house. Suspicious, he questions her motives, but relates the trauma of his past, whereupon they comfort each other. Ruth reluctantly allows him to take a blood sample but knocks him unconscious when the sample reveals that she is infected.
When he wakes, Neville discovers a note from Ruth confessing that she is actually infected and that Neville was responsible for her husband's death. Ruth admits that she was sent to spy on him. The infected have slowly overcome their disease until they can spend short periods of time in sunlight, and are attempting to build a new society. They have developed medication which helps them to overcome the most severe symptoms of the infection. Ruth warns Neville that her people will attempt to capture him, and that he should leave his house and escape to the mountains.
Neville cannot bring himself to leave his house, however, and assumes that he will be captured and treated fairly by the new society. Infected members of the new society eventually attack the house. During the attack, the members of the new society violently dispatch the other vampires outside the house, and Neville becomes alarmed at the grim enjoyment they appear to take from this task. Realising that the intention of the attackers may be to kill him rather than to capture him he tries to defend himself with a pistol, leading to one of the infected shooting and badly injuring him.
Neville wakes in a barred cell where he is visited by Ruth, who informs him that she is a ranking member of the new society but, unlike the others, does not resent him. Ruth attempts to present a facade of indifference to Neville, but is unable to maintain it during her discussion with him. After discussing the effects of Neville's vampire killing activities on the new society, she acknowledges the need for Neville's execution and gives him pills, claiming they will "make it easier". Badly injured, Neville accepts his fate and asks Ruth not to let this society become heartless. Ruth kisses him and leaves.
Neville goes to his prison window and sees the infected waiting for his execution. He now sees that the infected view him with the same hatred and fear that he once felt for the vampires; he realizes that he, a remnant of old humanity, is now a legend to the new race born of the infection. He recognises that their desire to kill him is not something he can condemn. As the pills take effect, he thinks: "[I am] a new superstition entering the unassailable fortress of forever. I am legend".
Critical reception.
As related from "In Search of Wonder" (1956), Damon Knight wrote:
The book is full of good ideas, every other one of which is immediately dropped and kicked out of sight. The characters are child's drawings, as blank-eyed and expressionless as the author himself in his back-cover photograph. The plot limps. All the same, the story could have been an admirable minor work in the tradition of "Dracula", if only the author, or somebody, had not insisted on encumbering it with the year's most childish set of 'scientific' rationalizations.
"Galaxy" reviewer Groff Conklin described "Legend" as "a weird [and] rather slow-moving first novel ... a horrid, violent, sometimes exciting but too often overdone tour de force." Anthony Boucher praised the novel, saying "Matheson has added a new variant on the Last Man theme ... and has given striking vigor to his invention by a forceful style of storytelling which derives from the best hard-boiled crime novels".
Dan Schneider from "International Writers Magazine: Book Review" wrote in 2005:
... despite having vampires in it, [the novel] is not a novel on vampires, nor even a horror nor sci-fi novel at all, in the deepest sense. Instead, it is perhaps the greatest novel written on human loneliness. It far surpasses Daniel Defoe’s "Robinson Crusoe" in that regard. Its insights into what it is to be human go far beyond genre, and is all the more surprising because, having read his short stories--which range from competent but simplistic, to having classic "Twilight Zone" twists (he was a major contributor to the original TV series)--there is nothing within those short stories that suggests the supreme majesty of the existential masterpiece "I Am Legend" was aborning.
In 2012, the Horror Writers Association gave "I Am Legend" the special Vampire Novel of the Century Award.
Influence.
Although Matheson calls the assailants in his novel "vampires", and though their condition is transmitted through blood and garlic is an apotropaic-like repellant, there is little similarity between them and vampires as developed by John William Polidori and his successors, which come straight out of the gothic novel tradition. "I Am Legend" influenced the zombie genre and popularized the concept of a worldwide zombie apocalypse. Although the idea has now become commonplace, a scientific origin for vampirism or zombies was fairly original when written. According to Clasen:
""I Am Legend" is the product of an anxious artistic mind working in an anxious cultural climate. However, it is also a playful take on an old archetype, the vampire (the reader is even treated to Neville’s reading and put-down of Bram Stoker's "Dracula"). Matheson goes to great lengths to rationalize or naturalize the vampire myth, transplanting the monster from the otherworldly realms of folklore and Victorian supernaturalism to the test tube of medical inquiry and rational causation. With "I Am Legend", Matheson instituted the germ theory of vampirism, a take on the old archetype which has since been tackled by other writers (notably, Dan Simmons in "Children of the Night" from 1992)."
Though referred to as "the first modern vampire novel", it is as a novel of social theme that "I Am Legend" made a lasting impression on the cinematic zombie genre, by way of director George A. Romero, who acknowledged its influence and that of its 1964 adaptation, "The Last Man on Earth", upon his seminal film "Night of the Living Dead" (1968). Discussing the creation of "Night of the Living Dead", Romero remarked, "I had written a short story, which I basically had ripped off from a Richard Matheson novel called "I Am Legend"." Moreover, film critics noted similarities between "Night of the Living Dead" (1968) and "The Last Man on Earth" (1964).
Stephen King said, "Books like "I Am Legend" were an inspiration to me". Film critics noted that the British film "28 Days Later" (2002) and its sequel "28 Weeks Later" both feature a rabies-type plague ravaging Great Britain, analogous to "I Am Legend".
Adaptations.
Comics.
The book has also been adapted into a comic book mini-series titled "Richard Matheson's I Am Legend" by Steve Niles and Elman Brown. It was published in 1991 by Eclipse Comics and collected into a trade paperback by IDW Publishing.
An unrelated film tie in was released in 2007 as a one-shot "I Am Legend: Awakening" published in a San Diego Comic Con special by Vertigo.
Radio play.
A nine-part abridged reading of the novel performed by Angus MacInnes was originally broadcast on BBC Radio 7 in January 2006.
Films.
"I Am Legend" has been adapted to a feature-length film four times (one of which does not credit Matheson as the source). Differing from the book each of them portrays the Neville character as an accomplished scientist. The first three adaptations show him finding a remedy and passing it on. Adaptations differ from the novel by setting the events after three years from the disaster, instead of happening “in the span of” three years. Also adaptations are set in the near future, a few years after film release, while the novel is set twenty years after its publication date.
"The Last Man on Earth".
In 1964, Vincent Price starred as Dr. Robert Morgan (rather than "Neville") in "The Last Man on Earth" (the original title of this Italian production was "L'ultimo uomo della Terra"). Matheson wrote the original screenplay for this adaptation, but due to later rewrites did not wish his name to appear in the credits; as a result, Matheson is credited under the pseudonym "Logan Swanson."
"The Omega Man".
In 1971, a far different version was produced, entitled "The Omega Man". It starred Charlton Heston (as Robert Neville) and Anthony Zerbe. Matheson had no influence on the screenplay for this film, and although the general premise remains, it deviates from the novel in several ways, completely removing the infected's vampiric characteristics except for their sensitivity to light. In this version, the infected are portrayed as nocturnal, black-robed, albino mutants, collectively known as The Family. Though intelligent, they eschew all modern technology, believing it (and those who use it, such as Neville) to be evil and the cause of humanity's downfall.
"I Am Legend".
In 2007, a third adaptation of the novel was produced, this time titled "I Am Legend". Directed by Francis Lawrence and starring Will Smith as Robert Neville, this film uses both Matheson's novel and the 1971 "Omega Man" film as its source. However, this adaptation also deviates significantly from the novel. In this version, the infection is caused by a virus originally intended to cure cancer. Some vampiric elements are retained, such as sensitivity to UV light and attraction to blood. The infected are portrayed as nocturnal, feral creatures of limited intelligence but with superhuman strength who feed on the uninfected. Other creatures, such as dogs, are also infected by the virus. The ending of the film was also altered to portray Neville as sacrificing his life to save humanity, rather than being executed for crimes against the surviving vampiric humans. The film takes place in New York City in the years 2009 and 2012 rather than Los Angeles in 1975-1977.
"I Am Omega".
The Asylum production "I Am Omega" was 2007 American feature length direct to video release, starring Mark Dacascos. The film takes place in a post-apocalyptic Los Angeles, which is overrun by savage, cannibalistic humans who have degenerated into a feral subspecies as the result of a genetic virus. Once again, the adaption deviates from the novel, and does not credit Matheson.
In this adaption, 'Renchard' has been forced to live in a daily struggle for survival against the mutants. Renchard is contacted via webcam by Brianna (Jennifer Lee Wiggins), another survivor who was stranded in Los Angeles while trying to find Antioch, a community of survivors. Renchard is forced to aid her and two others escape the city in which he has strategically placed time bombs, set to go off in 24 hours.
This film was rushed into production by The Asylum and released a month prior to the bigger budget Francis Lawrence project.

</doc>
<doc id="45493" url="http://en.wikipedia.org/wiki?curid=45493" title="Kenneth Ring">
Kenneth Ring

Kenneth Ring (born 1936) is Professor Emeritus of psychology at the University of Connecticut, and a researcher within the field of near-death studies.
Biography.
Ring is the co-founder and past president of the International Association for Near-Death Studies (IANDS) and is the founding editor of the "Journal of Near-Death Studies".
Ring was born in San Francisco, California and currently lives in Kentfield, California. In November 2008, Ring visited Israel as part of a peace delegation and subsequently protested the Israeli air strikes on the Gaza Strip as completely disproportionate.
Ring's book "Life at Death" was published by William Morrow and Company in 1980. In 1984, the company published Ring's second book, "Heading Toward Omega." Both books deal with near-death experiences and how they change people's lives. Other books by Ring include "The Omega Project: Near-Death Experiences, Ufo Encounters, and Mind at Large" (1992), "Mindsight: Near-death and out-of-body experiences in the blind" (1999) and "Lessons from the Light" (2000). He is also the coauthor of "Methods of Madness: The Mental Hospital as a Last Resort".

</doc>
<doc id="45497" url="http://en.wikipedia.org/wiki?curid=45497" title="Dennis Bergkamp">
Dennis Bergkamp

Dennis Nicolaas Maria Bergkamp (]; born 10 May 1969) is a Dutch former professional footballer, who is the assistant manager to Frank de Boer at Ajax. Originally a wide midfielder, Bergkamp was moved to main striker and then to second striker, where he remained throughout his playing career. Bergkamp has been described by Jan Mulder as having "the finest technique" of any Dutch international and a "dream for a striker" by teammate Thierry Henry.
The son of an electrician, Bergkamp was born in Amsterdam and played as an amateur in the lower leagues. He was spotted by Ajax at age 11 and made his professional debut in 1986. Good form led to an international call-up a year later, attracting the attention of several European clubs. Bergkamp signed for Italian club Internazionale in 1993, where he had two disappointing seasons. After joining Arsenal in 1995, he rejuvenated his career, helping the club to win three Premier League titles, four FA Cup trophies, and reach the 2006 UEFA Champions League Final, which marked his last appearance as a player. With the Netherlands national team, Bergkamp surpassed Faas Wilkes's record to become the country's top goalscorer of all time in 1998, a record later eclipsed by Patrick Kluivert and Robin van Persie.
Widely regarded as one of the greatest players in his generation, Bergkamp twice finished third in the FIFA World Player of the Year award and was selected by Pelé as one of the FIFA 100 greatest living players. In 2007, he was inducted into the English Football Hall of Fame, the first and so far only Dutch player ever to receive such honour. Due to his fear of flying, Bergkamp has been affectionately nicknamed the "Non-Flying Dutchman" by Arsenal supporters.
Early life.
Born in Amsterdam, Bergkamp was the last of Wim and Tonnie Bergkamp's four sons. He was brought up in a working-class suburb, in a family aspiring to reach middle-class status. His father, an electrician and amateur footballer in the lower leagues, named him in honour of Scottish striker Denis Law. To comply with Dutch given name customs, an extra "n" was inserted in Bergkamp's first name by his father after it was not accepted by the registrar. Bergkamp was raised as a Catholic by his family and regularly attended church during his childhood. Although in later years he said visits to church did not appeal to him, Bergkamp still maintains his faith.
Club career.
Ajax: 1986–1993.
Bergkamp was brought up through Ajax's youth system, joining the club at age 11. Manager Johan Cruyff gave him his professional debut on 14 December 1986 against Roda JC; the game ended in a 2–0 victory for Ajax. Bergkamp scored his first senior goal for the club against HFC Haarlem on 22 February 1987 in a match Ajax won 6–0. He went on to make 23 appearances in the 1986–87 season, including a European debut against Malmö FF in the 1986–87 European Cup Winners' Cup, earning him praise. Ajax won the competition, beating Lokomotive Leipzig 1–0 as Bergkamp made an appearance as a substitute.
In later seasons, Bergkamp established himself as a first-team player for Ajax. This culminated in a period of success for the club, which won the Eredivisie title in the 1989–90 season for the first time in five years. Bergkamp scored 29 goals in 36 games the following season and became the joint top goalscorer in the league, sharing the accolade with PSV Eindhoven striker Romário. Ajax won the 1992 UEFA Cup Final, beating Torino through the away goals ruling. They then defeated SC Heerenveen 6–2 in the final of the KNVB Cup on 20 May 1993. Bergkamp was the top scorer in the Eredivisie from 1991 to 1993, and was voted Dutch Footballer of the Year in 1992 and 1993. In total, he scored 122 goals in 239 games for his hometown club.
Internazionale: 1993–1995.
Bergkamp attracted the attention of several European clubs as a result of his performances for Ajax. Cruyff advised him not to join Real Madrid, one of the teams said to have been interested in him. But Bergkamp was insistent on playing in Italy. He considered Serie A "the biggest league at the time" and preferred a move to either Juventus or Internazionale. On 16 February 1993, Bergkamp agreed a £7.1 million move to the latter club in a deal which included his Ajax teammate Wim Jonk. Upon signing, Bergkamp said Inter "met all my demands. The most important thing for me was the stadium, the people at the club and their style of play."
Bergkamp made his debut against Reggiana on 29 August 1993 at the San Siro in a 2–1 victory. He scored his first goal for the club against Cremonese in September 1993 but had a difficult time against the highly organised and resolute Italian defences, scoring a further seven goals in the league. This was partly due to manager Osvaldo Bagnoli's inability to find a stable forward partnership, preferring Bergkamp in a three with Uruguayan Rubén Sosa and Italian Salvatore Schillaci. Inter's poor league form cultimated in the sacking of Bagnoli in February 1994 and his replacement by Giampiero Marini, a member of Italy's World Cup winning squad of 1982. The club finished 13th in Serie A, one point away from relegation, but enjoyed success in the UEFA Cup, beating FC Salzburg in the final over two legs. Bergkamp was the competition's joint top goalscorer with eight goals and scored a hat-trick against Rapid Bucureşti in the first round.
In Bergkamp's second season at Inter, the club changed managers again, appointing Ottavio Bianchi. Bergkamp endured a disappointing campaign, troubled with stress injuries and fatigue from the 1994 World Cup. He managed to score five goals in 26 appearances. Off the field, Bergkamp's relationship with the Italian press and fans became uncomfortable. His shy persona and his propensity to go home after matches was interpreted as apathy. Because of his poor performance on the pitch, one Italian publication renamed their award given to the worst performance of the week, "L'asino della settimana" (Donkey of the Week) to "Bergkamp della settimana". Inter ended the league season in sixth position and failed to retain the UEFA Cup, with the club eliminated in the second round. In February 1995, the club was purchased by Italian businessman and fan Massimo Moratti, who promised to invest heavily in the squad. Bergkamp's future in the first team was uncertain following the signing of Maurizio Ganz a month after the takeover.
Arsenal: 1995–2006.
As Moratti prepared to make wholesale changes at the club, Bergkamp left Inter and signed with Arsenal in June 1995 for a transfer fee estimated at £7.5 million. He became manager Bruce Rioch's first signing at Arsenal and broke the club's transfer fee record set at £2.5 million. Bergkamp's arrival at the club was significant not only because he was an established international footballer who looked to have his best years ahead of him but also because he was a major contributor to Arsenal's return to success after much decline in the mid-1990s. On the opening day of the 1995–96 league season, Bergkamp made his full debut against Middlesbrough. He struggled to adapt to the English game and failed to score in the club's next six league matches, prompting ridicule by the national press. On 23 September 1995, Bergkamp scored his first and second goals for Arsenal against Southampton at Highbury. Bergkamp ended his first season with 33 appearances and a goal tally of 11, helping Arsenal finish fifth and earn a place in the UEFA Cup by scoring the winner against Bolton Wanderers on the final day of the season.
The appointment of Arsène Wenger as Arsenal manager in September 1996 marked a turning point in Bergkamp's career. Wenger, who had moderate success coaching in France and Japan, recognised Bergkamp's talent and wanted to use him as a fulcrum of the team's forward play. Both were advocates of a continental style of attacking football, and Wenger's decision to impose a strict fitness and health regime pleased Bergkamp. Despite making fewer appearances in the 1996–97 season, Bergkamp was more influential in the first team, creating 13 assists. Against Tottenham Hotspur in November 1996, he set up an 88th-minute winner for captain Tony Adams to volley in using his left foot. He then scored in injury time, controlling a high ball with his left foot and evading his marker Stephen Carr in a tight area to set up his shot. Bergkamp received his first red card against Sunderland in January 1997 for a high tackle on midfielder Paul Bracewell in the 26th minute. Arsenal went on to lose the match 1–0, but a run of eight wins in their final 16 matches gave the club a third-place finish, missing out on a spot in the Champions League via goal difference.
Bergkamp was instrumental the following season in helping Arsenal complete a domestic league and cup double. He became the club's top scorer with 22 goals and recorded a strike rate of 0.57. Arsenal's achievement was all the more astonishing given that the team, written off by many in December 1997, had made ground on reigning Premier League champions Manchester United. Early in the season away to Leicester City at Filbert Street on 23 August 1997, Bergkamp scored his first hat-trick for the club. The third goal, which he regarded as his favourite for Arsenal, required just one touch to control the ball in the penalty box, another to flick it past his marker Matt Elliott before juggling it with his feet and shooting past goalkeeper Kasey Keller. After the match, Leicester manager Martin O'Neill was gracious enough to admit that Bergkamp's was "the best hat-trick I've ever seen". In an FA Cup quarter-final replay against West Ham United on 17 March 1998, Bergkamp was sent off for elbowing midfielder Steve Lomas and missed three matches due to suspension. He played no further part in Arsenal's season after overstretching his hamstring against Derby County on 29 April 1998, missing the 1998 FA Cup Final. Bergkamp was consoled with the PFA Players' Player of the Year award, becoming only the third non-British player to be recognised by his fellow professionals as the outstanding performer in English football.
After an effective 1998 World Cup campaign with the national team, Bergkamp had another productive season in 1998–99. Although Arsenal failed to retain the Premier League after losing the title on the final day of the season to Manchester United, Bergkamp was the club's second-top scorer with 16 goals. The team were also defeated in a FA Cup semi-final replay against Manchester United in April 1999. With the score 1–1 heading into injury time, Arsenal were awarded a penalty after midfielder Ray Parlour was brought down by Phil Neville inside the 18-yard box. Bergkamp took the penalty shot, but it was saved by goalkeeper Peter Schmeichel. In the second half of extra time, Ryan Giggs scored the winner, a goal regarded by many as the greatest in the competition's history. After this miss, Bergkamp did not take another penalty for the remainder of his career.
The 1999–2000 season proved to be a frustrating one for both Arsenal and Bergkamp. The club finished second in the league, 18 points behind Manchester United, and lost in the 2000 UEFA Cup Final to Turkish opponents Galatasaray on penalties. The departure of compatriot Marc Overmars and French midfielder Emmanuel Petit in the close season led to speculation over Bergkamp's future. He ultimately agreed terms on a contract extension in December 2000. Despite an array of new signings made in the 2000–01 season, Arsenal were runners-up in the league for a third year in succession. The emergence of Thierry Henry and Sylvain Wiltord as the main strikers saw Bergkamp's first-team opportunities limited as a result. He was used as a late substitute in Liverpool's win over Arsenal in the 2001 FA Cup Final.
"You can't blame anyone for that. You just have to accept that Bergkamp did a beautiful thing."
 Sir Bobby Robson on Dennis Bergkamp's goal against Newcastle United in March 2002.
Success finally came in the 2001–02 season. Arsenal regained the league, beating Manchester United at Old Trafford in the penultimate game of the season to complete the club's second double under Wenger; Arsenal defeated Chelsea 2–0 to win the FA Cup four days prior. Bergkamp played in 33 league matches, setting up 15 goals, one of which was against Juventus in the second group stage of the Champions League. Holding off two markers, he twisted and turned before feeding the ball to Freddie Ljungberg in the penalty box to score. Bergkamp headed in the winner against Liverpool in a FA Cup fourth-round tie on 27 January 2002, but was shown a red card for a two-footed lunge on defender Jamie Carragher, who himself was sent off for throwing a coin into the crowd. He was subsequently banned for three matches (two league, one FA Cup round). Bergkamp appealed for his ban, but was unsuccessful. He made his return against Newcastle United on 3 March 2002. Early in the match, Arsenal midfielder Robert Pirès played a low pass from the left flank to Bergkamp in the edge of the opponent area with his back to goal. Under pressure from his marker Nikos Dabizas, Bergkamp controlled the ball with one flick and went around the other side before placing the ball precisely into the bottom right-hand corner to score. Wenger described the goal as "unbelievable", adding "It was not only a magnificent goal but a very important one – I enjoyed it a lot". Bergkamp featured in nine out of the last ten league games, forming a productive partnership with Ljungberg.
Bergkamp reached a personal landmark during the 2002–03 season, scoring his 100th goal for Arsenal against Oxford United in a FA Cup third-round tie on 4 January 2003. In the league, Arsenal failed to retain the championship despite having led by eight points in March 2003. They did, however, win the FA Cup for a second successive year, beating Southampton in the 2003 FA Cup Final. On 20 July 2003, Bergkamp signed a one-year extension at the club. The 2003–04 season ended on a high point for Bergkamp as Arsenal reclaimed the league title, becoming the first English team in more than a century to go through the entire domestic league season unbeaten. Against Leicester City in the final league match of the campaign with the score tied at 1–1, Bergkamp set up the winner with a pass to captain Patrick Vieira. Vieira rounded the goalkeeper and scored. The team, dubbed "The Invincibles" did not achieve similar dominance in Europe; Arsenal were beaten by Chelsea in the quarter-finals of the Champions League over two legs. Bergkamp committed himself to Arsenal at the end of the season, signing a further extension to his contract.
Bergkamp started in 29 league matches in the 2004–05 season, but Arsenal's title defence ended unsuccessfully. The team finished second, 12 points behind Chelsea. At home against Middlesbrough on 22 August 2004, Bergkamp acted as captain for the injured Vieira in a match where Arsenal came back from 1–3 down to win 5–3 and equal Nottingham Forest's record of 42 league matches undefeated. Against Sheffield United in the FA Cup on 19 February 2005, Bergkamp was shown a straight red card by referee Neale Barry for shoving defender Danny Cullip. His appeal of the decision was rejected by The Football Association, meaning that he missed the club's next three domestic games. In Arsenal's final home match of the season against Everton, Bergkamp had a man of the match game, scoring once and assisting three of the goals in a 7–0 win. Bergkamp was moved by Arsenal supporters chanting "one more year", describing it as "quite special". "They obviously feel there is another year left in me, so that's great as it shows they're really behind me," he said. Following Arsenal's penalty shootout victory over Manchester United in the 2005 FA Cup Final, he signed a one-year contract extension.
The team finished fourth in the league in Bergkamp's final season at Arsenal. Bergkamp scored an injury-time winner against FC Thun on Matchday 1 of the Champions League, having come on as a substitute in the 72nd minute. After much campaigning from Arsenal supporters, the club designated one of its Highbury matchday themes, organised to commemorate the stadium's final season as home of Arsenal, to Dennis Bergkamp. "Bergkamp Day" took place on 15 April 2006 and saw Arsenal up against West Bromwich Albion. It celebrated the player's contribution to Arsenal; fans were given commemorative orange 'DB10' T-shirts – the colour of his national team, his initials and his squad number. Bergkamp himself came on as a second-half substitute and set up the winning Pirès goal moments after Nigel Quashie had levelled the scoreline. Fittingly, Bergkamp's 89th-minute goal proved to be his last for Arsenal in competitive football. Bergkamp was an unused substitute in his final match for Arsenal against Barcelona in the Champions League final; Barcelona scored twice in the last 13 minutes to overturn Arsenal's early lead and win the competition.
Bergkamp was the focus of the first match at Arsenal's new ground, the Emirates Stadium. On 22 July 2006, a testimonial was played in his honour at the new stadium as Arsenal played his old club Ajax. Bergkamp kicked off the match with his father, Wim, and son, Mitchel. All four children acted as the match's mascots. The first half was played by members of Arsenal and Ajax's current squads, while the second was played by famous ex-players from both sides, including Ian Wright, Vieira, Overmars, Petit and David Seaman for Arsenal, and Cruyff, Marco van Basten, Danny Blind, Frank and Ronald de Boer for Ajax. Arsenal won the match 2–1 with goals from Henry and Nwankwo Kanu. Klaas-Jan Huntelaar had earlier opened the scoring for Ajax, making him the first goalscorer at the Emirates Stadium.
International career.
Bergkamp made his international debut for the Netherlands national team against Italy on 26 September 1990 as a substitute for Frank de Boer. He scored his first goal for the team against Greece on 21 November 1990. Bergkamp was selected for Euro 1992, where his national team were the defending champions. Although Bergkamp impressed, scoring three goals in the tournament, the team lost on penalties to eventual champions Denmark.
In the qualification for the 1994 FIFA World Cup, Bergkamp scored five goals and was selected for the finals, staged in the United States. He featured in every game for the national team, getting goals against Morocco in the group stages and the Republic of Ireland in the round-of-16. Bergkamp scored the second goal for the Netherlands against Brazil, but the team lost 3–2, exiting in the quarter-finals. At Euro 1996, Bergkamp scored against Switzerland and set up striker Patrick Kluivert's consolation goal against England, who advanced into the quarter-finals.
Against Wales in the 1998 FIFA World Cup qualification on 9 November 1996, he scored his first hat-trick for the national team. The Netherlands finished first in their group and qualified for the 1998 FIFA World Cup, held in France. Bergkamp scored three times in the competition, including a memorable winning goal in the final minute of the quarterfinal against Argentina. 
He took one touch to control a long 60-yard aerial pass from Frank de Boer, brought the ball down through Argentine defender Roberto Ayala's legs, and finally finished by firing a volley with the outside of his right foot, past the keeper at a tight angle from the right. The goal, cited by Bergkamp as his favourite in his career, was his 36th for the national team, overtaking Faas Wilkes as the record goalscorer. In the semi-finals, the Netherlands lost to Brazil on penalties after drawing 1–1 in normal time. Bergkamp made the All-Star team of the tournament, alongside Frank de Boer and Edgar Davids.
On 9 October 1999, Bergkamp scored his final goal for the Netherlands, against Brazil. As the Netherlands were co-hosts for Euro 2000, the team automatically qualified for the tournament and were considered favourites. In the semi-finals, the Netherlands lost 3–1 on penalties to Italy. Following the defeat, Bergkamp announced his retirement from international football, choosing to focus on his club career. His final goal tally of 37 goals in 79 appearances was overtaken by Patrick Kluivert in June 2003.
Coaching career.
Upon retiring, Bergkamp insisted he would not move into coaching. He turned down an offer to scout for Arsenal and instead concentrated on travelling and spending time with his family. However, in April 2008 he began a fast-track coaching diploma for former Dutch international footballers and undertook a trainee role at Ajax. Having completed the Coach Betaald Voetbal course by the KNVB, Bergkamp was appointed assistant to Johan Neeskens for the newly formed Netherlands B team on 26 October 2008. For the 2008–09 season, Bergkamp returned to Ajax in a formal coaching position with responsibility for the D2 (U12) youth team. Following the promotion of Frank de Boer as manager of Ajax in December 2010, Bergkamp was appointed assistant manager to Fred Grim, dealing with Ajax' flagship A1 (U19) youth team.
As of August 2011, Bergkamp has been De Boer's assistant at Ajax.
Personal life.
Bergkamp has been married to Henrita Ruizendaal since 16 June 1993. The couple have four children: Estelle Deborah, Mitchel Dennis, Yasmin Naomi and Saffron Rita. His nephew, Roland Bergkamp currently plays for FC Emmen, having previously played for Brighton & Hove Albion.
Aviophobia.
Bergkamp's nickname is the Non-Flying Dutchman due to his fear of flying. This stemmed from an incident with the Netherlands national team at the 1994 World Cup where the engine of the plane cut out during a flight, prompting a journalist to joke about having a bomb in his bag. Following this incident, Bergkamp decided he would never fly again but did consider seeking psychiatric help:I've got this problem and I have to live with it. I can't do anything about it, it is a psychological thing and I can't explain it. I have not flown on a plane for two years. The Dutch FA has been sympathetic, so have Arsenal, so far. I am considering psychiatric help. I can't fly. I just freeze. I get panicky. It starts the day before, when I can't sleep.
The condition severely limited his ability to play in away matches in European competitions and to travel with the national team. In some cases, he would travel overland by car or train, but the logistics of some matches were such that he would not travel at all. In the build-up to Arsenal's Champions League match against Olympique Lyonnais in February 2001, Wenger spoke of his concerns for Bergkamp travelling by train and car, because of the exertions involved.
Style of play.
"He needs fewer touches to score. Sometimes just one, when others need two or three."
Wenger on Bergkamp
Bergkamp was schooled in Total Football, a playing style and philosophy which relied on versatility. This was primarily to maximise the footballer's potential; players tried out every outfield position before finding one that suited them best. Every age group at Ajax played in the same style and formation as the first team – 3–4–3 – to allow individuals to slot in without effort when moving up the pyramid. Bergkamp "played in every position apart from goalie" and believed he benefited from the experience of playing as a defender, as it helped him "know how they think and how to beat them". When he made his debut as a substitute against Roda JC, Bergkamp was positioned on the right wing, where he remained for three years. 
During his time at Inter Milan, Bergkamp was switched to the position of a main striker, but failed to cooperate with his offensive partner Ruben Sosa, whom he later called "selfish". When Bergkamp joined Arsenal in 1995, he enjoyed a successful strike partnership with Wright, and in later seasons Anelka and Henry, playing in his preferred position as a creative second striker. The arrival of Overmars in the 1997–98 season enhanced Bergkamp's play, as he was getting more of the ball. Between August and October 1997, he scored seven goals in seven league matches. A similar rapport developed between him and Ljungberg during the 2001–02 season. In addition to his ability to score goals, Bergkamp was also capable of functioning as playmaking attacking midfielder, due to his technical skills, vision, and passing range.
Throughout his playing career Bergkamp was referred to as a "cheat" and "dirty player", something his former manager Wenger refuted. In an interview with "The Times" in 2004, he said that while he was at Inter Milan, he realized the importance of being mentally tough in order to survive: "A lot of people there try to hurt you, not just physically but mentally as well, and coming from the easygoing culture in Holland, I had to adopt a tougher approach. There, it was a case of two strikers up against four or five hard defenders who would stop at nothing." Bergkamp says his aggression often stems from frustration.
Honours.
Individual.
Bergkamp has received several accolades during his playing career. He twice finished in third place for the 1993 and 1996 FIFA World Player of the Year award and was named in FIFA 100, a list compiled by footballer Pelé of the 125 greatest living footballers. In his club career, Bergkamp won two successive Dutch Footballer of the Year awards in 1991 and 1992 and was the Eredivisie Top Scorer winner for three consecutive seasons (1990–91 to 1992–93). He was named the FWA Footballer of the Year and PFA Players' Player of the Year in April and May 1998 and made the PFA Team of the Year for the 1997–98 season. Bergkamp also achieved a unique feat in being voted first, second and third on Match of the Day's Goal of the Month competition in August 1997. For his national team, Bergkamp was the top goalscorer in Euro 1992 and was selected in the FIFA World Cup All-Star Team for the 1998 FIFA World Cup.
In April 2007, Bergkamp was inducted into the English Football Hall of Fame by viewers of BBC's "Football Focus". A year later, he was voted second by Arsenal fans behind Thierry Henry in a list of the "50 Gunners Greatest Players". In February 2014, Arsenal unveiled a statue of Bergkamp outside the Emirates Stadium to honour his time at the club. A statue of Dennis Bergkamp will be erected outside the KNVB headquarters in Zeist, as he has been chosen as the best Dutch international soccer player in the period 1990-2015. The statue will join those of "the eleven of the century", erected in 1999, with statues of Johan Cruijff, Ruud Gullit, Frank Rijkaard and Marco van Basten amongst others there.
A summary of Bergkamp's individual achievements are as follows in chronological order:
Career statistics.
Club.
<div id="notes gs1"/>1
Media.
Bergkamp features in EA Sports' "FIFA" video game series; he was on the cover for the International edition of FIFA 99, and was named in the Ultimate Team Legends in "FIFA 14".

</doc>
<doc id="45499" url="http://en.wikipedia.org/wiki?curid=45499" title="Great Lakes Commission">
Great Lakes Commission

The Great Lakes Commission is a United States interstate agency established in 1955 through the Great Lakes Compact, in order to "promote the orderly, integrated and comprehensive development, use and conservation of the water resources of the Great Lakes Basin," which includes the Saint Lawrence River. The commission provides policy development, coordination, and advocacy on issues of regional concern, as well as communication and research services.
The commission, in a cooperative venture with other Great Lakes agencies and organizations, hosts the Great Lakes Information Network (GLIN), an Internet-based network that serves as a decision-support tool for those who make, implement or otherwise influence public policy in the region. The GLIN website contains data and information about the region's environment, economy, tourism, education, and more; and provides access to GLIN-Announce, an email list that covers news and information about the region.
Another Commission project, Great Lakes GIS Online, involves creating an online spatial data library, including the Great Lakes shoreline, soils, land use and land cover, hazardous waste sites, demographics, watersheds and transport. The project is planned to include an online mapping system that will enable users to perform GIS analysis and other tasks over the Internet. As of summer 1998, the project was under development.
The eight member states are Illinois, Indiana, Michigan, Minnesota, New York, Ohio, Pennsylvania, and Wisconsin.
The Canadian provinces of Ontario and Quebec are associate members.
There is a separate and distinct entity with a similar brief, the International Joint Commission, which exists between the federal levels of the US and Canada.

</doc>
<doc id="45503" url="http://en.wikipedia.org/wiki?curid=45503" title="Coelacanth">
Coelacanth

The coelacanths ( ) constitute a now rare order of fish that includes two extant species in the genus "Latimeria": the West Indian Ocean coelacanth ("Latimeria chalumnae") and the Indonesian coelacanth ("Latimeria menadoensis"). They follow the oldest known living lineage of Sarcopterygii (lobe-finned fish and tetrapods), which means they are more closely related to lungfish, reptiles and mammals than to the common ray-finned fishes. They are found along the coastlines of the Indian Ocean and Indonesia. Since there are only two species of coelacanth and both are threatened, it is the most endangered order of animals in the world. The West Indian Ocean coelacanth is a critically endangered species.
Coelacanths belong to the subclass Actinistia, a group of lobed-finned fish related to lungfish and certain extinct Devonian fish such as osteolepiforms, porolepiforms, rhizodonts, and "Panderichthys". Coelacanths were thought to have gone extinct in the Late Cretaceous, but were rediscovered in 1938 off the coast of South Africa. Traditionally, the coelacanth was considered a “living fossil” due to it being the sole remaining member of a taxon otherwise known only from fossils, with no close relations alive; and the coelacanth was thought to have evolved into roughly its current form approximately 400 million years ago. However, several recent studies have shown that coelacanth body shapes are much more diverse than is generally said.
Etymology.
"Coelacanth" is an adaptation of Modern Latin "Cœlacanthus" "hollow spine," from Greek κοῖλ-ος "koilos ""hollow" + ἄκανθ-α "akantha" "spine," referring to the hollow caudal fin rays of the first fossil specimen described and named by Louis Agassiz in 1839.
Discovery.
The coelacanths, which are related to lungfishes and tetrapods, were believed to have been extinct since the end of the Cretaceous period. More closely related to tetrapods than even the ray-finned fish, coelacanths were considered transitional species between fish and tetrapods. The first "Latimeria" specimen was found off the east coast of South Africa, off the Chalumna River (now Tyolomnqa) in 1938. Museum curator Marjorie Courtenay-Latimer discovered the fish among the catch of a local angler, Captain Hendrick Goosen, on 22 December 1938. A Rhodes university ichthyologist, J.L.B. Smith, confirmed the fish's importance with a famous cable: "MOST IMPORTANT PRESERVE SKELETON AND GILLS = FISH DESCRIBED".
The discovery of a species still living, when they were believed to have gone extinct 66 million years previously, makes the coelacanth the best-known example of a Lazarus taxon, an evolutionary line that seems to have disappeared from the fossil record only to reappear much later. Since 1938, "Latimeria chalumnae" have been found in the Comoros, Kenya, Tanzania, Mozambique, Madagascar, and in iSimangaliso Wetland Park, Kwazulu-Natal in South Africa.
The second extant species, "L. menadoensis", was described from Manado, North Sulawesi, Indonesia in 1999 by Pouyaud et al. based on a specimen discovered by Mark V. Erdmann in 1998 and deposited at the Indonesian Institute of Sciences (LIPI). Only a photograph of the first specimen of this species was made at a local market by Erdmann and his wife Arnaz Mehta before it was bought by a shopper.
The coelacanth has no real commercial value, apart from being coveted by museums and private collectors. As a food fish the coelacanth is almost worthless, as its tissues exude oils that give the flesh a foul flavor. The continued survival of the coelacanth may be threatened by commercial deep-sea trawling, in which coelacanths are caught as bycatch.
Physical description.
Coelacanths are a part of the clade Sarcopterygii, or the lobe-finned fishes. Externally, several characteristics distinguish the coelacanth from other lobe-finned fish. They possess a three-lobed caudal fin, also called a trilobate fin or a diphycercal tail. A secondary tail extending past the primary tail separates the upper and lower halves of the coelacanth. Cosmoid scales act as thick armor to protect the coelacanth's exterior. Several internal traits also aid in differentiating coelacanths from other lobe-finned fish. At the back of the skull, the coelacanth possesses a hinge, the intracranial joint, which allows it to open its mouth extremely wide. Coelacanths also retain an oil-filled notochord, a hollow, pressurized tube which is replaced by the vertebral column early in embryonic development in most other vertebrates. The coelacanth heart is shaped differently from that of most modern fish, with its chambers arranged in a straight tube. The coelacanth braincase is 98.5% filled with fat; only 1.5% of the braincase contains brain tissue. The cheeks of the coelacanths are unique because the opercular bone is very small and holds a large soft-tissue opercular flap. A spiracular chamber is present, but the spiracle is closed and never opens during development. Coelacanth also possess a unique rostral organ within the ethmoid region of the braincase. Also unique to extant coelacanths is the presence of a "fatty lung" or a fat-filled single-lobed vestigial lung, homologous to other fishes' swim bladder. Due to its size, it is assumed to be responsible for the kidney's unusual relocation. The two kidneys, which are fused into one, are located ventrally within the abdominal cavity, posterior to the cloaca.
General description.
"Latimeria chalumnae" and "L. menadoensis" are the only two known living coelacanth species. The word "coelacanth" is derived from the Greek for “hollow spine”, because of the fish's unique hollow spine fins. Coelacanths are large, plump, lobe-finned fish that grow up to 1.8 meters. They are nocturnal piscivorous drift-hunters. The body is covered in cosmoid scales that act as armor. Coelacanths have eight fins – 2 dorsal fins, 2 pectoral fins, 2 pelvic fins, 1 anal fin and 1 caudal fin. The tail is very nearly equally proportioned and is split by a terminal tuft of fin rays that make up its caudal lobe. The eyes of the coelacanth are very large, while the mouth is very small. The eye is acclimatized to seeing in poor light by rods that absorb mostly low wavelengths. Coelacanth vision has evolved to a mainly blue-shifted color capacity. Pseudomaxillary folds surround the mouth and replace the maxilla, a structure absent in coelacanths. Two nostrils, along with four other external openings, appear between the premaxilla and lateral rostral bones. The nasal sacs resemble those of many other fish and do not contain an internal nostril. The coelacanth's rostral organ, contained within the ethmoid region of the braincase, has three unguarded openings into the environment and is used as a part of the coelacanth's laterosensory system. The coelacanth's auditory reception is mediated by its inner ear, which is very similar to that of tetrapods because it is classified as being a basilar papilla.
Coelacanth locomotion is unique. To move around they most commonly take advantage of up- or down-wellings of current and drift. Their paired fins stabilize movement through the water. While on the ocean floor, they do not use the paired fins for any kind of movement. Coelacanths create thrust with their caudal fins for quick starts. Due to the abundance of its fins, the coelacanth has high maneuverability and can orient its body in almost any direction in the water. They have been seen doing headstands as well as swimming belly up. It is thought that the rostral organ helps give the coelacanth electroperception, which aids in movement around obstacles.
DNA.
A group led by Chris Amemiya and Neil Shubin published the genome sequence of the coelacanth in the journal "Nature". The African coelacanth genome was sequenced and assembled using DNA from a Comoros Islands "Latimeria chalumnae" specimen. It was sequenced by Illumina sequencing technology and assembled using the short read genome assembler ALLPATHS-LG.
The vertebrate land transition is one of the most important steps in our evolutionary history. We conclude that the closest living fish to the tetrapod ancestor is the lungfish, not the coelacanth. However, the coelacanth is critical to our understanding of this transition, as the lungfish have intractable genome sizes (estimated at 50–100Gb).
Taxonomy.
The following is a classification of known coelacanth genera and families:
Fossil record.
According to genetic analysis of current species, the divergence of coelacanths, lungfish and tetrapods is thought to have occurred 390 million years ago. Coelacanths were thought to have undergone extinction 66 million years ago during the Cretaceous–Paleogene extinction event. The first recorded coelacanth fossil, found in Australia, was of a jaw that dated back 360 million years, named "Eoachtinistia foreyi". The most recent species of coelacanth in the fossil record is the "Macropoma", a sister species to "Latimeria chalumnae", separated by 80 million years. The fossil record is unique because coelacanth fossils were found 100 years before the first live specimen was identified. In 1938, Courtenay-Latimer rediscovered the first live specimen, "L. chalumnae", caught off the coast of East London, South Africa. In 1997, a marine biologist on honeymoon discovered the second live species, "Latimeria menadoensis", in an Indonesian market. 
In July 1998, the first live specimen of "Latimeria menadoensis" was caught in Indonesia. Approximately 80 species of coelacanth have been described, including the two extant species. Before the discovery of a live specimen, the coelacanth time range was thought to have spanned from the Middle Devonian to the Upper Cretaceous period. Although fossils found during that time were claimed to demonstrate a similar morphology, recent studies have expressed the view that coelacanth morphological conservatism is a belief not based on data.
The following cladogram is based on multiple sources.
Geographical distribution.
The current coelacanth range is primarily along the eastern African coast, although "Latimeria menadoensis" was discovered off Indonesia. Coelacanths have been found in the waters of Kenya, Tanzania, Mozambique, South Africa, Madagascar, Comoros and Indonesia. Most "Latimeria chalumnae" specimens that have been caught have been captured around the islands of Grande Comore and Anjouan in the Comoros Archipelago (Indian Ocean). Though there are cases of "L. chalumnae" caught elsewhere, amino acid sequencing has shown no big difference between these exceptions and those found around Comore and Anjouan. Even though these few may be considered strays, there are several reports of coelacanths being caught off of the coast of Madagascar. This leads scientists to believe that the endemic range of "Latimeria chalumnae" coelacanths stretches along the eastern coast of Africa from the Comoros Islands, past the western coast of Madagascar to the South African coastline. The geographical range of the Indonesia coelacanth, "Latimeria menadoensis", is believed to be off the coast of Manado Tua Island, Sulawesi, Indonesia in the Celebes Sea. Key components confining coelacanths to these areas are food and temperature restrictions.
Ecology.
Anjouan Island and the Grande Comore provide ideal underwater cave habitats for coelacanths. The islands' underwater volcanic slopes, steeply eroded and covered in sand, house a system of caves and crevices which allow coelacanths resting places during the daylight hours. These islands support a large benthic fish population that help to sustain coelacanth populations.
During the daytime, coelacanths will rest in caves anywhere from 100 to 500 meters deep; others migrate to deeper waters. The cooler waters (below 120 meters) reduce the coelacanths' metabolic costs. Drifting toward reefs and night feeding saves vital energy. Resting in caves during the day also saves energy otherwise used to fight currents.
Coelacanths are nocturnal piscivores who feed mainly on benthic fish populations. Drifting along the lava cliffs, they presumably feed on whatever fish they encounter.
Coelacanths are fairly peaceful when encountering others of their kind; remaining calm even in a crowded cave. They do avoid body contact, however, withdrawing immediately if contact occurs. When approached by foreign potential predators (e.g. a submersible), they show panic flight reactions, suggesting that coelacanths are most likely prey to large deepwater predators. Shark bite marks have been seen on coelacanths; sharks are common in areas inhabited by coelacanths. Electrophoresis testing of 14 coelacanth enzymes shows little genetic diversity between coelacanth populations. Among the fish that have been caught were about equal numbers of males and females. Population estimates range from 210 individuals per population all the way to 500 per population. Because coelacanths have individual color markings, scientists think that they recognize other coelacanths via electric communication.
Life history.
Coelacanths are ovoviviparous, meaning that the female retains the fertilized eggs within her body while the embryos develop during a gestation period of over a year. Typically, females are larger than the males; their scales and the skin folds around the cloaca differ. The male coelacanth has no distinct copulatory organs, just a cloaca, which has a urogenital papilla surrounded by erectile caruncles. It is hypothesized that the cloaca everts to serve as copulatory organ. Coelacanth eggs are large with only a thin layer of membrane to protect them. Embryos hatch within the female and eventually are given live birth. Young coelacanths resemble the adult, the main differences being an external yolk sac, larger eyes relative to body size and a more pronounced downward slope of the body. The juvenile coelacanth's broad yolk sac hangs below the pelvic fins. The scales and fins of the juvenile are completely matured; however, it does lack odontodes, which it gains during maturation.
Conservation.
Because little is known about the coelacanth, the conservation status is difficult to characterize. According to Fricke "et al." (1995), there should be some stress put on the importance of conserving this species. From 1988 to 1994, Fricke counted some 60 individuals on each dive. In 1995 that number dropped to 40. Even though this could be a result of natural population fluctuation, it also could be a result of overfishing. Coelacanths usually are caught when local fishermen are fishing for oilfish. Fishermen sometimes snag a coelacanth instead of an oilfish because they traditionally fish at night, when oilfish (and coelacanths) feed. Before scientists became interested in coelacanths, they were thrown back into the water if caught. Now that there is an interest in them, fishermen trade them in to scientists or other officials once they have been caught. Before the 1980s, this was a problem for coelacanth populations. In the 1980s, international aid gave fiberglass boats to the local fishermen, which resulted in fishing out of coelacanth territories into more fish-productive waters. Since then, most of the motors on the boats have broken down so the local fishermen are now back in the coelacanth territory, putting the species at risk again.
Different methods to minimize the number of coelacanths caught include moving fishers away from the shore, using different laxatives and malarial salves to reduce the quantity of oilfish needed, using coelacanth models to simulate live specimens, and increasing awareness of the need to protect the species. In 1987 the Coelacanth Conservation Council was established to help protect and encourage population growth of coelacanths.
In 2002, the South African Coelacanth Conservation and Genome Resource Programme was launched to help further the studies and conservation of the coelacanth. The South African Coelacanth Conservation and Genome Resource Programme focuses on biodiversity conservation, evolutionary biology, capacity building, and public understanding. The South African government committed to spending R10 million on the program.
Human consumption.
Coelacanths are considered a poor source of food for humans and likely most other fish-eating animals. Coelacanth flesh has high amounts of oil, urea, wax esters, and other compounds that are difficult to digest and can cause diarrhea. Where the coelacanth is more common, local fishermen avoid it because of its potential to sicken consumers.

</doc>
<doc id="45504" url="http://en.wikipedia.org/wiki?curid=45504" title="United Negro College Fund">
United Negro College Fund

The United Negro College Fund, or UNCF, is an American philanthropic organization that funds scholarships for black students and general scholarship funds for 39 private historically black colleges and universities. The UNCF was incorporated on April 25, 1944 by Frederick D. Patterson (then president of what is now Tuskegee University), Mary McLeod Bethune, and others. The UNCF is headquartered at 1805 7th Street, NW in Washington, DC. In 2005, the UNCF supported approximately 65,000 students at over 900 colleges and universities with approximately $113 million in grants and scholarships. About 60% of these students are the first in their families to attend college, and 62% have annual family incomes of less than $25,000. UNCF also administers over 450 named scholarships.
The UNCF's president and chief executive officer is Michael Lomax. Past presidents of the UNCF included William H. Gray and Vernon Jordan.
Scholarships.
Though set up to address funding inequities in education resources for African Americans, the UNCF-administered scholarships are open to all ethnicities; the great majority of recipients are still African-American. It provides scholarships to students attending its member colleges as well as to those going elsewhere.
Graduates of UNCF scholarships have included many blacks in the fields of business, politics, health care and the arts. Some prominent UNCF alumni include Dr. Martin Luther King, Jr., a Nobel Peace Prize recipient and leader in the Civil Rights movement in the 1960s; Alexis Herman, former U.S. Secretary of Labor; noted movie director Spike Lee; actor Samuel L. Jackson; General Chappie James, the U.S. Air Force’s first black four-star general; and Dr. David Satcher, a former U.S. Surgeon General and director of the Centers for Disease Control.
History.
In 1944 William Trent, a long time activist for education for blacks, joined with Tuskegee Institute President Frederick D. Patterson and Mary McLeod Bethune to found the UNCF, a nonprofit that united college presidents to raise money collectively through an “appeal to the national conscience.” As the first executive director from the organization’s start in 1944 until 1964, Trent raised $78 million for historically black colleges so they could become “strong citadels of learning, carriers of the American dream, seedbeds of social evolution and revolution.” 
Fundraising and the Lou Rawls Parade of Stars.
The UNCF has received charitable donations for its scholarship programs. One of the more high profile donations made was by former U.S. President John F. Kennedy who donated the money from the Pulitzer Prize for his book "Profiles in Courage" to the Fund. The largest ever single donation was made in 1990 by Walter Annenberg who donated $50 million to the fund.
Beginning in 1980, singer Lou Rawls began the "Lou Rawls Parade of Stars" telethon to benefit the UNCF. The annual event, now known as "An Evening of Stars", consists of stories of successful African-American students who have graduated or benefited from one of the many historically black colleges and universities and who received support from the UNCF. The telethon featured comedy and musical performances from various artists in support of the UNCF's and Rawls' efforts. The event has raised over $200 million in 27 shows for the fund through 2006.
In January 2004, Rawls was honored by the United Negro College Fund for his more than 25 years of charity work with the organization. Instead of Rawls' hosting and performing, he was given the seat of honor and celebrated by his performing colleagues, including Stevie Wonder, The O'Jays, Gerald Levert, Ashanti, and several others. Before his death in January 2006, Rawls' last performance was a taping for the 2006 telethon that honored Wonder, months before entering the hospital after being diagnosed with cancer earlier in the year.
In addition to the telethon there are a number of other fundraising activities, including the "Walk for Education" held annually in Los Angeles, California, which includes a five kilometer walk/run. In Houston, Texas, the Cypresswood Golf Club hosts an annual golf tournament in April.
In 2014, Koch Industries Inc. and the Charles Koch Foundation made a $25 million grant to UNCF. In protest of the Kochs, the American Federation of State, County and Municipal Employees, a major labor union, ended its yearly $50,000–$60,000 support for UNCF.
The UNCF motto.
In 1972, the UNCF adopted as its motto the maxim "A mind is a terrible thing to waste." This maxim has become one of the most widely recognized slogans in advertising history.
The motto, which has been used in numerous award-winning UNCF ad campaigns, was created by Forest Long, of the advertising agency Young & Rubicam, in partnership with the Ad Council.
A lesser-known slogan the UNCF also uses, in reference to its intended beneficiaries, points out about them, "[They're] not asking for a handout, just a hand."

</doc>
<doc id="45506" url="http://en.wikipedia.org/wiki?curid=45506" title="World peace">
World peace

World peace is an ideal of freedom, peace, and happiness among and within all nations and/or people. World peace is an idea of planetary non-violence by which nations willingly cooperate, either voluntarily or by virtue of a system of governance that prevents warfare. The term is sometimes used to refer to a cessation of all hostility amongst all humanity. For example, World Peace could be crossing boundaries via human rights, technology, education, engineering, medicine, diplomats and/or an end to all forms of fighting. Since 1945, the United Nations and the 5 permanent members of its Security Council (the US, Russia, China, France, and the UK) have worked to resolve conflicts without war or declarations of war. However, nations have entered numerous military conflicts since that time.
World peace theories.
Many theories as to how world peace could be achieved have been proposed. Several of these are listed below.
Various political ideologies.
World peace is sometimes claimed to be the inevitable result of a certain political ideology." Leon Trotsky, a Marxist theorist, assumed that a proletariat world revolution would lead to world peace.
Democratic peace theory.
Proponents of the controversial democratic peace theory claim that strong empirical evidence exists that democracies never or rarely wage war against each other.
There are, however, several wars between democracies that have taken place, historically.
Capitalism peace theory.
In her essay "The Roots of War," Ayn Rand held that the major wars of history were started by the more controlled economies of the time against the freer ones and that capitalism gave mankind the longest period of peace in history—a period during which there were no wars involving the entire civilized world—from the end of the Napoleonic wars in 1815 to the outbreak of World War I in 1914, with the exceptions of the Franco-Prussian War (1870), the Spanish–American War (1898), and the American Civil War (1860–1863), which notably occurred in perhaps the most liberal economy in the world at the beginning of the industrial revolution.
Cobdenism.
Proponents of cobdenism claim that by removing tariffs and creating international free trade wars would become impossible, because free trade prevents a nation from becoming self-sufficient, which is a requirement for long wars.
However, free trade does not prevent a nation from establishing some sort of emergency plan to become temporarily self-sufficient in case of war or that a nation could simply acquire what it needs from a different nation. A good example of this is World War I, during which both Britain and Germany became partially self-sufficient. This is particularly important because Germany had no plan for creating a War economy.
More generally, free trade—while not making wars impossible—can make wars, and restrictions on trade caused by wars, very costly for international companies with production, research, and sales in many different nations. Thus, a powerful lobby—unless there are only national companies—will argue against wars.
Mutual assured destruction.
Mutual assured destruction is a doctrine of military strategy in which a full-scale use of nuclear weapons by two opposing sides would effectively result in the destruction of both belligerents. Proponents of the policy of mutual assured destruction during the Cold War attributed this to the increase in the lethality of war to the point where it no longer offers the possibility of a net gain for either side, thereby making wars pointless.
United Nations Charter and International law.
After World War II, the United Nations was established by the United Nations Charter to "save successive generations from the two scourge of war which twice in our lifetime has brought untold sorrow to mankind" (Preamble). The Preamble to the United Nations Charter also aims to further the adoption of fundamental human rights, to respect obligations to sources of international law as well as to unite the strength of independent countries in order to maintain international peace and security. All treaties on international human rights law make reference to or consider "the principles proclaimed in the Charter of the United Nations, recognition of the inherent dignity and of the equal and inalienable rights of all members of the human family is the foundation of freedom, justice and "peace in the world".
Globalization.
Gordon B. Hinckley saw a trend in national politics by which city-states and nation-states have unified and suggests that the international arena will eventually follow suit. Many countries such as China, Italy, the United States, Australia, Germany, India and Britain have unified into single nation-states with others like the European Union following suit, suggesting that further globalization will bring about a unified world order.
Isolationism and non-interventionism.
Proponents of isolationism and non-interventionism claim that a world made up of many nations can peacefully coexist as long as they each establish a stronger focus on domestic affairs and do not try to impose their will on other nations.
Non-interventionism should not be confused with isolationism. Isolationism, like non-interventionism, advises avoiding interference into other nation's internal affairs but also emphasizes protectionism and restriction of international trade and travel. Non-interventionism, on the other hand, advocates combining free trade (like Cobdenism) with political and military non-interference.
Nations like Japan are perhaps the best known for establishing isolationist policies in the past. The Japanese Shogun Tokugawa initiated the Edo Period, an isolationist period where Japan cut itself off from the world as a whole.
Self-organized peace.
World peace has been depicted as a consequence of local, self-determined behaviors that inhibit the institutionalization of power and ensuing violence. The solution is not so much based on an agreed agenda, or an investment in higher authority whether divine or political, but rather a self-organized network of mutually supportive mechanisms, resulting in a viable politico-economic social fabric. The principle technique for inducing convergence is thought experiment, namely backcasting, enabling anyone to participate no matter what cultural background, religious doctrine, political affiliation or age demographic. Similar collaborative mechanisms are emerging from the Internet around open-source projects, including Wikipedia, and the evolution of other social media.
Economic norms theory.
Economic norms theory links economic conditions with institutions of governance and conflict, distinguishing personal clientelist economies from impersonal market-oriented ones, identifying the latter with permanent peace within and between nations.
Through most of human history societies have been based on personal relations: individuals in groups know each other and exchange favors. Today in most lower-income societies hierarchies of groups distribute wealth based on personal relationships among group leaders, a process often linked with clientelism and corruption. Michael Mousseau argues that in this kind of socio-economy conflict is always present, latent or overt, because individuals depend on their groups for physical and economic security and are thus loyal to their groups rather than their states, and because groups are in a constant state of conflict over access to state coffers. Through processes of bounded rationality, people are conditioned towards strong in-group identities and are easily swayed to fear outsiders, psychological predispositions that make possible sectarian violence, genocide, and terrorism.
Market-oriented socio-economies are integrated not with personal ties but the impersonal force of the market where most individuals are economically dependent on trusting strangers in contracts enforced by the state. This creates loyalty to a state that enforces the rule of law and contracts impartially and reliably and provides equal protection in the freedom to contract – that is, liberal democracy. Wars cannot happen within or between nations with market-integrated economies because war requires the harming of others, and in these kinds of economies everyone is always economically better off when others in the market are also better off, not worse off. Rather than fight, citizens in market-oriented socio-economies care deeply about everyone’s rights and welfare, so they demand economic growth at home and economic cooperation and human rights abroad. In fact, nations with market-oriented socio-economies tend to agree on global issues and not a single fatality has occurred in any dispute between them.
Economic norms theory should not be confused with classical liberal theory. The latter assumes that markets are natural and that freer markets promote wealth. In contrast, Economic norms theory shows how market-contracting is a learned norm, and state spending, regulation, and redistribution are necessary to ensure that almost everyone can participate in the “social market” economy, which is in everyone’s interests. One proposed mechanism for world peace involves consumer purchasing of renewable and equitable local food and power sources involving artificial photosynthesis ushering in a period of social and ecological harmony known as the Sustainocene.
International Day of Peace.
The International Day of Peace, sometimes unofficially known as World Peace Day, is observed annually on 21 September. It is dedicated to peace, and specifically the absence of war and violence, such as might be occasioned by a temporary ceasefire in a combat zone for humanitarian aid access. The day was first celebrated in 1982, and is kept by many nations, political groups, military groups, and peoples. In 2013, for the first time, the Day has been dedicated to peace education, i.e. by the key preventive means to reduce war sustainably.
Religious views.
Many religions and religious leaders have expressed a desire for an end to violence.
Bahá'í Faith.
The central aim of the Bahá'í Faith is the establishment of the unity of the peoples of the world. Bahá'u'lláh, the founder of the Bahá'í Faith, stated in no uncertain terms, "the fundamental purpose animating the Faith of God and His Religion is to safeguard the interests and promote the unity of the human race ..." In His writings, Bahá'u'lláh described two distinct stages of world peace – a lesser peace and a most great peace. 
The lesser peace is essentially a collective security agreement between the nations of the world. In this arrangement, nations agree to protect one another by rising up against an aggressor nation, should it seek the usurpation of territory or the destruction of its neighbors. The lesser peace is limited in scope and is concerned with the establishment of basic order and the universal recognition of national borders and the sovereignty of nations. Bahá'ís believe that the lesser peace is taking place largely through the operation of the Divine Will, and that Bahá'í influence on the process is relatively minor. 
The most great peace is the eventual end goal of the lesser peace and is envisioned as a time of spiritual and social unity – a time when the peoples of the world genuinely identify with and care for one another, rather than simply tolerating one other's existence. The Bahá'ís view this process as taking place largely as a result of the spread of Bahá'í teachings, principles and practices throughout the world. The larger world peace process and its foundational elements are addressed in the document The Promise of World Peace, written by the Universal House of Justice.
Buddhism.
Many Buddhists believe that world peace can only be achieved if we first establish peace within our minds. The idea is that anger and other negative states of mind are the cause of wars and fighting. Buddhists believe people can live in peace and harmony only if we abandon negative emotions such as anger in our minds and cultivate positive emotions such as love and compassion. As with all Dharmic religions (Hinduism, Jainism, Buddhism and Sikhism), ahimsa (avoidance of violence) is a central concept.
Peace Pagodas are monuments that are built to symbolize and inspire world peace and have been central to the peace movement throughout the years. These are typically of Buddhist origin, being built by the Japanese Buddhist organisation Nipponzan Myohoji. They exist around the world in cities such as London, Vienna, New Delhi, Tokyo and Lumbini.
Christianity.
The basic Christian ideal specifies that peace can only come by the Word and love of God, which is perfectly demonstrated in the life of Christ: "Peace I leave with you; my peace I give you. I do not give to you as the world gives. Do not let your hearts be troubled and do not be afraid."
As christologically interpreted from , whereupon the "Word of the Lord" is established on the earth, the material human-political result will be 'nation not taking up sword against nation; nor will they train for war anymore'. Christian world peace necessitates the living of a proactive life replete with all good works in direct light of the Word of God. The details of such a life can be observed in the Gospels, especially the historically renowned Sermon on the Mount, where forgiving those who do wrong things against oneself is advocated among other pious precepts.
However, not all Christians expect a lasting world peace on this earth:
"Do not suppose that I have come to bring peace to the earth. I did not come to bring peace, but a sword. For I have come to turn a man against his father, a daughter against her mother, a daughter-in-law against her mother-in-law—a man's enemies will be the members of his own household."
 Many Christians believe that world peace is expected to be manifest upon the "new earth" that is promised in Christian scripture such as .
The Roman Catholic religious conception of "Consecration of Russia", related to the Church's high-priority Fátima Marian apparitions, promises "world peace" as a result of this process being fulfilled.
Hinduism.
Traditionally, Hinduism has adopted an ancient Sanskrit phrase "Vasudha eva kutumbakam", which translates as "The world is one family." The essence of this concept is the observation that only base minds see dichotomies and divisions. The more we seek wisdom, the more we become inclusive and free our internal spirit from worldly illusions or "Maya". World peace is hence only achieved through internal means—by liberating ourselves from artificial boundaries that separate us all. As with all Dharmic religions (Hinduism, Jainism, Buddhism and Sikhism), ahimsa (avoidance of violence) is a central concept.
Islam.
According to Islam, faith in only one God and having common parents Adam and Eve is the greatest reason for humans to live together with peace and brotherhood. Islamic view of global peace is mentioned in the Quran where the whole of humanity is recognized as one family. All the people are children of Adam and Eve. The purpose of the Islamic faith is to help people recognize their own natural inclination towards their fraternity. According to Islamic eschatology the whole world will be united under the leadership of prophet Isa (Jesus) in his second coming. At that time love, justice and peace will be so abundant that the world will be in likeness of paradise.
Judaism.
Judaism teaches that at some future time a Jewish Messiah will rise up to bring all Jews back to the Land of Israel, followed by everlasting global peace and prosperity. This idea originates from passages in the Written Bible and the Oral Bible.
And he shall judge between the nations and reprove many peoples, and they shall beat their swords into plowshares and their spears into pruning hooks; nation shall not lift the sword against nation, neither shall they learn war anymore.
There also exists the idea of "Tikkun olam" (Repairing the World). "Tikkun olam" is accomplished through various means, such as ritualistically performing God's commandments, charity and social justice, as well as through example persuading the rest of the world to behave morally. This would result in the beginning of the Messianic Age. It has been said that in every generation, a person is born with the potential to be the Mashiach. If the time is right for the messianic age within that person's lifetime, then that person will be the mashiach. But if that person dies before he completes the mission of the mashiach, then that person is not the mashiach.
Jainism.
Compassion for all life, human and non-human, is central to Jainism.They have adopted the wordings of Lord Mahvira Jiyo aur Jeeno Do Human life is valued as a unique, rare opportunity to reach enlightenment; to kill any person, no matter what crime he may have committed, is considered unimaginably abhorrent. It is a religion that requires monks and laity, from all its sects and traditions, to be vegetarian. Some Indian regions, such as Gujarat, have been strongly influenced by Jains and often the majority of the local Hindus of every denomination have also become vegetarian. Famous quote on World Peace as per jainism by a 19th Century Indian Legend, Virchand Gandhi "May peace rule the universe; may peace rule in kingdoms and empires; may peace rule in states and in the lands of the potentates; may peace rule in the house of friends and may peace also rule in the house of enemies." As with all Dharmic religions (Hinduism, Jainism, Buddhism and Sikhism), ahimsa (avoidance of violence) is a central concept.
Sikhism.
Peace comes from God. Meditation, the means of communicating with God, is unfruitful without the noble character of a devotee, there can be no worship without performing good deeds. Guru Nanak stressed now "kirat karō": that a Sikh should balance work, worship, and charity, and should defend the rights of all creatures, and in particular, fellow human beings. They are encouraged to have a "chaṛdī kalā", or "optimistic" - "resilience", view of life. Sikh teachings also stress the concept of sharing—"vaṇḍ chakkō"—through the distribution of free food at Sikh gurdwaras ("laṅgar"), giving charitable donations, and working for the good of the community and others ("sēvā"). Sikhs believe that no matter what race, sex, or religion one is, all are equal in God's eyes. Men and women are equal and share the same rights, and women can lead in prayers. As with all Dharmic religions (Hinduism, Jainism, Buddhism and Sikhism), ahimsa (avoidance of violence) is a central concept.
Economic implications.
A report in May 2011 on the Global Peace Index highlighted that had the world been 25% more peaceful in the past year, the global economy would have benefited by an additional $2 trillion, which would account for 2% of global GDP per annum required to mitigate global warming, cover all costs to achieve the Millennium Development Goals, cancel all public debt held by Greece, Ireland and Portugal, and cover the rebuilding costs for the 2011 Tōhoku earthquake and tsunami.

</doc>
<doc id="45511" url="http://en.wikipedia.org/wiki?curid=45511" title="Treeshrew">
Treeshrew

The treeshrews (or tree shrews or banxrings) are small mammals native to the tropical forests of Southeast Asia. They make up the families Tupaiidae, the treeshrews, and Ptilocercidae, the pen-tailed treeshrew, and the entire order Scandentia. The 20 species are placed in five genera. Treeshrews have a higher brain to body mass ratio than any other mammals, including humans, but high ratios are not uncommon for animals weighing less than a kilogram.
Though called 'treeshrews', they are not true shrews – though they were previously classified in Insectivora – and not all species live in trees. Among other things, treeshrews eat "Rafflesia" fruit.
Among orders of mammals, treeshrews are closely related to primates, and have been used as an alternative to primates in experimental studies of myopia, psychosocial stress, and hepatitis.
Characteristics.
Treeshrews are slender animals with long tails and soft, greyish to reddish-brown fur. The terrestrial species tend to be larger than the arboreal forms, and to have larger claws, which they use for digging up insect prey. They are omnivorous, feeding on insects, small vertebrates, fruit, and seeds. They have poorly developed canine teeth and unspecialised molars, with an overall dental formula of: 2.1.3.33.1.3.3
Treeshrews have good vision, which is binocular in the case of the more arboreal species. Most are diurnal, although the pen-tailed treeshrew is nocturnal.
Female treeshrews have a gestation period of 45 to 50 days and give birth to up to three young in nests lined with dry leaves inside tree hollows. The young are born blind and hairless, but are able to leave the nest after about a month. During this period, the mother provides relatively little maternal care, visiting her young only for a few minutes every other day to suckle them. Treeshrews reach sexual maturity after around four months, and breed for much of the year, with no clear breeding season in most species.
These animals live in small family groups, which defend their territory from intruders. They mark their territories using various scent glands, or urine, depending on the particular species.
The name "Tupaia" is derived from "tupai", the Malay word for squirrel, and was provided by Sir Stamford Raffles.
The pen-tailed Treeshrew in Malaysia is able to consume large amounts of naturally fermented nectar of up to 3.8% alcohol content the entire year without having any effects on behaviour. An investigation as to how these animals cope with this diet is still ongoing.
Classification.
Treeshrews were moved from Insectivora to the Primates order because of certain internal similarities to the latter (for example, similarities in the brain anatomy, highlighted by Sir Wilfred Le Gros Clark), and classified as a "primitive prosimian". However, molecular phylogenetic studies have strongly suggested the treeshrews should be given the same rank (order) as the primates and, with the primates and the flying lemurs (colugos), belong to the clade Euarchonta. According to this classification, the Euarchonta are sister to the Glires (lagomorphs and rodents), and the two groups are combined into the clade Euarchontoglires. Other arrangements of these orders were proposed in the past.
Fossil record.
The fossil record of treeshrews is poor. The oldest putative treeshrew, "Eodendrogale parva", is from the Middle Eocene of Henan, China, but the identity of this animal is uncertain. Other fossils have come from the Miocene of Thailand, Pakistan, India, and Yunnan, China, as well as the Pliocene of India. Most belong to the family Tupaiidae, but some still-undescribed fossils from Yunnan are thought to be closer to the pen-tailed treeshrew. Named fossil species include "Prodendrogale yunnanica", "Prodendrogale engesseri", and "Tupaia storchi" from Yunnan, "Tupaia miocenica" from Thailand, and "Palaeotupaia sivalicus" from India.

</doc>
<doc id="45515" url="http://en.wikipedia.org/wiki?curid=45515" title="Free good">
Free good

A free good is a good that is not scarce, and therefore is available without limits. A free good is available in as great a quantity as desired with zero opportunity cost to society. 
A good that is made available at zero price is not necessarily a free good. For example, a shop might give away its stock in its promotion, but producing these goods would still have required the use of scarce resources. 
Examples of free goods are ideas and works that are reproducible at zero cost, or almost zero cost. For example, if someone invents a new device, many people could copy this invention, with no danger of this "resource" running out. Other examples include computer programs and web pages.
Earlier schools of economic thought proposed a third type of free good: resources that are scarce but so abundant in nature that there is enough for everyone to have as much as they want. Examples in textbooks included seawater and air.
Intellectual property laws such as copyrights and patents have the effect of converting some intangible goods to scarce goods. Even though these works are free goods by definition and can be reproduced at minimal cost, the production of these works does require scarce resources, such as skilled labour. Thus these laws are used to give exclusive rights to the creators, in order to encourage resources to be appropriately allocated to these activities.
Many post scarcity futurists theorize that advanced nanotechnology with the ability to turn any kind of material automatically into any other combination of equal mass will make all goods essentially free goods, since all raw materials and manufacturing time will become perfectly interchangeable. 

</doc>
<doc id="45519" url="http://en.wikipedia.org/wiki?curid=45519" title="Seven deadly sins">
Seven deadly sins

The seven deadly sins, also known as the capital vices or cardinal sins, is a classification of vices (part of Christian ethics) that has been used since early Christian times to educate and instruct Christians concerning fallen humanity's tendency to sin. In the currently recognized version, the sins are usually given as wrath, greed, sloth, pride, lust, envy, and gluttony. Each is a form of Idolatry-of-Self wherein the subjective reigns over the objective.
The Catholic Church divides sin into two categories: venial sins, in which guilt is relatively minor, and the more severe mortal sins. According to the Catechism of the Catholic Church, a mortal or deadly sin is believed to destroy the life of grace and charity within a person. "Mortal sin, by attacking the vital principle within us – that is, charity – necessitates a new initiative of God's mercy and a conversion of heart which is normally accomplished within the setting of the sacrament of reconciliation."
According to Catholic moral thought, the seven deadly sins are not discrete from other sins, but are instead the origin ("capital" comes from the Latin "caput", head) of the others. Vices can be either venial or mortal, depending on the situation, but "are called 'capital' because they engender other sins, other vices".
Beginning in the early 14th century, the popularity of the seven deadly sins as a theme among European artists of the time eventually helped to ingrain them in many areas of Catholic culture and Catholic consciousness in general throughout the world. One means of such ingraining was the creation of the mnemonic acronym "SALIGIA" based on the first letters in Latin of the seven deadly sins: "superbia", "avaritia", "luxuria", "invidia", "gula", "ira", "acedia".
Biblical lists.
In the Book of Proverbs 6:16-19, among the verses traditionally associated with King Solomon, it states that the Lord specifically regards "six things the Lord hateth, and seven that are an abomination unto Him", namely:
Another list, given this time by the Epistle to the Galatians (Galatians 5:19-21), includes more of the traditional seven sins, although the list is substantially longer: adultery, fornication, uncleanness, lasciviousness, idolatry, sorcery, hatred, variance, emulations, wrath, strife, seditions, heresies, envyings, murders, drunkenness, revellings, "and such like". Since the apostle Paul goes on to say that the persons who practice these sins "shall not inherit the Kingdom of God", they are usually listed as (possible) mortal sins rather than capital vices.
History.
The modern concept of the seven deadly sins is linked to the works of the 4th century monk Evagrius Ponticus, who listed eight "evil thoughts" in Greek as follows:
They were translated into the Latin of Western Christianity (largely due to the writings of John Cassian), thus becoming part of the Western tradition's spiritual pietas (or Catholic devotions), as follows:
These "evil thoughts" can be categorized into three types:
In AD 590, a little over two centuries after Evagrius wrote his list, Pope Gregory I revised this list to form the more common "Seven Deadly Sins", by folding "(sorrow/despair/despondency)" into "acedia", "vainglory" into "pride", and adding "envy". In the order used by Pope Gregory, and repeated by Dante Alighieri (1265-1321) centuries later in his epic poem "The Divine Comedy", the seven deadly sins are as follows:
The identification and definition of the seven deadly sins over their history has been a fluid process and the idea of what each of the seven actually encompasses has evolved over time. Additionally, as a result of semantic change:
It is this revised list that Dante uses. The process of semantic change has been aided by the fact that the personality traits are not collectively referred to, in either a cohesive or codified manner, by the Bible itself; other literary and ecclesiastical works were instead consulted, as sources from which definitions might be drawn. Part II of Dante's "Divine Comedy", "Purgatorio", has almost certainly been the best known source since the Renaissance.
The modern Catholic Catechism lists the sins in Latin as ""superbia, avaritia, invidia, ira, luxuria, gula, pigritia seu acedia", with an English translation of "pride, avarice, envy, wrath, lust, gluttony, and sloth/acedia". Each of the seven deadly sins now also has an opposite among corresponding seven holy virtues (sometimes also referred to as the "contrary virtues"). In parallel order to the sins they oppose, the seven holy virtues are humility, charity, kindness, patience, chastity, temperance, and diligence (see below).
Historical and modern definitions.
Lust.
Lust, or lechery (carnal "luxuria"") is an intense and uncontrolled desire. It is usually thought of as uncontrolled sexual wants, however the word was originally a general term for desire. Therefore lust could include the uncontrolled desire for money, food, fame, or power.
In Dante's "Purgatorio", the penitent walks within flames to purge himself of lustful thoughts and feelings. In Dante's "Inferno", unforgiven souls of the sin of lust are blown about in restless hurricane-like winds symbolic of their own lack of self-control to their lustful passions in earthly life.
Gluttony.
Derived from the Latin "gluttire", meaning to gulp down or swallow, gluttony (Latin, "gula") is the over-indulgence and over-consumption of anything to the point of waste.
In Christianity, it is considered a sin if the excessive desire for food causes it to be withheld from the needy.
Because of these scripts, gluttony can be interpreted as selfishness; essentially placing concern with one's own interests above the well-being or interests of others.
Medieval church leaders (e.g., Thomas Aquinas) took a more expansive view of gluttony, arguing that it could also include an obsessive anticipation of meals, and the constant eating of delicacies and excessively costly foods. Aquinas went so far as to prepare a list of six ways to commit gluttony, comprising:
Greed.
Greed (Latin, "avaritia"), also known as avarice, cupidity or covetousness, is, like lust and gluttony, a sin of excess. However, greed (as seen by the church) is applied to a very excessive or rapacious desire and pursuit of material possessions. Thomas Aquinas wrote, "Greed is a sin against God, just as all mortal sins, in as much as man condemns things eternal for the sake of temporal things." In Dante's Purgatory, the penitents were bound and laid face down on the ground for having concentrated too much on earthly thoughts. Hoarding of materials or objects, theft and robbery, especially by means of violence, trickery, or manipulation of authority are all actions that may be inspired by Greed. Such misdeeds can include simony, where one attempts to purchase or sell sacraments, including Holy Orders and, therefore, positions of authority in the Church hierarchy.
As defined outside of Christian writings, greed is an inordinate desire to acquire or possess more than one needs, especially with respect to material wealth.
Sloth.
Sloth (Latin, "acedia") can entail different vices. While sloth is sometimes defined as physical laziness, spiritual laziness is emphasized. Failing to develop spiritually will lead to becoming guilty of sloth. In the Christian faith, sloth rejects grace and God.
Sloth has also been defined as a failure to do things that one should do. By this definition, evil exists when good men fail to act.
Edmund Burke (1729-1797) wrote in "Present Discontents" (II. 78) "No man, who is not inflamed by vain-glory into enthusiasm, can flatter himself that his single, unsupported, desultory, unsystematic endeavours are of power to defeat the subtle designs and united Cabals of ambitious citizens. When bad men combine, the good must associate; else they will fall, one by one, an unpitied sacrifice in a contemptible struggle."
Over time, the "acedia" in Pope Gregory's order has come to be closer in meaning to sloth. The focus came to be on the consequences of acedia rather than the cause, and so, by the 17th century, the exact "deadly sin" referred to was believed to be the failure to utilize one's talents and gifts. Even in Dante's time there were signs of this change; in his "Purgatorio" he had portrayed the penance for acedia as running continuously at top speed.
Wrath.
Wrath (Latin, "ira"), also known as "rage", may be described as inordinate and uncontrolled feelings of hatred and anger. Wrath, in its purest form, presents with self-destructiveness, violence, and hate that may provoke feuds that can go on for centuries. Wrath may persist long after the person who did another a grievous wrong is dead. Feelings of anger can manifest in different ways, including impatience, revenge, and self-destructive behavior, such as drug abuse or suicide.
Wrath is the only sin not necessarily associated with selfishness or self-interest, although one can of course be wrathful for selfish reasons, such as jealousy (closely related to the sin of envy). Dante described vengeance as "love of justice perverted to revenge and spite". In its original form, the sin of wrath also encompassed anger pointed internally as well as externally. Thus suicide was deemed as the ultimate, albeit tragic, expression of hatred directed inwardly, a final rejection of God's gifts. 
Envy.
Like greed and lust, Envy (Latin, "invidia") is characterized by an insatiable desire. Envy is similar to jealousy in that they both feel discontent towards someone's traits, status, abilities, or rewards. The difference is the envious also desire the entity and covet it.
Envy can be directly related to the Ten Commandments, specifically, "Neither shall you desire... anything that belongs to your neighbour."
Dante defined this as "a desire to deprive other men of theirs". In Dante's Purgatory, the punishment for the envious is to have their eyes sewn shut with wire because they have gained sinful pleasure from seeing others brought low. Aquinas described envy as "sorrow for another's good".
Pride.
In almost every list, pride (Latin, "superbia"), or hubris (Greek), is considered the original and most serious of the seven deadly sins, and the source of the others. It is identified as believing that one is essentially better than others, failing to acknowledge the accomplishments of others, and excessive admiration of the personal self (especially holding self out of proper position toward God); it also includes vainglory (Latin, "vanagloria") which is unjustified boasting. Dante's definition of pride was "love of self perverted to hatred and contempt for one's neighbour". In Jacob Bidermann's medieval miracle play, "Cenodoxus", pride is the deadliest of all the sins and leads directly to the damnation of the titulary famed Parisian doctor. In perhaps the best-known example, the story of Lucifer, pride (his desire to compete with God) was what caused his fall from Heaven, and his resultant transformation into Satan. In Dante's "Divine Comedy", the penitents are burdened with stone slabs on their necks which force them to keep their heads bowed.
Historical sins.
Acedia.
Acedia (Latin, "acedia") (from Greek ἀκηδία) is the neglect to take care of something that one should do. It is translated to apathetic listlessness; depression without joy. It is related to melancholy: "acedia" describes the behaviour and "melancholy" suggests the emotion producing it. In early Christian thought, the lack of joy was regarded as a willful refusal to enjoy the goodness of God and the world God created; by contrast, apathy was considered a refusal to help others in time of need.
When Thomas Aquinas described "acedia" in his interpretation of the list, he described it as an "uneasiness of the mind", being a progenitor for lesser sins such as restlessness and instability. Dante refined this definition further, describing acedia as the "failure to love God with all one's heart, all one's mind and all one's soul"; to him it was the "middle sin", the only one characterised by an absence or insufficiency of love. Some scholars have said that the ultimate form of acedia was despair which leads to suicide.
Vainglory.
Vainglory (Latin, "vanagloria") is unjustified boasting. Pope Gregory viewed it as a form of pride, so he folded "vainglory" into pride for his listing of sins. 
The Latin term "gloria" roughly means "boasting", although its English cognate - "glory" - has come to have an exclusively positive meaning; historically, "vain" roughly meant "futile", but by the 14th century had come to have the strong narcissistic undertones, of irrelevant accuracy, that it retains today. As a result of these semantic changes, "vainglory" has become a rarely used word in itself, and is now commonly interpreted as referring to "vanity" (in its modern narcissistic sense).
Catholic seven virtues.
The Catholic Church also recognizes seven virtues, which correspond inversely to each of the seven deadly sins. 
Associations with demons.
In 1409-1410 "The Lanterne of Light" (an anonymous English Lollard tract often erroneously attributed to Wycliffe) paired each of the deadly sins with a demon, who tempted people by means of the associated sin. According to this classification system, the pairings are as follows:
In 1589, Peter Binsfeld again paired each of the deadly sins with a demon, in a slightly contrasting classification system, whereby the pairings are as follows:
In Marlowe's Doctor Faustus, there is a "parade" of the seven deadly sins that is conducted by Mephistopheles, Satan, and Beelzebub suggesting that the demons do not match with each deadly sin, but the demons are in command of the seven deadly sins.
Patterns.
According to a 2009 study by a Jesuit scholar, the most common deadly sin confessed by men is lust, and for women, pride. It was unclear whether these differences were due to different rates of commission, or different views on what "counts" or should be confessed.
Cultural references.
The seven deadly sins have long been a source of inspiration for writers and artists, from medieval works such as Dante Alighieri's "Divine Comedy", to modern works such as the film "Se7en" and the manga/anime series "Fullmetal Alchemist" and "Nanatsu no Taizai".

</doc>
<doc id="45522" url="http://en.wikipedia.org/wiki?curid=45522" title="Toltec">
Toltec

The Toltec culture is an archaeological Mesoamerican culture that dominated a state centered in Tula, in the early post-classic period of Mesoamerican chronology (ca 800–1000 CE). The later Aztec culture saw the Toltecs as their intellectual and cultural predecessors and described Toltec culture emanating from "Tōllān" (Nahuatl for Tula) as the epitome of civilization; indeed in the Nahuatl language the word "Tōltēcatl" (singular) or "Tōltēcah" (plural) came to take on the meaning "artisan". The Aztec oral and pictographic tradition also described the history of the Toltec Empire, giving lists of rulers and their exploits.
Among modern scholars it is a matter of debate whether the Aztec narratives of Toltec history should be given credence as descriptions of actual historical events. While all scholars acknowledge that there is a large mythological part of the narrative, some maintain that by using a critical comparative method some level of historicity can be salvaged from the sources. Others maintain that continued analysis of the narratives as sources of actual history is futile and hinders access to actual knowledge of the culture of Tula, Hidalgo.
Other controversies relating to the Toltecs include the question of how best to understand the reasons behind the perceived similarities in architecture and iconography between the archaeological site of Tula and the Mayan site of Chichén Itzá. No consensus has yet emerged about the degree or direction of influence between these two sites. Another source of controversy is the claims by New Age authors such as Carlos Castaneda and Don Miguel Ruiz, who claim to represent "Toltec" teachings, for this tradition of knowledge see Toltec (Castaneda).
Archaeology.
Some archaeologists such as Richard Diehl, argue for the existence of a Toltec archaeological horizon characterized by certain stylistic traits associated with Tula, Hidalgo and extending to other cultures and polities in Mesoamerica. Traits associated with this horizon are: The Mixteca-Puebla style of iconography, Tohil plumbate ceramic ware and Silho or X-Fine Orange Ware ceramics. The presence of stylistic traits associated with Tula in Chichén Itzá is also taken as evidence for a Toltec horizon. Especially the nature of interaction between Tula and Chichén Itzá has been controversial with scholars arguing for either military conquest of Chichén Itzá by Toltecs, Chichén Itzá establishing Tula as a colony or only loose connections between the two. The existence of any meaning of the Mixteca-Puebla art style has also been questioned.
A contrary viewpoint is argued in a 2003 study by Michael E. Smith and Lisa Montiel who compare the archaeological record related to Tula Hidalgo to those of the polities centered in Teotihuacan and Tenochtitlan. They conclude that relative to the influence exerted in Mesoamerica by Teotihuacan and Tenochtitlan, Tula's influence on other cultures was negligible and was probably not deserving of being defined as an empire, but more of a kingdom. While Tula does have the urban complexity expected of an imperial capital, its influence and dominance was not very far reaching. Evidence for Tula's participation in extensive trade networks has been uncovered, for example, the remains of a large obsidian workshop.
History of research.
The debate about the nature of the Toltec culture goes back to the late 19th century. Mesoamericanist scholars such as Veitia, Manuel Orozco y Berra, Charles Etienne Brasseur de Bourbourg, and Francisco Clavigero all read the Aztec chronicles and believed them to be realistic historic descriptions of a pan-Mesoamerican empire based at Tula, Hidalgo. This historicist view was first challenged by Daniel Garrison Brinton who argued that the "Toltecs" as described in the Aztec sources were merely one of several Nahuatl-speaking city-states in the postclassic period, and not a particularly influential one at that. He attributed the Aztec view of the Toltecs to the "tendency of the human mind to glorify the good old days", and the confounding of the place of Tollan with the myth of the struggle between Quetzalcoatl and Tezcatlipoca. Désiré Charnay, the first archaeologist to work at Tula, Hidalgo, defended the historicist views based on his impression of the Toltec capital, and was the first to note similarities in architectural styles between Tula and Chichén Itza. This led him to posit the theory that Chichén Itzá had been violently taken over by a Toltec military force under the leadership of Kukulcan. Following Charnay the term "Toltec" has since been associated with the influx of certain Central Mexican cultural traits into the Mayan sphere of dominance that took place in the late classic and early Postclassic periods; the Postclassic Mayan civilizations of Chichén Itzá, Mayapán and the Guatemalan highlands have been referred to as "Toltecized" or "Mexicanized" Mayas.
The historicist school of thought persisted well in to the 20th century, represented in the works of scholars such as David Carrasco, Miguel León Portilla, Nigel Davies and H. B. Nicholson, which all held the Toltecs to have been an actual ethnic group. This school of thought connected the "Toltecs" to the archaeological site of Tula, which was taken to be the Tollan of Aztec myth. This tradition assumes that much of central Mexico was dominated by a "Toltec empire" between the 10th and 12th century CE. The Aztecs referred to several Mexican citystates as Tollan, "Place of Reeds", such as "Tollan Cholollan". Archaeologist Laurette Sejourné, followed by the historian Enrique Florescano, have argued that the "original" Tollan was probably Teotihuacán. Florescano adds that the Mayan sources refer to Chichén Itzá when talking about the mythical place Zuyua (Tollan).
Many historicists such as H. B. Nicholson (2001 (1957)) and Nigel Davies (1977) were fully aware that the Aztec chronicles were a mixture of mythical and historical accounts; this led them to try to separate the two by applying a comparative approach to the varying Aztec narratives. For example they seek to discern between the deity Quetzalcoatl and a Toltec ruler often referred to as Topiltzin Ce Acatl Quetzalcoatl.
Toltecs as myth.
In recent decades the historicist position has fallen out of favor for a more critical and interpretive approach to the historicity of the Aztec mythical accounts based on the original approach of Brinton. This approach applies a different understanding of the word Toltec to the interpretation of the Aztec sources, interpreting it as largely a mythical and philosophical construct by either the Aztecs or Mesoamericans generally that served to symbolize the might and sophistication of several civilizations during the Mesoamerican Postclassic period.
Scholars such as Michel Graulich (2002) and Susan D. Gillespie (1989) maintained that the difficulties in salvaging historic data from the Aztec accounts of Toltec history are too great to overcome. For example, there are two supposed Toltec rulers identified with Quetzalcoatl: the first ruler and founder of the Toltec dynasty and the last ruler, who saw the end of the Toltec glory and was forced into humiliation and exile. The first is described as a valiant triumphant warrior, but the last as a feeble and self-doubting old man. This caused Graulich and Gillespie to suggest that the general Aztec cyclical view of time, where events repeated themselves at the end and beginning of cycles or eras was being inscribed into the historical record by the Aztecs, making it futile to attempt to distinguish between a historical Topiltzin Ce Acatl and a Quetzalcoatl deity. Graulich argued that the Toltec era is best considered the fourth of the five Aztec mythical "Suns" or ages, the one immediately preceding the fifth sun of the Aztec people, presided over by Quetzalcoatl. This caused Graulich to consider that the only possibly historical data in the Aztec chronicles are the names of some rulers and possibly some of the conquests ascribed to them.
Furthermore, among the Nahuan peoples the word "Tolteca" was synonymous with artist, artisan or wise man, and ""toltecayotl"." "Toltecness" meant art, culture and civilization, and urbanism and was seen as the opposite of "Chichimecayotl" ("Chichimecness"), which symbolized the savage, nomadic state of peoples who had not yet become urbanized. This interpretation argues that any large urban center in Mesoamerica could be referred to as "Tollan" and its inhabitants as Toltecs – and that it was common practice among ruling lineages in Postclassic Mesoamerica to strengthen claims to power by claiming Toltec ancestry. Mesoamerican migration accounts often state that Tollan was ruled by Quetzalcoatl (or "Kukulkan" in Yucatec and "Q'uq'umatz" in K'iche'), a godlike mythical figure who was later sent into exile from Tollan and went on to found a new city elsewhere in Mesoamerica. Claims of Toltec ancestry and a ruling dynasty founded by Quetzalcoatl have been made by such diverse civilizations as the Aztec, the K'iche' and the Itza' Mayas.
While the skeptical school of thought does not deny that cultural traits of a seemingly central Mexican origin have diffused into a larger area of Mesoamerica, it tends to ascribe this to the dominance of Teotihuacán in the Classic period and the general diffusion of cultural traits within the region. Recent scholarship, then, does not see Tula, Hidalgo as the capital of the Toltecs of the Aztec accounts. Rather, it takes "Toltec" to mean simply an inhabitant of Tula during its apogee. Separating the term "Toltec" from those of the Aztec accounts, it attempts to find archaeological clues to the ethnicity, history and social organization of the inhabitants of Tula.

</doc>
<doc id="45524" url="http://en.wikipedia.org/wiki?curid=45524" title="Pseudorandom number generator">
Pseudorandom number generator

A pseudorandom number generator (PRNG), also known as a deterministic random bit generator (DRBG), is an algorithm for generating a sequence of numbers whose properties approximate the properties of sequences of random numbers. The PRNG-generated sequence is not truly random, because it is completely determined by a relatively small set of initial values, called the PRNG's "seed" (which may include truly random values). Although sequences that are closer to truly random can be generated using hardware random number generators, "pseudorandom" number generators are important in practice for their speed in number generation and their reproducibility.
PRNGs are central in applications such as simulations (e.g. for the Monte Carlo method), electronic games (e.g. for procedural generation), and cryptography. Cryptographic applications require the output not to be predictable from earlier outputs, and more elaborate algorithms, which do not inherit the linearity of simpler PRNGs, are needed.
Good statistical properties are a central requirement for the output of a PRNG. In general, careful mathematical analysis is required to have any confidence that a PRNG generates numbers that are sufficiently close to random to suit the intended use. John von Neumann cautioned about the misinterpretation of a PRNG as a truly random generator, and joked that "Anyone who considers arithmetical methods of producing random digits is, of course, in a state of sin."
Periodicity.
A PRNG can be started from an arbitrary initial state using a seed state. It will always produce the same sequence when initialized with that state. The "period" of a PRNG is defined thus: the maximum, over all starting states, of the length of the repetition-free prefix of the sequence. The period is bounded by the number of the states, usually measured in bits. However, since the length of the period potentially doubles with each bit of "state" added, it is easy to build PRNGs with periods long enough for many practical applications.
If a PRNG's internal state contains "n" bits, its period can be no longer than 2n results, and may be much shorter. For some PRNGs, the period length can be calculated without walking through the whole period. Linear Feedback Shift Registers (LFSRs) are usually chosen to have periods of exactly 2n−1. Linear congruential generators have periods that can be calculated by factoring. Although PRNGs will repeat their results after they reach the end of their period, a repeated result does not imply that the end of the period has been reached, since its internal state may be larger than its output; this is particularly obvious with PRNGs with a one-bit output.
Most PRNG algorithms produce sequences which are uniformly distributed by any of several tests. It is an open question, and one central to the theory and practice of cryptography, whether there is any way to distinguish the output of a high-quality PRNG from a truly random sequence without knowing the algorithm(s) used and the state with which it was initialized. The security of most cryptographic algorithms and protocols using PRNGs is based on the assumption that it is infeasible to distinguish use of a suitable PRNG from use of a truly random sequence. The simplest examples of this dependency are stream ciphers, which (most often) work by exclusive or-ing the plaintext of a message with the output of a PRNG, producing ciphertext. The design of cryptographically adequate PRNGs is extremely difficult, because they must meet additional criteria (see below). The size of its period is an important factor in the cryptographic suitability of a PRNG, but not the only one.
Potential problems with deterministic generators.
In practice, the output from many common PRNGs exhibit artifacts that cause them to fail statistical pattern-detection tests. These include:
Defects exhibited by flawed PRNGs range from unnoticeable (and unknown) to very obvious. An example was the RANDU random number algorithm used for decades on mainframe computers. It was seriously flawed, but its inadequacy went undetected for a very long time.
In many fields, much research work prior to the 21st century that relied on random selection or on Monte Carlo simulations, or in other ways relied on PRNGs, is much less reliable than it might have been as a result of using poor-quality PRNGs. Even today, caution is sometimes required, as illustrated by the following warning, which is given in the "International Encyclopedia of Statistical Science" (2010).
 The list of widely used generators that should be discarded is [long] ... Check the default [PRNG] of your favorite software and be ready to replace it if needed. This last recommendation has been made over and over again over the past 40 years. Perhaps amazingly, it remains as relevant today as it was 40 years ago.
As an illustration, consider the widely used programming language Java. As of 2014, Java still relies on a linear congruential generator (LCG) for a PRNG; yet it is well known that LCGs are of low quality (see further below).
The first PRNG to avoid major problems and still run fairly quickly was the Mersenne Twister (discussed below), which was published in 1998. Other high-quality PRNGs have since been developed.
Generators based on linear recurrences.
In the second half of the 20th century, the standard class of algorithms used for PRNGs comprised linear congruential generators. The quality of LCGs was known to be inadequate, but better methods were unavailable. Press et al. (2007) described the result thus: "If all scientific papers whose results are in doubt because of [LCGs and related] were to disappear from library shelves, there would be a gap on each shelf about as big as your fist".
A major advance in the construction of pseudorandom generators was the introduction of techniques based on linear recurrences on the two-element field; such generators are related to linear feedback shift registers.
The 1997 invention of the Mersenne twister, in particular, avoided many of the problems with earlier generators. The Mersenne Twister has a period of 219937−1 iterations (≈4.3×106001), is proven to be equidistributed in (up to) 623 dimensions (for 32-bit values), and at the time of its introduction was running faster than other statistically reasonable generators.
Subsequently, the WELL family of generators was developed to improve the quality of the Mersenne Twister, which has a too large state space and a very slow recovery from state spaces with a large number of zeroes. 
In 2003, George Marsaglia introduced the family of xorshift generators, again based on a linear recurrence. Such generators are extremely fast and, combined with a nonlinear operation, they pass strong statistical tests.
Cryptographically secure pseudorandom number generators.
A PRNG suitable for cryptographic applications is called a "cryptographically secure PRNG" (CSPRNG). A requirement for a CSPRNG is that an adversary not knowing the seed has only negligible advantage in distinguishing the generator's output sequence from a random sequence. In other words, while a PRNG is only required to pass certain statistical tests, a CSPRNG must pass all statistical tests that are restricted to polynomial time in the size of the seed. Though such property cannot be proven, strong evidence may be provided by reducing the CSPRNG to a problem that is assumed to be hard, such as integer factorization. In general, years of review may be required before an algorithm can be certified as a CSPRNG.
Some classes of CSPRNGs include the following:
It has been shown to be likely that the NSA has inserted an asymmetric backdoor into the NIST certified pseudorandom number generator Dual_EC_DRBG.
BSI evaluation criteria.
The German Federal Office for Information Security ("Bundesamt für Sicherheit in der Informationstechnik", BSI) has established four criteria for quality of deterministic random number generators. They are summarized here:
For cryptographic applications, only generators meeting the K3 or K4 standard are acceptable.
Mathematical definition.
Given
we call a function formula_19 (where formula_20 is the set of positive integers) a pseudo-random number generator for formula_1 given formula_4 taking values in formula_11 iff
It can be shown that if formula_28 is a pseudo-random number generator for the uniform distribution on formula_29 and if formula_30 is the CDF of some given probability distribution formula_1, then formula_32 is a pseudo-random number generator for formula_1, where formula_34 is the percentile of formula_1, i.e. formula_36. Intuitively, an arbitrary distribution can be simulated from a simulation of the standard uniform distribution.
Early approaches.
An early computer-based PRNG, suggested by John von Neumann in 1946, is known as the middle-square method. The algorithm is as follows: take any number, square it, remove the middle digits of the resulting number as the "random number", then use that number as the seed for the next iteration. For example, squaring the number "1111" yields "1234321", which can be written as "01234321", an 8-digit number being the square of a 4-digit number. This gives "2343" as the "random" number. Repeating this procedure gives "4896" as the next result, and so on. Von Neumann used 10 digit numbers, but the process was the same.
A problem with the "middle square" method is that all sequences eventually repeat themselves, some very quickly, such as "0000". Von Neumann was aware of this, but he found the approach sufficient for his purposes, and was worried that mathematical "fixes" would simply hide errors rather than remove them.
Von Neumann judged hardware random number generators unsuitable, for, if they did not record the output generated, they could not later be tested for errors. If they did record their output, they would exhaust the limited computer memories then available, and so the computer's ability to read and write numbers. If the numbers were written to cards, they would take very much longer to write and read. On the ENIAC computer he was using, the "middle square" method generated numbers at a rate some hundred times faster than reading numbers in from punched cards.
The middle-square method has since been supplanted by more elaborate generators.
Non-uniform generators.
Numbers selected from a non-uniform probability distribution can be generated using a uniform distribution PRNG and a function that relates the two distributions.
First, one needs the cumulative distribution function formula_37 of the target distribution formula_38:
Note that formula_40. Using a random number "c" from a uniform distribution as the probability density to "pass by", we get
so that
is a number randomly selected from distribution formula_38.
For example, the inverse of cumulative Gaussian distribution
formula_44 with an ideal uniform PRNG with range (0, 1) as input formula_45 would produce a sequence of (positive only) values with a Gaussian distribution; however
Similar considerations apply to generating other non-uniform distributions such as Rayleigh and Poisson.

</doc>
<doc id="45527" url="http://en.wikipedia.org/wiki?curid=45527" title="Linear congruential generator">
Linear congruential generator

A linear congruential generator (LCG) is an algorithm that yields a sequence of pseudo-randomized numbers calculated with a discontinuous piecewise linear equation. The method represents one of the oldest and best-known pseudorandom number generator algorithms. The theory behind them is relatively easy to understand, and they are easily implemented and fast, especially on computer hardware which can provide modulo arithmetic by storage-bit truncation.
The generator is defined by the recurrence relation:
where formula_2 is the sequence of pseudorandom values, and
are integer constants that specify the generator. If "c" = 0, the generator is often called a multiplicative congruential generator (MCG), or Lehmer RNG. If "c" ≠ 0, the method is called a "mixed congruential generator".
Period length.
The period of a general LCG is at most "m", and for some choices of factor "a" much less than that. Provided that the offset "c" is nonzero, the LCG will have a full period for all seed values if and only if:
These three requirements are referred to as the Hull-Dobell Theorem. While LCGs are capable of producing pseudorandom numbers which can pass formal tests for randomness, this is extremely sensitive to the choice of the parameters "c", "m", and "a".
Historically, poor choices had led to ineffective implementations of LCGs. A particularly illustrative example of this is RANDU, which was widely used in the early 1970s and led to many results which are currently being questioned because of the use of this poor LCG.
Parameters in common use.
The most efficient LCGs have an "m" equal to a power of 2, most often "m" = 232 or "m" = 264, because this allows the modulus operation to be computed by merely truncating all but the rightmost 32 or 64 bits. The following table lists the parameters of LCGs in common use, including built-in "rand()" functions in runtime libraries of various compilers.
As shown above, LCGs do not always use all of the bits in the values they produce. For example, the Java implementation operates with 48-bit values at each iteration but returns only their 32 most significant bits. This is because the higher-order bits have longer periods than the lower-order bits (see below). LCGs that use this truncation technique produce statistically better values than those that do not.
The Knuth representation for 3 variables is as below:
"X""n"+1 = (8121 "X""n" + 28411) mod 134456
Because there are only 134456 distinct possible values, according to the parameter definition, it tends to make it a bit more predictable. If "X""n" is even then "X""n"+1 will be odd, and vice versa, so the lowest order of bit oscillates at each step. This makes the generator to produce bits in each number that are usually not equally random.
Advantages and disadvantages of LCGs.
LCGs are fast and require minimal memory (typically 32 or 64 bits) to retain state. This makes them valuable for simulating multiple independent streams.
LCGs should not be used for applications where high-quality randomness is critical. For example, it is not suitable for a Monte Carlo simulation because of the serial correlation (among other things). They also must not be used for cryptographic applications; see cryptographically secure pseudo-random number generator for more suitable generators. If a linear congruential generator is seeded with a character and then iterated once, the result is a simple classical cipher called an affine cipher; this cipher is easily broken by standard frequency analysis.
LCGs tend to exhibit some severe defects. For instance, if an LCG is used to choose points in an n-dimensional space, the points will lie on, at most, (n!m)1/n hyperplanes (Marsaglia's Theorem, developed by George Marsaglia). This is due to serial correlation between successive values of the sequence "X""n". The spectral test, which is a simple test of an LCG's quality, is based on this fact.
A further problem of LCGs is that the lower-order bits of the generated sequence have a far shorter period than the sequence as a whole if "m" is set to a power of 2. In general, the "n"th least significant digit in the base "b" representation of the output sequence, where "b""k" = "m" for some integer "k", repeats with at most period "b""n".
Yet another problem is that LCGs are not suitable for parallel programming. Multiple threads may access the currently stored state simultaneously causing a race condition. In implementations which use same initialization for different threads, equal sequences of random numbers may occur on simultaneously executing threads. Random number generators, particularly for parallel computers, should not be trusted. It is strongly recommended to check the results of simulation with more than one RNG to check if bias is introduced. Among the recommended generators for use on a parallel computer include combined linear congruential generators using sequence splitting and lagged Fibonacci generators using independent sequences.
Nevertheless, for some applications LCGs may be a good option. For instance, in an embedded system, the amount of memory available is often severely limited. Similarly, in an environment such as a video game console taking a small number of high-order bits of an LCG may well suffice. The low-order bits of LCGs when m is a power of 2 should never be relied on for any degree of randomness whatsoever. Indeed, simply substituting 2"n" for the modulus term reveals that the low order bits go through very short cycles. In particular, any full-cycle LCG when m is a power of 2 will produce alternately odd and even results.
The recent "PCG" algorithm uses several conditioning techniques which make a simple LCG competitive with more expensive and non-linear generators. The resulting generator retains the advantages of LCG's such as simplicity and very small state.
Comparison with other PRNGs.
If higher-quality random numbers are needed, and sufficient memory is available (~ 2 kilobytes), then the Mersenne twister algorithm provides a vastly longer period (219937 − 1) and variate uniformity. A common Mersenne twister implementation, interestingly enough, uses an LCG to generate seed data.
Linear congruential generators have the problem that all of the bits in each number are usually not equally random. A Linear Feedback Shift Register PRNG produces a stream of pseudo-random bits, each of which are truly pseudo-random, and can be implemented with essentially the same amount of memory as a linear congruential generator, albeit with a bit more computation.
The linear feedback shift register has a strong relationship to linear congruential generators.
Given a few values in the sequence, some techniques can predict the following values in the sequence for not only linear congruent generators but any other polynomial congruent generator.

</doc>
<doc id="45528" url="http://en.wikipedia.org/wiki?curid=45528" title="Opportunity cost">
Opportunity cost

In microeconomic theory, the opportunity cost of a choice is the value of the best alternative forgone, in a situation in which a choice needs to be made between several mutually exclusive alternatives given limited resources. Assuming the best choice is made, it is the "cost" incurred by not enjoying the "benefit" that would be had by taking the second best choice available. The "New Oxford American Dictionary" defines it as "the loss of potential gain from other alternatives when one alternative is chosen". Opportunity cost is a key concept in economics, and has been described as expressing "the basic relationship between scarcity and choice". The notion of opportunity cost plays a crucial part in ensuring that scarce resources are used efficiently. Thus, opportunity costs are not restricted to monetary or financial costs: the real cost of output forgone, lost time, pleasure or any other benefit that provides utility should also be considered opportunity costs.
History.
The term was coined in 1914 by Austrian economist Friedrich von Wieser in his book "Theorie der gesellschaftlichen Wirtschaft". It was first described in 1848 by French classical economist Frédéric Bastiat in his essay "".
Opportunity costs in production.
Explicit costs.
Explicit costs are opportunity costs that involve direct monetary payment by producers. The explicit opportunity cost of the factors of production not already owned by a producer is the price that the producer has to pay for them. For instance, if a firm spends $100 on electrical power consumed, its explicit opportunity cost is $100. This cash expenditure represents a lost opportunity to purchase something else with the $100.
Implicit costs.
Implicit costs (also called implied, imputed or notional costs) are the opportunity costs not reflected in cash outflow but implied by the failure of the firm to allocate its existing (owned) resources, or factors of production to the best alternative use. For example: a manufacturer has previously purchased 1000 tons of steel and the machinery to produce a widget. The implicit part of the opportunity cost of producing the widget is the revenue lost by not selling the steel and not renting out the machinery instead of using them for production.
Evaluation.
Note that opportunity cost is not the "sum" of the available alternatives when those alternatives are, in turn, mutually exclusive to each other – it is the "next best" alternative given up selecting the best option. The opportunity cost of a city's decision to build the hospital on its vacant land is the loss of the land for a sporting center, or the inability to use the land for a parking lot, or the money which could have been made from selling the land. Use for any one of those purposes would preclude the possibility to implement any of the other.
 Example<br>
Q: Suppose you have a free ticket to a concert by "Band A". The ticket has no resale value. On the night of the concert your next-best alternative entertainment is a performance by "Band B" for which the tickets cost $40. You like "Band B" and would usually be willing to pay $50 for a ticket to see them. What is the "opportunity cost" of using your free ticket and seeing "Band A"?
A: The benefit you forgo (that is, the value to you) is the benefit of seeing "Band B". As well as the gross benefit of $50 for seeing "Band B", you also forgo the actual $40 of cost, so the net benefit you forgo is $10. So, the "opportunity cost" of seeing "Band A" is $10.
The main point is that opportunity cost is the one forgone and gets the best available alternative.

</doc>
<doc id="45534" url="http://en.wikipedia.org/wiki?curid=45534" title="Aldo Rossi">
Aldo Rossi

Aldo Rossi (3 May 1931 – 4 September 1997) was an Italian architect and designer who accomplished the unusual feat of achieving international recognition in four distinct areas: theory, drawing, architecture and product design.
He was the first Italian to receive the Pritzker Prize for architecture.
Early life.
He was born in Milan, Italy. After early education by the Somascan Religious Order and then at in Lecco, in 1949 he went to the school of architecture at the Polytechnic University of Milan. His thesis advisor was and he graduated in 1959.
In 1955 he had started writing for, and from 1959 was one of the editors of, the architectural magazine Casabella-Continuità, with editor in chief Ernesto Nathan Rogers. Rossi left in 1964, when the chief editorship went to Gian Antonio Bernasconi. Rossi went on to work for and Il_contemporaneo, making Rossi one of the most active participants in the fervent cultural debate of the time.
His early articles cover architects such as Alessandro Antonelli, , Auguste Perret and Emil Kaufmann and much of this material became part of his second book, "Scritti scelti sull'architettura e la città 1956-1972" ("Selected writings on architecture and the city from 1956 to 1972"). He married the Swiss actress Sonia Gessner, who introduced him to the world of film and theater. Culture and his family became central to his life. His son Fausto was active in movie-making both in front of and behind the camera and his daughter Vera was involved with theatre.
Career.
He began his professional career at the studio of Ignazio Gardella in 1956, moving on to the studio of Marco Zanuso. In 1963 also he began teaching, firstly as an assistant to (1963) at the school of urban planning in Arezzo, then to Carlo Aymonino at the Institute of Architecture in Venice. In 1965 he was appointed lecturer at the Polytechnic University of Milan and the following year he published "The architecture of the city" which soon became a classic of architectural literature.
His professional career, initially dedicated to architectural theory and small building work took a huge leap forward when Aymonino allowed Rossi to design part of the Monte Amiata complex in the Gallaratese quarter of Milan. In 1971 he won the design competition for the extension of the San Cataldo Cemetery in Modena, which made him internationally famous.
After suspension from teaching in Italy in those politically troubled times, he moved to ETH Zurich, occupying the chair in architectural design from 1971 to 1975.
In 1973 he was director of the International Architecture Section at the , where he presented, among others, his student Arduino Cantafora. Rossi's design ideas for the exhibition are explained in the International Architecture Catalogue and in a 16mm documentary "Ornament and crime" directed by Luigi Durissi and produced along with Gianni Braghieri and Franco Raggi. In 1975, Rossi returned to the teaching profession in Italy, teaching architectural composition in Venice.
In 1979 he was made a member of the prestigious Academy of Saint Luke. Meanwhile, there was international interest in his skills. He taught at several universities in the United States, including Cooper Union in New York City and Cornell University in Ithaca (New York State). At Cornell he participated in the "Institute for Architecture and Urban Studies" joint venture with New York's Museum of Modern Art, travelling to China and Hong Kong and attending conferences in South America.
In 1981 he published his autobiography, "A scientific autobiography". In this work the author, "in discrete disorder", brings back memories, objects, places, forms, literature notes, quotes, and insights and tries to "... go over things or impressions, describe, or look for ways to describe." In the same year he won first prize at the international competition for the design of an apartment block on the corner of Kochstraße and Wilhelmstraße in central Berlin.
In 1984 together with Ignazio Gardella and Fabio Reinhart, he won the competition for the renovation of the Teatro Carlo Felice in Genoa, which was not fully completed until 1991. In 1985 and 1986 Rossi was director of the 3rd (respectively 4th) International Architecture Exhibition at the Venice Biennale including further away display spaces such as Villa Farsetti in Santa Maria di Sala.
In 1987 he won two international competitions: one for a site at the Parc de la Villette in Paris, the other for the Deutsches Historisches Museum in Berlin, which was never brought to fruition. In 1989 he continued product design work for Unifor (now part of ) and Alessi. His espresso maker "La Cupola", designed for Alessi came out in 1988.
In 1990 he was awarded the Pritzker Prize. The city of Fukuoka in Japan honoured him for his work on the hotel complex "The Palace" and he won the 1991 Thomas Jefferson Medal in Public Architecture from the American Institute of Architects. These prestigious awards were followed by exhibitions at the Centre Georges Pompidou in Paris, the Beurs van Berlage in Amsterdam , the Berlinische Galerie in Berlin and the Museum of Contemporary Art in Ghent, Belgium.
In 1996 he became an honorary member of the American Academy of Arts and Letters and the following year he received their special cultural award in architecture and design. He died in Milan on 4 September 1997, following a car accident. Postumously he received the "Torre Guinigi" prize for his contribution to urban studies and the "Seaside Prize" of the , Florida, where he had built a detached family home in 1995.
On appeal his proposals won the 1999 competition for the restoration of the Teatro La Fenice, Venice and it reopened in 2004. In 1999 the Faculty of Architecture of the University of Bologna, based in Cesena, was named after him.
Work.
His earliest works of the 1960s were mostly theoretical and displayed a simultaneous influence of 1920s Italian modernism ("see Giuseppe Terragni"), classicist influences of Viennese architect Adolf Loos, and the reflections of the painter Giorgio de Chirico. A trip to the Soviet Union to study Stalinist architecture also left a marked impression.
In his writings Rossi criticized the lack of understanding of the city in current architectural practice. He argued that a city must be studied and valued as something constructed over time; of particular interest are urban artifacts that withstand the passage of time. Rossi held that the city remembers its past (our "collective memory"), and that we use that memory through monuments; that is, monuments give structure to the city.
He became extremely influential in the late 1970s and 1980s as his body of built work expanded and for his theories promoted in his books "The Architecture of the City" ("L'architettura della città", 1966) and "A Scientific Autobiography" ("Autobiografia scientifica", 1981).The largest of Rossi's projects in terms of scale was the San Cataldo Cemetery, in Modena, Italy, which began in 1971 but is yet to be completed. Rossi referred to it as a "city of the dead".
The distinctive independence of his buildings is reflected in the micro-architectures of the products designed by Rossi. In the 1980s Rossi designed stainless steel cafetières and other products for Alessi, Pirelli, and others.
Exhibits.
For the Venice Biennale in 1979 Rossi designed a floating "Teatro del Mondo" that seated 250 people. For the Venice Biennale in 1984, he designed a triumphal arch at the entrance to the exhibition site. In 2006 two pylons based on an original 1989 design by Aldo Rossi were erected in front of the Bonnefanten Museum in Maastricht by the Delft architectural firm Ufo Architecten.
Awards.
Aldo Rossi won the prestigious Pritzker Prize for architecture in 1990. Ada Louise Huxtable, architectural critic and Pritzker juror, has described Rossi as "a poet who happens to be an architect."
Product design.
In addition to architecture, Rossi, created product designs, including:

</doc>
<doc id="45535" url="http://en.wikipedia.org/wiki?curid=45535" title="Alessi (Italian company)">
Alessi (Italian company)

Alessi is a housewares and kitchen utensil company from Italy. They make everyday items from plastic and metal, designed by famous designers.
From the 1980s onward, Alessi has been particularly associated with the notion of "designer" objects - otherwise ordinary tools and objects executed as high design, particularly in a post-modern mode, from designers such as Philippe Starck. Many of the early memorable "designer kettles", "designer toothbrushes", "designer kitchenware" and so on were Alessi products, though competition in this product category has greatly increased since then.
History.
1920s to 1940s.
Alessi was founded in 1921 by Giovanni Alessi. The firm began as a workshop in Valle Strona near Lake Orta in the Italian Alps near Switzerland. An area known for its tradition in making small objects of wood or metal for in the house and in the kitchen in general. Alessi started with producing a wide range of tableware items in nickel, chromium and silver-plated brass. The company’s intention was to produce hand-crafted items with the aid of machines. Design in the current sense of the term began when Carlo Alessi (born 1916), son of Giovanni, was named chief designer. Carlo was trained as an industrial designer. Between 1935 and 1945 he developed virtually all of the products Alessi produced. In 1945 he ascended to chief executive and designed the coffee service. 
1950s and 1960s.
In the 1950s the company was under the leadership of Carlo Alessi. It was his brother Ettore Alessi who introduced the collaboration with external designers in 1955. With some architects, he designed a number of items which were created for the hotel needs. Through his intervention caused many individual objects, which were best-sellers, such as the historical series of "wire baskets". One of the key designs of this period is the "shaker" from 1957 by Luigi Massoni and Carlo Mazzeri. This was designed in a series with an "Ice bucket" and "Ice tongs" as part of the Program 4 for the 11 triennale in Milan. This was the first time that the Alessi products got shown with manufactured goods. 1950s would also of been a hard time for them to sell there designer objects as it has only been 5 years past the world war so a lot of people don't have much expendable income to spend on designer objects.
1970s and 1980s.
In 1970 Alberto Alessi was responsible for the third transformation of the company. Alessi was considered one of the "Italian Design Factories". In this decade under the leadership of Alberto Alessi the company collaborated with some design maestros like Achille Castiglioni, Richard Sapper, Alessandro Mendini and Ettore Sottsass, who are now all icons of the 1970s. In the '70s Alessi produced the "Condiment set" (salt, pepper and toothpicks) by Ettore Sottsass, the "Espressomaker" by Sapper.
The 1980s marked a period in which Italian Design Factories had to compete with mass production. These movements had a different view on design, for the Italian Design Factories the design and therefore the designer was the most important part of the process while for the mass production the design had to be functional and easy to be reproduced. Also in the 1980s they changed their ideology from factory to industrial research lab, meaning that it is a place for research and production. For Alessi the '80s are marked with some iconic designs like the "two tone kettle" by Sapper, their first cutlery set "Dry" by Castiglioni. Alessi collaborated with new designers like Aldo Rossi, Michael Graves and Philippe Starck who have been responsible for the some of Alessi's all time bestseller like the "kettle" with a bird whistle by Graves.
1990s to the present.
In recent decades as the "designer houseware" market greatly expanded, Alessi faced increasing competition from other international manufacturers, especially in lower-cost products mass-produced for retailers such as Target Corporation and J. C. Penney.
In the 1990s Alessi started to work more with plastic, at the request of designers who found it an easier material to work with than metal, offering more design freedom and innovative possibilities. The 1990s were marked by the theme "Family Follows Fiction" with playful and imaginative objects. Artists designing for this theme included Stefano Giovannoni and Alessandro Mendini, who designed "Fruit Mama" and the bestseller "Anna G." Metal still remained a popular material, for example the "Girotondo" family by King Kong. 
During the 2000s Alessi collaborated with several architects for the "coffee and tea towers", with a new generation of architects such as Wiel Arets, Zaha Hadid, Toyo Ito, Tom Kovac, Greg Lynn, MVRDV, Jean Nouvel, and UN Studio. These sets had a limited production of 99 copies. 
Another remarkable design in the 2000s is the "Blow Up" series by Fratelli Campana. The brothers played with form and shape to create baskets and other objects that look like they would fall apart when touched.
In 2006, the company reclassified its products under three lines: "A di Alessi", "Alessi" and "Officina Alessi". "A di Alessi" is more ‘democratic’ and more ‘pop’. This product line is the lower price range of Alessi. "Alessi" is the historic brand company and continues to develop the best of industrial mass production industry from the view of quality and design in a medium/high price range. The "Officina Alessi" is more exclusive, innovative and experimental, this is marked by small batch production series and limited series.
Alessi products are on display in museums worldwide like Museum of Modern Art New York, Metropolitan Museum of Art, Victoria and Albert Museum, Pompidou Centre and Stedelijk Museum Italy.
Designers and their designs.
From 1945 until today Alessi has collaborated with designers and even other brands or companies for their products. Some key designs and their designers:
Another notable collection is the collaboration with the National Palace Museum of Taiwan, a collection of various kitchenware products with Asian themes.

</doc>
<doc id="45537" url="http://en.wikipedia.org/wiki?curid=45537" title="Ustad Isa">
Ustad Isa

Ustad Isa Shirazi (Persian: استاد عيسى شیرازی‎ translation "Master Isa") was a Persian architect, often described as the chief architect of the Taj Mahal in Agra, India.
The lack of complete and reliable information as to whom the credit for the design belongs, led to innumerable speculations. Scholars suggest the story of Ustad Isa was born of the eagerness of the British in the 19th century to believe that such a beautiful building should be credited to a European architect. Local informants were reported to have sated British curiosity regarding the origins of the Taj by also supplying them with fictitious lists of workmen and materials from all over Asia. Typically, he is described as a Persian architect.
Recent research suggests the Persian architect, Ustad Ahmad Lahauri was the most likely candidate as the chief architect of the Taj, an assertion based on a claim made in writings by Lahauri's son Lutfullah Muhandis.

</doc>
<doc id="45538" url="http://en.wikipedia.org/wiki?curid=45538" title="Mersenne Twister">
Mersenne Twister

The Mersenne Twister is a pseudorandom number generator (PRNG). It is by far the most widely used PRNG. Its name derives from the fact that its period length is chosen to be a Mersenne prime.
The Mersenne Twister was developed in 1997 by Makoto Matsumoto (松本 眞) and Takuji Nishimura (西村 拓士). It was designed specifically to rectify most of the flaws found in older PRNGs. It was the first PRNG to provide fast generation of high-quality pseudorandom integers.
The most commonly-used version of the Mersenne Twister algorithm is based on the Mersenne prime 219937−1. The standard implementation of that, MT19937, uses a 32-bit word length. There is another implementation that uses a 64-bit word length, MT19937-64; it generates a different sequence.
Adoption in software systems.
The Mersenne Twister is the default PRNG for 
R,
Python,
Ruby,
IDL,
Free Pascal, 
PHP, 
Maple, 
MATLAB, 
GAUSS,
Julia,
CMU Common Lisp,
Steel Bank Common Lisp,
Microsoft Visual C++,
the GNU Multiple Precision Arithmetic Library,
and the GNU Scientific Library.
It is also available in C++ since C++11. Add-on implementations are provided by the Boost C++ Libraries,
GLib,
and the NAG Numerical Library.
The Mersenne Twister is one of two PRNGs in SPSS: the other generator is kept only for compatibility with older programs, and the Mersenne Twister is stated to be "more reliable".
The Mersenne Twister is similarly one of the PRNGs in SAS: the other generators are older and deprecated.
Advantages.
The commonly-used version of Mersenne Twister, MT19937, which produces a sequence of 32-bit integers, has the following desirable properties:
Disadvantages.
The state space is very large and may needlessly stress the CPU cache (a period above 2512 is enough for any application). In 2011, Saito & Matsumoto proposed a version of the Mersenne Twister to address this issue. The tiny version, TinyMT, uses just 127 bits of state space.
By today's standards, the Mersenne Twister is fairly slow, unless the SFMT implementation is used (see section below).
It passes most, but not all, of the stringent TestU01 randomness tests.
It can take a long time to start generating output that passes randomness tests, if the initial state is highly non-random—particularly if the initial state has many zeros. A consequence of this is that two instances of the generator, started with initial states that are almost the same, will usually output nearly the same sequence for many iterations, before eventually diverging. The 2002 update to the MT algorithm has improved initialization, so that reaching such a state is very unlikely.
"k"-distribution.
A pseudorandom sequence "xi" of "w"-bit integers of period "P" is said to be "k"-distributed to "v"-bit accuracy if the following holds.
Alternatives.
The algorithm in its native form is not cryptographically secure. The reason is that observing a sufficient number of iterations (624 in the case of MT19937, since this is the size of the state vector from which future iterations are produced) allows one to predict all future iterations.
A pair of cryptographic stream ciphers based on output from the Mersenne Twister has been proposed by Matsumoto, Nishimura, and co-authors. The authors claim speeds 1.5 to 2 times faster than Advanced Encryption Standard in counter mode.
An alternative generator, WELL ("Well Equidistributed Long-period Linear"), offers quicker recovery, and equal randomness, and nearly-equal speed. Marsaglia's xorshift generators and variants are the fastest in this class.
Algorithmic detail.
For a "k"-bit word length, the Mersenne Twister generates integers in the range [0, 2"k"−2].
The Mersenne Twister algorithm is based on a matrix linear recurrence over a finite binary field "F"2. The algorithm is a twisted generalised feedback shift register (twisted GFSR, or TGFSR) of rational normal form (TGFSR(R)), with state bit reflection and tempering. It is characterized by the following quantities:
with the restriction that 2"nw" − "r" − 1 is a Mersenne prime. This choice simplifies the primitivity test and "k"-distribution test that are needed in the parameter search.
For a word x with "w" bit width, it is expressed as the recurrence relation
with | as the bitwise or and formula_3 as the bitwise exclusive or (XOR), x"u", x"l" being x with upper and lower bitmasks applied. The twist transformation "A" is defined in rational normal form
formula_4
with "I""n" − 1 as the ("n" − 1) × ("n" − 1) identity matrix (and in contrast to normal matrix multiplication, bitwise XOR replaces addition). The rational normal form has the benefit that it can be efficiently expressed as
formula_5
where
In order to achieve the 2"nw" − "r" − 1 theoretical upper limit of the period in a TGFSR, "φ""B"("t") must be a primitive polynomial, "φ""B"("t") being the characteristic polynomial of
formula_7
formula_8
The twist transformation improves the classical GFSR with the following key properties:
As like TGFSR(R), the Mersenne Twister is cascaded with a tempering transform to compensate for the reduced dimensionality of equidistribution (because of the choice of "A" being in the rational normal form), which is equivalent to the transformation "A" = "R" → "A" = "T"−1"RT", "T" invertible. The tempering is defined in the case of Mersenne Twister as
with «, » as the bitwise left and right shifts, and & as the bitwise and. The first and last transforms are added in order to improve lower bit equidistribution. From the property of TGFSR, formula_9 is required to reach the upper bound of equidistribution for the upper bits.
The coefficients for MT19937 are:
Pseudocode.
The following piece of pseudocode generates uniformly distributed 32-bit integers in the range [0, 232 − 1] with the MT19937 algorithm:
 "// Create a length 624 array to store the state of the generator"
 int[0..623] MT
 int index = 0
 "// Initialize the generator from a seed"
 function initialize_generator(int seed) {
 index := 0
 MT[0] := seed
 for i from 1 to 623 { "// loop over each element"
 MT[i] := lowest 32 bits of(1812433253 * (MT[i-1] xor (right shift by 30 bits(MT[i-1]))) + i) "// 0x6c078965"
 "// Extract a tempered pseudorandom number based on the index-th value,"
 "// calling generate_numbers() every 624 numbers"
 function extract_number() {
 if index == 0 {
 generate_numbers()
 
 int y := MT[index]
 y := y xor (right shift by 11 bits(y))
 y := y xor (left shift by 7 bits(y) and (2636928640)) "// 0x9d2c5680"
 y := y xor (left shift by 15 bits(y) and (4022730752)) "// 0xefc60000"
 y := y xor (right shift by 18 bits(y))
 index := (index + 1) mod 624
 return y
 
 "// Generate an array of 624 untempered numbers"
 function generate_numbers() {
 for i from 0 to 623 {
 int y := (MT[i] and 0x80000000) "// bit 31 (32nd bit) of MT[i]"
 + (MT[(i+1) mod 624] and 0x7fffffff) "// bits 0-30 (first 31 bits) of MT[...]"
 MT[i] := MT[(i + 397) mod 624] xor (right shift by 1 bit(y))
 if (y mod 2) != 0 { "// y is odd"
 MT[i] := MT[i] xor (2567483615) "// 0x9908b0df"
SFMT.
SFMT, the Single instruction, multiple data-oriented Fast Mersenne Twister, is a variant of Mersenne Twister, introduced in 2006, designed to be fast when it runs on 128-bit SIMD.
Intel SSE2 and PowerPC AltiVec are supported by SFMT. It is also used for games with the Cell BE in the PlayStation 3.
MTGP.
MTGP is a variant of Mersenne Twister optimised for graphics processing units published by Mutsuo Saito and Makoto Matsumoto. The basic linear recurrence operations are extended from MT and parameters are chosen to allow many threads to compute the recursion in parallel, while sharing their state space to reduce memory load. The paper claims improved equidistribution over MT and performance on a high specification GPU (Nvidia GTX260 with 192 cores) of 4.7ms for 5x107 random 32-bit integers.

</doc>
<doc id="45541" url="http://en.wikipedia.org/wiki?curid=45541" title="Social Darwinism">
Social Darwinism

Social Darwinism is a modern name given to various theories of society that emerged in the United Kingdom, North America, and Western Europe in the 1870s, and which are claimed to have applied biological concepts of natural selection and survival of the fittest to sociology and politics. Social Darwinists generally argue that the strong should see their wealth and power increase while the weak should see their wealth and power decrease. Different social Darwinists have different views about which groups of people are "the strong" and "the weak", and they also hold different opinions about the precise mechanism that should be used to promote strength and punish weakness. Many such views stress competition between individuals in "laissez-faire" capitalism, while others motivated ideas of eugenics, racism, imperialism, fascism, Nazism, and struggle between national or racial groups.
The term "social Darwinism" gained widespread currency when used after 1944 by opponents of these earlier concepts. The majority of those who have been categorised as social Darwinists, did not identify themselves by such a label.
Creationists have often maintained that social Darwinism—leading to policies designed to make the weak perish—is a logical consequence of "Darwinism" (the theory of natural selection in biology). Biologists and historians have stated that this is a fallacy of appeal to nature, since the theory of natural selection is merely intended as a description of a biological phenomenon and should not be taken to imply that this phenomenon is "good" or that it ought to be used as a moral guide in human society. Social Darwinism owed more to Herbert Spencer's ideas, together with genetics and a Protestant Nonconformist tradition with roots in Hobbes and Malthus, than to Charles Darwin's research. While most scholars recognize some historical links between the popularisation of Darwin's theory and forms of social Darwinism, they also maintain that social Darwinism is not a necessary consequence of the principles of biological evolution.
Scholars debate the extent to which the various social Darwinist ideologies reflect Charles Darwin's own views on human social and economic issues. His writings have passages that can be interpreted as opposing aggressive individualism, while other passages appear to promote it. Some scholars argue that Darwin's view gradually changed and came to incorporate views from the leading social interpreters of his theory such as Spencer, but Spencer's Lamarckian evolutionary ideas about society were published before Darwin first published his theory, and both promoted their own conceptions of moral values. Spencer supported "laissez-faire" capitalism on the basis of his Lamarckian belief that struggle for survival spurred self-improvement which could be inherited.
Origin of the term.
The term first appeared in Europe in 1877, and around this time it was used by sociologists opposed to the concept. The term was popularized in the United States in 1944 by the American historian Richard Hofstadter who used it in the ideological war effort against fascism to denote a reactionary creed which promoted competitive strife, racism and chauvinism. Hofstadter later also recognized (what he saw as) the influence of Darwinist and other evolutionary ideas upon those with collectivist views, enough to devise a term for the phenomenon, "Darwinist collectivism." Before Hofstadter's work the use of the term "social Darwinism" in English academic journals was quite rare. In fact,
...there is considerable evidence that the entire concept of "social Darwinism" as we know it today was virtually invented by Richard Hofstadter. Eric Foner, in an introduction to a then-new edition of Hofstadter's book published in the early 1990s, declines to go quite that far. "Hofstadter did not invent the term Social Darwinism," Foner writes, "which originated in Europe in the 1860s and crossed the Atlantic in the early twentieth century. But before he wrote, it was used only on rare occasions; he made it a standard shorthand for a complex of late-nineteenth-century ideas, a familiar part of the lexicon of social thought."—Jeff Riggenbach
The term "social Darwinism" has rarely been used by advocates of the supposed ideologies or ideas; instead it has almost always been used pejoratively by its opponents. The term draws upon the common use of the term "Darwinism", which has been used to describe a range of evolutionary views, but in the late 19th century was applied more specifically to natural selection as first advanced by Charles Darwin to explain speciation in populations of organisms. The process includes competition between individuals for limited resources, popularly but inaccurately described by the phrase "survival of the fittest," a term coined by sociologist Herbert Spencer.
While the term has been applied to the claim that Darwin's theory of evolution by natural selection can be used to understand the social endurance of a nation or country, social Darwinism commonly refers to ideas that predate Darwin's publication of "On the Origin of Species". Others whose ideas are given the label include the 18th century clergyman Thomas Malthus, and Darwin's cousin Francis Galton who founded eugenics towards the end of the 19th century.
Theories and origins.
The term Darwinism had been coined by Thomas Henry Huxley in his April 1860 review of "On the Origin of Species", and by the 1870s it was used to describe a range of concepts of evolutionism or development, without any specific commitment to Charles Darwin's own theory.
The first use of the phrase "social Darwinism" was in Joseph Fisher's 1877 article on "The History of Landholding in Ireland" which was published in the "Transactions of the Royal Historical Society". Fisher was commenting on how a system for borrowing livestock which had been called "tenure" had led to the false impression that the early Irish had already evolved or developed land tenure;
 These arrangements did not in any way affect that which we understand by the word " tenure," that is, a man's farm, but they related solely to cattle, which we consider a chattel. It has appeared necessary to devote some space to this subject, inasmuch as that usually acute writer Sir Henry Maine has accepted the word " tenure " in its modern interpretation, and has built up a theory under which the Irish chief " developed " into a feudal baron. I can find nothing in the Brehon laws to warrant this theory of social Darwinism, and believe further study will show that the Cain Saerrath and the Cain Aigillue relate solely to what we now call chattels, and did not in any way affect what we now call the freehold, the possession of the land.
 — Fisher 1877.
Despite the fact that social Darwinism bears Charles Darwin's name, it is also linked today with others, notably Herbert Spencer, Thomas Malthus, and Francis Galton, the founder of eugenics. In fact, Spencer was not described as a social Darwinist until the 1930s, long after his death.
Darwin himself gave serious consideration to Galton's work, but considered the ideas of "hereditary improvement" impractical. Aware of weaknesses in his own family, Darwin was sure that families would naturally refuse such selection and wreck the scheme. He thought that even if compulsory registration was the only way to improve the human race, this illiberal idea would be unacceptable, and it would be better to publicize the "principle of inheritance" and let people decide for themselves.
In "The Descent of Man, and Selection in Relation to Sex" of 1882 Darwin described how medical advances meant that the weaker were able to survive and have families, and as he commented on the effects of this, he cautioned that hard reason should not override sympathy and considered how other factors might reduce the effect:
Thus the weak members of civilized societies propagate their kind. No one who has attended to the breeding of domestic animals will doubt that this must be highly injurious to the race of man. It is surprising how soon a want of care, or care wrongly directed, leads to the degeneration of a domestic race; but excepting in the case of man himself, hardly any one is so ignorant as to allow his worst animals to breed.The aid which we feel impelled to give to the helpless is mainly an incidental result of the instinct of sympathy, which was originally acquired as part of the social instincts, but subsequently rendered, in the manner previously indicated, more tender and more widely diffused. Nor could we check our sympathy, even at the urging of hard reason, without deterioration in the noblest part of our nature. The surgeon may harden himself whilst performing an operation, for he knows that he is acting for the good of his patient; but if we were intentionally to neglect the weak and helpless, it could only be for a contingent benefit, with an overwhelming present evil.
... We must therefore bear the undoubtedly bad effects of the weak surviving and propagating their kind; but there appears to be at least one check in steady action, namely that the weaker and inferior members of society do not marry so freely as the sound; and this check might be indefinitely increased by the weak in body or mind refraining from marriage, though this is more to be hoped for than expected.
Social Darwinists.
Herbert Spencer's ideas, like those of evolutionary progressivism, stemmed from his reading of Thomas Malthus, and his later theories were influenced by those of Darwin. However, Spencer's major work, "Progress: Its Law and Cause" (1857) was released two years before the publication of Darwin's "On the Origin of Species", and "First Principles" was printed in 1860.
In "The Social Organism" (1860), Spencer compares society to a living organism and argues that, just as biological organisms evolve through natural selection, society evolves and increases in complexity through analogous processes.
In many ways, Spencer's theory of cosmic evolution has much more in common with the works of Lamarck and Auguste Comte's positivism than with Darwin's.
Jeff Riggenbach argues that Spencer's view was that culture and education made a sort of Lamarckism possible and notes that Herbert Spencer was a proponent of private charity.
Spencer's work also served to renew interest in the work of Malthus. While Malthus's work does not itself qualify as social Darwinism, his 1798 work "An Essay on the Principle of Population", was incredibly popular and widely read by social Darwinists. In that book, for example, the author argued that as an increasing population would normally outgrow its food supply, this would result in the starvation of the weakest and a Malthusian catastrophe.
According to Michael Ruse, Darwin read Malthus' famous "Essay on a Principle of Population" in 1838, four years after Malthus' death. Malthus himself anticipated the social Darwinists in suggesting that charity could exacerbate social problems.
Another of these social interpretations of Darwin's biological views, later known as eugenics, was put forth by Darwin's cousin, Francis Galton, in 1865 and 1869. Galton argued that just as physical traits were clearly inherited among generations of people, the same could be said for mental qualities (genius and talent). Galton argued that social morals needed to change so that heredity was a conscious decision in order to avoid both the over-breeding by less fit members of society and the under-breeding of the more fit ones.
In Galton's view, social institutions such as welfare and insane asylums were allowing inferior humans to survive and reproduce at levels faster than the more "superior" humans in respectable society, and if corrections were not soon taken, society would be awash with "inferiors." Darwin read his cousin's work with interest, and devoted sections of "Descent of Man" to discussion of Galton's theories. Neither Galton nor Darwin, though, advocated any eugenic policies such as those that would be undertaken in the early 20th century, for government coercion of any form was very much against their political opinions.
Friedrich Nietzsche's philosophy addressed the question of artificial selection, yet Nietzsche's principles did not concur with Darwinian theories of natural selection. Nietzsche's point of view on sickness and health, in particular, opposed him to the concept of biological adaptation as forged by Spencer's "fitness". Nietzsche criticized Haeckel, Spencer, and Darwin, sometimes under the same banner by maintaining that in specific cases, sickness was necessary and even helpful. Thus, he wrote:
Wherever progress is to ensue, deviating natures are of greatest importance. Every progress of the whole must be preceded by a partial weakening. The strongest natures retain the type, the weaker ones help to advance it.
Something similar also happens in the individual. There is rarely a degeneration, a truncation, or even a vice or any physical or moral loss without an advantage somewhere else. In a warlike and restless clan, for example, the sicklier man may have occasion to be alone, and may therefore become quieter and wiser; the one-eyed man will have one eye the stronger; the blind man will see deeper inwardly, and certainly hear better. To this extent, the famous theory of the survival of the fittest does not seem to me to be the only viewpoint from which to explain the progress of strengthening of a man or of a race. 
The publication of Ernst Haeckel's best-selling "Welträtsel" ('Riddle of the Universe') in 1899 brought social Darwinism and earlier ideas of racial hygiene to a wider audience. His recapitulation theory was not Darwinism, but rather attempted to combine the ideas of Goethe, Lamarck and Darwin. It was adopted by emerging social sciences to support the concept that non-European societies were "primitive" in an early stage of development towards the European ideal, but since then it has been heavily refuted on many fronts Haeckel's works led to the formation of the Monist League in 1904 with many prominent citizens among its members, including the Nobel Prize winner Wilhelm Ostwald. By 1909, it had a membership of some six thousand people. 
The simpler aspects of social Darwinism followed the earlier Malthusian ideas that humans, especially males, require competition in their lives in order to survive in the future. Further, the poor should have to provide for themselves and not be given any aid. However, amidst this climate, most social Darwinists of the early twentieth century actually supported better working conditions and salaries. Such measures would grant the poor a better chance to provide for themselves yet still distinguish those who are capable of succeeding from those who are poor out of laziness, weakness, or inferiority.
Darwinism and hypotheses of social change.
"Social Darwinism" was first described by Oscar Schmidt of the University of Strasbourg, reporting at a scientific and medical conference held in Munich in 1877. He noted how socialists, although opponents of Darwin's theory, nonetheless used it to add force to their political arguments. Schmidt's essay first appeared in English in "Popular Science" in March 1879. There followed an anarchist tract published in Paris in 1880 entitled "Le darwinisme social" by Émile Gautier. However, the use of the term was very rare — at least in the English-speaking world (Hodgson, 2004)— until the American historian Richard Hofstadter published his influential "Social Darwinism in American Thought" (1944) during World War II.
Hypotheses of social evolution and cultural evolution were common in Europe. The Enlightenment thinkers who preceded Darwin, such as Hegel, often argued that societies progressed through stages of increasing development. Earlier thinkers also emphasized conflict as an inherent feature of social life. Thomas Hobbes's 17th century portrayal of the state of nature seems analogous to the competition for natural resources described by Darwin. Social Darwinism is distinct from other theories of social change because of the way it draws Darwin's distinctive ideas from the field of biology into social studies.
Darwin, unlike Hobbes, believed that this struggle for natural resources allowed individuals with certain physical and mental traits to succeed more frequently than others, and that these traits accumulated in the population over time, which under certain conditions could lead to the descendants being so different that they would be defined as a new species.
However, Darwin felt that "social instincts" such as "sympathy" and "moral sentiments" also evolved through natural selection, and that these resulted in the strengthening of societies in which they occurred, so much so that he wrote about it in "Descent of Man":
The following proposition seems to me in a high degree probable—namely, that any animal whatever, endowed with well-marked social instincts, the parental and filial affections being here included, would inevitably acquire a moral sense or conscience, as soon as its intellectual powers had become as well, or nearly as well developed, as in man. For, firstly, the social instincts lead an animal to take pleasure in the society of its fellows, to feel a certain amount of sympathy with them, and to perform various services for them.
United States.
Spencer proved to be a popular figure in the 1880s primarily because his application of evolution to areas of human endeavor promoted an optimistic view of the future as inevitably becoming better. In the United States, writers and thinkers of the gilded age such as Edward L. Youmans, William Graham Sumner, John Fiske, John W. Burgess, and others developed theories of social evolution as a result of their exposure to the works of Darwin and Spencer.
In 1883, Sumner published a highly influential pamphlet entitled "What Social Classes Owe to Each Other", in which he insisted that the social classes owe each other nothing, synthesizing Darwin's findings with free enterprise Capitalism for his justification. According to Sumner, those who feel an obligation to provide assistance to those unequipped or under-equipped to compete for resources, will lead to a country in which the weak and inferior are encouraged to breed more like them, eventually dragging the country down. Sumner also believed that the best equipped to win the struggle for existence was the American businessman, and concluded that taxes and regulations serve as dangers to his survival. This pamphlet makes no mention of Darwinism, and only refers to Darwin in a statement on the meaning of liberty, that "There never has been any man, from the primitive barbarian up to a Humboldt or a Darwin, who could do as he had a mind to."
Sumner never fully embraced Darwinian ideas, and some contemporary historians do not believe that Sumner ever actually believed in social Darwinism. The great majority of American businessmen rejected the anti-philanthropic implications of the theory. Instead they gave millions to build schools, colleges, hospitals, art institutes, parks and many other institutions. Andrew Carnegie, who admired Spencer, was the leading philanthropist in the world (1890–1920), and a major leader against imperialism and warfare.
H. G. Wells was heavily influenced by Darwinist thoughts, and novelist Jack London wrote stories of survival that incorporated his views on social Darwinism.
Japan.
Social Darwinism has influenced political, public health and social movements in Japan since the late 19th and early 20th century. Social Darwinism was originally brought to Japan through the works of Francis Galton and Ernst Haeckel as well as United States, British and French Lamarkian eugenic written studies of the late 19th and early 20th centuries. Eugenism as a science was hotly debated at the beginning of the 20th century, in "Jinsei-Der Mensch", the first eugenics journal in the empire. As Japan sought to close ranks with the west, this practice was adopted wholesale along with colonialism and its justifications.
China.
Social Darwinism was formally introduced to China through the translation by Yan Fu of Huxley's "Evolution and Ethics", in the course of an extensive series of translations of influential Western thought. Yan's translation strongly impacted Chinese scholars because he added national elements not found in the original. He understood Spencer's sociology as "not merely analytical and descriptive, but prescriptive as well," and saw Spencer building on Darwin, whom Yan summarized thus:
By the 1920s, social Darwinism found expression in the promotion of eugenics by the Chinese sociologist Pan Guangdan.
When Chiang Kai-shek started the New Life movement in 1934, he
Nazi Germany.
Nazi Germany's justification for its aggression was regularly promoted in Nazi propaganda films depicting scenes such as beetles fighting in a lab setting to demonstrate the principles of "survival of the fittest" as depicted in Alles Leben ist Kampf (English translation: All Life is Struggle). Hitler often refused to intervene in the promotion of officers and staff members, preferring instead to have them fight amongst themselves to force the "stronger" person to prevail—"strength" referring to those social forces void of virtue or principle. Key proponents were Alfred Rosenberg, who was hanged later at Nuremberg.
The argument that Nazi ideology was strongly influenced by social Darwinist ideas is often found in historical and social science literature. For example, the Jewish philosopher and historian Hannah Arendt analysed the historical development from a politically indifferent scientific Darwinism via social Darwinist ethics to racist ideology.
By 1985, the argument has been taken up by opponents of evolutionary theory.
Such claims have been presented by creationists such as Jonathan Sarfati.
 Intelligent design creationism supporters have promoted this position as well. For example, it is a theme in the work of Richard Weikart, who is a historian at California State University, Stanislaus, and a senior fellow for the Center for Science and Culture of the Discovery Institute.
It is also a main argument in the 2008 intelligent-design/creationist movie "". These claims are widely criticized within the academic community. The Anti-Defamation League has rejected such attempts to link Darwin's ideas with Nazi atrocities, and has stated that "Using the Holocaust in order to tarnish those who promote the theory of evolution is outrageous and trivializes the complex factors that led to the mass extermination of European Jewry."
Similar criticisms are sometimes applied (or misapplied) to other political or scientific theories that resemble social Darwinism, for example criticisms leveled at evolutionary psychology. For example, a critical reviewer of Weikart's book writes that "(h)is historicization of the moral framework of evolutionary theory poses key issues for those in sociobiology and evolutionary psychology, not to mention bioethicists, who have recycled many of the suppositions that Weikart has traced."
Another example is recent scholarship that portrays Ernst Haeckel's Monist League as a mystical progenitor of the Völkisch movement and, ultimately, of the Nazi Party of Adolf Hitler. Scholars opposed to this interpretation, however, have pointed out that the Monists were freethinkers who opposed all forms of mysticism, and that their organizations were immediately banned following the Nazi takeover in 1933 because of their association with a wide variety of causes including feminism, pacifism, human rights, and early gay rights movements.
Criticism and controversy.
Multiple incompatible definitions.
Social Darwinism has many definitions, and some of them are incompatible with each other. As such, social Darwinism has been criticized for being an inconsistent philosophy, which does not lead to any clear political conclusions. For example, "The Concise Oxford Dictionary of Politics" states:
Part of the difficulty in establishing sensible and consistent usage is that commitment to the biology of natural selection and to 'survival of the fittest' entailed nothing uniform either for sociological method or for political doctrine. A 'social Darwinist' could just as well be a defender of laissez-faire as a defender of state socialism, just as much an imperialist as a domestic eugenist.
Nazism, Eugenics, Fascism, Imperialism.
Social Darwinism was predominantly found in laissez-faire societies where the prevailing view was that of an individualist order to society. As such, social Darwinism supposed that human progress would generally favor the most individualistic races, which were those perceived as stronger. A different form of social Darwinism was part of the ideological foundations of Nazism and other fascist movements. This form did not envision survival of the fittest within an individualist order of society, but rather advocated a type of racial and national struggle where the state directed human breeding through eugenics. Names such as "Darwinian collectivism" or "Reform Darwinism" have been suggested to describe these views, in order to differentiate them from the individualist type of social Darwinism.
Some pre-twentieth century doctrines subsequently described as social Darwinism appear to anticipate state imposed eugenics and the race doctrines of Nazism. Critics have frequently linked evolution, Charles Darwin and social Darwinism with racialism, nationalism, imperialism and eugenics, contending that social Darwinism became one of the pillars of fascism and Nazi ideology, and that the consequences of the application of policies of "survival of the fittest" by Nazi Germany eventually created a very strong backlash against the theory.
As mentioned above, social Darwinism has often been linked to nationalism and imperialism.
During the age of New Imperialism, the concepts of evolution justified the exploitation of "lesser breeds without the law" by "superior races." To elitists, strong nations were composed of white people who were successful at expanding their empires, and as such, these strong nations would survive in the struggle for dominance. With this attitude, Europeans, except for Christian missionaries, seldom adopted the customs and languages of local people under their empires.
Peter Kropotkin – "Mutual Aid: A Factor of Evolution".
Peter Kropotkin argued in his 1902 book "" that Darwin did not define the fittest as the strongest, or most clever, but recognized that the fittest could be those who cooperated with each other. In many animal societies, "struggle is replaced by co-operation."
It may be that at the outset Darwin himself was not fully aware of the generality of the factor which he first invoked for explaining one series only of facts relative to the accumulation of individual variations in incipient species. But he foresaw that the term [evolution] which he was introducing into science would lose its philosophical and its only true meaning if it were to be used in its narrow sense only—that of a struggle between separate individuals for the sheer means of existence. And at the very beginning of his memorable work he insisted upon the term being taken in its "large and metaphorical sense including dependence of one being on another, and including (which is more important) not only the life of the individual, but success in leaving progeny." [Quoting "Origin of Species," chap. iii, p. 62 of first edition.]
While he himself was chiefly using the term in its narrow sense for his own special purpose, he warned his followers against committing the error (which he seems once to have committed himself) of overrating its narrow meaning. In "The Descent of Man" he gave some powerful pages to illustrate its proper, wide sense. He pointed out how, in numberless animal societies, the struggle between separate individuals for the means of existence disappears, how struggle is replaced by co-operation, and how that substitution results in the development of intellectual and moral faculties which secure to the species the best conditions for survival. He intimated that in such cases the fittest are not the physically strongest, nor the cunningest, but those who learn to combine so as mutually to support each other, strong and weak alike, for the welfare of the community. "Those communities," he wrote, "which included the greatest number of the most sympathetic members would flourish best, and rear the greatest number of offspring" (2nd edit., p. 163). The term, which originated from the narrow Malthusian conception of competition between each and all, thus lost its narrowness in the mind of one who knew Nature.
Noam Chomsky discussed briefly Kropotkin's views in a July 8, 2011 YouTube video from Renegade Economist, in which he said Kropotkin argued
...the exact opposite [of Social Darwinism]. He argued that on Darwinian grounds, you would expect cooperation and mutual aid to develop leading towards community, workers' control and so on. Well, you know, he didn't prove his point. It's at least as well argued as Herbert Spencer is...

</doc>
<doc id="45547" url="http://en.wikipedia.org/wiki?curid=45547" title="Refugee">
Refugee

A refugee is a person who is outside their home country because they have suffered (or feared) persecution on account of race, religion, nationality, or political opinion; because they are a member of a persecuted social category of persons; or because they are fleeing a war. Such a person may be called an "asylum seeker" until recognized by the state where they make a claim.
In 2014, Syria, Palestine and Afghanistan were the largest source territories of refugees. As of February 2015, Turkey is the biggest refugee hosting country which hosts 1.7 million Syrian refugees in the world. Pakistan is second, hosting 1.6 million Afghan refugees.
Definition.
The 1951 United Nations Convention Relating to the Status of Refugees has adopted the following definition of a refugee (in Article 1.A.2):[A]ny person who: owing to a well-founded fear of being persecuted for reasons of race, religion, nationality, membership of a particular social group, or political opinion, is outside the country of his nationality, and is unable to or, owing to such fear, is unwilling to avail himself of the protection of that country".
This original definition with all its legacies has been criticized as based on three political framings:
The concept of a refugee was expanded by the Convention's 1967 Protocol and by regional conventions in Africa and Latin America to include persons who had fled war or other violence in their home country. European Union's minimum standards definition of refugee, underlined by Art. 2 (c) of Directive No. 2004/83/EC, essentially reproduces the narrow definition of refugee offered by the UN 1951 Convention; nevertheless, by virtue of articles 2 (e) and 15 of the same Directive, persons who have fled a war-caused generalized violence are, at certain conditions, eligible for a complementary form of protection, called subsidiary protection. The same form of protection is foreseen for people who, without being refugees, are nevertheless exposed, if returned to their countries of origin, to death penalty, torture or other inhuman or degrading treatments.
The term refugee is often used to include displaced persons who may fall outside the legal definition in the Convention, either because they have left their home countries because of war and not because of a fear of persecution, or because they have been forced to migrate within their home countries. The Convention Governing the Specific Aspects of Refugee Problems in Africa, adopted by the Organization of African Unity in 1969, accepted the definition of the 1951 Refugee Convention and expanded it to include people who left their countries of origin not only because of persecution but also due to acts of external aggression, occupation, domination by foreign powers or serious disturbances of public order.
Refugees were defined as a legal group in response to the large numbers of people fleeing Eastern Europe following World War II. The lead international agency coordinating refugee protection is the Office of the United Nations High Commissioner for Refugees (UNHCR), which counted 8,400,000 refugees worldwide at the beginning of 2006. This was the lowest number since 1980. The major exception is the 4,600,000 Palestinian refugees under the authority of the United Nations Relief and Works Agency for Palestine Refugees in the Near East (UNRWA). In June 2011, the UNHCR estimated the number of refugees to 15.1 million. The majority of refugees who leave their country seek asylum in countries neighboring their country of nationality. The "durable solutions" to refugee populations, as defined by UNHCR and governments, are: voluntary repatriation to the country of origin; local integration into the country of asylum; and resettlement to a third country.
Although similar and frequently confused with refugees, internally displaced persons have a different legal definition and are essentially refugees who have not crossed any international border. At the end of 2012, the Office of the United Nations High Commissioner for Refugees (UNHCR), the United Nations' refugee agency, reported that there were 15.4 million refugees worldwide. By contrast there were 28.8 million (about twice as many) IDPs at the end of 2012
History.
The idea that a person who sought sanctuary in a holy place couldn't be harmed without inviting divine retribution was familiar to the ancient Greeks and ancient Egyptians. However, the right to seek asylum in a church or other holy place was first codified in law by King Æthelberht of Kent in about 600 AD. Similar laws were implemented throughout Europe in the Middle Ages. The related concept of political exile also has a long history: Ovid was sent to Tomis; Voltaire was sent to England. Through the 1648 Peace of Westphalia, nations recognized each other's sovereignty. However, it was not until the advent of romantic nationalism in late 18th-century Europe that nationalism gained sufficient prevalence for the phrase 'country of nationality' to become practically meaningful, and for people crossing borders to be required to provide identification.
The term 'refugee' is sometimes applied to people who may have fit the definition outlined by the 1951 Convention, were it to be applied retroactively. There are many candidates. For example, after the Edict of Fontainebleau in 1685 outlawed Protestantism in France, hundreds of thousands of Huguenots fled to England, the Netherlands, Switzerland, South Africa, Germany and Prussia. The repeated waves of pogroms that swept Eastern Europe in the 19th and early 20th century prompted mass Jewish emigration (more than 2 million Russian Jews emigrated in the period 1881–1920). Beginning in the 19th century, Muslim people emigrated to Turkey from Europe. The Balkan Wars of 1912–1913 caused 800,000 people to leave their homes. Various groups of people were officially designated refugees beginning in World War I.
League of Nations.
The first international co-ordination of refugee affairs came with the creation by the League of Nations in 1921 of High Commissioner for Refugees and the appointment of Fridtjof Nansen as its head. Nansen and the Commission were charged with assisting the approximately 1,500,000 people who fled the Russian Revolution of 1917 and the subsequent civil war (1917–1921), most of them aristocrats fleeing the Communist government. It is estimated that about 800,000 Russian refugees became stateless when Lenin revoked citizenship for all Russian expatriates in 1921.
In 1923, the mandate of the Commission was expanded to include the more than one million Armenians who left Turkish Asia Minor in 1915 and 1923 due to a series of events now known as the Armenian Genocide. Over the next several years, the mandate was expanded further to cover Assyrians and Turkish refugees. In all of these cases, a refugee was defined as a person in a group for which the League of Nations had approved a mandate, as opposed to a person to whom a general definition applied.
The 1923 population exchange between Greece and Turkey involved approximately two million people (around 1.5 million Anatolian Greeks and 500,000 Muslims in Greece) most forcibly repatriated and denaturalized from homelands of centuries or millennia (and guaranteed the nationality of the destination country) in a treaty promoted and overseen by the international community as part of the Treaty of Lausanne.
The U.S. Congress passed the Emergency Quota Act in 1921, followed by the Immigration Act of 1924. The Immigration Act of 1924 was aimed at further restricting the Southern and Eastern Europeans, especially Jews, Italians and Slavs, who had begun to enter the country in large numbers beginning in the 1890s. Most of the European refugees (principally Jews and Slavs) fleeing Stalin, the Nazis and World War II were barred from coming to the United States.
In 1930, the Nansen International Office for Refugees (Nansen Office) was established as a successor agency to the Commission. Its most notable achievement was the Nansen passport, a refugee travel document, for which it was awarded the 1938 Nobel Peace Prize. The Nansen Office was plagued by problems of financing, an increase in refugee numbers, and a lack of co-operation from some member states, which led to mixed success overall.
However, it managed to lead fourteen nations to ratify the 1933 Refugee Convention, an early, and relatively modest, attempt at a human rights charter, and in general assisted around one million refugees worldwide.
1933 (rise of Nazism) to 1944.
The rise of Nazism led to such a very large increase in the number of refugees from Germany that in 1933 the League created a High Commission for Refugees Coming from Germany. Besides other measures by the Nazis which created fear and flight, Jews were stripped of German citizenship by the "Reich Citizenship Law" of 1935. On July 4, 1936 an agreement was signed under League auspices that defined a refugee coming from Germany as "any person who was settled in that country, who does not possess any nationality other than German nationality, and in respect of whom it is established that in law or in fact he or she does not enjoy the protection of the Government of the Reich" (article 1). 
The mandate of the High Commission was subsequently expanded to include persons from Austria and Sudetenland, which Germany annexed after October 1, 1938 in accordance with the Munich Agreement. According to the Institute for Refugee Assistance, the actual count of refugees from Czechoslovakia on March 1, 1939 stood at almost 150,000. Between 1933 and 1939, about 200,000 Jews fleeing Nazism were able to find refuge in France, while at least 55,000 Jews were able to find refuge in Palestine before the British authorities closed that destination in 1939.
On 31 December 1938, both the Nansen Office and High Commission were dissolved and replaced by the Office of the High Commissioner for Refugees under the Protection of the League. This coincided with the flight of several hundred thousand Spanish Republicans to France after their loss to the Nationalists in 1939 in the Spanish Civil War.
The conflict and political instability during World War II led to massive numbers of refugees (see World War II evacuation and expulsion). In 1943, the Allies created the United Nations Relief and Rehabilitation Administration (UNRRA) to provide aid to areas liberated from Axis powers, including parts of Europe and China. By the end of the War, Europe had more than 40 million refugees. UNRRA was involved in returning over seven million refugees, then commonly referred to as displaced persons or DPs, to their country of origin and setting up displaced persons camps for one million refugees who refused to be repatriated. Even two years after the end of War, some 850,000 people still lived in DP camps across Western Europe. After the establishment of Israel in 1948, Israel accepted more than 650,000 refugees by 1950. By 1953, over 250,000 refugees were still in Europe, most of them old, infirm, crippled, or otherwise disabled.
Post-World War II population transfers.
After the Soviet armed forces recaptured eastern Poland from the Germans in 1944, the Soviets unilaterally declared a new frontier between the Soviet Union and Poland approximately at the Curzon Line, despite the protestations from the Polish government-in-exile in London and the western Allies at the Teheran Conference and the Yalta Conference of February 1945. After the German surrender on 7 May 1945, the Allies occupied the remainder of Germany, and the Berlin declaration of 5 June 1945 confirmed the division of Allied-occupied Germany according to the Yalta Conference, which stipulated the continued existence of the German Reich as a whole, which would include its eastern territories as of 31 December 1937. This did not impact on Poland's eastern border, and Stalin refused to be removed from these eastern Polish territories.
In the last months of World War II, about five million German civilians from the German provinces of East Prussia, Pomerania and Silesia fled the advance of the Red Army from the east and became refugees in Mecklenburg, Brandenburg and Saxony. Since the spring of 1945 the Poles had been forcefully expelling the remaining German population in these provinces. When the Allies met in Potsdam on 17 July 1945 at the Potsdam Conference, a chaotic refugee situation faced the occupying powers. The Potsdam Agreement, signed on 2 August 1945, defined the Polish western border as that of 1937, (Article VIII) placing one fourth of Germany's territory under the Provisional Polish administration. Article XII ordered that the remaining German populations in Poland, Czechoslovakia and Hungary be transferred West in an "orderly and humane" manner. (See Flight and expulsion of Germans (1944–50).)
Although not approved by Allies at Potsdam, hundreds of thousands of ethnic Germans living in Yugoslavia and Romania were deported to slave labour in the Soviet Union, to Allied-occupied Germany, and subsequently to the German Democratic Republic (East Germany), Austria and the Federal Republic of Germany (West Germany). This entailed the largest population transfer in history. In all 15 million Germans were affected, and more than two million perished during the expulsions of the German population. (See Flight and expulsion of Germans (1944–1950).) Between the end of War and the erection of the Berlin Wall in 1961, more than 563,700 refugees from East Germany traveled to West Germany for asylum from the Soviet occupation.
During the same period, millions of former Russian citizens were forcefully repatriated against their will into the USSR. On 11 February 1945, at the conclusion of the Yalta Conference, the United States and United Kingdom signed a Repatriation Agreement with the USSR. The interpretation of this Agreement resulted in the forcible repatriation of all Soviets regardless of their wishes. When the war ended in May 1945, British and United States civilian authorities ordered their military forces in Europe to deport to the Soviet Union millions of former residents of the USSR, including many persons who had left Russia and established different citizenship decades before. The forced repatriation operations took place from 1945 to 1947.
At the end of World War II, there were more than 5 million "displaced persons" from the Soviet Union in Western Europe. About 3 million had been forced laborers (Ostarbeiters) in Germany and occupied territories. The Soviet POWs and the Vlasov men were put under the jurisdiction of SMERSH (Death to Spies). Of the 5.7 million Soviet prisoners of war captured by the Germans, 3.5 million had died while in German captivity by the end of the war. The survivors on their return to the USSR were treated as traitors (see Order No. 270). Over 1.5 million surviving Red Army soldiers imprisoned by the Nazis were sent to the Gulag.
Poland and Soviet Ukraine conducted population exchanges following the imposition of a new Poland-Soviet border at the Curzon Line in 1944. About 2,100,000 Poles were expelled west of the new border (see Repatriation of Poles), while about 450,000 Ukrainians were expelled to the east of the new border. The population transfer to Soviet Ukraine occurred from September 1944 to May 1946 (see Repatriation of Ukrainians). A further 200,000 Ukrainians left southeast Poland more or less voluntarily between 1944 and 1945.
The International Refugee Organization (IRO) was founded on April 20, 1946, and took over the functions of the United Nations Relief and Rehabilitation Administration, which was shut down in 1947. While the handover was originally planned to take place at the beginning of 1947, it did not occur until July 1947. The International Refugee Organization was a temporary organization of the United Nations (UN), which itself had been founded in 1945, with a mandate to largely finish the UNRRA's work of repatriating or resettling European refugees. It was dissolved in 1952 after resettling about one million refugees. The definition of a refugee at this time was an individual with either a Nansen passport or a "Certificate of identity" issued by the International Refugee Organization.
The Constitution of the International Refugee Organization, adopted by the United Nations General Assembly on December 15, 1946, specified the agency's field of operations. Controversially, this defined "persons of German ethnic origin" who had been expelled, or were to be expelled from their countries of birth into the postwar Germany, as individuals who would "not be the concern of the Organization." This excluded from its purview a group that exceeded in number all the other European displaced persons put together. Also, because of disagreements between the Western allies and the Soviet Union, the IRO only worked in areas controlled by Western armies of occupation.
UNHCR.
Headquartered in Geneva, Switzerland, the Office of the United Nations High Commissioner for Refugees (UNHCR) (established December 14, 1950) protects and supports refugees at the request of a government or the United Nations and assists in their return or resettlement. All refugees in the world are under the UNHCR mandate except Palestinian refugees who fled the future Jewish state between 1947 and 1949, as a result of the 1948 Palestine war, and their descendants, who are assisted by the United Nations Relief and Works Agency (UNRWA). However, Palestinian Arabs who fled the West Bank and Gaza after 1949 (for example, during the 1967 Six Day war) are under the jurisdiction of the UNHCR.
UNHCR provides protection and assistance not only to refugees, but also to other categories of displaced or needy people. These include asylum seekers, refugees who have returned home but still need help in rebuilding their lives, local civilian communities directly affected by the movements of refugees, stateless people and so-called internally displaced people (IDPs). IDPs are civilians who have been forced to flee their homes, but who have not reached a neighboring country and therefore, unlike refugees, are not protected by international law and may find it hard to receive any form of assistance. As the nature of war has changed in the last few decades, with more and more internal conflicts replacing interstate wars, the number of IDPs has increased significantly to an estimated 5 million people worldwide. According to Bogumil Terminski the stabilization of refugee problem worldwide is the main cause of the development of the studies on internal displacement.
The agency is mandated to lead and co-ordinate international action to protect refugees and resolve refugee problems worldwide. Its primary purpose is to safeguard the rights and well-being of refugees. It strives to ensure that everyone can exercise the right to seek asylum and find safe refuge in another State, with the option to return home voluntarily, integrate locally or to resettle in a third country.
UNHCR's mandate has gradually been expanded to include protecting and providing humanitarian assistance to what it describes as other persons "of concern", including internally displaced persons (IDPs) who would fit the legal definition of a refugee under the 1951 Refugee Convention and 1967 Protocol, the 1969 Organization for African Unity Convention, or some other treaty if they left their country, but who presently remain in their country of origin. UNHCR thus has missions in Colombia, Democratic Republic of the Congo, Serbia and Montenegro and Côte d'Ivoire to assist and provide services to IDPs.
Asia – 8,603,600
Africa – 5,169,300
Europe – 3,666,700
Latin America and Caribbean – 2,513,000
North America – 716,800
Oceania – 82,500.
International attitude.
Law.
Under international law, refugees are individuals who:
Refugee law encompasses both customary law, peremptory norms, and international legal instruments. These include:
World Refugee Day.
World Refugee Day occurs on June 20. The day was created in 2000 by a special United Nations General Assembly Resolution. June 20 had previously been commemorated as African Refugee Day in a number of African countries.
In the United Kingdom World Refugee Day is celebrated as part of Refugee Week. Refugee Week is a nationwide festival designed to promote understanding and to celebrate the cultural contributions of refugees, and features many events such as music, dance and theatre.
In the Roman Catholic Church, the World Day of Migrants and Refugees is celebrated in January each year. It was instituted in 1914 by Pope Pius X.
Reasons for refugee crises.
Asylum seekers.
International refugee law defines a refugee as someone who seeks refuge in a foreign country because of war and violence, or out of fear of persecution. The United States recognizes persecution "on account of race, religion, nationality, political opinion, or membership in a particular social group" as grounds for seeking asylum. Until a request for refuge has been accepted, the person is referred to as an "asylum seeker". Only after the recognition of the asylum seeker's protection needs is he or she is officially referred to as a refugee and enjoys refugee status. This carries certain rights and obligations according to the legislation of the receiving country.
The practical determination of whether a person is a refugee or not is most often left to certain government agencies within the host country. This can lead to a situation where the country will neither recognize the refugee status of the asylum seekers nor see them as legitimate migrants and treat them as illegal aliens.
The percentage of asylum/refugee seekers who (it has been deemed) do not meet the international standards of special-needs refugee, and for whom resettlement is deemed proper, varies from country to country. Failed asylum applicants are most often deported, sometimes after imprisonment or detention, as in the United Kingdom. In the United Kingdom, more than one in four decisions to refuse an asylum seeker protection are overturned by immigration judges. Campaigners have suggested that this figure suggests the process of allocation refugee status is inefficient or flawed.
A claim for asylum may also be made onshore, usually after making an unauthorized arrival. Some governments are tolerant and accepting of onshore asylum claims; other governments arrest or detain those who attempt to seek asylum; sometimes while processing their claims.
Non-governmental organizations concerned with refugees and asylum seekers have pointed out difficulties for displaced persons to seek asylum in industrialized countries. As their immigration policy often focuses on the fight of irregular migration and the strengthening of border controls it deters displaced persons from entering territory in which they could lodge an asylum claim. The lack of opportunities to legally access the asylum procedures can force asylum seekers to undertake often expensive and hazardous attempts at illegal entry.
Concerns over arbitrariness in asylum adjudication in the United States have led some commentators to describe the process as refugee roulette; that is, a system in which the identity of the adjudicator, rather than the strength of the asylum seeker's claim, is the determining factor in winning an asylum claim.
Climate.
Although they do not fit the definition of refugees set out in the UN Convention, people displaced by the effects of climate change have often been termed "climate refugees" or "climate change refugees". The term 'environmental refugee' is also commonly used and an estimate 25 million people can currently be classified as such. The alarming predictions by the UN, charities and some environmentalists, that between 200 million and 1 billion people could flood across international borders to escape the impacts of climate change in the next 40 years are realistic. Case studies from Bolivia, Senegal and Tanzania, three countries extremely prone to climate change, show that people affected by environmental degradation rarely move across borders. Instead, they adapt to new circumstances by moving short distances for short periods, often to cities. Millions of people live in places that are vulnerable to the effects of climate change. They face extreme weather conditions such as droughts or floods. Their lives and livelihoods might be threatened in new ways and create new vulnerabilities. Migration is in many developing countries a coping strategy to mitigate poverty and is already happening independent of the effects of climate change and environmental degradation. It is a selective process and the poorest and most vulnerable people are often excluded as they will find it almost impossible to move due to a lack of necessary funds or social support.
Security threats.
Very rarely, refugees have been used and recruited as refugee warriors, and the humanitarian aid directed at refugee relief has very rarely been utilized to fund the acquisition of arms. Support from a refugee-receiving state has rarely been used to enable refugees to mobilize militarily, enabling conflict to spread across borders.
Economic migrants.
Not all migrants seeking shelter in another country fall under the definition of "refugee" according to article 1A of the Geneva Convention. In 1951, when the text of the Convention was discussed, the parties of the treaty had the idea that slavery was a thing from the past: therefore escaped and fleeing slaves are a group not mentioned in the definition, as well as a category that later emerged: the climate refugee ("environmental migrant") (see below).
In 2008-2009, the humanitarian nature of the mass movement of Zimbabweans to neighbouring Southern African blurred the distinction between what is a "refugee" and an "economic migrant". Such people fit neither category perfectly and have more general needs, rights and responsibilities, that fall outside the specific mandate of the UNHCR. They fall between the cracks, according to the report "Zimbabwean Migration into Southern Africa: New Trends and Responses", released in November 2009 by the Forced Migration Studies Programme (FMSP) at the University of the Witwatersrand, South Africa. According to the researchers, a lack of protection of migrants in the region was based on a "false distinction" between a forced and an economic migrant, instead of focusing on the real and urgent needs some of these migrants have. The report suggested that a better term would be "forced humanitarian migrants", who moved for the purpose of their and their dependents' basic survival.
To emphasize the importance of a common humanitarian position on the outflow of Zimbabweans into the region the Regional Office for Southern Africa of the UN Office for the Coordination of Humanitarian Affairs coined the term "migrants of humanitarian concern" in 2008.
Official responses to Zimbabwean migration in Botswana, Malawi, Zambia and Mozambique are still premised on the original definition from the 1951 Convention, and so were said to be failing to protect both Zimbabweans and their own citizens". Those crossing the border were neither refugees – most did not even apply for refugee status – and, given the extent of economic collapse at home, nor they could hardly be considered as "voluntary" economic migrants. So many of them were not legally protected, nor do they receive humanitarian support, as they fell outside the mandates of the support structures offered by government and non-government institutions. In Botswana, Zambia and Malawi, asylum is available to Zimbabweans; in Mozambique, the few applicants for asylum had been rejected due to the state's decision to consider Zimbabweans as 'economic' and not forced humanitarian migrants.
Except for South Africa, protection and access to services in most countries in the region is contingent on receiving the refugee status, and require asylum seekers to stay in isolated camps, unable to work or travel, and thus send money to relatives that stayed behind in Zimbabwe. South Africa was considering the introduction of a special permit for Zimbabweans, but the policy was still under review.
Boat people.
The term "boat people" came into common use in the 1970s with the mass exodus of Vietnamese refugees following the Vietnam War. It is a widely used form of migration for people migrating from Cuba, Haiti, Morocco, Vietnam or Albania. They often risk their lives on dangerously crude and overcrowded boats to escape oppression or poverty in their home nations. Events resulting from the Vietnam War led many people in Cambodia, Laos, and especially Vietnam to become refugees in the late 1970s and 1980s. In 2001, 353 asylum seekers sailing from Indonesia to Australia drowned when their vessel sank.
The main danger to a boat person is that the boat he or she is sailing in may actually be anything that floats and is large enough for passengers. Although such makeshift craft can result in tragedy, in 2003 a small group of 5 Cuban refugees attempted (unsuccessfully, but un-harmed) to reach Florida in a 1950s pickup truck made buoyant by oil barrels strapped to its sides.
Boat people are frequently a source of controversy in the nation they seek to immigrate to, such as the United States, New Zealand, Germany, France, Russia, Canada, Italy, Japan, South Korea, Spain and Australia. Boat people are often forcibly prevented from landing at their destination, such as under Australia's Pacific Solution (which operated from 2001 until 2008), or they are subjected to mandatory detention after their arrival.
Refugee absorption solutions.
Camps.
A refugee camp is a place built by governments or NGOs (such as the International Committee of the Red Cross) to receive refugees. People may stay in these camps, receiving emergency food and medical aid, until it is safe to return to their homes or until they are retrieved by other people outside the camps. In some cases, often after several years, other countries decide it will never be safe to return these people, and they are resettled in "third countries", away from the border they crossed. However, more often than not, refugees are not resettled. In the meantime, they are at risk for disease, child soldier recruitment, terrorist recruitment, and physical and sexual violence. There are estimated to be 700 refugee camp locations.
Resettlement.
Resettlement involves the assisted movement of refugees who are unable to return home to safe third countries. The UNHCR has traditionally seen resettlement as the least preferable of the "durable solutions" to refugee situations. However, in April 2000 the then UN High Commissioner for Refugees, Sadako Ogata, stated:
Resettlement involves a number of difficulties, most of them involving the often extreme cultural transition needed to adapt to life in the country of resettlement. For the many refugees going from rural undeveloped countries to life in urban centers, public transport, education, health care systems, job applications, and even grocery shopping can be difficult to navigate. Language barriers also frequently pose a problem. Even aside from material problems, resettled refugees can struggle with issues of identity and belonging, as societal integration can be very difficult in a completely different culture, and discrimination frequently further inhibits the process.
The UNHCR does recognize benefits to resettlement as well, however. On their website, they bring attention to the fact that refugees have much to bring to the countries in which they are resettled in terms of culture and labor, going as far as to say that "both refugee resettlement and general migration are now recognized as critical factors in the economic success of a number of industrialized countries." According to the UNHCR, resettlement serves three primary functions: securing fundamental human rights such as "life, liberty, safety, health," etc.for refugees who are at risk in camps, providing a long-term solution to the issue of displacement for large numbers of refugees, and alleviating the burden on countries offering asylum to such displaced peoples. Frequently, these countries of asylum are some of the world's poorest nations and cannot handle the large influx of persons that occur when war, persecution, or other events drive refugees across their borders into their country.
However, only about 1% of the over 10.5 million refugees the UNHCR typically deals with are submitted for resettlement. Around 108,000 refugees were considered for the opportunity to be resettled in 2010, with the primary countries of origin being Iraq, Myanmar, and Bhutan.
UNHCR referred more than 121,000 refugees for consideration for resettlement in 2008. This was the highest number for 15 years. In 2007, 98,999 people were referred. UNHCR referred 33,512 refugees from Iraq, 30,388 from Burma/Myanmar and 23,516 from Bhutan in 2008.
In terms of resettlement "departures", in 2008, 65,548 refugees were resettled in 26 countries, up from 49,868 in 2007. The largest number of UNHCR-assisted departures were from Thailand (16,807), Nepal (8,165), Syria (7,153), Jordan (6,704) and Malaysia (5,865). Note that these are the countries that refugees were resettled from, not their countries of origin.
A number of third countries run specific resettlement programmes in co-operation with UNHCR. The size of these programmes is shown in the table. The largest programmes are run by the United States, Canada and Australia. A number of European countries run smaller schemes and in 2004 the United Kingdom established its own scheme, known as the Gateway Protection Programme with an initial annual quota of 500, which rose to 750 in the financial year 2008/09.
In September 2009, the European Commission unveiled plans for new Joint EU Resettlement Programme. The scheme would involve EU member states deciding together each year which refugees should be given priority. Member states would receive €4,000 from the European Refugee Fund per refugee resettled.
 Japan recognized only 305 persons as refugees between 1981 and 2002. Only six people were accepted in 2013.
The United States helped resettle roughly 2 million refugees between 1945 and 1979, when their refugee resettlement program was restructured. They now make use of 11 "Voluntary Agencies" (VOLAGS), which are non-governmental organizations that assist the government in the resettlement process. These organizations assist the refugees with the day-to-day needs of the large transition into a completely new culture. Usually, they are not funded by the government, but instead rely on their own resources and volunteers. Most of them have local offices, and caseworkers that provide individualized aid to each refugee's situation. They do rely on the sponsorship of individuals or groups, such as faith-based congregations or local organizations. The largest of the VOLAGS is the Migration and Refugee Services of the U.S. Catholic Conference. Others include Church World Service, Episcopal Migration Ministries, the Ethiopian Community Development Council, the Hebrew Immigrant Aid Society, the International Rescue Committee, Lutheran Immigration and Refugee Service, the U.S. Committee for Refugees and Immigrants, and World Relief.
There are a number of advantages to the strategy of using agencies other than the government to directly assist in resettlement. First of all, it has been estimated that for a federal or state bureaucracy to resettle refugees instead of the VOLAGS would double the overall cost. These agencies are often able to procure large quantities of donations and, more importantly, volunteers. According to one study, when the fact that resettlement workers often have to work nights, weekends, and overtime in order to meet the demands of the large cultural transition of new refugees is taken into account, the use of volunteers reduces the overall cost down to roughly a quarter. VOLAGS are also more flexible and responsive than the government since they are smaller and rely on their own funds.
Right of return.
Even in a supposedly "post-conflict" environment, it is not a simple process for refugees to return home. The UN Pinheiro Principles are guided by the idea that people not only have the right to return home, but also the right to the same property. It seeks to return to the pre-conflict status quo and ensure that no one profits from violence. Yet this is a very complex issue and every situation is different; conflict is a highly transformative force and the pre-war status-quo can never be reestablished completely, even if that were desirable (it may have caused the conflict in the first place). Therefore, the following are of particular importance to the right to return:
Historical and contemporary crises.
Movements in Africa.
Since the 1950s, many nations in Africa have suffered civil wars and ethnic strife, thus generating a massive number of refugees of many different nationalities and ethnic groups. The number of refugees in Africa increased from 860,000 in 1968 to 6,775,000 by 1992. By the end of 2004, that number had dropped to 2,748,400 refugees, according to the United Nations High Commission for Refugees. (That figure does not include internally displaced persons, who do not cross international borders and so do not fit the official definition of refugee.)
Many refugees in Africa cross into neighboring countries to find haven; often, African countries are simultaneously countries of origin for refugees and countries of asylum for other refugees. The Democratic Republic of Congo, for instance, was the country of origin for 462,203 refugees at the end of 2004, but a country of asylum for 199,323 other refugees.
Countries in Africa from where 5,000 or more refugees originated as of the end of 2004 are listed below. The largest number of refugees are from Sudan and have fled either the longstanding and recently concluded Sudanese Civil War or the War in Darfur and are located mainly in Chad, Uganda, Ethiopia, and Kenya.
Angola.
Decolonisation during the 1960s and 1970s often resulted in the mass exodus of European-descended settlers out of Africa – especially from North Africa (1.6 million European "pieds noirs"), Congo, Mozambique and Angola. By the mid-1970s, the Portugal's African territories were lost, and nearly one million Portuguese or persons of Portuguese descent left those territories (mostly Portuguese Angola and Mozambique) as destitute refugees – the "retornados".
The Angolan Civil War (1975–2002), one of the largest and deadliest Cold War conflicts, erupted shortly after and spread out across the newly independent country. At least one million people were killed, four million were displaced internally and another half million fled as refugees.
Uganda.
In the 1970s Uganda and other East African nations implemented racist policies that targeted the Asian population of the region. Uganda under Idi Amin's leadership was particularly most virulent in its anti-Asian policies, eventually resulting in the expulsion and ethnic cleansing of Uganda's Asian minority. Uganda's 80,000 Asians were mostly Indians born in the country. India had refused to accept them. Most of the expelled Indians eventually settled in the United Kingdom, Canada and in the United States.
The Lord's Resistance Army insurgency forced many civilians to live in internally displaced person camps.
Great Lakes crisis.
In the aftermath of the 1994 Rwandan Genocide, over two million people fled into neighboring countries, in particular Zaire. The refugee camps were soon controlled by the former government and Hutu militants who used the camps as bases to launch attacks against the new government in Rwanda. Little action was taken to resolve the situation and the crisis did not end until Rwanda-supported rebels forced the refugees back across the border at the beginning of the First Congo War.
Darfur.
An estimated 2.5 million people, roughly one-third the population of the Darfur area, have been forced to flee their homes after attacks by Janjaweed Arab militia backed by Sudanese troops during the ongoing war in Darfur in western Sudan since roughly 2003.
African refugees in Israel.
Since 2003, an estimated 70,000 illegal immigrants from various African countries have crossed into Israel. Some 600 refugees from the Darfur region of Sudan have been granted temporary resident status to be renewed every year, though not official refugee state. Another 2,000 refugees from the conflict between Eritrea and Ethiopia have been granted temporary resident status on humanitarian grounds. Israel prefers not to recognize them as refugees so as not to offend Eritrea and Ethiopia, though Sudanese, who are from an enemy state, are also not recognized as refugees. In effect, Israeli politicians, including the current prime minister Benjamin Netanyahu, have referred to the refugees as a threat to Israel's "Jewish character". African refugees are sometimes subject to racism and racial riots, as well as on-man assaults, have been occurring in Israel, especially in southern Tel Aviv since mid-2012.
During the past years, conflicts have occurred between Israelis and African immigrants in southern Tel-aviv, mostly due to poverty issues of both sides. Locals accuse African immigrants of Rape, Stealing and assault, making racial issues emerge in the south part of Tel-aviv, which became an immigrant populated area.
In 2012, Reuters reported that Israel may jail "illegal immigrants" for up to three years under a law put into effect to stem the flow of Africans across the desert border with Egypt. Netanyahu said that "If we don't stop their entry, the problem that currently stands at 60,000 could grow to 600,000, and that threatens our existence as a Jewish and democratic state."
African refugees in Egypt.
There are tens of thousands of Sudanese refugees in Egypt, most of them seeking refuge from ongoing military conflicts in their home country of Sudan. Their official status as refugees is highly disputed, and they have been subject to racial discrimination and police violence. They live among a much larger population of Sudanese migrants in Egypt, more than two million people of Sudanese nationality (by most estimates; a full range is 750,000 to 4 million (FMRS 2006:5) who live in Egypt. The U.S. Committee for Refugees and Immigrants believes many more of these migrants are in fact refugees, but see little benefit in seeking recognition.
Western Sahara conflict.
It is estimated that between 165,000 – 200,000 Sahrawis – people from the disputed territory of Western Sahara – have lived in five large refugee camps near Tindouf in the Algerian part of the Sahara Desert since 1975. The UNHCR and WFP are presently engaged in supporting what they describe as the "90,000 most vulnerable" refugees, giving no estimate for total refugee numbers.
Libyan Civil War.
Refugees of the 2011 Libyan civil war are the people, predominantly of Libyan nationality, who fled or were expelled from their homes during the 2011 Libyan civil war, from within the borders of Libya to the neighbouring states of Tunisia, Egypt and Chad, as well as to European countries, across the Mediterranean, as Boat people. The majority of Libyan refugees are Arabs and Berbers, though many of other ethnicities, temporarily living in Libya, originated from sub-Saharan Africa, were also among the first refugee waves to exit the country. The total Libyan refugee numbers are estimated at near one million as of June 2011. About half of them had returned to Libyan territory during summer 2011, though large refugee camps on Tunisian and Chad border kept being overpopulated.
Movements in the Americas.
Latin Americans.
More than one million Salvadorans were displaced during the Salvadoran Civil War from 1975 to 1982. About half went to the United States, most settling in the Los Angeles area. There was also a large exodus of Guatemalans during the 1980s, trying to escape from the civil war there as well. These people went to Southern Mexico and the U.S.
From 1991 through 1994, following the military coup d'état against President Jean-Bertrand Aristide, thousands of Haitians fled violence and repression by boat. Although most were repatriated to Haiti by the U.S. government, others entered the United States as refugees. Haitians were primarily regarded as economic migrants from the grinding poverty of Haiti, the poorest nation in the Western Hemisphere.
The victory of the forces led by Fidel Castro in the Cuban Revolution led to a large exodus of Cubans between 1959 and 1980. Thousands of Cubans yearly continue to risk the waters of the Straits of Florida seeking better economic and political conditions in the U.S. In 1999 the highly publicized case of six-year-old Elián González brought the covert migration to international attention. Measures by both governments have attempted to address the issue. The U.S. government instituted a wet feet, dry feet policy allowing refuge to those travelers who manage to complete their journey, and the Cuban government has periodically allowed for mass migration by organizing leaving posts. The most famous of these agreed migrations was the Mariel boatlift of 1980.
Colombia has one of the world's largest populations of internally displaced persons (IDPs), with estimates ranging from 2.6 to 4.3 million people, due to the ongoing Colombian armed conflict. The larger figure is cumulative since 1985. It is now estimated by the U.S. Committee for Refugees and Immigrants that there are about 150,000 Colombians in "refugee-like situations" in the United States, not recognized as refugees or subject to any formal protection.
United States.
During the Vietnam War, many U.S. citizens who were conscientious objectors and wished to avoid the draft sought political asylum in Canada. President Jimmy Carter issued an amnesty. Since 1975, the U.S. has resettled approximately 2.6 million refugees, with nearly 77% being either Indochinese or citizens of the former Soviet Union. Since the enactment of the Refugee Act of 1980, annual admissions figures have ranged from a high of 207,116 in 1980 to a low of 27,100 in 2002.
Currently, nine national voluntary agencies resettle refugees nationwide on behalf of the U.S. government: Church World Service, Ethiopian Community Development Council, Episcopal Migration Ministries, Hebrew Immigrant Aid Society, International Rescue Committee, U.S. Committee for Refugees and Immigrants, Lutheran Immigration and Refugee Service, United States Conference of Catholic Bishops, and World Relief.
Jesuit Refugee Service/USA (JRS/USA) has worked to help resettle Bhutanese refugees in the United States. The mission of JRS/USA is to accompany, serve and defend the rights of refugees and other forcibly displaced persons. JRS/USA is one of 10 geographic regions of Jesuit Refugee Service, an international Catholic organization sponsored by the Society of Jesus. In coordination with JRS's International Office in Rome, JRS/USA provides advocacy, financial and human resources for JRS regions throughout the world.
The U.S. Office of Refugee Resettlement (ORR) funds a number of organizations that provide technical assistance to voluntary agencies and local refugee resettlement organizations. RefugeeWorks, headquartered in Baltimore, Maryland, is ORR's training and technical assistance arm for employment and self-sufficiency activities, for example. This nonprofit organization assists refugee service providers in their efforts to help refugees achieve self-sufficiency. RefugeeWorks publishes white papers, newsletters and reports on refugee employment topics.
In 2005, as a result of hurricane Katrina, New Orleans citizens were referred to be the media as "refugees". Many New Orleanians consider the term refugee to be an insult. Resident Joseph Melancon explains, "And they had the nerve to call us refugees! When I heard they called us refugees, I couldn’t do nothing but drop my head cause I said I’m a United States citizen!" Actor Wendell Pierce says, "Damn, when the storm came it blew away our citizenship too?" Such narratives regarding the loss of citizenship are used to illustrate the trauma endured and the degradation citizens inflicted during the storm and they are also to show the federal government failing to uphold some contractual responsibility. In Sanctuary: African Americans and Empire, Waligora Davis remarks that, "The problem of the refugee, the stateless, the semi-colonial that DuBois names the black American, is a problem of the refugees relationship to the law and the state. Collectively, such persons signify a community outside the precincts of laws, they remain marginalized as a result of their loss of withheld citizenship." 
Yet, these narratives and Waligora-Davis’ definition does not fully engage the ways in which ‘refugee’ can be deployed as a diasporic trope for empowerment, similar to how The Fugees utilize the term in the diaspora. Here, I would then like to incorporate Alexander Weyheliye's "Sounding Diasporic Citizenship" to consider the possibility for the term "refugee" to be a liberating call to build community and redress trauma throughout the New Orleans diaspora using the example of the hip hop group, the Fugees.
Movements in Asia.
Afghanistan.
From the Soviet invasion of Afghanistan in 1979 until the late 2001 US-led invasion, about six million Afghan refugees have fled to neighboring Pakistan (mainly NWFP) and Iran, making Afghanistan the largest refugee-producing country. Since early 2002, more than 5 million Afghan refugees have repatriated through the UNHCR from both Pakistan and Iran back to their native country, Afghanistan. Approximately 3.5 million from Pakistan while the remaining 1.5 million from Iran. Since 2007 the Iranian government has forcibly deported mostly unregistered (and some registered) Afghan refugees back to Afghanistan, with 362,000 being deported in 2008.
More impormation:
• The first Afghanistan people to arrive in Australia was during the 1860s
• In 1979, the second group of immigrants from Afghanistan came to Australia, attacking hospitals, schools and mosques. Australians kept a small number as refugees
As of March 2009, some 1.7 million registered Afghan refugees still remain in Pakistan. This include the many who were born in Pakistan during the last 30 years but still counted as citizens of Afghanistan. They are allowed to work and study until the end of 2012. 935,600 registered Afghans are living in Iran, which also include the ones born inside Iran.
The 2011 industrialized country asylum data notes a 30% increase in applications from Afghans from 2010 to 2011, primarily towards Germany and Turkey. As of November 2012, there were still 1.8 million Afghans living in Pakistan given both security and economic instability in their home country. However, the country that for decades has hosted Afghan refugees has become the site of extensive military activity that has displaced Pakistanis internally as well as back and forth into Afghanistan. In recent years political momentum has also been building in Pakistan to compel Afghan refugees to repatriate. In July 2012, the Pakistani government announced it would not renew the ID cards of registered Afghan refugees, and as of January 2013, will treat them as illegal immigrants.
Dissolution of the British Raj, The Partition of 1947 and Independence.
The partition of the British Raj provinces of Panjab and Bangal and the subsequent independence of Pakistan and one day later of India in 1947 resulted in the largest human movement in history. In this population exchange, approximately 7 million Hindus and Sikhs from Bangladesh and Pakistan moved to India while approximately 7 million Muslims from India moved to Pakistan. Approximately one million Muslims, Hindus and Sikhs died during this event.
Bangladeshis in India in 1971.
As a result of the Bangladesh Liberation War, on 27 March 1971, Prime Minister of India, Indira Gandhi, expressed full support of her Government to the Bangladeshi struggle for freedom. The Bangladesh-India border was opened to allow panic-stricken Bangladeshis' safe shelter in India. The governments of West Bengal, Bihar, Assam, Meghalaya and Tripura established refugee camps along the border. Exiled Bangladeshi army officers and the Indian military immediately started using these camps for recruitment and training members of Mukti Bahini. During the Bangladesh War of Independence around 10 million Bangladeshis fled the country to escape the killings and atrocities committed by the Pakistan Army.
Bangladeshi refugees are known as '"Chakmas"' in India.
Pakistani Biharis in Bangladesh after 1971.
During the period of united Pakistan (1947–1971), the Urdu-speaking Biharis were not assimilated into the society of East Pakistan and remained a distinct cultural-linguistic group. Due to being a different linguistic group they were assaulted by Bengalis and the Indian Army in the 1971 war. Many atrocities took place against Biharis and even after the war they are still living in the same conditions. At the end of the war many Biharis took shelter in refugee camps in different cities, the biggest being the Geneva Camp in Dhaka. It is estimated that about 250,000 Biharis are living in those camps today.
Rohingyas in Bangladesh and Pakistan from Burma.
Bangladesh hosts more than 250,000 Muslim Rohingya refugees forced from western Burma (Myanmar) who fled in 1991-92 to escape persecution by the Burmese military junta. Many have lived there for close to twenty years. The Bangladeshi government divides the Rohingya into two categories – recognized refugees living in official camps and unrecognized refugees living in unofficial sites or among Bangladeshi communities. Around 30,000 Rohingyas are residing in two camps in Nayapara and Kutupalong area of Cox's Bazar district in Bangladesh. These camp residents have access to basic services, those outside do not. With no changes inside Burma in sight, Bangladesh must come to terms with the long-term needs of all the Rohingya refugees in the country, and allow international organizations to expand services that benefit the Rohingya as well as local communities.
The agency has been supporting Rohingya refugees staying in the camps. On the other hand, it is not receiving applications for refugee status from the newly arrived Rohingyas. This amounts to compromising of its mandate.
The brutal campaign of ethnic cleansing of Muslims in Arakan State by the Burmese military in 1991-92 thousands of people have been detained in crowded refugee camps in Bangladesh and tens of thousands have been repatriated to Burma to face further repression. There are widespread allegations of religious persecution, use of forced labor and denial of citizenship of many Rohingya forced to return to Burma since 1996.
Many have fled again to Bangladesh to seek work or shelter, or flee from Burmese military oppression, and some are forced across the border by Burmese security forces. In the past few months, abuses against Rohingya in Arakan State has continued, including strict registration laws that continue to deny Rohingya citizenship, restrictions on movement, land confiscation and forced evictions to make way for Buddhist Burmese settlements, widespread forced labor in infrastructure projects and closure of some mosques, including nine in North Buthidaung Township of Western Arakan State in the last half of 2006.
An estimated 90,000 people have been displaced in the 2012 sectarian violence between Rohingya Muslims and Buddhists in Burma's western Rakhine State.
There are also large number of Muslim Rohingya refugees in Pakistan. Most of them have made perilous journey across Bangladesh and India and have settled in Karachi.
Himalayas.
After the 1959 Tibetan exodus, there are more than 150,000 Tibetans who live in India, many in settlements in Dharamsala and Mysore, and Nepal. These include people who have escaped over the Himalayas from Tibet, as well as their children and grandchildren. In India the overwhelming majority of Tibetans born in India are still stateless and carry a document called an Identity Card issued by the Indian government in lieu of a passport. This document states the nationality of the holder as Tibetan. It is a document that is frequently rejected as a valid travel document by many customs and immigrations departments. The Tibetan refugees also own a Green Book issued by the Tibetan Government in Exile for rights and duties towards this administration.
In 1991–92, Bhutan expelled roughly 100,000 ethnic Nepalis known as Lhotshampas from the southern part of the country. Most of them have been living in seven refugee camps run by UNHCR in eastern Nepal ever since; some of them resettled in India. In March 2008, this population began a multiyear resettlement to third countries including the United States, New Zealand, Denmark, Canada, Norway and Australia. At present, the United States is working towards resettling more than 60,000 of these refugees in the US as a third country settlement programme.
Meanwhile, as many as 200,000 Nepalese were displaced during the Maoist insurgency and Nepalese Civil War which ended in 2006.
By 2009, more than 3 million civilians had been displaced by the War in North-West Pakistan (2004–present).
Sri Lanka.
The civil war in Sri Lanka, from 1983 to 2009 had generated thousands of internally displaced people as well as refugees most of them being the Tamils. Many Sri Lankans have fled to neighbourly India and western countries such as Canada, France, Denmark, the United Kingdom, and Germany.
While successive policies of discrimination and intimidation of the Tamils drove thousands to flee seeking asylum, the brutal end to the Civil War and the ongoing repression have forced a wave of thousands of refugees migrate, to countries like Canada, the UK and especially Australia. Australia in particular, receives hundreds of refugees every month.
About 69,000 Sri Lankan Tamil refugees live in 112 camps in the southern Indian state of Tamil Nadu.
Jammu and Kashmir.
According to the National Human Rights Commission (NHRC), about 300,000 Hindu Kashmiri Pandits have been forced to leave the state of Jammu and Kashmir due to Islamic militancy and religious discrimination from the Muslim majority, making them refugees in their own country. Some have found refuge in Jammu and its adjoining areas, while others in camps in Delhi and others in other states of India and other countries too. Kashmiri groups peg the number of migrants closer to 500,000.
Tajikistan civil war.
Since 1991, much of the country's non-Muslim population, including non-ethnic Tajikistan's Russians and Bukharian Jews, have fled Tajikistan due to severe poverty, instability and Tajikistan Civil War (1992–1997). In 1992, most of the country's Jewish population was evacuated to Israel. Most of the ethnic Russian population fled to Russia.By the end of the civil war Tajikistan was in a state of complete devastation. Around 1.2 million people were refugees inside and outside of the country. Due to severe poverty a lot of Tajiks had to migrate to Russia.47% of Tajikistan's GDP comes from immigrant remittances (from Tajiks working in Russia).
Uzbekistan.
In 1989, after bloody pogroms against the Meskhetian Turks in Central Asia's Ferghana Valley, nearly 90,000 Meskhetian Turks left Uzbekistan.
The 2010 ethnic violence in Kyrgyzstan left some 300,000 people internally displaced, and around 100,000 sought refuge in Uzbekistan.
Southeast Asia (Vietnam War).
Following the communist takeovers in Vietnam, Cambodia, and Laos in 1975, about three million people attempted to escape in the subsequent decades. With massive influx of refugees daily, the resources of the receiving countries were severely strained. The plight of the boat people became an international humanitarian crisis. The United Nations High Commissioner for Refugees (UNHCR) set up refugee camps in neighboring countries to process the boat people. The budget of the UNHCR increased from $80 million in 1975 to $500 million in 1980. Partly for its work in Indochina, the UNHCR was awarded the 1981 Nobel Peace Prize.
Movements in Europe.
World War II refugee issues.
Jewish refugees.
Between the first and second world wars, Jewish immigration to the British Mandate for Palestine was encouraged by the nascent Zionist movement, but was restricted by the British Mandate government, under the pressure of native Palestinians. In Europe, Nazi persecution culminated in the Holocaust and the mass murder of millions of European Jews.
The Evian Conference, Bermuda Conference, and other summits failed to resolve the problem of finding a home for large numbers of Jewish refugees from Nazi-occupied Europe. Following its formation in 1948, according to 1947 UN Partition Plan, Israel adopted the Law of Return, granting Israeli citizenship to any Jewish immigrant.
European Union.
According to the European Council on Refugees and Exiles, a network of European refugee-assisting non-governmental organizations (NGOs), huge differences exist between national asylum systems in Europe, making the asylum system a 'lottery' for refugees. For example, Iraqis who flee their home country and end up in Germany have an 85% chance of being recognised as a refugee and those who apply for asylum in Slovenia do not get a protection status at all.
United Kingdom.
In the United Kingdom the Asylum Support Partnership was created to enable all the agencies working to support and assist Asluym Seekers in making Asylum claims was established in 2012 and is part funded by the home office.
France.
In 2010, President Nicolas Sarkozy began the systematic dismantling of illegal Romani camps and squats in France, deporting thousands of Roma residing in France illegally to Romania, Bulgaria or elsewhere.
Hungary.
In 1956–57 following the Hungarian Revolution of 1956 nearly 200,000 persons, about two percent of the population of Hungary, fled as refugees to Austria and West Germany.
Czechoslovakia.
The Warsaw Pact invasion of Czechoslovakia in 1968 was followed by a wave of emigration, unseen before. It stopped shortly after (estimate: 70,000 immediately, 300,000 in total).
Southeastern Europe.
Following the Greek Civil War (1946–1949) hundreds of thousands of Greeks and Ethnic Macedonians were expelled or fled the country. The number of refugees ranged from 35,000 to over 213,000. Over 28,000 children were evacuated by the Partisans to the Eastern Bloc and the Socialist Republic of Macedonia. This left thousands of Greeks and Aegean Macedonians spread across the world.
The forced assimilation campaign of the late 1980s directed against ethnic Turks resulted in the emigration of some 300,000 Bulgarian Turks to Turkey.
Beginning in 1991, political upheavals in Southeastern Europe such as the breakup of Yugoslavia, displaced about 2,700,000 people by mid-1992, of which over 700,000 of them sought asylum in European Union member states. In 1999, about one million Albanians escaped from Serbian persecution.
Today there are still thousands of refugees and internally displaced persons in Southeastern Europe who cannot return to their homes. Most of them are Serbs who cannot return to Kosovo, and who still live in refugee camps in Serbia today. Over 200,000 Serbs and other non-Albanian minorities fled or were expelled from Kosovo after the Kosovo War in 1999.
In 2009, between 7% and 7.5% of Serbia's population were refugees and IDPs. Around 500,000 refugees, mainly from Croatia and Bosnia and Herzegovina, arrived following the Yugoslav wars. The IDPs were primarily from Kosovo. s of 2007[ [update]], Serbia had the largest refugee population in Europe.
Cyprus crisis of 1974.
It is estimated that 40% of the Greek population of Cyprus, as well as over half of the Turkish Cypriot population, were displaced during the Turkish invasion of Cyprus in 1974. The figures for internally displaced Cypriots varies, the United Peacekeeping force in Cyprus (UNFICYP) estimates 165,000 Greek Cypriots and 45,000 Turkish Cypriots. The UNHCR registers slightly higher figures of 200,000 and 65,000 respectively, being partly based on official Cypriot statistics which register children of displaced families as refugees. The separation of the two communities via the UN patrolled Green Line prohibited the return of all internally displaced people.
Chechnya.
From 1992 ongoing conflict has taken place in Chechenya, Caucasus due to independence proclaimed by this republic in 1991 which is not accepted by the Russian Federation or any other state in the world. As a consequence about 2 million people have been displaced and still cannot return to their homes. At the end of the Soviet era, ethnic Russians comprised about 23% of the population (269,000 in 1989). Due to widespread lawlessness and ethnic cleansing under the government of Dzhokhar Dudayev most non-Chechens (and many Chechens as well) fled the country during the 1990s or were killed.
Nagorno Karabakh.
The Nagorno Karabakh conflict has resulted in the displacement of 528,000 Azerbaijanis (this figure does not include new born children of these IDPs) from Armenian occupied territories including Nagorno Karabakh, and 220,000 Azeris and 18,000 Kurds fled from Armenia to Azerbaijan from 1988 to 1989. 280,000 persons—virtually all ethnic Armenians—fled Azerbaijan during the 1988–1993 war over the disputed region of Nagorno-Karabakh. By the time both Azerbaijan and Armenia had finally agreed to a ceasefire in 1994, an estimated 17,000 people had been killed, 50,000 had been injured, and over a million had been displaced.
Georgia.
More than 8,0050,000 people, mostly Georgians but some others too, were the victims of forcible displacement and ethnic-cleansing from Abkhazia during the War in Abkhazia between 1992 and 1993, and afterwards in 1993 and 1998.
As a result of 1991–1992 South Ossetia War, about 100,000 ethnic Ossetians fled South Ossetia and Georgia proper, most across the border into Russian North Ossetia. A further 23,000 ethnic Georgians fled South Ossetia and settled in other parts of Georgia.
The United Nations estimated 100,000 Georgians have been uprooted as a result of the 2008 South Ossetia war; some 30,000 residents of South Ossetia fled into the neighboring Russian province of North Ossetia.
Ukraine.
According to the United Nations (UNHCR's European director Vincent Cochetel), 814,000 Ukrainians have fled to Russia since the beginning of 2014, including those who did not register as asylum seekers, and 260,000 left to other parts of Ukraine. 
However, also quoting UNHCR, Deutsche Welle says 197,000 Ukrainians fled to Russia by August 20, 2014 and not less than 190,000 have fled to other parts of Ukraine, 14,000 to Belarus and 14,000 to Poland.
Movements in the Near and Middle East.
Palestinians.
As a result of the 1948 Palestine war and the 1948 Arab-Israel war, much of the Palestinian Arabs of what has become Israel fled or were expelled from their homes, either driven forcefully by Zionist paramilitary groups, by fear, or by instruction from Arab leadership. By the end of 1948, there were about 700,000 Palestinian refugees.
Palestinian refugees and their descendents spread throughout the Arab world; the largest populations are found in neighboring Levantine countries—Syria, Lebanon and Jordan. The populations of the West Bank and Gaza are also composed to a large extent of refugees and their descendents. 
Until 1967, the West Bank and Gaza were officially ruled, respectively, by Egypt and Jordan, whose Hashemite Kingdom was the only Arab government to grant citizenship to Palestinian refugees. Most Arab states have refused to absorb Palestinians as equal citizens.
Palestinian refugees from 1948 and their descendants do not come under the 1951 UN Convention Relating to the Status of Refugees, but under the UN Relief and Works Agency for Palestine Refugees in the Near East, which created its own criteria for refugee classification. The great majority of Palestinian refugees have kept the refugee status for generations, under a special decree of the UN, and legally defined to include descendants of refugees, as well as others who might otherwise be considered internally displaced persons.
As of December 2005, the World Refugee Survey of the U.S. Committee for Refugees and Immigrants estimates the total number of Palestinian refugees and their descendants to be 2,966,100. Palestinian refugees number almost half of Jordan's population, however they have assimilated into Jordanian society, having a full citizenship. In Syria, though not officially becoming citizens, most of the Palestinian refugees were granted resident rights and issued travel documents. Following the Oslo Agreements, attempts were made to integrate the displaced Palestinians and their descendants into the Palestinian community. In addition, Israel granted permissions for family reunions and return of only about 10,000 Fatah members to the West Bank. The refugee situation and the presence of numerous refugee camps continues to be a point of contention in the Israeli-Palestinian conflict.
Jews of Arab and Muslim countries.
Following the Jewish exodus from Arab and Muslim countries, the combined population of Jewish communities of the Middle East (excluding Israel) and North Africa was reduced from about 900,000 in 1948 to less than 8,000 today. The history of the exodus is politicized, given its proposed relevance to a final settlement Israeli-Palestinian peace negotiations. When presenting the history, those who view the Jewish exodus as equivalent to the 1948 Palestinian exodus, such as the Israeli government and NGOs such as JJAC and JIMENA, emphasize "push factors", such as cases of anti-Jewish violence and forced expulsions, and refer to those affected as "refugees". Those who argue that the exodus does not equate to the Palestinian exodus emphasize "pull factors", such as the actions of local Zionist agents aiming to fulfil the One Million Plan, highlight good relations between the Jewish communities and their country's governments, emphasize the impact of other push factors such as the decolonization in the Maghreb and the Suez War and Lavon Affair in Egypt, and argue that many or all of those who left were not refugees.
Israel absorbed approximately 600,000 Jews from Arab and Muslim countries, many of whom were temporarily settled in tent cities called "Ma'abarot". They were eventually absorbed into Israeli society, and the last "Ma'abarah" was dismantled in 1958. By contrast European Jews were quickly settled in lands and homes belonging to Palestinian refugees. Their descendants, and those of Iranian and Turkish Jews, now number 3.06 million of Israel's 5.4 to 5.8 million Jewish citizens.
In 2007, both the US Senate and House of Representatives passed simple resolutions and to Make clear that the United States Government supports the position that, as an integral part of any comprehensive peace, the issue of refugees and the mass violations of human rights of minorities in Arab and Muslim countries throughout the Middle East, North Africa, and the Persian Gulf must be resolved in a manner that includes (A) consideration of the legitimate rights of all refugees displaced from Arab and Muslim countries throughout the Middle East, North Africa, and the Persian Gulf; and (B) recognition of the losses incurred by Jews, Christians, and other minority groups as a result of the Arab-Israeli conflict.
The resolutions had been written together with lobbyist group JJAC, whose founder Stanley Urman described the resolution in 2009 as "perhaps our most significant accomplishment". The House of Representatives resolution was sponsored by AIPAC-member Jerrold Nadler. Michael Fischbach explain the resolutions as "a tactic to help the Israeli government deflect Palestinian refugee claims in any final Israeli-Palestinian peace deal, claims that include Palestinian refugees’ demand for the "right of return" to their pre-1948 homes in Israel."
Other Israeli academics and leaders state that Oriental Jews did not come to Israel as refugees, pointing out that many decided to migrate despite leading comfortable lives in the Arab world and arrived to Israel under the directive of underground Zionist activists acting on behalf of the Israeli state.
Some Arab countries, like Iraq, did take a number of measures against Jews who left the country, including the confiscation of assets left behind.
Internally displaced Syrians from the Golan Heights.
After the 1967 war, when Israel launched pre-emptive attacks on Egypt and Syrian and annexed the Golan Heights. Israel destroyed 139 Syrian villages in the occupied territory of the Golan Heights and 130,000 of its residents fled or were expelled from their lands, which now serve the purpose of settlements and military bases. About 9,000 Syrians, all of whom of the Druze ethno-religious group, were allowed to remain in their lands.
Lebanon Civil War crisis.
It is estimated that some 900,000 people, representing one-fifth of the pre-war population, were displaced from their homes during the Lebanese Civil War (1975–90).
The 2006 Lebanon War temporarily displaced approximately one million Lebanese and approximately 500,000 Israelis, although most were able to return to their homes. Lebanese desire to emigrate has increased since the war. Over a fifth of Shias, a quarter of Sunnis, and nearly half of Maronites have expressed the desire to leave Lebanon. Nearly a third of such Maronites have already submitted visa applications to foreign embassies, and another 60,000 Lebanese Christians have already fled, as of April 2007. Lebanese Christians are concerned that their influence is waning, fear the apparent rise of radical Islam, and worry of potential Sunni-Shia rivalry.
Kurdish population displacement due to Turkish conflict.
Between 1984 and 1999, the Turkish Armed Forces and various groups claiming to represent the Kurdish people have engaged in open war, and much of the countryside in the southeast was depopulated, with Kurdish civilians moving to local defensible centers such as Diyarbakır, Van, and Şırnak, as well as to the cities of western Turkey and even to western Europe. The causes of the depopulation included Kurdistan Workers' Party atrocities against Kurdish clans they could not control, the poverty of the southeast, and the Turkish state's military operations. Human Rights Watch has documented many instances where the Turkish military forcibly evacuated villages, destroying houses and equipment to prevent the return of the inhabitants. An estimated 3,000 Kurdish villages in Turkey were virtually wiped from the map, representing the displacement of more than 378,000 people.
Iran-Iraq war.
The Iran–Iraq War from 1980 to 1988, the 1990 Iraqi invasion of Kuwait, the first Gulf War and subsequent conflicts all generated hundreds of thousands if not millions of refugees. Iran also provided asylum for 1,400,000 Iraqi refugees who had been uprooted as a result of the 1991 uprisings in Iraq (1990–91). At least one million Iraqi Kurds were displaced during the Al-Anfal Campaign (1986–1989).
Refugees of the Gulf War.
The Palestinian exodus from Kuwait took place during and after the Gulf War. There were 400,000 Palestinians in Kuwait before the Gulf War. During the Gulf War, more than 200,000 Palestinians fled Kuwait during the Iraqi occupation of Kuwait due to harassment and intimidation by Iraqi security forces, in addition to getting fired from work by Iraqi authority figures in Kuwait. After the Gulf War in 1991, Kuwaiti authorities pressured nearly 200,000 Palestinians to leave Kuwait. The policy which partly led to this exodus was a response to the alignment of PLO leader Yasser Arafat with Saddam Hussein.
Iraq War (2003-today).
The current Iraq war has generated millions of refugees and internally displaced persons. As of 2007 more Iraqis have lost their homes and become refugees than the population of any other country. Over 4,700,000 people, more than 16% of the Iraqi population, have become uprooted. Of these, about 2 million have fled Iraq and flooded other countries, and 2.7 million are estimated to be refugees inside Iraq, with nearly 100,000 Iraqis fleeing to Syria and Jordan each month. Only 1% of the total Iraqi displaced population was estimated to be in the Western countries.
Roughly 40% of Iraq's middle class is believed to have fled, the U.N. said. Most are fleeing systematic persecution and have no desire to return. All kinds of people, from university professors to bakers, have been targeted by militias, insurgents and criminals. An estimated 331 school teachers were slain in the first four months of 2006, according to Human Rights Watch, and at least 2,000 Iraqi doctors have been killed and 250 kidnapped since the 2003 U.S. invasion. Iraqi refugees in Syria and Jordan live in impoverished communities with little international attention
to their plight and little legal protection. In Syria alone an estimated 50,000 Iraqi girls and women, many of them widows, are forced into prostitution just to survive.
According to Washington-based Refugees International, out of the 4.2 million refugees fewer than 800 have been allowed into the US since the 2003 invasion. Sweden had accepted 18,000 and Australia had resettled almost 6,000. By 2006 Sweden had granted protection to more Iraqis than all the other EU Member States combined. However, and following repeated unanswered calls to its European partners for greater solidarity, July 2007 saw Sweden introduce a more restrictive policy towards Iraqi asylum seekers, which is expected to reduce the recognition rate in 2008.
As of September 2007 Syria had decided to implement a strict visa regime to limit the number of Iraqis entering the country at up to 5,000 per day, cutting the only accessible escape route for thousands of refugees fleeing the civil war in Iraq. A government decree that took effect on 10 September 2007 bars Iraqi passport holders from entering Syria except for businessmen and academics. Until then, the Syria was the only country that had resisted strict entry regulations for Iraqis.
In June 2014, More than 500,000 people fled Mosul to escape from the advancing Islamic State of Iraq and Syria (ISIS).
Mandaeans and Yazidis.
Furthermore, the small Mandaean and Yazidi communities are at the risk of elimination due to ethnic cleansing by Islamic militants. Entire neighborhoods in Baghdad were ethnically cleansed by Shia and Sunni Militias. Satellite shows ethnic cleansing in Iraq was key factor in "surge" success.
Refugees in Jordan.
Jordan has one of the world's largest immigrant populations with some sources putting the immigrant percentage to being 60%. Iraqi refugees number between 750,000 and 1 million in Jordan with most living in Amman. Jordan also has Armenian, Chechen, Circassian minorities, and about half of its population is said to be of Palestinian refugees and their descendents.
Syrian refugees.
To escape the violence, nearly 2.5 million Syrian refugees have fled the country to neighboring Jordan, Lebanon, Turkey and Iraq.
Religious minorities in the Middle East.
The US government position on refugees states that there is repression of religious minorities in the Middle East and in Pakistan such as Christians, Hindus, as well as Ahmadi, and Zikri denominations of Islam. In Sudan, where Islam is the state religion, Muslims dominate the government and restrict activities of Christians, practitioners of traditional African indigenous religions and other non-Muslims. The question of Jewish, Christian and other refugees from Arab and Muslim countries was introduced in March 2007 in the US Congress.
Refugee issues.
Medical problems.
Apart from physical wounds or starvation, a large percentage of refugees develop symptoms of post-traumatic stress disorder (PTSD) or depression. These long-term mental problems can severely impede the functionality of the person in everyday situations; it makes matters even worse for displaced persons who are confronted with a new environment and challenging situations. They are also at high risk for suicide.
Among other symptoms, post-traumatic stress disorder involves anxiety, over-alertness, sleeplessness, chronic fatigue syndrome, motor difficulties, failing short term memory, amnesia, nightmares and sleep-paralysis. Flashbacks are characteristic to the disorder: the patient experiences the traumatic event, or pieces of it, again and again. Depression is also characteristic for PTSD-patients and may also occur without accompanying PTSD.
PTSD was diagnosed in 34.1% of Palestinian children, most of whom were refugees, males, and working. The participants were 1,000 children aged 12 to 16 years from governmental, private, and United Nations Relief Work Agency UNRWA schools in East Jerusalem and various governorates in the West Bank.
Another study showed that 28.3% of Bosnian refugee women had symptoms of PTSD three or four years after their arrival in Sweden. These women also had significantly higher risks of symptoms of depression, anxiety, and psychological distress than Swedish-born women. For depression the odds ratio was 9.50 among Bosnian women.
A study by the Department of Pediatrics and Emergency Medicine at the Boston University School of Medicine demonstrated that twenty percent of Sudanese refugee minors living in the United States had a diagnosis of post-traumatic stress disorder. They were also more likely to have worse scores on all the Child Health Questionnaire subscales.
Many more studies illustrate the problem. One meta-study was conducted by the psychiatry department of Oxford University at Warneford Hospital in the United Kingdom. Twenty surveys were analyzed, providing results for 6,743 adult refugees from seven countries. In the larger studies, 9% were diagnosed with post-traumatic stress disorder and 5% with major depression, with evidence of much psychiatric co-morbidity. Five surveys of 260 refugee children from three countries yielded a prevalence of 11% for post-traumatic stress disorder. According to this study, refugees resettled in Western countries could be about ten times more likely to have PTSD than age-matched general populations in those countries. Worldwide, tens of thousands of refugees and former refugees resettled in Western countries probably have post-traumatic stress disorder.
Exploitation.
Refugee populations consist of people who are terrified and are away from familiar surroundings. There can be instances of exploitation at the hands of enforcement officials, citizens of the host country, and even United Nations peacekeepers. Instances of
human rights violations, child labor, mental and physical trauma/torture, violence-related trauma, and sexual exploitation, especially of children, are not entirely unknown. In many refugee camps in three war-torn West African countries, Sierra Leone, Guinea, and Liberia, young girls were found to be exchanging sex for money, a handful of fruit, or even a bar of soap. Most of these girls were between 13 and 18 years of age. In most cases, if the girls had been forced to stay, they would have been forced into marriage. They became pregnant around the age of 15 on average. This happened as recently as in 2001. Parents tended to turn a blind eye because sexual exploitation had become a "mechanism of survival" in these camps.

</doc>
<doc id="45548" url="http://en.wikipedia.org/wiki?curid=45548" title="Ozieri">
Ozieri

 Ozieri (Sardinian: "Othieri") is a town and "comune" of approximatively 11,000 inhabitants in the province of Sassari, northern Sardinia (Italy), in the Logudoro historical region. Ozieri is the centre of the earliest known archaeological culture on Sardinia (known as Ozieri Culture).
Main sights.
Ozieri gives its name to the Ozieri culture, a prehistoric civilization whose first findings were excavated in the nearby grotto of San Michele starting from 1914.
Transportation.
Ozieri can be reached from Sassari through the SS.597 National road, and by Olbia (SS.199).
The city has a railways station located in the "frazione" of Chilivani (lines to Olbia, Porto Torres and Cagliari).
References.
<BR>

</doc>
<doc id="45549" url="http://en.wikipedia.org/wiki?curid=45549" title="West Timor">
West Timor

West Timor (Indonesian: "Timor Barat") is the western and Indonesian portion of the island of Timor and part of the province of East Nusa Tenggara, (Indonesian: "Nusa Tenggara Timur").
During the colonial period it was known as Dutch Timor and was a centre of Dutch loyalists during the Indonesian National Revolution (1945–1949).
From 1949 to 1975 it was known as Indonesian Timor.
History.
European colonization of Timor began in the 16th century. Although the Portuguese claimed the island of Timor in 1520, the Dutch (in the form of the Dutch East India Company) settled West Timor in 1640, forcing the Portuguese out to East Timor. The subsequent collapse of the company meant that in 1799 the area returned to official Dutch rule. Finally, in 1914 the border between East and West Timor was finalized by a treaty between Portugal and the Netherlands that was originally signed in 1859 and modified in 1893.
West Timor had the status of "residentie" within the Dutch East Indies.
Japan conquered the island during World War II in early 1942. Upon Indonesian independence, West Timor became part of the new Republic of Indonesia.
On 6 September 2000, three UNHCR staff members were attacked and killed in Atambua, a town in West Timor. Pero Simundza from Croatia, Carlos Caceres-Collazio from Puerto Rico and Samson Aregahegn from Ethiopia were killed in an attack by 5,000 members of a pro-Indonesian militia, armed with machetes, on the office of UNHCR in Atambua, West Timor, where the main refugee camp was located in the vicinity of the border with East Timor. (see Attacks on humanitarian workers).
Geography.
West Timor is a political region that comprises the western half of Timor island with the exception of Oecusse district (which is politically part of East Timor) and forms a part of the Indonesian province of Nusa Tenggara Timur, (NTT or East Nusa Tenggara). The land area of West Timor is 15850 km2. The highest point of West Timor is Mount Mutis at 2427 m.
Rote Island, the southernmost island of Indonesia, is southwest of West Timor.
West Timor's largest town and chief port is Kupang. It is the capital of Nusa Tenggara Timur province.
Administration.
West Timor is part of the East Nusa Tenggara province. It was formerly split into the City of Kupang (a "kabupaten" or regency-level administrative area) and four regencies (kabupaten); from west to east these are: Kupang, Timor Tengah Selatan (South Central Timor), Timor Tengah Utara (North Central Timor) and Belu. However a fifth regency - Malaka - was in 2012 formed from the southern half of Belu Regency. Note the administrative area has shrunk as Rote Ndao Regency (Rote and Ndoa islands to the southwest) and Sabu Raijua Regency (the Savu Islands further west) were split off in 2002 and 2009 respectively from Kupang Regency. The island accounts for 35.5% of the provincial population.
Population.
West Timor's main religions are Catholicism (56%), Protestantism (35%) and Islam (8%). There are approximately 1.8 million inhabitants in 2008, some of them are still refugees who fled the 1999 violence in East Timor.
In addition to the national language, Indonesian, native languages belonging to the Fabronic Stock of the Austronesian group of languages are spoken in West Timor, the others in East Timor. These languages include Uab Meto, Tetum, Ndaonese, Rotinese, and Helong. Knowledge of Dutch is now limited to the older generations.
Economy.
West Timor has an average unemployment rate of 2.39%. 30% of the population lived below the poverty line in 1998; as of 2012, it stays at 30%. The economy is mainly agricultural, using slash and burn methods to produce corn, rice, coffee, copra and fruit. Some timber harvesting is undertaken, producing eucalyptus, sandalwood, teak, bamboo and rosewood.

</doc>
<doc id="45550" url="http://en.wikipedia.org/wiki?curid=45550" title="Porto Torres">
Porto Torres

Porto Torres (Sassarese: "Posthudorra", Sardinian: "Pòrtu Turre") is a "comune" and city in northern Sardinia, in the Province of Sassari.
It is situated on the north-west coast about 25 km east of the Gorditanian promontory (Capo del Falcone), and on the spacious bay of the Gulf of Asinara.
History.
In ancient times, Turris Libyssonis was one of the most considerable cities in Sardinia. It was probably of purely Roman origin, founded apparently by Julius Caesar, as it bore the title "Colonia Julia". Pliny described it as a colony, the only on the island in his time, suggesting that there was previously no town on the spot, but merely a fort or "castellum". It is noticed also by Ptolemy and in the Itineraries, but without any indication that it was a place of any importance.
The ancient remains still existing prove that it must have been a considerable town under the Roman Empire. According from the inscriptions on ancient milestones, the principal road through the island ran directly from Caralis (Cagliari) to Turris, a sufficient proof that the latter was a place much frequented. Indeed, two roads, which diverged at Othoca (modern Santa Giusta) connected Caralis to Turris, the more important keeping inland and the other following the west coast. It was also an episcopal see during the early part of the Middle Ages.
The existing port at Porto Torres, which is almost wholly artificial, is based in great part on Roman foundations; and there exist also the remains of a temple (which, as we learn from an inscription, was dedicated to Fortune, and restored in the reign of Philip), of "thermae", of a basilica and an aqueduct, as well as a bridge over the adjoining small river, still called the "Fiume Turritano". The ancient city continued to be inhabited till the 11th century, when the greater part of the population migrated to Sassari, about 15 km inland, and situated on a hill. It was partly under Genoese hands before, in the early 15th century, it was conquered by the Aragonese. After the Spanish rule it was part of the Kingdom of Sardinia.
Torres was separated from the comune of Sassari in 1842. At the time the area which had been built around the basilica of San Gavino joined the fishermen's community near the port to form the new "Porto Torres".
Transportation and industry.
The port is connected by ferries with Genoa, Marseille, Toulon, Barcelona, Civitavecchia, Propriano, Expressway SS131/E25 to Sassari and Cagliari, and a national road to Santa Teresa Gallura (SS200).
A railway operated by Trenitalia connects the town with Sassari, and the rest of the island.
Chemical industries support the modern economy of Porto Torres. Fiume Santo, a 1,040 MW power station owned by E.ON, is 5 to west from the port, in the municipality of Sassari.
Plans related to industrial conversion are is in progress in Porto Torres, where seven research centres are developing the transformation from traditional fossil fuel related industry to an integrated production chain from vegetable oil using oleaginous seeds to bio-plastics.

</doc>
<doc id="45551" url="http://en.wikipedia.org/wiki?curid=45551" title="Alghero">
Alghero

Alghero (]; Catalan: "L'Alguer", ], ]; Sardinian: "S'Alighèra"; Sassarese: "La Liéra"), is a town of about 44,000 inhabitants in Italy. It lies in the province of Sassari in northwestern Sardinia, next to the Mediterranean Sea.
The name Alghero comes from the medieval Latin "Aleguerium", meaning stagnation of algae ("Posidonia oceanica", actually a seagrass) on the coast. The Catalan language is co-official in the city, unique in Italy.
History.
The area of today's Alghero has been settled since pre-historic times. The so-called Ozieri culture was present here in the 4th millennium BC; while the Nuraghe civilization was present in the area around 1500 BC.
The Phoenicians would settle there by the 8th century BC, and the metalworking town of Sant'Imbenia, with a mixed Phoenician and Nuragic population, engaged in trade with the Etruscans on the Italian mainland.
Due to its strategic position in the Mediterranean Sea, Alghero was built around a fortified port, founded around 1102 by the Genoese Doria family. The Doria ruled it for centuries, apart from a brief period under the rule of Pisa (1283–84). In 1353 it was captured by the forces of the Crown of Aragon under Bernardo de Cabrera; in 1372, following several revolts, the indigenous Sardinian and Genoese population was expelled, and Alghero could later grow in number because of the arrival of Catalan colonists. In the early 16th century Alghero received the status of King's City ("ciutat de l'Alguer") and developed economically.
The Aragonese were followed by the Spanish Habsburgs, whose dominion, ending in 1702, brought some stylish elegance to the city. In 1720 Alghero and Sardinia were handed over to the Piedmont based House of Savoy. Around 1750 a wide channel was excavated to improve the defensive position of the peninsula. In 1821 famine led to a revolt of the population, which was bloodily suppressed. At the end of the same century Alghero was de-militarised and, during the Fascist era, part of the surrounding marshes was reclaimed and the suburbs of Fertilia and S.M. La Palma were founded. During World War II (1943) Alghero was bombed, and its historical centre suffered heavy damage. The presence of malaria in the countryside was finally overcome in the 1950s.
Since then, Alghero has become a popular tourist resort.
Language.
In Alghero, a dialect of Catalan is spoken, introduced when Catalan settlers repopulated the town after the Crown of Aragon conquered the city from the Genoese in 1353 and subsequently expelled the indigenous population. Catalan was replaced as the official language by Spanish in the 17th century, then by Italian. The most recent linguistic research conducted showed that 22.4% of the population speak Algherese Catalan as a first language and around 90% have some understanding of the language. Currently, there has been a revival of the arts in Algherese Catalan, with singers such as Franca Masu performing original compositions in the language.
It is noted, however, that after the rural exodus of the surrounding villages towards the city, much of the population speaks or has some proficiency in Sardinian as well, in addition to Italian and Catalan. Historically, moreover, the spread of Catalan was limited to the city "Intra Moenia" and part of the coast, given that the countryside has always been highly frequented by the Sardinian speaking-community (as evidenced by the place names: "Sa Segada", "Sa Londra", "Pala Pirastru", etc.).
Main sights.
The area northwest of Alghero Bay with Porto Conte and the relatives and Capo Caccia limestone promontory offer several fields of study and activities, from geology to biology studies and researches, to sport and adventures like caving scuba diving and cave-diving, trekking and climbing. There are more than 300 discovered caves upon and under water and semisubmerged. Neptune's Grotte is the most famous and visited, thanks to the accessibility and connection by ferries and stepped path from land. Underwater the Nereo Cave, considered the biggest and spectacular marine underwater cave of the Mediterranean Sea and Europe, is the most visited by scuba-divers.
Some 100 Nuraghe remains can be seen in the neighbouring areas of Sant'Imbenia (including also a Phoenician necropolis and Roman remains near the airport of Alghero), Palmavera and Anghelu Ruju.
Books.
In the 1930s the Swedish writer Amelie Posse Brazdova wrote a book entitled "Sardinia Side Show", where she told the complete story of 2 years she spent "interned" in Alghero old town during World War I.
External links.
<br>

</doc>
<doc id="45552" url="http://en.wikipedia.org/wiki?curid=45552" title="United Nations Trust Territories">
United Nations Trust Territories

United Nations trust territories were the successors of the remaining League of Nations mandates, and came into being when the League of Nations ceased to exist in 1946. All of the trust territories were administered through the United Nations Trusteeship Council. The one territory not turned over was South-West Africa, which South Africa insisted remained under the League of Nations Mandate. It eventually gained independence in 1990 as Namibia. The main objection was that the trust territory guidelines required that the lands be prepared for independence and majority rule.
The concept is distinct from a United Nations protectorate, a temporary government under direct UN administration. 
Trust territories (and administering powers).
Former German Schutzgebiete.
All these territories previously were League of Nations mandates.
Former German and/or Japanese colonies.
These territories were also former League of Nations mandates.
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="45553" url="http://en.wikipedia.org/wiki?curid=45553" title="Portuguese Timor">
Portuguese Timor

 |style="width:1.0em; padding:0 0 0 0.6em;"| - 
 |style="padding-left:0;text-align:left;"| King<br> 1515–21
 |- class="mergedrow"
 |style="width:1.0em; padding:0 0 0 0.6em;"| - ||style="padding-left:0;text-align:left;"|1702–05 (first)|| 
Portuguese Timor was the name of East Timor when it was under Portuguese control. During most of this period, Portugal shared the island of Timor with the Dutch East Indies.
The first Europeans to arrive in the region were Portuguese in 1515. Dominican friars established a presence on the island in 1556, and the territory was declared a Portuguese colony in 1702. Following a Lisbon-instigated decolonisation process, Indonesia invaded East Timor in 1975. However, the invasion was never accepted by other countries, so Portuguese Timor existed officially until independence in 2002.
Early colonialists.
Prior to the arrival of European colonial powers, the island of Timor was part of the trading networks that stretched between India and China and incorporating Maritime Southeast Asia. The island's large stands of fragrant sandalwood were its main commodity. The first European powers to arrive in the area were the Portuguese in the early sixteenth century followed by the Dutch in the late sixteenth century. Both came in search of the fabled Spice Islands of Maluku. In 1515, Portuguese first landed near modern Pante Macassar. Portuguese merchants exported sandalwood from the island, until the tree nearly became extinct. In 1556 a group of Dominican friars established the village of Lifau.
In 1613, the Dutch take control of the Western part of the island. Over the following three centuries, the Dutch would come to dominate the Indonesian archipelago with the exception of the eastern half of Timor, which would become Portuguese Timor. The Portuguese introduced maize as a food crop and coffee as an export crop. Timorese systems of tax and labour control were preserved, through which taxes were paid through their labour and a portion of the coffee and sandalwood crop. The Portuguese introduced mercenaries into Timor communities and Timor chiefs hired Portuguese soldiers for wars against neighbouring tribes. With the use of the Portuguese musket, Timorese men became deer hunters and suppliers of deer horn and hide for export.
The Portuguese introduced Roman Catholicism to East Timor, the Latin writing system, the printing press, and formal schooling. Two groups of people were introduced to East Timor: Portuguese men, and Topasses. Portuguese language was introduced into church and state business, and Portuguese Asians used Malay in addition to Portuguese. Under colonial policy, Portuguese citizenship was available to men who assimilated Portuguese language, literacy, and religion; by 1970, 1,200 East Timorese, largely drawn from the aristocracy, Dili residents, or larger towns, had obtained Portuguese citizenship. By the end of the colonial administration in 1974, 30 percent of Timorese were practicing Roman Catholics while the majority continued to worship spirits of the land and sky.
Establishment of the colonial state.
In 1702, Lisbon sent its first governor successfully, António Coelho Guerreiro, to Lifau, which became capital of all Portuguese dependencies on Lesser Sunda Islands. Former capitals were Solor and Larantuka. Portuguese control over the territory was tenuous particularly in the mountainous interior. Dominican friars, the occasional Dutch raid, and the Timorese themselves competed with Portuguese merchants. The control of colonial administrators was largely restricted to the Dili area, and they had to rely on traditional tribal chieftains for control and influence.
The capital was moved to Dili in 1769, due to attacks from the Topasses, who became rulers of several local kingdoms (Liurai). At the same time, the Dutch were colonising the west of the island and the surrounding archipelago that is now Indonesia. The border between Portuguese Timor and the Dutch East Indies was formally decided in 1859 with the Treaty of Lisbon. In 1913, the Portuguese and Dutch formally agreed to split the island between them. The definitive border was drawn by the Permanent Court of Arbitration in 1916, and it remains the international boundary between the modern states of East Timor and Indonesia.
For the Portuguese, East Timor remained little more than a neglected trading post until the late nineteenth century. Investment in infrastructure, health, and education was minimal. Sandalwood remained the main export crop with coffee exports becoming significant in the mid-nineteenth century. In places where Portuguese rule was asserted, it tended to be brutal and exploitative.
Twentieth century.
At the beginning of the twentieth century, a faltering home economy prompted the Portuguese to extract greater wealth from its colonies, resulting in increased resistance to Portuguese rule in East Timor. In 1910–12, a Timorese rebellion was quashed after Portugal brought in troops from its colonies in Mozambique and Macau, resulting in the deaths of 3,000 East Timorese.
In the 1930s, the Japanese semi-governmental "Nan’yō Kōhatsu" development company, with the secret sponsorship of the Imperial Japanese Navy invested heavily in a joint-venture with the primary plantation company of Portuguese Timor, SAPT. The joint-venture effectively controlled imports and exports into the island by the mid-1930s and the extension of Japanese interests greatly concerned the British, Dutch and Australian authorities.
Although Portugal was neutral during World War II, in December 1941, Portuguese Timor was occupied by a small British, Australian and Dutch force, to preempt a Japanese invasion. However, the Japanese did invade in the Battle of Timor in February 1942. Under Japanese occupation, the borders of the Dutch and Portuguese were overlooked with Timor island being made a single Japanese army administration zone. 400 Australian and Dutch commandos trapped on the island by the Japanese invasion waged a guerrilla campaign, which tied up Japanese troops and inflicted over 1,000 casualties. Timorese and the Portuguese helped the guerillas but following the Allies' eventual evacuation, Japanese retribution from their soldiers and Timorese militia raised in West Timor was severe. By the end of the War, an estimated 40–60,000 Timorese had died, the economy was in ruins, and famine widespread. (see Battle of Timor).
Following World War II, the Portuguese promptly returned to reclaim their colony, while West Timor became part of Indonesia, which secured its independence in 1949. To rebuild the economy, colonial administrators forced local chiefs to supply labourers which further damaged the agricultural sector. The role of the Catholic Church in East Timor grew following the Portuguese government handing over the education of the Timorese to the Church in 1941. In post-war Portuguese Timor, primary and secondary school education levels significantly increased, albeit on a very low base. Although illiteracy in 1973 was estimated at 93 per cent of the population, the small educated elite of East Timorese produced by the Church in the 1960s and 70s, became the independence leaders during the Indonesian occupation.
End of Portuguese rule.
Following a 1974 coup (the "Carnation Revolution"), the new government of Portugal favoured a gradual decolonisation process for Portuguese territories in Asia and Africa. When East Timorese political parties were first legalised in April 1974, three major players emerged. The Timorese Democratic Union (UDT), was dedicated to preserving East Timor as a protectorate of Portugal and in September announced its support for independence. Fretilin endorsed "the universal doctrines of socialism", as well as "the right to independence", and later declared itself "the only legitimate representative of the people". A third party, APODETI emerged advocating East Timor's integration with Indonesia expressing concerns that an independent East Timor would be economically weak and vulnerable.
On 28 November 1975, East Timorese declared the territory's independence.
Nine days later, Indonesia invaded the territory declaring it Indonesia's 27th province Timor Timur in 1976. The United Nations, however, did not recognise the annexation. The last governor of Portuguese Timor was Mário Lemos Pires from 1974–75. Following the end of Indonesian occupation in 1999, and a United Nations administered transition period, East Timor became formally independent in 2002.
The first Timorese currency was the Portuguese Timor pataca (introduced 1894), and after 1959 the Portuguese Timor escudo, linked to the Portuguese escudo, was used. In 1975 the currency ceased to exist as East Timor was annexed by Indonesia and began using the Indonesian rupiah.

</doc>
<doc id="45556" url="http://en.wikipedia.org/wiki?curid=45556" title="United States Postmaster General">
United States Postmaster General

The United States Postmaster General is the chief executive officer of the United States Postal Service. The office, in one form or another, is older than both the United States Constitution and the United States Declaration of Independence. Benjamin Franklin was appointed by the Continental Congress as the first Postmaster General in 1775, serving slightly longer than 15 months.
Until 1971, the postmaster general was the head of the Post Office Department (or simply "Post Office" until the 1820s). From 1829 to 1971, he was a member of the President's Cabinet.
The Cabinet post of Postmaster General was often given to a new President's campaign manager or other key political supporter, and was considered something of a sinecure. The Postmaster General was in charge of the governing party's patronage, and was a powerful position which held much influence within the party.
In 1971, the Post Office Department was re-organized into the United States Postal Service, an independent agency of the executive branch. Thus, the Postmaster General is no longer a member of the Cabinet and is no longer in Presidential succession.
During the American Civil War, the Confederate States of America Post-office Department provided mail service for the Confederate States, headed by a Postmaster General, John Henninger Reagan.
The Postmaster General is second-highest paid U.S. government official, based on publicly available salary information, after the President of the United States. 
Living former Postmasters General.
s of October 2014[ [update]], there are six living former Postmasters General, the oldest being W. Marvin Watson (1968-1969, born 1924). The most recent Postmaster General to pass away was Preston Robert Tisch (1986–1988), on November 15, 2005.

</doc>
<doc id="45557" url="http://en.wikipedia.org/wiki?curid=45557" title="Sartène">
Sartène

Sartène (] ; Corsican: "Sartè", Italian: "Sartena" ), is a commune in the Corse-du-Sud department of France on the island of Corsica.
Its history dates back to medieval times and granite buildings from the early 16th century still line some of the streets. One of the main incidents in the town's history was an attack by pirates from Algiers in 1583, after which 400 people were taken away. These attacks continued into the 18th century.
The town is centred on the Place de la Liberation (previously the Place Porta), at the edge of which is the church of Sainte Marie. The town allows good views across the valley. Sartene wine is appreciated by wine connoisseurs for its good quality.
Sartene has given its name to one of the southern Corsican dialects that are most like the sardinian Gallurese dialect.
Sights.
Genoese towers in the commune of Sartène:
There are numerous archaeological sites in the commune or Sartène:

</doc>
<doc id="45558" url="http://en.wikipedia.org/wiki?curid=45558" title="United States Secretary of Housing and Urban Development">
United States Secretary of Housing and Urban Development

The United States Secretary of Housing and Urban Development is the head of the United States Department of Housing and Urban Development, a member of the President's Cabinet, and Twelfth in the Presidential line of succession. The post was created with the formation of the Department of Housing and Urban Development on September 9, 1965, by President Lyndon B. Johnson's signing of the Department of Housing and Urban Development Act () into law. The Department's mission is "to increase homeownership, support community development and increase access to affordable housing free from discrimination."
Robert C. Weaver became the first African American Cabinet member by being appointed to the position. The department was also the first Cabinet department to be headed by an African American woman, Patricia Roberts Harris, in 1977. Henry Cisneros became the first Hispanic HUD Secretary in 1993.
Julian Castro was confirmed by Senate on July 9, 2014 and assumed office on July 28, 2014, succeeding Shaun Donovan who was nominated to be the next Director of the Office of Management and Budget.
Secretaries of Housing and Urban Development.
  Denotes acting Secretary
Living former Secretaries of Housing and Urban Development.
s of October 2014[ [update]], there are eight living former Secretaries of Housing and Urban Development, the oldest being Maurice E. Landrieu (1979-1981, born 1930). The most recent Secretary of Housing and Urban Development to pass away was James T. Lynn (1973-1975), on December 6, 2010. 

</doc>
<doc id="45559" url="http://en.wikipedia.org/wiki?curid=45559" title="United States Secretary of Transportation">
United States Secretary of Transportation

The United States Secretary of Transportation is the head of the United States Department of Transportation, a member of the President's Cabinet, and thirteenth in the Presidential Line of Succession. The post was created with the formation of the Department of Transportation on October 15, 1966, by President Lyndon B. Johnson's signing of the Department of Transportation Act. The Department's mission is "to develop and coordinate policies that will provide an efficient and economical national transportation system, with due regard for need, the environment, and the national defense." The Secretary of Transportation oversees eleven agencies, including the Federal Aviation Administration, the Federal Highway Administration, and the National Highway Traffic Safety Administration. In April 2008, Mary Peters launched the official blog of the Secretary of Transportation called The Fast Lane.
The first Secretary of Transportation was Alan Stephenson Boyd, nominated to the post by Democratic President Lyndon B. Johnson. Ronald Reagan's second Secretary of Transportation, Elizabeth Dole, was the first female holder, and Mary Peters was the second. Gerald Ford's nominee William Thaddeus Coleman, Jr. was the first African American to serve as Transportation Secretary, and Federico Peña, serving under Bill Clinton, was the first Hispanic to hold the position, subsequently becoming Secretary of Energy. Japanese American Norman Mineta, who had previously been Secretary of Commerce, is the longest-serving Secretary, holding the post for over five and a half years, and Andrew Card is the shortest-serving Secretary, serving only eleven months. Neil Goldschmidt was the youngest secretary, taking office at age thirty-nine, while Norman Mineta was the oldest, retiring at age seventy-four. On January 23, 2009, the sixteenth secretary Ray LaHood took office, serving under the administration of Democrat Barack Obama; he had previously been a Republican Congressman from Illinois for fourteen years. The salary of the Secretary of Transportation is $199,700.
Anthony Foxx, the Mayor of Charlotte, North Carolina was nominated by President Barack Obama on April 29, 2013, to succeed Ray LaHood. On June 27, 2013 the Senate confirmed his appointment by a vote of 100-0.
Line of succession.
The line of succession regarding who would act as Secretary of Transportation in the event of a vacancy or incapacitation is as follows:
Living former Secretaries of Transportation.
s of October 2014[ [update]], there are thirteen living former Secretaries of Transportation, the oldest being William T. Coleman, Jr. (1975-1977, born 1920). The most recent Secretary of Transportation to pass away was Claude Brinegar (1973-1975), on March 13, 2009. 
References.
The order of succession of the U.S. Department of Transportation is as follows <http://edocket.access.gpo.gov/2008/pdf/E8-5543.pdf>:

</doc>
<doc id="45561" url="http://en.wikipedia.org/wiki?curid=45561" title="United States Secretary of Energy">
United States Secretary of Energy

The United States Secretary of Energy is the head of the U.S. Department of Energy, a member of the U.S. President's Cabinet, and Fourteenth in the presidential line of succession. The position was formed on October 1, 1977 with the creation of the Department of Energy when President Jimmy Carter signed the Department of Energy Organization Act. Originally the post focused on energy production and regulation. The emphasis soon shifted to developing technology for better, more efficient energy sources as well as energy education. After the end of the Cold War, the department's attention also turned toward radioactive waste disposal and maintenance of environmental quality.
Former Secretary of Defense James Schlesinger was the first Secretary of Energy, who was a Republican nominated to the post by Democratic President Jimmy Carter, the only time a president has appointed someone of another party to the post. Schlesinger is also the only secretary to be dismissed from the post. Hazel O'Leary, Bill Clinton's first Secretary of Energy, was first female and African-American holder. The first Hispanic to serve as Energy Secretary was Clinton's second, Federico Peña. Steven Chu became the first Asian American to hold the position on January 20, 2009, serving under the administration of Barack Obama. He is also the first and only Nobel Prize winner to be a Cabinet secretary and the longest-serving Secretary of Energy.
On February 1, 2013, Chu announced his resignation, stating that he will continue to serve until after the ARPA-E Summit at the end of February and possibly until a new secretary is appointed. Following Chu's resignation, Ernest Moniz was nominated and confirmed as Secretary of Energy, taking office on May 16, 2013.
Living former Secretaries of Energy.
s of December 2014[ [update]], there are nine living former Secretaries of Energy, the oldest being Charles Duncan, Jr. (1979-1981, born 1926). The most recent Secretary of Energy to die was James B. Edwards (1981–1982), on December 26, 2014. 

</doc>
<doc id="45562" url="http://en.wikipedia.org/wiki?curid=45562" title="Cartagena Protocol on Biosafety">
Cartagena Protocol on Biosafety

The Cartagena Protocol on Biosafety to the Convention on Biological Diversity is an international agreement on biosafety, as a supplement to the Convention on Biological Diversity. The Biosafety Protocol seeks to protect biological diversity from the potential risks posed by genetically modified organisms resulting from modern biotechnology.
The Biosafety Protocol makes clear that products from new technologies must be based on the precautionary principle and allow developing nations to balance public health against economic benefits. It will for example let countries ban imports of a genetically modified organisms if they feel there is not enough scientific evidence that the product is safe and requires exporters to label shipments containing genetically altered commodities such as corn or cotton.
The required number of 50 instruments of ratification/accession/approval/acceptance by countries was reached in May 2003. In accordance with the provisions of its Article 37, the Protocol entered into force on 11 September 2003. As of March 2015, the Protocol has 170 parties, which includes 167 United Nations member states, Niue, the State of Palestine, and the European Union.
Objective.
In accordance with the precautionary approach, contained in Principle 15 of the Rio Declaration on Environment and Development, the objective of the Protocol is to contribute to ensuring an adequate level of protection in the field of the safe transfer, handling and use of 'living modified organisms resulting from modern biotechnology' that may have adverse effects on the conservation and sustainable use of biological diversity, taking also into account risks to human health, and specifically focusing on transboundary movements (Article 1 of the Protocol, SCBD 2000).
Living modified organisms (LMOs).
The protocol defines a 'living modified organism' as any living organism that possesses a novel combination of genetic material obtained through the use of modern biotechnology, and 'living organism' means any biological entity capable of transferring or replicating genetic material, including sterile organisms, viruses and viroids. 'Modern biotechnology' is defined in the Protocol to mean the application of in vitro nucleic acid techniques, or fusion of cells beyond the taxonomic family, that overcome natural physiological reproductive or recombination barriers and are not techniques used in traditional breeding and selection. 'Living modified organism (LMO) Products' are defined as processed material that are of living modified organism origin, containing detectable novel combinations of replicable genetic material obtained through the use of modern biotechnology (for instance, flour from GM maize). 'Living modified organism intended for direct use as food or feed, or for processing (LMO-FFP)' are agricultural commodities from GM crops. Overall the term 'living modified organisms' is equivalent to genetically modified organism – the Protocol did not make any distinction between these terms and did not use the term 'genetically modified organism.'
Precautionary approach.
One of the outcomes of the United Nations Conference on Environment and Development (also known as the Earth Summit) held in Rio de Janeiro, Brazil, in June 1992, was the adoption of the Rio Declaration on Environment and Development, which contains 27 principles to underpin sustainable development. Commonly known as the precautionary principle, Principle 15 states that "In order to protect the environment, the precautionary approach shall be widely applied by States according to their capabilities. Where there are threats of serious or irreversible damage, lack of full scientific certainty shall not be used as a reason for postponing cost-effective measures to prevent environmental degradation."
Elements of the precautionary approach are reflected in a number of the provisions of the Protocol, such as:
Application.
The Protocol applies to the transboundary movement, transit, handling and use of all living modified organisms that may have adverse effects on the conservation and sustainable use of biological diversity, taking also into account risks to human health (Article 4 of the Protocol, SCBD 2000).
Parties and non-parties.
The governing body of the Protocol is called the Conference of the Parties to the Convention serving as the meeting of the Parties to the Protocol (also the COP-MOP). The main function of this body is to review the implementation of the Protocol and make decisions necessary to promote its effective operation. Decisions under the Protocol can only be taken by Parties to the Protocol. Parties to the Convention that are not Parties to the Protocol may only participate as observers in the proceedings of meetings of the COP-MOP.
The Protocol addresses the obligations of Parties in relation to the transboundary movements of LMOs to and from non-Parties to the Protocol. The transboundary movements between Parties and non-Parties must be carried out in a manner that is consistent with the objective of the Protocol. Parties are required to encourage non-Parties to adhere to the Protocol and to contribute information to the Biosafety Clearing-House.
Relationship with the WTO.
A number of agreements under the World Trade Organization (WTO), such as the Agreement on the Application of Sanitary and Phytosanitary Measures (SPS Agreement) and the Agreement on Technical Barriers to Trade (TBT Agreement), and the Agreement on Trade-Related Aspects of Intellectual Property Rights (TRIPs), contain provisions that are relevant to the Protocol. The Protocol states in its preamble that parties:
Main features.
Overview of features.
The Protocol promotes biosafety by establishing rules and procedures for the safe transfer, handling, and use of LMOs, with specific focus on transboundary movements of LMOs. It features a set of procedures including one for LMOs that are to be intentionally introduced into the environment called the advance informed agreement procedure, and one for LMOs that are intended to be used directly as food or feed or for processing. Parties to the Protocol must ensure that LMOs are handled, packaged and transported under conditions of safety. Furthermore, the shipment of LMOs subject to transboundary movement must be accompanied by appropriate documentation specifying, among other things, identity of LMOs and contact point for further information. These procedures and requirements are designed to provide importing Parties with the necessary information needed for making informed decisions about whether or not to accept LMO imports and for handling them in a safe manner.
The Party of import makes its decisions in accordance with scientifically sound risk assessments. The Protocol sets out principles and methodologies on how to conduct a risk assessment. In case of insufficient relevant scientific information and knowledge, the Party of import may use precaution in making their decisions on import. Parties may also take into account, consistent with their international obligations, socio-economic considerations in reaching decisions on import of LMOs.
Parties must also adopt measures for managing any risks identified by the risk assessment, and they must take necessary steps in the event of accidental release of LMOs.
To facilitate its implementation, the Protocol establishes a Biosafety Clearing-House for Parties to exchange information, and contains a number of important provisions, including capacity-building, a financial mechanism, compliance procedures, and requirements for public awareness and participation.
Procedures for moving LMOs across borders.
Advance Informed Agreement.
The "Advance Informed Agreement" (AIA) procedure applies to the first intentional transboundary movement of LMOs for intentional introduction into the environment of the Party of import. It includes four components: notification by the Party of export or the exporter, acknowledgment of receipt of notification by the Party of import, the decision procedure, and opportunity for review of decisions. The purpose of this procedure is to ensure that importing countries have both the opportunity and the capacity to assess risks that may be associated with the LMO before agreeing to its import. The Party of import must indicate the reasons on which its decisions are based (unless consent is unconditional). A Party of import may, at any time, in light of new scientific information, review and change a decision. A Party of export or a notifier may also request the Party of import to review its decisions.
However, the Protocol's AIA procedure does not apply to certain categories of LMOs:
While the Protocol's AIA procedure does not apply to certain categories of LMOs, Parties have the right to regulate the importation on the basis of domestic legislation. There are also allowances in the Protocol to declare certain LMOs exempt from application of the AIA procedure.
LMOs intended for food or feed, or for processing.
LMOs intended for direct use as food or feed, or processing (LMOs-FFP) represent a large category of agricultural commodities. The Protocol, instead of using the AIA procedure, establishes a more simplified procedure for the transboundary movement of LMOs-FFP. Under this procedure, A Party must inform other Parties through the Biosafety Clearing-House, within 15 days, of its decision regarding domestic use of LMOs that may be subject to transboundary movement.
Decisions by the Party of import on whether or not to accept the import of LMOs-FFP are taken under its domestic regulatory framework that is consistent with the objective of the Protocol. A developing country Party or a Party with an economy in transition may, in the absence of a domestic regulatory framework, declare through the Biosafety Clearing-House that its decisions on the first import of LMOs-FFP will be taken in accordance with risk assessment as set out in the Protocol and time frame for decision-making.
Handling, transport, packaging and identification.
The Protocol provides for practical requirements that are deemed to contribute to the safe movement of LMOs. Parties are required to take measures for the safe handling, packaging and transportation of LMOs that are subject to transboundary movement. The Protocol specifies requirements on identification by setting out what information must be provided in documentation that should accompany transboundary shipments of LMOs. It also leaves room for possible future development of standards for handling, packaging, transport and identification of LMOs by the meeting of the Parties to the Protocol.
Each Party is required to take measures ensuring that LMOs subject to intentional transboundary movement are accompanied by documentation identifying the LMOs and providing contact details of persons responsible for such movement. The details of these requirements vary according to the intended use of the LMOs, and, in the case of LMOs for food, feed or for processing, they should be further addressed by the governing body of the Protocol. (Article 18 of the Protocol, SCBD 2000).
The first meeting of the Parties adopted decisions outlining identification requirements for different categories of LMOs (Decision BS-I/6, SCBD 2004). However, the second meeting of the Parties failed to reach agreement on the detailed requirements to identify LMOs intended for direct use as food, feed or for processing and will need to reconsider this issue at its third meeting in March 2006.
Biosafety Clearing-House.
The Protocol established a Biosafety Clearing-House (BCH), in order to facilitate the exchange of scientific, technical, environmental and legal information on, and experience with, living modified organisms; and to assist Parties to implement the Protocol (Article 20 of the Protocol, SCBD 2000). It was established in a phased manner, and the first meeting of the Parties approved the transition from the pilot phase to the fully operational phase, and adopted modalities for its operations (Decision BS-I/3, SCBD 2004).

</doc>
<doc id="45564" url="http://en.wikipedia.org/wiki?curid=45564" title="United States Secretary of Veterans Affairs">
United States Secretary of Veterans Affairs

The United States Secretary of Veterans Affairs is the head of the U.S. Department of Veterans Affairs, the department concerned with veterans' benefits, health care, and national veterans' memorials and cemeteries. The Secretary is a member of the Cabinet and second to last at Sixteenth in the line of succession to the presidency (the position was last until the addition of the United States Department of Homeland Security in 2006). To date, all appointees and acting appointees to the post have been United States military veterans, but that is not a requirement to fill the position.
When the post of Secretary is vacant, the United States Deputy Secretary of Veterans Affairs or any other person designated by the President serves as Acting Secretary until the President nominates and the United States Senate confirms a new Secretary.
On December 8, 2008, U.S. President Barack Obama announced he would nominate retired U.S. Army general, Eric Shinseki, to be the 7th Secretary of Veterans Affairs. He was unanimously confirmed by the United States Senate on January 20, 2009. General Shinseki resigned as Secretary of Veterans Affairs on May 30, 2014, making deputy secretary Sloan Gibson the acting secretary. On June 29, 2014, President Obama nominated former Procter & Gamble CEO Robert A. McDonald to serve as VA secretary. The United States Senate confirmed McDonald on July 29, 2014.
List of Secretaries of Veterans Affairs.
      No party
  Denotes acting Secretary
1 Anthony Principi served as acting secretary in his capacity as Deputy Secretary of Veterans Affairs September 26, 1992—January 20, 1993.
2 Hershel W. Gober served as acting secretary in his capacity as Deputy Secretary of Veterans Affairs July 1, 1997—January 2, 1998 and July 25, 2000—January 20, 2001.
3 West served as acting Secretary from January 2, 1998 to May 5, 1998.
4 Gordon H. Mansfield served as acting secretary in his capacity as Deputy Secretary of Veterans Affairs October 1—December 20, 2007.
Living former Secretaries of Veterans Affairs.
s of October 2014[ [update]], there are five living former Secretaries of Veterans Affairs, the oldest being Jim Nicholson (2005-2007, born 1938). The most recent Secretary of Veterans Affairs to pass away was Ed Derwinski (1989-1992), on January 15, 2012. 

</doc>
<doc id="45567" url="http://en.wikipedia.org/wiki?curid=45567" title="Portuguese Mozambique">
Portuguese Mozambique

 |style="width:1.0em; padding:0 0 0 0.6em;"| - 
 |style="padding-left:0;text-align:left;"| 1498–1521
 |- class="mergedrow"
 |style="width:1.0em; padding:0 0 0 0.6em;"| - ||style="padding-left:0;text-align:left;"|1609–1611 (first)|| 
 |- class="mergedbottomrow"
 | style="width:1.0em; padding:0 0 0 0.6em;"|  -  ||style="padding-left:0;text-align:left;"| 1967 
 |  km² ( sq mi)
 |- class="mergedbottomrow"
 |- class="mergedbottomrow"
 |- class="mergedbottomrow"
 |- class="mergedbottomrow"
 |style="padding-left:0;text-align:left;"| 1967 est.
 |- class="mergedbottomrow"
 |colspan="2"| Density
 |style="white-space:nowrap;"| /km²  ( /sq mi)
Portuguese Mozambique or Portuguese East Africa are the common terms by which 'Mozambique' is designated when referring to the historic period when it was a Portuguese overseas territory. Former Portuguese Mozambique constituted a string of Portuguese colonies and later a single Portuguese overseas province along the south-east African coast, which now form the Republic of Mozambique. 
During its history, Portuguese Mozambique had the following formal designations: Captaincy of Sofala (1501-1569), Captaincy of Mozambique and Sofala (1570-1676), Captaincy-General of Mozambique and Rivers of Sofala (1676-1836), Province of Mozambique (1836-1926), Colony of Mozambique (1926-1951), Province of Mozambique (1951-1972) and State of Mozambique (1972-1975). 
Portuguese trading settlements and, later, colonies were formed along the coast from 1498, when Vasco da Gama first reached the Mozambican coast. Lourenço Marques explored the area that is now Maputo Bay in 1544. He settled permanently in present-day Mozambique, where he spent most of his life, and his work was followed by other Portuguese explorers, sailors and traders. Some of these colonies were handed over in the late 19th century for rule by chartered companies such as the "Companhia de Moçambique" and the "Companhia do Niassa". In 1951 the colonies were combined into a single overseas province under the name Moçambique as an integral part of Portugal. Most of the original colonies have given their names to the modern provinces of Mozambique.
Mozambique, according to official policy, was not a colony at all but rather a part of the "pluricontinental and multiracial nation" of Portugal. Portugal sought in Mozambique, as it did in all its colonies, to Europeanise the local population and assimilate them into Portuguese culture. Lisbon also wanted to retain the colonies as trading partners and markets for its goods. African inhabitants of the colony were ultimately supposed to become full citizens with full political rights through a long development process. To that end, segregation in Mozambique was minimal compared to that in neighbouring South Africa. However, paid forced labour, to which all Africans were liable if they failed to pay head taxes, was not abolished until the early 1960s.
Overview.
Until the 20th century the land and peoples of Mozambique were barely affected by the Europeans who came to its shores and entered its major rivers. As the Muslim traders, mostly Swahili, were displaced from their coastal centres and routes to the interior by the Portuguese, migrations of Bantu peoples continued and tribal federations formed and reformed as the relative power of local chiefs changed. For four centuries the Portuguese presence was meagre. Coastal and river trading posts were built, abandoned, and built again. Governors sought personal profits to take back to Portugal, and colonists were not attracted to the distant area with its relatively unattractive climate; those who stayed were traders who married local women and successfully maintained relations with local chiefs.
In Portugal, however, Mozambique was considered to be a vital part of a world empire. Periodic recognition of the relative insignificance of the revenues it could produce was tempered by the mystique which developed regarding the mission of the Portuguese to bring their civilization to the African territory. It was believed that through missionary activity and other direct contact between Africans and Europeans, the Africans could be taught to appreciate and participate in Portuguese culture.
In the last decade of the 19th century and the first part of the 20th century, integration of Mozambique into the structure of the Portuguese nation was begun. After all of the area of the present province had been recognized by other European powers as belonging to Portugal, pacification of the tribes of the interior was completed and the traditional holders of political power were subordinated to the Portuguese. Civil administration was established throughout the area, the building of an infrastructure was begun, and agreements regarding the transit trade of Mozambique's land-locked neighbours to the west were made.
Portugal never officially had a racist policy or sanctioned discrimination based on race. Its concept of what it called a "multiracial society" envisaged complete racial integration, including intermarriage, as well as cultural adaptation. The historically determined position of the Portuguese as conquerors and governors of the Africans, however, resulted in barriers to the formation of this ideal. The fact that most Africans were not "cultivated" in the Portuguese sense, and that many participated in what were considered by the Portuguese to be pagan beliefs and uncivilized behaviour, tended to create a low opinion of Africans as a group. The uneducated Portuguese immigrant peasants in urban areas were frequently in direct competition with Africans for jobs and demonstrated jealousies and prejudices with racial overtones.
The society was divided into two peripherally interrelated sectors. The urban-based modern sector, comprising altogether between 2 and 2.5 percent of the population, consisting mostly of Europeans but including a few thousand Europeanised Africans, Indians, and Chinese, was dominant in the economic, political, and social realms. Communication between this sector and the large majority of rural Africans was limited; only a small proportion of the Africans could speak Portuguese, the language of the administration and the modern economic sector. Communication between members of the ten different major ethnolinguistic groups was also difficult.
Economically and socially, all but a few educated and Europeanised Africans were at a disadvantage vis-à-vis the Europeans. Access to education above the primary level was limited by lack of means, by age limitations, or by lack of sufficient preparations. Access to economic opportunity was limited by lack of adequate training.
Between the modern urban and traditional rural sectors of the society was a steadily increasing group of Africans who were loosening their ties with the village and starting to participate in the money economy, to settle in suburbs, and to adopt new customs. This transitional group included individuals who had acquired a modicum of education or skills and some of the aspirations associated with a modern European way of life. Many of them, especially those who had an education beyond the primary level, were more alert politically than the majority of the population, who are either unaware of or uninterested in political issues. It was members of this group, allied with forward-looking European leaders and intellectuals, who had shown the greatest interest in reforms and benefits for the African population. Some among them left the country to become active participants in the independence movement.
History.
When Portuguese explorers reached East Africa in 1498, Swahili commercial settlements had existed along the coast and outlying islands for several centuries. From about 1500, Portuguese trading posts and forts became regular ports of call on the new route to the east.
The voyage of Vasco da Gama around the Cape of Good Hope into the Indian Ocean in 1498 marked the Portuguese entry into trade, politics, and society in the Indian Ocean world. The Portuguese gained control of the Island of Mozambique and the port city of Sofala in the early 16th century. Vasco da Gama having visited Mombasa in 1498, was then successful in reaching India and this permitted the Portuguese to trade with the Far East directly by sea, thus challenging older trading networks of mixed land and sea routes, such as the spice trade routes that utilized the Persian Gulf, Red Sea and caravans to reach the eastern Mediterranean.
The Republic of Venice had gained control over much of the trade routes between Europe and Asia. After traditional land routes to India had been closed by the Ottoman Turks, Portugal hoped to use the sea route pioneered by da Gama to break the Venetian trading monopoly. Initially, Portuguese rule in East Africa focused mainly on a coastal strip centred in Mombasa. With voyages led by Vasco da Gama, Francisco de Almeida and Afonso de Albuquerque, the Portuguese dominated much of southeast Africa's coast, including Sofala and Kilwa, by 1515. Their main goal was to dominate the trade with India. As the Portuguese settled along the coast, they made their way into the hinterland as "sertanejos" (backwoodsmen). These "sertanejos" lived alongside Swahili traders and even took up service among Shona kings as interpreters and political advisors. One such "sertanejo" managed to travel through almost all the Shona kingdoms, including the Mutapa Empire's (Mwenemutapa) metropolitan district, between 1512 and 1516.
By the 1530s, small groups of Portuguese traders and prospectors penetrated the interior regions seeking gold, where they set up garrisons and trading posts at Sena and Tete on the Zambezi River and tried to gain exclusive control over the gold trade. The Portuguese finally entered into direct relations with the Mwenemutapa in the 1560s.
They recorded a wealth of information about the Mutapa kingdom as well as its predecessor, Great Zimbabwe. According to Swahili traders whose accounts were recorded by the Portuguese historian João de Barros, Great Zimbabwe was an ancient capital city built of stones of marvellous size without the use of mortar. And while the site was not within Mutapa's borders, the Mwenemutapa kept noblemen and some of his wives there.
The Portuguese attempted to legitimate and consolidate their trade and settlement positions through the creation of "prazos" (land grants) tied to Portuguese settlement and administration. While "prazos" were originally developed to be held by Portuguese, through intermarriage they became African Portuguese or African Indian centres defended by large African slave armies known as "Chikunda". Historically, within Mozambique, there was slavery. Human beings were bought and sold by African tribal chiefs, Arab traders, and the Portuguese. Many Mozambican slaves were supplied by tribal chiefs who raided warring tribes and sold their captives to the "prazeiros".
Although Portuguese influence gradually expanded, its power was limited and exercised through individual settlers and officials who were granted extensive autonomy. The Portuguese were able to wrest much of the coastal trade from Arabs between 1500 and 1700, but, with the Arab seizure of Portugal's key foothold at Fort Jesus on Mombasa Island (now in Kenya) in 1698, the pendulum began to swing in the other direction. As a result, investment lagged while Lisbon devoted itself to the more lucrative trade with India and the Far East and to the colonisation of Brazil. During the 18th and 19th centuries, the Mazrui and Omani Arabs reclaimed much of the Indian Ocean trade, forcing the Portuguese to retreat south.
Many "prazos" had declined by the mid-19th century, but several of them survived. During the 19th century other European powers, particularly the British and the French, became increasingly involved in the trade and politics of the region. In the Island of Mozambique, the hospital, a majestic neo-classical building constructed in 1877 by the Portuguese, with a garden decorated with ponds and fountains, was for many years the biggest hospital south of the Sahara. By the early 20th century the Portuguese had shifted the administration of much of Mozambique to large private companies, like the Mozambique Company, the Zambezia Company and the Niassa Company, controlled and financed mostly by the British, which established, with the Portuguese, railroad lines to neighbouring countries. The companies, granted a charter by the Portuguese government to foster economic development and maintain Portuguese control in the territory's provinces, would lose their purpose when the territory was transferred to the control of the Portuguese colonial government between 1929 and 1942.
Although slavery had been legally abolished in Mozambique by the Portuguese authorities, at the end of the 19th century the Chartered companies enacted a forced labor policy and supplied cheap – often forced – African labor to the mines and plantations of the nearby British colonies and South Africa. The Zambezia Company, the most profitable chartered company, took over a number of smaller "prazeiro" holdings, and requested Portuguese military outposts to protect its property. The chartered companies and the Portuguese administration built roads and ports to bring their goods to market including a railroad linking present day Zimbabwe with the Mozambican port of Beira. However, the development's administration gradually started to pass directly from the trading companies to the Portuguese government itself.
Because of their unsatisfactory performance and because of the shift, under the "Estado Novo" regime of Oliveira Salazar, towards a stronger Portuguese control of the Portuguese Empire's economy, the companies' concessions were not renewed when they ran out. This was what happened in 1942 with the Mozambique Company, which however continued to operate in the agricultural and commercial sectors as a corporation, and had already happened in 1929 with the termination of the Niassa Company's concession.
In the 1950s, the Portuguese overseas colony was rebranded an overseas province of Portugal, and by the early 1970s it was officially upgraded to the status of Portuguese non-sovereign state, by which it would remain a Portuguese territory but with a wider administrative autonomy. The Front for the Liberation of Mozambique (FRELIMO), initiated a guerrilla campaign against Portuguese rule in September 1964. This conflict, along with the two others already initiated in the other Portuguese colonies of Angola and Guinea, became part of the so-called Portuguese Colonial War (1961–74). From a military standpoint, the Portuguese regular army held the upper hand during all of the conflicts against the independentist guerrilla forces, which created favourable conditions for social development and economic growth until the end of the conflict in 1974.
After ten years of sporadic warfare and after Portugal's return to democracy through a leftist military coup in Lisbon which replaced Portugal's "Estado Novo" regime in favor of a military junta (the Carnation Revolution of April 1974), FRELIMO took control of the territory. The talks that led to an agreement on Mozambique's independence, signed in Lusaka, were started. Within a year, almost all ethnic Portuguese population had left, many fleeing in fear (in mainland Portugal they were known as "retornados"); others were expelled by the ruling power of the newly independent territory. Mozambique became independent from Portugal on 25 June 1975.
Government.
At least since the early 19th century, the legal status of Mozambique always considered it as much a part of Portugal as Lisbon, but as an overseas province enjoyed special derogations to account for its distance from Europe.
From 1837, the highest government official in the province of Mozambique has always been the governor-general, who reported directly to the Government in Lisbon, usually through the minister of the Overseas. During some periods in the late 19th and the early 20th century, the governors-general of Mozambique received the status of royal commissioners or of high commissioners, which gave them extended executive and legislative powers, equivalent to those of a government minister.
In the 20th century, the province was also subject to the authoritarian "Estado Novo" regime that ruled Portugal from 1933 to 1974, until the military coup at Lisbon, known as the Carnation Revolution. Most members of the government of Mozambique were from Portugal, but a few were Africans. Nearly all members of the bureaucracy were from Portugal, as most Africans did not have the necessary qualifications to obtain positions.
The government of Mozambique, as it was in Portugal, was highly centralized. Power was concentrated in the executive branch, and all elections where they occurred were carried out using indirect methods. From the Prime Minister's office in Lisbon, authority extended down to the most remote posts and "regedorias" of Mozambique through a rigid chain of command. The authority of the government of Mozambique was residual, primarily limited to implementing policies already decided in Europe. In 1967, Mozambique also sent seven delegates to the National Assembly in Lisbon.
The highest official in the province was the governor-general, appointed by the Portuguese cabinet on recommendation of the Overseas Minister. The governor-general had both executive and legislative authority. A Government Council advised the governor-general in the running of the province. The functional cabinet consisted of five secretaries appointed by the Overseas Minister on the advice of the governor. A Legislative Council had limited powers and its main activity was approving the provincial budget. Finally, an Economic and Social Council had to be consulted on all draft legislation, and the governor-general had to justify his decision to Lisbon if he ignored its advice.
Mozambique was divided into nine districts, which were further subdivided into 61 municipalities ("concelhos") and 33 circumscriptions ("circunscrições"). Each subdivision was then made up of three or four individual posts, 166 in all with an average of 40,000 Africans in each. Each district, except Lourenço Marques which was run by the governor-general, was overseen by a governor. Most Africans only had contact with the Portuguese through the post administrator, who was required to visit each village in his domain at least once a year.
The lowest level of administration was the "regedoria", settlements inhabited by Africans living according to customary law. Each "regedoria" was run by a "regulo", an African or Portuguese official chosen on the recommendation of local residents. Under the "regulos", each village had its own African headman.
Each level of government could also have an advisory board or council. They were established in municipalities with more than 500 electors, in smaller municipalities or circumscriptions with more than 300 electors, and in posts with more than 20 electors. Each district also had its own board as well.
Two legal systems were in force — Portuguese civil law and African customary law. As part of its policy of assimilation, the Portuguese sought to break down the African legal system and did not study or codify much of it. Until 1961, Africans were considered to be "indígenas" or natives, rather than citizens. After 1961, the previous native laws were repealed and Africans gained "de facto" Portuguese citizenship. From then on, the status of Africans depended merely on whether or not they chose to be governed by civil law, and the number of Africans that made the choice was very small.
Geography.
Portuguese East Africa was located in south-eastern Africa. It was a long coastal strip with Portuguese strongholds, from current day Tanzania and Kenya, to the south of current-day Mozambique.
In 1900, the part of modern Mozambique northwest of the Zambezi and Shire Rivers was called Moçambique; the rest of it was Lourenço Marques. Various districts existed, and even issued stamps, during the first part of the century, including Inhambane, Lourenço Marques, Mozambique Colony, Mozambique Company, Nyassa Company, Quelimane, Tete, and Zambésia. The Nyassa Company territory is now Cabo Delgado and Niassa.
In the early- and mid-20th century, a number of changes occurred. Firstly, on 28 June 1919, the Treaty of Versailles transferred the Kionga Triangle, a 1000 km2 territory south of the Rovuma River from German East Africa to Mozambique.
During World War II, the Charter of the Mozambique Company expired, on 19 July 1942; its territory, known as Manica and Sofala, became a district of Mozambique. Mozambique was constituted as four districts on 1 January 1943 — Manica and Sofala, Niassa, Sul do Save (South of the Save River), and Zambézia.
On 20 October 1954, administrative reorganization caused Cabo Delgado and Mozambique districts to be split from Niassa. At the same time, the Sul do Save district was divided into Gaza, Inhambane and Lourenço Marques, while the Tete district was split from Manica and Sofala.
By the early 1970s, Mozambique was bordering the Mozambique Channel, bordering the countries of Malawi, Rhodesia, South Africa, Swaziland, Tanzania, and Zambia. Covering a total area of 801590 km2. With a tropical to subtropical climate, the Zambezi flows through the north-central and most fertile part of the country. Its coastline had 2470 km, with 4571 km of land boundaries, its highest point at Monte Binga (2436 m). The Gorongosa National Park, founded in 1920, was the main natural park in the territory.
The districts with its respective capitals were:
Other important urban centres included Sofala, Nacala, António Enes, Island of Mozambique and Vila Junqueiro.
Demographics.
By 1970, the Portuguese Overseas Province of Mozambique had about 8,168,933 inhabitants. Nearly 300,000 were white ethnic Portuguese. There was a number of mulattoes, from both European and African ancestry, living across the territory. However, the majority of the population belonged to local tribal groups which included the Makua–Lomwe, the Shona and the Tsonga. Other ethnic minorities included British, Greeks, Chinese and Indians. Most inhabitants were black indigenous Africans with a diversity of ethnic and cultural backgrounds, ranging from Shangaan and Makonde to Yao or Shona peoples. The Makua were the largest ethnic group in the north. The Sena and Shona (mostly Ndau) were prominent in the Zambezi valley, and the Shangaan (Tsonga) dominated in the south. In addition, several other minority groups lived a tribal lifestyle across the territory.
Mozambique had around 250,000 Europeans in 1974 that made up around 3% of the population. Mozambique was cosmopolitan as it had around Indian, Chinese, Greek and Anglophone communities living there too (over 25,000 Indians and 5,000 Chinese by the early 1970s). The white population was more influenced from South Africa. The capital of Portuguese Mozambique, Lourenço Marques (Maputo), had a population of 355,000 in 1970 with around 100,000 Europeans. Beira had around 115,000 inhabitants at the time with around 30,000 Europeans. Most of the other cities ranged from 10 to 15% in the number of Europeans, while Portuguese Angola cities had European majorities ranging from 50% to 60%.
Society.
The establishment of a dual, racialized civil society was formally recognized in "Estatuto do Indigenato" (The Statute of Indigenous Populations) adopted in 1929, and was based in the subjective concept of civilization versus tribalism. Portugal's colonial authorities were totally committed to develop a fully multiethnic "civilized" society in its African colonies, but that goal or "civilizing mission", would only be achieved after a period of Europeanization or enculturation of the native black tribes and ethnocultural groups. It was a policy which had already been stimulated in the former Portuguese colony of Brazil and in Portuguese Angola. The "Estatuto" established a distinction between the "colonial citizens", subject to the Portuguese laws and entitled to all citizenship rights and duties effective in the "metropole", and the "indígenas" (natives), subjected to colonial legislation and, in their daily lives, to their customary, tribal native laws. Between the two groups there was a third small group, the "assimilados", comprising native blacks, mulatos, Asians, and mixed-race people, who had at least some formal education, were not subjected to paid forced labor, were entitled to some citizenship rights, and held a special identification card that differed from the one imposed on the immense mass of the African population (the "indígenas"), a card that the colonial authorities conceived of as a means of controlling the movements of forced labor (CEA 1998). The "indígenas" were subject to the traditional authorities, who were gradually integrated into the colonial administration and charged with solving disputes, managing the access to land, and guaranteeing the flows of workforce and the payment of taxes. As several authors have pointed out (Mamdani 1996; Gentili 1999; O'Laughlin 2000), the "Indigenato" regime was the political system that subordinated the immense majority of Mozambicans to local authorities entrusted with governing, in collaboration with the lowest echelon of the colonial administration, the "native" communities described as tribes and assumed to have a common ancestry, language, and culture. The colonial use of traditional law and structures of power was thus an integral part of the process of colonial domination (Young 1994; Penvenne 1995; O'Laughlin 2000) obsessed with the maximization of economic development and growth through the use of idle or unproductive African workforce.
In the 1940s, the integration of traditional authorities into the colonial administration was deepened, a level of social integration, miscegenation and social promotion based in skill and human qualities of each individual, rather than in the ethnic background, which was coined "lusotropicalismo" and had been a major feature of the Portuguese Empire throughout history. The Portuguese colony was divided into "concelhos" (municipalities), in urban areas, governed by colonial and metropolitan legislation, and "circunscrições" (localities), in rural areas. The "circunscrições" were led by a colonial administrator and divided into "regedorias" (subdivisions of circunscrições), headed by "régules" (tribal chieftains), the embodiment of traditional authorities. Provincial Portuguese Decree No. 5.639, of July 29, 1944, attributed to "régulos" and their assistants, the "cabos de terra", the status of "auxiliares da administração" (administrative assistants). Gradually, these "traditional" titles lost some of their content, and the "régulos" and "cabos de terra" came to be viewed as an effective part of the colonial state, remunerated for their participation in the collection of taxes, recruitment of the labor force, and agricultural production in the area under their control. Within the areas of their jurisdiction, the "régulos" and "cabos de terra" also controlled the distribution of land and settled conflicts according to customary norms (Geffray 1990; Alexander 1994; Dinerman 1999). To exercise their power, the "régulos" and "cabos de terra" had their own police force. This system of indirect rule illustrates what the disjunction between political and administrative control. In major urban areas, most notoriously the cosmopolitan provincial ports of Lourenço Marques and Beira, racial integration and socioeconomic opportunities for all kind of skilled citizens were already very deep. It continued after the "Indigenato" system was abolished in the early 1960s after the Portuguese colony of Mozambique has been rebranded the Overseas Province of Mozambique in the 1950s. From then on, all Africans were considered Portuguese citizens, and racial discrimination became a sociological rather than a legal feature of colonial society. The rule of traditional authorities was indeed integrated more than before in the colonial administration.
Ethnic African inhabitants of the Portuguese overseas provinces were ultimately supposed to become full citizens with full political rights through a long development process. To that end, by the 1960s and 1970s, segregation in Mozambique was minimal compared to that in neighbouring South Africa.
Urban centres.
The largest coastal cities, the first founded or settled by Portuguese people since the 16th century, like the capital Lourenço Marques, Beira, Quelimane, Nacala and Inhambane were modern cosmopolitan ports and a melting pot of several cultures, with a strong South African influence. The Southeast African and Portuguese cultures were dominant, but the influence of Arab, Indian, and Chinese cultures were also felt. The cuisine was diverse, owing especially to the Portuguese cuisine and Muslim heritage, and seafood was also quite abundant.
Lourenço Marques had always been a point of interest for artistic and architectural development since the first days of its urban expansion and this strong artistic spirit was responsible for attracting some of the world's most forward architects at the turn of the 20th century. The city was home to masterpieces of building work by, Pancho Guedes, Herbert Baker and Thomas Honney amongst others. The earliest architectural efforts around the city focused on classical European designs such as the Central Train Station (CFM) designed by architects Alfredo Augusto Lisboa de Lima, Mario Veiga and Ferreira da Costa and built between 1913 and 1916 (sometimes mistaken with the work of Gustav Eiffel), and the Hotel Polana designed by Herbert Baker.
As the 1960s and 1970s approached, Lourenço Marques was yet again at the center of a new wave of architectural influences made most popular by Pancho Guedes. The designs of the 1960s and 1970s were characterized by modernist movements of clean, straight and functional structures. However, prominent architects such as Pancho Guedes fused this with local art schemes giving the city's buildings a unique Mozambican theme. As a result most of the properties erected during the second construction boom take on these styling cues.
Economy.
Since the 15th century, Portugal founded settlements, trading posts, forts and ports in the Sub-Saharan Africa's coast. Cities, towns and villages were founded all over East African territories by the Portuguese, especially since the 19th century, like Lourenço Marques, Beira, Vila Pery, Vila Junqueiro, Vila Cabral and Porto Amélia. Others were expanded and developed greatly under Portuguese rule, like Quelimane, Nampula and Sofala. By this time, Mozambique had become a Portuguese colony, but administration was left to the trading companies (like Mozambique Company and Niassa Company) who had received long-term leases from Lisbon. By the mid-1920s, the Portuguese succeeded in creating a highly exploitative and coercive settler economy, in which African natives were forced to work on the fertile lands taken over by Portuguese settlers. Indigenous African peasants mainly produced cash crops designated for sale in the markets of the colonial metropole (the center, i.e. Portugal). Major cash crops included cotton, cashews, tea and rice. This arrangement ended in 1932 after the takeover in Portugal by the new António de Oliveira Salazar's government — the "Estado Novo". Thereafter, Mozambique, along with other Portuguese colonies, was put under the direct control of Lisbon. In 1951, it became an overseas province. The economy expanded rapidly during the 1950s and 1960s, attracting thousands of Portuguese settlers to the country. It was around this time that the first nationalist guerrilla groups began to form in Tanzania and other African countries. The strong industrial and agricultural development that did occur throughout the 1950s, 1960s and early 1970s was based on Portuguese development plans, and also included British and South African investment.
In 1959–60, Mozambique's major exports included cotton, cashew nuts, tea, sugar, copra and sisal. Other major agricultural productions included rice and coconut. The expanding economy of the Portuguese overseas province was fuelled by foreign direct investment, and public investment which included ambitious state-managed development plans. British capital owned two of the large sugar concessions (the third was Portuguese), including the famous Sena states. The Matola Oil Refinery, Procon, was controlled by Britain and the United States. In 1948 the petroleum concession was given to the Mozambique Gulf Oil Company. At Maotize coal was mined; the industry was chiefly financed by Belgian capital. 60% of the capital of the "Compagnie de Charbons de Mozambique" was held by the "Société Minière et Géologique Belge", 30% by the Mozambique Company, and the remaining 10% by the Government of the territory. Three banks were in operation, the Banco Nacional Ultramarino, Portuguese, Barclays Bank, D.C.O., British, and the Banco Totta e Standard de Moçambique (a partnership between Standard Bank of South Africa and mainland's Banco Totta & Açores). Nine out of the twenty-three insurance companies were Portuguese. 80% of life assurance was in the hands of foreign companies which testifies to the openness of the economy.
The Portuguese overseas province of Mozambique was the first territory of Portugal, including the European mainland, to distribute Coca Cola. Lately the Lourenço Marques Oil Refinery was established by the "Sociedade Nacional de Refinação de Petróleo" (SONAREP) — a Franco-Portuguese syndicate. In the sisal plantations Swiss capital was invested, and in copra concerns, a combination of Portuguese, Swiss and French capital was invested. The large availability of capital from both Portuguese and international origin, allied to the wide range of natural resources and the growing urban population, lead to an impressive growth and development of the economy.
From the late stages of this notable period of high growth and huge development effort started in the 1950s, was the construction of Cahora Bassa dam by the Portuguese, which started to fill in December 1974 after construction was commenced in 1969. In 1971 construction work of the Massingir Dam began. At independence, Mozambique's industrial base was well-developed by Sub-Saharan Africa standards, thanks to a boom in investment in the 1960s and early 1970s. Indeed, in 1973, value added in manufacturing was the sixth highest in Sub-Saharan Africa.
Economically, Mozambique was a source of agricultural raw materials and an earner of foreign exchange. It also provided a market for Portuguese manufacturers which were protected from local competition. Transportation facilities had been developed to exploit the transit trade of South Africa, Swaziland, Rhodesia, Malawi, and Zambia, agricultural production for export purposes had been encouraged, and profitable arrangements for the export of labour had been made with neighbouring countries. Industrial production had been relatively insignificant, but did begin to increase in the 1960s. The economic structure generally favoured the taking of profits to Portugal rather than their reinvestment in Mozambique. The Portuguese interests which dominate in banking, industry, and agriculture, exerted a powerful influence on policy.
Education.
Mozambique's rural black populations were largely illiterate, as were a majority of Portugal's peasantry. However, a number of natives from diverse tribal backgrounds were educated in Portuguese language and history by several missionary schools established across the vast countryside areas. In mainland Portugal, the homeland of the colonial authorities which ruled Mozambique from the 16th century until 1975, by the end of the 19th century the illiteracy rates were at over 80 percent and higher education was reserved for a small percentage of the population. 68.1 percent of mainland Portugal's population was still classified as illiterate by the 1930 census. Mainland Portugal's literacy rate by the 1940s and early 1950s was low for North American and Western European standards at the time. Only in the mid-1960s did the country make public education available for all children between the ages of six and twelve, and the overseas territories in Africa profited from this new educational developments and change in policy at Lisbon. Starting in the early 1950s, the access to basic, secondary and technical education was expanded and its availability was being increasingly opened to both the African "indígenes" and the European Portuguese of the African territories. A comprehensive network of secondary schools (the "Liceus") and technical or vocational education schools were implemented across the cities and main towns of the territory. In 1962, the first Mozambican university was founded by the Portuguese authorities in the provincial capital, Lourenço Marques, the "Universidade de Lourenço Marques", awarding a wide range of degrees from engineering to medicine, during a time that in the European Portuguese mainland only four public universities were in operation.
Sports.
The Portuguese-ruled territory was introduced to several popular European and North American sports disciplines since the early urbanistic and economic booms of the 1920s and 1940s. This period was a time of city and town expansion and modernization that included the construction of several sports facilities for football, rink hockey, basketball, volleyball, handball, athletics, gymnastics and swimming. Several sports clubs were founded across the entire territory, among them were some of the largest and oldest sports organizations of Mozambique like Sporting Clube de Lourenço Marques established in 1920. Other major sports clubs were founded in the following years like Grupo Desportivo de Lourenço Marques (1921), Clube Ferroviário de Lourenço Marques (1924), Sport Club de Vila Pery (1928), Clube Ferroviário da Beira (1943), Grupo Desportivo da Companhia Têxtil do Punguè (1943), and Sport Lourenço Marques e Benfica (1955). Several sportsmen, especially football players, that achieved wide notability in Portuguese sports were from Mozambique. Eusébio and Mário Coluna were examples of that, and excelled in the Portugal national football team. Since the 1960s, with the latest developments on commercial aviation, the highest ranked football teams of Mozambique and the other African overseas provinces of Portugal, started to compete in the Taça de Portugal (the Portuguese Cup). There were also several facilities and organizations for golf, tennis and wild hunting.
The nautical sports were also well developed and popular, especially in Lourenço Marques, home to the Clube Naval de Lourenço Marques. The largest stadium was the Estádio Salazar, located near Lourenço Marques. Opened in 1968, it was at the time the most advanced in Mozambique conforming to standards set by both FIFA and the International Cycling Union (UCI). The cycling track could be adjusted to allow for 20,000 more seats.
Beginning in the 1950s, motorsport was introduced to Mozambique. At first race cars would compete in areas around the city, Polana and along the "marginal" but as funding and interest increased, a dedicated race track was built in the Costa Do Sol area along and behind the "marginal" with the ocean to the east with a length of 1.5 km. The initial surface of the new track, named Autódromo de Lourenço Marques did not provide enough grip and an accident in the late 1960s killed 8 people and injured many more. Therefore, in 1970, the track was renovated and the surface changed to meet the highest international safety requirements that were needed at large events with many spectators. The length then increased to 3909 km. The city became host to several international and local events beginning with the inauguration on 26 November 1970.
Carnation Revolution and independence.
As communist and anti-colonial ideologies spread out across Africa, many clandestine political movements were established in support of Mozambique's independence. Regardless of whether it was exaggerated anti-Portuguese / anti-"Colonial" propaganda, a dominant tendency in Mozambique, or a mix of both, these movements claimed that since policies and development plans were primarily designed by the ruling authorities for the benefit of the Mozambican ethnic Portuguese population, little attention was paid to local tribal integration and the development of its native communities. According to the official guerrilla statements, this affected a majority of the indigenous population who suffered both state-sponsored discrimination and enormous social pressure. Many felt they had received too little opportunity or resources to upgrade their skills and improve their economic and social situation to a degree comparable to that of the Europeans. Statistically, Portuguese Mozambique's whites were indeed wealthier and more skilled than the black indigenous majority, but the late 1950s, the 1960s and the early 1970s, were being testimony of a gradual change based in new socio-economic developments and egalitarian policies regarding underprivileged rural black communities.
The Front for the Liberation of Mozambique (FRELIMO), headquartered in Tanzania, initiated a guerrilla campaign against Portuguese rule in September 1964. This conflict, along with the two others already initiated in the other Portuguese overseas territories of Angola and Portuguese Guinea, became part of the Portuguese Colonial War (1961–74). Several African territories under European rule had achieved independence in recent decades. Oliveira Salazar attempted to resist this tide and maintain the integrity of the Portuguese empire. By 1970, the anti-guerrilla war in Africa was consuming an important part of the Portuguese budget and there was no sign of a final solution in sight. This year was marked by a large-scale military operation in northern Mozambique, the Gordian Knot Operation, which displaced the FRELIMO's bases and destroyed much of the guerrillas' military capacity. At a military level, a part of Guinea-Bissau was de facto independent since 1973, but the capital and the major towns were still under Portuguese control. In Angola and Mozambique, independence movements were only active in a few remote countryside areas from where the Portuguese Army had retreated. However, their impending presence and the fact that they wouldn't go away dominated public anxiety. Throughout the war period Portugal faced increasing dissent, arms embargoes and other punitive sanctions imposed by most of the international community. For the Portuguese society the war was becoming even more unpopular due to its length and financial costs, the worsening of diplomatic relations with other United Nations members, and the role it had always played as a factor of perpetuation of the Estado Novo regime. It was this escalation that would led directly to the mutiny of members of the FAP in the Carnation Revolution in 1974 — an event that would led to the independence of the former Portuguese colonies in Africa. A leftist military coup in Lisbon on 24 April 1974 by the "Movimento das Forças Armadas" (MFA), overthrow the Estado Novo regime headed by Prime Minister Marcelo Caetano.
As one of the objectives of the MFA, all the Portuguese overseas territories in Africa were offered independence. FRELIMO took complete control of the Mozambican territory after a transition period, as agreed in the Lusaka Accord which recognized Mozambique's right to independence and the terms of the transfer of power.
Within a year of the Portuguese military coup at Lisbon, almost all Portuguese population had left the African territory as refugees (in mainland Portugal they were known as "retornados") – some expelled by the new ruling power of Mozambique, some fleeing in fear. A parade and a state banquet completed the independence festivities in the capital, which was expected to be renamed Can Phumo, or "Place of Phumo", after a Shangaan chief who lived in the area before the Portuguese navigator Lourenço Marques founded the city in 1545 and gave his name to it. Most city streets, named for Portuguese heroes or important dates in Portuguese history, had their names changed.
Portuguese population's rapid exodus left the Mozambican economy in disarray. In addition, after the independence day on 25 June 1975, the eruption of the Mozambican Civil War (1977–92) destroyed the remaining wealth and left the former Portuguese Overseas Province in a state of absolute disrepair.
References.
Herrick, Allison and others (1969). "Area Handbook for Mozambique", US Government Printing Office.

</doc>
<doc id="45568" url="http://en.wikipedia.org/wiki?curid=45568" title="The Abyss">
The Abyss

The Abyss is a 1989 American science fiction-adventure film written and directed by James Cameron, starring Ed Harris, Mary Elizabeth Mastrantonio, and Michael Biehn. When an American submarine sinks in the Caribbean, the US search and recovery team works with an oil platform crew, racing against Russian vessels to recover the ship. Deep in the ocean, they encounter something unexpected.
Plot.
In 1988 a US ballistic missile submarine, the USS "Montana", sinks near the edge of the Cayman Trough after an accidental encounter with an unidentified submerged object. As Soviet ships and submarines head toward the area in an attempt to salvage the sub, and with a hurricane moving in, the Americans decide that the quickest way to mount a rescue is to insert a SEAL team onto the "Deep Core", a privately owned, experimental underwater oil drilling platform, located 1,700 feet (518 meters) below sea level, which will serve as their base of operations. The designer of the platform, Dr. Lindsey Brigman, insists on accompanying the SEAL team, even though her estranged husband, Virgil "Bud" Brigman, is currently serving as the platform's foreman.
As the SEALs and the platform crew attempt to discover the cause of the "Montana"‍ '​s failure, they spot strange creatures they cannot identify, only later discovering that the creatures have intelligence and dubbing them "NTIs"—"non-terrestrial intelligence". On orders from the SEAL leader Lt. Hiram Coffey and without the platform crew's knowledge, the SEALs use one of the platform's mini-subs to retrieve a warhead from a Trident missile aboard the "Montana". However, they do so at an inopportune time, as the hurricane strikes the surface and they are unable to release the tether from the rig's surface support ship, the "Benthic Explorer". Tossed by the storm, the "Explorer"‍ '​s entire crane and cable system break off and fall into the water. The crane barely misses the platform when it hits the ocean floor, but falls into the trench, its weight pulling the tether and the whole platform toward the drop-off. The rig hangs up on the very edge of the cliff, preventing a plummet into the depths. Several crew members are lost due to flooding in the platform, while the surviving crew and SEALs tend to wounds and attempt to restore the platform's critical power.
An NTI probe in the form of a living column of water explores the platform, and while the platform crew believes it to be harmless, Coffey sees it as a threat. The platform crew realizes Coffey is suffering from high-pressure nervous syndrome, which is making him paranoid. Using one of the remote operated vehicles to spy on Coffey from outside the platform, they discover he is planning on sending the warhead down into the chasm to destroy whatever may be down there. Bud attempts to subdue Coffey before he can leave the platform in one of the mini-subs, but he is unable to do so. Bud and Lindsey chase Coffey in the station's other sub; they manage to damage Coffey's sub, causing it to fall into the trench, where Coffey is killed when the pressure crushes the vehicle. However, Bud and Lindsey are too late to stop the remote vehicle and its attached warhead, on a pre-programmed course and set to explode within 3 hours, from dropping into the trench. Furthermore, their sub is flooding due to a rupture in the hull. Lindsey realizes that the sub's crippled systems, the distance between the sub and the platform, and the fact that their sole source of oxygen is a backpack and regulator that are hard-mounted to Bud's diving helmet combine to leave just one solution. After being convinced, Bud locks his helmet onto his diving suit, watches Lindsey drown, and then tows her body back to Deep Core, hoping that the cold water shocked her body into deep hibernation. The Deep Core crew, trained and equipped for medical emergencies, is able to restart Lindsey's heart via CPR and a defibrillator. The two reaffirm their lost love.
The crew tracks the warhead, finding the remote vehicle has failed from the pressure and landed on a ledge partway down the trench. The SEALs have brought along special diving equipment featuring a liquid breathing apparatus that would allow for a human to dive that far. However, only one of the two surviving SEALs is trustworthy and his injuries prevent him from using it. Bud volunteers; he will not be able to talk and is instead forced to communicate through a keypad on his suit. Bud begins his 7 km dive into the trench, reaches the ledge where the warhead sits, and is guided by the SEAL in disarming it. However, the dive has taken too long for Bud to return to the top of the trench before the oxygen in the liquid runs out. Bud, aware this could happen, writes that he has only five minutes left, and despite Lindsey's pleas to return, decides to remain on the ledge. He types his love to Lindsey in a final message, saying, "Knew this was a one-way ticket, but you know I had to come. Love you, wife."
As Bud lies on the ledge awaiting his death, bright lights appear below him and he encounters an aquatic NTI. The being reaches out and takes Bud's hand and then leads him even farther down to a massive NTI spacecraft sitting 8 km deep in the trench. Deep within the ship, the NTIs provide Bud with an atmosphere that allows him to breathe. The NTIs replay Bud's message to Lindsey for him, and they exchange meaningful looks.
On the platform, believing Bud to be dead, Lindsey and the crew are surprised to find Bud radioing back to them, telling them to get ready. The crew observes something very large quickly rising out of the trench, and sees the lights from the NTI spacecraft as it rises. The enormous ship eventually surfaces, lifting many of the naval ships out of the water and leaving them aground on the NTI ship's hull, as well as the platform itself. Leaving the platform on the surface of the ship, the platform crew and remaining SEALS are surprised to find that they are fine and not suffering from decompression sickness after rising so fast out of the water, and credit the NTIs. Bud emerges from the NTI ship, and he and Lindsey rush to meet each other; engaging in a passionate kiss.
Special Edition.
The special edition includes many more character scenes, interactions and conflicts. Bud and Lindsey are seen bickering after she arrives on the platform, which builds up to the scene in the theatrical version where Bud throws his wedding ring down the chemical toilet. Another sequence sees the crew assessing personnel losses and damage to the platform after it is dragged by the fallen crane to the drop-off. There is also more emphasis placed on the conflict between the United States and Soviet forces over the crash of the "Montana", with each side initially blaming the other for the disaster. When Bud arrives on the NTI ship, he is shown images of humanity's destructive behavior on a view screen, such as the Bay Lop execution. The NTIs create enormous megatsunami-level waves that threaten every coastline, including those of New York City (shown by the Statue of Liberty and the Verrazano Narrows Bridge) and San Francisco (shown by the Golden Gate Bridge), but then stall them moments before they would come crashing down. After showing Bud his messages of self-sacrifice and caring and believing all humanity to be capable of the same, the NTIs cause the standing waves to harmlessly recede back to normal ocean levels. The message is that it is time for humanity to end its self-destructive ways and unite. After Bud relays this through his keyboard, the NTIs start to bring their ship to the surface.
Production.
H. G. Wells was the first to introduce the notion of a sea alien in his 1897 short story "In the Abyss". The idea for "The Abyss" came to James Cameron when, at age 17 and in high school, he attended a science lecture about deep sea diving by a man, Francis J. Falejczyk, who was the first human to breathe fluid through his lungs in experiments conducted by Dr. Johannes A. Kylstra. He subsequently wrote a short story that focused on a group of scientists in a laboratory at the bottom of the ocean. The basic idea did not change, but many of the details evolved over the years. Once Cameron arrived in Hollywood, he quickly realized that a group of scientists was not that commercial and changed it to a group of blue-collar workers. While making "Aliens", Cameron saw a "National Geographic" film about remote operated vehicles operating deep in the North Atlantic Ocean. These images reminded him of his short story. He and producer Gale Anne Hurd decided that "The Abyss" would be their next film. Cameron wrote a treatment combined with elements of a shooting script, which generated a lot of interest in Hollywood. He then wrote the script, basing the character of Lindsey on Hurd and finished it by the end of 1987. Cameron and Hurd were married before "The Abyss", separated during pre-production, and divorced in February 1989, two months after principal photography.
Pre-production.
The cast and crew trained for underwater diving for one week in the Cayman Islands. This was necessary because 40% of all live-action principal photography took place underwater. Furthermore, Cameron's production company had to design and build experimental equipment and develop a state-of-the-art communications system that allowed the director to talk underwater to the actors and dialogue to be recorded directly onto tape for the first time.
Cameron had originally planned to shoot on location in the Bahamas where the story was set but quickly realized that he needed to have a completely controlled environment because of the stunts and special visual effects involved. He considered shooting the film in Malta, which had the largest unfiltered tank of water, but it was not adequate for Cameron's needs. Underwater sequences for the film were shot at a unit of the Gaffney Studios, situated outside Gaffney, South Carolina, which had been abandoned by Duke Power officials after previously spending $700 million constructing the Cherokee Nuclear Power Plant.
Two specially constructed tanks were used. The first one held 7.5 e6USgal of water, was 55 feet (18 m) deep and 209 feet (70 m) across. At the time, it was the largest fresh-water filtered tank in the world. Additional scenes were shot in the second tank, which held 2.5 e6USgal of water. As the production crew rushed to finish painting the main tank, millions of gallons of water poured in and took five days to fill. The Deepcore rig was anchored to a 90-ton concrete column at the bottom of the large tank. It consisted of six partial and complete modules that took over half a year to plan and build from scratch.
Can-Dive Services Ltd., a Canadian commercial diving company that specialized in “saturation” diving systems and underwater technology, specially manufactured the two working craft (Flatbed and Cab One) for the film. Two million dollars was spent on set construction.
Filming was also done at the largest underground lake in the world — a mine in Bonne Terre, Missouri, which was the background for several underwater shots.
Principal photography.
The main tank was not ready in time for the first day of principal photography. Cameron delayed filming for a week and pushed the smaller tank's schedule forward, demanding that it be ready weeks ahead of schedule. Filming eventually began on August 15, 1988, but there were still problems. On the first day of shooting in the main water tank, it sprang a leak and 150,000 USgal of water a minute rushed out. The studio brought in dam-repair experts to seal it. In addition, enormous pipes with elbow fittings had been improperly installed. There was so much water pressure in them that the elbows blew off.
Cameron's choice of cinematographer on the movie was - a US-based Danish national who would go on to work on other blockbusters such as "Backdraft" and "Arachnophobia" before moving into the director's slot on a myriad of films and TV shows including two editions of the acclaimed HBO WW2 series "Band of Brothers". Salomon used three cameras in watertight housings that were specially designed. Another special housing was designed for scenes that went from above-water dialogue to below-water dialogue. The filmmakers had to figure out how to keep the water clear enough to shoot and dark enough to look realistic at 2,000 feet (700 m), which was achieved by floating a thick layer of plastic beads in the water and covering the top of the tank with an enormous tarpaulin. Cameron wanted to see the actors' faces and hear their dialogue, and thus hired Western Space and Marine to engineer helmets which would remain optically clear underwater and installed state-of-the-art aircraft quality microphones into each helmet. Safety conditions were also a major factor with the installation of a decompression chamber on site, along with a diving bell and a safety diver for each actor.
The breathing fluid used in the film actually exists but has only been thoroughly investigated in animals. Over the previous 20 years it had been tested on several animals, who survived. The rat shown in the film was actually breathing fluid and survived unharmed, although the scene was censored in Britain for perceived cruelty to animals.
Ed Harris did not breathe the fluid. He held his breath inside a helmet full of liquid while being towed 30 feet (10 m) below the surface of the large tank. He recalled that the worst moments were being towed with fluid rushing up his nose and his eyes swelling up. Actors played their scenes at 33 feet (11 m), too shallow a depth for them to need decompression, and rarely stayed down for more than an hour at a time. Cameron and the 26-person underwater diving crew sank to 50 feet (17 m) and stayed down for five hours at a time. To avoid decompression sickness, they would have to hang from hoses halfway up the tank for as long as two hours, breathing pure oxygen.
The cast and crew endured over six months of grueling six-day, 70-hour weeks on an isolated set. At one point, Mary Elizabeth Mastrantonio had a physical and emotional breakdown on the set and on another occasion, Ed Harris burst into spontaneous sobbing while driving home. Cameron himself admitted, "I knew this was going to be a hard shoot, but even I had no idea just how hard. I don't "ever" want to go through this again". For example, for the scene where portions of the rig are flooded with water, he realized that he initially did not know how to minimize the sequence's inherent danger. It took him more than four hours to set up the shot safely. Actor Leo Burmester said, "Shooting "The Abyss" has been the hardest thing I've ever done. Jim Cameron is the type of director who pushes you to the edge, but he doesn't make you do anything he wouldn't do himself." A lightning storm caused a 200-foot (65 m) tear in the black tarpaulin covering the main tank. Repairing it would have taken too much time, so the production began shooting at night. In addition, blooming algae often reduced visibility to 20 feet (6 m) within hours. Over-chlorination led to divers' skin burning and exposed hair being stripped off.
Some of the actors did not like the slow pace of filming. Mary Elizabeth Mastrantonio remembered, "We never started and finished any one scene in any one day". At one point, Cameron told the actors to relieve themselves in their wetsuits to save time between takes. On another occasion, while filming one of many takes of Mastrantonio's character's death scene, the camera ran out of film, prompting Mastrantonio to storm off the set yelling, "We are not animals!" Michael Biehn also grew frustrated by the waiting. He claimed that he was in South Carolina for five months and only acted for three to four weeks. He remembered one day being ten meters underwater and "suddenly the lights went out. It was so black I couldn't see my hand. I couldn't surface. I realized I might not get out of there." Harris said that the daily mental and physical strain was very intense and remembered, "One day we were all in our dressing rooms and people began throwing couches out the windows and smashing the walls. We just had to get our frustrations out." Cameron responded to these complaints, saying, "For every hour they spent trying to figure out what magazine to read, we spent an hour at the bottom of the tank breathing compressed air." After 140 days and $4 million over budget, filming finally wrapped on December 8, 1988. Before the film's release, there were reports from South Carolina that Ed Harris was so upset by the physical demands of the film and Cameron's dictatorial directing style that he said he would refuse to help promote the motion picture. Harris later denied this rumor and helped promote the film. However, after its release and initial promotion, Harris publicly refused to ever again discuss the film, saying "I'm never talking about it and never will." Mary Elizabeth Mastrantonio has also since brushed off the film, commenting, ""The Abyss" was a lot of things. Fun to make was not one of them."
Post-production.
To create the alien water tentacle, Cameron initially considered cel animation or a tentacle sculpted in clay and then animated via stop-motion techniques with water reflections projected onto it. Phil Tippett suggested Cameron contact Industrial Light & Magic. The special visual effects work was divided up among seven FX divisions with motion control work by Dream Quest Images and computer graphics and opticals by ILM. ILM designed a program to produce surface waves of differing sizes and kinetic properties for the pseudopod. For the moment where it mimics Bud and Lindsey's faces, Ed Harris had eight of his facial expressions scanned while twelve of Mastrantonio's were scanned via software used to create computer-generated sculptures. The set was photographed from every angle and digitally recreated so that the pseudopod could be accurately composited into the live-action footage. The company spent six months to create 75 seconds of computer graphics needed for the creature. The film was to have opened on July 4, 1989, but its release was delayed for more than a month by production and special effects problems.
Studio executives were nervous about the film's commercial prospects when preview audiences laughed at scenes of serious intent. Industry insiders said that the release delay was because nervous executives ordered the film's ending completely re-shot. There was also a question of the size of the film's budget. One executive claimed $47 million while "The Wall Street Journal" reported a figure of $60 million. Box office revenue tracker site "The Numbers" lists the production budget at $70 million. When promoting the film on Late Night with David Letterman, Cameron himself said the production budget was $43 million. None of these figures include marketing or distribution costs.
Reception.
Box office.
"The Abyss" was released on August 9, 1989, in 1,533 theaters, where it grossed $9,319,797 on its opening weekend ranking #2 at the box office. It went on to make $54,461,047 in North America and $35,539,051 throughout the rest of the world for a worldwide total of $90,000,098, above its estimated $70 million production budget.
Critical response.
On Rotten Tomatoes, a Review aggregator, "The Abyss" has "Certified Fresh" score of 89% based on 44 reviews with an average rating of 7.2 out of 10. The critical consensus states: "The utterly gorgeous special effects frequently overshadow the fact that "The Abyss" is also a totally gripping, claustrophobic thriller, complete with an interesting crew of characters." On Metacritic, the film has an average score of 62 out of 100, based on 14 critics indicating "generally favorable reviews". 
"Newsweek" magazine's David Ansen, summarizing the theatrical release, wrote, "The payoff to "The Abyss" is pretty damn silly — a portentous "deus ex machina" that leaves too many questions unanswered and evokes too many other films." In her review for "The New York Times", Caryn James claimed that the film had "at least four endings," and "by the time the last ending of this two-and-a-quarter-hour film comes along, the effect is like getting off a demon roller coaster that has kept racing several laps after you were ready to get off." Chris Dafoe, in his review for "The Globe and Mail", wrote, "At its best, "The Abyss" offers a harrowing, thrilling journey through inky waters and high tension. In the end, however, this torpedo turns out to be a dud - it swerves at the last minute, missing its target and exploding ineffectually in a flash of fantasy and fairy-tale schtick."
While praising the film's first two hours as "compelling", the "Toronto Star" remarked, "But when Cameron takes the adventure to the next step, deep into the heart of fantasy, it all becomes one great big deja boo. If we are to believe what Cameron finds way down there, E.T. didn't really phone home, he went surfing and fell off his board." "USA Today" gave the film three out of four stars and wrote, "Most of this underwater blockbuster is 'good,' and at least two action set pieces are great. But the dopey wrap-up sinks the rest 20,000 leagues." In her review for "The Washington Post", Rita Kempley wrote that the film "asks us to believe that the drowned return to life, that the comatose come to the rescue, that driven women become doting wives, that Neptune cares about landlubbers. I'd sooner believe that Moby Dick could swim up the drainpipe." "Halliwell's Film Guide" claimed the film was "despite some clever special effects, a tedious, overlong fantasy that is more excited by machinery than people." Conversely, "Rolling Stone" magazine's Peter Travers enthused, "["The Abyss" is] the greatest underwater adventure ever filmed, the most consistently enthralling of the summer blockbusters...one of the best pictures of the year."
The reviews tallied therein contain reviews for both the theatrical release and the Special Edition. The release of the Special Edition in 1993 garnered much praise. Each giving it thumbs up, Siskel remarked, ""The Abyss" has been improved," and Ebert added, "It makes the film seem more well rounded." In the book "Reel Views 2", James Berardinelli comments, "James Cameron's "The Abyss" may be the most extreme example of an available movie that demonstrates how the vision of a director, once fully realized on screen, can transform a good motion picture into a great one."
Accolades.
"The Abyss" won the 1990 Oscar for Best Visual Effects (John Bruno, Dennis Muren, Hoyt Yeatman, and Dennis Skotak). It was also nominated for:
The studio unsuccessfully lobbied hard to get Michael Biehn nominated for the Academy Award for Best Supporting Actor.
Many other film organizations, such as the Academy of Science Fiction, Fantasy & Horror Films and the American Society of Cinematographers, also nominated "The Abyss". The film ended up winning a total of three other awards from these organizations.
Soundtrack.
The soundtrack to "The Abyss" was released on August 22, 1989.
Deluxe Edition.
Varese Sarabande, which released the original album, issued a limited edition (3000 copies) two-disc album in 2014 featuring the complete score.
"Disc One":
"Disc Two": Tracks 10-19 are bonus tracks.
History of the Special Edition.
Even as the film was in the first weeks of its 1989 theatrical release, rumors were circulating of a wave sequence missing from the film's end. As chronicled in the 1993 laserdisc Special Edition release and later in the 2000 DVD, the pressure to cut the film's running time stemmed from both distribution concerns and Industrial Light & Magic's then-inability to complete the required sequences. From the distributor's perspective, the looming three-hour length limited the number of times the film could be shown each day, assuming that audiences would be willing to sit through the entire film, though 1990's "Dances with Wolves" would shatter both industry-held notions. Further, test audience screenings revealed a surprisingly mixed reaction to the sequences as they appeared in their unfinished form; in post-screening surveys, they dominated both the "Scenes I liked most" and "Scenes I liked least" fields. Contrary to speculation, studio meddling was not the cause of the shortened length; Cameron held final cut as long as the film met a running time of roughly two hours and 15 minutes. He later noted, "Ironically, the studio brass were horrified when I said I was cutting the wave."
Cameron elected to remove the sequences along with other, shorter scenes elsewhere in the film, reducing the running time from roughly two hours and 50 minutes to two hours and 20 minutes and diminishing his signature themes of nuclear peril and disarmament. Subsequent test audience screenings drew substantially better reactions.
Star Mary Elizabeth Mastrantonio publicly expressed regret about some of the scenes selected for removal from the film's theatrical cut.
Shortly after the film's premiere, Cameron and video editor Ed Marsh created a longer video cut of "The Abyss" for their own use that incorporated dailies. With the tremendous success of Cameron's "" in 1991, Lightstorm Entertainment secured a five-year, $500 million financing deal with 20th Century Fox for films produced, directed or written by Cameron. The contract allocated roughly $500,000 of the amount to complete "The Abyss." ILM was commissioned to finish the work they had started three years earlier, with many of the same people who had worked on it originally.
The CGI tools developed for "Terminator 2: Judgment Day" allowed ILM to complete the rumored tidal-wave sequence, as well as correcting flaws in rendering for all their other work done for the film.
The tidal wave sequence had originally been designed by ILM as a physical effect, using a plastic wave, but Cameron was dissatisfied with the end result, and the sequence was scrapped. By the time Cameron was ready to revisit "The Abyss," ILM's CGI prowess had finally progressed to an appropriate level, and the wave was rendered as a CGI effect. "Terminator 2: Judgment Day" screenwriter and frequent Cameron collaborator William Wisher had a cameo in the scene as a reporter in Santa Monica who catches the first tidal wave on camera.
When it was discovered that original production sound recordings had been lost, new dialogue and foley were recorded, but since Captain Kidd Brewer had died of a self-inflicted gunshot before he could return to re-loop his dialog, producers and editors had to lift his original dialogue tracks from the remaining optical-sound prints of the dailies. The Special Edition was therefore dedicated to his memory as a result.
As Alan Silvestri was not available to compose new music for the restored scenes, Robert Garrett, who had composed temporary music for the film's initial cutting in 1989, was chosen to create new music. The Special Edition was completed in December 1992, with 28 minutes added to the film, and saw a limited theatrical release in New York City and Los Angeles on February 26, 1993, and expanded to key cities nationwide in the following weeks.
On home video, in addition to the conventional two-tape VHS release, the first THX-certified LaserDisc title of the Special Edition Box Set was released in May 1993 and was a best seller for the rest of the year. Both the theatrical and SE editions remain available on DVD; however all available DVDs are non-anamorphic, with the exception of the Chinese DVD produced for Region 6 by Excel Media.
Adaptations.
Science-fiction author Orson Scott Card was hired to write a novelization of the film based on the screenplay and discussions with Cameron. He wrote back-stories for Bud, Lindsey and Coffey as a means not only of helping the actors define their roles, but also to justify some of their behavior and mannerisms in the film. Card also wrote the aliens as a colonizing species which preferentially sought high-pressure deep-water worlds to build their ships as they traveled further into the galaxy (their mothership was in orbit on the far side of the moon). The NTIs' knowledge of neuroanatomy and nanoscale manipulation of biochemistry was responsible for many of the "deus ex machina" aspects of the film.
A licensed interactive fiction video game based on the script was being developed for Infocom by Bob Bates, but was cancelled when Infocom was shut down by its then-parent company Activision. Source Interactive later created an action video game entitled "The Abyss: Incident at Europa". The game takes place a few years after the film, where the player must find a cure for a deadly virus.

</doc>
<doc id="45569" url="http://en.wikipedia.org/wiki?curid=45569" title="Dedekind cut">
Dedekind cut

In mathematics, a Dedekind cut, named after Richard Dedekind, is a partition of the rational numbers into two non-empty parts "A" and "B", such that all elements of "A" are less than all elements of "B", and "A" contains no greatest element. Dedekind cuts are one method of construction of the real numbers.
The set "B" may or may not have a smallest element among the rationals. If "B" has a smallest element among the rationals, the "cut" corresponds to that rational. Otherwise, that cut defines a unique irrational number which, loosely speaking, fills the "gap" between "A" and "B". In other words, "A" contains every rational number "less than" the cut, and "B" contains every rational number "greater than" the cut. An irrational cut is equated to an irrational number which is in neither set. Every real number, rational or not, is equated to one and only one cut of rationals.
Whenever, then, we have to do with a cut produced by no rational number, we create a new "irrational" number, which we regard as completely defined by this cut ... . From now on, therefore, to every definite cut there corresponds a definite rational or irrational number ...—Richard Dedekind
More generally, a Dedekind cut is a partition of a totally ordered set into two non-empty parts "A" and "B", such that "A" is closed downwards (meaning that for all "a" in "A", "x" ≤ "a" implies that "x" is in "A" as well) and "B" is closed upwards, and "A" contains no greatest element. See also completeness (order theory).
It is straightforward to show that a Dedekind cut among the real numbers is uniquely defined by the corresponding cut among the rational numbers. Similarly, every cut of reals is identical to the cut produced by a specific real number (which can be identified as the smallest element of the "B" set). In other words, the number line where every real number is defined as a Dedekind cut of rationals is a complete continuum without any further gaps.
Dedekind used the German word "Schnitt" (cut) in a visual sense rooted in Euclidean geometry. His theorem asserting the completeness of the real number system is nevertheless a theorem about numbers and not geometry. Classical Euclidean geometry lacked a treatment of continuity (although Eudoxus did construct a sophisticated theory of incommensurable quantities such as formula_1 ): thus the very first proposition of the very first book of Euclid's geometry (constructing an equilateral triangle) was criticised by Pappus of Alexandria on the grounds that there was nothing in the axioms that asserted two intersecting circles in fact intersect in points. In David Hilbert's axiom system, continuity is provided by the Axiom of Archimedes, while in Alfred Tarski's system continuity is provided by what is essentially Dedekind's section. In mathematical logic, the identification of the real numbers with the real number line is provided by the Cantor–Dedekind axiom.
Representations.
It is more symmetrical to use the ("A","B") notation for Dedekind cuts, but each of "A" and "B" does determine the other. It can be a simplification, in terms of notation if nothing more, to concentrate on one "half" — say, the lower one — and call any downward closed set "A" without greatest element a "Dedekind cut".
If the ordered set "S" is complete, then, for every Dedekind cut ("A", "B") of "S", the set "B" must have a minimal element "b", 
hence we must have that "A" is the interval ( −∞, "b"), and "B" the interval ["b", +∞).
In this case, we say that "b" "is represented by" the cut ("A","B").
The important purpose of the Dedekind cut is to work with number sets that are "not" complete. The cut itself can represent a number not in the original collection of numbers (most often rational numbers). The cut can represent a number "b", even though the numbers contained in the two sets "A" and "B" do not actually include the number "b" that their cut represents.
For example if "A" and "B" only contain rational numbers, they can still be cut at √2 by putting every negative rational number in "A", along with every non-negative number whose square is less than 2; similarly "B" would contain every positive rational number whose square is greater than or equal to 2. Even though there is no rational value for √2, if the rational numbers are partitioned into "A" and "B" this way, the partition itself represents an irrational number.
Ordering of cuts.
Regard one Dedekind cut ("A", "B") as "less than" another Dedekind cut ("C", "D") (of the same superset) if "A" is a proper subset of "C". Equivalently, if "D" is a proper subset of "B", the cut ("A", "B") is again "less than" ("C", "D"). In this way, set inclusion can be used to represent the ordering of numbers, and all other relations ("greater than", "less than or equal to", "equal to", and so on) can be similarly created from set relations.
The set of all Dedekind cuts is itself a linearly ordered set (of sets). Moreover, the set of Dedekind cuts has the least-upper-bound property, i.e., every nonempty subset of it that has any upper bound has a "least" upper bound. Thus, constructing the set of Dedekind cuts serves the purpose of embedding the original ordered set "S", which might not have had the least-upper-bound property, within a (usually larger) linearly ordered set that does have this useful property.
Construction of the real numbers.
A typical Dedekind cut of the rational numbers is given by
This cut represents the irrational number √2 in Dedekind's construction. To establish this truly, one must show that this really is a cut and that it is the square root of two. However, neither claim is immediate. Showing that it is a cut requires showing that for any positive rational formula_4 with formula_5, there is a rational formula_6 with formula_7 and formula_8 The choice formula_9 works. Then we have a cut and it has a square no larger than 2, but to show equality requires showing that if formula_10 is any rational number less than 2, then there is positive formula_4 in formula_12 with formula_13.
Note that the equality "b"2 = 2 cannot hold since √2 is not rational.
Generalizations.
A construction similar to Dedekind cuts is used for the construction of surreal numbers.
Partially ordered sets.
More generally, if "S" is a partially ordered set, a "completion" of "S" means a complete lattice "L" with an order-embedding of "S" into "L". The notion of "complete lattice" generalizes the least-upper-bound property of the reals.
One completion of "S" is the set of its "downwardly closed" subsets, ordered by inclusion. A related completion that preserves all existing sups and infs of "S" is obtained by the following construction: For each subset "A" of "S", let "A"u denote the set of upper bounds of "A", and let "A"l denote the set of lower bounds of "A". (These operators form a Galois connection.) Then the Dedekind–MacNeille completion of "S" consists of all subsets "A" for which ("A"u)l = "A"; it is ordered by inclusion. The Dedekind-MacNeille completion is the smallest complete lattice with "S" embedded in it.

</doc>
<doc id="45570" url="http://en.wikipedia.org/wiki?curid=45570" title="DNA vaccination">
DNA vaccination

DNA vaccination is a technique for protecting an organism against disease by injecting it with genetically engineered DNA to produce an immunological response. Nucleic acid vaccines are still experimental, and have been applied to a number of viral, bacterial and parasitic models of disease, as well as to several tumour models. Although unproven in the clinical setting, DNA vaccines have a number of potential advantages over conventional vaccines, including the ability to induce a wider range of immune response types.
History.
Many believe vaccines are among the greatest achievements of modern medicine – in industrial nations, they have eliminated naturally occurring cases of smallpox, and nearly eliminated polio, while other diseases, such as typhus, rotavirus, hepatitis A and B and others are well controlled. Conventional vaccines, however, only cover a small number of diseases, and infections that lack effective vaccines kill millions of people every year, with AIDS, hepatitis C and malaria being particularly common.
"First generation" vaccines are whole-organism vaccines – either live and weakened, or killed forms. Live, attenuated vaccines, such as smallpox and polio vaccines, are able to induce killer T-cell (TC or CTL) responses, helper T-cell (TH) responses and antibody immunity. However, there is a small risk that attenuated forms of a pathogen can revert to a dangerous form, and may still be able to cause disease in immunocompromised vaccine recipients (such as those with AIDS). While killed vaccines do not have this risk, they cannot generate specific killer T cell responses, and may not work at all for some diseases. In order to minimise these risks, so-called "second generation vaccines" were developed. These are subunit vaccines, consisting of defined protein antigens (such as tetanus or diphtheria toxoid) or recombinant protein components (such as the hepatitis B surface antigen). These, too, are able to generate TH and antibody responses, but not killer T cell responses.
DNA vaccines are "third generation vaccines", and are made up of a small, circular piece of bacterial DNA (called a plasmid) that has been genetically engineered to produce one or two specific proteins (antigens) from a pathogen. The vaccine DNA is injected into the cells of the body, where the "inner machinery" of the host cells "reads" the DNA and uses it to synthesize the pathogen's proteins. Because these proteins are recognised as foreign, when they are processed by the host cells and displayed on their surface, the immune system is alerted, which then triggers a range of immune responses. These DNA vaccines were developed from “failed” gene therapy experiments. The first demonstration of a plasmid-induced immune response was when mice inoculated with a plasmid expressing human growth hormone elicited antibodies instead of altering growth.
Current use.
Thus far, few experimental trials have evoked a response strong enough to protect against disease, and the usefulness of the technique, while tantalizing, remains to be conclusively proven in human trials. As of 2014 no human DNA vaccines have been approved for human use by the FDA. However, a veterinary DNA vaccine to protect horses from West Nile virus has been approved. In August 2007, a preliminary study in DNA vaccination against multiple sclerosis was reported as being effective.
Plasmid vectors for use in vaccination.
Vector design.
DNA vaccines elicit the best immune response when highly active expression vectors are used. These are plasmids which usually consist of a strong viral promoter to drive the in vivo transcription and translation of the gene (or complementary DNA) of interest. Intron A may sometimes be included to improve mRNA stability and hence increase protein expression. Plasmids also include a strong polyadenylation/transcriptional termination signal, such as bovine growth hormone or rabbit beta-globulin polyadenylation sequences. Multicistronic vectors are sometimes constructed to express more than one immunogen, or to express an immunogen and an immunostimulatory protein.
Because the plasmid is the “vehicle” from which the immunogen is expressed, optimising vector design for maximal protein expression is essential. One way of enhancing protein expression is by optimising the codon usage of pathogenic mRNAs for eukaryotic cells. Pathogens often have different AT-contents than the species being immunized, so altering the gene sequence of the immunogen to reflect the codons more commonly used in the target species may improve its expression.
Another consideration is the choice of promoter. The SV40 promoter was conventionally used until research showed that vectors driven by the Rous Sarcoma Virus (RSV) promoter had much higher expression rates. More recently, expression rates have been further increased by the use of the cytomegalovirus (CMV) immediate early promoter. Inclusion of the Mason-Pfizer monkey virus (MPV)-CTE with/without rev increased envelope expression. Furthermore the CTE+rev construct was significantly more immunogenic than CTE-alone vector.
 Additional modifications to improve expression rates have included the insertion of enhancer sequences, synthetic introns, adenovirus tripartite leader (TPL) sequences and modifications to the polyadenylation and transcriptional termination sequences. An example of DNA vaccine plasmid is pVAC, it uses SV40 promoter.
Mechanism of plasmids.
Once the plasmid inserts itself into the nucleus of the transfected cell, it starts to encode for a gene resulting in production of peptide string of a foreign antigen. The cell on its surface displays the foreign antigen with both histocompatibility complex (MHC) classes I and class II molecule. The antigen-presenting cell then travels to the lymph nodes and presents the antigen peptide and costimulatory molecule signaled by T-cell results in initiation of the immune response.
Vaccine insert design.
Immunogens can be targeted to various cellular compartments in order to improve antibody or cytotoxic T-cell responses. Secreted or plasma membrane-bound antigens are more effective at inducing antibody responses than cytosolic antigens, while cytotoxic T-cell responses can be improved by targeting antigens for cytoplasmic degradation and subsequent entry into the major histocompatibility complex (MHC) class I pathway. This is usually accomplished by the addition of N-terminal ubiquitin signals.
The conformation of the protein can also have an effect on antibody responses, with “ordered” structures (like viral particles) being more effective than unordered structures. Strings of minigenes (or MHC class I epitopes) from different pathogens are able to raise cytotoxic T-cell responses to a number of pathogens, especially if a TH epitope is also included.
Delivery methods.
DNA vaccines have been introduced into animal tissues by a number of different methods. These delivery methods are briefly reviewed in Table 2, with the advantages and disadvantages of the most commonly used methods summarised in Table 3.
The two most popular approaches are injection of DNA in saline, using a standard hypodermic needle, and gene gun delivery. A schematic outline of the construction of a DNA vaccine plasmid and its subsequent delivery by these two methods into a host is illustrated at . Injection in saline is normally conducted intramuscularly (IM) in skeletal muscle, or intradermally (ID), with DNA being delivered to the extracellular spaces. This can be assisted by electroporation; by temporarily damaging muscle fibres with myotoxins such as bupivacaine; or by using hypertonic solutions of saline or sucrose. Immune responses to this method of delivery can be affected by many factors, including needle type, needle alignment, speed of injection, volume of injection, muscle type, and age, sex and physiological condition of the animal being injected.
Gene gun delivery, the other commonly used method of delivery, ballistically accelerates plasmid DNA (pDNA) that has been adsorbed onto gold or tungsten microparticles into the target cells, using compressed helium as an accelerant.
Alternative delivery methods have included aerosol instillation of naked DNA on mucosal surfaces, such as the nasal and lung mucosa, and topical administration of pDNA to the eye and vaginal mucosa. Mucosal surface delivery has also been achieved using cationic liposome-DNA preparations, biodegradable microspheres, attenuated "Shigella" or "Listeria" vectors for oral administration to the intestinal mucosa, and recombinant adenovirus vectors. Another alternative vector is a hybrid vehicle composed of bacteria cell and synthetic polymers. An "E. coli" inner core and poly(beta-amino ester) outer coat function synergistically to increase the gene delivery efficiency by addressing barriers associated with antigen-presenting cell gene delivery which include cellular uptake and internalization, phagosomal escape and intracellular cargo concentration. Tested in mice, the hybrid vector was found to induce immune response.
The method of delivery determines the dose of DNA required to raise an effective immune response. Saline injections require variable amounts of DNA, from 10 μg-1 mg, whereas gene gun deliveries require 100 to 1000 times less DNA than intramuscular saline injection to raise an effective immune response. Generally, 0.2 μg – 20 μg are required, although quantities as low as 16 ng have been reported. These quantities vary from species to species, with mice, for example, requiring approximately 10 times less DNA than primates. Saline injections require more DNA because the DNA is delivered to the extracellular spaces of the target tissue (normally muscle), where it has to overcome physical barriers (such as the basal lamina and large amounts of connective tissue, to mention a few) before it is taken up by the cells, while gene gun deliveries bombard DNA directly into the cells, resulting in less “wastage”.
Another approach to DNA vaccination is expression library immunization (ELI). Using this technique, potentially all the genes from a pathogen can be delivered at one time, which may be useful for pathogens which are difficult to attenuate or culture. ELI can be used to identify which of the pathogen’s genes induce a protective response. This has been tested with "Mycoplasma pulmonis", a murine lung pathogen with a relatively small genome, and it was found that even partial expression libraries can induce protection from subsequent challenge.
Immune response raised by DNA vaccines.
Helper T cell responses.
DNA immunization is able to raise a range of TH responses, including lymphoproliferation and the generation of a variety of cytokine profiles. A major advantage of DNA vaccines is the ease with which they can be manipulated to bias the type of T-cell help towards a TH1 or TH2 response. Each type of response has distinctive patterns of lymphokine and chemokine expression, specific types of immunoglobulins expressed, patterns of lymphocyte trafficking, and types of innate immune responses generated.
Raising of different types of T-cell help.
The type of T-cell help raised is influenced by the method of delivery and the type of immunogen expressed, as well as the targeting of different lymphoid compartments. Generally, saline needle injections (either IM or ID) tend to induce TH1 responses, while gene gun delivery raises TH2 responses. This is true for intracellular and plasma membrane-bound antigens, but not for secreted antigens, which seem to generate TH2 responses, regardless of the method of delivery.
Generally the type of T-cell help raised is stable over time, and does not change when challenged or after subsequent immunizations which would normally have raised the opposite type of response in a naïve animal. However, Mor "et al.". (1995) immunized and boosted mice with pDNA encoding the circumsporozoite protein of the mouse malarial parasite "Plasmodium yoelii" (PyCSP) and found that the initial TH2 response changed, after boosting, to a TH1 response.
Mechanistic basis for different types of T-cell help.
It is not understood how these different methods of DNA immunization, or the forms of antigen expressed, raise different profiles of T-cell help. It was thought that the relatively large amounts of DNA used in IM injection were responsible for the induction of TH1 responses. However, evidence has shown no differences in TH type due to dose. It has been postulated that the type of T-cell help raised is determined by the differentiated state of antigen presenting cells. Dendritic cells can differentiate to secrete IL-12 (which supports TH1 cell development) or IL-4 (which supports TH2 responses). pDNA injected by needle is endocytosed into the dendritic cell, which is then stimulated to differentiate for TH1 cytokine production, while the gene gun bombards the DNA directly into the cell, thus bypassing TH1 stimulation.
Practical uses of polarised T-cell help.
This polarisation in T-cell help is useful in influencing allergic responses and autoimmune diseases. In autoimmune diseases, the goal would be to shift the self-destructive TH1 response (with its associated cytotoxic T cell activity) to a non-destructive TH2 response. This has been successfully applied in predisease priming for the desired type of response in preclinical models and somewhat successful in shifting the response for an already established disease.
Cytotoxic T-cell responses.
One of the greatest advantages of DNA vaccines is that they are able to induce cytotoxic T lymphocytes (CTL) without the inherent risk associated with live vaccines. CTL responses can be raised against immunodominant and immunorecessive CTL epitopes, as well as subdominant CTL epitopes, in a manner which appears to mimic natural infection. This may prove to be a useful tool in assessing CTL epitopes of an antigen, and their role in providing immunity.
Cytotoxic T-cells recognise small peptides (8-10 amino acids) complexed to MHC class I molecules (Restifo et al., 1995). These peptides are derived from endogenous cytosolic proteins which are degraded and delivered to the nascent MHC class I molecule within the endoplasmic reticulum (ER). Targeting gene products directly to the ER (by the addition of an amino-terminal insertion sequence) should thus enhance CTL responses. This has been successfully demonstrated using recombinant vaccinia viruses expressing influenza proteins, but the principle should be applicable to DNA vaccines too. Targeting antigens for intracellular degradation (and thus entry into the MHC class I pathway) by the addition of ubiquitin signal sequences, or mutation of other signal sequences, has also been shown to be effective at increasing CTL responses.
CTL responses can also be enhanced by co-inoculation with co-stimulatory molecules such as B7-1 or B7-2 for DNA vaccines against influenza nucleoprotein, or GM-CSF for DNA vaccines against the murine malaria model "P. yoelii". Co-inoculation with plasmids encoding co-stimulatory molecules IL-12 and TCA3 have also been shown to increase CTL activity against HIV-1 and influenza nucleoprotein antigens.
Humoral (antibody) response.
Antibody responses elicited by DNA vaccinations are influenced by a number of variables, including type of antigen encoded; location of expressed antigen (i.e. intracellular vs. secreted); number, frequency and dose of immunizations; site and method of antigen delivery, to name a few.
Kinetics of antibody response.
Humoral responses after a single DNA injection can be much longer-lived than after a single injection with a recombinant protein. Antibody responses against hepatitis B virus (HBV) envelope protein (HBsAg) have been sustained for up to 74 weeks without boost, while lifelong maintenance of protective response to influenza haemagglutinin has been demonstrated in mice after gene gun delivery. Antibody-secreting cells migrate to the bone marrow and spleen for long-term antibody production, and are generally localised there after one year.
Comparisons of antibody responses generated by natural (viral) infection, immunization with recombinant protein and immunization with pDNA are summarised in Table 4. DNA-raised antibody responses rise much more slowly than when natural infection or recombinant protein immunization occurs. It can take as long as 12 weeks to reach peak titres in mice, although boosting can increase the rate of antibody production. This slow response is probably due to the low levels of antigen expressed over several weeks, which supports both primary and secondary phases of antibody response.
DNA vaccine expressing HBV small and middle envelope protein was injected into adults with chronic hepatitis. The vaccine resulted in specific interferon gamma cell production. Also specific T-cells for middle envelop proteins antigens were developed. The immune response of the patients was not robust enough to control HBV infection (Mancini - Bourgine et al.)
Additionally, the titres of specific antibodies raised by DNA vaccination are lower than those obtained after vaccination with a recombinant protein. However, DNA immunization-induced antibodies show greater affinity to native epitopes than recombinant protein-induced antibodies. In other words, DNA immunization induces a qualitatively superior response. Antibody can be induced after just one vaccination with DNA, whereas recombinant protein vaccinations generally require a boost. As mentioned previously, DNA immunization can be used to bias the TH profile of the immune response, and thus the antibody isotype, which is not possible with either natural infection or recombinant protein immunization. Antibody responses generated by DNA are useful not just in vaccination but as a preparative tool, too. For example, polyclonal and monoclonal antibodies can be generated for use as reagents.
Mechanistic basis for DNA raised immune responses.
DNA uptake mechanism.
When DNA uptake and subsequent expression was first demonstrated "in vivo" in muscle cells, it was thought that these cells were unique in this ability because of their extensive network of T-tubules. Using electron microscopy, it was proposed that DNA uptake was facilitated by caveolae (or, non-clathrin coated pits). However, subsequent research revealed that other cells (such as keratinocytes, fibroblasts and epithelial Langerhans cells) could also internalize DNA. This phenomenon has not been the subject of much research, so the actual mechanism of DNA uptake is not known.
Two theories are currently popular – that "in vivo" uptake of DNA occurs non-specifically, in a method similar to phago- or pinocytosis, or through specific receptors. These might include a 30kDa surface receptor, or macrophage scavenger receptors. The 30kDa surface receptor binds very specifically to 4500-bp genomic DNA fragments (which are then internalised) and is found on professional APCs and T-cells. Macrophage scavenger receptors bind to a variety of macromolecules, including polyribonucleotides, and are thus also candidates for DNA uptake. Receptor mediated DNA uptake could be facilitated by the presence of polyguanylate sequences. Further research into this mechanism might seem pointless, considering that gene gun delivery systems, cationic liposome packaging, and other delivery methods bypass this entry method, but understanding it might be useful in reducing costs (e.g. by reducing the requirement for cytofectins), which will be important in the food animals industry.
Antigen presentation by bone marrow-derived cells.
Studies using chimeric mice have shown that antigen is presented by bone-marrow derived cells, which include dendritic cells, macrophages and specialised B-cells called professional antigen presenting cells (APC) Iwasaki et al., 1997). After gene gun inoculation to the skin, transfected Langerhans cells migrate to the draining lymph node to present antigen. After IM and ID injections, dendritic cells have also been found to present antigen in the draining lymph node and transfected macrophages have been found in the peripheral blood.
Besides direct transfection of dendritic cells or macrophages, cross priming is also known to occur following IM, ID and gene gun deliveries of DNA. Cross priming occurs when a bone marrow-derived cell presents peptides from proteins synthesised in another cell in the context of MHC class 1. This can prime cytotoxic T-cell responses and seems to be important for a full primary immune response.
Role of the target site.
IM and ID delivery of DNA initiate immune responses differently. In the skin, keratinocytes, fibroblasts and Langerhans cells take up and express antigen, and are responsible for inducing a primary antibody response. Transfected Langerhans cells migrate out of the skin (within 12 hours) to the draining lymph node where they prime secondary B- and T-cell responses. In skeletal muscle, on the other hand, striated muscle cells are most frequently transfected, but seem to be unimportant in mounting an immune response. Instead, IM inoculated DNA “washes” into the draining lymph node within minutes, where distal dendritic cells are transfected and then initiate an immune response. Transfected myocytes seem to act as a “reservoir” of antigen for trafficking professional APCs.
Maintenance of immune response.
DNA vaccination generates an effective immune memory via the display of antigen-antibody complexes on follicular dendritic cells (FDC), which are potent B-cell stimulators. T-cells can be stimulated by similar, germinal centre dendritic cells. FDC are able to generate an immune memory because antibodies production “overlaps” long-term expression of antigen, allowing antigen-antibody immunocomplexes to form and be displayed by FDC.
Interferons.
Both helper and cytotoxic T-cells can control viral infections by secreting interferons. Cytotoxic T cells usually kill virally infected cells. However, they can also be stimulated to secrete antiviral cytokines such as IFN-γ and TNF-α, which don’t kill the cell but place severe limitations on viral infection by down-regulating the expression of viral components. DNA vaccinations can thus be used to curb viral infections by non-destructive IFN-mediated control. This has been demonstrated for the hepatitis B virus. IFN-γ is also critically important in controlling malaria infections, and should be taken into consideration when developing anti-malarial DNA vaccines.
Modulation of the immune response.
Cytokine modulation.
For a vaccine to be effective, it must induce an appropriate immune response for a given pathogen, and the ability of DNA vaccines to polarise T-cell help towards TH1 or TH2 profiles, and generate CTL and/or antibody when required, is a great advantage in this regard. This can be accomplished by modifications to the form of antigen expressed (i.e. intracellular vs. secreted), the method and route of delivery, and the dose of DNA delivered. However, it can also be accomplished by the co-administration of plasmid DNA encoding immune regulatory molecules, i.e. cytokines, lymphokines or co-stimulatory molecules. These “genetic adjuvants” can be administered a number of ways:
In general, co-administration of pro-inflammatory agents (such as various interleukins, tumor necrosis factor, and GM-CSF) plus TH2 inducing cytokines increase antibody responses, whereas pro-inflammatory agents and TH1 inducing cytokines decrease humoral responses and increase cytotoxic responses (which is more important in viral protection, for example). Co-stimulatory molecules like B7-1, B7-2 and CD40L are also sometimes used.
This concept has been successfully applied in topical administration of pDNA encoding IL-10. Plasmid encoding B7-1 (a ligand on APCs) has successfully enhanced the immune response in anti-tumour models, and mixing plasmids encoding GM-CSF and the circumsporozoite protein of P. yoelii (PyCSP) has enhanced protection against subsequent challenge (whereas plasmid-encoded PyCSP alone did not). It was proposed that GM-CSF may cause dendritic cells to present antigen more efficiently, and enhance IL-2 production and TH cell activation, thus driving the increased immune response. This can be further enhanced by first priming with a pPyCSP and pGM-CSF mixture, and later boosting with a recombinant poxvirus expressing PyCSP. However, co-injection of plasmids encoding GM-CSF (or IFN-γ, or IL-2) and a fusion protein of "P. chabaudi" merozoite surface protein 1 (C-terminus)-hepatitis B virus surface protein (PcMSP1-HBs) actually abolished protection against challenge, compared to protection acquired by delivery of pPcMSP1-HBs alone.
The advantages of using genetic adjuvants are their low cost and simplicity of administration, as well as avoidance of unstable recombinant cytokines and potentially toxic, “conventional” adjuvants (such as alum, calcium phosphate, monophosphoryl lipid A, cholera toxin, cationic and mannan-coated liposomes, QS21, carboxymethylcellulose and ubenimix). However, the potential toxicity of prolonged cytokine expression has not been established, and in many commercially important animal species, cytokine genes still need to be identified and isolated. In addition, various plasmid encoded cytokines modulate the immune system differently according to the time of delivery. For example, some cytokine plasmid DNAs are best delivered after the immunogen pDNA, because pre- or co-delivery can actually decrease specific responses, and increase non-specific responses.
Immunostimulatory CpG motifs.
Plasmid DNA itself appears to have an adjuvant effect on the immune system. Bacterially derived DNA has been found to trigger innate immune defence mechanisms, the activation of dendritic cells, and the production of TH1 cytokines. This is due to recognition of certain CpG dinucleotide sequences which are immunostimulatory. CpG stimulatory (CpG-S) sequences occur twenty times more frequently in bacterially derived DNA than in eukaryotes. This is because eukaryotes exhibit “CpG suppression” – i.e. CpG dinucleotide pairs occur much less frequently than expected. Additionally, CpG-S sequences are hypomethylated. This occurs frequently in bacterial DNA, while CpG motifs occurring in eukaryotes are all methylated at the cytosine nucleotide. In contrast, nucleotide sequences which inhibit the activation of an immune response (termed CpG neutralising, or CpG-N) are over represented in eukaryotic genomes. The optimal immunostimulatory sequence has been found to be an unmethylated CpG dinucleotide flanked by two 5’ purines and two 3’ pyrimidines. Additionally, flanking regions outside this immunostimulatory hexamer must be guanine-rich to ensure binding and uptake into target cells.
The innate system works synergistically with the adaptive immune system to mount a response against the DNA encoded protein. CpG-S sequences induce polyclonal B-cell activation and the upregulation of cytokine expression and secretion. Stimulated macrophages secrete IL-12, IL-18, TNF-α, IFN-α, IFN-β and IFN-γ, while stimulated B-cells secrete IL-6 and some IL-12.
Manipulation of CpG-S and CpG-N sequences in the plasmid backbone of DNA vaccines can ensure the success of the immune response to the encoded antigen, and drive the immune response toward a TH1 phenotype. This is useful if a pathogen requires a TH response for protection. CpG-S sequences have also been used as external adjuvants for both DNA and recombinant protein vaccination with variable success rates. Other organisms with hypomethylated CpG motifs have also demonstrated the stimulation of polyclonal B-cell expansion. However, the mechanism behind this may be more complicated than simple methylation – hypomethylated murine DNA has not been found to mount an immune response.
Most of the evidence for the existence of immunostimulatory CpG sequences comes from murine studies. Clearly, extrapolation of this data to other species should be done with caution – different species may require different flanking sequences, as binding specificities of scavenger receptors differ between species. Additionally, species such as ruminants may be insensitive to immunostimulatory sequences due to the large gastrointestinal load they exhibit. Further research may be useful in the optimisation of DNA vaccination, especially in the food animal industry.
Alternative boosts.
DNA-primed immune responses can be boosted by the administration of recombinant protein or recombinant poxviruses. “Prime-boost” strategies with recombinant protein have successfully increased both neutralising antibody titre, and antibody avidity and persistence, for weak immunogens, such as HIV-1 envelope protein. Recombinant virus boosts have been shown to be very efficient at boosting DNA-primed CTL responses. Priming with DNA focuses the immune response on the required immunogen, while boosting with the recombinant virus provides a larger amount of expressed antigen, leading to a large increase in specific CTL responses.
Prime-boost strategies have been successful in inducing protection against malarial challenge in a number of studies. Primed mice with plasmid DNA encoding "Plasmodium yoelii" circumsporozoite surface protein (PyCSP), then boosted with a recombinant vaccinia virus expressing the same protein had significantly higher levels of antibody, CTL activity and IFN-γ, and hence higher levels of protection, than mice immunized and boosted with plasmid DNA alone. This can be further enhanced by priming with a mixture of plasmids encoding PyCSP and murine GM-CSF, before boosting with recombinant vaccinia virus. An effective prime-boost strategy for the simian malarial model "P. knowlesi" has also been demonstrated. Rhesus monkeys were primed with a multicomponent, multistage DNA vaccine encoding two liver-stage antigens - the circumsporozoite surface protein (PkCSP) and sporozoite surface protein 2 (PkSSP2) - and two blood stage antigens - the apical merozoite surface protein 1 (PkAMA1) and merozoite surface protein 1 (PkMSP1p42). They were then boosted with a recombinant canarypox virus encoding all four antigens (ALVAC-4). Immunized monkeys developed antibodies against sporozoites and infected erythrocytes, and IFN-γ-secreting T-cell responses against peptides from PkCSP. Partial protection against sporozoite challenge was achieved, and mean parasitemia was significantly reduced, compared to control monkeys. These models, while not ideal for extrapolation to "P. falciparum" in humans, will be important in pre-clinical trials.
Additional methods of enhancing DNA-induced immune responses.
Formulations of DNA.
The efficiency of DNA immunization can be improved by stabilising DNA against degradation, and increasing the efficiency of delivery of DNA into antigen presenting cells. This has been demonstrated by coating biodegradable cationic microparticles (such as poly(lactide-co-glycolide) formulated with cetyltrimethylammonium bromide) with DNA. Such DNA-coated microparticles can be as effective at raising CTL as recombinant vaccinia viruses, especially when mixed with alum. Particles 300 nm in diameter appear to be most efficient for uptake by antigen presenting cells.
Alphavirus vectors.
Recombinant alphavirus-based vectors have also been used to improve DNA vaccination efficiency. The gene encoding the antigen of interest is inserted into the alphavirus replicon, replacing structural genes but leaving non-structural replicase genes intact. The Sindbis virus and Semliki Forest virus have been used to build recombinant alphavirus replicons. Unlike conventional DNA vaccinations, however, alphavirus vectors kill transfected cells, and are only transiently expressed. Also, alphavirus replicase genes are expressed in addition to the vaccine insert. It is not clear how alphavirus replicons raise an immune response, but it is thought that this may be due to the high levels of protein expressed by this vector, replicon-induced cytokine responses, or replicon-induced apoptosis leading to enhanced antigen uptake by dendritic cells.

</doc>
<doc id="45575" url="http://en.wikipedia.org/wiki?curid=45575" title="Southern Schleswig">
Southern Schleswig

Southern Schleswig (German: "Südschleswig" or "Landesteil Schleswig", Danish: "Sydslesvig") denotes the southern half of the former Duchy of Schleswig on the Jutland Peninsula. The geographical area today covers the large area between the Eider river in the south and the Flensburg Fjord in the north, where it borders Denmark. Northern Schleswig, congruent with the former South Jutland County.
History.
The Schleswig lands north of the Eider river and the Bay of Kiel had been a fief of the Danish Crown since the Early Middle Ages. The southern Holstein region belonged to Francia and later to the Holy Roman Empire, it was however held as an Imperial fief by the Danish kings since the 1460 Treaty of Ribe.
The Schleswig-Holstein Question at first culminated in the course of the Revolutions of 1848, when from 1848 to 1851 revolting German-speaking National liberals backed by Prussia fought for the separation of Schleswig and Holstein from Denmark in the First Schleswig War. Though the "status quo" was restored, the conflict lingered on and on 1 February 1864 the Prussian and Austrian troops crossed the Eider sparking off the Second Schleswig War, after which Denmark had to cede Schleswig and Holstein according to the Treaty of Vienna. After the Austro-Prussian War of 1866, victorious Prussia took control over all Schleswig and Holstein but was obliged by the Peace of Prague to hold a referendum in predominantly Danish-speaking Northern Schleswig, which it never did.
After the German defeat in World War I the Schleswig Plebiscites were decreed by the Treaty of Versailles, in which the present-day German-Danish border was drawn taking effect on 15 June 1920, dividing Schleswig in a southern and northern part and leaving a considerable Danish and German minority on both sides 
Today.
Southern Schleswig is part of the German state ("Bundesland") of Schleswig-Holstein, therefore its denotation as "Landesteil Schleswig". It does not however form an administrative entity, but consists of the districts ("Landkreise") of Schleswig-Flensburg, Nordfriesland, the urban district ("Kreisfreie Stadt") of Flensburg and the northern part of Rendsburg-Eckernförde.
Beside Standard German, Low Saxon dialects (Schleswigsch) are spoken, as well as Danish (Standard Danish or South Schleswig Danish) and its South Jutlandic variant, furthermore North Frisian in the west. Danish and North Frisian are official minority languages. Many of the inhabitants who only speak German and not Danish do not consider the region any different from the rest of Schleswig-Holstein. This notion is disputed by those defining themselves as Danes, South Schleswigans or Schleswigans, particularly historians and people organised in the institutions of the Danish minority of Southern Schleswig, such as the South Schleswig Voter Federation. Many of the Last names found in the region are very often of Scandinavian or Danish form, with the "-sen" endings like Petersen.
The major cities of Southern Schleswig are Flensburg, Rendsburg, the city of Schleswig, and Husum.

</doc>
<doc id="45576" url="http://en.wikipedia.org/wiki?curid=45576" title="County Donegal">
County Donegal

County Donegal (pronounced or ; Irish: "Contae Dhún na nGall") is a county in Ireland. It is part of the Border Region and is in the province of Ulster. It is named after the town of Donegal ("Dún na nGall") in the south of the county. Donegal County Council is the local council for the county and Lifford serves as the county town. The population of the county was 161,137 according to the 2011 census. It has also been known as (County) Tyrconnell ("Tír Chonaill"), after the historic territory of the same name.
Geography and political subdivisions.
In terms of size and area, it is the largest county in Ulster and the fourth-largest county in all of Ireland. Uniquely, County Donegal shares a small border with only one other county in the Republic of Ireland – County Leitrim. The greater part of its land border is shared with three counties of Northern Ireland: County Londonderry, County Tyrone and County Fermanagh. This geographic isolation from the rest of the Republic has led to Donegal people maintaining a distinct cultural identity and has been used to market the county with the slogan "Up here it's different". While Lifford is the county town, Letterkenny is by far the largest town in the county with a population of 19,588. Letterkenny and the nearby city of Derry form the main economic axis of the northwest of Ireland. Indeed, what became the City of Derry was officially part of County Donegal up until 1610.
Baronies.
There are eight historic baronies in the county:
Informal districts.
The county may be informally divided into a number of traditional districts. There are two Gaeltacht districts in the west: The Rosses (Irish: "Na Rosa"), centred on the town of Dungloe (Irish: "An Clochán Liath"), and Gweedore (Irish: "Gaoth Dobhair"). Another Gaeltacht district is located in the north-west: Cloughaneely (Irish: "Cloich Chionnaola"), centred on the town of Falcarragh (Irish: "An Fál Carrach"). The most northerly part of the island of Ireland is the location for three peninsulas of outstanding natural beauty: Inishowen, Fanad and Rosguill. The main population centre of Inishowen, Ireland's largest peninsula, is Buncrana. In the east of the county lies the Finn Valley (centred on Ballybofey). The Laggan district (not to be confused with the more famous Lagan Valley in the south of County Antrim) is centred on the town of Raphoe. 
Demographics.
According to the 1841 Census, County Donegal had a population of 296,000 people. As a result of famine and emigration, the population had reduced by 41,000 by 1851 and further reduced by 18,000 by 1861. By the time of the 1951 Census the population was only 44% of what it had been in 1841. The 2006 Census, undertaken by the State's Central Statistics Office, had County Donegal's population standing at 147,264. According to the 2011 Census, the county's population had grown to 161,137.
Physical geography.
The county is the most mountainous in Ulster consisting chiefly of two ranges of low mountains; the Derryveagh Mountains in the north and the Bluestack Mountains in the south, with Mount Errigal at 749 m the highest peak. It has a deeply indented coastline forming natural sea loughs, of which both Lough Swilly and Lough Foyle are the most notable. The Slieve League cliffs are the sixth-highest sea cliffs in Europe, while Malin Head is the most northerly point on the island of Ireland.
The climate is temperate and dominated by the Gulf Stream, with warm, damp summers and mild wet winters. Two permanently inhabited islands, Arranmore and Tory Island, lie off the coast, along with a large number of islands with only transient inhabitants. Ireland's second longest river, the Erne, enters Donegal Bay near the town of Ballyshannon. The River Erne, along with other Donegal waterways, has been dammed to produce hydroelectric power. The River Foyle separates part of County Donegal from parts of both counties Londonderry and Tyrone.
Botany.
A survey of the macroscopic marine algae of County Donegal was published in 2003. The survey was compiled using the algal records held in the herbaria of the following institutions: the Ulster Museum, Belfast; Trinity College, Dublin; National University of Ireland, Galway, and the Natural History Museum, London.
Records of flowering plants include: "Dactylorhiza purpurella" (Stephenson and Stephenson) Soó.
Zoology.
The animals included in the county include Badgers ("Meles meles" L.)
There are habitats for the rare Corncrake in the county.
History.
At various times in its history, it has been known as County Tirconaill, County Tirconnell or County Tyrconnell (Irish: "Tír Chonaill"). The former was used as its official name during 1922–1927. This is in reference to both the old "túath" of Tír Chonaill and the earldom that succeeded it.
County Donegal is famous for being the home of the once mighty Clann Dálaigh, whose most famous branch were the Clann Ó Domhnaill, better known in English as the O'Donnell Clan. Until around 1600, the O'Donnells were one of Ireland's richest and most powerful "Gaelic" (native Irish) ruling-families. Within the Province of Ulster only the Clann Uí Néill (known in English as the O'Neill Clan) of modern County Tyrone were more powerful. The O'Donnells were Ulster's second most powerful "clan" or ruling-family from the early 13th-century through to the start of the 17th-century. For several centuries the O'Donnells ruled Tír Chonaill, a Gaelic kingdom in West Ulster that covered almost all of modern County Donegal. The head of the O'Donnell family had the titles "An Ó Domhnaill" (meaning "The O'Donnell" in English) and "Rí Thír Chonaill" (meaning "King of Tír Chonaill" in English). Based at Donegal Castle in "Dún na nGall" (modern Donegal Town), the O'Donnell "Kings of Tír Chonaill" were traditionally "inaugurated" at Doon Rock near Kilmacrenan. O'Donnell royal or "chiefly" power was finally ended in what was then the newly created County Donegal in September 1607, following the Flight of the Earls from near Rathmullan. The modern "County Arms of Donegal" (dating from the early 1970s) was influenced by the design of the old O'Donnell royal arms. The "County Arms" is the official coat of arms of both County Donegal and Donegal County Council.
The modern County Donegal was shired by order of the English Crown in 1585. The English authorities at Dublin Castle formed the new county by amalgamating the old Kingdom of Tír Chonaill with the old Lordship of Inishowen. However, the English authorities were unable to establish control over Tír Chonaill and Inishowen until after the Battle of Kinsale in 1602. Full control over the new County Donegal was only achieved after the Flight of the Earls in September 1607. It was the centre of O'Doherty's Rebellion of 1608 with the key Battle of Kilmacrennan taking place there. The county was one of those 'planted' during the Plantation of Ulster from around 1610 onwards. What became the City of Derry was officially part of County Donegal up until 1610.
County Donegal was one of the worst affected parts of Ulster during the Great Famine of the late 1840s in Ireland. Vast swathes of the county were devastated by this catastrophe, many areas becoming permanently depopulated. Vast numbers of County Donegal's people emigrated at this time, chiefly through Londonderry Port.
The Partition of Ireland in the early 1920s had a massive direct impact on County Donegal. Partition cut the county off, economically and administratively, from Derry, which had acted for centuries as the county's main port, transport hub and financial centre. Derry, together with west Tyrone, was henceforward in a new, different jurisdiction officially called Northern Ireland. Partition also meant that County Donegal was now almost entirely cut off from the rest of the jurisdiction in which it now found itself, the new dominion called the Irish Free State, which in April 1949 became the Republic of Ireland. Only a few miles of the county is physically connected by land to the rest of the Republic. The existence of a border cutting Donegal off from her natural hinterlands in Derry City and West Tyrone greatly exacerbated the economic difficulties of the county after partition. The county's economy is particularly susceptible, just like that of Derry City, to the currency fluctuations of the Euro against sterling.
Added to all this, in the late 20th century County Donegal was adversely affected by The Troubles in Northern Ireland. The county suffered several bombings and assassinations. In June 1987, Constable Samuel McClean, a Donegal man who was a serving member of the Royal Ulster Constabulary, was shot dead by the Provisional Irish Republican Army at his family home near Drumkeen. In May 1991, the prominent Sinn Féin politician Councillor Eddie Fullerton was assassinated by the Ulster Defence Association at his home in Buncrana. This added further to the economic and social difficulties of the county. However, the greater economic and administrative integration following the Good Friday Agreement of April 1998 has been of benefit to the county.
It has been labelled the 'forgotten county' by its own politicians, owing to the perception that it is ignored by the Irish Government, even in times of crisis.
Irish language.
Much of the county is seen as being a bastion of Gaelic culture and the Irish language, the Donegal Gaeltacht being the second-largest in the country. The version of the Irish language spoken in County Donegal is Ulster Irish.
Of the Gaeltacht population of 24,744, 16% of the county's total, 17,132 say they can speak Irish. There are three Irish-speaking parishes: Gweedore, The Rosses and Cloughaneely. Other Irish-speaking areas include Gaeltacht an Láir: Glencolmcille, Fanad and Rosguill, the islands of Aranmore, Tory Island and Inishbofin. Gweedore is the largest Irish-speaking parish, with over 5,000 inhabitants. All schools in the region use Irish as the language of instruction. One of the constituent colleges of NUI Galway, Acadamh na hOllscolaíochta Gaeilge, is based in Gweedore.
There are 1,005 students attending the five Gaelscoileanna and two Gaelcholáistí in the rest of the county. According to the 2006 Census, there are also 7,218 people who identify as being daily Irish speakers outside the Gaeltacht in the rest of the county.
Government and politics.
Donegal County Council (which has officially been in existence since 1899) has responsibility for local administration, and is headquartered at the County House in Lifford. The County Council runs alongside Town Councils in Letterkenny, Bundoran, Ballyshannon and Buncrana. Both the County Council and the Town Councils have elections every five years (alongside local elections nationally, and elections to the European Parliament), the last of which took place on 5 June 2009. Twenty-nine councillors are elected using the system of Proportional representation-Singe Transferable Vote (STV), across five electoral areas (also known as wards). These electoral areas are: Inishowen – 7 seats, Letterkenny – 7 seats, Donegal – 5 seats, Stranorlar – 5 seats, and Glenties – 5 seats.
For General elections, the county is divided into two constituencies, Donegal South–West and Donegal North–East, with both having three representatives in Dáil Éireann. For elections to the European Parliament, the county is part of the Midlands–North-West constituency.
Freedom of Donegal.
The Freedom of Donegal is an award that is given to people who have been recognised for outstanding achievements on behalf of the people and county of Donegal. Such people include Daniel O' Donnell, Phil Coulter, Shay Given, Packie Bonner, Paddy Crerand and the Brennan family.
In 2009 the members of the 28th Infantry Battalion of the Irish Defence Forces were also awarded the Freedom of the County from Donegal County Council ""in recognition of their longstanding service to the County of Donegal"."
Access.
An extensive rail network used to exist throughout the county and was mainly operated by the County Donegal Railways Joint Committee and the Londonderry and Lough Swilly Railway Company (known as the L. & L.S.R. or the Lough Swilly Company for short). Unfortunately all theses lines were laid to a 3-foot gauge where the connecting lines were all laid to the Irish standard gauge of . This meant that all goods had to be transhipped at Derry and Strabane. Like all narrow gauge railways this became a major handicap after World War 1 when road transport began to seriously erode the railways goods traffic. By 1953 the Lough Swilly had closed its entire railway system and become a bus and road haulage concern. The County Donegal lasted until 1960 as it had largely dieselised its passenger trains by 1951. By the late 1950s major work was required to upgrade the track and the Irish Government was unwilling to supply the necessary funds, so 'the Wee Donegal', as it was affectionally known, was closed in 1960. The Great Northern Railway (Ireland) L.t.d. (the G.N.R.) also ran a line from Strabane through The Laggan, a district in the east of the county, along the River Foyle into Derry. However, the railway network within County Donegal was completely closed by 1960. Today, the closest railway station to the county is Waterside Station in the City of Derry, which is operated by Northern Ireland Railways (N.I.R.). Train services along the Belfast-Derry railway line run, via Coleraine, to Belfast Central and Belfast Great Victoria Street.
County Donegal is served by both Donegal Airport, located at Carrickfinn in The Rosses in the west of the county, and by City of Derry Airport, located at Eglinton to the east. The nearest main international airport to the county is Belfast International Airport (popularly known as Aldergrove Airport), which is located to the east at Aldergrove, near Antrim Town, in County Antrim, 92 km from Derry City and 127 km from Letterkenny.
Culture.
The variant of the Irish language spoken in Donegal shares many traits with Scottish Gaelic. The Irish spoken in the Donegal Gaeltacht (Irish-speaking area) is of the Ulster dialect, while Inishowen (parts of which only became English-speaking in the early 20th century) used the East Ulster dialect. Ulster Scots is often spoken in both the Finn Valley and The Laggan district of East Donegal. Donegal Irish has a strong influence on learnt Irish across Ulster.
Like other areas on the western seaboard of Ireland, Donegal has a distinctive fiddle tradition which is of world renown. Donegal is also well known for its songs which have, like the instrumental music, a distinctive sound. Donegal musical artists such as the bands Clannad, The Pattersons, and Altan and solo artist Enya, have had international success with traditional or traditional flavoured music. Donegal music has also influenced people not originally from the county including folk and pop singers Paul Brady and Phil Coulter. Singer Daniel O'Donnell has become a popular ambassador for the county. Popular music is also common, the county's most acclaimed rock artist being the Ballyshannon-born Rory Gallagher. Other famous acts to come out of Donegal include folk-rock band Goats Don't Shave, Eurovision contestant Mickey Joe Harte and indie rock group The Revs and in more recent years bands such as In Their Thousands and Mojo Gogo have featured on the front page of "Hot Press" magazine.
Donegal has a long literary tradition in both Irish and English. The famous Irish navvy-turned-novelist Patrick MacGill, author of many books about the experiences of Irish migrant itinerant labourers in Britain at around the start of the 20th century, such as "The Rat Pit" and the autobiographical "Children of the Dead End", is from the Glenties area. There is a literary summer school in Glenties named in his honour. The novelist and socialist politician Peadar O'Donnell hailed from The Rosses in west Donegal. The poet William Allingham was also from Ballyshannon. Modern exponents include the Inishowen playwright and poet Frank McGuinness and the playwright Brian Friel. Many of Friel's plays are set in the fictional Donegal town of Ballybeg.
Authors in Donegal have been creating works, like the Annals of the Four Masters, in Gaelic and Latin since the Early Middle Ages. The Irish philosopher John Toland was born in Inishowen in 1670. He was thought of as the original freethinker by George Berkeley. Toland was also instrumental in the spread of freemasonry throughout Continental Europe. In modern Irish Donegal has produced famous, and sometimes controversial, authors such as the brothers Séamus Ó Grianna and Seosamh Mac Grianna from The Rosses and the contemporary (and controversial) Irish-language poet Cathal Ó Searcaigh from Gortahork in Cloughaneely, and where he is known to locals as "Gúrú na gCnoc" ('the Guru of the Hills').
Although approximately 85% of its population is Catholic, County Donegal also has a sizeable Protestant minority. Most Donegal Protestants would trace their ancestors to settlers who arrived during the Plantation of Ulster in the early 17th-century. The Church of Ireland is the largest Protestant denomination but is closely rivalled by a large number of Presbyterians. The areas of Donegal with the highest percentage of Protestants are The Laggan area of East Donegal around Raphoe, the Finn Valley and areas around Ramelton, Milford and Dunfanaghy – where their proportion reaches up to 30–45 percent. There is also a large Protestant population between Donegal Town and Ballyshannon in the south of the county. In absolute terms, Letterkenny has the largest number of Protestants (over 1000) and is the most Presbyterian town (among those settlements with more than 3000 people) in the Republic of Ireland. Some County Donegal Protestants (mainly those concentrated in The Laggan, the Finn Valley, Inishowen and the Donegal Town/Ballintra areas) are members of the Orange Order.
The Earagail Arts Festival is held within the county each July.
People from Donegal have also contributed to culture elsewhere. Francis Alison was one of the founders of the College of Philadelphia, which would later become the University of Pennsylvania. The Rev. Francis Makemie (originally from Ramelton) founded the Presbyterian Church in America. The Rev. David Steele, from Upper Creevaugh, was a prominent Reformed Presbyterian, or Covenanter, minister who emigrated to the United States in 1824. The Rt. Rev. Dr Charles Inglis, who was the first Church of England Bishop of the Diocese of Nova Scotia, was the third son of The Rev. Archibald Inglis, the Rector in Glencolumbkille.
Places of interest.
County Donegal is a favoured destination for many travellers. One of the attractions is Glenveagh National Park (formerly part of the Glenveagh Estate), as yet (March 2012) the only official "national park" anywhere in the Province of Ulster. The park is a 140 km² (about 35,000 acre) nature reserve with scenery of mountains, raised boglands, lakes and woodlands. At its heart is Glenveagh Castle, a late Victorian 'folly' that was originally built as a summer residence.
The Donegal Gaeltacht (Irish-speaking district) also attracts young people to County Donegal each year during the school summer holidays. The three-week-long summer Gaeltacht courses give young Irish people from other parts of the country a chance to learn the Irish language and traditional Irish cultural traditions that are still prevalent in parts of Donegal. The Donegal Gaeltacht has traditionally been a very popular destination each summer for young people from Northern Ireland. Scuba diving is also very popular with a club being located in Donegal Town.
Education.
Third-level education within the county is provided by Letterkenny Institute of Technology (L.Y.I.T.; popularly known locally as 'the Regional'), established in the 1970s in Letterkenny. In addition, many young people from the county attend third-level institutions elsewhere in Ireland, especially in Derry and also at the University of Ulster at Coleraine (U.U.C.), the University of Ulster at Jordanstown (U.U.J.), The Queen's University of Belfast ('Queen's'), and NUI Galway. Many Donegal students also attend the Limavady Campus of the North West Regional College (popularly known as Limavady Tech) and the Omagh Campus of South West College (popularly known as Omagh Tech or Omagh College).
Sport.
Gaelic football and hurling.
The Gaelic Athletic Association (G.A.A.) sport of Gaelic football is very popular in County Donegal. Donegal's inter-county football team have won the All-Ireland Senior Football Championship title twice (in 1992 and 2012). Donegal emerged victorious from the 2012 All-Ireland Senior Football Championship Final on 23 September 2012 to take the Sam Maguire Cup for only the second time, with early goals from Michael Murphy and Colm McFadden setting up victory of 2–11 to 0–13 over Mayo. In 2007, Donegal won only their second national title by winning the National Football League. On 24 April 2011, Donegal added their third national title when they defeated Laois to capture the National Football League Division Two. There are 16 clubs in the Donegal Senior Football Championship, with many others playing at a lower level.
Hurling, handball and rounders are also played but are less widespread, as in other parts of northwestern Ireland. The Donegal county senior hurling won the Lory Meagher Cup in 2011 and the Nicky Rackard Cup in 2013
Rugby Union.
There are several rugby teams in the county. These include Ulster Qualifying League Two side Letterkenny RFC, whose ground is named after Dave Gallaher, the captain of the 1905 New Zealand All Blacks touring team, who have since become known as The Originals. He was born in nearby Ramelton.
Ulster Qualifying League Three sides include Ballyshannon RFC, Donegal Town RFC and Inishowen RFC. Finn Valley RFC and Tir Chonaill RFC both compete in the Ulster Minor League North.
Association football.
Finn Harps plays in the League of Ireland and won promotion to the Premier Division in 2007 following a 6–3 aggregate win in the playoff final. They are now back alongside their arch-rivals Derry City F.C., with whom they contest Ireland's "North-West Derby". There are numerous other clubs in Donegal, but none has achieved the status of Finn Harps.
Golf.
Many people travel to Donegal for the superb golf links—long sandy beaches and extensive dune systems are a feature of the county, and many links courses have been developed.
Golf is a very popular sport within the county, including world class golf courses such as Ballyliffin (Glashedy), Ballyliffin (Old), both of which are located in the Inishowen peninsula. Other courses to note are Murvagh (located outside Donegal Town) and Rosapenna (Sandy Hills) located in Downings (near Carrigart). The Glashedy Links has been ranked 6th in a recent ranking taken by Golf Digest on the best courses in Ireland. The Old links was ranked 28th, Murvagh 36th and Sandy Hills 38th.
Mountain Biking.
Because of Donegal's hilly and mountain landscape, Mountain Biking has become a significant and growing interest. The Donegal Mountain Bike Club is the newest Mountain Bike club in Donegal, and held its first race on 31 August 2011. The 'Bogman Race' was entered by more than 50 people from different backgrounds of cycling. Due to the overwhelming popularity of their first ever race, the club plans to organise more races in the near future over different seasons, and aims to make it a major tourist attraction throughout Donegal.
Cricket.
Cricket is also played in County Donegal. This sport is chiefly confined to The Laggan district and the Finn Valley in the east of the county. The town of Raphoe and the nearby village of St. Johnston, both in The Laggan, are the traditional strongholds of cricket within the county. The game is mainly played and followed by members of County Donegal's Protestant community.
Other sports.
Donegal's rugged landscape lends itself to active sports like climbing, hillwalking, surfing and kite-flying.
Rock climbing is of very high quality and still under-developed in the county. There is a wealth of good quality climbs in the county, from granite rocks in the south to quartzite and dolerite in the north; from long mountain routes in the Poisoned Glen to boulder challenges of excellent quality in the west and in the Inishowen Peninsula. The current Donegal rock climbing guidebook contains over 2800 recorded rock climbs covering the entire county including the Donegal Sea Stacks, Malinbeg, Gola, and Cruit Islands.
Surfing on Donegal's Atlantic coast is considered to be as good as any in Ireland. The seaside resort of Bundoran, located in the very south of the county, along with nearby Rossnowlagh, have been 'reborn' as the centre of surfing in County Donegal. Indeed, these areas are renowned as the main surfing centres in Ulster.
Greyhound racing based in Lifford is home to the Lifford Greyhound Racing Stadium, a state of the art stadium built at a cost of €12 million and also the East Donegal Coursing Club.
Panoramic view of Mount Errigal's summit.
People.
See also: .
A
B
C
D
E
G
H
I
J
L
Mac/Mc
M
Ballybofey.
O
P
R
S
T
Surnames.
Most common surnames in County Donegal at the time of the United Kingdom Census of 1901:

</doc>
<doc id="45579" url="http://en.wikipedia.org/wiki?curid=45579" title="Queens">
Queens

Queens is the easternmost and largest in area of the five boroughs of New York City, geographically adjacent to the borough of Brooklyn at the western end of Long Island. Coterminous with Queens County since 1899, the borough of Queens is the second-largest in population (behind Brooklyn), with a Census-estimated 2,321,580 residents in 2014, approximately 48% of them foreign-born. Queens County is also the second most populous county in New York State, behind neighboring Kings County, which is coterminous with the borough of Brooklyn. Queens is the fourth-most densely populated county among New York City's boroughs, as well as in the United States; and if each New York City borough were an independent city, Queens would also be the nation's fourth most populous city, after Los Angeles, Chicago, and Brooklyn. Queens is the most ethnically diverse urban area in the world.
The differing character in the neighborhoods of Queens is reflected by its diverse housing stock ranging from high-rise apartment buildings, especially prominent in the more densely urban areas of western and central Queens, such as Jackson Heights, Flushing, Astoria, and Long Island City, to large, free-standing single-family homes, common in the eastern part of the borough, in neighborhoods that have a more suburban layout like neighboring Nassau County, such as Little Neck, Douglaston, and Bayside.
Queens has the second-largest and most diversified economy of all the five boroughs of New York City; the borough is home to two of the three busiest New York City metropolitan area airports (and both major airports in New York City proper), JFK International Airport and LaGuardia Airport. These airports are among the busiest in the world, causing the airspace above Queens to be the most congested in the country. Attractions in Queens include Flushing Meadows Park—home to the New York Mets baseball team and the US Open tennis tournament—Kaufman Astoria Studios, Silvercup Studios, and Aqueduct Racetrack.
Queens was established in 1683 as one of the original 12 counties of New York and was named for the Portuguese Princess Catherine of Braganza (1638–1705), who was at the time queen of England, Scotland, and Ireland.
Queens became a borough of New York City in 1898. From 1683 until 1899, the County of Queens included what is now Nassau County.
History.
Colonial and post-colonial history.
European colonization brought Dutch and English settlers, as a part of the New Netherland colony. First settlements occurred in 1635 followed by early colonizations at Maspeth in 1642,
and Vlissingen (now Flushing) in 1643.
Other early settlements included Newtown (now Elmhurst) and Jamaica. However, these towns were mostly inhabited by English settlers from New England via eastern Long Island (Suffolk County) subject to Dutch law. After the capture of the colony by the English and its renaming as New York in 1664, the area (and all of Long Island) became known as Yorkshire.
The Flushing Remonstrance signed by colonists in 1657 is considered a precursor to the United States Constitution's provision on freedom of religion in the Bill of Rights. The signers protested the Dutch colonial authorities' persecution of Quakers in what is today the borough of Queens.
Originally, Queens County included the adjacent area now comprising Nassau County. It was an original county of New York State, one of twelve created on November 1, 1683. It is believed that the county was named after Catherine of Braganza, since she was queen of England at the time. The county was founded alongside Kings County (Brooklyn, which was named after her husband, King Charles II), and Richmond County (Staten Island, named after his illegitimate son, the 1st Duke of Richmond).
On October 7, 1691, all counties in the Colony of New York were redefined. Queens gained North Brother Island, South Brother Island, and Huletts Island (today known as Rikers Island).
On December 3, 1768, Queens gained other islands in Long Island Sound that were not already assigned to a county but that did not abut on Westchester County (today's Bronx County).
Queens played a minor role in the American Revolution, as compared to Brooklyn, where the Battle of Long Island was largely fought. Queens, like the rest of Long Island, remained under British occupation after the Battle of Long Island in 1776 and was occupied throughout most of the rest of the war. Under the Quartering Act, British soldiers used, as barracks, the public inns and uninhabited buildings belonging to Queens residents. Even though many local people were against unannounced quartering, sentiment throughout the county remained in favor of the British crown. The quartering of soldiers in private homes, except in times of war, was banned by the Third Amendment to the United States Constitution. Nathan Hale was captured by the British on the shore of Flushing Bay in Queens before being executed by hanging in Manhattan for gathering intelligence.
From 1683 until 1784, Queens County consisted of five towns: Flushing, Hempstead, Jamaica, Newtown, and Oyster Bay. On April 6, 1784, a sixth town, the Town of North Hempstead, was formed through secession by the northern portions of the Town of Hempstead.
The seat of the county government was located first in Jamaica,
but the courthouse was torn down by the British during the American Revolution to use the materials to build barracks.
After the war, various buildings in Jamaica temporarily served as courthouse and jail until a new building was erected about 1787 (and later completed) in an area near Mineola (now in Nassau County) known then as Clowesville.
The 1850 census was the first in which the population of the three western towns exceeded that of the three eastern towns that are now part of Nassau County. Concerns were raised about the condition and distance of the old courthouse, and several sites were in contention for the construction of a new one.
In 1870, Long Island City split from the Town of Newtown, incorporating itself as a city, consisting of what had been the Village of Astoria and some unincorporated areas within the Town of Newtown. Around 1874, the seat of county government was moved to Long Island City from Mineola.
On March 1, 1860, the eastern border between Queens County (later Nassau County) and Suffolk County was redefined with no discernible change.
On June 8, 1881, North Brother Island was transferred to New York County.
On May 8, 1884, Rikers Island was transferred to New York County.
In 1885, Lloyd Neck, which was part of the Town of Oyster Bay and was earlier known as Queens Village, seceded from Queens and became part of the Town of Huntington in Suffolk County.
On April 16, 1964, South Brother Island was transferred to Bronx County.
Incorporation as borough.
The New York City Borough of Queens was authorized on May 4, 1897, by a vote of the New York State Legislature after an 1894 referendum on consolidation.
The eastern 280 sqmi of Queens that became Nassau County was partitioned on January 1, 1899.
Queens Borough was established on January 1, 1898.
Long Island City, the towns of Newtown, Flushing, and Jamaica, and the Rockaway Peninsula portion of the Town of Hempstead were merged to form the new borough, dissolving all former municipal governments (Long Island City, the county government, all towns, and all villages) within the new borough.
The areas of Queens County that were not part of the consolidation plan,
consisting of the towns of North Hempstead and Oyster Bay, and the major remaining portion of the Town of Hempstead, remained part of Queens County until they seceded to form the new Nassau County on January 1, 1899. At this point, the boundaries of Queens County and the Borough of Queens became coterminous. With consolidation, Jamaica once again became the county seat, though county offices now extend to nearby Kew Gardens also.
The borough's administrative and court buildings are presently located in Kew Gardens and downtown Jamaica respectively, two neighborhoods that were villages of the former Town of Jamaica.
From 1905 to 1908 the Long Island Rail Road in Queens became electrified. Transportation to and from Manhattan, previously by ferry or via bridges in Brooklyn, opened up with the Queensboro Bridge finished in 1909, and with railway tunnels under the East River in 1910. From 1915 onward, much of Queens was connected to the New York City Subway system.
With the 1915 construction of the Steinway Tunnel carrying the IRT Flushing Line between Queens and Manhattan, and the robust expansion of the use of the automobile, the population of Queens more than doubled in the 1920s, from 469,042 in 1920 to 1,079,129 in 1930.
In later years, Queens was the site of the 1939 New York World's Fair and the 1964 New York World's Fair. LaGuardia Airport, in northern Queens, opened in 1939. Idlewild Airport, in southern Queens and now called JFK Airport, opened in 1948. American Airlines Flight 587 took off from the latter airport on November 12, 2001, but ended up crashing in Queens' Belle Harbor area, killing 265 people. In late October 2012, much of Queens's Breezy Point area was destroyed by a massive six-alarm fire caused by Hurricane Sandy.
Geography.
Queens is located on the far western portion of geographic Long Island and includes a few smaller islands, most of which are in Jamaica Bay, forming part of the Gateway National Recreation Area, which in turn is one of the National Parks of New York Harbor. According to the U.S. Census Bureau, Queens County has a total area of 178 sqmi, of which 109 sqmi is land and 70 sqmi (39%) is water.
Brooklyn, the only other New York City borough on geographic Long Island, lies just south and west of Queens, with Newtown Creek, an estuary that flows into the East River, forming part of the border. To the west and north is the East River, across which is Manhattan to the west and The Bronx to the north. Nassau County is east of Queens on Long Island. Staten Island is southwest of Brooklyn, and shares only a 3-mile-long water border (in the Outer Bay) with Queens.
The Rockaway Peninsula, the most southernly part of all of Long Island, sits between Jamaica Bay and the Atlantic Ocean, featuring the most prominent public beaches in Queens. Flushing Bay and the Flushing River are in the north, connecting to the East River. The East River opens into Long Island Sound. The midsection of Queens is crossed by the Long Island straddling terminal moraine created by the Wisconsin Glacier.
Climate.
Queens and the rest of New York City has a humid subtropical climate (Cfa) with partial shielding from the Appalachian Mountains and moderating influences from the Atlantic Ocean. Queens receives plentiful precipitation all year round with 44.8 inches yearly. Extremes range from 107 °F (41.6 °C) to -3 °F (-19.4 °C). Winters are relatively mild compared to other areas of New York State, though snow is common and blizzards occur about every 4–6 years. Springs are unpredictable and can be chilly to very warm. Summers are hot, humid, and wet. Autumn is similar to spring, while snowfall generally begins in December.
Neighborhoods.
Four United States Postal Service post offices serve Queens, based roughly on those serving the towns in existence at the consolidation of the five boroughs into New York City: Long Island City (ZIP codes starting with 111), Jamaica (114), Flushing (113), and Far Rockaway (116). In addition the Floral Park post office (110), based in Nassau County, serves a small part of northeastern Queens. Each of these main post offices have neighborhood stations with individual ZIP codes, and unlike the other boroughs, these station names are often used in addressing letters. These ZIP codes do not always reflect traditional neighborhood names and boundaries; "East Elmhurst", for example, was largely coined by the USPS and is not an official community. Most neighborhoods have no solid boundaries. The Forest Hills and Rego Park neighborhoods, for instance, overlap.
Residents of Queens often closely identify with their neighborhood rather than with the borough or city. The borough is a patchwork of dozens of unique neighborhoods, each with its own distinct identity:
Demographics.
Population estimates.
Since 2010, the population of Queens was estimated by the Census Bureau to have increased 2.9% to 2,296,175 as of 2013, representing 27.3% of New York City's population, 29.7% of Long Island's population, and 11.7% of New York State's population.
According to 2012 census estimates, 27.2% of the population was Non-Hispanic White, 20.9% Black or African American, 24.8% Asian, 12.9% from some other race, and 2.7% of two or more races. 27.9% of Queens's population was of Hispanic or Latino origin (of any race).
The census totals for Queens for 2012 and 2013 are questionable because the New York City Department of City Planning was alarmed by the negligible reported increase in population between 2000 and 2010. Areas with high proportions of immigrants and undocumented aliens are traditionally undercounted for a variety of reasons. New housing and transit statistics suggest otherwise but corrective formulas were not applied. The racial breakdown of the population is similarly suspect. Foreign born people frequently do not interpret racial definitions as the Census suggests.
As of the most stable census of 2000[ [update]], there were 2,229,379 people, 782,664 households, and 537,690 families residing in the county. The population density was 20,409.0 inhabitants per square mile (7,879.6/km²). There were 817,250 housing units at an average density of 7,481.6 per square mile (2,888.5/km²). The racial makeup of the county was 44.08% White, 20.01% Black or African American, 0.50% Native American, 17.56% Asian, 0.06% Pacific Islander, 11.68% from other races, and 6.11% from two or more races. 24.97% of the population were Hispanic or Latino of any race.
Ethnic groups.
In Queens, approximately 48.5% of the population was foreign-born as of 2010. Of that, 49.5% were born in Latin America, 33.5% in Asia, 14.8% in Europe, 1.8% in Africa, and 0.4% in North America. Roughly 2.1% of the population was born in Puerto Rico, a U.S. territory, or abroad to American parents. In addition, 51.2% of the population was born in the United States. Approximately 44.2% of the population over 5 years of age speak English at home; 23.8% speak Spanish at home. Also, 16.8% of the populace speak other Indo-European languages at home. Another 13.5% speak an Asian language at home.
Among the Asian population, people of Chinese ethnicity make up the largest ethnic group at 9.0% of Queens' population, with about 200,205 people; the other East and Southeast Asian groups are: Koreans (2.9%), Filipinos (1.7%), Japanese (0.3%), Thais (0.2%), Vietnamese (0.2%), and Indonesians and Burmese both make up 0.1% of the population. People of South Asian descent make up 7.8% of Queens' population: Indians (5.3%), Bangladeshi (1.5%), Pakistanis (0.7%), and Nepali (0.2%).
Among the Hispanic population, Puerto Ricans make up the largest ethnic group at 4.6%, next to Mexicans, who make up 4.2% of the population, and Dominicans at 3.9%. Central Americans make up 2.4% and are mostly Salvadorans. South Americans constitute 9.6% of Queens's population, mainly of Ecuadorian (4.4%) and Colombian descent (3.2%).
Some main European ancestries in Queens as of 2000 include:
The Hispanic or Latino population increased by 61% to 597,773 between 1990 and 2006 and now accounts for 26.5% of the borough's population. Queens is now home to hundreds of thousands of Latinos and Hispanics:
Queens is home to 49.6% of the city's Asian population. Among the five boroughs, Queens has the largest population of Chinese, Indian, Korean, Filipino, Bangladeshi and Pakistani Americans. Queens has the largest Asian American population by county outside the Western United States; according to the 2006 American Community Survey, Queens ranks fifth among US counties with 477,772 (21.18%) Asian Americans, behind Los Angeles County, California, Honolulu County, Hawaii, Santa Clara County, California, and Orange County, California.
The borough is also home to the largest concentration of Indian Americans in the nation, with a total population of 129,715 (5.79% of the borough population), as well as Pakistani Americans, who number at 15,604. Queens has the second largest Sikh population in the nation after California.
Queens has the third largest Bosnian population in the United States behind only St. Louis and Chicago, numbering more than 15,000.
According to author Mordecai Plaut, a 2011 UJA/Federation of New York study found that Queens was home to 198,000 Jewish Americans, up from 186,000 in 2002.
There were 782,664 households out of which 31.5% had children under the age of 18 living with them, 46.9% were married couples living together, 16.0% had a female householder with no husband present, and 31.3% were non-families. 25.6% of all households were made up of individuals and 9.7% had someone living alone who was 65 years of age or older. The average household size was 2.81 and the average family size was 3.39.
In the county the population was spread out with 22.8% under the age of 18, 9.6% from 18 to 24, 33.1% from 25 to 44, 21.7% from 45 to 64, and 12.7% who were 65 years of age or older. The median age was 35 years. For every 100 females there were 92.9 males. For every 100 females age 18 and over, there were 89.6 males.
The median income for a household in the county was $37,439, and the median income for a family was $42,608. Males had a median income of $30,576 versus $26,628 for females. The per capita income for the county was $19,222. About 16.9% of families and 24.7% of the population were below the poverty line, including 18.8% of those under age 18 and 13.0% of those age 65 or over. In Queens, the black population earns more than whites on average.
Many of these African Americans live in quiet, middle class suburban neighborhoods near the Nassau County border, such as Laurelton and Cambria Heights which have large black populations whose family income is higher than average. Those areas are known for their well kept homes, suburban feel, and low crime rate. The migration of European Americans from parts of Queens has been long ongoing with departures from Ozone Park, Woodhaven, Bellerose, Floral Park, and Flushing, etc. (most of the outgoing population has been replaced with Asian Americans). Neighborhoods such as Whitestone, College Point, North Flushing, Auburndale, Bayside, Middle Village, Little Neck, and Douglaston have not had a substantial exodus of white residents, but have seen an increase of Asian population (mostly Korean). Queens has recently experienced a real estate boom making most of its neighborhoods very desirable for people who want to reside near Manhattan in a less urban setting. According to a 2001 Claritas study, Queens is the most diverse county in the United States among counties of 100,000+ population.
Culture.
While Queens has not been the center of any major artistic movements, it has been the home of such notable artists as Tony Bennett, Francis Ford Coppola, Paul Simon, and Robert Mapplethorpe. The current poet laureate of Queens is Paolo Javier.
Queens has notably fostered African-American culture, with establishments such as The Afrikan Poetry Theatre and the Black Spectrum Theater Company catering specifically to African-Americans in Queens. In the 1940s, Queens was an important center of jazz; such jazz luminaries as Louis Armstrong, Charlie Parker, and Ella Fitzgerald took up residence in Queens, seeking refuge from the segregation they found elsewhere in New York. Additionally, many notable hip-hop acts hail from Queens, including Nas, Run-D.M.C., Kool G Rap, A Tribe Called Quest, LL Cool J, Mobb Deep, 50 Cent, Nicki Minaj, and Heems of Das Racist.
Queens hosts various museums and cultural institutions that serve its diverse communities. They range from the historical (such as the John Bowne House) to the scientific (such as the New York Hall of Science), from conventional art galleries (such as the Noguchi Museum) to unique graffiti exhibits (such as 5 Pointz). Queens's cultural institutions include, but are not limited to:
The travel magazine "Lonely Planet" also named Queens the No. 1 destination in the country for 2015 for its cultural and culinary diversity.
Languages.
There are 138 languages spoken in the borough. As of 2010, 43.84% (905,890) of Queens residents age 5 and older spoke English at home as a primary language, while 23.88% (493,462) spoke Spanish, 8.06% (166,570) Chinese, 3.44% (71,054) various Indic languages, 2.74% (56,701) Korean, 1.67% (34,596) Russian, 1.56% (32,268) Italian, 1.54% (31,922) Tagalog, 1.53% (31,651) Greek, 1.32% (27,345) French Creole, 1.17% (24,118) Polish, 0.96% (19,868) Hindi, 0.93% (19,262) Urdu, 0.92% (18,931) other Asian languages, 0.80% (16,435) other Indo-European languages, 0.71% (14,685) French, 0.61% (12,505) Arabic, 0.48% (10,008) Serbo-Croatian, and Hebrew was spoken as a main language by 0.46% (9,410) of the population over the age of five. In total, 56.16% (1,160,483) of Queens's population age 5 and older spoke a mother language other than English.
Food.
The cuisine available in Queens reflects its vast cultural diversity. The cuisine of a particular neighborhood often represents its demographics; for example, Astoria hosts many Greek restaurants, in keeping with its traditionally Greek population. Jackson Heights is known for its many Indian and Latin-American eateries.
Government.
Since New York City's consolidation in 1898, Queens has been governed by the New York City Charter that provides for a strong mayor-council system. The centralized New York City government is responsible for public education, correctional institutions, libraries, public safety, recreational facilities, sanitation, water supply, and welfare services in Queens.
The office of Borough President was created in the consolidation of 1898 to balance centralization with local authority. Each borough president had a powerful administrative role derived from having a vote on the New York City Board of Estimate, which was responsible for creating and approving the city's budget and proposals for land use. In 1989 the Supreme Court of the United States declared the Board of Estimate unconstitutional because Brooklyn, the most populous borough, had no greater effective representation on the Board than Staten Island, the least populous borough, a violation of the Fourteenth Amendment's Equal Protection Clause following the high court's 1964 "one man, one vote" decision.
Since 1990 the Borough President has acted as an advocate for the borough at the mayoral agencies, the City Council, the New York state government, and corporations. Queens' Borough President is Melinda Katz, elected in November 2013 as a Democrat with 80.3% of the vote . Queens Borough Hall is the seat of government and is located in Kew Gardens.
The Democratic Party holds most public offices. Sixty-three percent of registered Queens voters are Democrats. Local party platforms center on affordable housing, education and economic development. Controversial political issues in Queens include development, noise, and the cost of housing.
s of 2012[ [update]], there were four Democrats and one Republican representing Queens in the U.S. Congress:
Each of the city's five counties has its own criminal court system and District Attorney, the chief public prosecutor who is directly elected by popular vote. Richard A. Brown, who ran on both the Republican and Democratic Party tickets, has been the District Attorney of Queens County since 1991.
Queens has 12 seats on the New York City Council, the second largest number among the five boroughs. It is divided into 14 community districts, each served by a local Community Board. Community Boards are representative bodies that field complaints and serve as advocates for local residents.
Although Queens is heavily Democratic, it is considered a swing county in New York politics. Republican political candidates who do well in Queens usually win citywide or statewide elections. Republicans such as former Mayors Rudolph Giuliani and Michael Bloomberg won majorities in Queens. Republican State Senator Serphin Maltese represented a district in central and southern Queens for twenty years until his defeat in 2008 by Democratic City Councilman Joseph Addabbo. In 2002, Queens voted against incumbent Republican Governor of New York George Pataki in favor of his Democratic opponent, Carl McCall by a slim margin.
Queens has not voted for a Republican candidate in a presidential election since 1972, when Queens voters chose Richard Nixon over George McGovern. Since the 1996 presidential election, Democratic presidential candidates have received over 70% of the popular vote in Queens.
Economy.
Queens has the second-largest economy of New York City's five boroughs, second only to Manhattan. In 2004, Queens had 15.2% (440,310) of all private sector jobs in New York City and 8.8% of private sector wages. Queens has the most diversified economy of the five boroughs, with evenly spread jobs across the health care, retail trade, manufacturing, construction, transportation, and film and television production sectors. No single sector is overwhelmingly dominant.
The diversification in Queens' economy is reflected in the large amount of employment in the export-oriented portions of its economy—such as transportation, manufacturing, and business services—that serve customers outside the region. This accounts for more than 27% of all Queens jobs and offers an average salary of $43,727, 14% greater than that of jobs in the locally oriented sector.
The borough's largest employment sector—trade, transportation, and utilities—accounted for nearly 30% of all jobs in 2004. Queens is home to two of the three major New York City area airports, JFK International Airport and LaGuardia Airport. These airports are among the busiest in the world, leading the airspace above Queens to be the most congested in the country. This airline industry is particularly important to the economy of Queens, providing almost one quarter of the sector's employment and more than 30% of the sector's wages.
Education and health services is the next largest sector in Queens and comprised almost 24% of the borough's jobs in 2004. The manufacturing and construction industries in Queens are the largest of the City and account for nearly 17% of the borough's private sector jobs. Comprising almost 17% of the jobs in Queens is the information, financial activities, and business and professional services sectors.
s of 2003[ [update]], Queens had almost 40,000 business establishments. Small businesses act as an important part of the borough's economic vitality with two thirds of all business employing between one to four people.
Several large companies have their headquarters in Queens, including watchmaker Bulova, based in East Elmhurst; internationally renowned piano manufacturer Steinway & Sons in Astoria; Glacéau, the makers of Vitamin Water, headquartered in Whitestone; and JetBlue Airways, an airline based in Long Island City.
Long Island City is a major manufacturing and back office center. Flushing is a major commercial hub for Chinese American and Korean American businesses, while Jamaica is the major civic and transportation hub for the borough.
Sports.
Citi Field, home ballpark of the New York Mets of Major League Baseball is located in Flushing Meadows-Corona Park. Shea Stadium, the former home of the Mets and the New York Jets of the National Football League, as well as the temporary home of the New York Yankees and the New York Giants Football Team stood where Citi Field's parking lot is now located. The US Open tennis tournament is played at the USTA Billie Jean King National Tennis Center, located just south of Citi Field. Arthur Ashe Stadium is the biggest tennis stadium in the world. The US Open was formerly played at the West Side Tennis Club in Forest Hills. Queens is also the home of Aqueduct Racetrack, located in Ozone Park. Extreme Championship Wrestling has been held at an Elks lodge in Elmhurst.
Transportation.
Queens has crucial importance in international and interstate air traffic. Two of the New York metropolitan area's three major airports are located there; LaGuardia Airport is in northern Queens, while John F. Kennedy International Airport is to the south on the shores of Jamaica Bay. 
Twelve New York City Subway routes traverse Queens, serving 81 stations on seven main lines. The A, G, J, M, and Z routes connect Queens to Brooklyn without going through Manhattan first. The F, M, N, Q, and R trains connect Queens and Brooklyn via Manhattan, while the E, 7, and 7d trains connect Queens to Manhattan only. It is of note that M trains travel through Queens twice in the same trip, as both terminals are in Queens.
A commuter train system, the Long Island Rail Road, operates 22 stations in Queens with service to Manhattan, Brooklyn, and Long Island. Jamaica station is a hub station where all the lines in the system but one (the Port Washington Branch) converge. It is the busiest commuter rail hub in the United States. Sunnyside Yard is used as a staging area by Amtrak and New Jersey Transit for intercity and commuter trains from Penn Station in Manhattan. 61st Street – Woodside acts as one of the many LIRR connections to the New York City Subway. The elevated AirTrain people mover system connects JFK International Airport to the New York City Subway and the Long Island Rail Road; a separate AirTrain system is planned alongside the Grand Central Parkway to connect LaGuardia Airport to these transit systems.
About 100 local bus routes operate within Queens, and another 15 express routes shuttle commuters between Queens and Manhattan, under the MTA New York City Bus and MTA Bus brands.
Queens is traversed by three trunk east-west highways. The Long Island Expressway (Interstate 495) runs from the Queens Midtown Tunnel on the west through the borough to Nassau County on the east. The Grand Central Parkway, whose western terminus is the Triborough Bridge, extends east to the Queens/Nassau border, where its name changes to the Northern State Parkway. The Belt Parkway begins at the Gowanus Expressway in Brooklyn, and extends east into Queens, past Aqueduct Racetrack and JFK Airport. On its eastern end at the Queens/Nassau border, it splits into the Southern State Parkway which continues east, and the Cross Island Parkway which turns north.
There are also several major north-south highways in Queens, including the Brooklyn-Queens Expressway (Interstate 278), the Van Wyck Expressway (Interstate 678), the Clearview Expressway (Interstate 295), and the Cross Island Parkway.
Streets.
The streets of Queens are laid out in a semi-grid system, with a numerical system of street names (similar to Manhattan and the Bronx). Nearly all roadways oriented north-south are "Streets", while east-west roadways are "Avenues", beginning with the number 1 in the west for Streets and in the north for Avenues. In some parts of the borough, several consecutive streets may share numbers (for instance, 72nd Street followed by 72nd Place and 72nd Lane, or 52nd Avenue followed by 52nd Road, 52nd Drive, and 52nd Court), often causing confusion for non-residents. In addition, incongruous alignments of street grids, unusual street paths due to geography, or other circumstances often lead to the skipping of numbers (for instance, on Ditmars Boulevard, 70th Street is followed by Hazen Street which is followed by 49th Street).
The structure of a Queens address was designed to provide convenience in locating the address itself; the first half of a number in a Queens address refers to the nearest cross street, the second half refers to the house or lot number from where the street begins from that cross street, followed by the name of the street itself. For example, to find an address in Queens, 14-01 120th Street, one could ascertain from the address structure itself that the listed address is at the intersection of 14th Avenue and 120th Street, and that the address must be closest to 14th Avenue rather than 15th Avenue, as it is the first lot on the block. This structure doesn't stop when a street is named either, assuming that there is an existing numbered cross-street. For example, Queens College is situated at 65–30 Kissena Boulevard, and is so named because the cross-street closest to the entrance is 65th Avenue.
This confusion stems from the fact that many of the village street grids of Queens had only worded names, some were numbered according to local numbering schemes, and some had a mix of words and numbers. In the early 1920s a "Philadelphia Plan" was instituted to overlay one numbered system upon the whole borough. The Topographical Bureau, Borough of Queens, worked out the details. Subway stations were only partly renamed, thus now share dual names after the original street names. On the IRT Flushing Line in Sunnyside, there are 33rd – Rawson St., 40th – Lowery St., 46th – Bliss St., 52nd St. – Lincoln Ave. and so forth. Numbered roads tend to be residential, although numbered commercial streets are not rare.
A fair number of streets that were country roads in the 18th and 19th centuries (especially major thoroughfares such as Northern Boulevard, Queens Boulevard, Hillside Avenue, and Jamaica Avenue) carry names rather than numbers, typically though not uniformly called "Boulevards" or "Parkways".
The Rockaway Peninsula does not follow the same system as the rest of the borough and has its own numbering system. Streets are numbered in ascending order heading west from near the Nassau County border, and are prefixed with the word "Beach." Streets at the easternmost end, however, are nearly all named. Streets in Bayswater, which is on Jamaica Bay, has its numbered streets prefixed with the word "Bay" rather than "Beach". Another deviation from the norm is Broad Channel; it maintains the north-south numbering progression but uses only the suffix "Road," as well as the prefixes "West" and "East," depending on location relative to Cross Bay Boulevard, the neighborhood's major through street. Broad Channel's streets were a continuation of the mainland Queens grid in the 1950s; formerly the highest numbered avenue in Queens was 208th Avenue rather today's 165th Avenue in Howard Beach & Hamilton Beach.
The other exception is the neighborhood of Ridgewood, which for the most part shares a grid and house numbering system with the Brooklyn neighborhood of Bushwick. The grid runs east-west from the LIRR Bay Ridge Branch right-of-way to Flushing Avenue; and north-south from Forest Avenue in Ridgewood to Bushwick Avenue in Brooklyn before adjusting to meet up with the Bedford-Stuyvesant grid at Broadway. All streets on the grid have names.
According to the 2000 Census, 37.7% of all Queens households did not own a car. The citywide rate is 55%.
In 2012, some numbered streets in the Douglaston Hill historic district of Queens were renamed to their original names, with 43rd Avenue becoming Pine Street.
Waterways.
Queens is connected to the Bronx by the Bronx Whitestone Bridge, the Throgs Neck Bridge, the Robert F. Kennedy Bridge and the Hell Gate Bridge. Queens is connected to Manhattan by the Robert F. Kennedy Bridge, the Queensboro Bridge, and the Queens Midtown Tunnel; and to Roosevelt Island by the Roosevelt Island Bridge.
While most of the Queens/Brooklyn border is on land, the Kosciuszko Bridge crosses the Newtown Creek connecting Maspeth to Greenpoint, Brooklyn. The Pulaski Bridge connects McGuinness Boulevard in Greenpoint to 11th Street, Jackson Avenue, and Hunters Point Avenue in Long Island City. The J. J. Byrne Memorial Bridge (a.k.a. Greenpoint Avenue Bridge) connects Greenpoint and Long Island City avenues of the same name, which, east of Queens Boulevard (NY-25), becomes Roosevelt Avenue. A lesser bridge connects Grand Avenue in Queens to Grand Street in Brooklyn.
The Cross Bay Veterans Memorial Bridge traverses Jamaica Bay to connect the Rockaway Peninsula to the rest of Queens. Marine Parkway–Gil Hodges Memorial Bridge links the western part of the Peninsula with Flatbush Avenue, Brooklyn's longest thoroughfare. Both crossings were built and continue to be operated by what is now known as MTA Bridges and Tunnels. The IND Rockaway Line parallels the Cross Bay, has a mid-bay station at Broad Channel which is just a short walk from the Jamaica Bay Wildlife Refuge, now part of Gateway National Recreation Area and a major stop on the Atlantic Flyway.
One year-round scheduled ferry service connects Queens and Manhattan. New York Water Taxi operates service across the East River from Hunters Point in Long Island City to Manhattan at 34th Street and south to Pier 11 at Wall Street.
In 2007, limited weekday service was begun between Breezy Point, the westernmost point in the Rockaways, to Pier 11 via the Brooklyn Army Terminal. Summertime weekend service provides service from Lower Manhattan and southwest Brooklyn to the peninsula's Gateway beaches.
In the aftermath of Hurricane Sandy on October 29, 2012, which caused massive infrastructure damage to the IND Rockaway Line ( trains) south of the station at Howard Beach – JFK Airport, severing all direct subway connections between the Rockaway Peninsula and Broad Channel, Queens and the Queens mainland for many months, ferry operator SeaStreak began running a city-subsidized ferry service between a makeshift ferry slip at Beach 108th Street and Beach Channel Drive in Rockaway Park, Queens and Pier 11/Wall Street, then continuing on to the East 34th Street Ferry Landing. In August 2013, a stop was added at Brooklyn Army Terminal. Originally intended as just a stopgap alternative transportation measure until subway service was restored to the Rockaways, the ferry proved to be popular with both commuters and tourists and was extended several times, as city officials evaluated the ridership numbers to determine whether to establish the service on a permanent basis. The fare was raised to $3.50 per ride during the extension period from $2 previously. Between its inception and December 2013, the service had carried close to 200,000 riders, city officials said. When the city government announced its budget in late June 2014 for the upcoming fiscal year beginning July 1, the ferry only received a $2 million further appropriation, enough to temporarily extend it again through October, but did not receive the approximately $8 million appropriation needed to keep the service running for the full fiscal year. Local officials and community activists expressed dismay with the decision, saying it was a blow to the Rockaways as the area continues to struggle economically in the aftermath of the 2012 hurricane. A spokesperson for the city government's Economic Development Corporation said that “We will continue to examine ridership and seek a sustainable funding stream that can support the $25-$30 subsidy per trip — the highest by far of any public transportation in the city.” Despite last-minute efforts by local transportation advocates, civic leaders and elected officials, ferry service ended on Oct. 31, 2014. They promised to continue efforts to have the service restored.
Education.
Elementary and secondary education.
Elementary and secondary school education in Queens is provided by a vast number of public and private institutions. Public schools in the borough are managed by the New York City Department of Education, the largest public school system in the United States. Most private schools are affiliated to or identify themselves with the Roman Catholic or Jewish religious communities. Townsend Harris High School is a Queens public magnet high school for the humanities consistently ranked as among the top 100 high schools in the United States.
Queens Library.
The Queens Borough Public Library is the public library system for the borough and one of three library systems serving New York City. Dating back to the foundation of the first Queens library in Flushing in 1858, the Queens Borough Public Library is one of the largest public library systems in the United States. Separate from the New York Public Library, it is composed of 63 branches throughout the borough. In fiscal year 2001, the Library achieved a circulation of 16.8 million. First in circulation in New York State since 1985, the Library has maintained the highest circulation of any city library in the country since 1985 and the highest circulation of any library in the nation since 1987. The Library maintains collections in many languages, including Spanish, Chinese, Korean, Russian, Haitian Creole, Polish, and six Indic languages, as well as smaller collections in 19 other languages.
Notable people.
Various public figures have grown up or lived in Queens such as business magnate and television personality Donald Trump and author Jack Kerouac. Musicians who have lived in the borough include singer Nadia Ali, rappers Nas, Ja Rule, 50 Cent, Run–D.M.C., Nicki Minaj, Simon & Garfunkel and Johnny Ramone. Actors such as Adrien Brody, and Lucy Liu grew up in the borough. Actress and singer Idina Menzel was born in Queens. Porn star Ron Jeremy was born in Queens and attended Queens College. Other actors such as Mae West have also lived in Queens. Physician Joshua Prager was born in Whitestone and attended Flushing High School. Mafia Boss John Gotti lived in Queens for many years.
Queens has also been home to athletes such as professional basketball player Rafer Alston Basketball Players Kareem Abdul-Jabbar and Metta World Peace were both born in Queens. Olympic Athlete Bob Beamon. Tennis star John McEnroe was born in Douglaston.

</doc>
<doc id="45582" url="http://en.wikipedia.org/wiki?curid=45582" title="Duchy of Schleswig">
Duchy of Schleswig

The Duchy of Schleswig (Danish: "Hertugdømmet Slesvig"; German: "Herzogtum Schleswig"; Low German: "Sleswig"; North Frisian: "Slaswik") was a principality in Southern Jutland ("Sønderjylland") covering the area about 60 km north and 70 km south of the current border between Germany and Denmark; the territory has been divided between the two countries since 1920, with Northern Schleswig in Denmark and Southern Schleswig in Germany. The region is also called Sleswick in English.
The area's traditional significance lies in the transfer of goods between the North Sea and the Baltic Sea, connecting the trade route through Russia with the trade routes along the Rhine and the Atlantic coast (see also Kiel Canal).
History.
Early history.
Roman sources place the homeland of the Jute tribe north of the river Eider and that of the Angles to its south, who in turn abutted the neighbouring Saxons. By the early Middle Ages the population of Schleswig consisted of Danes to the north of Danevirke and Schlei and on the peninsula Schwansen, North Frisians on the west coast below a line slightly south of the present border and on the islands, and Saxon (or Low German) in the far South. During the 14th century the population on Schwansen began to speak German, but otherwise the ethnic borders remained remarkably stable until around 1800 with the exception of the population in the towns that became increasingly German from the 14th century onwards.
During the early Viking Age, Haithabu - Scandinavia's biggest trading centre - was located in this region, which is also the location of the interlocking fortifications known as the "Danewerk". Its construction, and in particular its great expansion around 737, has been interpreted as an indication of the emergence of a unified Danish state. In May 1931 scientists of the National Museum of Denmark announced the finding of eighteen Viking graves with the remains of eighteen men in them. The discovery came during excavations in Schleswig. The skeletons indicated that the men were bigger proportioned than twentieth-century Danish men. Each of the graves was laid out from east to west. Researchers surmised that the bodies were entombed in wooden coffins originally, but only the iron nails remained. Towards the end of the Early Middle Ages, Schleswig formed part of the historical Lands of Denmark as Denmark unified out of a number of petty chiefdoms in the 8th to 10th centuries (the puissant of the Viking incursions).
Southern boundary of Denmark in the region of the Eider River and the Danevirke was a source of continuous dispute. The Treaty of Heiligen was signed in 811 between the Danish King Hemming and Charlemagne, by which the border was established at the Eider. During the 10th century there were several wars between East Francia and Denmark. In 1027, Conrad II and Canute the Great again settled their mutual border at the Eider.
In 1115, king Niels created his nephew Canute Lavard - a son of his predecessor Eric I - Earl of Schleswig, a title used for only a short time before the recipient began to style himself Duke.
Early modern times.
In 1230s, Southern Jutland (Duchy of Slesvig) was allotted as an appanage to Abel Valdemarsen, Canute's great-grandson, a younger son of Valdemar II of Denmark. Abel, having wrested the Danish throne to himself for a brief period, left his duchy to his sons and their successors, who pressed claims to the throne of Denmark for much of the next century, so that the Danish kings were at odds with their cousins, the dukes of Slesvig. Feuds and marital alliances brought the Abel dynasty into a close connection with the German Duchy of Holstein by the 15th century. The latter was a fief subordinate to the Holy Roman Empire, while Schleswig remained a Danish fief. These dual loyalties were to become a main root of the dispute between the German states and Denmark in the 19th century, when the ideas of romantic nationalism and the nation-state won popular support. The title Duke of Schleswig was inherited in 1460 by the hereditary kings of Norway who were also regularly elected kings of Denmark simultaneously, and their sons (unlike Denmark which was not hereditary). This was an anomaly – a king holding a ducal title, which he as king was the fount of and its liege lord. The title and anomaly survived presumably because it was already co-regally held by the king's sons. Between 1544 and 1713/20 the ducal reign had become a condominium, with the royal House of Oldenburg and its cadet branch House of Holstein-Gottorp jointly holding the stake. A third branch in the condominium, the short-lived House of Haderslev, was already extinct in 1580 by the time of John the Elder.
Following the Protestant Reformation when Latin was replaced as the medium of church service by the vernacular languages, the diocese of Schleswig was divided and an autonomous archdeaconry of Haderslev created. On the west coast the Danish diocese of Ribe stopped about 5 km north of the present border. This created a new cultural dividing line in the duchy because German was used for church services and teaching in the diocese of Schleswig and Danish was used in the diocese of Ribe and the archdeaconry of Haderslev. This line corresponds remarkably well with the present border.
In the 17th century a series of wars between Denmark and Sweden—which Denmark lost—devastated the region economically. However the nobility responded with a new agricultural system that restored prosperity. In the period 1600 to 1800 the region experienced the growth of manorialism of the sort common in the rye-growing regions of eastern Germany. The manors were large holdings with the work done by feudal peasant farmers. They specialized in high quality dairy products. Feudal lordship was combined with technical modernization, and the distinction between unfree labour and paid work was often vague. The feudal system was gradually abolished in the late 18th century, starting with the crown lands in 1765 and later the estates of the nobility. In 1805 all serfdom was abolished and land tenure reforms allowed former peasants to own their own farms.
19th century and the rise of nationalism.
From around 1800 to 1840 the Danish speaking population on the Angeln peninsula between Schleswig and Flensburg began to switch to Low German and in the same period many North Frisians also switched to Low German. This linguistic change created a new de facto dividing line between German and Danish speakers north of Tønder and south of Flensburg.
From around 1830 large segments of the population began to identify with either German or Danish nationality and mobilized politically. In Denmark, the National Liberal Party used the Schleswig Question as part of their agitation and demanded that the Duchy be incorporated in the Danish kingdom under the slogan "Denmark to the Eider". This caused a conflict between Denmark and the German states over Schleswig and Holstein which led to the Schleswig-Holstein Question of the 19th century. When the National Liberals came to power in Denmark, in 1848, it provoked an uprising of ethnic Germans who supported Schleswig's ties with Holstein. This led to the First War of Schleswig. Denmark was victorious and the Prussian troops were ordered to pull out of Schleswig and Holstein following the London Protocol of 1852.
Denmark again attempted to integrate Schleswig, by creating a new common constitution (the so-called November Constitution) for Denmark and Schleswig in 1863, but the German Confederation, led by Prussia and Austria, defeated the Danes in the Second War of Schleswig the following year. Prussia and Austria then assumed administration of Schleswig and Holstein respectively under the Gastein Convention of 14 August 1865. However, tensions between the two powers culminated in the Austro-Prussian War of 1866. In the Peace of Prague, the victorious Prussians annexed both Schleswig and Holstein, creating the province of Schleswig-Holstein. Provision for the cession of northern Schleswig to Denmark was made pending a popular vote in favour of this. In 1878, however, Austria went back on this provision, and Denmark, in a Treaty of 1907, with Germany, recognized that, by the agreement between Austria and Prussia, the frontier between Prussia and Denmark had finally been settled.
Modern times.
The Treaty of Versailles provided for a plebiscite to determine the ownership of the region. Thus, two referendums were held in 1920, resulting in the partition of the region. Northern Schleswig joined Denmark, whereas Central Schleswig voted, by an 80% majority, to remain part of Germany. In Southern Schleswig, no referendum was held, as the likely outcome was apparent. The name Southern Schleswig is now used for all of German Schleswig. This decision left substantial minorities on both sides of the new border.
Following the Second World War, a substantial part of the German population in Southern Schleswig changed their nationality and declared themselves as Danish. This change was caused by a number of factors, most importantly the German defeat and an influx of a large number of refugees from eastern Germany, whose culture and appearance differed from the local Germans, who were mostly descendents of Danish families that had changed their nationality in the 19th century. The change created a temporary Danish majority in the region and a demand for a new referendum from the Danish population in South Schleswig and some Danish politicians, including prime minister Knud Kristensen. But the majority in the Danish parliament refused to support a referendum in South Schleswig, fearing that the "new Danes" were not genuine in their change of nationality. This proved to be the case and, from 1948 the Danish population began to shrink again. By the early 1950s, it nevertheless had stabilised at a level four times higher than the pre-war number.
In the Copenhagen-Bonn declaration of 1955, Germany and Denmark promised to uphold the rights of each other's minority population. Today, both parts co-operate as a Euroregion, despite a national border dividing the former duchy. As Denmark and Germany are both part of the Schengen Area, there are no controls at the border.
Name and naming dispute.
In the 19th century, there was a naming dispute concerning the use of "Schleswig" or "Slesvig" and "Sønderjylland" (Southern Jutland). Originally the duchy was called "Sønderjylland" (Southern Jutland) but in the late 14th century the name of the city Slesvig (now Schleswig) started to be used for the whole territory. The term "Sønderjylland" was hardly used between the 16th and 19th centuries, and in this period the name "Schleswig" had no special political connotations. But around 1830, some Danes started to re-introduce the archaic term Sønderjylland to emphasize the area's history before its association with Holstein and its connection with the rest of Jutland. Its revival and widespread use in the 19th Century therefore had a clear Danish nationalist connotation of laying a claim to the territory and objecting to the German claims. "Olsen's Map", published by the Danish cartographer Olsen in the 1830s that used this term, aroused a storm of protests by the duchy's German inhabitants. Even though many Danish nationalists, such as the National Liberal ideologue and agitator Orla Lehmann, used the name "Schleswig", it began to assume a clear German nationalist character in the mid 19th century – especially when included in the combined term "Schleswig-Holstein". A central element of the German nationalistic claim was the insistence on Schleswig and Holstein being a single, indivisible entity. Since Holstein was legally part of the German Confederation, and ethnically entirely German with no Danish population, use of that name implied that both provinces should belong to Germany and that their connection with Denmark should be weakened or altogether severed. 
After the German conquest in 1864, the term Sønderjylland became increasingly dominant among the Danish population, even though most Danes still had no objection to the use of "Schleswig" as such (it is etymologically of Danish origin) and many of them still used it, themselves, in its Danish version "Slesvig". An example is the founding of De Nordslesvigske Landboforeninger (The North Schleswig Farmers Association). In 1866 Schleswig and Holstein were legally merged into the Prussian province of Schleswig-Holstein. 
The naming dispute was resolved with the 1920 plebiscites and partition, each side applying its preferred name to the part of the territory remaining in its possession – though both terms can, in principle, still refer to the entire region. Northern Schleswig was, after the 1920 plebiscites, officially named The Southern Jutland districts ("de sønderjyske landsdele"), while Southern Schleswig then remained a part of the Prussian province, which became the German state of Schleswig-Holstein in 1946.

</doc>
<doc id="45584" url="http://en.wikipedia.org/wiki?curid=45584" title="Biodefense">
Biodefense

Biodefense refers to short term, local, usually military measures to restore biosecurity to a given group of persons in a given area who are, or may be, subject to biological warfare— in the civilian terminology, it is a very robust biohazard response. It is technically possible to apply biodefense measures to protect animals or plants, but this is generally uneconomic. However, protection of water supplies and food supplies are often a critical part of biodefense. Various definitions of biosafety emerged in different professions to guarantee non-human health.
Biodefense is most often discussed in the context of biowar or bioterrorism, and is generally considered a military or emergency response term.
Biodefense applies to two distinct target populations: civilian non-combatant and military combatant (troops in the field).
Military.
Biodefense of troops in the field.
Military biodefense in the United States began with the United States Army Medical Unit (USAMU) at Fort Detrick, Maryland, in 1956. (In contrast to the U.S. Army Biological Warfare Laboratories [1943–1969], also at Fort Detrick, the USAMU's mission was purely to develop defensive measures against bio-agents, as opposed to weapons development.) The USAMU was disestablished in 1969 and succeeded by today's United States Army Medical Research Institute of Infectious Diseases (USAMRIID).
The United States Department of Defense (or "DoD") has focused since at least 1998 on the development and application of vaccine-based biodefenses. In a July 2001 report commissioned by the DoD, the "DoD-critical products" were stated as vaccines against anthrax (AVA and Next Generation), smallpox, plague, tularemia, botulinum, ricin, and equine encephalitis. Note that two of these targets are toxins (botulinum and ricin) while the remainder are infectious agents.
Civilian.
Role of public health and disease surveillance.
It is important to note that all of the classical and modern biological weapons organisms are animal diseases, the only exception being smallpox. Thus, in any use of biological weapons, it is highly likely that animals will become ill either simultaneously with, or perhaps earlier than humans.
Indeed, in the largest biological weapons accident known– the anthrax outbreak in Sverdlovsk (now Yekaterinburg) in the Soviet Union in 1979, sheep became ill with anthrax as far as 200 kilometers from the release point of the organism from a military facility in the southeastern portion of the city (known as Compound 19 and still off limits to visitors today, see Sverdlovsk Anthrax leak).
Thus, a robust surveillance system involving human clinicians and veterinarians may identify a bioweapons attack early in the course of an epidemic, permitting the prophylaxis of disease in the vast majority of people (and/or animals) exposed but not yet ill.
For example in the case of anthrax, it is likely that by 24 – 36 hours after an attack, some small percentage of individuals (those with compromised immune system or who had received a large dose of the organism due to proximity to the release point) will become ill with classical symptoms and signs (including a virtually unique chest X-ray finding, often recognized by public health officials if they receive timely reports). By making these data available to local public health officials in real time, most models of anthrax epidemics indicate that more than 80% of an exposed population can receive antibiotic treatment before becoming symptomatic, and thus avoid the moderately high mortality of the disease.
Identification of bioweapons.
The goal of biodefense is to integrate the sustained efforts of the national and homeland security, medical, public health, intelligence, diplomatic, and law enforcement communities. Health care providers and public health officers are among the first lines of defense. In some countries private, local, and provincial (state) capabilities are being augmented by and coordinated with federal assets, to provide layered defenses against biological weapons attacks. During the first Gulf War the United Nations activated a biological and chemical response team, Task Force Scorpio, to respond to any potential use of weapons of mass destruction on civilians.
The traditional approach toward protecting agriculture, food, and water: focusing on the natural or unintentional introduction of a disease is being strengthened by focused efforts to address current and anticipated future biological weapons threats that may be deliberate, multiple, and repetitive.
The growing threat of biowarfare agents and bioterrorism has led to the development of specific field tools that perform on-the-spot analysis and identification of encountered suspect materials. One such technology, being developed by researchers from the Lawrence Livermore National Laboratory (LLNL), employs a "sandwich immunoassay", in which fluorescent dye-labeled antibodies aimed at specific pathogens are attached to silver and gold nanowires.
The U.S. National Institute of Allergy and Infectious Diseases (NIAID) also participates in the identification and prevention of biowarfare and first released a strategy for biodefense in 2002, periodically releasing updates as new pathogens are becoming topics of discussion. Within this list of strategies, responses for specific infectious agents are provided, along with the classification of these agents. NIAID provides countermeasures after the U.S. Department of Homeland Security details which pathogens hold the most threat.
Planning and response.
Planning may involve the development of biological identification systems.Until recently in the United States, most biological defense strategies have been geared to protecting soldiers on the battlefield rather than ordinary people in cities. Financial cutbacks have limited the tracking of disease outbreaks. Some outbreaks, such as food poisoning due to "E. coli" or "Salmonella", could be of either natural or deliberate origin.
Preparedness<br>
Biological agents are relatively easy to obtain by terrorists and are becoming more threatening in the U.S., and laboratories are working on advanced detection systems to provide early warning, identify contaminated areas and populations at risk, and to facilitate prompt treatment. Methods for predicting the use of biological agents in urban areas as well as assessing the area for the hazards associated with a biological attack are being established in major cities. In addition, forensic technologies are working on identifying biological agents, their geographical origins and/or their initial son. Efforts include decontamination technologies to restore facilities without causing additional environmental concerns.
Early detection and rapid response to bioterrorism depend on close cooperation between public health authorities and law enforcement; however, such cooperation is currently lacking. National detection assets and vaccine stockpiles are not useful if local and state officials do not have access to them.
Biosurveillance<br>
In 1999, the University of Pittsburgh's Center for Biomedical Informatics deployed the first automated bioterrorism detection system, called RODS (Real-Time Outbreak Disease Surveillance). RODS is designed to draw collect data from many data sources and use them to perform signal detection, that is, to detect a possible bioterrorism event at the earliest possible moment. RODS, and other systems like it, collect data from sources including clinic data, laboratory data, and data from over-the-counter drug sales. In 2000, Michael Wagner, the codirector of the RODS laboratory, and Ron Aryel, a subcontractor, conceived the idea of obtaining live data feeds from "non-traditional" (non-health-care) data sources. The RODS laboratory's first efforts eventually led to the establishment of the National Retail Data Monitor, a system which collects data from 20,000 retail locations nation-wide.
On February 5, 2002, George W. Bush visited the RODS laboratory and used it as a model for a $300 million spending proposal to equip all 50 states with biosurveillance systems. In a speech delivered at the nearby Masonic temple, Bush compared the RODS system to a modern "DEW" line (referring to the Cold War ballistic missile early warning system).
The principles and practices of biosurveillance, a new interdisciplinary science, were defined and described in the "Handbook of Biosurveillance", edited by Michael Wagner, Andrew Moore and Ron Aryel, and published in 2006. Biosurveillance is the science of real-time disease outbreak detection. Its principles apply to both natural and man-made epidemics (bioterrorism).
Data which potentially could assist in early detection of a bioterrorism event include many categories of information. Health-related data such as that from hospital computer systems, clinical laboratories, electronic health record systems, medical examiner record-keeping systems, 911 call center computers, and veterinary medical record systems could be of help; researchers are also considering the utility of data generated by ranching and feedlot operations, food processors, drinking water systems, school attendance recording, and physiologic monitors, among others. Intuitively, one would expect systems which collect more than one type of data to be more useful than systems which collect only one type of information (such as single-purpose laboratory or 911 call-center based systems), and be less prone to false alarms, and this appears to be the case.
In Europe, disease surveillance is beginning to be organized on the continent-wide scale needed to track a biological emergency. The system not only monitors infected persons, but attempts to discern the origin of the outbreak.
Researchers are experimenting with devices to detect the existence of a threat:
New research shows that ultraviolet avalanche photodiodes offer the high gain, reliability and robustness needed to detect anthrax and other bioterrorism agents in the air. The fabrication methods and device characteristics were described at the 50th Electronic Materials Conference in Santa Barbara on June 25, 2008. Details of the photodiodes were also published in the February 14, 2008 issue of the journal Electronics Letters and the November 2007 issue of the journal IEEE Photonics Technology Letters.
The United States Department of Defense conducts global biosurveillance through several programs, including the Global Emerging Infections Surveillance and Response System.
Response to bioterrorism incident or threat.
Government agencies which would be called on to respond to a bioterrorism incident would include law enforcement, hazardous materials/decontamination units and emergency medical units. The US military has specialized units, which can respond to a bioterrorism event; among them are the United States Marine Corps' Chemical Biological Incident Response Force and the U.S. Army's 20th Support Command (CBRNE), which can detect, identify, and neutralize threats, and decontaminate victims exposed to bioterror agents.
There are four hospitals capable of caring for anyone with an exposure to a BSL3 or BSL4 pathogen, the special clinical studies unit at National Institutes of Health is one of them. National Institutes of Health built a facility in April 2010. This unit has state of the art isolation capabilities with a unique airflow system. This unit is also being trained to care for patients who are ill due to a highly infectious pathogen outbreak, such as ebola. The doctors work closely with USAMRIID, NBACC and IRF. Special trainings take place regularly in order to maintain a high level of confidence to care for these patients.

</doc>
<doc id="45585" url="http://en.wikipedia.org/wiki?curid=45585" title="Emperor Juntoku">
Emperor Juntoku

Emperor Juntoku (順徳天皇, Juntoku-tennō) (October 22, 1197 – October 7, 1242) was the 84th emperor of Japan, according to the traditional order of succession. His reign spanned the years from 1210 through 1221.
Genealogy.
Before his ascension to the Chrysanthemum Throne, his personal name (his "imina") was Morinari"-shinnō" (守成親王).
He was the third son of Emperor Go-Toba. His mother was Shigeko (重子), the daughter of Fujiwara Hanki (藤原範季)
Events of Juntoku's life.
Morinari-shinnō became Crown Prince in 1200. He was elevated to the throne after Emperor Go-Toba pressured Emperor Tsuchimikado into abdicating.
In actuality, Emperor Go-Toba wielded effective power as a cloistered emperor during the years of Juntoku's reign.
In 1221, he was forced to abdicate because of his participation in Go-Toba's unsuccessful attempt to displace the Kamakura bakufu with re-asserted Imperial power. This political and military struggle was called the Jōkyū War or the Jōkyū Incident ("Jōkyū-no ran").
After the "Jōkyū-no ran", Juntoku was sent into exile on Sado Island (佐渡島 or 佐渡ヶ島, both "Sadogashima"), where he remained until his death in 1242.
This emperor is known posthumously as Sado-no In (佐渡院) because his last years were spent at Sado. He was buried in a mausoleum, the Mano Goryo, on Sado's west coast. Juntoku's official Imperial tomb ("misasagi") is in Kyoto.
Juntoku was tutored in poetry by Fujiwara no Sadaie, who was also known as Teika. One of the emperor's poems was selected for inclusion in the what became a well-known anthology, the Ogura Hyakunin Isshu. This literary legacy in Teika's collection of poems has accorded Juntoku a continuing popular prominence beyond the scope of his other lifetime achievements. The poets and poems of the Hyakunin isshu form the basis for a card game ("uta karuta") which is still widely played today.
"Kugyō".
"Kugyō" (公卿) is a collective term for the very few most powerful men attached to the court of the Emperor of Japan in pre-Meiji eras.
In general, this elite group included only three to four men at a time. These were hereditary courtiers whose experience and background would have brought them to the pinnacle of a life's career. During juntoku's reign, this apex of the "Daijō-kan" included:
Eras of Juntoku's reign.
The years of Juntoku's reign are more specifically identified by more than one era name or "nengō".
References.
</dl>

</doc>
<doc id="45586" url="http://en.wikipedia.org/wiki?curid=45586" title="Indifference curve">
Indifference curve

In microeconomic theory, an indifference curve is a graph showing different bundles of goods between which a consumer is "indifferent." That is, at each point on the curve, the consumer has no preference for one bundle over another. One can equivalently refer to each point on the indifference curve as rendering the same level of utility (satisfaction) for the consumer. In other words an indifference curve is the locus of various points showing different combinations of two goods providing equal utility to the consumer. Utility is then a device to represent preferences rather than something from which preferences come. The main use of indifference curves is in the representation of potentially observable demand patterns for individual consumers over commodity bundles.
There are infinitely many indifference curves: one passes through each combination. A collection of (selected) indifference curves, illustrated graphically, is referred to as an indifference map.
History.
The theory of indifference curves was developed by Francis Ysidro Edgeworth, who explained in his book "Mathematical Psychics: an Essay on the Application of Mathematics to the Moral Sciences,” 1881, the mathematics needed for its drawing; later on, Vilfredo Pareto was the first author to actually draw these curves, in his book "Manual of Political Economy," 1906; and others in the first part of the 20th century. The theory can be derived from William Stanley Jevons's ordinal utility theory, which posits that individuals can always rank any consumption bundles by order of preference.
Map and properties of indifference curves.
A graph of indifference curves for an individual consumer associated with different utility levels is called an indifference map. Points yielding different utility levels are each associated with distinct indifference curves and these indifference curves on the indifference map are like contour lines on a topographical map. Each point on the curve represents the same elevation. If you move "off" an indifference curve traveling in a northeast direction (assuming positive marginal utility for the goods) you are essentially climbing a mound of utility. The higher you go the greater the level of utility. The non-satiation requirement means that you will never reach the "top," or a "bliss point," a consumption bundle that is preferred to all others
Indifference curves are typically represented to be:
Assumptions of consumer preference theory.
It also implies that the commodities are good rather than bad. Examples of bad commodities can be disease, pollution etc. because we always desire less of such things.
Application.
Consumer theory uses indifference curves and budget constraints to generate consumer demand curves. For a single consumer, this is a relatively simple process. First, let one good be an example market e.g., carrots, and let the other be a composite of all other goods. Budget constraints give a straight line on the indifference map showing all the possible distributions between the two goods; the point of maximum utility is then the point at which an indifference curve is tangent to the budget line (illustrated). This follows from common sense: if the market values a good more than the household, the household will sell it; if the market values a good less than the household, the household will buy it. The process then continues until the market's and household's marginal rates of substitution are equal. Now, if the price of carrots were to change, and the price of all other goods were to remain constant, the gradient of the budget line would also change, leading to a different point of tangency and a different quantity demanded. These price / quantity combinations can then be used to deduce a full demand curve. A line connecting all points of tangency between the indifference curve and the budget constraint is called the expansion path.
Examples of indifference curves.
In Figure 1, the consumer would rather be on "I3" than "I2", and would rather be on "I2" than "I1", but does not care where he/she is on a given indifference curve. The slope of an indifference curve (in absolute value), known by economists as the marginal rate of substitution, shows the rate at which consumers are willing to give up one good in exchange for more of the other good. For "most" goods the marginal rate of substitution is not constant so their indifference curves are curved. The curves are convex to the origin, describing the negative substitution effect. As price rises for a fixed money income, the consumer seeks less the expensive substitute at a lower indifference curve. The substitution effect is reinforced through the income effect of lower real income (Beattie-LaFrance). An example of a utility function that generates indifference curves of this kind is the Cobb-Douglas function formula_1. The negative slope of the indifference curve incorporates the willingness of the consumer to make trade offs.
If two goods are perfect substitutes then the indifference curves will have a constant slope since the consumer would be willing to switch between at a fixed ratio. The marginal rate of substitution between perfect substitutes is likewise constant. An example of a utility function that is associated with indifference curves like these would be formula_2.
If two goods are perfect complements then the indifference curves will be L-shaped. Examples of perfect complements include left shoes compared to right shoes: the consumer is no better off having several right shoes if she has only one left shoe - additional right shoes have zero marginal utility without more left shoes, so bundles of goods differing only in the number of right shoes they include - however many - are equally preferred. The marginal rate of substitution is either zero or infinite. An example of the type of utility function that has an indifference map like that above is the Leontief function: formula_3.
The different shapes of the curves imply different responses to a change in price as shown from demand analysis in consumer theory. The results will only be stated here. A price-budget-line change that kept a consumer in equilibrium on the same indifference curve:
Preference relations and utility.
Choice theory formally represents consumers by a preference relation, and use this representation to derive indifference curves showing combinations of equal preference to the consumer.
Preference relations.
Let
In the language of the example above, the set formula_4 is made of combinations of apples and bananas. The symbol formula_5 is one such combination, such as 1 apple and 4 bananas and formula_6 is another combination such as 2 apples and 2 bananas.
A preference relation, denoted formula_11, is a binary relation define on the set formula_4.
The statement
is described as 'formula_5 is weakly preferred to formula_6.' That is, formula_5 is at least as good as formula_6 (in preference satisfaction).
The statement
is described as 'formula_5 is weakly preferred to formula_6, and formula_6 is weakly preferred to formula_5.' That is, one is "indifferent" to the choice of formula_5 or formula_6, meaning not that they are unwanted but that they are equally good in satisfying preferences.
The statement
is described as 'formula_5 is weakly preferred to formula_6, but formula_6 is not weakly preferred to formula_5.' One says that 'formula_5 is strictly preferred to formula_6.'
The preference relation formula_11 is complete if all pairs formula_33 can be ranked. The relation is a transitive relation if whenever formula_13 and formula_35 then formula_36.
For any element formula_37, the corresponding indifference curve, formula_38 is made up of all elements of formula_4 which are indifferent to formula_40. Formally,
formula_41.
Formal link to utility theory.
In the example above, an element formula_5 of the set formula_4 is made of two numbers: The number of apples, call it formula_44 and the number of bananas, call it formula_45
In utility theory, the utility function of an agent is a function that ranks "all" pairs of consumption bundles by order of preference ("completeness") such that any set of three or more bundles forms a transitive relation. This means that for each bundle formula_46 there is a unique relation, formula_47, representing the utility (satisfaction) relation associated with formula_46. The relation formula_49 is called the utility function. The range of the function is a set of real numbers. The actual values of the function have no importance. Only the ranking of those values has content for the theory. More precisely, if formula_50, then the bundle formula_46 is described as at least as good as the bundle formula_52. If formula_53, the bundle formula_46 is described as strictly preferred to the bundle formula_52.
Consider a particular bundle formula_56 and take the total derivative of formula_47 about this point:
where formula_60 is the partial derivative of formula_47 with respect to its first argument, evaluated at formula_46. (Likewise for formula_63)
The indifference curve through formula_56 must deliver at each bundle on the curve the same utility level as bundle formula_56. That is, when preferences are represented by a utility function, the indifference curves are the level curves of the utility function. Therefore, if one is to change the quantity of formula_66 by formula_67, without moving off the indifference curve, one must also change the quantity of formula_68 by an amount formula_69 such that, in the end, there is no change in "U":
Thus, the ratio of marginal utilities gives the absolute value of the slope of the indifference curve at point formula_56. This ratio is called the marginal rate of substitution between formula_66 and formula_68.
Examples.
Linear utility.
If the utility function is of the form formula_75 then the marginal utility of formula_66 is formula_77 and the marginal utility of formula_68 is formula_79. The slope of the indifference curve is, therefore,
Observe that the slope does not depend on formula_66 or formula_68: the indifference curves are straight lines.
Cobb-Douglas utility.
If the utility function is of the form formula_83 the marginal utility of formula_66 is formula_85 and the marginal utility of formula_68 is formula_87.Where formula_88. The slope of the indifference curve, and therefore the negative of the marginal rate of substitution, is then
CES utility.
A general CES (Constant Elasticity of Substitution) form is
where formula_91 and formula_92. (The Cobb-Douglas is a special case of the CES utility, with formula_93.) The marginal utilities are given by
and
Therefore, along an indifference curve,
These examples might be useful for modelling individual or aggregate demand.
Biology.
As used in Biology, the indifference curve is a model for how animals 'decide' whether to perform a particular behavior, based on changes in two variables which can increase in intensity, one along the x-axis and the other along the y-axis. For example, the x-axis may measure the quantity of food available while the y-axis measures the risk involved in obtaining it. The indifference curve is drawn to predict the animal's behavior at various levels of risk and food availability.

</doc>
<doc id="45587" url="http://en.wikipedia.org/wiki?curid=45587" title="Chukyo">
Chukyo

Chukyo can refer to:
the city of Nagoya (中京 Chūkyō). Various things are named after the city:
仲恭 (Chūkyō)

</doc>
<doc id="45588" url="http://en.wikipedia.org/wiki?curid=45588" title="Emperor Go-Horikawa">
Emperor Go-Horikawa

Emperor Go-Horikawa (後堀河天皇, Go-Horikawa-tennō) (March 22, 1212 – August 31, 1234) was the 86th emperor of Japan, according to the traditional order of succession. This reign spanned the years from 1221 through 1232.
This 13th-century sovereign was named after the 10th-century Emperor Horikawa and "go-" (後), translates literally as "later"; and thus, he is sometimes called the "Later Emperor Horikawa". The Japanese word "go" has also been translated to mean the "second one;" and in some older sources, this emperor may be identified as "Horikawa, the second," or as "Horikawa II."
Genealogy.
Before his ascension to the Chrysanthemum Throne, his personal name (his "imina") was Yutahito"-shinnō" (茂仁親王), also known as Motsihito"-shinnō".
Events of Go-Horikawa's life.
In 1221, because of the Jōkyū Incident, an unsuccessful attempt by Emperor Go-Toba to seize real power, the Kamakura shogunate completely excluded those of the imperial family descended from Emperor Go-Toba from the Chrysanthemum throne, thus forcing Emperor Chūkyō to abdicate. After the Genpei War, he, as the grandson of the late Emperor Takakura, who was also a nephew of the then-exiled Retired Emperor Go-Toba, and Chūkyō's first cousin, was enthroned as Go-Horikawa. He ruled from July 29, 1221 to October 26 (?), 1232.
As Go-Horikawa was only ten-years-old at this time, his father Imperial Prince Morisada acted as cloistered emperor under the name Go-Takakura-in.
In 1232, he began his own cloistered rule, abdicating to his 1-year-old son, Emperor Shijō. However, he had a weak constitution, and his cloistered rule lasted just under two years before he died.
Emperor Go-Horikawa's Imperial tomb ("misasagi") is at Sennyū-ji in the "Nochi no Tsukinowa no Higashiyama no misasagi" (後月輪東山陵).
"Kugyō".
"Kugyō" (公卿) is a collective term for the very few most powerful men attached to the court of the Emperor of Japan in pre-Meiji eras.
In general, this elite group included only three to four men at a time. These were hereditary courtiers whose experience and background would have brought them to the pinnacle of a life's career. During Go-Horikawa's reign, this apex of the "Daijō-kan" included:
Eras of Go-Horikawa's reign.
The years of Go-Horikawa's reign are more specifically identified by more than one era name or "nengō".
References.
</dl>

</doc>
<doc id="45589" url="http://en.wikipedia.org/wiki?curid=45589" title="Shijō">
Shijō

Shijō (四条 or 四條) literally means "star" in Japanese. It may refer to:

</doc>
<doc id="45590" url="http://en.wikipedia.org/wiki?curid=45590" title="Emperor Go-Saga">
Emperor Go-Saga

Emperor Go-Saga (後嵯峨天皇 "Go-Saga-tennō") (April 1, 1220 – March 17, 1272) was the 88th emperor of Japan, according to the traditional order of succession. This reign spanned the years 1242 through 1246.
This 13th-century sovereign was named after the 8th-century Emperor Saga and "go-" (後), translates literally as "later"; and thus, he is sometimes called the "Later Emperor Saga". The Japanese word "go" has also been translated to mean the "second one;" and in some older sources, this emperor may be identified as "Saga, the second," or as "Saga II."
Genealogy.
Before his ascension to the Chrysanthemum Throne, his personal name (his "imina") was Kunihito"-shinnō" (邦仁親王).
He was the second son of Emperor Tsuchimikado, and second cousin of his predecessor Emperor Shijō.
Events of Go-Saga's life.
He ruled from February 21, 1242 to February 16, 1246.
When Emperor Tsuchimikado moved to Tosa Province (on Shikoku), he was raised by his mother's side of the family.
Because of the sudden death of Emperor Shijō at the age of 10, the question of succession arose. Because the expectations of the court nobility and the Bakufu conflicted, the issue was bitterly contested. Kujō Michiie and the court nobility supported Prince Tadanari (忠成王), a son of Retired Emperor Juntoku, but the "shikken" Hōjō Yasutoki was opposed to the sons of Juntoku because of his involvement in the Jōkyū War. Michiie instead supported Tsuchimikado's son Prince Kunihito as a neutral figure for Emperor. During these negotiations, there was a vacancy on the throne of 11 days.
In 1242, Prince Kunihito became emperor. In 1246 he abdicated to his son, Emperor Go-Fukakusa, beginning his reign as cloistered emperor. In 1259, he compelled Emperor Go-Fukakusa to abdicate to his younger brother, Emperor Kameyama. Imperial Prince Munetaka became shōgun instead of the Hōjō regents. Henceforth, the shōguns of the Kamakura Bakufu came from the imperial house. Still, the Hōjō regents increased their control of the shogunate, setting up the system of rule by regents.
The descendants of his two sons contested the throne between them, forming into two lines, the Jimyōin-tō (Go-Fukakusa's descendants) and the Daikakuji-tō (Kameyama's descendants).
In 1272, Go-Saga died.
Go-Saga's final resting place is designated as an Imperial mausoleum ("misasagi") at Saa no minami no "Misasagi" in Kyoto.
Kugyō.
"Kugyō" (公卿) is a collective term for the very few most powerful men attached to the court of the Emperor of Japan in pre-Meiji eras. Even during those years in which the court's actual influence outside the palace walls was minimal, the hierarchic organization persisted.
In general, this elite group included only three to four men at a time. These were hereditary courtiers whose experience and background would have brought them to the pinnacle of a life's career. During Go-Saga's reign, this apex of the "Daijō-kan included:
Eras of Go-Saga's reign.
The years of Go-saga's reign are more specifically identified by more than one era name or "nengō".
References.
</dl>

</doc>
<doc id="45591" url="http://en.wikipedia.org/wiki?curid=45591" title="Emperor Go-Fukakusa">
Emperor Go-Fukakusa

Emperor Go-Fukakusa (後深草天皇, Go-Fukakusa-tennō) (June 28, 1243 – August 17, 1304) was the 89th emperor of Japan, according to the traditional order of succession. This reign spanned the years 1246 through 1260.
This 13th-century sovereign was named after the 9th-century Emperor Nimmyō and "go-" (後), translates literally as "later;" and thus, he could be called the "Later Emperor Fukakusa". The Japanese word "go" has also been translated to mean the "second one;" and in some older sources, this emperor may be identified as "Fukakusa, the second," or as "Fukakusa II."
Name.
Before his ascension to the Chrysanthemum Throne, his personal name (his "imina") was Hisahito (久仁).
Although the Roman-alphabet spelling of the name of this 13th-century emperor is the same as that of the personal name of a current member of the Imperial family, the kanji are different:
He was the second son of Emperor Go-Saga.
Events of Go-Fukakusa's life.
Hisahito"-shinnō" (久仁親王) formally became Go-Fukakusa"-tennō" (後深草天皇) at the age of 2; and Go-Saga began to exercise power as cloistered Emperor.
In 1259, at the insistence of Retired Emperor Go-Saga, he abdicated at the age of 15 to his younger brother, who would become Emperor Kameyama.
After Emperor Go-Uda's ascension in 1260, Saionji Sanekane negotiated with the Bakufu, and succeeded in getting Emperor Go-Fukakusa's son Hirohito named as Crown Prince. In 1287, with his ascension as Emperor Fushimi, Go-Fukakusa's cloistered rule began.
In 1290, he entered the priesthood, retiring from the position of cloistered Emperor. But, with his seventh son, Imperial Prince Hisaaki becoming the 8th Kamakura shōgun among other things, the position of his Jimyōin-tō became strengthened.
In 1304, he died. He is enshrined with other emperors at the imperial tomb called "Fukakusa no kita no misasagi" (深草北陵) in Fushimi-ku, Kyoto.
Kugyō.
"Kugyō" (公卿) is a collective term for the very few most powerful men attached to the court of the Emperor of Japan in pre-Meiji eras. Even during those years in which the court's actual influence outside the palace walls was minimal, the hierarchic organization persisted.
In general, this elite group included only three to four men at a time. These were hereditary courtiers whose experience and background would have brought them to the pinnacle of a life's career. During Go-Fukakusa's reign, this apex of the "Daijō-kan included:
Eras of Go-Fukakusa's reign.
The years of Go-Fukakusa's reign are more specifically identified by more than one era name or "nengō".
References.
</dl>

</doc>
<doc id="45592" url="http://en.wikipedia.org/wiki?curid=45592" title="Bartolomeu Dias">
Bartolomeu Dias

Bartolomeu Dias (]; Anglicized: Bartholomew Diaz; c. 1451 – 29 May 1500), a nobleman of the Portuguese royal household, was a Portuguese explorer. He sailed around the southernmost tip of Africa in 1488, reaching the Indian Ocean from the Atlantic, the first European known to have done so.
Purposes of the Dias expedition.
Bartolomeu Dias was a Knight of the royal court, superintendent of the royal warehouses, and sailing-master of the man-of-war, "São Cristóvão" (Saint Christopher). King John II of Portugal appointed him, on 10 October 1487, to head an expedition to sail around the southern tip of Africa in the hope of finding a trade route to India. Dias was also charged with searching for the lands ruled by Prester John, who was a fabled Christian priest and ruler.
The expedition.
Dias' ship "São Cristóvão" was piloted by Pêro de Alenquer. A second caravel, the "São Pantaleão", was commanded by João Infante and piloted by Álvaro Martins. Dias' brother Pêro Dias was the captain of the square-rigged support ship with João de Santiago as pilot.
The expedition sailed south along the West coast of Africa. Extra provisions were picked up on the way at the Portuguese fortress of São Jorge de Mina on the Gold Coast. After having sailed past Angola, Dias reached the Golfo da Conceicão (Walvis Bay) by December. Continuing south, he discovered first Angra dos Ilheus, being hit, then, by a violent storm. Thirteen days later, from the open ocean, he searched the coast again to the east, discovering and using the westerlies winds - the ocean gyre, but finding just ocean. Having rounded the Cape of Good Hope at a considerable distance to the west and southwest, he turned towards the east, and taking advantage of the winds of Antarctica that blow strongly in the South Atlantic, he sailed northeast. After 30 days without seeing land, he entered what he named Aguada de São Brás (Bay of Saint Blaise)—later renamed Mossel Bay—on 4 February 1488. Dias's expedition reached its furthest point on 12 March 1488 when they anchored at Kwaaihoek, near the mouth of the Bushman's River, where a padrão—the Padrão de São Gregório—was erected before turning back. Dias wanted to continue sailing to India, but he was forced to turn back when his crew refused to go further. It was only on the return voyage that he actually discovered the Cape of Good Hope, in May 1488. Dias returned to Lisbon in December of that year, after an absence of sixteen months.
The discovery of the passage around southern Africa was significant because, for the first time, Europeans realized they could trade directly with India and the other parts of Asia, bypassing the overland route through the Middle East, with its expensive middlemen. The official report of the expedition has been lost.
Bartolomeu Dias originally named the Cape of Good Hope the "Cape of Storms" ("Cabo das Tormentas"). It was later renamed (by King John II of Portugal) the Cape of Good Hope ("Cabo da Boa Esperança") because it represented the opening of a route to the east.
Follow-up voyages.
After these early attempts, the Portuguese took a decade-long break from Indian Ocean exploration. During that hiatus, it is likely that they received valuable information from a secret agent, Pêro da Covilhã, who had been sent overland to India and returned with reports useful to their navigators.
Using his experience with explorative travel, Dias helped in the construction of the "São Gabriel" and its sister ship, the "São Rafael" that were used by Vasco da Gama to circumnavigate the Cape of Good Hope and continue the route to India. Dias only participated in the first leg of Da Gama's voyage, until the Cape Verde Islands. He was then one of the captains of the second Indian expedition, headed by Pedro Álvares Cabral. This flotilla first reached the coast of Brazil, landing there in 1500, and then continued eastwards to India. Dias perished near the Cape of Good Hope that he presciently had named Cape of Storms. Four ships encountered a huge storm off the cape and were lost, including Dias', on 29 May 1500. A shipwreck found in 2008 by the Namdeb Diamond Corporation off Namibia was at first thought to be Dias' ship; however, recovered coins come from a later time.
Personal life.
Bartolomeu Dias was married and had two children:

</doc>
<doc id="45594" url="http://en.wikipedia.org/wiki?curid=45594" title="Kameyama">
Kameyama

Kameyama can refer to:

</doc>
<doc id="45595" url="http://en.wikipedia.org/wiki?curid=45595" title="Emperor Go-Uda">
Emperor Go-Uda

Emperor Go-Uda (後宇多天皇 "Go-Uda-tennō") (December 17, 1267 – July 16, 1324) was the 91st emperor of Japan, according to the traditional order of succession. His reign spanned the years from 1274 through 1287.
This 13th-century sovereign was named after the 9th-century Emperor Uda and "go-" (後), translates literally as "later"; and thus, he is sometimes called the "Later Emperor Uda". The Japanese word "go" has also been translated to mean the "second one;" and in some older sources, this emperor may be identified as "Uda, the second," or as "Uda II."
Genealogy.
Before his ascension to the Chrysanthemum Throne, his personal name ("imina") was Yohito-shinnō (世仁親王).
He was the second son of Emperor Kameyama. They were from the Daikakuji line.
Events of Go-Uda's life.
Yohito"-shinnō" became crown prince in 1268. According to the terms of the late emperor's will (Go-Saga died in 1272), in 1274, he would became emperor upon the death or abdication of Emperor Kameyama.
The retired Emperor Kameyama continued to exercise power as cloistered emperor.
During his reign, the unsuccessful Mongol invasions of Japan occurred, first in 1274 and again in 1281. Though they established a beachhead at Hakata, Kyushu, they were driven out within a short time.
In 1287, retired Emperor Go-Fukakusa, dissatisfied with the fact that his own lineage (the "Jimyōin-tō") did not control the throne, while that of his younger brother, the retired Emperor Kameyama (the "Daikakuji-tō") did, persuaded both the Bakufu and the imperial court to compel the Emperor to abdicate in favor of Go-Fukakusa's son (Emperor Fushimi).
After this time, the struggle between the Jimyōin-tō and the Daikakuji-tō over the imperial throne continued. After Go-Uda's abdication, his Daikakuji-tō controlled the throne from 1301 to 1308 (Emperor Go-Nijō) and again from 1318 until the era of northern and southern courts (begun 1332) when they became the southern court (ending in 1392).
Go-Uda was cloistered emperor during the reign of his own son, Go-Nijō, from 1301 until 1308, and again from 1318, when his second son Go-Daigo took the throne until 1321, when Go-Daigo began direct rule.
Emperor Go-Uda's Imperial mausoleum is the "Rengebuji no misasagi" (蓮華峯寺陵) in Ukyō-ku, Kyoto.
Kugyō.
"Kugyō" (公卿) is a collective term for the very few most powerful men attached to the court of the Emperor of Japan in pre-Meiji eras. Even during those years in which the court's actual influence outside the palace walls was minimal, the hierarchic organization persisted.
In general, this elite group included only three to four men at a time. These were hereditary courtiers whose experience and background would have brought them to the pinnacle of a life's career. During Go-Uda's reign, this apex of the "Daijō-kan included:
Eras of Go-Uda's reign.
The years of Go-Uda's reign are more specifically identified by more than one era name or "nengō".

</doc>
<doc id="45596" url="http://en.wikipedia.org/wiki?curid=45596" title="Thomas E. Dewey">
Thomas E. Dewey

Thomas Edmund Dewey (March 24, 1902 – March 16, 1971) was the 47th Governor of New York (1943–1954). In 1944 he was the Republican candidate for President, but lost to President Franklin D. Roosevelt in the closest of Roosevelt's four presidential elections. In 1948 he was again the Republican candidate for President, but lost to the incumbent President, Harry S. Truman, in one of the greatest upsets in presidential election history.
Dewey led the progressive/moderate faction of the Republican Party, in which he fought conservative Ohio Senator Robert A. Taft. Dewey was an advocate for the professional and business community of the Northeastern United States, which would later be called the "Eastern Establishment." This group supported most of the New Deal social-welfare reforms enacted during the administration of Franklin D. Roosevelt, and it consisted of internationalists who were in favor of the United Nations and the "Cold War" fight against communism and the Soviet Union. In addition, he played a large part in the election of Dwight D. Eisenhower as President in 1952. Dewey's successor as leader of the progressive Republicans was Nelson Rockefeller, who became governor of New York in 1959. The New York State Thruway is named in Dewey's honor.
Early life and family.
Dewey was born and raised in Owosso, Michigan, where his father, George Dewey, owned, edited, and published the local newspaper, the "Owosso Times." His mother, Annie Thomas (whom he called "Mater"), bequeathed her son "a healthy respect for common sense and the average man or woman who possessed it. She also left a headstrong assertiveness that many took for conceit, a set of small-town values never entirely erased by exposure to the sophisticated East, and a sense of proportion that moderated triumph and eased defeat." One journalist noted that "[as a boy] he did show leadership and ambition above the average; by the time he was thirteen, he had a crew of nine other youngsters working for him" selling magazines in Owosso. He graduated from the University of Michigan in 1923, and from Columbia Law School in 1925. While at the University of Michigan, he joined Phi Mu Alpha Sinfonia, a national fraternity for men of music, and was a member of the Men's Glee Club. He was an excellent singer with a deep, baritone voice, and in 1923 he finished in third place in the National Singing Contest. He briefly considered a career as a professional singer, but decided against it after a temporary throat ailment convinced him that such a career would be risky. He then decided to pursue a career as a lawyer. He also wrote for "The Michigan Daily," the university's student newspaper.
On June 16, 1928 Dewey married Frances Eileen Hutt. A native of Sherman, Texas, she was a stage actress; after their marriage she dropped her acting career. They had two sons, Thomas E. Dewey Jr. and John Martin Dewey. Although Dewey served as a prosecutor and District Attorney in New York City for many years, his home from 1939 until his death was a large farm, called "Dapplemere," located near the town of Pawling some 65 mi north of New York City. According to biographer Richard Norton Smith, Dewey "loved Dapplemere as [he did] no other place", and Dewey was once quoted as saying that "I work like a horse five days and five nights a week for the privilege of getting to the country on the weekend." Dewey once told a reporter that "my farm is my roots...the heart of this nation is the rural small town." Dapplemere was part of a tight-knit rural community called "Quaker Hill," which was known as a haven for the prominent and well-to-do. Among Dewey's neighbors on Quaker Hill were the famous reporter and radio broadcaster Lowell Thomas, the Reverend Norman Vincent Peale, and the legendary CBS News journalist Edward R. Murrow. Dewey was an active, lifelong member of the Episcopal Church.
Prosecutor.
Federal prosecutor.
Dewey first served as a federal prosecutor, then started a lucrative private practice on Wall Street; however, he left his practice for an appointment as special prosecutor to look into corruption in New York City—with the official title of Chief Assistant U.S. Attorney for the Southern District of New York. It was in this role that he first achieved headlines in the early 1930s, when he prosecuted bootlegger Waxey Gordon.
Dewey had used his excellent recall of details of crimes to trip up witnesses as a federal prosecutor; as a state prosecutor, he used telephone taps (which were perfectly legal at the time) to gather evidence, with the ultimate goal of bringing down entire criminal organizations. On that account, Dewey successfully lobbied for an overhaul in New York's criminal procedure law, which at that time required separate trials for each count of an indictment. Dewey's thoroughness and attention to detail became legendary; for one case he and his staff sifted "through 100,000 telephone slips to convict a Prohibition-era bootlegger".
Special prosecutor.
Dewey rocketed to fame in 1935, when he was appointed special prosecutor in New York County (Manhattan) by Governor Herbert H. Lehman. A "runaway grand jury" had publicly complained that William C. Dodge, the District Attorney, was not aggressively pursuing the mob and political corruption. Lehman, to avoid charges of partisanship, asked four prominent Republicans to serve as special prosecutor. All four refused and recommended Dewey.
Dewey moved ahead vigorously. He recruited a staff of over 60 assistants, investigators, process servers, stenographers, and clerks. New York Mayor Fiorello H. La Guardia assigned a picked squad of 63 police officers to Dewey's office. One writer stated that "Dewey...put on a very impressive show. All the paraphernalia, the hideouts and tapped telephones and so on, became famous. More than any other American of his generation except [Charles] Lindbergh, Dewey became a creature of folklore and a national hero. What he appealed to most was the great American love of "results." People were much more interested in his ends than in his means. Another key to all this may be expressed in a single word: honesty. Dewey was honest."
Dewey's targets were "organized" racketeering: the large-scale criminal enterprises, especially extortion, the "numbers game", and prostitution. He pursued Tammany Hall political leaders known for their ties to gangsters, such as James Joseph Hines.
One of his biggest prizes was gangster Dutch Schultz, whom he had battled as both a federal and state prosecutor. Schultz's first trial ended in a deadlock; prior to his second trial, Schultz had the venue moved to Malone, New York, then moved there and garnered the sympathy of the townspeople through charitable acts so that when it came time for his trial, the jury found him innocent, liking him too much to convict him.
Dewey and La Guardia threatened Schultz with instant arrest and further charges. Schultz now proposed to murder Dewey. Dewey would be killed while he made his daily morning call to his office from a pay phone near his home. However, New York crime boss Lucky Luciano and the "Mafia Commission" decided that Dewey's murder would provoke an all-out crackdown. Instead they had Schultz killed.
Dewey next turned his attention to Luciano. Dewey raided 80 houses of prostitution in the New York City area and arrested hundreds of prostitutes and "madams". Many of the prostitutes — some of whom told of being beaten and abused by Mafia thugs — were willing to testify to avoid prison time. Three implicated Luciano as controller of organized prostitution in the New York/New Jersey area — one of the largest prostitution rings in American history. In the greatest victory of his legal career, Dewey won the conviction of Luciano for the prostitution racket, with a sentence of 30 to 50 years.
However, Dewey did more than simply prosecute gangsters. In 1936 Dewey helped indict and convict Richard Whitney, the former president of the New York Stock Exchange, for embezzlement. Dewey also led efforts to protect dockworkers and poultry farmers and workers from racketeering in New York. In 1936 Dewey received The Hundred Year Association of New York's Gold Medal Award "in recognition of outstanding contributions to the City of New York". In 1939 Dewey prosecuted American Nazi leader Fritz Julius Kuhn for embezzlement, crippling Kuhn's organization and limiting its ability to support Nazi Germany in World War II.
Manhattan District Attorney.
In 1937, Dewey was elected District Attorney of New York County (Manhattan), defeating the Democratic nominee after Dodge decided not to run for re-election. Dewey was such a popular candidate for District Attorney that "election officials in Brooklyn posted large signs at polling places reading 'Dewey Isn't Running in This County'." By the late 1930s Dewey's successful efforts against organized crime—and especially his conviction of Lucky Luciano—had turned him into a national celebrity. His nickname, the "Gangbuster", was used for the popular 1935 "Gang Busters" radio series based on his fight against the mob. Hollywood film studios made several movies inspired by his exploits; "Marked Woman" starred Humphrey Bogart as a Dewey-like DA and Bette Davis as a "party girl" whose testimony helps convict the gang boss. A popular story from the time, possibly apocryphal, featured a young girl who told her father that she wanted to sue God to stop a prolonged spell of rain. When her father replied "you can't sue God and win", the girl said "I can if Dewey is my lawyer."
Governor of New York.
In 1938, Edwin Jaeckle, the New York Republican Party Chairman, selected Dewey to run, unsuccessfully, for Governor of New York against the popular Democratic incumbent, Herbert H. Lehman. Dewey was only 36 years old at the time. He based his campaign on his record as a famous prosecutor of organized-crime figures in New York City. Although he was defeated, Dewey's surprisingly strong showing against Lehman (he lost by only 1.4%) brought him national political attention and made him a frontrunner for the 1940 Republican presidential nomination. Jaeckle was one of Dewey's top advisors and mentors for the remainder of his political career.
In 1942, Dewey ran for governor again, and won with a large plurality over Democrat John J. Bennett Jr. Bennett was not endorsed by the American Labor Party, whose candidate drew almost 10%. The ALP did endorse incumbent Lieutenant Governor Charles Poletti who lost narrowly to Dewey's running mate Thomas W. Wallace. In 1946, Dewey was re-elected by the greatest margin in state history to that point, almost 700,000 votes. In 1950, he was elected to a third term by 572,000 votes.
Usually regarded as an honest and highly effective governor, Dewey doubled state aid to education; increased salaries for state employees; and still reduced the state's debt by over $100 million. He referred to his program as "pay-as-you-go liberalism...government can be progressive and solvent at the same time." Additionally, he put through the first state law in the country that prohibited racial discrimination in employment. As governor, Dewey also signed legislation that created the State University of New York. He played a leading role in securing support and funding for the New York State Thruway, which was eventually named in his honor. Dewey also streamlined and consolidated many state agencies to make them more efficient. During the Second World War construction in New York was limited, which allowed Dewey to create a $623 million budget surplus, which he placed into his "Postwar Reconstruction Fund." The fund would eventually create 14,000 new beds in the state's mental health system, provide public housing for 30,000 families, allow for the reforestation of 34 million trees, create a water pollution program, provide slum clearance, and pay for a "model veterans' program." Shortly after becoming governor in 1943, Dewey learned that some state workers and teachers were being paid only $900 a year, leading him to give "hefty raises, some as high as 150%" to state workers and teachers. His governorship was also "friendlier by far than his [Democratic] predecessors to the private sector", as Dewey created a state Department of Commerce to "lure new businesses and tourists to the Empire State, ease the shift from wartime boom, and steer small businessmen, in particular, through the maze of federal regulation and restriction." Between 1945 and 1948, 135,000 new businesses were started in New York.
One criticism that was made of Dewey as governor lay in his treatment of New York legislators and political opponents. Dewey "cracked the whip ruthlessly on (Republican) legislators who strayed from the party fold. Assemblymen have found themselves under investigation by the State Tax Department after opposing the Governor over an insurance regulation bill. Others discover job-rich construction projects, state buildings, even highways, directed to friendlier [legislators]." Dewey "forced the legislature his own party dominates to reform its comfortable ways of payroll padding. Now legislative workers must verify in writing every two weeks what they have been doing to earn their salary; every state senator and assemblyman must verify that [they] are telling the truth. All this has occasioned more than grumbling. Some Assemblymen have quit in protest. Others have been denied renomination by Dewey's formidable political organization. Reporters mutter among themselves about government by blackmail." However, Dewey did receive positive publicity for his reputation for honesty and integrity, as he "insisted on having every prospective holder of a job paying $2,500 or more rigorously probed by state police...[Dewey] accepted no anonymous campaign contributions, and had every large contributor not known personally to him investigated for motive", and, when he signed autographs, he would date them so that no one could imply a closer relationship than actually existed.
The journalists Neal Peirce and Jerry Hagstrom summarized Dewey's governorship by writing that "for sheer administrative talent, it is difficult to think of a twentieth-century governor who has excelled Thomas E. Dewey...hundreds of thousands of New York youngsters owe Dewey thanks for his leadership in creating a state university...a vigorous health-department program virtually eradicated tuberculosis in New York, highway building was pushed forward, and the state's mental hygiene program was thoroughly reorganized." With Jaeckle's help, Dewey also created a powerful political organization that allowed him to dominate New York state politics and influence national politics.
During his governorship, one writer observed that "A blunt fact about Mr. Dewey should be faced: it is that many people do not like him. He is, unfortunately, one of the least seductive personalities in public life. That he has made an excellent record as governor is indisputable. Even so, people resent what they call his vindictiveness, the "metallic" nature of his efficiency, his cockiness (which actually conceals a nature basically shy), and his suspiciousness. People say...that he is as devoid of charm as a rivet or a lump of stone."
He also strongly supported the death penalty. During his 12 years as Governor, over 90 people were electrocuted under New York authority. Among these were several of the mob-affiliated hitmen belonging to the murder-for-hire group Murder, Inc., which was headed up by major mob leaders Louis "Lepke" Buchalter and Albert Anastasia. Lepke himself went to the chair in 1944.
Presidential elections.
1940.
Dewey sought the 1940 Republican presidential nomination. He was considered the early favorite for the nomination, but his support ebbed in the late spring of 1940, as World War II suddenly became much more dangerous for America.
Some Republican leaders considered Dewey to be too young (he was only 38) and too inexperienced to lead the nation in wartime. Furthermore, Dewey's non-interventionist stance became problematic when Germany quickly conquered France, and seemed poised to invade Britain. As a result, many Republicans switched to Wendell Willkie, who was a decade older and supported aid to the Allies fighting Germany. Willkie lost to Franklin D. Roosevelt in the general election.
Dewey's foreign-policy position evolved during the 1940s; by 1944 he was considered an internationalist and a supporter of projects such as the United Nations. It was in 1940 that Dewey first clashed with Taft. Taft—who maintained his non-interventionist views and economic conservatism to his death—became Dewey's great rival for control of the Republican Party in the 1940s and early 1950s. Dewey became the leader of moderate-to-liberal Republicans, who were based in the northeastern and Pacific Coast states, while Taft became the leader of conservative Republicans who dominated most of the Midwest and parts of the South.
Dewey's biographer Richard Norton Smith wrote, "For fifteen years...these two combatants waged political warfare. Their dispute pitted East against Midwest, city against countryside, internationalist against isolationist, pragmatic liberals against principled conservatives. Each man thought himself the genuine spokesman of the future; each denounced the other as a political heretic." In a 1949 speech, Dewey criticized Taft and his followers by saying that "we have in our party some fine, high-minded patriotic people who honestly oppose farm price supports, unemployment insurance, old age benefits, slum clearance, and other social programs...these people believe in a laissez-faire society and look back wistfully to the miscalled 'good old days' of the nineteenth century...if such efforts to turn back the clock are actually pursued, you can bury the Republican Party as the deadest pigeon in the country." He added that people who opposed such social programs should "go out and try to get elected in a typical American community and see what happens to them. But they ought not to do it as Republicans."
However, in the speech Dewey added that the Republican Party believed in social progress "under a flourishing, competitive system of private enterprise where every human right is expanded...we are opposed to delivering the nation into the hands of any group who will have the power to tell the American people whether they may have food or fuel, shelter or jobs." Dewey believed in what he called "compassionate capitalism", and argued that "in the modern age, man's needs include as much economic security as is consistent with individual freedom." When Taft and his supporters criticized Dewey's policies as liberal "me-tooism", or "aping the New Deal in a vain attempt to outbid Roosevelt's heirs", Dewey responded that he was following in the tradition of Republicans such as Abraham Lincoln and Theodore Roosevelt, and that "it was conservative reforms like anti-trust laws and federal regulation of railroads...that retained the allegiance of the people for a capitalist system combining private incentive and public conscience."
1944.
Dewey was the frontrunner for the 1944 Republican nomination. In April 1944 he won the key Wisconsin primary, where he defeated Wendell Willkie, former Minnesota Governor Harold Stassen, and General Douglas MacArthur. Willkie's poor showing in Wisconsin forced him to quit the race. At the 1944 Republican Convention, Dewey's chief rivals—Stassen and Ohio Governor John W. Bricker—both withdrew and Dewey was nominated almost unanimously. Dewey then made Bricker (who was supported by Taft) his running mate.
In the general election campaign, Dewey crusaded against the alleged inefficiencies, corruption and Communist influences in incumbent President Roosevelt's New Deal programs, but avoided military and foreign policy debates.
Dewey lost the election to Roosevelt. However, Dewey polled 46% of the popular vote, a stronger showing against Roosevelt than any previous Republican opponent. Dewey was the first presidential candidate to be born in the 20th century. As of 2015, he was also the youngest Republican presidential nominee.
Dewey nearly included, in his campaign, claims that Roosevelt knew ahead of time about the attack on Pearl Harbor; Dewey added, "and instead of being re-elected he should be impeached." The U.S. military was extremely worried because that would let the Japanese know that the U.S. had broken the Purple code. Army General George C. Marshall made a persistent effort to persuade Dewey not to touch this topic; Dewey eventually yielded.
1948.
Dewey was the Republican candidate in the 1948 presidential election in which, in almost unanimous predictions by pollsters and the press, he was projected as the winner. His running mate was California governor Earl Warren. The "Chicago Daily Tribune" printed "DEWEY DEFEATS TRUMAN" as its post-election headline, issuing a few hundred copies before the returns showed that the winner was Harry S. Truman, the incumbent.
Indeed, given Truman's sinking popularity and the Democratic Party's three-way split (between Truman, Henry A. Wallace, and Strom Thurmond), Dewey had seemed unstoppable. Republicans figured that all they had to do to win was to avoid making any major mistakes, and as such Dewey did not take any risks. He spoke in platitudes, trying to transcend politics. Speech after speech was filled with empty statements of the obvious, such as the famous quote: "You know that your future is still ahead of you." An editorial in the "Louisville Courier-Journal" summed it up:
No presidential candidate in the future will be so inept that four of his major speeches can be boiled down to these historic four sentences: Agriculture is important. Our rivers are full of fish. You cannot have freedom without liberty. Our future lies ahead.
Part of the reason Dewey ran such a cautious, vague campaign came from his experience as a presidential candidate in 1944. In that election Dewey felt that he had allowed Roosevelt to draw him into a partisan, verbal "mudslinging" match, and he believed that this had cost him votes. As such, Dewey was convinced in 1948 to appear as non-partisan as possible, and to emphasize the positive aspects of his campaign while ignoring his opponent. This strategy proved to be a major mistake, as it allowed Truman to repeatedly criticize and ridicule Dewey, while Dewey never answered any of Truman's criticisms. Near the end of the campaign, Dewey considered adopting a more aggressive style and responding directly to Truman's criticisms, going so far as to tell his aides one evening that he wanted to "tear to shreds" a speech draft and make it more critical of the Democratic ticket. However, nearly all of his major advisors - including Edwin Jaeckle, Press Secretary James Hagerty, and aide Paul Lockwood - insisted that it would be a mistake to change tactics. Dewey's wife Frances strongly opposed her husband changing tactics, telling him, "If I have to stay up all night to see that you don't tear up that speech [draft], I will." Dewey relented and continued to ignore Truman's attacks and to focus on positive generalities instead of issue specifics.
Dewey was not as conservative as the Republican-controlled 80th Congress, which also proved problematic for him. Truman tied Dewey to the "do-nothing" Congress. Indeed, Dewey had successfully battled Taft and his conservatives for the nomination at the Republican Convention. Taft had remained a non-interventionist even through the Second World War. Dewey, however, supported the Marshall Plan, the Truman Doctrine, recognition of Israel, and the Berlin airlift.
Dewey was repeatedly urged by the right wing of his party to engage in red-baiting, but he refused. In a debate before the Oregon primary with Harold Stassen, Dewey argued against outlawing the Communist Party of the United States of America, saying "you can't shoot an idea with a gun." He later told Styles Bridges, the Republican national campaign manager, that he was not "going around looking under beds". Dewey was the only Republican to be nominated for President twice and lose both times.
1952.
Dewey did not run for President in 1952, but he played a major role in securing the Republican nomination for General Dwight Eisenhower. The 1952 campaign culminated in a climactic moment in the fierce rivalry between Dewey and Taft for control of the Republican Party.
Dewey played a key role in convincing Eisenhower to run against Taft. When Eisenhower became a candidate Dewey used his powerful political machine to win Eisenhower the support of delegates in New York and elsewhere.
Taft was an announced candidate and, given his age, he freely admitted 1952 would be his last chance to win the presidency. At the Republican Convention, pro-Taft delegates and speakers verbally attacked Dewey as the real power behind Eisenhower, but Dewey had the satisfaction of seeing Eisenhower win the nomination and end Taft's presidential hopes for the last time.
Dewey played a major role in helping California Senator Richard Nixon become Eisenhower's running mate. When Eisenhower won the Presidency later that year, many of Dewey's closest aides and advisors became leading figures in the Eisenhower Administration. Among them were Herbert Brownell, who would become Eisenhower's Attorney General, James Hagerty, who would become White House Press Secretary, and John Foster Dulles, who would become Eisenhower's Secretary of State.
Later career.
Dewey's third term as governor of New York expired at the end of 1954, after which he retired from public service and returned to his law practice, Dewey Ballantine, although he remained a power broker behind the scenes in the Republican Party. In 1956, when Eisenhower mulled not running for a second term, he suggested Dewey as his choice as successor, but party leaders made it plain that they would not entrust the nomination to Dewey yet again, and ultimately Eisenhower decided to run for re-election. Dewey also played a major role that year in convincing Eisenhower to keep Nixon as his running mate; Eisenhower had considered dropping Nixon from the Republican ticket and picking someone he felt would be less partisan and controversial. However, Dewey argued that dropping Nixon from the ticket would only anger Republican voters while winning Eisenhower few votes from the Democrats. Dewey's arguments helped convince Eisenhower to keep Nixon on the ticket. In 1960 Dewey would strongly support Nixon's ultimately unsuccessful presidential campaign against Democrat John F. Kennedy.
By the 1960s, as the conservative wing assumed more and more power within the Republican Party, Dewey removed himself further and further from party matters. When the Republicans in 1964 gave Senator Barry Goldwater of Arizona, Taft's successor as the conservative leader, their presidential nomination, Dewey declined to even attend the GOP Convention in San Francisco; it was the first Republican Convention he had missed since 1936. President Lyndon Johnson offered Dewey a number of positions on several blue ribbon commissions, as well as a seat on the U.S. Supreme Court, but Dewey declined them all, for he preferred to remain in political retirement and concentrate on his highly profitable law firm. By the early 1960s Dewey's law practice had made him into a multimillionaire.
Although closely identified with the Republican Party for virtually his entire adult life, Dewey was a close friend of Democratic Senator Hubert H. Humphrey, and Dewey aided Humphrey in being named as the Democratic nominee for vice-president in 1964, advising Lyndon Johnson on ways to block efforts at the party convention by Kennedy loyalists to stampede Robert Kennedy onto the ticket as Johnson's running mate.
Frances Dewey died in the summer of 1970 after battling breast cancer for six years. Later in 1970 Dewey began to date actress Kitty Carlisle, and there was talk of marriage between them. However, he died suddenly of a massive heart attack on March 16, 1971, eight days before his 69th birthday, while vacationing with friend Dwayne Andreas in Miami, Florida, following a round of golf with Boston Red Sox player Carl Yastrzemski. He was 68 years old. Following a public memorial service at Saint James' Episcopal Church in New York City, which was attended by President Richard Nixon, former Vice-President Humphrey, and other prominent politicians, Dewey was buried next to his wife Frances in the town cemetery of Pawling, New York. After his death his farm of Dapplemere was sold and renamed "Dewey Lane Farm" in his honor.
Public perception.
Dewey first came to nationwide attention as the "gangbuster", becoming a household name in the U.S. even before he entered presidential politics. At the age of 37, he was perceived as a rising star in the Republican Party and frontrunner for the presidential nomination in 1940. During that campaign with the war in Europe intensifying, he was widely considered too young and inexperienced for the presidency and lost the nomination to Wendell Willkie. His visibility propelled him to the governorship in 1942 and the 1944 Republican presidential nomination. Dewey was a forceful and inspiring speaker, traveling the whole country during his presidential campaigns and attracting uncommonly huge crowds.
During the 1944 election campaign, Dewey suffered an unexpected blow when a remark attributed to socialite Alice Roosevelt Longworth (daughter of Theodore Roosevelt) mocked Dewey as "the little man on the wedding cake" (alluding to his neat mustache and dapper dress). It was ridicule he could never shake.
Dewey received varied reactions from the public. Most praising his good intentions, honesty, administrative talents, and vague yet inspiring speeches, but most also criticized his perceived stiffness, coldness, and aggressiveness in public. One of his biographers wrote that he had "a personality that attracted contempt and adulation in equal proportion." His friend and neighbor Lowell Thomas believed that Dewey was "an authentic colossus" whose "appetite for excellence [tended] to frighten less obsessive types", and his 1948 running mate, California Governor and future Chief Justice Earl Warren, "professed little personal affection for Dewey, but [believed] him a born executive who would make a great president." On the other hand, President Franklin D. Roosevelt privately called Dewey "the little man" and a "son of a bitch", and to Robert Taft and other conservative Republicans Dewey "became synonymous with...New York newspapers, New York banks, New York arrogance - the very city Taft's America loves to hate." A Taft supporter once referred to Dewey as "that snooty little governor of New York."
Dewey alienated former Republican president Herbert Hoover, who confided to a friend "Dewey has no inner reservoir of knowledge on which to draw for his thinking," elaborating that "A man couldn't wear a mustache like that without having it affect his mind." However, the famed newspaper editor William Allen White praised Dewey as "an honest cop with the mind of an honest cop" and the pollster George Gallup once stated that Dewey was "the ablest public figure of his lifetime...the most misunderstood man in recent American history."
His presidential campaigns were hampered by Dewey's habit of making overly vague statements, defining his strategy as not being "prematurely specific" on controversial issues. In 1948, President Truman poked fun at Dewey's vague campaign by joking that the GOP (Republican Party) actually stood for "grand old platitudes." Dewey's frequent refusal to discuss specific issues and proposals in his campaigns was based partly on his belief in public opinion polls; one biographer claimed that he "had an almost religious belief in the revolutionary science of public-opinion sampling." He was the first presidential candidate to employ his own team of pollsters, and when a worried businessman told Dewey in the 1948 presidential campaign that he was losing ground to Truman and urged him to "talk specifics in his closing speeches", Dewey and his aide Paul Lockwood displayed polling data that showed Dewey still well ahead of Truman, and Dewey told the businessman "when you're leading, don't talk."
In 1940, Walter Lippman regarded him as an opportunist, who "changes his views from hour to hour… always more concerned with taking the popular position than he is in dealing with the real issues." The journalist John Gunther wrote that "There are plenty of vain and ambitious and uncharming politicians. This would not be enough to cause Dewey's lack of popularity. What counts more is that so many people think of him as opportunistic. Dewey seldom goes out on a limb by taking a personal position which may be unpopular...every step is carefully calculated and prepared." Adding to that, he had a tendency towards pomposity and was considered stiff and unapproachable in public, with his aide Ruth McCormick Simms once describing him as "cold, cold as a February iceberg". She however added that "he was brilliant and thoroughly honest." Leo O'Brien, a reporter for the United Press International (UPI), recalled Dewey in an interview by saying that "I hated his guts when he first came to Albany, and I loved him by the time he left. It was almost tragic – how he put on a pose that alienated people. Behind a pretty thin veneer he was a wonderful guy." John Gunther wrote in 1947 that some "people may not "like" Dewey, but (a) an inner core of advisers and friends, including some extremely distinguished people, have a loyalty to him little short of idolatrous, and (b) he is one of the greatest vote-getters in the history of the nation."
Journalist Irwin Ross summed up the contradictions in Dewey's personality by noting that "more than most politicians, he displayed an enormous gap between his private and his public manner. To friends and colleagues he was warm and gracious, considerate of others' views… He could tell a joke and was not dismayed by an off-color story. In public, however, he tended to freeze up, either out of diffidence or too stern a sense of the dignity of office. The smiles would seem forced… the glad-handing gesture awkward."
Legacy.
In 1964, the New York State legislature officially renamed the New York State Thruway in honor of Dewey. Signs on Interstate 95 between the end of the Bruckner Expressway (in the Bronx) and the Connecticut state line, as well as on the Thruway mainline (Interstate 87 between the Bronx-Westchester line and Albany, and Interstate 90 between Albany and the New York-Pennsylvania line) designate the name as "Governor Thomas E. Dewey Thruway," though this official designation is rarely used in reference to these roads.
Dewey's official papers from his years in politics and public life were given to the University of Rochester; they are housed in the university library and are available to historians and other writers.
In 2005, the New York City Bar Association named an award after Dewey. The Thomas E. Dewey Medal, sponsored by the law firm of Dewey & LeBoeuf LLP, is awarded annually to one outstanding Assistant District Attorney in each of New York City's five counties (New York, Kings, Queens, Bronx, and Richmond). The Medal was first awarded on November 29, 2005.
In May 2012, Dewey & LeBoeuf (the successor firm to Dewey Ballantine) filed for bankruptcy.

</doc>
<doc id="45597" url="http://en.wikipedia.org/wiki?curid=45597" title="Henry V of England">
Henry V of England

Henry V (9 August 1387 – 31 August 1422) was King of England from 1413 until his death at the age of 35 in 1422. He was the second English monarch who came from the House of Lancaster.
After military experience fighting the Welsh during the revolt of Owain Glyn Dwr, and against the powerful aristocratic Percys of Northumberland at the Battle of Shrewsbury, Henry came into political conflict with his father, whose health was increasingly precarious from 1405 onward. After his father's death in 1413, Henry assumed control of the country and embarked on war with France in the ongoing Hundred Years' War (1337–1453) between the two nations. His military successes culminated in his famous victory at the Battle of Agincourt (1415) and saw him come close to conquering France. After months of negotiation with Charles VI of France, the Treaty of Troyes (1420) recognized Henry V as regent and heir-apparent to the French throne, and he was subsequently married to Charles's daughter, Catherine of Valois (1401–37). Following Henry V's sudden and unexpected death in France two years later, he was succeeded by his infant son, who reigned as Henry VI (1422–61, 1470–71).
Early life.
Henry was born in the tower above the gatehouse of Monmouth Castle (and for that reason was sometimes called Henry of Monmouth). He was the son of Henry of Bolingbroke, later Henry IV, and sixteen-year-old Mary de Bohun. He was also the grandson of the influential John of Gaunt and great-grandson of Edward III of England. At the time of his birth, Richard II, his cousin once removed, was king. As he was not close to the line of succession to the throne, Henry's date of birth was not officially documented. His grandfather, John of Gaunt, was the guardian of the king at that time. 16 September is the date accepted by historians (although 9 August has also been suggested), in either 1386 or 1387.
Upon the exile of Henry's father in 1398, Richard II took the boy into his own charge and treated him kindly. The young Henry accompanied King Richard to Ireland, and while in the royal service, he visited Trim Castle in County Meath, the ancient meeting place of the Irish Parliament. In 1399, Henry's grandfather died. The same year King Richard II was overthrown by the Lancastrian usurpation that brought Henry's father to the throne, and Henry was recalled from Ireland into prominence as heir apparent to the Kingdom of England. He was created Prince of Wales at his father's coronation, and Duke of Lancaster on 10 November 1399, the third person to hold the title that year. His other titles were Duke of Cornwall, Earl of Chester, and Duke of Aquitaine. A contemporary record notes that during that year Henry spent time at The Queen's College, Oxford, under the care of his uncle Henry Beaufort, the Chancellor of the university. From 1400 to 1404, he carried out the duties of High Sheriff of Cornwall.
Less than three years later, Henry was in command of part of the English forces—he led his own army into Wales against Owain Glyndŵr and joined forces with his father to fight Harry Hotspur at the Battle of Shrewsbury in 1403. It was there that the sixteen-year-old prince was almost killed by an arrow that became stuck in his face. An ordinary soldier might have died from such a wound, but Henry had the benefit of the best possible care. Over a period of several days, John Bradmore, the royal physician, treated the wound with honey to act as an antiseptic, crafted a tool to screw into the broken arrow shaft and thus extract the arrow without doing further damage, and then flushed the wound with alcohol. The operation was successful, but it left Henry with permanent scars, evidence of his experience in battle. For eighteen months, in 1410–11, Henry was in control of the country during his father's ill health, and he took full advantage of the opportunity to impose his own policies, but when the king recovered, he reversed most of these and dismissed the prince from his council.
Role in government and conflict with Henry IV.
The Welsh revolt of Owain Glyndŵr absorbed Henry's energies until 1408. Then, as a result of the king's ill health, Henry began to take a wider share in politics. From January 1410, helped by his uncles Henry and Thomas Beaufort – legitimised sons of John of Gaunt – he had practical control of the government.
Both in foreign and domestic policy he differed from the king, who in November 1411 discharged the prince from the council. The quarrel of father and son was political only, though it is probable that the Beauforts had discussed the abdication of Henry IV, and their opponents certainly endeavoured to defame the prince.
Supposed riotous youth.
It may be that the tradition of Henry's riotous youth, immortalised by Shakespeare, is partly due to political enmity. Henry's record of involvement in war and politics, even in his youth, disproves this tradition. The most famous incident, his quarrel with the chief justice, has no contemporary authority and was first related by Sir Thomas Elyot in 1531.
The story of Falstaff originated in Henry's early friendship with Sir John Oldcastle, a supporter of the Lollards. Shakespeare's Falstaff was originally named "Oldcastle", following his main source, "The Famous Victories of Henry V". However, his descendants objected, and the name was changed (the character became a composite of several real persons, including Sir John Fastolf). That friendship, and the prince's political opposition to Thomas Arundel, Archbishop of Canterbury, perhaps encouraged Lollard hopes. If so, their disappointment may account for the statements of ecclesiastical writers like Thomas Walsingham that Henry, on becoming king, was suddenly changed into a new man.
Accession to the throne.
After Henry IV died on 20 March 1413, Henry V succeeded him and was crowned on 9 April 1413 at Westminster Abbey. The ceremony was marked by a terrible snowstorm, but the common people were undecided as to whether it was a good or bad omen. Henry was described as having been "very tall (6ft 3 in), slim, with dark hair cropped in a ring above the ears, and clean-shaven". His complexion was ruddy, the face lean with a prominent and pointed nose. Depending on his mood, his eyes "flashed from the mildness of a dove's to the brilliance of a lion's".
Domestic policy.
Henry tackled all of the domestic policies together and gradually built on them a wider policy. From the first, he made it clear that he would rule England as the head of a united nation. On the one hand, he let past differences be forgotten – the late Richard II was honourably re-interred; the young Mortimer was taken into favour; the heirs of those who had suffered in the last reign were restored gradually to their titles and estates. On the other hand, where Henry saw a grave domestic danger, he acted firmly and ruthlessly – such as the Lollard discontent in January 1414, including the execution by burning of Henry's old friend Sir John Oldcastle, so as to "nip the movement in the bud" and make his own position as ruler secure.
His reign was generally free from serious trouble at home. The exception was the Southampton Plot in favour of Mortimer, involving Henry Scrope, 3rd Baron Scrope of Masham and Richard, Earl of Cambridge (grandfather of the future King Edward IV), in July 1415.
Starting in August 1417, Henry V promoted the use of the English language in government, and his reign marks the appearance of Chancery Standard English as well as the adoption of English as the language of record within Government. He was the first king to use English in his personal correspondence since the Norman conquest, which had occurred 350 years earlier.
Foreign affairs.
Diplomacy.
Henry could now turn his attention to foreign affairs. A writer of the next generation was the first to allege that Henry was encouraged by ecclesiastical statesmen to enter into the French war as a means of diverting attention from home troubles. This story seems to have no foundation. Old commercial disputes and the support the French had lent to Owain Glyndŵr were used as an excuse for war, while the disordered state of France afforded no security for peace. The French king, Charles VI, was prone to mental illness, at times he thought he was made of glass, and his eldest son was an unpromising prospect. However, it was the old dynastic claim to the throne of France, first pursued by Edward III, that justified war with France in English opinion.
Following Agincourt, Hungarian King (later Holy Roman Emperor 1433–37) Sigismund made a visit to Henry in hopes of making peace between England and France. His goal was to persuade Henry to modify his demands against the French. Henry lavishly entertained the emperor and even had him enrolled in the Order of the Garter. Sigismund, in turn, inducted Henry into the Order of the Dragon. Henry had intended to crusade for the order after uniting the English and French thrones, but he died before fulfilling his plans. Sigismund left England several months later, having signed the Treaty of Canterbury, acknowledging English claims to France.
Campaigns in France.
Henry may have regarded the assertion of his own claims as part of his royal duty, but in any case, a permanent settlement of the national debate was essential to the success of his foreign policy.
1415 campaign.
On 11 August 1415, Henry sailed for France, where his forces besieged the fortress at Harfleur, capturing it on 22 September. Afterwards, Henry decided to march with his army across the French countryside towards Calais, despite the warnings of his council. On 25 October 1415, on the plains near the village of Agincourt, a French army intercepted his route. Despite his men-at-arms being exhausted, outnumbered and malnourished, Henry led his men into battle, decisively defeating the French, who suffered severe losses. It is often argued that the French men-at-arms were bogged down in the muddy battlefield, soaked from the previous night of heavy rain, and that this hindered the French advance, allowing them to be sitting targets for the flanking English and Welsh archers. Most were simply hacked to death while completely stuck in the deep mud. Nevertheless, the victory is seen as Henry's greatest, ranking alongside the battle of Poitiers.
During the battle, Henry ordered that the French prisoners taken during the battle be put to death, including some of the most illustrious who could be used for ransom. Cambridge Historian Brett Tingley posits that Henry was concerned that the prisoners might turn on their captors when the English were busy repelling a third wave of enemy troops, thus jeopardising a hard-fought victory.
The victorious conclusion of Agincourt, from the English viewpoint, was only the first step in the campaign to recover the French possessions that he felt belonged to the English crown. Agincourt also held out the promise that Henry's pretensions to the French throne might be realised.
Diplomacy and command of the sea.
Command of the sea was secured by driving the Genoese allies of the French out of the English Channel. While Henry was occupied with peace negotiations in 1416, a French and Genoese fleet surrounded the harbour at the English-garrisoned Harfleur. A French land force also besieged the town. To relieve Harfleur, Henry sent his brother, John of Lancaster, the Duke of Bedford, who raised a fleet and set sail from Beachy Head on 14 August. The Franco-Genoese fleet was defeated the following day after a gruelling seven-hour battle, and Harfleur was relieved. Diplomacy successfully detached Emperor Sigismund from France, and the Treaty of Canterbury (1416) paved the way to end the schism in the Church.
1417–20 campaign.
So, with those two potential enemies gone, and after two years of patient preparation following the Battle of Agincourt, Henry renewed the war on a larger scale in 1417. Lower Normandy was quickly conquered, and Rouen was cut off from Paris and besieged. This siege cast an even darker shadow on the reputation of the king than his order to slay the French prisoners at Agincourt. Rouen, starving and unable to support the women and children of the town, forced them out through the gates believing that Henry would allow them to pass through his army unmolested. However, Henry refused to allow this, and the expelled women and children died of starvation in the ditches surrounding the town. The French were paralysed by the disputes between Burgundians and Armagnacs. Henry skilfully played them off one against the other, without relaxing his warlike approach.
In January 1419, Rouen fell. Those Norman French who had resisted were severely punished: Alain Blanchard, who had hanged English prisoners from the walls of Rouen, was summarily executed; Robert de Livet, Canon of Rouen, who had excommunicated the English king, was packed off to England and imprisoned for five years.
By August, the English were outside the walls of Paris. The intrigues of the French parties culminated in the assassination of John the Fearless by the Dauphin's partisans at Montereau (10 September 1419). Philip the Good, the new Duke, and the French court threw themselves into Henry's arms. After six months of negotiation, the Treaty of Troyes recognised Henry as the heir and regent of France (see English Kings of France), and on 2 June 1420, he married Catherine of Valois, the French king's daughter. From June to July 1420, Henry's army besieged and took the castle at Montereau. He besieged and captured Melun in November, returning to England shortly thereafter.
1421 campaign and death.
While he was in England, Henry's brother Thomas, Duke of Clarence led the English forces in France. In March 1421, Thomas led the English to a disastrous defeat at the Battle of Baugé against an army of mainly Scottish allies of the Dauphin. The Duke himself was killed in the battle. On 10 June 1421, Henry sailed back to France to retrieve the situation. It would be his last military campaign. From July to August, Henry's forces besieged and captured Dreux, thus relieving allied forces at Chartres. That October, his forces lay siege to Meaux, capturing it on 2 May 1422.
Henry V died suddenly on 31 August 1422 at the Château de Vincennes near Paris, apparently from dysentery, perhaps toxic megacolon, which he had contracted during the siege of Meaux. He was 36 years old.
Shortly before his death, Henry V named his brother John, Duke of Bedford, regent of France in the name of his son Henry VI, then only a few months old. Henry V did not live to be crowned King of France himself, as he might confidently have expected after the Treaty of Troyes, because ironically, the sickly Charles VI, to whom he had been named heir, survived him by two months. Henry's comrade-in-arms and Lord Steward John Sutton, 1st Baron Dudley brought the body of King Henry home to England and bore the royal standard at his funeral. Henry V was buried in Westminster Abbey on 7 November 1422.
Arms.
As Prince of Wales, Henry's arms were those of the kingdom, differenced by a label argent of three points. Upon his accession, he inherited use of the arms of the kingdom undifferenced.
Marriage and ancestry.
He married Catherine of Valois in 1420, and their only child was Henry, who became Henry VI of England.

</doc>
<doc id="45598" url="http://en.wikipedia.org/wiki?curid=45598" title="Strepsiptera">
Strepsiptera

The Strepsiptera (translation: "twisted wing", giving rise to the insects' common name, twisted-wing parasites) are an endopterygote order of insects with nine extant families making up about 600 species. The early-stage larvae and the short-lived adult males are non-sessile, but most of their lives are spent as endoparasites in other insects, such as bees, wasps, leafhoppers, silverfish, and cockroaches.
Appearance and biology.
Males of the Strepsiptera have wings, legs, eyes, and antennae, and superficially look like flies, though their mouthparts cannot be used for feeding. Many of their mouthparts are modified into sensory structures. Adult males are very 
short-lived, usually surviving less than five hours, and do not feed. Females, in all families except the Mengenillidae, are not known to leave their hosts and are neotenic in form, lacking wings, legs, and eyes. Virgin females release a pheromone which the males use to locate them. In the Stylopidia, the female's anterior region protrudes out of the host body and the male mates by rupturing the female's brood canal opening, which lies between the head and 
prothorax. Sperm passes through the opening in a process termed hypodermic insemination. The offspring 
consume their mother from the inside in a process known as hemocelous viviparity. Each female thus produces many thousands of triungulin larvae that emerge from the brood opening on the head, which protrudes outside the host body. These larvae have legs (which lack a trochanter, the leg segment that forms the articulation between the basal coxa and the femur), and actively search out new hosts. Their hosts include members belonging to the orders Zygentoma, Orthoptera, Blattodea, Mantodea, Heteroptera, Hymenoptera, and Diptera. In the Strepsipteran family Myrmecolacidae, the males parasitize ants while the females parasitize Orthoptera. 
Strepsiptera eggs hatch inside the female, and the planidium larvae can move around freely within the female's haemocoel, which is unique to these animals. The female has a brood canal that communicates with the outside world, and the larvae escape through this. The larvae are very active, as they only have a limited amount of time to find a host before they exhaust their food reserves. These first-instar larvae have stemmata (simple, single-lens eyes), and once they latch onto a host, they enter it by secreting enzymes that soften the cuticle, 
usually in the abdominal region of the host. Some species have been reported to enter the eggs of hosts. Larvae of "Stichotrema dallatorreanurn" Hofeneder from Papua New Guinea were found to enter their orthopteran host's tarsus 
(foot). Once inside the host, they undergo hypermetamorphosis and become a less-mobile, legless larval form. They induce the host to produce a bag-like structure inside which they feed and grow. This structure, made from host tissue, protects them from the immune defences of the host. Larvae go through four more 
instars, and in each moult there is separation of the older cuticle but no discarding ("apolysis without ecdysis"), leading to multiple layers being formed around the larvae.
Male larvae produce pupae after the last moult, but females directly become neotenous adults. The colour and shape of the host's abdomen may be changed and the host usually becomes sterile. The parasites then undergo pupation to become adults. Adult males emerge from the host bodies, while females stay inside. Females may occupy up to 90% of the abdominal volume of their hosts.
Adult male Strepsiptera have eyes unlike those of any other insect, 
resembling the schizochroal eyes found in the trilobite group known 
as Phacopida. Instead of a compound eye consisting of hundreds 
to thousands of ommatidia, each with a single lens and capable of 
producing a picture element (pixel), the strepsipteran eyes consist of 
only a few dozen "eyelets" that each produce a complete image. These 
eyelets are separated by cuticle and/or setae, giving the cluster eye as
a whole a blackberry-like appearance.
Very rarely multiple females may live within a single stylopized host; 
multiple males within a single host are somewhat more common. Adult 
males are rarely observed, however, although specimens may be lured 
using cages containing virgin females. Nocturnal specimens can also be 
collected at light traps.
Strepsiptera may alter the behaviour of their hosts. Myrmecolacids may 
cause their ant hosts to climb up the tips of grass leaves, possibly to 
increase the spread of female pheromones to increase the chances of 
being located by males.
Classification.
The order, named by William Kirby in 1813, is named for the hind wings ("strepsi" = twisted, "ptera" = wing), which are held at a twisted angle when at rest. The forewings are reduced to halteres (and initially were thought to be dried and twisted).
Strepsiptera are an enigma to taxonomists. Originally, they were believed to be the sister group to the beetle families Meloidae and Ripiphoridae, which have similar parasitic development and forewing reduction; early molecular research suggested their inclusion as a sister group to the flies, in a clade called the halteria, which have one pair of the wings modified into halteres, and failed to support their relationship to the beetles. More recent molecular studies, however, suggest they are outside the clade Mecopterida (containing the Diptera and Lepidoptera), yet there is no strong evidence for affinity with any other extant group. Study of their evolutionary position has been problematic due to difficulties in phylogenetic analysis arising from long branch attraction. The most basal strepsipteran is the fossil "Protoxenos janzeni" discovered in Baltic amber, while the most basal living strepsipteran is "Bahiaxenos relictus", the sole member of the family Bahiaxenidae. The earliest known strepsipteran fossil is that of "Cretostylops engeli", discovered in middle Cretaceous amber from Myanmar.
A molecular study strongly suggests the closest relations to this order are the Coleoptera (beetles).
Families.
The Strepsiptera have two major groups: Stylopidia and Mengenillidia. The Mengenillidia include three extinct families (Cretostylopidae, Protoxenidae, and Mengeidae) plus two extant families (Bahiaxenidae and Mengenillidae; the latter is not monophyletic, however.) They are considered more primitive, and the known females (Mengenillidae only) are free-living, with rudimentary legs and antennae. The females have a single genital opening. The males have strong mandibles, a distinct labrum, and more than five antennal segments.
The other group, Stylopidia, includes seven families: Corioxenidae, Halictophagidae, Callipharixenidae, Bohartillidae, Elenchidae, Myrmecolacidae, and Stylopidae. All Stylopidia have endoparasitic females having multiple genital openings.
Stylopidae have four-segmented tarsi and four- to six-segmented antennae, with the third segment having a lateral process. The family Stylopidae may be paraphyletic. The Elenchidae have two-segmented tarsi and four-segmented antennae, with the third segment having a lateral process. The Halictophagidae have three-segmented tarsi and seven-segmented antennae, with lateral processes from the third and fourth segments.
The Stylopidae mostly parasitize wasps and bees, the Elenchidae are known to parasitize Fulgoroidea, while the Halictophagidae are found on leafhoppers, treehoppers and mole cricket hosts.
Strepsipteran insects in the genus "Xenos" parasitize Polistes carnifex, a species of social wasps. These obligate parasites infect the developing wasp larvae in the nest and are present within the abdomens of female wasps when they hatch out. Here they remain until they thrust through the cuticle and pupate (males) or release infective first-instar larvae onto flowers (females). These larvae are transported back to their nests by foraging wasps.

</doc>
<doc id="45599" url="http://en.wikipedia.org/wiki?curid=45599" title="Surgery">
Surgery

Surgery (from the Greek: χειρουργική "cheirourgikē" (composed of χείρ, "hand", and ἔργον, "work"), via Latin: "chirurgiae", meaning "hand work") is an ancient medical specialty that uses operative manual and instrumental techniques on a patient to investigate and/or treat a pathological condition such as disease or injury, to help improve bodily function or appearance or to repair unwanted ruptured areas (for example, a perforated ear drum).
An act of performing surgery may be called a "surgical procedure", operation, or simply surgery. In this context, the verb operate means to perform surgery. The adjective surgical means pertaining to surgery; e.g. surgical instruments or surgical nurse. The patient or subject on which the surgery is performed can be a person or an animal. A surgeon is a person who practises surgery and a surgeon's assistant is a person who practices surgical assistance. A surgical team is made up of surgeon, surgeon's assistant, anesthesia provider, circulating nurse and surgical technologist. Surgery usually spans minutes to hours, but it is typically not an ongoing or periodic type of treatment. The term "surgery" can also refer to the place where surgery is performed, or simply the office of a physician, dentist, or veterinarian.
Elective surgery generally refers to a surgical procedure that can be scheduled in advance because it does not involve a medical emergency. Plastic, or cosmetic surgeries are common elective procedures.
Definitions.
Surgery is a technology consisting of a physical intervention on tissues, and muscle.
As a general rule, a procedure is considered surgical when it involves cutting of a patient's tissues or closure of a previously sustained wound. Other procedures that do not necessarily fall under this rubric, such as angioplasty or endoscopy, may be considered surgery if they involve "common" surgical procedure or settings, such as use of a sterile environment, anesthesia, antiseptic conditions, typical surgical instruments, and suturing or stapling. All forms of surgery are considered invasive procedures; so-called "noninvasive surgery" usually refers to an excision that does not penetrate the structure being excised (e.g. laser ablation of the cornea) or to a radiosurgical procedure (e.g. irradiation of a tumor).
Types of surgery.
Surgical procedures are commonly categorized by urgency, type of procedure, body system involved, degree of invasiveness, and special instrumentation.
Description of surgical procedure.
Location.
At a hospital, modern surgery is often done in an operating theater using surgical instruments, an operating table for the patient, and other equipment. Among United States hospitalizations for nonmaternal and nonneonatal conditions in 2012, more than one-fourth of stays and half of hospital costs involved stays that included operating room (OR) procedures. The environment and procedures used in surgery are governed by the principles of aseptic technique: the strict separation of "sterile" (free of microorganisms) things from "unsterile" or "contaminated" things. All surgical instruments must be sterilized, and an instrument must be replaced or re-sterilized if it becomes contaminated (i.e. handled in an unsterile manner, or allowed to touch an unsterile surface). Operating room staff must wear sterile attire (scrubs, a scrub cap, a sterile surgical gown, sterile latex or non-latex polymer gloves and a surgical mask), and they must scrub hands and arms with an approved disinfectant agent before each procedure. There is moderate-quality evidence that usage of two layers of gloves compared to single gloving during surgery reduces perforations and blood stains on the skin, indicating a decrease in percutaneous exposure incidents.
Patient Safety.
The suggests that patients should ask your doctor questions.
Preoperative care.
Prior to surgery, the patient is given a medical examination, certain pre-operative tests, and their physical status is rated according to the ASA physical status classification system. If these results are satisfactory, the patient signs a consent form and is given a surgical clearance. If the procedure is expected to result in significant blood loss, an autologous blood donation may be made some weeks prior to surgery. If the surgery involves the digestive system, the patient may be instructed to perform a bowel prep by drinking a solution of polyethylene glycol the night before the procedure. Patients are also instructed to abstain from food or drink (an NPO order after midnight on the night before the procedure), to minimize the effect of stomach contents on pre-operative medications and reduce the risk of aspiration if the patient vomits during or after the procedure.
Some medical systems have a practice of routinely performing chest x-rays before surgery. The premise behind this practice is that the physician might discover some unknown medical condition which would complicate the surgery, and that upon discovering this with the chest x-ray, the physician would adapt the surgery practice accordingly. In fact, medical specialty professional organizations recommend against routine pre-operative chest x-rays for patients who have an unremarkable medical history and presented with a physical exam which did not indicate a chest x-ray. Routine x-ray examination is more likely to result in problems like misdiagnosis, overtreatment, or other negative outcomes than it is to result in a benefit to the patient. Likewise, other tests including complete blood count, prothrombin time, partial thromboplastin time, basic metabolic panel, and urinalysis should not be done unless the results of these tests can help evaluate surgical risk.
Staging for surgery.
In the pre-operative holding area, the patient changes out of his or her street clothes and is asked to confirm the details of his or her surgery. A set of vital signs are recorded, a peripheral IV line is placed, and pre-operative medications (antibiotics, sedatives, etc.) are given. When the patient enters the operating room, the skin surface to be operated on, called the operating field, is cleaned and prepared by applying an antiseptic such as chlorhexidine gluconate or povidone-iodine to reduce the possibility of infection. If hair is present at the surgical site, it is clipped off prior to prep application. The patient is assisted by an anesthesiologist or resident to make a specific surgical position, then sterile drapes are used to cover all of the patient's body except for the head and the surgical site or at least a wide area surrounding the operating field; the drapes are clipped to a pair of poles near the head of the bed to form an "ether screen", which separates the anesthetist/anesthesiologist's working area (unsterile) from the surgical site (sterile).
Anesthesia is administered to prevent pain from incision, tissue manipulation and suturing. Based on the procedure, anesthesia may be provided locally or as general anesthesia. Spinal anesthesia may be used when the surgical site is too large or deep for a local block, but general anesthesia may not be desirable. With local and spinal anesthesia, the surgical site is anesthetized, but the patient can remain conscious or minimally sedated. In contrast, general anesthesia renders the patient unconscious and paralyzed during surgery. The patient is intubated and is placed on a mechanical ventilator, and anesthesia is produced by a combination of injected and inhaled agents. 
Choice of surgical method and anaesthetic technique aims to reduce risk of complications, shorten time needed for recovery and minimise the surgical stress response.
Surgery.
An incision is made to access the surgical site. Blood vessels may be clamped or cauterized to prevent bleeding, and retractors may be used to expose the site or keep the incision open. The approach to the surgical site may involve several layers of incision and dissection, as in abdominal surgery, where the incision must traverse skin, subcutaneous tissue, three layers of muscle and then peritoneum. In certain cases, bone may be cut to further access the interior of the body; for example, cutting the skull for brain surgery or cutting the sternum for thoracic (chest) surgery to open up the rib cage. Whilst in surgery health and safety is used to prevent infection or further spreading of the disease. The surgeon will remove hair from the face and eyes, using a head hat. Hands, wrists and forearms are washed thoroughly to prevent germs getting into the operated body, then gloves are placed onto the hands. A PVC apron will be worn at all times, to stop any contamination. A yellow substance – typically an antiseptic iodine solution – is lighly coated onto the located area of the patient's body that will be performed on, this stops germs and disease infecting areas of the body, whilst the patient is being cut into.
Work to correct the problem in body then proceeds. This work may involve:
Blood or blood expanders may be administered to compensate for blood lost during surgery. Once the procedure is complete, sutures or staples are used to close the incision. Once the incision is closed, the anesthetic agents are stopped and/or reversed, and the patient is taken off ventilation and extubated (if general anesthesia was administered).
Post-operative care.
After completion of surgery, the patient is transferred to the post anesthesia care unit and closely monitored. When the patient is judged to have recovered from the anesthesia, he/she is either transferred to a surgical ward elsewhere in the hospital or discharged home. During the post-operative period, the patient's general function is assessed, the outcome of the procedure is assessed, and the surgical site is checked for signs of infection. There are several risk factors associated with postoperative complications, such as immune deficienty and obesity. Obesity has long been considered a risk factor for adverse post-surgical outcomes. It has been linked to many disorders such as obesity hypoventilation syndrome, atelectasis and pulmonary embolism, adverse cardiovascular effects, and wound healing complications. If removable skin closures are used, they are removed after 7 to 10 days post-operatively, or after healing of the incision is well under way.
It is not uncommon for surgical drains (see Drain (surgery)) to be required to remove blood or fluid from the surgical wound during recovery. Mostly these drains stay in until the volume tapers off, then they are removed. These drains can become clogged, leading to retained blood complications or abscess.
Postoperative therapy may include adjuvant treatment such as chemotherapy, radiation therapy, or administration of medication such as anti-rejection medication for transplants. Other follow-up studies or rehabilitation may be prescribed during and after the recovery period.
The use of topical antibiotics on surgical wounds does not reduce infection rates in comparison with non-antibiotic ointment or no ointment at all. Antibiotic ointments will irritate the skin, slow healing, and greatly increase risk of developing contact dermatitis and antibiotic resistance. Because of this, they should only be used when a person shows signs of infection and not as a preventative.
Through a retrospective analysis of national administrative data, the association between mortality and day of elective surgical procedure suggests a higher risk in procedures carried out later in the working week and on weekends. The odds of death were 44% and 82% higher respectively when comparing procedures on a Friday to a weekend procedure. This “weekday effect” has been postulated to be from several factors including poorer availability of services on a weekend, and also, decrease number and level of experience over a weekend.
Epidemiology.
Of the 38.6 million hospital stays that occurred in U.S. hospitals in 2011, 29% included at least one operating room procedure. These stays accounted for 48% of the total $387 billion in hospital costs.
In total, there were over 15 million operating room procedures performed in U.S. hospitals in 2011. The overall number of procedures remained stable from 2001-2011.
A study of data from 2003-2011 showed that U.S. hospital costs were highest for the surgical service line; the surgical service line costs were $17,600 in 2003 and projected to be $22,500 in 2013. For hospital stays in 2012 in the United States, private insurance had the highest percentage of surgical expenditure. Mean hospital costs in the United States in 2012 were highest for surgical stays.
In special populations.
Elderly people.
Older adults have widely varying physical health. Frail elderly people are at significant risk of post-surgical complications and the need for extended care. Assessment of older patients before elective surgeries can accurately predict the patients' recovery trajectories. One frailty scale uses five items: unintentional weight loss, muscle weakness, exhaustion, low physical activity, and slowed walking speed. A healthy person scores 0; a very frail person scores 5. Compared to non-frail elderly people, people with intermediate frailty scores (2 or 3) are twice as likely to have post-surgical complications, spend 50% more time in the hospital, and are three times as likely to be discharged to a skilled nursing facility instead of to their own homes. Frail elderly patients (score of 4 or 5) have even worse outcomes, with the risk of being discharged to a nursing home rising to twenty times the rate for non-frail elderly people.
Other populations.
Surgery on children requires considerations which are not common in adult surgery. Children and adolescents are still developing physically and mentally making it difficult for them to make informed decisions and give consent for surgical treatments. Bariatric surgery in youth is among the controversial topics related to surgery in children.
A person with a debilitating medical condition may have special needs during a surgery which a typical patient would not.
Doctors perform surgery with the consent of the patient. Some patients are able to give better informed consent than others. Populations such as incarcerated persons, the mentally incompetent, persons subject to coercion, and other people who are not able to make decisions with the same authority as a typical patient have special needs when making decisions about their personal healthcare, including surgery.
In Low and Middle Income Countries.
In 2014 the Lancet Commission on Global Surgery was launched to examine the case for surgery as an integral component of global health care and to provide recommendations regarding the delivery of surgical and anesthesia services in low and middle income countries. The primary conclusions of the study were as follows:
History.
Surgical treatments date back to the prehistoric era. The oldest for which there is evidence is trepanation, in which a hole is drilled or scraped into the skull, thus exposing the dura mater in order to treat health problems related to intra cranial pressure and other diseases. Prehistoric surgical techniques are seen in Ancient Egypt, where a mandible dated to approximately 2650 BCE shows two perforations just below the root of the first molar, indicating the draining of an abscessed tooth. Remains from the early Harappan periods of the Indus Valley Civilization (c. 3300 BCE) show evidence of teeth having been drilled dating back 9,000 years. Instruments resembling surgical tools have also been found in the archaeological sites of Bronze Age China dating from the Shang Dynasty, along with seeds likely used for herbalism.
The oldest known surgical texts date back to ancient Egypt about 3500 years ago. Surgical operations were performed by priests, specialized in medical treatments similar to today. and the use of sutures to close wounds. Infections were treated with honey. In ancient Greece, temples dedicated to the healer-god Asclepius, known as "Asclepieia" (Greek: Ασκληπιεία, sing. "Asclepieion" "Ασκληπιείον"), functioned as centers of medical advice, prognosis, and healing. In the Asclepieion of Epidaurus, some of the surgical cures listed, such as the opening of an abdominal abscess or the removal of traumatic foreign material, are realistic enough to have taken place. The Greek Galen was one of the greatest surgeons of the ancient world and performed many audacious operations—including brain and eye surgery—that were not tried again for almost two millennia.
In the Middle East, surgery was developed to a high degree in the Islamic world. Abulcasis (Abu al-Qasim Khalaf ibn al-Abbas Al-Zahrawi), an Andalusian-Arab physician and scientist who practised in the Zahra suburb of Córdoba, wrote medical texts that influenced European surgical procedures.
In Europe, the demand grew for surgeons to formally study for many years before practicing; universities such as Montpellier, Padua and Bologna were particularly renowned. In the 15th century, Rogerius Salernitanus composed his "Chirurgia", laying the foundation for modern Western surgical manuals. Barber-surgeons generally had a bad reputation that was not to improve until the development of academic surgery as a specialty of medicine, rather than an accessory field. Basic surgical principles for asepsis etc., are known as Halsteads principles.
Early modern Europe.
There were some important advances to the art of surgery during this period. The professor of anatomy at the University of Padua, Andreas Vesalius, was a pivotal figure in the Renaissance transition from classical medicine and anatomy based on the works of Galen, to an empirical approach of 'hands-on' dissection. In his anatomic treatis, "De humani corporis fabrica", he exposed the many anatomical errors in Galen and advocated that all surgeons should train by engaging in practical dissections themselves.
The second figure of importance in this era was Ambroise Paré (sometimes spelled "Ambrose"), a French army surgeon from the 1530s until his death in 1590. The practice for cauterizing gunshot wounds on the battlefield had been to use boiling oil; an extremely dangerous and painful procedure. Paré began to employ a less irritating emollient, made of egg yolk, rose oil and turpentine. He also described more efficient techniques for the effective ligation of the blood vessels during an amputation.
Modern surgery.
The discipline of surgery was put on a sound, scientific footing during the Age of Enlightenment in Europe. An important figure in this regard was the English surgical scientist, John Hunter, generally regarded as the father of modern scientific surgery. He brought an empirical and experimental approach to the science and was renowned around Europe for the quality of his research and his written works. Hunter reconstructed surgical knowledge from scratch; refusing to rely on the testimonies of others he conducted his own surgical experiments to determine the truth of the matter. To aid comparative analysis, he built up a collection of over 13,000 specimens of separate organ systems, from the simplest plants and animals to humans.
He greatly advanced knowledge of venereal disease and introduced many new techniques of surgery, including new methods for repairing damage to the Achilles tendon and a more effective method for applying ligature of the arteries in case of an aneurysm. He was also one of the first to understand the importance of pathology, the danger of the spread of infection and how the problem of inflammation of the wound, bone lesions and even tuberculosis often undid any benefit that was gained from the intervention. He consequently adopted the position that all surgical procedures should be used only as a last resort.
Other important 18th and early 19th century surgeons included Percival Pott (1713 -1788) who described tuberculosis on the spine and first demonstrated that a cancer may be caused by an environmental carcinogen - (he noticed a connection between chimney sweep's exposure to soot and their high incidence of scrotal cancer. Astley Paston Cooper (1768-1841) first performed a successful ligation of the abdominal aorta, and James Syme (1799-1870) pioneered the Symes Amputation for the ankle joint and successfully carried out the first hip disarticulation.
Modern pain control through anesthesia was discovered in the mid-19th century. Before the advent of anesthesia, surgery was a traumatically painful procedure and surgeons were encouraged to be as swift as possible to minimize patient suffering. This also meant that operations were largely restricted to amputations and external growth removals. Beginning in the 1840s, surgery began to change dramatically in character with the discovery of effective and practical anaesthetic chemicals such as ether, first used by the American surgeon Crawford Long, and chloroform, discovered by James Young Simpson and later pioneered by John Snow, physician to Queen Victoria. In addition to relieving patient suffering, anaesthesia allowed more intricate operations in the internal regions of the human body. In addition, the discovery of muscle relaxants such as curare allowed for safer applications.
Infection and antisepsis.
Unfortunately, the introduction of anesthetics encouraged more surgery, which inadvertently caused more dangerous patient post-operative infections. The concept of infection was unknown until relatively modern times. The first progress in combating infection was made in 1847 by the Hungarian doctor Ignaz Semmelweis who noticed that medical students fresh from the dissecting room were causing excess maternal death compared to midwives. Semmelweis, despite ridicule and opposition, introduced compulsory handwashing for everyone entering the maternal wards and was rewarded with a plunge in maternal and fetal deaths, however the Royal Society dismissed his advice.
Until the pioneering work of British surgeon Joseph Lister in the 1860s, most medical men believed that chemical damage from exposures to bad air (see "miasma") was responsible for infections in wounds, and facilities for washing hands or a patient's wounds were not available. Lister became aware of the work of French chemist Louis Pasteur, who showed that rotting and fermentation could occur under anaerobic conditions if micro-organisms were present. Pasteur suggested three methods to eliminate the micro-organisms responsible for gangrene: filtration, exposure to heat, or exposure to chemical solutions. Lister confirmed Pasteur's conclusions with his own experiments and decided to use his findings to develop antiseptic techniques for wounds. As the first two methods suggested by Pasteur were inappropriate for the treatment of human tissue, Lister experimented with the third, spraying carbolic acid on his instruments. He found that this remarkably reduced the incidence of gangrene and he published his results in "The Lancet".
 Later, on 9 August 1867, he read a paper before the British Medical Association in Dublin, on the "Antiseptic Principle of the Practice of Surgery", which was reprinted in "The British Medical Journal". His work was groundbreaking and laid the foundations for a rapid advance in infection control that saw modern antiseptic operating theatres widely used within 50 years.
Lister continued to develop improved methods of antisepsis and asepsis when he realised that infection could be better avoided by preventing bacteria from getting into wounds in the first place. This led to the rise of sterile surgery. Lister introduced the Steam Steriliser to sterilize equipment, instituted rigorous hand washing and later implemented the wearing of rubber gloves. These three crucial advances - the adoption of a scientific methodology toward surgical operations, the use of anaesthetic and the introduction of sterilised equipment - laid the groundwork for the modern invasive surgical techniques of today.
The use of X-rays as an important medical diagnostic tool began with their discovery in 1895 by German physicist Wilhelm Röntgen. He noticed that these rays could penetrate the skin, allowing the skeletal structure to be captured on a specially treated photographic plate.
Surgical specialties.
Diagnoses and treatments that involve the use of invasive methods – such as catheterization and endoscopy – are also sometimes considered to be "surgery" or "surgical", although not by most of the medical profession.

</doc>
<doc id="45600" url="http://en.wikipedia.org/wiki?curid=45600" title="Reed–Solomon error correction">
Reed–Solomon error correction

Reed–Solomon codes are an important group of error-correcting codes that were introduced by Irving S. Reed and Gustave Solomon in 1960.
They have many important applications, the most prominent of which include consumer technologies such as CDs, DVDs, Blu-ray Discs, QR Codes, data transmission technologies such as DSL and WiMAX, broadcast systems such as DVB and ATSC, and storage systems such as RAID 6. They are also used in satellite communication.
In coding theory, the Reed–Solomon code belongs to the class of non-binary cyclic error-correcting codes. The Reed–Solomon code is based on univariate polynomials over finite fields.
It is able to detect and correct multiple symbol errors. By adding t check symbols to the data, a Reed–Solomon code can detect any combination of up to t erroneous symbols, or correct up to ⌊"t"/2⌋ symbols. As an erasure code, it can correct up to t known erasures, or it can detect and correct combinations of errors and erasures. 
Furthermore, Reed–Solomon codes are suitable as multiple-burst bit-error correcting codes, since a sequence of "b" + 1 consecutive bit errors can affect at most two symbols of size b. The choice of t is up to the designer of the code, and may be selected within wide limits.
History.
Reed–Solomon codes were developed in 1960 by Irving S. Reed and Gustave Solomon, who were then staff members of MIT Lincoln Laboratory. Their seminal article was entitled "Polynomial Codes over Certain Finite Fields." . When the article was written, an efficient decoding algorithm was not known. A solution for the latter was found in 1969 by Elwyn Berlekamp and James Massey, and is since known as the Berlekamp–Massey decoding algorithm. In 1977, Reed–Solomon codes were notably implemented in the Voyager program in the form of concatenated codes. The first commercial application in mass-produced consumer products appeared in 1982 with the compact disc, where two interleaved Reed–Solomon codes are used. Today, Reed–Solomon codes are widely implemented in digital storage devices and digital communication standards, though they are being slowly replaced by more modern low-density parity-check (LDPC) codes or turbo codes. For example, Reed–Solomon codes are used in the digital video broadcasting (DVB) standard DVB-S, but LDPC codes are used in its successor DVB-S2.
Basis.
The Reed–Solomon code is based on univariate polynomials over finite fields; in particular, for some parameters k and n, the codewords of the Reed–Solomon code consists of all function tables of polynomials of degree less than k over the finite field with n elements - for this to work, n has to be prime power.
The encoding scheme of the Reed–Solomon code turns k symbols into such a function table, which is essentially a list of n symbols.
One way to perform this encoding is by interpreting the k given symbols as the first segment of the function table of a polynomial of degree less than k.
A simple argument shows that there is exactly one such polynomial, and the remaining "n" − "k" symbols can thus be generated by evaluating the polynomial at those points.
Since the n transmitted symbols form an overdetermined system that specifies a polynomial of degree less than k, we can use interpolation techniques at the receiver to recover the original message in case not too many errors happened.
In order to achieve the most efficient decoding procedures, the encoding procedure of the Reed–Solomon code is often constructed a bit differently, namely as cyclic BCH codes: the k source symbols are interpreted as the "coefficients" of a polynomial of degree less than k, and the additional "n" − "k" symbols are derived from the coefficients of a polynomial constructed by multiplying "p"("x") with a cyclic generator polynomial.
Applications.
Data storage.
Reed–Solomon coding is very widely used in mass storage systems to correct
the burst errors associated with media defects.
Reed–Solomon coding is a key component of the compact disc. It was the first use of strong error correction coding in a mass-produced consumer product, and DAT and DVD use similar schemes. In the CD, two layers of Reed–Solomon coding separated by a 28-way convolutional interleaver yields a scheme called Cross-Interleaved Reed–Solomon Coding (CIRC). The first element of a CIRC decoder is a relatively weak inner (32,28) Reed–Solomon code, shortened from a (255,251) code with 8-bit symbols. This code can correct up to 2 byte errors per 32-byte block. More importantly, it flags as erasures any uncorrectable blocks, i.e., blocks with more than 2 byte errors. The decoded 28-byte blocks, with erasure indications, are then spread by the deinterleaver to different blocks of the (28,24) outer code. Thanks to the deinterleaving, an erased 28-byte block from the inner code becomes a single erased byte in each of 28 outer code blocks. The outer code easily corrects this, since it can handle up to 4 such erasures per block.
The result is a CIRC that can completely correct error bursts up to 4000 bits, or about 2.5 mm on the disc surface. This code is so strong that most CD playback errors are almost certainly caused by tracking errors that cause the laser to jump track, not by uncorrectable error bursts.
DVDs use a similar scheme, but with much larger blocks, a (208,192) inner code, and a (182,172) outer code.
Reed–Solomon error correction is also used in parchive files which are commonly posted accompanying multimedia files on USENET. The Distributed online storage service Wuala also makes use of Reed–Solomon when breaking up files.
Bar code.
Almost all two-dimensional bar codes such as PDF-417, MaxiCode, Datamatrix, QR Code, and Aztec Code use Reed–Solomon error correction to allow correct reading even if a portion of the bar code is damaged. When the bar code scanner cannot recognize a bar code symbol, it will treat it as an erasure.
Reed–Solomon coding is less common in one-dimensional bar codes, but is used by the PostBar symbology.
Data transmission.
Specialized forms of Reed–Solomon codes, specifically Cauchy-RS and Vandermonde-RS, can be used to overcome the unreliable nature of data transmission over erasure channels. The encoding process assumes a code of RS("N", "K") which results in "N" codewords of length "N" symbols each storing "K" symbols of data, being generated, that are then sent over an erasure channel.
Any combination of "K" codewords received at the other end is enough to reconstruct all of the "N" codewords. The code rate is generally set to 1/2 unless the channel's erasure likelihood can be adequately modelled and is seen to be less. In conclusion, "N" is usually 2"K", meaning that at least half of all the codewords sent must be received in order to reconstruct all of the codewords sent.
Reed–Solomon codes are also used in xDSL systems and CCSDS's Space Communications Protocol Specifications as a form of forward error correction.
Space transmission.
One significant application of Reed–Solomon coding was to encode the digital pictures sent back by the Voyager space probe.
Voyager introduced Reed–Solomon coding concatenated with convolutional codes, a practice that has since become very widespread in deep space and satellite (e.g., direct digital broadcasting) communications.
Viterbi decoders tend to produce errors in short bursts. Correcting these burst errors is a job best done by short or simplified Reed–Solomon codes.
Modern versions of concatenated Reed–Solomon/Viterbi-decoded convolutional coding were and are used on the Mars Pathfinder, Galileo, Mars Exploration Rover and Cassini missions, where they perform within about 1–1.5 dB of the ultimate limit imposed by the Shannon capacity.
These concatenated codes are now being replaced by more powerful turbo codes.
Constructions.
The Reed–Solomon code is actually a family of codes:
For every choice of the three parameters "k" < "n" ≤ "q", there is a Reed–Solomon code that has an alphabet of size "q", a block length "n", and a message length "k".
Moreover, the alphabet is interpreted as the finite field of order "q", and thus, "q" has to be a prime power.
In the most useful parameterizations of the Reed–Solomon code, the block length is usually some constant multiple of the message length, that is, the rate "R" = "k"/"n" is some constant, and furthermore, the block length is equal to or one less than the alphabet size, that is, "n" = "q" or "n" = "q"−1.
Reed & Solomon's original view: The codeword as a sequence of values.
There are different encoding procedures for the Reed–Solomon code, and thus, there are different ways to describe the set of all codewords.
In the original view of , every codeword of the Reed–Solomon code is a sequence of function values of a low-degree polynomial.
More precisely, in order to obtain a codeword of the Reed–Solomon code, the message is interpreted as the description of a polynomial "p" of degree less than "k" over the finite field "F" with "q" elements.
In turn, the polynomial "p" is evaluated at "n" distinct points formula_1 of the field "F", and the sequence of values is the corresponding codeword.
Formally, the set formula_2 of codewords of the Reed–Solomon code is defined as follows:
Since any two "different" polynomials of degree less than formula_4 agree in at most formula_5 points, this means that any two codewords of the Reed–Solomon code disagree in at least formula_6 positions.
Furthermore, there are two polynomials that do agree in formula_5 points but are not equal, and thus, the distance of the Reed–Solomon code is exactly formula_8.
Then the relative distance is formula_9, where formula_10 is the rate.
This trade-off between the relative distance and the rate is asymptotically optimal since, by the Singleton bound, "every" code satisfies formula_11.
Being a code that achieves this optimal trade-off, the Reed–Solomon code belongs to the class of maximum distance separable codes.
While the number of different polynomials of degree less than "k" and the number of different messages are both equal to formula_12, and thus every message can be uniquely mapped to such a polynomial, there are different ways of doing this encoding.
The original construction of interprets the message "x" as the "coefficients" of the polynomial "p", whereas subsequent constructions interpret the message as the "values" of the polynomial at the first "k" points formula_13 and obtain the polynomial "p" by interpolating these values with a polynomial of degree less than "k".
The latter encoding procedure, while being slightly less efficient, has the advantage that it gives rise to a systematic code, that is, the original message is always contained as a subsequence of the codeword.
In many contexts it is convenient to choose the sequence formula_1 of evaluation points so that they exhibit some additional structure.
In particular, it is useful to choose the sequence of successive powers of a primitive root formula_15 of the field formula_16, that is, formula_15 is generator of the finite field's multiplicative group and the sequence is defined as formula_18 for all formula_19.
This sequence contains all elements of formula_16 except for formula_21, so in this setting, the block length is formula_22.
Then it follows that, whenever formula_23 is a polynomial over formula_16, then the function formula_25 is also a polynomial of the same degree, which gives rise to a codeword that is a cyclic left-shift of the codeword derived from formula_23; thus, this construction of Reed–Solomon codes gives rise to a cyclic code.
Simple encoding procedure: The message as a sequence of coefficients.
In the original construction of , the message formula_27 is mapped to the polynomial formula_28 with
As described above, the codeword is then obtained by evaluating formula_30 at formula_31 different points formula_1 of the field formula_16.
Thus the classical encoding function formula_34 for the Reed–Solomon code is defined as follows: 
This function formula_36 is a linear mapping, that is, it satisfies formula_37 for the following formula_38-matrix formula_39 with elements from formula_16:
This matrix is the transpose of a Vandermonde matrix over formula_16.
In other words, the Reed–Solomon code is a linear code, and in the classical encoding procedure, its generator matrix is formula_39.
Systematic encoding procedure: The message as an initial sequence of values.
As mentioned above, there is an alternative way to map codewords formula_44 to polynomials formula_28.
In this alternative encoding procedure, the polynomial formula_28 is the unique polynomial of degree less than formula_4 such that
To compute this polynomial formula_28 from formula_44, one can use Lagrange interpolation.
Once it has been found, it is evaluated at the other points formula_52 of the field.
The alternative encoding function formula_34 for the Reed–Solomon code is then again just the sequence of values: 
This time, however, the first formula_4 entries of the codeword are exactly equal to formula_44, so this encoding procedure gives rise to a systematic code.
It can be checked that the alternative encoding function is a linear mapping as well.
The BCH view: The codeword as a sequence of coefficients.
In this view, the sender again maps the message formula_44 to a polynomial formula_28, and for this, any of the two mappings above can be used (where the message is either interpreted as the coefficients of formula_28 or as the initial sequence of values of formula_28).
Once the sender has constructed the polynomial formula_28 in some way, however, instead of sending the "values" of formula_28 at all points, the sender computes some related polynomial formula_63 of degree at most formula_64 for formula_22 and sends the formula_31 "coefficients" of that polynomial.
The polynomial formula_67 is constructed by multiplying the message polynomial formula_68, which has degree at most formula_5, with a generator polynomial formula_70 of degree formula_71 that is known to both the sender and the receiver.
The generator polynomial formula_72 is defined as the polynomial whose roots are exactly formula_73, i.e.,
The transmitter sends the formula_22 coefficients of formula_76.
Thus, in the BCH view of Reed Solomon codes, the set formula_77 of codewords is defined for formula_22 as follows:
With this definition of the codewords, it can be immediately seen that a Reed–Solomon code is a polynomial code, and in particular a BCH code. The generator polynomial formula_70 is the minimal polynomial with roots formula_81 as defined above, and the codewords are exactly the polynomials that are divisible by formula_70.
Since Reed–Solomon codes are a special case of BCH codes and the Berlekamp–Massey algorithm has been designed for the decoding of such codes, it is applicable to Reed–Solomon codes:
The receiver interprets the received word as the coefficients of a polynomial formula_83.
If no error has occurred during the transmission, that is, if formula_84, then the receiver can use polynomial division to determine the message polynomial formula_85.
In general, the receiver can use polynomial division to construct the unique polynomials formula_23 and formula_87, such that formula_87 has degree less than the degree of formula_70 and
If the remainder polynomial formula_87 is not identically zero, then an error has occurred during the transmission.
The receiver can evaluate formula_83 at the roots of formula_70 and build a system of equations that eliminates formula_67 and identifies which coefficients of formula_83 are in error, and the magnitude of each coefficient's error ( and ).
If the system of equations can be solved, then the receiver knows how to modify the received word formula_83 to get the most likely codeword formula_67 that was sent.
Systematic encoding procedure.
The above encoding procedure for the BCH view of Reed–Solomon codes is classical, but does not give rise to a systematic encoding procedure, i.e., the codewords do not necessarily contain the message as a subsequence.
To remedy this fact, instead of sending formula_98, the encoder constructs the transmitted polynomial formula_99 such that the coefficients of the formula_4 largest monomials are equal to the corresponding coefficients of formula_101, and the lower-order coefficients of formula_99 are chosen exactly in such a way that formula_99 becomes evenly divisible by formula_72.
Then the coefficients of formula_101 are a subsequence of the coefficients of formula_99.
To get a code that is overall systematic, we construct the message polynomial formula_101 by interpreting the message as the sequence of its coefficients.
Formally, the construction is done by multiplying formula_101 by formula_109 to make room for the formula_110 check symbols, dividing that product by formula_72 to find the remainder, and then compensating for that remainder by subtracting it.
The formula_112 check symbols are created by computing the remainder formula_113:
Note that the remainder has degree at most formula_115, whereas the coefficients of formula_116 in the polynomial formula_117 are zero. Therefore, the following definition of the codeword formula_99 has the property that the first formula_4 coefficients are identical to the coefficients of formula_101:
As a result, the codewords formula_99 are indeed elements of formula_77, that is, they are evenly divisible by the generator polynomial formula_72:
Equivalence of the two views.
At first sight, the two views of Reed–Solomon codes above seem very different. In the first definition, codewords in the set formula_2 are "values" of polynomials, whereas in the second set formula_77, they are "coefficients". Moreover, the polynomials in the first definition are required to be of small degree, whereas those in the second definition are required to have specific roots.
Yet, it can be shown that the two sets are actually equal, that is, formula_128 holds (for an appropriate choice of formula_1).
The equivalence of the two definitions is proved using the discrete Fourier transform. This transform, which exists in all finite fields as well as the complex numbers, establishes a duality between the coefficients of polynomials and their values. This duality can be approximately summarized as follows: Let formula_101 and formula_131 be two polynomials of degree less than formula_31. If the "values" of formula_101 are the "coefficients" of formula_131, then (up to a scalar factor and reordering), the "values" of formula_131 are the "coefficients" of formula_101. For this to make sense, the values must be taken at locations formula_137, for formula_138, where formula_15 is a primitive formula_31th root of unity.
To be more precise, let
and assume formula_101 and formula_131 are related by the discrete Fourier transform. Then the coefficients and values of formula_101 and formula_131 are related as follows: for all formula_138, formula_148 and formula_149.
Using these facts, we have: formula_150 is a code word of the Reed–Solomon code according to the first definition
This shows that the two definitions are equivalent.
Remarks.
Designers are not required to use the "natural" sizes of Reed–Solomon code blocks. A technique known as "shortening" can produce a smaller code of any desired size from a larger code. For example, the widely used (255,223) code can be converted to a (160,128) code by padding the unused portion of the source block with 95 binary zeroes and not transmitting them. At the decoder, the same portion of the block is loaded locally with binary zeroes. The Delsarte-Goethals-Seidel theorem illustrates an example of an application of shortened Reed–Solomon codes. In parallel to shortening, a technique known as puncturing allows omitting some of the encoded parity symbols.
Properties.
The Reed–Solomon code is a ["n", "k", "n" − "k" + 1] code; in other words, it is a linear block code of length "n" (over "F") with dimension "k" and minimum Hamming distance "n" − "k" + 1. The Reed–Solomon code is optimal in the sense that the minimum distance has the maximum value possible for a linear code of size ("n", "k"); this is known as the Singleton bound. Such a code is also called a maximum distance separable (MDS) code.
The error-correcting ability of a Reed–Solomon code is determined by its minimum distance, or equivalently, by formula_161, the measure of redundancy in the block. If the locations of the error symbols are not known in advance, then a Reed–Solomon code can correct up to formula_162 erroneous symbols, i.e., it can correct half as many errors as there are redundant symbols added to the block. Sometimes error locations are known in advance (e.g., "side information" in demodulator signal-to-noise ratios)—these are called erasures. A Reed–Solomon code (like any MDS code) is able to correct twice as many erasures as errors, and any combination of errors and erasures can be corrected as long as the relation 2"E" + "S" ≤ "n" − "k" is satisfied, where formula_163 is the number of errors and formula_164 is the number of erasures in the block.
For practical uses of Reed–Solomon codes, it is common to use a finite field formula_16 with formula_166 elements. In this case, each symbol can be represented as an formula_167-bit value. 
The sender sends the data points as encoded blocks, and the number of symbols in the encoded block is formula_168. Thus a Reed–Solomon code operating on 8-bit symbols has formula_169 symbols per block. (This is a very popular value because of the prevalence of byte-oriented computer systems.) The number formula_4, with formula_171, of "data" symbols in the block is a design parameter. A commonly used code encodes formula_172 eight-bit data symbols plus 32 eight-bit parity symbols in an formula_173-symbol block; this is denoted as a formula_174 code, and is capable of correcting up to 16 symbol errors per block.
The Reed–Solomon code properties discussed above make them especially well-suited to applications where errors occur in bursts. This is because it does not matter to the code how many bits in a symbol are in error — if multiple bits in a symbol are corrupted it only counts as a single error. Conversely, if a data stream is not characterized by error bursts or drop-outs but by random single bit errors, a Reed–Solomon code is usually a poor choice compared to a binary code.
The Reed–Solomon code, like the convolutional code, is a transparent code. This means that if the channel symbols have been inverted somewhere along the line, the decoders will still operate. The result will be the inversion of the original data. However, the Reed–Solomon code loses its transparency when the code is shortened. The "missing" bits in a shortened code need to be filled by either zeros or ones, depending on whether the data is complemented or not. (To put it another way, if the symbols are inverted, then the zero-fill needs to be inverted to a one-fill.) For this reason it is mandatory that the sense of the data (i.e., true or complemented) be resolved before Reed–Solomon decoding.
Error correction algorithms.
Theoretical decoder.
 described a theoretical decoder that corrected errors by finding the most popular message polynomial. The decoder for a RS formula_175 code would look at all possible subsets of formula_4 symbols from the set of formula_31 symbols that were received. For the code to be correctable in general, at least formula_4 symbols had to be received correctly, and formula_4 symbols are needed to interpolate the message polynomial. The decoder would interpolate a message polynomial for each subset, and it would keep track of the resulting polynomial candidates. The most popular message is the corrected result. Unfortunately, there are a lot of subsets, so the algorithm is impractical. The number of subsets is the binomial coefficient, formula_180, and the number of subsets is infeasible for even modest codes. For a formula_181 code that can correct 3 errors, the naive theoretical decoder would examine 359 billion subsets. The Reed–Solomon code needed a practical decoder.
Peterson decoder.
 developed a practical decoder based on syndrome decoding. Berlekamp (below) would improve on that decoder.
Syndrome decoding.
The transmitted message is viewed as the coefficients of a polynomial "s"("x") that is divisible by a generator polynomial "g"("x"). 
where "α" is a primitive root.
Since "s"("x") is divisible by generator "g"("x"), it follows that
The transmitted polynomial is corrupted in transit by an error polynomial "e"("x") to produce the received polynomial "r"("x").
where "ei" is the coefficient for the "i"-th power of "x". Coefficient "ei" will be zero if there is no error at that power of "x" and nonzero if there is an error. If there are "ν" errors at distinct powers "ik" of "x", then
The goal of the decoder is to find the number of errors ("ν"), the positions of the errors ("ik"), and the error values at those positions ("eik"). From those, e(x) can be calculated and subtracted from r(x) to get the original message s(x).
The syndromes "S""j" are defined as
The advantage of looking at the syndromes is that the message polynomial drops out. Another possible way of calculating e(x) is using polynomial interpolation to find the only polynomial that passes through the points formula_189 (Because formula_190), however, this is not used widely because polynomial interpolation is not always feasible in the fields used in Reed–Solomon error correction. For example, it is feasible over the integers (of course), but it is infeasible over the integers modulo a prime.
Error locators and error values.
For convenience, define the error locators "Xk" and error values "Yk" as:
Then the syndromes can be written in terms of the error locators and error values as
The syndromes give a system of "n" − "k" ≥ 2"ν" equations in 2"ν" unknowns, but that system of equations is nonlinear in the "Xk" and does not have an obvious solution. However, if the "Xk" were known (see below), then the syndrome equations provide a linear system of equations that can easily be solved for the "Yk" error values.
Consequently, the problem is finding the "Xk", because then the leftmost matrix would be known, and both sides of the equation could be multiplied by its inverse, yielding Y"k"
Error locator polynomial.
Peterson found a linear recurrence relation that gave rise to a system of linear equations. 
Solving those equations identifies the error locations.
Define the error locator polynomial Λ("x") as
The zeros of Λ("x") are the reciprocals formula_195:
Multiply both sides by formula_198 and it will still be zero. j is any number such that 1≤j≤v.
Sum for "k" = 1 to "ν"
This reduces to
This yields a system of linear equations that can be solved for the coefficients Λ"i" of the error location polynomial:
The above assumes the decoder knows the number of errors "ν", but that number has not been determined yet. The PGZ decoder does not determine "ν" directly but rather searches for it by trying successive values. The decoder first assumes the largest value for a trial "ν" and sets up the linear system for that value. If the equations can be solved (i.e., the matrix determinant is nonzero), then that trial value is the number of errors. If the linear system cannot be solved, then the trial "ν" is reduced by one and the next smaller system is examined. 
Obtain the error locators from the error locator polynomial.
Use the coefficients Λ"i" found in the last step to build the error location polynomial. The roots of the error location polynomial can be found by exhaustive search. The error locators are the reciprocals of those roots. Chien search is an efficient implementation of this step.
Calculate the error locations.
Calculate ik by taking the log base a of "Xk". This is generally done using a precomputed lookup table.
Calculate the error values.
Once the error locators are known, the error values can be determined. This can be done by direct solution for "Yk" in the error equations given above, or using the Forney algorithm.
Fix the errors.
Finally, e(x) is generated from ik and eik and then is subtracted from r(x) to get the sent message s(x).
Berlekamp–Massey decoder.
The Berlekamp–Massey algorithm is an alternate iterative procedure for finding the error locator polynomial. During each iteration, it calculates a discrepancy based on a current instance of Λ(x) with an assumed number of errors "e":
and then adjusts Λ(x) and "e" so that a recalculated Δ would be zero. The article Berlekamp–Massey algorithm has a detailed description of the procedure. In the following example, C(x) is used to represent Λ(x).
Example.
Consider the Reed–Solomon code defined in "GF"(929) with and (this is used in PDF417 barcodes). The generator polynomial is
If the message polynomial is , then the codeword is calculated as follows.
Errors in transmission might cause this to be received instead.
The syndromes are calculated by evaluating "r" at powers of "α".
To correct the errors, first use the Berlekamp–Massey algorithm to calculate the error locator polynomial.
The final value of "C" is the error locator polynomial, Λ("x"). The zeros can be found by trial substitution. They are "x"1 = 757 = 3−3 and "x"2 = 562 = 3−4, corresponding to the error locations. To calculate the error values, apply the Forney algorithm.
Subtracting "e"1"x"3 and "e"2"x"4 from the received polynomial "r" reproduces the original codeword "s".
Euclidean decoder.
Another iterative method for calculating the error locator polynomial is based on the Euclidean algorithm
"Ai"(0) is the constant (least significant) term of "Ai".
Here is an example of the Euclidean method, using the same data as the Berlekamp Massey example above. In the table below, R and S are forward, A and B are reversed.
Decoding in frequency domain (sketch).
The above algorithms are presented in the time domain. Decoding in the frequency domain, using Fourier transform techniques, can offer computational and implementation advantages. 
The following is a sketch of the main idea behind this error correction technique.
By definition, a code word of a Reed–Solomon code is given by the sequence of values of a low-degree polynomial over a finite field. A key fact for the error correction algorithm is that the "values" and the "coefficients" of a polynomial are related by the discrete Fourier transform.
The purpose of a Fourier transform is to convert a signal from a time domain to a frequency domain or vice versa. 
In case of the Fourier transform over a finite field, the frequency domain signal corresponds to the coefficients of a polynomial, and the time domain signal correspond to the values of the same polynomial.
As shown in Figures 1 and 2, an isolated value in the frequency domain corresponds to a smooth wave in the time domain. The wavelength depends on the location of the isolated value.
Conversely, as shown in Figures 3 and 4, an isolated value in the time domain corresponds to a smooth wave in the frequency domain.
In a Reed–Solomon code, the frequency domain is divided into two regions as shown in Figure 5: a left (low-frequency) region of length formula_4, and a right (high-frequency) region of length formula_71. A data word is then embedded into the left region (corresponding to the formula_4 coefficients of a polynomial of degree at most formula_5), while the right region is filled with zeros. The result is Fourier transformed into the time domain, yielding a code word that is composed only of low frequencies. In the absence of errors, a code word can be decoded by reverse Fourier transforming it back into the frequency domain.
Now consider a code word containing a single error, as shown in red in Figure 6. The effect of this error in the frequency domain is a smooth, single-frequency wave in the right region, called the "syndrome" of the error. The error location can be determined by determining the frequency of the syndrome signal.
Similarly, if two or more errors are introduced in the code word, the syndrome will be a signal composed of two or more frequencies, as shown in Figure 7. As long as it is possible to determine the frequencies of which the syndrome is composed, it is possible to determine the error locations. Notice that the error "locations" depend only on the "frequencies" of these waves, whereas the error "magnitudes" depend on their amplitudes and phase.
The problem of determining the error locations has therefore been reduced to the problem of finding, given a sequence of formula_71 values, the smallest set of elementary waves into which these values can be decomposed. It is known from digital signal processing that this problem is equivalent to finding the roots of the minimal polynomial of the sequence, or equivalently, of finding the shortest linear feedback shift register (LFSR) for the sequence. The latter problem can either be solved inefficiently by solving a system of linear equations, or more efficiently by the Berlekamp–Massey algorithm.
Decoding beyond the error-correction bound.
The Singleton bound states that the minimum distance "d" of a linear block code of size ("n","k") is upper-bounded by "n" − "k" + 1. The distance "d" was usually understood to limit the error-correction capability to ⌊"d"/2⌋. The Reed–Solomon code achieves this bound with equality, and can thus correct up to ⌊("n" − "k" + 1)/2⌋ errors. However, this error-correction bound is not exact.
In 1999, Madhu Sudan and Venkatesan Guruswami at MIT published "Improved Decoding of Reed–Solomon and Algebraic-Geometry Codes" introducing an algorithm that allowed for the correction of errors beyond half the minimum distance of the code. It applies to Reed–Solomon codes and more generally to algebraic geometric codes. This algorithm produces a list of codewords (it is a list-decoding algorithm) and is based on interpolation and factorization of polynomials over formula_220 and its extensions.
Soft-decoding.
The algebraic decoding methods described above are hard-decision methods, which means that for every symbol a hard decision is made about its value. For example, a decoder could associate with each symbol an additional value corresponding to the channel demodulator's confidence in the correctness of the symbol. The advent of LDPC and turbo codes, which employ iterated soft-decision belief propagation decoding methods to achieve error-correction performance close to the theoretical limit, has spurred interest in applying soft-decision decoding to conventional algebraic codes. In 2003, Ralf Koetter and Alexander Vardy presented a polynomial-time soft-decision algebraic list-decoding algorithm for Reed–Solomon codes, which was based upon the work by Sudan and Guruswami.

</doc>
<doc id="45606" url="http://en.wikipedia.org/wiki?curid=45606" title="Zorba the Greek (film)">
Zorba the Greek (film)

 
Zorba the Greek (Greek title: "Αλέξης Ζορμπάς", "Alexis Zorba(s)") is a 1964 British-Greek drama film directed by Cypriot Michael Cacoyannis and starring Anthony Quinn as the title character. It is based on the novel "Zorba the Greek" by Nikos Kazantzakis. The supporting cast includes Alan Bates, Lila Kedrova, Irene Papas and Sotiris Moustakas.
Plot.
Basil (Alan Bates) is a half-English, half-Greek writer raised in Britain who bears the hallmarks of an uptight, middle-class Englishman. He is waiting at the Athens port of Piraeus on mainland Greece to catch a boat to Crete when he meets a gruff, yet enthusiastic peasant and musician named Zorba (Anthony Quinn). Basil explains to Zorba that he is traveling to a rural Cretan village where his father owns some land, with the intention of reopening a lignite mine and perhaps curing his writer's block. Zorba relates his experience with mining and convinces Basil to take him along.
When they arrive at Crete, they take a car to the village where they are greeted enthusiastically by the town's impoverished peasant community. They stay with an old French war widow and cortesan named Madame Hortense (Lila Kedrova) in her self-styled "Hotel Ritz". The audacious Zorba tries to persuade Basil into making a move on the much older Madame Hortense, but when he is understandably reluctant, Zorba seizes the opportunity, and they form a relationship.
Over the next few days, Basil and Zorba attempt to work the old lignite mine, but find it unsafe and shut it down. Zorba then has an idea to use the forest in the nearby mountains for logging (although his specific plan is left ambiguous), however the land is owned by a powerful monastery, so Zorba visits and befriends the monks, getting them drunk. Afterwards, he comes home to Basil and begins to dance in a way that mesmerizes Basil.
Meanwhile, Basil and Zorba get their first introduction to "the Widow" (Irene Papas), a young and attractive widowed woman, who is incessantly teased by the townspeople for not remarrying, especially to a young, local boy who is madly in love with her, but whom she has spurned repeatedly. One rainy afternoon, Basil offers her his umbrella, which she reluctantly takes. Zorba suggests that she is attracted to him, but Basil, ever shy, denies this and refuses to pursue the widow.
Basil hands Zorba some money, and sends him off to the large town of Chania, where Zorba is to buy cable and other supplies for the implementation of his grand plan. Zorba says goodbye to Basil and Madame Hortense, who is by now madly in love with him. In Chania, Zorba entertains himself at a cabaret and strikes up a brief romance with a much younger dancer. In a letter to Basil, he details his exploits and indicates that he has found love. Angered by Zorba's apparent irresponsibility and the squandering of his money, Basil untruthfully tells Madame Hortense that Zorba has declared his love to her and intends to marry her upon his return – to which she is ecstatic to the point of tears. Meanwhile, the Widow returns Basil's umbrella by way of Mimithos (Sotiris Moustakas), the village idiot.
When Zorba eventually returns with supplies and gifts, he is surprised and angered to hear of Basil's lie to Madame Hortense. He also asks Basil about his whereabouts the night before. That night, Basil had gone to the Widow's house, made love to her and spent the night. The brief encounter comes at great cost. A villager catches sight of them, and word spreads, and the young, local boy who is in love with the Widow is taunted mercilessly about it. The next morning, the villagers find his body by the sea, where he has drowned himself out of shame.
The boy's father holds a funeral which the villagers attend. The widow attempts to come inconspicuously, but is blocked from entering the church. She is eventually trapped in the courtyard, then beaten and stoned by the villagers, who hold her responsible for the boy's suicide. Basil, meek and fearful of intervening, tells Mimithos to quickly fetch Zorba. Zorba arrives just as a villager, a friend of the boy, tries to pull a knife and kill the widow. Zorba overpowers the much younger man and disarms him. Thinking that the situation is under control, Zorba asks the Widow to follow him and turns his back. At that moment, the dead boy's father pulls his knife and cuts the widow's throat. She dies at once, as the villagers shuffle away apathetically, whisking the father away. Only Basil, Zorba and Mimithos show any emotion over her murder. Basil proclaims his inability to intervene whereupon Zorba laments the futility of death.
On a rainy day, Basil and Zorba come home and find Madame Hortense waiting. She expresses anger at Zorba for making no progress on the wedding. Zorba conjures up a story that he had ordered a white satin wedding dress, lined with pearls and adorned with real gold. Madame Hortense presents two golden rings she had made and proposes their immediate engagement. Zorba tries to stall, but eventually agrees with gusto, to Basil's surprise.
Some time later, Madame Hortense has contracted pneumonia, and is seen on her deathbed. Zorba stays by her side, along with Basil. Meanwhile, word has spread that "the foreigner" is dying, and since she has no heirs, the State will take her possessions and money. The desperately poor villagers crowd around her hotel, impatiently waiting for her demise so they can steal her belongings. As two old ladies enter her room and gaze expectantly at her, other women try to enter, but Zorba manages to fight them off. At the instant of her death, the women re-enter Madame Hortense's bedroom "en masse" to steal her valued possessions. Zorba leaves with a sigh, as the hotel is ransacked and stripped bare by the shrieking and excited villagers. When Zorba returns to Madame Hortense's bedroom, the room is barren apart from her bed (where she lies) and the bird in her cage. Zorba takes the birdcage with him.
Finally, Zorba's elaborate contraption to transport timber down the hill is complete. A festive ceremony, including lamb on a spit is held, and all the villagers turned out. After a blessing from the priests, Zorba signals the start by firing a rifle in the air. A log comes hurtling down the zip line at a worrying pace, destroying the log itself and slightly damaging part of the contraption. Zorba remains unconcerned and gives orders for a second log. This one also speeds down and shoots straight into the sea. By now the villagers and priests have grown fearful and head for cover. Zorba remains unfazed and orders a third log, which accelerates downhill with such violence that it dislodges the entire contraption, destroying everything. The villagers flee, leaving Basil and Zorba behind.
Basil and Zorba sit by the shore to eat roasted lamb for lunch. Zorba pretends to tell the future from the lamb shank, saying that he foresees a great journey to a big city. He then asks Basil directly when he plans to leave, and Basil replies that he will leave in a few days. Zorba declares his sadness about Basil's imminent departure to England and tells Basil that he is missing madness. Basil asks Zorba to teach him to dance. Zorba teaches him the sirtaki and Basil begins to laugh hysterically at the catastrophic outcome. The story ends with both men enthusiastically dancing the sirtaki on the beach.
Production.
Simone Signoret began filming the role of Madame Hortense; Lila Kedrova replaced her early in the production.
The film was shot on location on the Greek island of Crete. Specific locations featured include the town of Chania, the Apokoronas region and the Akrotiri peninsula. The famed scene in which Quinn's character dances the Sirtaki was filmed on the beach of the village of Stavros.
Reception.
The film was a smash-hit. Produced on a budget of only $783,000, it grossed $9 million at the U.S. box office, earning $4.4 million in U.S. theatrical rentals. At the worldwide box office, the film earned $9.4 million in rentals, placing the worldwide gross between $18.8 million to $23.5 million. It was the 19th highest grossing film of 1964.
The film won three Academy Awards.
The film has an 83% rating at Rotten Tomatoes. On both sides of the Atlantic, the film was applauded and Anthony Quinn came in for the 
best reviews. He was loved as Zorba, along with the other stars, including Greek born Irene Papas, who worked with Quinn on "The Guns of Navarone".
Cultural influence.
The dance at the end of the film, choreographed by Giorgos Provias, formerly known as "Zorba's dance" and later called Sirtaki, has become a popular cliché of Greek dance.
"Zorba the Greek" was adapted into a 1968 Broadway musical named "Zorba". The play starred Herschel Bernardi: then, the show was revived in 1983, with Anthony Quinn and Lila Kedrova reprising their film roles. It opened to big box office receipts and good reviews, plus 362 performances, more than the original stage production.
The film's music by Mikis Theodorakis, especially the main song, "Zorbas", is well known in popular culture. For example, the song has been used at Yankee Stadium for years to incite crowd participation during a potential rally by the home team. A remake of "Zorbas" by John Murphy and David Hughes was used during the climax shootout-scene in the 1998 Guy Richie film, "Lock Stock and Two Smoking Barrels".
A short film made in Scotland in 1999, "Billy and Zorba", is about a man who believes he is Zorba the Greek.
The film has been referenced in two of actress Nia Vardalos' films. In "My Big Fat Greek Wedding", the family-owned restaurant her character works at is called "Dancing Zorba's"; this is also seen in the short-lived 2003 show "My Big Fat Greek Life". In "My Life In Ruins", Vardalos' character Georgia expresses contempt for the film because of the Greek's love of dancing and Anthony Quinn.
A web browser flash game akin to Dance Dance Revolution was created by Pippin Barr, wherein the player competes with Zorba by taking turns performing Zorba's dance, trying to out-dance each other.

</doc>
<doc id="45607" url="http://en.wikipedia.org/wiki?curid=45607" title="Biosecurity protocol">
Biosecurity protocol

Biosecurity protocol refers to several politically controversial attempts to unify global biosecurity measures and responses, in a similar manner to a biosafety protocol. Although some propose a "Biosecurity Protocol" to extend the Biosafety Protocol to organisms considered weapons (already controlled by UN arms proliferation treaties), others argue this is an inappropriate response to military threats, and argue for a broad biodefense instead. 
As of 2002, the latter view was prevalent in military and scientific circles, the former in NGOs, some United Nations agencies, and Green Parties. Most cooperation was restricted to attempts to define "biosecurity" itself. 
A major issue is whether to accept the Precautionary Principle, or by extension, any restrictions on militarily useful research by major powers. Compromises, e.g. the 2002 US government request to scientists to delete procedural details of experiments on dangerous organisms, are strongly resisted by most American scientists. Extreme positions, e.g. Bill Joy's argument for relenquishment of nanotechnology and artificial intelligence, are considered a legitimate part of the debate, but have yet to achieve any serious political support, even among Green Parties.
Whether these negotiations and compromises will yield a comprehensive biosecurity protocol is doubtful. It seems that basic political differences drive the debate which are unlikely to easily resolve, and that military alliances will influence tax, trade, and tariff systems and import/export restrictions more than any diplomatic or scientific protocol.

</doc>
<doc id="45609" url="http://en.wikipedia.org/wiki?curid=45609" title="Cheetah">
Cheetah

The cheetah ("Acinonyx jubatus") is a large feline (family Felidae, subfamily Felinae) inhabiting most of Africa and parts of Iran. It is the only extant member of the genus "Acinonyx". The cheetah can run faster than any other land animal— as fast as 112 to 120 km/h (70 to 75 mph) in short bursts covering distances up to 500 m (1,600 ft), and has the ability to accelerate from 0 to 100 km/h (62 mph) in three seconds.
The cheetah is a unique felid, with its closest living relatives being the puma and jaguarundi of the Americas. This cat is notable for modifications in the species' paws, being one of the few felids with only semi-retractable claws.
Its main hunting strategy is to run down swift prey such as various antelope species and hares. Almost every facet of the cheetah's anatomy has evolved to maximise its success in the chase, the result of an evolutionary arms race with its prey. Due to this specialisation, however, the cheetah is poorly equipped to defend itself against other large predators, with speed being its main means of defence.
In the wild, the cheetah is a prolific breeder, with up to nine cubs in a litter. The majority of cubs do not survive to adulthood, mainly as a result of depredation from other predators. The rate of cub mortality varies from area to area, from 50% to 75%, and in extreme cases such as the Serengeti ecosystem, up to 90%. Cheetahs are notoriously poor breeders in captivity, though several organizations, such as the De Wildt Cheetah and Wildlife Centre, have succeeded in breeding high numbers of cubs.
The cheetah is listed as vulnerable, facing various threats including competition with and predation by other carnivores, a gene pool with very low variability, and persecution by mankind. It is a charismatic species and many captive cats are "ambassadors" for their species and wildlife conservation in general.
Etymology.
The word "cheetah" is derived from Hindi 'चीता' ("cītā"), in turn from Sanskrit "citrakāyaḥ", meaning "variegated".
Genetics, evolution, and classification.
The genus name, "Acinonyx", means "no-move-claw" in Greek, while the species name, "jubatus", means "maned" or "crested" in Latin, a reference to the dorsal crest found in cheetah cubs.
The cheetah has unusually low genetic variability. This is accompanied by a very low sperm count, motility, and deformed flagella. Skin grafts between unrelated cheetahs illustrate the former point, in that there is no rejection of the donor skin. It is thought that the species went through a prolonged period of inbreeding following a genetic bottleneck during the last ice age. This suggests that genetic monomorphism did not prevent the cheetah from flourishing across two continents for thousands of years.
The cheetah likely evolved in Africa during the Miocene epoch (26 million to 7.5 million years ago), before migrating to Asia. Recent research has placed the last common ancestor of all existing populations as living in Asia 11 million years ago, which may lead to revision and refinement of existing ideas about cheetah evolution.
The now-extinct species include "Acinonyx pardinensis" (Pliocene epoch), much larger than the modern cheetah and found in Europe, India, and China; and "Acinonyx intermedius" (mid-Pleistocene period), found over the same range. The extinct genus "Miracinonyx" was extremely cheetah-like, but recent DNA analysis has shown that "Miracinonyx inexpectatus", "Miracinonyx studeri", and "Miracinonyx trumani" (early to late Pleistocene epoch), found in North America and called the "North American cheetah" are not true cheetahs, instead being close relatives to the cougar.
Subspecies.
Although many sources list six or more subspecies of cheetah, the taxonomic status of most of these subspecies is unresolved. "Acinonyx rex"—the king cheetah—was abandoned as a species after it was discovered that the variation was caused by a single recessive gene. The subspecies "Acinonyx jubatus guttatus", the woolly cheetah, may also have been a variation due to a recessive gene. Some of the most commonly recognized subspecies include:
Description.
The cheetah's chest is deep and its waist is narrow. The coarse, short fur of the cheetah is tan with round black spots measuring from 2 to across, affording it some camouflage while hunting. There are no spots on its white underside, but the tail has spots, which merge to form four to six dark rings at the end. The tail usually ends in a bushy white tuft. The cheetah has a small head with high-set eyes. Black "tear marks" running from the corner of its eyes down the sides of the nose to its mouth keep sunlight out of its eyes and aid in hunting and seeing long distances. Its thin and fragile body make it well-suited to short bursts of high speed, but not to long-distance running.
Agility, rather than raw speed, accounts for much of the cheetah's ability to catch prey. Cheetahs can accelerate four times as fast as a human (thanks to greater muscle power) and can slow down by 14 kilometers per hour in one stride. They can hunt successfully in densely vegetated areas.
The adult cheetah weighs from 21 to. Its total head-and-body length is from 110 to, while the tail can measure 60 to in length. Cheetahs are 66 to tall at the shoulder. Males tend to be slightly larger than females and have slightly bigger heads, but there is not a great variation in cheetah sizes and it is difficult to tell males and females apart by appearance alone. Compared to a similarly sized leopard, the cheetah is generally shorter-bodied, but is longer tailed and taller (it averages about 90 cm tall) and so it appears more streamlined.
Some cheetahs have a rare fur pattern mutation of larger, blotchy, merged spots. Known as "king cheetahs," they were once thought to constitute a separate subspecies but are in fact African cheetahs; their unusual fur pattern is the result of a single recessive gene. The "king cheetah" has only been seen in the wild a handful of times, but it has been bred in captivity.
The cheetah's paws have semi-retractable claws (known only in three other cat species: the fishing cat, the flat-headed cat and the Iriomote cat), offering extra grip in its high-speed pursuits. The ligament structure of the cheetah's claws is the same as those of other cats; it simply lacks the sheath of skin and fur present in other varieties, and therefore, with the exception of the dewclaw, the claws are always visible. The dewclaw is much shorter and straighter than that of other cats.
Adaptations that enable the cheetah to run as fast as it does include large nostrils that allow for increased oxygen intake, and an enlarged heart and lungs that work together to circulate oxygen efficiently. During a typical chase, its respiratory rate increases from 60 to 150 breaths per minute. While running, in addition to having good traction due to its semi-retractable claws, the cheetah uses its tail as a rudder-like means of steering to allow it to make sharp turns, necessary to outflank prey animals that often make such turns to escape.
Unlike true big cats of subfamily "Pantherinae", the cheetah can purr as it inhales, but cannot roar. By contrast, the big cats can roar but cannot purr, except while exhaling. The cheetah is still considered by some to be the smallest of the big cats. While it is often mistaken for the leopard, the cheetah does have distinguishing features, such as the aforementioned long "tear-streak" lines that run from the corners of its eyes to its mouth, and spots that are not "rosettes". The thinner body frame of the cheetah is also very different from that of the leopard.
The cheetah is a vulnerable species. Of all the big cats, it is the least able to adapt to new environments. It has always proved difficult to breed in captivity, although recently a few zoos have managed to succeed at this. One technique has been to introduce a dog as a playmate and guard dog to enable a captive cheetah to feel less threatened.
Once widely hunted for its fur, the cheetah now suffers more from the loss of both habitat and prey.
The cheetah was formerly considered to be particularly primitive among the cats and to have evolved approximately 18 million years ago. However, new research suggests the last common ancestor of all 40 existing species of felines lived more recently than about 11 million years ago. The same research indicates that the cheetah, while highly derived morphologically, is not of particularly ancient lineage, having separated from its closest living relatives ("Puma concolor", the cougar, and "Puma yaguarondi", the jaguarundi) around five million years ago. These felids have not changed appreciably since they first appeared in the fossil record.
Morphs and variations.
King cheetah.
The king cheetah is a rare mutation of the cheetah characterized by a distinct fur pattern. It was first noted in what was then Southern Rhodesia (modern-day Zimbabwe) in 1926. In 1927, the naturalist Reginald Innes Pocock declared it a separate species, but reversed this decision in 1939 due to lack of evidence; but in 1928, a skin purchased by Walter Rothschild was found to be intermediate in pattern between the king cheetah and spotted cheetah and Abel Chapman considered it to be a color form of the spotted cheetah. Twenty-two such skins were found between 1926 and 1974. Since 1927, the king cheetah was reported five more times in the wild. Although strangely marked skins had come from Africa, a live king cheetah was not photographed until 1974 in South Africa's Kruger National Park. Cryptozoologists Paul and Lena Bottriell photographed one during an expedition in 1975. They also managed to obtain stuffed specimens. It appeared larger than a spotted cheetah and its fur had a different texture. There was another wild sighting in 1986—the first in seven years. By 1987, thirty-eight specimens had been recorded, many from pelts.
Its species status was resolved in 1981 when king cheetahs were born at the De Wildt Cheetah and Wildlife Centre in South Africa. In May 1981, two spotted sisters gave birth there and each litter contained one king cheetah. The sisters had both mated with a wild-caught male from the Transvaal area (where king cheetahs had been recorded). Further king cheetahs were later born at the Centre. It has been known to exist in Zimbabwe, Botswana and in the northern part of South Africa's Transvaal province.
In 2012, the cause of this alternative coat pattern was found to be a mutation in the gene for "transmembrane aminopeptidase Q" ("Taqpep"), the same gene responsible for the striped "mackerel" versus blotchy "classic" patterning seen in tabby cats. The mutation is recessive and must be inherited from both parents for this pattern to appear, which is one reason why it is so rare.
Other color variations.
Other rare color morphs of the species include speckles, melanism, albinism and gray coloration. Most have been reported in Indian cheetahs, particularly in captive specimens kept for hunting.
The Mughal Emperor of India, Jahangir, recorded having a white cheetah presented to him in 1608. In the memoirs of Tuzk-e-Jahangiri, the Emperor, says that in the third year of his reign, "Raja Bir Singh Deo brought a white cheetah to show me. Although other sorts of creatures, both birds and beasts have white varieties ... I had never seen a white cheetah. Its spots, which are (usually) black, were of a blue color, and the whiteness of the body also inclined to bluishness." This suggests a chinchilla mutation which restricts the amount of pigment on the hair shaft. Although the spots were formed of black pigment, the less dense pigmentation gives a hazy, grayish effect. As well as Jahangir's white cheetah at Agra, a report of "incipient albinism" has come from Beaufort West according to Guggisberg.
In a letter to "Nature in East Africa", H. F. Stoneham reported a melanistic cheetah (black with ghost markings) in the Trans-Nzoia District of Kenya in 1925. Vesey Fitzgerald saw a melanistic cheetah in Zambia in the company of a spotted cheetah. Red (erythristic) cheetahs have dark tawny spots on a golden background. Cream (isabelline) cheetahs have pale red spots on a pale background. Some desert region cheetahs are unusually pale; probably they are better-camouflaged and therefore better hunters and more likely to breed and pass on their paler colouration. Blue (Maltese or grey) cheetahs have variously been described as white cheetahs with grey-blue spots (chinchilla) or pale grey cheetahs with darker grey spots (Maltese mutation). A cheetah with hardly any spots was shot in Tanzania in 1921 (Pocock); it had only a few spots on the neck and back, and these were unusually small. Another cheetah with this color-morph was photographed in Kenya in 2012.
Range and habitat.
There are several geographically isolated populations of cheetah, all of which are found in Africa or southwestern Asia. A small population (estimated at about 50) survive in the Khorasan Province of Iran, where conservationists are taking steps to protect them.
It is possible, though doubtful, that some cheetahs remain in India. There have also been several unconfirmed reports of Asiatic Cheetahs in the Balochistan province of Pakistan, with at least one dead animal being discovered recently.
The cheetah thrives in areas with vast expanses of land where prey is abundant. The cheetah likes to live in an open biotope, such as semidesert, prairie, and thick brush, though it can be found in a variety of habitats. In Namibia, for example, it lives in grasslands, savannahs, areas of dense vegetation, and mountainous terrain.
In much of its former range, the cheetah was tamed by aristocrats and used to hunt antelopes in much the same way as is still done with members of the greyhound group of dogs.
Reproduction and behavior.
Females reach maturity in twenty to twenty-four months, and males around twelve months (although they do not usually mate until at least three years old), and mating occurs throughout the year. A study of cheetahs in the Serengeti showed females are sexually promiscuous and often have cubs by many different males.
Females give birth to up to nine cubs after a gestation period of ninety to ninety-eight days, although the average litter size is four. Cubs weigh from 150 to at birth. Unlike some other cats, the cheetah is born with its characteristic spots. Cubs are also born with a downy underlying fur on their necks, called a "mantle", extending to mid-back. This gives them a mane or Mohawk-type appearance; this fur is shed as the cheetah grows older. It has been speculated this mane gives a cheetah cub the appearance of the honey badger (ratel), to scare away potential aggressors. Cubs leave their mother between thirteen and twenty months after birth. Life span is up to twelve years in the wild, but up to twenty years in captivity. The rate of cub mortality varies from area to area, from 50% to 75%, and in extreme cases such as the Serengeti ecosystem, up to 90%. In comparison to the Serengeti, the survival rate of cheetah cubs in the Kgalagadi area was seven times higher.
Unlike males, females are solitary and tend to avoid each other, though some mother/daughter pairs have been known to be formed for small periods of time. The cheetah has a unique, well-structured social order. Females live alone, except when they are raising cubs and they raise their cubs on their own. The first eighteen months of a cub's life are important; cubs must learn many lessons, because survival depends on knowing how to hunt wild prey species and avoid other predators. At eighteen months, the mother leaves the cubs, who then form a sibling ("sib") group that will stay together for another six months. At about two years, the female siblings leave the group, and the young males remain together for life.
Territories.
Males.
Males are often social and may group together for life, usually with their brothers in the same litter; although if a cub is the only male in the litter then two or three lone males may form a group, or a lone male may join an existing group. These groups are called "coalitions". In one Serengeti, 41% of the adult males were solitary, 40% lived in pairs and 19% lived in trios.
A coalition is six times more likely to obtain an animal territory than a lone male, although studies have shown that coalitions keep their territories just as long as lone males— between four to four and a half years.
Males are territorial. Females' home ranges can be very large and a territory including several females' ranges is impossible to defend. Instead, males choose the points at which several of the females' home ranges overlap, creating a much smaller space, which can be properly defended against intruders while maximizing the chance of reproduction. Coalitions will try their best to maintain territories to find females with whom they will mate. The size of the territory also depends on the available resources; depending on the part of Africa, the size of a male's territory can vary greatly from 37 to.
Males mark their territory by urinating on objects that stand out, such as trees, logs, or termite mounds. When male cheetahs urine-mark their territories, they stand less than one meter away from a tree or rock surface with the tail raised, pointing the penis either horizontally backward or 60° upward. The whole coalition contributes to the scent. Males will attempt to kill any intruders, and fights result in serious injury or death.
Females.
Unlike males and other felines, females do not establish territories. Instead, the area they live in is termed a "home range". These overlap with other females' home ranges, often those of their daughters, mothers, or sisters. Females always hunt alone, although cubs will accompany their mothers to learn to hunt once they reach the age of five to six weeks.
The size of a home range depends entirely on the availability of prey. Cheetahs in southern African woodlands have ranges as small as 34 km2, while in some parts of Namibia they can reach 1500 km2.
Vocalizations.
The cheetah cannot roar, but ranks among the more vocal felids. Several sources refer to a wide variety of cheetah vocalizations, but most of these lack a detailed acoustic description which makes it difficult to reliably assess exactly what terms refer to exactly what vocalizations. A short review of the terminology encountered is found in. Some of the vocalizations listed in the literature are:
Diet and hunting.
The cheetah is a carnivore, eating mostly mammalian herbivores under 40 kg and that which specialise in eating C3 plants, including the Thomson's gazelle, the Grant's gazelle, the springbok, impala and blesbok. The young of larger mammals such as wildebeests and zebras are taken at times, and adults too, when cheetahs hunt in groups. Guineafowl and hares are also prey. Ostriches are also taken on occasion. In Iran, cheetahs prey on the Chinkara, Goitered gazelle, ibexes and wild sheep.
While the other big cats often hunt by night, the cheetah is a diurnal hunter. It will, however, hunt on moonlit nights during the full Moon as well, where visibility is excellent. It hunts usually either early in the morning or later in the evening when it is not too hot, but there is still enough light.
The cheetah hunts by vision rather than by scent. Prey is stalked to within 10 -, then chased. This is usually over in less than a minute, and if the cheetah fails to make a catch quickly, it will give up. The cheetah has an average hunting success rate of around 50%. Cheetahs can run at a very high speed; in just two seconds they can reach a speed of 75 kilometers per hour. The estimated top speed of the cheetah ranges from 90 to 128 kilometers per hour. Cheetahs refuse to run when their body temperature reaches 40.5 °C.
Running at very high speeds puts a great deal of strain on the cheetah's body. When sprinting, the cheetah's body temperature quickly elevates. If it is a hard chase, it sometimes needs to rest for half an hour or more.
The cheetah kills its prey by tripping it during the chase, then biting it on the underside of the throat to suffocate it; the cheetah is not strong enough to break the necks of most prey. The bite may also puncture a vital artery in the neck. Then the cheetah proceeds to devour its catch as quickly as possible before the kill is taken by stronger predators.
Data from 367 runs by three female and two male adults, with an average run distance of 173 m, showed that hunting cheetahs can run 58 miles (93 km) per hour. A recent study that followed five African cheetahs indicated that cheetahs relied most heavily on acceleration. Most chases involved extreme maneuverability more than speed. The study indicated that cheetahs seemed to rarely run close to 60 mph or more; on most hunts they reached 30 to 35 mph, but they accelerated and changed direction much more rapidly than any other land animal.
The diet of a cheetah depends on the area in which it lives. For example, on the East African plains, its preferred prey is the Thomson's gazelle. This small antelope is smaller than the cheetah, which makes it an appropriate prey. In contrast, in Kwa-Zulu Natal, the main species of the cheetah's prey preference is the significantly larger nyala, which can weigh up to 130 kg in the male. Cheetahs look for individuals that have strayed some distance from their group, and do not necessarily seek out old or weak ones.
Interspecific predatory relationships.
Despite their speed and hunting prowess, cheetahs are largely outranked by other large predators in most of their range. Because they have evolved for short bursts of extreme speed at the expense of their power, they cannot defend themselves against most of Africa's other predator species. They usually avoid fighting and will surrender a kill immediately to even a single hyena, rather than risk injury. Because cheetahs rely on their speed to obtain their meals, any injury that slows them down could essentially be life-threatening.
A cheetah has a 50% chance of losing its kill to other predators. Cheetahs avoid competition by hunting at different times of the day and by eating immediately after the kill. Due to the reduction in habitat in Africa, cheetahs in recent years have faced greater pressure from other native African predators as available range declines.
The cheetah's mortality is very high during the early weeks of its life; up to 90% of cheetah cubs are killed during this time by lions, leopards, hyenas, African Wild Dogs, or even by eagles. Cheetah cubs often hide in thick brush for safety. Mother cheetahs will defend their young and are at times successful in driving predators away from their cubs. Coalitions of male cheetahs can also chase away other predators, depending on the coalition size and the size and number of the predator. Because of its speed, a healthy adult cheetah has few enemies.
Relationship with humans.
Economic importance.
Cheetah fur was formerly regarded as a status symbol. Today, cheetahs have a growing economic importance for ecotourism and they are also found in zoos. White Oak Conservation in Yulee, Florida, which maintains a significant population of cheetahs, has cited that captive management presents challenges because of health, nutrition and socialization of the cats, but that these have been overcome through collaborations among wildlife facilities.
Cheetahs are far less aggressive than other felids and can be tamed, so cubs are sometimes illegally sold as pets.
Cheetahs were formerly, and sometimes still are, hunted because many farmers believe that they eat livestock. When the species came under threat, numerous campaigns were launched to try to educate farmers and encourage them to conserve cheetahs. Recent evidence has shown that cheetahs will not attack and eat livestock if they can avoid doing so, as they prefer their wild prey. However, they have no problem with including farmland as part of their territory, leading to conflict.
Taming.
Ancient Egyptians often kept cheetahs as pets, and also tamed and trained them for hunting. (But not domesticated i.e., bred under human control.) Cheetahs would be taken to hunting fields in low-sided carts or by horseback, hooded and blindfolded, and kept on leashes while dogs flushed out their prey. When the prey was near enough, the cheetahs would be released and their blindfolds removed. This tradition was passed on to the ancient Persians and brought to India, where the practice was continued by Indian princes into the twentieth century. Cheetahs continued to be associated with royalty and elegance, their use as pets spreading just as their hunting skills were. Other such princes and kings kept them as pets, including Genghis Khan and Charlemagne, who boasted of having kept cheetahs within their palace grounds. Akbar the Great, ruler of the Mughal Empire from 1556 to 1605, kept as many as 1,000 cheetahs. As recently as the 1930s, the Emperor of Ethiopia, Haile Selassie, was often photographed leading a cheetah by a leash.
Cheetahs are still tamed in the modern world. One example is Burmani, who has been raised in England at Eagle Heights wild animal park from the age of three months. He was bred in a deer park in Germany. He is so tame that he has lost his hunting instinct.
Conservation status.
Cheetah cubs have a high mortality rate due to predation by other carnivores, such as the lion and hyena, and perhaps genetic factors. It has been suggested that the low genetic diversity of cheetahs is a cause of poor sperm, birth defects, cramped teeth, curled tails, and bent limbs. Some biologists even believe that they are too inbred to flourish as a species. Note, however, that they lost most of their genetic diversity thousands of years ago (see the beginning of this article), and yet seem to have only been in decline in the last century or so, suggesting that factors other than genetics are mainly responsible.
Cheetahs are included on the International Union for Conservation of Nature (IUCN) list of vulnerable species (African subspecies threatened, Asiatic subspecies in critical situation) as well as on the US Endangered Species Act: threatened species - Appendix I of CITES (Convention on International Trade in Endangered Species). Approximately 12,400 cheetahs remain in the wild in twenty-five African countries; Namibia has the most, with about 2,500. Another 50 to 60 critically endangered Asiatic cheetahs are thought to remain in Iran. There have been successful breeding programs, including the use of in vitro fertilisation, in zoos around the world.
Founded in Namibia in 1990, the Cheetah Conservation Fund's mission is to be the world's resource charged with protecting the cheetah and to ensure its future. The organization works with all stakeholders within the cheetah's ecosystem to develop best practices in research, education and ecology and create a sustainable model from which all other species, including people, will benefit.
The South African Cheetah Conservation Foundation has close links and assists in training and sharing program successes with other countries where cheetahs live, including Botswana, South Africa, Zimbabwe, Iran and Algeria. The organization's international program includes distributing materials, lending resources and support, and providing training through Africa and the rest of the world.
Re-introduction project in India.
Asiatic cheetahs have been known to exist in India for a very long time, but as a result of hunting and other causes, cheetahs have been extinct in India since the 1940s. A captive propagation project has been proposed. Minister of Environment and Forests Jairam Ramesh told the Rajya Sabha on 7 July 2009, "The cheetah is the only animal that has been described extinct in India in the last 100 years. We have to get them from abroad to repopulate the species." He was responding to a call for attention from Rajiv Pratap Rudy of the Bharatiya Janata Party (BJP). "The plan to bring back the cheetah, which fell to indiscriminate hunting and complex factors like a fragile breeding pattern is audacious given the problems besetting tiger conservation." Two naturalists, Divya Bhanusinh and MK Ranjit Singh, suggested importing cheetahs from Africa, after which they will be bred in captivity and, in time, released in the wild.
However, the plan to reintroduce the African cheetahs to India has been suspended after discovering the distinctness between the cheetahs from Asia and Africa, having been separated between 32,000 to 67,000 years ago.

</doc>
<doc id="45610" url="http://en.wikipedia.org/wiki?curid=45610" title="Mata Hari">
Mata Hari

Margaretha Geertruida "Margreet" MacLeod (née Zelle; 7 August 1876 – 15 October 1917), better known by the stage name Mata Hari, was a Frisian exotic dancer and courtesan who was convicted of being a spy and executed by firing squad in France under charges of espionage for Germany during World War I.
Early life.
Margaretha Geertruida Zelle was born in Leeuwarden, the Netherlands. She was the eldest of four children of Adam Zelle (2 October 1840 – 13 March 1910) and his first wife Antje van der Meulen (21 April 1842 – 9 May 1891). She had three brothers. Her father owned a hat shop, made successful investments in the oil industry, and became affluent enough to give Margaretha a lavish early childhood that included exclusive schools until the age of 13.
However, Margaretha's father went bankrupt in 1889, her parents divorced soon thereafter, and her mother died in 1891. Her father remarried in Amsterdam on 9 February 1893 to Susanna Catharina ten Hoove (11 March 1844 – 1 December 1913), with whom he had no children. The family had fallen apart and Margaretha moved to live with her godfather, Mr. Visser, in Sneek. In Leiden, she studied to be a kindergarten teacher, but when the headmaster began to flirt with her conspicuously, she was removed from the institution by her offended godfather. After only a few months, she fled to her uncle's home in The Hague.
Dutch East Indies.
At 18, Zelle answered an advertisement in a Dutch newspaper placed by Dutch Colonial Army Captain Rudolf MacLeod (1 March 1856 – 9 January 1928) who was living in the then Dutch East Indies (now Indonesia) and was looking for a wife. Zelle married MacLeod in Amsterdam on 11 July 1895. He was the son of Captain John Brienen MacLeod (a descendant of the Gesto branch of the MacLeods of Skye, hence his Scottish name) and Dina Louisa, Baroness Sweerts de Landas. This was significant as the marriage enabled her to move into the Dutch upper class and her finances were placed on a sound footing. They moved to Malang on the East side of the island of Java, traveling out on in May 1897, and had two children, Norman-John MacLeod (30 January 1897 – 27 June 1899) and Louise Jeanne MacLeod (2 May 1898 – 10 August 1919).
The marriage was an overall disappointment. MacLeod appears to have been an alcoholic who would take out his frustrations on his wife, who was twenty years younger, and whom he blamed for his lack of promotion. He also openly kept a concubine, a socially accepted practice in the Dutch East Indies at that time. The disenchanted Zelle abandoned him temporarily, moving in with Van Rheedes, another Dutch officer. For months, she studied the Indonesian traditions intensively, joining a local dance company. In 1897, she revealed her artistic name of "Mata Hari," Malay ("Indonesian as a standardized register did not exist in 1897") for "sun" (literally, "eye of the day"), in correspondence to her relatives in the Netherlands.
At MacLeod's urging, Zelle returned to him, although his aggressive demeanour did not change. She escaped her circumstances by studying the local culture. In 1899, their children fell violently ill from complications relating to the treatment of syphilis contracted from their parents, though the family claimed they were poisoned by an irate servant. Jeanne survived, but Norman died. Some sources maintain that one of MacLeod's enemies may have poisoned a supper to kill both of their children. After moving back to the Netherlands, MacLeod left her in 1902 and took Jeanne with him. The couple divorced in 1907. Zelle was awarded custody of Jeanne, but after MacLeod deliberately reneged on a support payment, Zelle was forced to give Jeanne back to him. Jeanne later died at the age of 21, also possibly from complications relating to syphilis.
Paris.
In 1903, Zelle moved to Paris, where she performed as a circus horse rider, using the name Lady MacLeod, much to the disapproval of the Dutch MacLeods. Struggling to earn a living, she also posed as an artist's model.
By 1905, Mata Hari began to win fame as an exotic dancer. She was a contemporary of dancers Isadora Duncan and Ruth St. Denis, leaders in the early modern dance movement, which around the turn of the 20th century looked to Asia and Egypt for artistic inspiration. Critics would later write about this and other such movements within the context of Orientalism. Gabriel Astruc became her personal booking agent.
Promiscuous, flirtatious, and openly flaunting her body, Mata Hari captivated her audiences and was an overnight success from the debut of her act at the Musée Guimet on 13 March 1905. She became the long-time mistress of the millionaire Lyon industrialist Émile Étienne Guimet, who had founded the Musée. She posed as a Java princess of priestly Hindu birth, pretending to have been immersed in the art of sacred Indian dance since childhood. She was photographed numerous times during this period, nude or nearly so. Some of these pictures were obtained by MacLeod and strengthened his case in keeping custody of their daughter.
Mata Hari brought this carefree provocative style to the stage in her act, which garnered wide acclaim. The most celebrated segment of her act was her progressive shedding of clothing until she wore just a jeweled bra and some ornaments upon her arms and head. She was seldom seen without a bra, as she was self-conscious about being small-breasted. She wore a bodystocking for her performances that was similar in color to her own skin.
Although Mata Hari's claims about her origins were fictitious, it was very common for entertainers of her era to invent colorful stories about their origins as part of the show. Her act was spectacularly successful because it elevated exotic dance to a more respectable status, and so broke new ground in a style of entertainment for which Paris was later to become world famous. Her style and her free-willed attitude made her a very popular woman, as did her eagerness to perform in exotic and revealing clothing. She posed for provocative photos and mingled in wealthy circles. At the time, as most Europeans were unfamiliar with the Dutch East Indies and thus thought of Mata Hari as exotic, it was assumed her claims were genuine.
By about 1910, myriad imitators had arisen. Critics began to opine that the success and dazzling features of the popular Mata Hari were due to cheap exhibitionism and lacked artistic merit. Although she continued to schedule important social events throughout Europe, she was held in disdain by serious cultural institutions as a dancer who did not know how to dance.
Mata Hari's career went into decline after 1912. On 13 March 1915, she performed in what would be the last show of her career. She had begun her career relatively late for a dancer, and had started putting on weight. However, by this time she had become a successful courtesan, though she was known more for her sensuality and eroticism rather than for striking classical beauty. She had relationships with high-ranking military officers, politicians, and others in influential positions in many countries. Her relationships and liaisons with powerful men frequently took her across international borders. Prior to World War I, she was generally viewed as an artist and a free-spirited bohemian, but as war approached, she began to be seen by some as a wanton and promiscuous woman, and perhaps a dangerous seductress.
"Double agent?".
During World War I, the Netherlands remained neutral. As a Dutch subject, Zelle was thus able to cross national borders freely. To avoid the battlefields, she travelled between France and the Netherlands via Spain and Britain, and her movements inevitably attracted attention. In 1916, she was travelling by steamer from Spain when her ship called at the English port of Falmouth. There she was arrested and brought to London where she was interrogated at length by Sir Basil Thomson, Assistant Commissioner at New Scotland Yard in charge of counter-espionage. He gave an account of this in his 1922 book "Queer People," saying that she eventually admitted to working for French intelligence. Initially detained in Cannon Street police station, she was then released and stayed at the Savoy Hotel. A full transcript of the interview is in Britain's National Archives and was broadcast, with Mata Hari played by Eleanor Bron, on the independent station London Broadcasting in 1980.
It is unclear if she lied on this occasion, believing the story made her sound more intriguing, or if French authorities were using her in such a way, but would not acknowledge her due to the embarrassment and international backlash it could cause.
In January 1917, the German military attaché in Madrid transmitted radio messages to Berlin describing the helpful activities of a German spy code-named H-21. French intelligence agents intercepted the messages and, from the information it contained, identified H-21 as Mata Hari. The messages were in a code that some claimed that German intelligence knew had already been broken by the French (in fact it had been broken not by the French, but by the British "Room 40" team), leaving some to claim that the messages were contrived. However, this same code, which the Germans were convinced was unbreakable was used to transmit the Zimmermann Telegram.
Trial and execution.
On 13 February 1917, Mata Hari was arrested in her room at the Hotel Elysée Palace on the Champs Elysées in Paris. She was put on trial on 24 July, accused of spying for Germany, and consequently causing the deaths of at least 50,000 soldiers. Although the French and British intelligence suspected her of spying for Germany, neither could produce definite evidence against her. Supposedly secret ink was found in her room, which was incriminating evidence in that period. She contended that it was part of her makeup. She wrote several letters to the Dutch Consul in Paris, claiming her innocence. "My international connections are due of my work as a dancer, nothing else ... Because I really did not spy, it is terrible that I cannot defend myself". Her defence attorney, veteran international lawyer Edouard Clunet, faced impossible odds; he was denied permission either to cross-examine the prosecution's witnesses or to examine his own witnesses directly . Under the circumstances, her conviction was a foregone conclusion. 
German documents unsealed in the 1970s proved that Mata Hari was truly a German agent. In the autumn of 1915, she entered German service, and on orders of section III B-Chief Walter Nicolai, she was instructed about her duties by Major Roepell during a stay in Cologne. Her reports were to be sent to the Kriegsnachrichtenstelle West (War News Post West) in Düsseldorf under Roepell as well as to the Agent mission in the German embassy in Madrid under Major Arnold Kalle, with her direct handler being Captain Hoffmann, who also gave her the code name H-21. It is contended by some historians, however, that Mata Hari may have merely accepted money from the Germans without actually carrying out any spy duties.
In December 1916, the French Second Bureau of the French War Ministry let Mata Hari obtain the names of six Belgian agents. Five were suspected of submitting fake material and working for the Germans, while the sixth was suspected of being a double agent for Germany and France. Two weeks after Mata Hari had left Paris for a trip to Madrid, the double agent was executed by the Germans, while the five others continued their operations. This development served as proof to the Second Bureau that the names of the six spies had been communicated by Mata Hari to the Germans.
She was executed by firing squad on 15 October 1917, at the age of 41.
Disappearance and rumours.
Mata Hari's body was not claimed by any family members and was accordingly used for medical study. Her head was embalmed and kept in the Museum of Anatomy in Paris, but in 2000, archivists discovered that the head had disappeared, possibly as early as 1954, when the museum had been relocated. Records dated from 1918 show that the museum also received the rest of the body, but none of the remains could later be accounted for.
A 1934 "New Yorker" article reported that at her execution she wore "a neat Amazonian tailored suit, especially made for the occasion, and a pair of new white gloves" though another account indicates she wore the same suit, low-cut blouse and tricorn hat ensemble which had been picked out by her accusers for her to wear at trial, and which was still the only full, clean outfit which she had in prison. Neither description matches photographic evidence. According to an eyewitness account by British reporter Henry Wales, she was not bound and refused a blindfold. Wales records her death, saying that after the volley of shots rang out, "Slowly, inertly, she settled to her knees, her head up always, and without the slightest change of expression on her face. For the fraction of a second it seemed she tottered there, on her knees, gazing directly at those who had taken her life. Then she fell backward, bending at the waist, with her legs doubled up beneath her". A non-commissioned officer then walked up to her body, pulled out his revolver, and shot her in the head to make sure she was dead.
Museum.
The Fries Museum in Leeuwarden, the Netherlands, exhibits a "Mata Hari Room". Included in the exhibit are two of her personal scrapbooks and an oriental rug embroidered with the footsteps of her fan dance. Located in Mata Hari's native town, the museum is well known for research into the life and career of Leeuwarden's world-famous citizen.
Legend and popular culture.
The idea of an exotic dancer working as a lethal double agent using her powers of seduction to extract military secrets from her many lovers made Mata Hari an enduring archetype of the femme fatale.

</doc>
<doc id="45611" url="http://en.wikipedia.org/wiki?curid=45611" title="Dr. Feelgood (band)">
Dr. Feelgood (band)

Dr. Feelgood are a British pub rock band formed in 1971. Hailing from Canvey Island, Essex, they are best known for early singles like "Back in the Night" and "Roxette". The group's original distinctively British R&B sound was centred on Wilko Johnson's choppy guitar style. Along with Johnson, the original band line-up included singer Lee Brilleaux and the rhythm section of John B. Sparks, known as "Sparko", on bass guitar and John Martin, known as "The Big Figure", on drums. Although their most commercially productive years were the early to mid-1970s, and in spite of Brilleaux's death in 1994 of lymphoma, a version of the band (featuring none of the original members) continues to tour and record to this day.
Career.
Early years.
The band was formed in Canvey Island in 1971 by Johnson, Brilleaux and Sparks, who had all been members of existing R&B bands, and soon added drummer John Martin. They took their name from a 1962 record by the American blues pianist and singer Willie Perryman (also known as "Piano Red") called "Dr. Feel-Good", which Perryman recorded under the name of Dr. Feelgood & The Interns. The song was covered by several British beat groups in the 1960s, including Johnny Kidd & The Pirates. The term is also a slang term for heroin or for a doctor who is willing to overprescribe drugs. 
By late 1973, the band's driving R&B had made them one of the most popular bands on the growing London pub rock circuit, and they recorded their debut album, "Down by the Jetty", for United Artists in 1974. Like many pub rock acts, Dr Feelgood were known primarily for their high energy live performances honed through constant touring and regular performances, although their studio albums like "Down by the Jetty" and "Malpractice" (1975) were also popular. 
Their breakthrough 1976 live album, "Stupidity", reached number one in the UK Albums Chart (their only chart-topper). But after the follow-up "Sneakin' Suspicion", Johnson left the group because of conflicts with Lee Brilleaux. He was replaced by John 'Gypie' Mayo. With Mayo, the band was never as popular as with Johnson, but still enjoyed their only Top Ten hit single in 1979, with "Milk and Alcohol". Johnson never achieved any great success outside of the band, apart from a brief spell with Ian Dury and The Blockheads from 1980. Fans always speculated about a return by Johnson that never occurred.
Later years.
Despite Mayo's departure in 1981, and various subsequent line-up changes which left Brilleaux the only remaining original member, Dr Feelgood continued touring and recording through the 1980s. However, the band then suffered an almost career-finishing blow, when Brilleaux died of cancer on 7 April 1994.
As Brilleaux had insisted prior to his demise, Dr Feelgood reunited in May 1995, initially with vocalist Pete Gage, (not to be confused with guitarist Pete Gage of Geno Washington and Vinegar Joe) and recommenced touring in 1996. Though the band contained no original members at this point, the musicians backing Gage had all previously played as members of Dr. Feelgood for at least 5 years, and in some cases for over a decade. In 1999 Gage was replaced by Robert Kane, formerly of The Animals II and The Alligators, who celebrated his 1,000th gig as the frontman of Dr. Feelgood in April 2007.
Every year since Brilleaux's death, a special concert, known as the Lee Brilleaux Birthday Memorial, has been held on Canvey Island, where ex and current Feelgoods celebrate the music of Dr Feelgood, and raise money for The Fair Havens Hospice in Westcliff-on-Sea. Fans attend from all over the globe, and the 17th event was held on 7 May 2010. Although still based in the UK, Dr Feelgood continue to play across the world, with concerts in 2010 including, Austria, Bahrain, Belgium, Finland, France, the Netherlands, Italy, Spain and Switzerland. 
Band manager Chris Fenwick also organises an annual walk around Canvey to commemorate Brileaux's life, as well as additional walking tours during which he points out landmarks from the band's career. These include the jetty featured in the photograph on the band's first album cover, and venues where they played early in their career such The Lobster Smack inn, The Monaco Nightclub and The Canvey Club (disguised as 'The Alibi Club' on the sleeve of the album "Sneakin' Suspicion"). 
A film by Julien Temple about the early days of the band, "Oil City Confidential", premiered at the London Film Festival on 22 October 2009, and received a standing ovation. Guest of honour was Lee Brilleaux's mother Joan Collinson, along with his widow Shirley and children Kelly and Nick. All the surviving members of the original band were present along with manager Chris Fenwick, former tour manager and Stiff Records boss Jake Riviera and other friends and colleagues of the band. The film has its own Facebook page.
Reviewing the film for "The Independent", Nick Hasted concluded: "Feelgood are remembered in rock history, if at all, as John the Baptists to punk's messiahs". On general release from 1 February 2010, the film was critically well received, with Peter Bradshaw of "The Guardian" describing it as “ ..a vivid study of period, music and place”. The film was first broadcast on BBC Four in April 2010.
A major exhibition of memorabilia celebrating the band's career ran at The Canvey Club between May and July 2013, having been extended several times.
Quotations.
"Lee Brilleaux"
"Lee Brilleaux (1976)"
"Wilko Johnson" (2009) 
"Wilko Johnson" (2013)

</doc>
<doc id="45619" url="http://en.wikipedia.org/wiki?curid=45619" title="Lagged Fibonacci generator">
Lagged Fibonacci generator

A Lagged Fibonacci generator (LFG or sometimes LFib) is an example of a pseudorandom number generator. This class of random number generator is aimed at being an improvement on the 'standard' linear congruential generator. These are based on a generalisation of the Fibonacci sequence.
The Fibonacci sequence may be described by the recurrence relation:
Hence, the new term is the sum of the last two terms in the sequence. This can be generalised to the sequence:
In which case, the new term is some combination of any two previous terms. m is usually a power of 2 (m = 2M), often 232 or 264. The formula_3 operator denotes a general binary operation. This may be either addition, subtraction, multiplication, or the bitwise arithmetic exclusive-or operator (XOR). The theory of this type of generator is rather complex, and it may not be sufficient simply to choose random values for j and k. These generators also tend to be very sensitive to initialisation.
Generators of this type employ k words of state (they 'remember' the last k values).
If the operation used is addition, then the generator is described as an "Additive Lagged Fibonacci Generator" or ALFG, if multiplication is used, it is a "Multiplicative Lagged Fibonacci Generator" or MLFG, and if the XOR operation is used, it is called a "Two-tap generalised feedback shift register" or GFSR. The Mersenne twister algorithm is a variation on a GFSR. The GFSR is also related to the linear feedback shift register, or LFSR.
Properties of lagged Fibonacci generators.
Lagged Fibonacci generators have a maximum period of (2k - 1)*2M-1 if addition or subtraction is used, and (2k-1)*k if exclusive-or operations are used to combine the previous values. If, on the other hand, multiplication is used, the maximum period is (2k - 1)*2M-3, or 1/4 of period of the additive case.
For the generator to achieve this maximum period, the polynomial:
must be primitive over the integers mod 2. Values of j and k satisfying this constraint have been published in the literature. Popular pairs are:
Another list of possible values for "j" and "k" is on page 29 of volume 2 of "The Art of Computer Programming":
Note that the smaller number have short periods (only a few "random" numbers are generated before the first "random" number is repeated and the sequence restarts).
If addition is used, it is required that at least one of the first k values chosen to initialise the generator be odd; if multiplication is used, instead, it is required that all the first k values be odd.
It has been suggested that good ratios between j and k are approximately the golden ratio.
Problems with LFGs.
In a paper on four-tap shift registers, Robert M. Ziff, referring to LFGs that use the XOR operator, states that "It is now widely known that such generators, in particular with the two-tap rules such as R(103, 250), have serious deficiencies. Marsaglia observed very poor behavior with R(24,55) and smaller generators, and advised against using generators of this type altogether. ... The basic problem of two-tap generators R(a, b) is that they have a built-in three-point correlation between formula_4, formula_5, and formula_6, simply given by the generator itself ... While these correlations are spread over the size formula_7 of the generator itself, they can evidently still lead to significant errors.". This only refers to the standard LFG where each new number in the sequence depends on two previous numbers. A three-tap LFG has been shown to eliminate some statistical problems such as failing the Birthday Spacings and Generalized Triple tests.
The initialization of LFGs is a very complex problem. The output of LFGs is very sensitive to initial conditions, and statistical defects may appear initially but also periodically in the output sequence unless extreme care is taken . Another potential problem with LFGs is that the mathematical theory behind them is incomplete, making it necessary to rely on statistical tests rather than theoretical performance.

</doc>
<doc id="45621" url="http://en.wikipedia.org/wiki?curid=45621" title="Psychopharmacology">
Psychopharmacology

Psychopharmacology (from Greek ψῡχή, "psȳkhē", "breath, life, soul"; φάρμακον, "pharmakon", "drug"; and -λογία, "-logia") is the scientific study of the effects drugs have on mood, sensation, thinking, and behavior. It is distinguished from neuropsychopharmacology, which emphasizes the correlation between drug-induced changes in the functioning of cells in the nervous system and changes in consciousness and behavior. 
The field of psychopharmacology studies a wide range of substances with various types of psychoactive properties, focusing primarily on the chemical interactions with the brain.
Psychoactive drugs interact with particular target sites or receptors found in the nervous system to induce widespread changes in physiological or psychological functions. The specific interaction between drugs and their receptors is referred to as "drug action", and the widespread changes in physiological or psychological function is referred to as "drug effect". These drugs may originate from natural sources such as plants and animals, or from artificial sources such as chemical synthesis in the laboratory.
Historical overview.
Early psychopharmacology.
Not often mentioned or included in the field of psychopharmacology today, are psychoactive substances not identified as useful in modern mental health settings or references. These substances are naturally occurring, but nonetheless psychoactive, and are compounds identified through the work of ethnobotanists and ethnomycologists (and others who study the native use of naturally occurring psychoactive drugs). However, although these substances have been used throughout history by various cultures, and have a profound effect on mentality and brain function, they have not always attained the degree of scrutinous evaluation that lab-made compounds have. Nevertheless, some, such as psilocybin and mescaline, have provided a basis of study for the compounds that are used and examined in the field today. Hunter-gatherer societies tended to favor psychedelics, dissociatives and deliriants, and today their use can still be observed in many surviving tribal cultures. The exact drug used depends on what the particular ecosystem a given tribe lives in can support, and are typically found growing wild. Such drugs include various psychedelic mushrooms containing psilocybin, muscimol, and muscarine (to name a few), and cacti containing mescaline and other chemicals, along with myriad other psychoactive-chemical-containing plants. These societies generally attach spiritual significance to such drug use, and often incorporate it into their religious practices. With the dawn of the Neolithic and the proliferation of agriculture, new psychoactives came into use as a natural by-product of farming. Among them were opium, cannabis, and alcohol derived from the fermentation of cereals and fruits. Most societies began developing herblores, lists of herbs which were good for treating various physical and mental ailments. For example, St. John's Wort was traditionally prescribed in parts of Europe for depression (in addition to use as a general-purpose tea), and Chinese medicine developed elaborate lists of herbs and preparations. These and various other substances that have an effect on the brain are still used as remedies in many cultures.
Modern psychopharmacology.
The dawn of contemporary psychopharmacology marked the beginning of the use of psychiatric drugs to treat psychological illnesses. It brought with it the use of opiates and barbiturates for the management of acute behavioral issues in patients. In the early stages, psychopharmacology was primarily used for sedation. Then with the 1950s came the establishment of chlorpromazine for psychoses, lithium carbonate for mania, and then in rapid succession, the development of tricyclic antidepressants, monoamine oxidase inhibitors, benzodiazepines, among other antipsychotics and antidepressants. A defining feature of this era includes an evolution of research methods, with the establishment of placebo-controlled, double blind studies, and the development of methods for analyzing blood levels with respect to clinical outcome and increased sophistication in clinical trials. The early 1960s revealed a revolutionary model by Julius Axelrod describing nerve signals and synaptic transmission, which was followed by a drastic increase of biochemical brain research into the effects of psychotropic agents on brain chemistry. After the 1960s, the field of psychiatry shifted to incorporate the indications for and efficacy of pharmacological treatments, and began to focus on the use and toxicities of these medications. The 1970s and 1980s were further marked by a better understanding of the synaptic aspects of the action mechanisms of drugs. However, the model has its critics, too – notably Joanna Moncrieff and the Critical Psychiatry Network.
Chemical signaling.
Neurotransmitters.
Psychoactive drugs exert their sensory and behavioral effects almost entirely by acting on neurotransmitters and by modifying one or more aspects of synaptic transmission. Neurotransmitters can be viewed as chemicals through which neurons primarily communicate; psychoactive drugs affect the mind by altering this communication. Drugs may act by 1) serving as a precursor for the neurotransmitter; 2) inhibiting neurotransmitter synthesis; 3) preventing storage of neurotransmitter in the presynaptic vesicle; 4) stimulating or inhibiting neurotransmitter release; 5) stimulating or blocking post-synaptic receptors; 6) stimulating autoreceptors, inhibiting neurotransmitter release; 7) blocking autoreceptors, increasing neurotransmitter release; 8) inhibiting neurotransmission breakdown; or 9) blocking neurotransmitter reuptake by the presynaptic neuron.
Hormones.
The other central method through which drugs act is by affecting communications between cells through hormones. Neurotransmitters can usually only travel a microscopic distance before reaching their target at the other side of the synaptic cleft, while hormones can travel long distances before reaching target cells anywhere in the body. Thus, the endocrine system is a critical focus of psychopharmacology because 1) drugs can alter the secretion of many hormones; 2) hormones may alter the behavioral responses to drugs; 3) hormones themselves sometimes have psychoactive properties; and 4) the secretion of some hormones, especially those dependent on the pituitary gland, is controlled by neurotransmitter systems in the brain.
Psychopharmacological substances.
Alcohol.
Alcohol is a depressant, the effects of which may vary according to dosage amount, frequency, and chronicity. As a member of the sedative-hypnotic class, at the lowest doses, the individual feels relaxed and less anxious. In quiet settings, the user may feel drowsy, but in settings with increased sensory stimulation, individuals may feel uninhibited and more confident. High doses of alcohol rapidly consumed may produce amnesia for the events that occur during intoxication. Other effects include reduced coordination, which leads to slurred speech, impaired fine-motor skills, and delayed reaction time. The effects of alcohol on the body’s neurochemistry are more difficult to examine than some other drugs. This is because the chemical nature of the substance makes it easy to penetrate into the brain, and it also influences the phospholipid bilayer of neurons. This allows alcohol to have a widespread impact on many normal cell functions and modifies the actions of several neurotransmitter systems. Alcohol inhibits glutamate (a major excitatory neurotransmitter in the nervous system) neurotransmission by reducing the effectiveness at the NMDA receptor, which is related to memory loss associated with intoxication. It also modulates the function of GABA, a major inhibitory amino acid neurotransmitter. The reinforcing qualities of alcohol leading to repeated use – and thus also the mechanisms of withdrawal from chronic alcohol use – are partially due to the substance’s action on the dopamine system. This is also due to alcohol’s effect on the opioid systems, or endorphins, that have opiate-like effects, such as modulating pain, mood, feeding, reinforcement, and response to stress.
Antidepressants.
Antidepressants reduce symptoms of mood disorders primarily through the regulation of norepinephrine and serotonin (particularly the 5-HT receptors). After chronic use, neurons adapt to the change in biochemistry, resulting in a change in pre- and postsynaptic receptor density and second messenger function.
Monoamine oxidase inhibitors (MAOIs) are the oldest class of antidepressants. They inhibit monoamine oxidase, the enzyme that metabolizes the monoamine neurotransmitters in the presynaptic terminals that are not contained in protective synaptic vesicles. The inhibition of the enzyme increases the amount of neurotransmitter available for release. It increases norepinephrine, dopamine, and 5-HT and thus increases the action of the transmitters at their receptors. MAOIs have been somewhat disfavored because of their reputation for more serious side effects.
Tricyclic antidepressants (TCAs) work through binding to the presynaptic transporter proteins and blocking the reuptake of norepinephrine or 5-HT into the presynaptic terminal, prolonging the duration of transmitter action at the synapse.
Selective serotonin reuptake inhibitors (SSRIs) selectively block the reuptake of serotonin (5-HT) through their inhibiting effects on the sodium/potassium ATP-dependent serotonin transporter in presynaptic neurons. This increases the availability of 5-HT in the synaptic cleft. The main parameters to consider in choosing an antidepressant are side effects and safety. Most SSRIs are available generically and are relatively inexpensive. Older antidepressants, such as the TCAs and MAOIs usually require more visits and monitoring, and this may offset the low expense of the drugs. The SSRIs are relatively safe in overdose and better tolerated than the TCAs and MAOIs for most patients.
Antipsychotics.
All antipsychotic substances, except clozapine, are relatively potent postsynaptic dopamine receptor blockers (dopamine antagonists). All of the effective antipsychotics, except clozapine, act on the nigrostriatal system. For an antipsychotic to be effective, it generally requires a dopamine antagonism of 60%-80% of dopamine D2 receptors.
First generation (typical) antipsychotics: Traditional neuroleptics modify several neurotransmitter systems, but their clinical effectiveness is most likely due to their ability to antagonize dopamine transmission by competitively blocking the receptors or by inhibiting dopamine release. The most serious and troublesome side effects of these classical antipsychotics are movement disorders that resemble the symptoms of Parkinson's disease, because the neuroleptics antagonize dopamine receptors broadly, also reducing the normal dopamine-mediated inhibition of cholinergic cells in the striatum.
Second-generation (atypical) antipsychotics: The concept of “atypicality” is from the finding that the second generation antipsychotics (SGAs) had a greater serotonin/dopamine ratio than did earlier drugs, and might be associated with improved efficacy (particularly for the negative symptoms of psychosis) and reduced extrapyramidal side effects. Some of the efficacy of atypical antipsychotics may be due to 5-HT2 antagonism or the blockade of other dopamine receptors. Agents that purely block 5-HT2 or dopamine receptors other than D2 have often failed as effective antipsychotics.
Benzodiazepines.
Benzodiazepines are often used to reduce anxiety symptoms, muscle tension, seizure disorders, insomnia, symptoms of alcohol withdrawal, and panic attack symptoms. Their action is primarily on specific benzodiazepine sites on the GABAA receptor. This receptor complex is thought to mediate the anxiolytic, sedative, and anticonvulsant actions of the benzodiazepines. Use of benzodiazepines carries the risk of tolerance (necessitating increased dosage), dependence, and abuse. Taking these drugs for a long period of time can lead to withdrawal symptoms upon abrupt discontinuation.
Hallucinogens.
Hallucinogens cause perceptual and cognitive distortions without delirium. The state of intoxication is often called a “trip”. Onset is the first stage after an individual ingests (LSD, psilocybin, or mescaline) or smokes (dimethyltryptamine) the substance. This stage may consist of visual effects, with an intensification of colors and the appearance of geometric patterns that can be seen with one’s eyes closed. This is followed by a plateau phase, where the subjective sense of time begins to slow and the visual effects increase in intensity. The user may experience synesthesia, a crossing-over of sensations (for example, one may “see” sounds and “hear” colors). In addition to the sensory-perceptual effects, hallucinogenic substances may induce feelings of depersonalization, emotional shifts to a euphoric or anxious/fearful state, and a disruption of logical thought. Hallucinogens are classified chemically as either indoleamines (specifically tryptamines), sharing a common structure with serotonin, or as phenethylamines, which share a common structure with norepinephrine. Both classes of these drugs are agonists at the 5-HT2 receptors; this is thought to be the central component of their hallucinogenic properties. Activation of 5-HT2A may be particularly important for hallucinogenic activity. However, repeated exposure to hallucinogens leads to rapid tolerance, likely through down-regulation of these receptors in specific target cells.
Hypnotics.
Hypnotics are often used to treat the symptoms of insomnia, or other sleep disorders. Benzodiazepines are still among the most widely prescribed sedative-hypnotics in the United States today. Certain non-benzodiazepine drugs are used as hypnotics as well. Although they lack the chemical structure of the benzodiazepines, their sedative effect is similarly through action on the GABAA receptor. They also have a reputation of being less addictive than benzodiazepines. Melatonin, a naturally-occurring hormone, is often used over the counter (OTC) to treat insomnia and jet lag. This hormone appears to be excreted by the pineal gland early during the sleep cycle and may contribute to human circadian rhythms. Because OTC melatonin supplements are not subject to careful and consistent manufacturing, more specific melatonin agonists are sometimes preferred. They are used for their action on melatonin receptors in the suprachiasmatic nucleus, responsible for sleep-wake cycles. Many barbiturates have or had an FDA-approved indication for use as sedative-hypnotics, but have become less widely used because of their limited safety margin in overdose, their potential for dependence, and the degree of central nervous system depression they induce. The amino-acid L-tryptophan is also available OTC, and seems to be free of dependence or abuse liability. However, it is not as powerful as the traditional hypnotics. Because of the possible role of serotonin in sleep patterns, a new generation of 5-HT2 antagonists are in current development as hypnotics.
Cannabis and the cannabinoids.
Cannabis consumption produces a dose-dependent state of intoxication in humans. There is commonly increased blood flow to the skin, which leads to sensations of warmth or flushing, and heart rate is also increased. It also frequently induces increased hunger. Iversen (2000) categorized the subjective and behavioral effects often associated with cannabis into four stages. The first is the "buzz," a brief period of initial responding, where the main effects are lightheadedness or slight dizziness, in addition to possible tingling sensations in the extremities or other parts of the body. The "high" is characterized by feelings of euphoria and exhilaration characterized by mild psychedelia, as well as a sense of disinhibition. If the individual has taken a sufficiently large dose of cannabis, the level of intoxication progresses to the stage of being “stoned,” and the user may feel calm, relaxed, and possibly in a dreamlike state. Sensory reactions may include the feeling of floating, enhanced visual and auditory perception, visual illusions, or the perception of the slowing of time passage, which are somewhat psychedelic in nature.
There exist two primary CNS cannabinoid receptors, on which marijuana and the cannabinoids act. Both the CB1 receptor and CB2 receptor are found in the brain. The CB2 receptor is also found in the immune system. CB1 is expressed at high densities in the basal ganglia, cerebellum, hippocampus, and cerebral cortex. Receptor activation can inhibit cAMP formation, inhibit voltage-sensitive calcium ion channels, and activate potassium ion channels. Many CB1 receptors are located on axon terminals, where they act to inhibit the release of various neurotransmitters. In combination, these drug actions work to alter various functions of the central nervous system including the motor system, memory, and various cognitive processes.
Opiates.
The opiate drugs, which include drugs like heroin, morphine, and oxycodone, belong to the class of narcotic analgesics, which reduce pain without producing unconsciousness, but do produce a sense of relaxation and sleep, and at high doses, may result in coma and death. The ability of opiates (both endogenous and exogenous) to relieve pain depends on a complex set of neuronal pathways at the spinal cord level, as well as various locations above the spinal cord. Small endorphin neurons in the spinal cord act on receptors to decrease the conduction of pain signals from the spinal cord to higher brain centers. Descending neurons originating in the periaqueductal gray give rise to two pathways that further block pain signals in the spinal cord. The pathways begin in the locus coeruleus (noradrenaline) and the nucleus of raphe (serotonin). Similar to other abused substances, opiate drugs increase dopamine release in the nucleus accumbens. Opiates are among the most physically addictive of all psychoactive drugs, and can lead to painful withdrawal symptoms if discontinued abruptly after regular use.
Stimulants.
Cocaine is one of the more common stimulants, and is a complex drug that interacts with various neurotransmitter systems. It commonly cause heightened alertness, increased confidence, feelings of exhilaration, reduced fatigue, and a generalized sense of well-being. The effects of cocaine are similar to those of the amphetamines, though cocaine tends to have a shorter duration of effect. In high doses and/or with prolonged use, cocaine can result in a number of negative effects as well, including irritability, anxiety, exhaustion, total insomnia, and even psychotic symptomatology. Most of the behavioral and physiological actions of cocaine can be explained by its ability to block the reuptake of the two catecholamines, dopamine and norepinephrine, as well as serotonin. Cocaine binds to transporters that normally clear these transmitters from the synaptic cleft, inhibiting their function. This leads to increased levels of neurotransmitter in the cleft and transmission at the synapses. Based on in-vitro studies using rat brain tissue, cocaine binds most strongly to the serotonin transporter, followed by the dopamine transporter, and then the norepinephrine transporter.
Amphetamines tend to cause the same behavioral and subjective effects of cocaine. Various forms of amphetamine are commonly used to treat the symptoms of attention deficit hyperactivity disorder (ADHD) and narcolepsy, or are used recreationally. Amphetamine and methamphetamine are indirect agonists of the catecholaminergic systems. They block catecholamine reuptake, in addition to releasing catecholamines from nerve terminals. There is evidence that dopamine receptors play a central role in the behavioral responses of animals to cocaine, amphetamines, and other psychostimulant drugs. One action causes the dopamine molecules to be released from inside the vesicles into the cytoplasm of the nerve terminal, which are then transported outside by the mesolimbic dopamine pathway to the nucleus accumbens. This plays a key role in the rewarding and reinforcing effects of cocaine and amphetamine in animals, and is the primary mechanism for amphetamine dependence.
Psychopharmacological research.
In psychopharmacology, researchers are interested in any substance that crosses the blood–brain barrier and thus has an effect on behavior, mood or cognition. Drugs are researched for their physiochemical properties, physical side effects, and psychological side effects. Researchers in psychopharmacology study a variety of different psychoactive substances that include alcohol, cannabinoids, club drugs, psychedelics, opiates, nicotine, caffeine, psychomotor stimulants, inhalants, and anabolic-androgenic steroids. They also study drugs used in the treatment of affective and anxiety disorders, as well as schizophrenia.
Clinical studies are often very specific, typically beginning with animal testing, and ending with human testing. In the human testing phase, there is often a group of subjects, one group is given a placebo, and the other is administered a carefully measured therapeutic dose of the drug in question. After all of the testing is completed, the drug is proposed to the concerned regulatory authority (e.g. the U.S. FDA), and is either commercially introduced to the public via prescription, or deemed safe enough for over the counter sale.
Though particular drugs are prescribed for specific symptoms or syndromes, they are usually not specific to the treatment of any single mental disorder. Because of their ability to modify the behavior of even the most disturbed patients, the antipsychotic, antianxiety, and antidepressant agents have greatly affected the management of the hospitalized mentally ill, enabling hospital staff to devote more of their attention to therapeutic efforts and enabling many patients to lead relatively normal lives outside of the hospital.
A somewhat controversial application of psychopharmacology is "cosmetic psychiatry": persons who do not meet criteria for any psychiatric disorder are nevertheless prescribed psychotropic medication. The antidepressant Wellbutrin is then prescribed to increase perceived energy levels and assertiveness while diminishing the need for sleep. The antihypertensive compound Inderal is sometimes chosen to eliminate the discomfort of day-to-day "normal" anxiety . Prozac in nondepressed people can produce a feeling of generalized well-being. Mirapex, a treatment for restless leg syndrome, can dramatically increase libido in women. These and other off-label life-style applications of medications are not uncommon. Although occasionally reported in the medical literature no guidelines for such usage have been developed. There is also a potential for the misuse of prescription psychoactive drugs by elderly persons, who may have multiple drug prescriptions.

</doc>
<doc id="45622" url="http://en.wikipedia.org/wiki?curid=45622" title="Iranian Green Movement">
Iranian Green Movement

The Iranian Green Movement refers to a political movement that arose after the 2009 Iranian presidential election, in which protesters demanded the removal of Mahmoud Ahmadinejad from office. Green was initially used as the symbol of Mir Hossein Mousavi's campaign, but after the election it became the symbol of unity and hope for those asking for annulment of what they regarded as a fraudulent election. Mir Hossein Mousavi and Mehdi Karroubi are recognized as political leaders of the Green Movement. Hossein-Ali Montazeri was also mentioned as spiritual leader of the movement.
The Green Movement protests were a major event in Iran's modern political history and observers claimed that protests were the largest since the Iranian Revolution of 1978-1979.
Outcome of 2009-2010 Iranian election protests.
The election was held on 12 June 2009. The official results showed Ahmadinejad winning by a landslide, though Mousavi and others believed the results were fraudulent. They suggested that the Interior Minister, Sadegh Mahsouli, an ally of Ahmadinejad, had interfered with the election and distorted the votes to keep Ahmadinejad in power. Mousavi then claimed victory, and called for his supporters to celebrate it which led to the 2009–2010 Iranian election protests.
"Previously, he was revolutionary, because everyone inside the system was a revolutionary. But now he's a reformer. Now he knows Gandhi – before he knew only Che Guevara. If we gain power through aggression we would have to keep it through aggression. That is why we're having a green revolution, defined by peace and democracy."— Mohsen Makhmalbaf, 19 June 2009 
Protests.
Clashes broke out between police and groups protesting the election results from early morning on Saturday onward. Initially, the protests were largely peaceful. However, as time passed, they became increasingly violent. Some protesters began to get violent after the results of the election were announced. Angry crowds in Tehran broke into shops, tore down signs, and smashed windows. Civil unrest took place as protesters set fire to tyres outside the Interior Ministry building and others formed a human chain of around 300 people to close off a major Tehran street.
The demonstrations grew bigger and more heated than the 1999 student protests. Al Jazeera English described the 13 June situation as the "biggest unrest since the 1979 revolution." It also reported that protests seemed spontaneous without any formal organization. Two hundred people protested outside Iran's embassy in London on 13 June. Ynet has stated that "tens of thousands" protested on 13 June. Demonstrators are chanting phrases such as "Down with the dictator", "Death to the dictator", and "Give us our votes back". Mousavi has urged for calm and asked that his supporters refrain from acts of violence.
Ynet reported on 14 June that two people had died in the rioting so far. That day, protests had been organized in front of the Iranian embassies in Turkey, Dubai, Paris, Berlin, London, Rome, Sydney, Vienna and The Hague. In response to the reformist protests, tens of thousands of people rallied in Tehran on 14 June to support the victory of Ahmadinejad.
On 15 June, Mousavi rallied, with anywhere from hundreds of thousands to three million, of his supporters in Tehran, despite being warned by state officials that any such rally would be illegal. The demonstration, the largest in the Islamic Republic of Iran's 30-year history, was Mousavi's first public appearance after the election. Protests focused around Azadi Tower, around which lines of people stretched for more than nine kilometers met. Gunshots were reported to have been fired at the rally, where Mousavi had spoken to his supporters saying, "The vote of the people is more important than Mousavi or any other person." All three opposition candidates appeared.
Competing rallies for Mousavi and for Ahmadinejad took place on 16 June. The pro-Ahmadinejad protesters, chanting the phrases "Death to America!" and "Death to Israel!", outnumbered their opponents, but they did not match the numbers of opponents who had protested the day before. Reports from the state media and elsewhere stated on 16 June that seven people have died in all of the protests so far. However, Times Online quoted a Rasoul Akram Hospital nurse that day who asserted that 28 people have suffered from "bullet wounds" and eight have died so far. Over half a million reformist Iranians marched silently from Haft-e-Tir Square to Vali Asr Square on 17 June. Huffington Post reported that day that 32 people had died protesting so far.
On 14 February 2011, the largest Green demonstrations in Iran in more than a year broke out. In response pro-government MPs called for the death of opposition leaders Mir Hussein Moussavi and Mehdi Karroubi.
Government actions.
Arrests.
On the weekend of 13 and 14 June, in a series of raids across Tehran, the government arrested over 170 people, according to police officials. Among them were prominent reformist politicians, including Mojahedin of the Islamic Revolution Organization (MIRO) founder Behzad Nabavi, Islamic Iran Participation Front (IIPF) leader Mohsen Mirdamadi, and former president Mohammad Khatami's brother Mohammad-Reza Khatami, who was later released. Also arrested were Mostafa Tajzadeh and Mohsen Aminzadeh, whom the IRNA said were involved in orchestrating protests on 13 June. Anonymous sources said that the police stormed the headquarters of the IIPF and arrested a number of people. Iranian journalist Mashallah Shamsolvaezin claimed that presidential candidate Mir-Hossein Mousavi was put under house arrest, although officials denied this. An estimated 200 people were detained after clashes with students at Tehran university, although many were later released.
Acting Police Chief Ahmad-Reza Radan stated via the state press service on the 14th that "in the interrogation of related rebels, we intend to find the link between the plotters and foreign media". A judiciary spokesman said they had not been arrested but that they were summoned, "warned not to increase tension," and later released. Intelligence minister Gholam Hossein Mohseni-Ejehei linked some arrests to terrorism supported from outside Iran, stating that "more than 20 explosive consignments were discovered". Others, he said, were "counter-revolutionary groups" who had "penetrated election headquarters" of the election candidates.
On 16 June, Reuters reported that former vice-president Mohammad-Ali Abtahi and former presidential advisor Saeed Hajjarian had been arrested. Human rights lawyer Abdolfattah Soltani, who had been demanding a recount of all votes, was also arrested on the Tuesday according to Shirin Ebadi, who said that security officials had posed as clients. Over 100 students were arrested after security forces fired tear gas at protesters at Shiraz University on the same day. Reporters Without Borders reported that 5 of 11 arrested journalists were still detention as of 16 June, and that a further 10 journalists were unaccounted for and may have been arrested.
On 17 June, former foreign minister and secretary-general of the Freedom Movement of Iran, Ebrahim Yazdi, was arrested while undergoing tests at Pars hospital in Tehran. He was held overnight in Evin Prison before being released and returning to hospital, where according to Human Rights Watch he remained under guard. In Tabriz, other Freedom Movement activists and eight members of the IIPF were arrested, with reports of at least 100 civic figures' arrests. The total number of arrests across Iran since the election was reported as 500.
Aaron Rhodes, a spokesman for the International Campaign for Human Rights in Iran, stated that "Iranian intelligence and security forces are using the public protests to engage in what appears to be a major purge of reform-oriented individuals whose situations in detention could be life-threatening". In Isfahan Province, prosecutor-general Mohammadreza Habibi warned that dissidents could face execution under Islamic law.
The Green Path of Hope.
Mousavi and other reformist leaders are now working in peaceful and legal methods to widen the influence of their reforms. They have set up a new coalition named "The Green Path of Hope". Iranian political parties and movements need to be authorized by the Interior Ministry. Mousavi neither recognizes the current government as legitimate nor is likely to receive permission; so, the movement was named a "path" in order to bypass this law.
The Green Path of Hope claims it seeks to continue protests against Ahmadinejad's presidency following lawful and peaceful methods, and the full execution of the constitution, as Mousavi says:
"You can't follow some parts of the constitution and throw the rest into a bin."—
According to organization officials, the movement functions encompasses numerous political parties, NGO's and social networks. Mousavi emphasized that existent, autonomous social networks in the community are part of this movement:
"During the election, our mottos supported and remained in the framework of the constitution; today we are devoted to those slogans. We believe that if the people's demands were treated fairly, instead of being distorted by the media and linked to foreigners, and the government promoted truth by fair criticism, our mottos could satisfy the public."
The "Green Path" has six main members of the central council, who are connected to reformist parties, NGOs, and social networks. The main body will be ordinary protesters. The strategy is to connect existent pressures and issues in society in a social network, and to therefore lead protests in a lawful manner.
Where is my vote?
"Where is my vote?" (Persian: رأی من کجاست؟‎ "raye-man kojast?") is a motto which was used during the protests. The Iranian government, headed by President Mahmoud Ahmadinejad, released results claiming a two-thirds majority. However, Mousavi had already claimed victory before the vote count was done and supporters of Mousavi and Karroubi accused the government of rigging the votes.
In the aftermath of the election, protests were widened and several massive protests were held around the country by the people. The government arrested a large number of the protesters and several were killed by the police and governmental militia forces.
Although the Iranian government prohibited any form of gathering by opposition-supporters in Tehran and across the country, significantly slowed down internet access and censored any form of media agreeing with the opposition, hundreds of thousands of Iranians chanted this motto, defying the law and challenging the Islamic Republic.
Legal ways.
Mousavi and the reformists later attempted to implement reforms though legal processes and set up a new coalition, named The Green Path of Hope, to support this.
Iran national football team.
During the Iran's final game of the 2010 FIFA World Cup qualifiers against South Korea in Seoul on 17 June 2009, seven members of the team, Javad Nekounam, Ali Karimi, Hossein Kaebi, Masoud Shojaei, Mohammad Nosrati, Vahid Hashemian, and captain Mehdi Mahdavikia wore green wristbands in support of the Iranian Green Movement during the 2009 Iranian election protests. Initial reports were that all seven players were banned for life by the Iranian Football Federation, however, state-run media claimed that all seven had "retired". On 24 June 2009, FIFA wrote to Iran's Football Federation asking for clarification on the situation. The Iranian Football Federation replied that no disciplinary action has been taken against any player. As of 2014 FIFA World Cup qualification, several of the above players have played again for the national team, notably Javad Nekounam, Masoud Shojaei, Mehdi Mahdavikia, and Ali Karimi.

</doc>
<doc id="45623" url="http://en.wikipedia.org/wiki?curid=45623" title="Informer">
Informer

 
Informer or nark may refer to:
In films and television:

</doc>
<doc id="45630" url="http://en.wikipedia.org/wiki?curid=45630" title="Peter III">
Peter III

Peter III may refer to:

</doc>
<doc id="45631" url="http://en.wikipedia.org/wiki?curid=45631" title="Telephony Application Programming Interface">
Telephony Application Programming Interface

The Telephony Application Programming Interface (TAPI) is a Microsoft Windows API, which provides computer telephony integration and enables PCs running Microsoft Windows to use telephone services. Different versions of TAPI are available on different versions of Windows. TAPI allows applications to control telephony functions between a computer and telephone network for data, fax, and voice calls. It includes basic functions, such as dialing, answering, and hanging up a call. It also supports supplementary functions, such as hold, transfer, conference, and call park found in PBX, ISDN, and other telephone systems. 
TAPI is used primarily to control either modems or, more recently, to control business telephone system (PBX) handsets. When controlling a PBX handset, the driver is provided by the manufacturer of the telephone system. Some manufacturers provide drivers that allow the control of multiple handsets. This is traditionally called "third-party control". Other manufacturers provide drivers that allow the control of a single handset. This is called "first-party control". Third-party drivers are designed to allow applications to see and/or control multiple extensions at the same time. Some telephone systems only permit one third-party connection at a time. First-party drivers are designed to allow applications to monitor and/or control one extension at a time. Telephone systems naturally permit many of these connections simultaneously. Modem connections are by nature first-party.
TAPI can also be used to control voice-enabled telephony devices, including voice modems and dedicated hardware such as Dialogic cards.
History.
TAPI was introduced in 1993 as the result of joint development by Microsoft and Intel. The first publicly available version of TAPI was version 1.3, which was released as a patch on top of Microsoft Windows 3.1. Version 1.3 drivers were 16-bit only. Version 1.3 is no longer supported, although some MSDN development library CDs still contain the files and patches.
With Microsoft Windows 95, TAPI was integrated into the operating system. The first version on Windows 95 was TAPI 1.4. TAPI 1.4 had support for 32-bit applications.
The TAPI standard supports both connections from individual computers and LAN connections serving any number of computers.
TAPI 2.0 was introduced with Windows NT 4.0. Version 2.0 was the first version on the Windows NT platform. It made a significant step forward by supporting ACD and PBX-specific functionality.
In 1997, Microsoft released TAPI version 2.1. This version of TAPI was available as a downloadable update and was the first version to be supported on both the Microsoft Windows 95 and Windows NT/2000 platforms.
TAPI 3.0 was released in 1999 together with Windows 2000. This version enables IP telephony (VoIP) by providing simple and generic methods for making connections between two (using H.323) or more (using IP Multicast) computers and now also offers the ability to access any media streams involved in the connection.
Windows XP included both TAPI 3.1 and TAPI 2.2. TAPI 3.1 supports the Microsoft Component Object Model and provides a set of COM objects to application programmers. This version uses File Terminals which allow applications to record streaming data to a file and play this recorded data back to a stream. A USB Phone TSP (Telephony Service Provider) was also included which allows an application to control a USB phone and use it as a streaming endpoint. TAPI 3.0 or TAPI 3.1 are not available on operating systems earlier than Windows 2000 and Windows XP respectively.
The Telephony Server Application Programming Interface (TSAPI) is a similar standard developed by Novell for NetWare servers.
TAPI 2.x vs TAPI 3.x.
It is a common misconception that TAPI 3.0 (or TAPI 3.1) replaces TAPI 2.x.
TAPI 2.x and earlier versions were written in C; the API uses pointers to structures. Consequently, TAPI 2.x is easy to access from C or C++ applications, but it can be awkward to use from many other programming languages. 
TAPI 3.x was designed with a Component Object Model (COM) interface. This was done with the intent of making it accessible to higher level applications such as developed in VB or other environments that provide easy access to COM but don't deal with C-style pointers. 
TAPI 3.x has a slightly different set of functionality than TAPI 2.x. The addition of integrated media control was the most significant addition. But TAPI 3.x doesn't include all functionality that TAPI 2.x does, like support for the Phone class.
One very notable issue with TAPI 3.x is the lack of support for managed code (.NET environment). As documented in Microsoft KB Article , Microsoft currently has no plans to support TAPI 3.x directly from .Net programming languages. However, Mark Smith has provided a Managed C++ library called ITAPI3 and other developers such as Mondago provide .Net libraries to work indirectly with TAPI enabled PBXs.
One often overlooked reason an application developer might choose between TAPI 2.x and TAPI 3.x should be the hardware vendors recommendation. Even though TAPI provides an abstract model of phone lines, telephony applications are still heavily impacted by the specific behavior of the underlying hardware. Troubleshooting behavior issues usually requires both software and hardware vendors to collaborate. Because there is almost a 1:1 relationship between the TAPI Service Provider (TSP) interface and the TAPI 2.x interface, collaboration is often easier if the application is designed using TAPI 2.x. Experience with TAPI 3.x varies significantly between hardware vendors.
TAPI compliant hardware.
Telephony hardware that supports TAPI includes most voice modems and some telephony cards such as Dialogic cards.
The following telephone systems provide Tapi drivers. Often these are only available for 32 bit operating systems. Many of these drivers are licensed and thus incur a charge to use. In other cases, alternative drivers are available for separate purchase from iQ NetSolutions, C4B Com For Business, Estos and Mondago:

</doc>
<doc id="45633" url="http://en.wikipedia.org/wiki?curid=45633" title="Economic history">
Economic history

Economic history is the study of economies or economic phenomena of the past. Analysis in economic history is undertaken using a combination of historical methods, statistical methods and the application of economic theory to historical situations and institutions. The topic includes and business history and overlaps with areas of social history such as demographic and labor history. The quantitative – in this case, econometric – study of economic history is also known as cliometrics.
Development as a separate field.
Treating economic history as a discrete academic discipline has been a contentious issue for many years. Academics at the London School of Economics and the University of Cambridge had numerous disputes over the separation of economics and economic history in the interwar era. Cambridge economists believed that pure economics involved a component of economic history and that the two were inseparably entangled. Those at the LSE believed that economic history warranted its own courses, research agenda and academic chair separated from mainstream economics.
In the initial period of the subject's development, the LSE position of separating economic history from economics won out. Many universities in the UK developed independent programmes in economic history rooted in the LSE model. Indeed, the Economic History Society had its inauguration at LSE in 1926 and the University of Cambridge eventually established its own economic history programme. However, the past twenty years have witnessed the widespread closure of these separate programmes in the UK and the integration of the discipline into either history or economics departments. Only the LSE, the University of Glasgow and the University of Edinburgh retain separate economic history departments and stand-alone undergraduate and graduate programmes in economic history. The LSE, Glasgow and the University of Oxford together train the vast majority of economic historians coming through the British higher education system.
In the US, economic history has for a long time been regarded as a form of applied economics. As a consequence, there are no specialist economic history graduate programs at any mainstream university anywhere in the country. Instead economic history is taught as a special field component of regular economics or history PhD programs in some places, including at University of California, Berkeley, Harvard University, Northwestern University and Yale University.
In recent decades economic historians, following Douglass North, have tended to move away from narrowly quantitative studies toward institutional, social, and cultural history affecting the evolution of economies. However, this trend has been criticized, most forcefully by Francesco Boldizzoni, as a form of economic imperialism "extending the neoclassical explanatory model to the realm of social relations." Conversely, economists in other specializations have started to write on topics concerning economic history.
History of socialism.
The history of socialism has its origins in the French Revolution of 1789 and the changes brought about by the Industrial Revolution, although it has precedents in earlier movements and ideas. The Communist Manifesto was written by Karl Marx and Friedrich Engels in 1848 just before the Revolutions of 1848 swept Europe, expressing what they termed 'scientific socialism'. In the last third of the 19th century in Europe social democratic parties arose in Europe drawing mainly from Marxism. The Australian Labor Party was the world's first elected socialist party when the party won the 1899 Queensland state election.
History of capitalism.
Since 2000, the new field of "History of Capitalism" has appeared. It includes topics such as insurance, banking and regulation, the political dimension, and the impact on the middle classes, the poor and women and minorities. The field utilizes the existing research of business history, but has sought to make it more relevant to the concerns of history departments in the United States, including by having limited or no discussion of individual business enterprises.
Relationship between economics and economic history.
Have a very healthy respect for the study of economic history, because that's the raw material out of which any of your conjectures or testings will come.
- Paul Samuelson (2009)
Yale University economist Irving Fisher wrote in 1933 on the relationship between economics and economic history in his "Debt-Deflation Theory of Great Depressions" ("Econometrica", Vol. 1, No. 4: 337–338):
The study of dis-equilibrium may proceed in either of two ways. We may take as our unit for study an actual historical case of great dis-equilibrium, such as, say, the panic of 1873; or we may take as our unit for study any constituent tendency, such as, say, deflation, and discover its general laws, relations to, and combinations with, other tendencies. The former study revolves around events, or facts; the latter, around tendencies. The former is primarily economic history; the latter is primarily economic science. Both sorts of studies are proper and important. Each helps the other. The panic of 1873 can only be understood in light of the various tendencies involved—deflation and other; and deflation can only be understood in the light of various historical manifestations—1873 and other.
There is a school of thought among economic historians that splits economic history—the study of how economic phenomena evolved in the past—from historical economics—testing the generality of economic theory using historical episodes. US economic historian Charles P. Kindleberger explained this position in his 1990 book "Historical Economics: Art or Science?".
The new economic history, also known as cliometrics, refers to the systematic use of economic theory and/or econometric techniques to the study of economic history. The term cliometrics was originally coined by Jonathan R. T. Hughes and Stanley Reiter in 1960 and refers to Clio, who was the muse of history and heroic poetry in Greek mythology. Cliometricians argue their approach is necessary because the application of theory is crucial in writing solid economic history, while historians generally oppose this view warning against the risk of generating anachronisms. Early cliometrics was a type of counterfactual history. However, counterfactualism is no longer its distinctive feature. Some have argued that cliometrics had its heyday in the 1960s and 1970s and that it is now neglected by economists and historians.

</doc>
<doc id="45634" url="http://en.wikipedia.org/wiki?curid=45634" title="Thread safety">
Thread safety

Thread safety is a computer programming concept applicable in the context of multi-threaded programs. A piece of code is thread-safe if it only manipulates shared data structures in a manner that guarantees safe execution by multiple threads at the same time. There are various strategies for making thread-safe data structures.
A program may execute code in several threads simultaneously in a shared address space where each of those threads has access to virtually all of the memory of every other thread. Thread safety is a property that allows code to run in multi-threaded environments by re-establishing some of the correspondences between the actual flow of control and the text of the program, by means of synchronization.
Levels of thread safety.
Software libraries can provide certain thread-safety guarantees. For example, concurrent reads might be guaranteed to be thread-safe, but concurrent writes might not be. Whether or not a program using such a library is thread-safe depends on whether it uses the library in a manner consistent with those guarantees.
Different vendors use slightly different terminology for thread-safety:
Thread safety guarantees usually also include design steps to prevent or limit the risk of different forms of deadlocks, as well as optimizations to maximize concurrent performance. However, deadlock-free guarantees can not always be given, since deadlocks can be caused by callbacks and violation of architectural layering independent of the library itself.
Implementation approaches.
Below we discuss two approaches for avoiding race conditions to achieve thread safety. 
The first class of approaches focuses on avoiding shared state, and includes:
The second class of approaches are synchronization-related, and are used in situations where shared state cannot be avoided:
Examples.
In the following piece of Java code, the function is thread-safe:
In the following piece of C code, the function is thread-safe, but not reentrant:
In the above, codice_1 can be called by different threads without any problem since a mutex is used to synchronize all access to the shared codice_2 variable. But if the function is used in a reentrant interrupt handler and a second interrupt arises inside the function, the second routine will hang forever. As interrupt servicing can disable other interrupts, the whole system could suffer.
The same function can be implemented to be both thread-safe and reentrant using the lock-free atomics in C++11:

</doc>
<doc id="45635" url="http://en.wikipedia.org/wiki?curid=45635" title="Top-down and bottom-up design">
Top-down and bottom-up design

Top-down and bottom-up are both strategies of information processing and knowledge ordering, used in a variety of fields including software, humanistic and scientific theories (see systemics), and management and organization. In practice, they can be seen as a style of thinking and teaching.
A top-down approach (also known as stepwise design and in some cases used as a synonym of "decomposition") is essentially the breaking down of a system to gain insight into its compositional sub-systems. In a top-down approach an overview of the system is formulated, specifying but not detailing any first-level subsystems. Each subsystem is then refined in yet greater detail, sometimes in many additional subsystem levels, until the entire specification is reduced to base elements. A top-down model is often specified with the assistance of "black boxes", these make it easier to manipulate. However, black boxes may fail to elucidate elementary mechanisms or be detailed enough to realistically validate the model. Top down approach starts with the big picture. It breaks down from there into smaller segments.
A bottom-up approach is the piecing together of systems to give rise to more complex systems, thus making the original systems sub-systems of the emergent system. Bottom-up processing is a type of information processing based on incoming data from the environment to form a perception. From a Cognitive Psychology perspective, information enters the eyes in one direction (sensory input, or the "bottom"), and is then turned into an image by the brain that can be interpreted and recognized as a perception (output that is "built up" from processing to final cognition). In a bottom-up approach the individual base elements of the system are first specified in great detail. These elements are then linked together to form larger subsystems, which then in turn are linked, sometimes in many levels, until a complete top-level system is formed. This strategy often resembles a "seed" model, whereby the beginnings are small but eventually grow in complexity and completeness. However, "organic strategies" may result in a tangle of elements and subsystems, developed in isolation and subject to local optimization as opposed to meeting a global purpose.
Product design and development.
During the design and development of new products, designers and engineers rely on both a bottom-up and top-down approach. The bottom-up approach is being utilized when off-the-shelf or existing components are selected and integrated into the product. An example would include selecting a particular fastener, such as a bolt, and designing the receiving components such that the fastener will fit properly. In a top-down approach, a custom fastener would be designed such that it would fit properly in the receiving components.
For perspective, for a product with more restrictive requirements (such as weight, geometry, safety, environment, etc.), such as a space-suit, a more top-down approach is taken and almost everything is custom designed. However, when it's more important to minimize cost and increase component availability, such as with manufacturing equipment, a more bottom-up approach would be taken, and as many off-the-shelf components (bolts, gears, bearings, etc.) would be selected as possible. In the latter case, the receiving housings would be designed around the selected components.
Computer science.
Software development.
In the software development process, the top-down and bottom-up approaches play a key role.
Top-down approaches emphasize planning and a complete understanding of the system. It is inherent that no coding can begin until a sufficient level of detail has been reached in the design of at least some part of the system. Top-down approaches are implemented by attaching the stubs in place of the module. This, however, delays testing of the ultimate functional units of a system until significant design is complete. Bottom-up emphasizes coding and early testing, which can begin as soon as the first module has been specified. This approach, however, runs the risk that modules may be coded without having a clear idea of how they link to other parts of the system, and that such linking may not be as easy as first thought. Re-usability of code is one of the main benefits of the bottom-up approach.
Top-down design was promoted in the 1970s by IBM researchers Harlan Mills and Niklaus Wirth. Mills developed structured programming concepts for practical use and tested them in a 1969 project to automate the "New York Times" morgue index. The engineering and management success of this project led to the spread of the top-down approach through IBM and the rest of the computer industry. Among other achievements, Niklaus Wirth, the developer of Pascal programming language, wrote the influential paper "Program Development by Stepwise Refinement". Since Niklaus Wirth went on to develop languages such as Modula and Oberon (where one could define a module before knowing about the entire program specification), one can infer that top down programming was not strictly what he promoted. Top-down methods were favored in software engineering until the late 1980s, and object-oriented programming assisted in demonstrating the idea that both aspects of top-down and bottom-up programming could be utilized.
Modern software design approaches usually combine both top-down and bottom-up approaches. Although an understanding of the complete system is usually considered necessary for good design, leading theoretically to a top-down approach, most software projects attempt to make use of existing code to some degree. Pre-existing modules give designs a bottom-up flavor. Some design approaches also use an approach where a partially functional system is designed and coded to completion, and this system is then expanded to fulfill all the requirements for the project
Programming.
Top-down is a programming style, the mainstay of traditional procedural languages, in which design begins by specifying complex pieces and then dividing them into successively smaller pieces. The technique for writing a program using top–down methods is to write a main procedure that names all the major functions it will need. Later, the programming team looks at the requirements of each of those functions and the process is repeated. These compartmentalized sub-routines eventually will perform actions so simple they can be easily and concisely coded. When all the various sub-routines have been coded the program is ready for testing. By defining how the application comes together at a high level, lower level work can be self-contained. By defining how the lower level abstractions are expected to integrate into higher level ones, interfaces become clearly defined.
In a bottom-up approach, the individual base elements of the system are first specified in great detail. These elements are then linked together to form larger subsystems, which then in turn are linked, sometimes in many levels, until a complete top-level system is formed. This strategy often resembles a "seed" model, whereby the beginnings are small, but eventually grow in complexity and completeness. Object-oriented programming (OOP) is a paradigm that uses "objects" to design applications and computer programs. In mechanical engineering with software programs such as Pro/ENGINEER, Solidworks, and Autodesk Inventor users can design products as pieces not part of the whole and later add those pieces together to form assemblies like building with LEGO. Engineers call this piece part design.
This bottom-up approach has one weakness. Good intuition is necessary to decide the functionality that is to be provided by the module. If a system is to be built from existing system, this approach is more suitable as it starts from some existing modules.
Parsing.
Parsing is the process of analyzing an input sequence (such as that read from a file or a keyboard) in order to determine its grammatical structure. This method is used in the analysis of both natural languages and computer languages, as in a compiler.
Bottom-up parsing is a strategy for analyzing unknown data relationships that attempts to identify the most fundamental units first, and then to infer higher-order structures from them. Top-down parsers, on the other hand, hypothesize general parse tree structures and then consider whether the known fundamental structures are compatible with the hypothesis. See Top-down parsing and Bottom-up parsing.
Nanotechnology.
Top-down and bottom-up are two approaches for the manufacture of products. These terms were first applied to the field of nanotechnology by the Foresight Institute in 1989 in order to distinguish between molecular manufacturing (to mass-produce large atomically precise objects) and conventional manufacturing (which can mass-produce large objects that are not atomically precise). Bottom-up approaches seek to have smaller (usually molecular) components built up into more complex assemblies, while top-down approaches seek to create nanoscale devices by using larger, externally controlled ones to direct their assembly.
The top-down approach often uses the traditional workshop or microfabrication methods where externally controlled tools are used to cut, mill, and shape materials into the desired shape and order. Micropatterning techniques, such as photolithography and inkjet printing belong to this category.
Bottom-up approaches, in contrast, use the chemical properties of single molecules to cause single-molecule components to (a) self-organize or self-assemble into some useful conformation, or (b) rely on positional assembly. These approaches utilize the concepts of molecular self-assembly and/or molecular recognition. See also Supramolecular chemistry. Such bottom-up approaches should, broadly speaking, be able to produce devices in parallel and much cheaper than top-down methods, but could potentially be overwhelmed as the size and complexity of the desired assembly increases.
Neuroscience and psychology.
These terms are also employed in neuroscience, cognitive neuroscience and cognitive psychology to discuss the flow of information in processing. Typically sensory input is considered "down", and higher cognitive processes, which have more information from other sources, are considered "up". A bottom-up process is characterized by an absence of higher level direction in sensory processing, whereas a top-down process is characterized by a high level of direction of sensory processing by more cognition, such as goals or targets (Beiderman, 19).
According to Psychology notes written by Dr. Charles Ramskov, a Psychology professor at De Anza College, Rock, Neiser, and Gregory claim that top-down approach involves perception that is an active and constructive process. Additionally, it is an approach not directly given by stimulus input, but is the result of stimulus, internal hypotheses, and expectation interactions. According to Theoretical Synthesis, "when a stimulus is presented short and clarity is uncertain that gives a vague stimulus, perception becomes a top-down approach."
Conversely, Psychology defines bottom-up processing as an approach wherein there is a progression from the individual elements to the whole. According to Ramskov, one proponent of bottom-up approach, Gibson, claims that it is a process that includes visual perception that needs information available from proximal stimulus produced by the distal stimulus. Theoretical Synthesis also claims that bottom-up processing occurs "when a stimulus is presented long and clearly enough."
Cognitively speaking, certain cognitive processes, such as fast reactions or quick visual identification, are considered bottom-up processes because they rely primarily on sensory information, whereas processes such as motor control and directed attention are considered top-down because they are goal directed. Neurologically speaking, some areas of the brain, such as area V1 mostly have bottom-up connections. Other areas, such as the fusiform gyrus have inputs from higher brain areas and are considered to have top-down influence.
The study of visual attention provides an example. If your attention is drawn to a flower in a field, it may be because the color or shape of the flower are visually salient. The information that caused you to attend to the flower came to you in a bottom-up fashion—your attention was not contingent upon knowledge of the flower; the outside stimulus was sufficient on its own. Contrast this situation with one in which you are looking for a flower. You have a representation of what you are looking for. When you see the object you are looking for, it is salient. This is an example of the use of top-down information.
In cognitive terms, two thinking approaches are distinguished. "Top-down" (or "big chunk") is stereotypically the visionary, or the person who sees the larger picture and overview. Such people focus on the big picture and from that derive the details to support it. "Bottom-up" (or "small chunk") cognition is akin to focusing on the detail primarily, rather than the landscape. The expression "seeing the wood for the trees" references the two styles of cognition.
Management and organization.
In management and organizational arenas, the terms "top-down" and "bottom-up" are used to indicate how decisions are made.
A "top-down" approach is one where an executive, decision maker, or other person or body makes a decision. This approach is disseminated under their authority to lower levels in the hierarchy, who are, to a greater or lesser extent, bound by them. For example, a structure in which decisions either are approved by a manager, or approved by his or her authorized representatives based on the manager's prior guidelines, is top-down management.
A "bottom-up" approach is one that works from the grassroots—from a large number of people working together, causing a decision to arise from their joint involvement. A decision by a number of activists, students, or victims of some incident to take action is a "bottom-up" decision. Positive aspects of top-down approaches include their efficiency and superb overview of higher levels. Also, external effects can be internalized. On the negative side, if reforms are perceived to be imposed ‘from above’, it can be difficult for lower levels to accept them (e.g. Bresser Pereira, Maravall, and Przeworski 1993). Evidence suggests this to be true regardless of the content of reforms (e.g. Dubois 2002). A bottom-up approach allows for more experimentation and a better feeling for what is needed at the bottom.
State organization.
Both approaches can be found in the organization of states, this involving political decisions.
In bottom-up organized organizations, e.g. ministries and their subordinate entities, decisions are prepared by experts in their fields, which define, out of their expertise, the policy they deem necessary. If they cannot agree, even on a compromise, they "escalate" the problem to the next higher hierarchy level, where a decision would be sought. Finally, the highest common principal might have to take the decision. Information is in the debt of the inferior to the superior, which means that the inferior owes information to the superior. In the effect, as soon as inferiors agree, the head of the organization only provides his or her “face″ for the decision which their inferiors have agreed upon.
Among several countries, the German political system provides one of the purest forms of a bottom-up approach. The German Federal Act on the Public Service provides that any inferior has to consult and support any superiors, that he or she – only – has to follow “general guidelines" of the superiors, and that he or she would have to be fully responsible for any own act in office, and would have to follow a specific, formal complaint procedure if in doubt of the legality of an order. Frequently, German politicians had to leave office on the allegation that they took wrong decisions because of their resistance to inferior experts' opinions (this commonly being called to be “beratungsresistent", or resistant to consultation, in German). The historical foundation of this approach lies with the fact that, in the 19th century, many politicians used to be noblemen without appropriate education, who more and more became forced to rely on consultation of educated experts, which (in particular after the Prussian reforms of Stein and Hardenberg) enjoyed the status of financially and personally independent, indismissable, and neutral experts as "Beamte" (public servants under public law).
The experience of two dictatorships in the country and, after the end of such regimes, emerging calls for the legal responsibility of the “aidees of the aidees" ("Helfershelfer") of such regimes also furnished calls for the principle of personal responsibility of any expert for any decision made, this leading to a strengthening of the bottom-up approach, which requires maximum responsibility of the superiors. A similar approach can be found in British police laws, where entitlements of police constables are vested in the constable in person and not in the police as an administrative agency, this leading to the single constable being fully responsible for his or her own acts in office, in particular their legality.
In the opposite, the French administration is based on a top-down approach, where regular public servants enjoy no other task than simply to execute decisions made by their superiors. As those superiors also require consultation, this consultation is provided by members of a "cabinet", which is distinctive from the regular ministry staff in terms of staff and organization. Those members who are not members of the "cabinet" are not entitled to make any suggestions or to take any decisions of political dimension.
The advantage of the bottom-up approach is the level of expertise provided, combined with the motivating experience of any member of the administration to be responsible and finally the independent “engine" of progress in that field of personal responsibility. A disadvantage is the lack of democratic control and transparency, this leading, from a democratic viewpoint, to the deferment of actual power of policy-making to faceless, if even known, public servants. Even the fact that certain politicians might “provide their face" to the actual decisions of their inferiors might not mitigate this effect, but rather strong parliamentary rights of control and influence in legislative procedures (as they do exist in the example of Germany).
The advantage of the top-down principle is that political and administrative responsibilities are clearly distinguished from each other, and that responsibility for political failures can be clearly identified with the relevant office holder. Disadvantages are that the system triggers demotivation of inferiors, who know that their ideas to innovative approaches might not be welcome just because of their position, and that the decision-makers cannot make use of the full range of expertise which their inferiors will have collected.
Administrations in "dictatorships" traditionally work according to a strict top-down approach. As civil servants below the level of the political leadership are discouraged from making suggestions, they use to suffer from the lack of expertise which could be provided by the inferiors, which regularly leads to a breakdown of the system after a few decades. Modern communist states, which the People's Republic of China forms an example of, therefore prefer to define a framework of permissible, or even encouraged, criticism and self-determination by inferiors, which would not affect the major state doctrine, but allows the use of professional and expertise-driven knowledge and the use of it for the decision-making persons in office.
Public health.
Both top-down and bottom-up approaches exist in public health. There are many examples of top-down programs, often run by governments or large inter-governmental organizations (IGOs); many of these are disease-specific or issue-specific, such as HIV control or Smallpox Eradication. Examples of bottom-up programs include many small NGOs set up to improve local access to healthcare. However, a lot of programs seek to combine both approaches; for instance, guinea worm eradication, a single-disease international program currently run by the Carter Center has involved the training of many local volunteers, boosting bottom-up capacity, as have international programs for hygiene, sanitation, and access to primary health-care.
Architecture.
Often, the École des Beaux-Arts school of design is said to have primarily promoted top-down design because it taught that an architectural design should begin with a parti, a basic plan drawing of the overall project.
By contrast, the Bauhaus focused on bottom-up design. This method manifested itself in the study of translating small-scale organizational systems to a larger, more architectural scale (as with the woodpanel carving and furniture design).
Ecology.
In ecology, top-down control refers to when a top predator controls the structure or population dynamics of the ecosystem. The classic example is of kelp forest ecosystems. In such ecosystems, sea otters are a keystone predator. They prey on urchins which in turn eat kelp. When otters are removed, urchin populations grow and reduce the kelp forest creating urchin barrens. In other words, such ecosystems are not controlled by productivity of the kelp but rather a top predator.
Bottom up control in ecosystems refers to ecosystems in which the nutrient supply and productivity and type of primary producers (plants and phytoplankton) control the ecosystem structure. An example would be how plankton populations are controlled by the availability of nutrients. Plankton populations tend to be higher and more complex in areas where upwelling brings nutrients to the surface.
There are many different examples of these concepts. It is common for populations to be influenced by both types of control.

</doc>
<doc id="45636" url="http://en.wikipedia.org/wiki?curid=45636" title="Borland Turbo C">
Borland Turbo C

Turbo C is an Integrated Development Environment and compiler for the C programming language from Borland. First introduced in 1987, it was noted for its integrated development environment, small size, fast compile speed, comprehensive manuals and low price.
In May 1990, Borland replaced Turbo C with Turbo C++. In 2006, Borland reintroduced the Turbo moniker.
Early history.
In the early 1980s, Borland enjoyed considerable success with their Turbo Pascal product and it became a popular choice when developing applications for the PC. Borland followed up that success by releasing Turbo Basic, Turbo Prolog and Turbo C. 
Turbo C had the same properties as Turbo Pascal: an integrated development environment (IDE), a fast compiler, a good editor and a competitive price. Turbo C was not as successful as the Pascal-sister product. First, C was a language for professional programming and systems development rather than a school language. Turbo C competed with other professional programming tools (Microsoft C, Lattice C, Watcom C, etc.). Turbo C did, however, have advantages in speed of compiled code, large project support and price. It is developed in C.
Version history.
Version 1.0, Released on May 13, 1987, offered the first integrated edit-compile-run development environment for C on IBM PCs. The software was, like many Borland products of the time, bought from another company and branded with the "Turbo" name, in this case Wizard C by Bob Jervis (Borland's flagship product at that time, Turbo Pascal, which at this time did not have pull-down menus, would be given a facelift with version 4 released late in 1987 to make it look more like Turbo C.) It ran in 384 kB of memory. It allowed inline assembly with full access to C symbolic names and structures, supported all memory models, and offered optimizations for speed, size, constant folding, and jump elimination.
Version 1.5, in January 1988 was an incremental improvement over version 1.0. It included more sample programs, improved manuals and bug fixes. It was shipped on five 360 KB diskettes of uncompressed files, and came with sample C programs, including a stripped down spreadsheet called mcalc. This version introduced the <conio.h> header file (which provided fast, PC-specific console I/O routines). (Note: The copyright date in the startup screen is 1987, but the files in the system distribution were created in January 1988.)
Version 2.0, in 1989 was released was in late 1988, and featured the first "blue screen" version, which would be typical of all future Borland releases for MS-DOS. The American release did not have Turbo Assembler or a separate debugger. (These were sold separately as Turbo Assembler.) Turbo C, Asm, and Debugger were sold together as a suite. This seems to describe another release: Featured Turbo Debugger, Turbo Assembler, and an extensive graphics library. This version of Turbo C was also released for the Atari ST, but distributed in Germany only.
"Note on later releases": The name "Turbo C" was not used after version 2.0, because with the release of Turbo C++ 1.0 in 1990, the two products were folded into a single product. That first C++ compiler was developed under contract by a company in San Diego and was one of the first true compilers for C++ (until then, most C++ work was done with pre-compilers that generated C code). The next version was named Borland C++ to emphasize its flagship status and completely rewritten in-house, with Peter Kukol as the lead engineer. The Turbo C++ name was briefly dropped, eventually reappearing as Turbo C++ 3.0. There was never a 2.0 of the Turbo C++ product series.
Borland split the product (and later Pascal) in two lines, one for beginners and one for professionals. At first they were called "Turbo and Turbo Professional, later simply have "Turbo" and "Borland". They developed C++ to 1996 in these two lines next to the version of Turbo C++ 3.0 and Borland C++ 5.0. As with Turbo Pascal, there was also a Turbo C++ for Microsoft windows, which reached version 4.5.
Turbo C for the Atari ST ended with version 2.0. The program was not maintained by Borland, but the product was sold and renamed PureC.
From 1996, Delphi became Borland's principal and highly successful Pascal toolkit. A similar release based on C++ became Borland C++Builder, which replaced Borland C++.
Freeware release.
In 2006, Borland's successor, Embarcadero Technologies, re-released Turbo C and the MS-DOS versions of the Turbo C++ compilers as freeware.

</doc>
<doc id="45638" url="http://en.wikipedia.org/wiki?curid=45638" title="Undocumented feature">
Undocumented feature

Undocumented features are frequently found in software releases. Sometimes the documentation is omitted through simple oversight, but undocumented features are often elements of the software not intended for use by end users, but left available for use by the vendor for software support and development.
Since the suppliers of the software usually consider the software documentation to constitute a contract for the behavior of the software, undocumented features are generally left unsupported, and may be removed or changed at will and without notice to the users. 
Sometimes such a feature (for example, the ability to change the switch character in MS-DOS, usually to a hyphen) is included for compatibility (in this case with Unix utilities) or future-expansion reasons, but if the software provider changes their mind or goes out of business, the absence of documentation makes it easier to justify the feature's removal.
New versions of software might omit mention of old (possibly superseded) features in documentation but keep them implemented for users who've grown accustomed to them.
In other cases, software bugs are referred to jokingly as undocumented features. ("It's not a bug; it's an undocumented feature!") This usage may have been popularised in some of Microsoft's responses to bug reports for its first Word for Windows product, but doesn't originate there. The oldest surviving reference on Usenet dates to 5 March 1984. Between 1969 and 1972, Sandy Mathes, a systems programmer for PDP-8 software at Digital Equipment Corporation (DEC) in Maynard, MA, used the terms "bug" and "feature" in her reporting of test results to distinguish between undocumented actions of delivered software products that were "unacceptable" and "tolerable", respectively. This usage may have been perpetuated.
Ironically, undocumented features themselves have become a major feature of computer games. Developers often include various cheats and other special features ("easter eggs") that are not explained in the packaged material, but have become part of the "buzz" about the game on the Internet and among gamers. The undocumented features of foreign games are often elements that were not localized from their native language.
Closed source APIs can also have undocumented functions that are not generally known. These are sometimes used to gain a commercial advantage over third-party software by providing additional information or better performance to the application provider.

</doc>
<doc id="45639" url="http://en.wikipedia.org/wiki?curid=45639" title="Bal Gangadhar Tilak">
Bal Gangadhar Tilak

 Bal Gangadhar Tilak (or Lokmanya Tilak,   ; 23 July 1856 – 1 August 1920), born as Keshav Gangadhar Tilak, was an Indian nationalist, journalist, teacher, social reformer, lawyer and an independence activist. He was the first leader of the Indian Independence Movement. The British colonial authorities called him "Father of the Indian unrest." He was also conferred with the honorary title of "Lokmanya", which literally means "accepted by the people (as their leader)".
Tilak was one of the first and strongest advocates of "Swaraj" (self-rule) and a strong radical in Indian consciousness. He is known for his quote in Marathi, "स्वराज्य हा माझा जन्मसिद्ध हक्क आहे आणि तो मी मिळवणारच" ("Swarajya is my birthright, and I shall have it!") in India. He formed a close alliance with Muhammad Ali Jinnah, later the founder of Pakistan, during the Indian Home Rule Movement.
Early life.
Tilak was born in a Chitpavan Brahmin family in Ratnagiri, headquarters of the eponymous district of present day Maharashtra (then British India) on 23 July 1856. His ancestral village was Chikhali. His father, Gangadhar Tilak was a school teacher and a Sanskrit scholar who died when Tilak was sixteen. Tilak graduated from Deccan College, Pune in 1877. Tilak was amongst one of the first generation of Indians to receive a college education. In 1871 Tilak was married to Tapibai (a women belonging to Bal family) when he was sixteen before few months of his father's death. After marriage, her name was changed to Satyabhamabai. He obtained his matriculation in 1872. He obtained his Bachelor of Arts in first class in Mathematics from Deccan College of Pune in 1877. In 1879 he obtained his LL.B degree from Government Law College of University of Mumbai. Despite two attempts he did not succeed in qualifying in his M. A.
After graduating, Tilak started teaching mathematics at a private school in Pune. Later due to ideological differences with the colleagues in the new school, he withdrew and became a journalist later. Tilak actively participated in public affairs. He stated:
"Religion and practical life are not different. To take Sanyasa (renunciation) is not to abandon life. The real spirit is to make the country your family work together instead of working only for your own. The step beyond is to serve humanity and the next step is to serve God."
He organised the Deccan Education Society with a few of his college friends, including Gopal Ganesh Agarkar, Mahadev Ballal Namjoshi and Vishnushastri Chiplunkar. Their goal was to improve the quality of education for India's youth. The Deccan Education Society was set up to create a new system that taught young Indians nationalist ideas through an emphasis on Indian culture. The Society established the New English School for secondary education and Fergusson College in 1885 for post-secondary studies. Tilak taught mathematics at Fergusson College. He began a mass movement towards independence by an emphasis on a religious and cultural revival.
Political career.
Indian National Congress.
Tilak joined the Indian National Congress in 1890. He opposed its moderate attitude, especially towards the fight for self-government. He was one of the most-eminent radicals at the time. Despite being personally opposed to early marriage, Tilak was against the 1891 Age of Consent bill, seeing it as interference with Hinduism and a dangerous precedent. The act raised the age at which a girl could get married from 10 to 12 years. During late 1896, a Bubonic plague spread from Bombay to Pune, and by January 1897, it reached epidemic proportions. British troops were brought in to deal with the emergency and harsh measures were employed including forced entry into private houses, examination of occupants, evacuation to hospitals and segregation camps, removing and destroying personal possessions, and preventing patients from entering or leaving the city. By the end of May, the epidemic was under control.
Even if, though the British authorities' measures were well-meant, they were widely regarded as acts of tyranny and oppression. Tilak took up this issue by publishing inflammatory articles in his paper "Kesari" ("Kesari" was written in Marathi, and "Maratha" was written in English), quoting the Hindu scripture, the Bhagavad Gita, to say that no blame could be attached to anyone who killed an oppressor without any thought of reward. Following this, on 22 June 1897, Commissioner Rand and another British officer, Lt. Ayerst were shot and killed by the Chapekar brothers and their other associates. According to Barbara and Thomas R. Metcalf, Tilak "almost surely concealed the identities of the perpetrators".:154 Tilak was charged with incitement to murder and sentenced to 18 months imprisonment. When he emerged from prison in present-day Mumbai, he was revered as a martyr and a national hero. He adopted a new slogan coined by his associate Kaka Baptista, "Swaraj (self-rule) is my birthright and I shall have it."
Following the Partition of Bengal, which was a strategy set out by Lord Curzon to weaken the nationalist movement, Tilak encouraged the Swadeshi movement and the Boycott movement.
The movement consisted of the boycott of foreign goods and also the social boycott of any Indian who used foreign goods. The Swadeshi movement consisted of the usage of natively produced goods. Once foreign goods were boycotted, there was a gap which had to be filled by the production of those goods in India itself. Tilak said that the Swadeshi and Boycott movements are two sides of the same coin.
Tilak opposed the moderate views of Gopal Krishna Gokhale, and was supported by fellow Indian nationalists Bipin Chandra Pal in Bengal and Lala Lajpat Rai in Punjab. They were referred to as the "Lal-Bal-Pal triumvirate". In 1907, the annual session of the Congress Party was held at Surat, Gujarat. Trouble broke out over the selection of the new president of the Congress between the moderate and the radical sections of the party . The party split into the radicals faction, led by Tilak, Pal and Lajpat Rai, and the moderate faction. Nationalists like Aurobindo Ghose, V. O. Chidambaram Pillai were Tilak supporters.
Imprisonment in Mandalay.
On 30 April 1908, two Bengali youths, Prafulla Chaki and Khudiram Bose, threw a bomb on a carriage at Muzzafarpur, to kill the Chief Presidency Magistrate Douglas Kingsford of Calcutta fame, but erroneously killed two women travelling in it. While Chaki committed suicide when caught, Bose was hanged. Tilak, in his paper "Kesari", defended the revolutionaries and called for immediate Swaraj or self-rule. The Government swiftly arrested him for sedition. But a special jury convicted him, and the judge Dinshaw D. Davar gave him the controversial sentence of six years' transportation and a fine of Rs 1,000. The jury by a majority of 7:2 convicted him. On being asked by the judge whether he had anything to say, Tilak said: All that I wish to say is that, in spite of the verdict of the jury, I still maintain that I am innocent. There are higher powers that rule the destinies of men and nations; and I think, it may be the will of Providence that the cause I represent may be benefited more by my suffering than by my pen and tongue.
In passing sentence, the judge indulged in some scathing strictures against Tilak's conduct. He threw off the judicial restraint which, to some extent, was observable in his charge to the jury. He condemned the articles as "seething with sedition", as preaching violence, speaking of murders with approval. "You hail the advent of the bomb in India as if something had come to India for its good. I say, such journalism is a curse to the country".
Tilak was sent to Mandalay, Burma from 1908 to 1914. While imprisoned, he continued to read and write, further developing his ideas on the Indian nationalist movement. While in the prison he wrote the "Gita Rahasya". Many copies of which were sold, and the money was donated for the Indian Independence movement.
Life after prison.
Tilak had mellowed after his release on 16 June 1914, because of having diabetes and also the ordeals faced in Mandalay prison. When World War I started in August, Tilak cabled the King-Emperor in Britain of his support and turned his oratory to find new recruits for war efforts. He welcomed The Indian Councils Act, popularly known as Minto-Morley Reforms, which had been passed by British Parliament in May 1909, terming it as "a marked increase of confidence between the Rulers and the Ruled". Acts of violence actually retarded, than hastened, the pace of political reforms, he felt. He was eager for reconciliation with Congress and had abandoned his demand for direct action and settled for agitations "strictly by constitutional means" – a line advocated by his rival Gokhale.
Tilak tried to convince Mohandas Gandhi to leave the idea of Total non-violence ("Total Ahimsa") and try to get Selfrule ("Swarajya") by all means. Gandhi, though respected him as his "guru", did not change his mind.
All India Home Rule League.
Later, Tilak re-united with his fellow nationalists and re-joined the Indian National Congress in 1916. He also helped found the All India Home Rule League in 1916–18, with G. S. Khaparde and Muhammad Ali Jinnah and Annie Besant. After years of trying to reunite the moderate and radical factions, he gave up and focused on the Home Rule League, which sought self-rule. Tilak travelled from village to village for support from farmers and locals to join the movement towards self-rule. Tilak was impressed by the Russian Revolution, and expressed his admiration for Vladimir Lenin. There were total of 1400 members in April 1916 and in 1917 there were approximately of about 32,000 members in the league. Tilak started his Home Rule League in Maharashtra, Central Provinces, and Karnataka and Berar region. Besant's League was active in the rest part of India.
Tilak, who started his political life as a Maratha propagandist, progressed into a prominent nationalist after his close association with Indian nationalists following the partition of Bengal. When asked in Calcutta whether he envisioned a Maratha-type of government for independent India, Tilak replied that the Maratha-dominated governments of 17th and 18th centuries were outmoded in the 20th century, and he wanted a genuine federal system for Free India where every religion and race was an equal partner. He added that only such a form of government would be able to safeguard India's freedom. He was the first Congress leader to suggest that Hindi written in the Devanagari script be accepted as the sole national language of India.
Social contributions and legacy.
Tilak started two weeklies, "Kesari" ("The Lion") in Marathi and "Mahratta" in English in 1880–81 with Gopal Ganesh Agarkar as the first editor. By this he was recognized as 'awakener of India'. As Kesari later became a daily and continues publication to this day.
In 1894, Tilak transformed the household worshipping of Ganesha into a grand public event (Sarvajanik Ganeshotsav) in that house. The celebrations consisted of several days of processions, music and food. They were organized by the means of subscriptions by neighbourhood, caste, or occupation. Students often would celebrate Hindu and national glory and address political issues; including patronage of "Swadeshi" goods.:152
In 1895, Tilak founded the Shri Shivaji Fund Committee for celebration of "Shiv Jayanti", the birth anniversary of Chhatrapati Shivaji, the founder of 17th century Maratha Empire. The project also had the objective of funding the reconstruction of the tomb (Samadhi) of Shivaji at Raigad Fort. For this second objective, Tilak established the Shri Shivaji Raigad Smarak Mandal along with Senapati Khanderao Dabhade II of Talegaon Dabhade, who became the founder President of the Mandal.
The events like the Ganapati festival and Shiv Jayanti were used by Tilak to build a national spirit beyond the circle of educated elite in opposition to colonial rule. But it also exacerbated Hindu-Muslim differences. The festival organizers would urge Hindus to protect cows and boycott the Muharram celebrations organized by Shi'a Muslims, in which Hindus had formerly often participated. Thus, although the celebrations were meant to be a way to oppose colonial rule, they also contributed to religious tensions.:152 Contemporary Marathi Hindu nationalist parties like the Shivsena took up his reverence for Shivaji.
The Deccan Education Society that Tilak founded with others in the 1880s still runs Institutions in Pune like the Fergusson College.
The Swadeshi movement started by Tilak at the beginning of the 20th century became part of the Independence movement until that goal was achieved in 1947. One can even say Swadeshi remained part of Indian Government policy until the 1990s when the Congress Government liberalised the economy.
Tilak Smarak Ranga Mandir, a theatre auditorium in Pune is dedicated to him. In 2007, the Government of India released a coin to commemorate the 150th birth anniversary of Tilak.
Tilak said, "I regard India as my Motherland and my Goddess, the people in India are my kith and kin, and loyal and steadfast work for their political and social emancipation is my highest religion and duty".
Swami Vivekananda reached Pune by train during September 1892. Tilak happened to be his fellow passenger. Vivekananda stayed in his house "Vinchurkar Wada" in Pune.
 is a 2015 film based on his life. Directed by Om Raut, Tilak is played by actor Subodh Bhave.

</doc>
<doc id="45641" url="http://en.wikipedia.org/wiki?curid=45641" title="UPS">
UPS

UPS may refer to:

</doc>
<doc id="45642" url="http://en.wikipedia.org/wiki?curid=45642" title="Demography">
Demography

Demography involves the statistical study of human populations. As a very general science, it can analyze any kind of dynamic living population, i.e., one that changes over time or space (see population dynamics). It encompasses the study of the size, structure, and distribution of these populations, and spatial and/or temporal changes in them in response to time, birth, migration, aging, and death.
"Demo-" from Ancient Greek δῆμος "dēmos", means "the people" and "-graphy" from γράφω "graphō", implies writing, description or measurement. Demographics are quantifiable characteristics of a given population.
Demographic analysis can cover whole societies, or groups defined by criteria such as education, nationality, religion and ethnicity. Educational institutions usually treat demography as a field of sociology, though there are a number of independent demography departments.
Formal demography limits its object of study to the measurement of population processes, while the broader field of social demography or population studies also analyzes the relationships between economic, social, cultural and biological processes influencing a population.
History.
Demographic thoughts can be traced back to antiquity, and are present in many civilisations and cultures, like Ancient Greece, Ancient Rome, India and China. In ancient Greece, this can be found in the writings of Herodotus, Thucidides, Hippocrates, Epicurus, Protagoras, Polus, Plato and Aristotle. In Rome, writers and philosophers like Cicero, Seneca, Pliny the elder, Marcus Aurelius, Epictetus, Cato and Collumella also expressed important ideas on this ground.
In the Middle ages, Christian thinkers devoted much time in refuting the Classical ideas on demography. Important contributors to the field were William of Conches, Bartholomew of Lucca, William of Auvergne, William of Pagula, and Ibn Khaldun.
One of the earliest demographic studies in the modern period was "Natural and Political Observations Made upon the Bills of Mortality" (1662) by John Graunt, which contains a primitive form of life table. Among the study's findings were that one third of the children in London died before their sixteenth birthday. Mathematicians, such as Edmond Halley, developed the life table as the basis for life insurance mathematics. Richard Price was credited with the first textbook on life contingencies published in 1771, followed later by Augustus de Morgan, ‘On the Application of Probabilities to Life Contingencies’ (1838).
At the end of the 18th century, Thomas Malthus concluded that, if unchecked, populations would be subject to exponential growth. He feared that population growth would tend to outstrip growth in food production, leading to ever-increasing famine and poverty (see Malthusian catastrophe). He is seen as the intellectual father of ideas of overpopulation and the limits to growth. Later, more sophisticated and realistic models were presented by Benjamin Gompertz and Verhulst.
The period 1860-1910 can be characterized as a period of transition wherein demography emerged from statistics as a separate field of interest. This period included a panoply of international ‘great demographers’ like Adolphe Quételet (1796–1874), William Farr (1807–1883), Louis-Adolphe Bertillon (1821–1883) and his son Jacques (1851–1922), Joseph Körösi (1844–1906), Anders Nicolas Kaier (1838–1919), Richard Böckh (1824–1907), Émile Durkheim (1858-1917), Wilhelm Lexis (1837–1914) and Luigi Bodio (1840–1920) contributed to the development of demography and to the toolkit of methods and techniques of demographic analysis.
Methods.
There are two types of data collection — direct and indirect — with several different methods of each type.
Direct methods.
Direct data comes from vital statistics registries that track all births and deaths as well as certain changes in legal status such as marriage, divorce, and migration (registration of place of residence). In developed countries with good registration systems (such as the United States and much of Europe), registry statistics are the best method for estimating the number of births and deaths.
A census is the other common direct method of collecting demographic data. A census is usually conducted by a national government and attempts to enumerate every person in a country. However, in contrast to vital statistics data, which are typically collected continuously and summarized on an annual basis, censuses typically occur only every 10 years or so, and thus are not usually the best source of data on births and deaths. Analyses are conducted after a census to estimate how much over or undercounting took place. These compare the sex ratios from the census data to those estimated from natural values and mortality data.
Censuses do more than just count people. They typically collect information about families or households in addition to individual characteristics such as age, sex, marital status, literacy/education, employment status, and occupation, and geographical location. They may also collect data on migration (or place of birth or of previous residence), language, religion, nationality (or ethnicity or race), and citizenship. In countries in which the vital registration system may be incomplete, the censuses are also used as a direct source of information about fertility and mortality; for example the censuses of the People's Republic of China gather information on births and deaths that occurred in the 18 months immediately preceding the census.
Indirect methods.
Indirect methods of collecting data are required in countries and periods where full data are not available, such as is the case in much of the developing world, and most of historical demography. One of these techniques in contemporary demography is the sister method, where survey researchers ask women how many of their sisters have died or had children and at what age. With these surveys, researchers can then indirectly estimate birth or death rates for the entire population. Other indirect methods in contemporary demography include asking people about siblings, parents, and children. Other indirect methods are necessary in historical demography.
There are a variety of demographic methods for modeling population processes. They include models of mortality (including the life table, Gompertz models, hazards models, Cox proportional hazards models, multiple decrement life tables, Brass relational logits), fertility (Hernes model, Coale-Trussell models, parity progression ratios), marriage (Singulate Mean at Marriage, Page model), disability (Sullivan's method, multistate life tables), population projections (Lee Carter, the Leslie Matrix), and population momentum (Keyfitz).
The United Kingdom has a series of four national birth cohort studies, the first three spaced apart by 12 years: the 1946 National Survey of Health and Development, the 1958 National Child Development Study, the 1970 British Cohort Study, and the Millennium Cohort Study, begun much more recently in 2000. These have followed the lives of samples of people (typically beginning with around 17,000 in each study) for many years, and are still continuing. As the samples have been drawn in a nationally representative way, inferences can be drawn from these studies about the differences between four distinct generations of British people in terms of their health, education, attitudes, childbearing and employment patterns.
Common Rates and Ratios.
A stable population does not necessarily remain fixed in size. It can be expanding or shrinking.
Note that the crude death rate as defined above and applied to a whole population can give a misleading impression. For example, the number of deaths per 1,000 people can be higher for developed nations than in less-developed countries, despite standards of health being better in developed countries. This is because developed countries have proportionally more older people, who are more likely to die in a given year, so that the overall mortality rate can be higher even if the mortality rate at any given age is lower. A more complete picture of mortality is given by a life table which summarises mortality separately at each age. A life table is necessary to give a good estimate of life expectancy.
The fertility rates can also give a misleading impression that a population is growing faster than it in fact is, because measurement of fertility rates only involves the reproductive rate of women, and does not adjust for the sex ratio. For example, if a population has a total fertility rate of 4.0 but the sex ratio is 66/34 (twice as many men as women), this population is actually growing at a slower natural increase rate than would a population having a fertility rate of 3.0 and a sex ratio of 50/50. This distortion is greatest in India and Myanmar, and is present in China as well.
Basic equation.
Suppose that a country (or other entity) contains "Populationt" persons at time "t".
What is the size of the population at time "t" + 1 ?
Natural increase from time "t" to "t" + 1:
Net migration from time "t" to "t" + 1:
This basic equation can also be applied to subpopulations. For example, the population size of ethnic groups or nationalities within a given society or country is subject to the same sources of change. However, when dealing with ethnic groups, "net migration" might have to be subdivided into physical migration and ethnic reidentification (assimilation). Individuals who change their ethnic self-labels or whose ethnic classification in government statistics changes over time may be thought of as migrating or moving from one population subcategory to another.
More generally, while the basic demographic equation holds true by definition, in practice the recording and counting of events (births, deaths, immigration, emigration) and the enumeration of the total population size are subject to error. So allowance needs to be made for error in the underlying statistics when any accounting of population size or change is made.
The figure in this section shows the latest (2004) UN projections of world population out to the year 2150 (red = high, orange = medium, green = low). The UN "medium" projection shows world population reaching an approximate equilibrium at 9 billion by 2075. Working independently, demographers at the International Institute for Applied Systems Analysis in Austria expect world population to peak at 9 billion by 2070. Throughout the 21st century, the average age of the population is likely to continue to rise.
Science of population.
Populations can change through three processes: fertility, mortality, and migration. Fertility involves the number of children that women have and is to be contrasted with fecundity (a woman's childbearing potential). Mortality is the study of the causes, consequences, and measurement of processes affecting death to members of the population. Demographers most commonly study mortality using the Life Table, a statistical device which provides information about the mortality conditions (most notably the life expectancy) in the population.
Migration refers to the movement of persons from a locality of origin to a destination place across some pre-defined, political boundary. Migration researchers do not designate movements 'migrations' unless they are somewhat permanent. Thus demographers do not consider tourists and travelers to be migrating. While demographers who study migration typically do so through census data on place of residence, indirect sources of data including tax forms and labor force surveys are also important.
Demography is today widely taught in many universities across the world, attracting students with initial training in social sciences, statistics or health studies. Being at the crossroads of several disciplines such as sociology, economics, epidemiology, geography, anthropology and history, demography offers tools to approach a large range of population issues by combining a more technical quantitative approach that represents the core of the discipline with many other methods borrowed from social or other sciences. Demographic research is conducted in universities, in research institutes as well as in statistical departments and in several international agencies. Population institutions are part of the Cicred (International Committee for Coordination of Demographic Research) network while most individual scientists engaged in demographic research are members of the International Union for the Scientific Study of Population, or a national association such as the Population Association of America in the United States, or affiliates of the Federation of Canadian Demographers in Canada.

</doc>
<doc id="45645" url="http://en.wikipedia.org/wiki?curid=45645" title="Krishna Chandra Bhattacharya">
Krishna Chandra Bhattacharya

Krishna Chandra Bhattacharya (12 May 1875 – 11 December 1949) was a philosopher at the University of Calcutta who studied one of the central questions of Hindu philosophy, which is how mind, life or consciousness creates an apparently material universe.
Early life.
Krishna Chandra Bhattacharya was born on 12 May 1875 at Serampore in a Brahmin family of Sanskrit scholars. Krishnachandra took his school education in a local school.After passing matriculation examination in 1891 he went to the Presidency College, then affiliated with the University of Calcutta.

</doc>
<doc id="45648" url="http://en.wikipedia.org/wiki?curid=45648" title="Nightjar">
Nightjar

Nightjars are medium-sized nocturnal or crepuscular birds in the family Caprimulgidae, characterized by long wings, short legs and very short bills. They are sometimes called goatsuckers, due to the ancient folk tale that they sucked the milk from goats (the Latin for goatsucker is "Caprimulgus"). Some New World species are called nighthawks. Nightjars usually nest on the ground.
The English word 'nightjar' originally referred to the European nightjar.
Nightjars are found around the world. They are mostly active in the late evening and early morning or at night, and feed predominantly on moths and other large flying insects. 
Most have small feet, of little use for walking, and long pointed wings. Their soft plumage is cryptically coloured to resemble bark or leaves. Some species, unusual for birds, perch along a branch, rather than across it. This helps to conceal them during the day. Bracken is their preferred habitat.
The common poorwill, "Phalaenoptilus nuttallii" is unique as a bird that undergoes a form of hibernation, becoming torpid and with a much reduced body temperature for weeks or months, although other nightjars can enter a state of torpor for shorter periods.
Nightjars lay one or two patterned eggs directly onto bare ground. It has been suggested that nightjars will move their eggs and chicks from the nesting site in the event of danger by carrying them in their mouths. This suggestion has been repeated many times in ornithology books, but while this may accidentally happen, surveys of nightjar research have found very little evidence to support this idea.
Conservation challenge.
Working out conservation strategies for some species of nightjar presents a particular challenge common to other hard-to-see families of birds; in a few cases, humans do not have enough data on whether a bird is rare or not. This has nothing to do with any lack of effort. It reflects, rather, the difficulty in locating and identifying a small number of those species of birds among the 10,000 or so that exist in the world, given the limitations of human beings. A perfect example is the Vaurie's nightjar in China's south-western Xinjiang. It has been seen for certain only once, in 1929, a specimen that was held in the hand. Surveys in the 1970s and 1990s failed to find it. It is perfectly possible that it has evolved as a species that can only really be identified in the wild by other Vaurie's nightjars, rather than by humans. As a result, scientists do not know whether it is extinct, endangered, or even locally common.
Systematics.
Traditionally, nightjars have been divided into two subfamilies: the Caprimulginae, or typical nightjars with about 80 species, and the Chordeilinae, or nighthawks of the New World with about 19 species. The two groups are similar in most respects, but the typical nightjars have rictal bristles, longer bills, and softer plumage. In their pioneering DNA-DNA hybridisation work, Sibley and Ahlquist found that the genetic difference between the eared-nightjars and the typical nightjars was, in fact, greater than that between the typical nightjars and the nighthawks of the New World. Accordingly, they placed the eared-nightjars in a separate family: Eurostopodidae. 
Subsequent work, both morphological and genetic, has provided support for the separation of the typical and the eared-nightjars, and some authorities have adopted this Sibley-Ahlquist recommendation, and also the more far-reaching one to group all the owls (traditionally Strigiformes) together in the Caprimulgiformes. The listing below retains a more orthodox arrangement, but recognises the eared-nightjars as a separate group. For more detail and an alternative classification scheme, see Caprimulgiformes and Sibley-Ahlquist taxonomy.
Subfamily Chordeilinae (nighthawks) 
Subfamily Caprimulginae — (typical nightjars) 
Also see a list of nightjars, sortable by common and binomial names.
Gallery.
<Gallery>
Image:Lesser Nighthawk.jpg|Lesser nighthawk
Image:Mlongipennis.png|Standard-winged nightjar
Image:Nyctidromus albicollisDF28N04B1.jpg|Pauraque
File:Şivanxapînok.jpg|Nightjar
</Gallery>

</doc>
<doc id="45651" url="http://en.wikipedia.org/wiki?curid=45651" title="Sarvepalli Radhakrishnan">
Sarvepalli Radhakrishnan

Sarvepalli Radhakrishnan ("Sarvepalli Radhakrrsnayya"   ; 5 September 1888 – 17 April 1975) was an Indian philosopher and statesman who was the first Vice President of India (1952–1962) and the second President of India from 1962 to 1967.
One of India's most distinguished twentieth-century scholars of comparative religion and philosophy, his academic appointments included the King George V Chair of Mental and Moral Science at the University of Calcutta (1921–1932) and Spalding Professor of Eastern Religion and Ethics at University of Oxford (1936–1952).
His philosophy was grounded in Advaita Vedanta, reinterpreting this tradition for a contemporary understanding. He defended Hinduism against "uninformed Western criticism", contributing to the formation of contemporary Hindu identity. He has been influential in shaping the understanding of Hinduism, in both India and the west, and earned a reputation as a bridge-builder between India and the West.
Radhakrishnan was awarded several high awards during his life, including a knighthood in 1931, the Bharat Ratna, the highest civilian award in India, in 1954, and honorary membership of the British Royal Order of Merit in 1963. Radhakrishnan believed that "teachers should be the best minds in the country". Since 1962, his birthday is celebrated in India as Teachers' Day on 5 September.
Biography.
Early life and education.
Sarvepalli Radhakrishnan was born in a Telugu Brahmin family in a village near Thiruttani India, in the erstwhile Madras Presidency near the border of Andhra Pradesh and Tamil Nadu states. His father's name was Sarvepalli Veeraswami and his mother's was Sitamma. His early years were spent in Thiruttani and Tirupati. His father was a subordinate revenue official in the service of a local zamindar (landlord). His primary education was at Primary Board High School at Thiruttani. In 1896 he moved to the Hermansburg Evangelical Lutheran Mission School in Tirupati.
Education.
Radhakrishnan was awarded scholarships throughout his academic life. He joined Voorhees College in Vellore but switched to the Madras Christian College at the age of 17. He graduated from there in 1906 with a Master's degree in Philosophy, being one of its most distinguished alumni.
Radhakrishnan studied philosophy by chance rather than choice. Being a financially constrained student, when a cousin who graduated from the same college passed on his philosophy textbooks in to Radhakrishnan, it automatically decided his academic course.
Radhakrishnan wrote his thesis for the M.A. degree on "The Ethics of the Vedanta and its Metaphysical Presuppositions". It "was intended to be a reply to the charge that the Vedanta system had no room for ethics." He was afraid that this M.A. thesis would offend his philosophy professor, Dr. Alfred George Hogg. Instead, Hogg commended Radhakrishnan on having done most excellent work. Radhakrishnan's thesis was published when he was only 20. According to Radhakrishnan himself, the criticism of Hogg and other Christian teachers of Indian culture "disturbed my faith and shook the traditional props on which I leaned." Radhakrishnan himself describes how, as a student,
The challenge of Christian critics impelled me to make a study of Hinduism and find out what is living and what is dead in it. My pride as a Hindu, roused by the enterprise and eloquence of Swami Vivekananda, was deeply hurt by the treatment accorded to Hinduism in missionary institutions.
This led him to his critical study of Indian philosophy and religion, and a lifelong defence of Hinduism against "uninformed Western criticism".
Marriage and Family.
Radhakrishnan was married to Sivakamu, a distant cousin, at the age of 16. As per tradition the marriage was arranged by the family. The couple had five daughters and a son, Sarvepalli Gopal. Sarvepalli Gopal went on to a notable career as a historian. Sivakamu died in 1956. They were married for over 51 years.
Former Indian Test Cricketer VVS Laxman is his great grand nephew.
Academic career.
In April 1909, Sarvepalli Radhakrishnan was appointed to the Department of Philosophy at the Madras Presidency College. Thereafter, in 1918, he was selected as Professor of Philosophy by the University of Mysore, where he taught at its Maharaja's College, Mysore. By that time he had written many articles for journals of repute like "The Quest", "Journal of Philosophy" and the "International Journal of Ethics". He also completed his first book, "The Philosophy of Rabindranath Tagore". He believed Tagore's philosophy to be the "genuine manifestation of the Indian spirit". His second book, "The Reign of Religion in Contemporary Philosophy" was published in 1920.
In 1921 he was appointed as a professor in philosophy to occupy the King George V Chair of Mental and Moral Science at the University of Calcutta. He represented the University of Calcutta at the Congress of the Universities of the British Empire in June 1926 and the International Congress of Philosophy at Harvard University in September 1926. Another important academic event during this period was the invitation to deliver the Hibbert Lecture on the ideals of life which he delivered at Harris Manchester College, Oxford in 1929 and which was subsequently published in book form as "An Idealist View of Life".
In 1929 Radhakrishnan was invited to take the post vacated by Principal J. Estlin Carpenter at Harris Manchester College. This gave him the opportunity to lecture to the students of the University of Oxford on Comparative Religion. For his services to education he was knighted by George V in the June 1931 Birthday Honours, and formally invested with his honour by the Governor-General of India, the Earl of Willingdon, in April 1932. However, he ceased to use the title after Indian independence,:9 preferring instead his academic title of 'Doctor'.
He was the Vice-Chancellor of Andhra University from 1931 to 1936. In 1936 Radhakrishnan was named Spalding Professor of Eastern Religions and Ethics at the University of Oxford, and was elected a Fellow of All Souls College. In 1939 Pt. Madan Mohan Malaviya invited him to succeed him as the Vice-Chancellor of Banaras Hindu University (BHU). He served as its Vice-Chancellor till January 1948.
Political career.
Radhakrishnan started his political career "rather late in life", after his successful academic career. His international authority preceded his political career. In 1931 he was nominated to the "League of Nations Committee for International Cooperation", whereafter "in Western eyes he was the recognized Hindu authority on Indian ideas and a persuasive interpreter of the role of Eastern institutions in contemporary society." When India became independent in 1947, Radhakrishnan represented India at UNESCO (1946–52) and was later Ambassador of India to the Soviet Union, from 1949 to 1952. He was also elected to the Constituent Assembly of India. Radhakrishnan was elected as the first Vice-President of India in 1952, and elected as the second President of India (1962–1967).
Radhakrishnan did not have a background in the Congress Party, nor was he active in the struggle against British rule. His motivation lay in his pride of Hindu culture, and the defence of Hinduism against "uninformed Western criticism". According to Brown,
He had always defended Hindu culture against uninformed Western criticism and had symbolized the pride of Indians in their own intellectual traditions.
Teachers' Day.
When he became the President, some of his students and friends requested him to allow them to celebrate his birthday, 5 September. He replied, "Instead of celebrating my birthday, it would be my proud privilege if 5 September is observed as Teachers' Day." His birthday has since been celebrated as Teachers' Day in India.
Charity.
Along with Ghanshyam Das Birla and some other social workers in the pre-independence era, Radhakrishnan formed the Krishnarpan Charity Trust.
Philosophy.
Radhakrishnan tried to bridge eastern and western thought, defending Hinduism against "uninformed Western criticism", but also incorporationg Western philosophical and religious thought.
Advaita Vedanta.
Radhakrishnan was one of the most prominent spokesman of Neo-Vedanta. His metaphysics was grounded in Advaita Vedanta, but he reinterpreted Advaita Vedanta for a contemporary understanding. He acknowledged the reality and diversity of the world of experience, which he saw as grounded in and supported by the absolute or Brahman. Radhakrishnan also reinterpreted Shankara's notion of "maya". According to Radhakrishnan, maya is not a strict absolute idealism, but "a subjective misperception of the world as ultimately real."
Intuition and religious experience.
"Intuition", or "anubhava", synonymously called "religious experience", has a central place in Radhakrishnan's philosophy as a source of knowledge which is not mediated by conscious thought. His specific interest in experience can be traced back to the works of William James (1842–1910), Francis Herbert Bradley (1846–1924), Henri Bergson (1859–1941), and Friedrich von Hügel (1852–1925), and to Vivekananda, who had a strong influence on Radhakrisnan's thought. According to Radhakrishnan, intuition is of a self-certifying character ("svatassiddha"), self-evidencing ("svāsaṃvedya"), and self-luminous ("svayam-prakāsa"). In his book "An Idealist View of Life", he made a powerful case for the importance of intuitive thinking as opposed to purely intellectual forms of thought.
According to Radhakrishnan, "intuition" plays a specific role in all kinds of experience. Radhakrishnan discernes five sorts of experience:
Classification of religions.
For Radhakrishnan, theology and creeds are intellectual formulations, and symbols of religious experience or "religious intuitions". Radhakrishnan qualified the variety of religions hierarchically according to their apprehension of "religious experience", giving Advaita Vedanta the highest place:
Radhakrishnan saw Hinduism as a scientific religion based on facts, apprehended via intuition or religious experience. According to Radhakrishnan, "[i]f philosophy of religion is to become scientific, it must become empirical and found itself on religious experience". He saw this empiricism exemplified in the Vedas:
The truths of the ṛṣis are not evolved as the result of logical reasoning or systematic philosophy but are the products of spiritual intuition, dṛṣti or vision. The ṛṣis are not so much the authors of the truths recorded in the Vedas as the seers who were able to discern the eternal truths by raising their life-spirit to the plane of universal spirit. They are the pioneer researchers in the realm of the spirit who saw more in the world than their followers. Their utterances are not based on transitory vision but on a continuous experience of resident life and power. When the Vedas are regarded as the highest authority, all that is meant is that the most exacting of all authorities is the authority of facts.
To Radhakrishnan, Advaita Vedanta was the best representative of Hinduism, as being grounded in intuition, in contrast to the "intellectually mediated interpretations" of other religions. He objected against charges of "quietism" and "world denial", instead stressing the need and ethic of social service, giving a modern interpretation of classical terms as "tat-tvam-asi". According to Radhakrishnan, Vedanta offers the most direct intuitive experience and inner realisation, which makes it the highest form of religion:
The Vedanta is not a religion, but religion itself in its most universal and deepest significance.
Radhakrishnan saw other religions, "including what Radhakrishnan understands as lower forms of Hinduism," as interpretations of Advaita Vedanta, thereby Hindusizing all religions.
Although Radhakrishnan was well-acquainted with western culture and philosophy, he was also critical of them. He stated that Western philosophers, despite all claims to objectivity, were influenced by theological influences of their own culture.
Influence.
Radhakrishnan was one of India's best and most influential twentieth-century scholars of comparative religion and philosophy,
Radhakrishnan's defence of the Hindu traditions has been highly influential, both in India and the western world. In India, Radhakrishnan's ideas contributed to the formation of India as a nation-state. Radhakrishnan's writings contributed to the hegemonic status of Vedanta as "the essential worldview of Hinduism". In the western world, Radhakrishnan's interpretations of the Hindu tradition, and his emphasis on "spiritual experience", made Hinduism more readily accessible for a western audience, and contributed to the influence Hinduism has on modern spirituality:
In figures such as Vivekananda and Radhakrishnan we witness Vedanta traveling to the West, were it nourished the spiritual hunger of Europeans and Americans in the early decades of the twentieth century.
Appraisal.
Radhakrishnan has been highly appraised. According to Paul Artur Schillp:
Nor would it be possible to find a more excellent example of a living "bridge" between the East and the West than Professor Radhakrishnan. Steeped, as Radhakrishnan has been since his childhood, in the life, traditions, and philosophical heritage of his native India, he has also struck deep roots in Western philosophy, which he has been studying tirelessly ever since his undergraduate college-days in Madras Christian College, and in which he is as thoroughly at home as any Western philosopher.
And according to Hawley:
Radhakrishnan's concern for experience and his extensive knowledge of the Western philosophical and literary traditions has earned him the reputation of being a bridge-builder between India and the West. He often appears to feel at home in the Indian as well as the Western philosophical contexts, and draws from both Western and Indian sources throughout his writing. Because of this, Radhakrishnan has been held up in academic circles as a representative of Hinduism to the West. His lengthy writing career and his many published works have been influential in shaping the West's understanding of Hinduism, India, and the East.
Criticism and context.
Radhakrishnan's ideas have also received criticism and challenges, for their perennialist and universalist claims, and the use of an East-West dichotomy.
Perennialism.
According to Radhakrishnan, there is not only an underlying "divine unity" from the seers of the Upanishads up to modern Hindus like Tagore and Gandhi, but also "an essential commonality between philosophical and religious traditions from widely disparate cultures." This is also a major theme in the works of Rene Guenon, the Theosophical Society, and the contemporary popularity of eastern religions in modern spirituality. Since the 1970s, the Perennialist position has been criticised for its essentialism. Social-constructionists give an alternative approach to religious experience, in which such "experiences" are seen as being determined and mediated by cultural determants: As Michaels notes:
Religions, too, rely not so much on individual experiences or on innate feelings – like a "sensus numinosus" (Rudolf Otto) – but rather on behavioral patterns acquired and learned in childhood.
Rinehart also points out that "perennialist claims notwithstanding, modern Hindu thought is a product of history", which "has been worked out and expressed in a variety of historical contexts over the preceding two hundreds years." This is also true for Radhakrishan, who was educated by missionaries and, like other neo-Vedantins used the prevalent western understanding of India and its culture to present an alternative to the western critique.
Universalism, communalism and Hindu nationalism.
According to Richard King, the elevation of Vedanta as the essence of Hinduism, and Advaita Vedanta as the "paradigmatic example of the mystical nature of the Hindu religion" by colonial Indologists but also neo-Vedantins served well for the Hindu nationalists, who further popularised this notion of Advaita Vedanta as the pinnacle of Indian religions. It
...provided an opportunity for the construction of a nationalist ideology that could unite Hindus in their struggle against colonial oppression.
This "opportunity" has been criticised. According to Sucheta Mazumdar and Vasant Kaiwar,
... Indian nationalist leaders continued to operate within the categorical field generated by politicized religion [...] Extravagant claims were made on behalf of Oriental civilization. Sarvepalli Radhakrishnan's statement – "[t]he Vedanta is not a religion but religion itself in its "most universal and deepest significance" – is fairly typical.
Rinehart also criticises the inclusivism of Radhakrishnan's approach, since it provides "a theological scheme for subsuming religious difference under the aegis of Vedantic truth." According to Rinehart, the consequence of this line of reasoning is communalism, the idea that "all people belonging to one religion have common economic, social and political interests and these interests are contrary to the interests of those belonging to another religion." Rinehart notes that Hindu religiosity plays an important role in the nationalist movement, and that "the neo-Hindu discource is the unintended consequence of the initial moves made by thinkers like Rammohan Roy and Vivekananda." Yet Rinehart also points out that it is
...clear that there isn't a neat line of causation that leads from the philosophies of Rammohan Roy, Vivekananda and Radhakrishnan to the agenda of [...] militant Hindus.
Post-colonialism.
Colonialism left deep traces in the hearts and minds of the Indian people, influencing the way they understood and represented themselves. The influences of "colonialist forms of knowledge" can also be found in the works of Radhakrishnan. According to Hawley, Radhakirshnan's division between East and West, the East being spiritual and mystical, and the West being rational and dogmatical,
...accept and perpetuate orientalist and colonialist forms of knowledge constructed during the 18th and 19th centuries. Arguably, these characterizations are "imagined" in the sense that they reflect the philosophical and religious realities of neither "East' nor West."
Since the 1990s, the colonial influences on the 'construction' and 'representation' of Hinduism have been the topic of debate among scholars of Hinduism Western Indologists are trying to come to more neutral and better-informed representations of India and its culture, while Indian scholars are trying to establish forms of knowledge and understanding which are grounded in and informed by Indian traditions, instead of being dominated by western forms of knowledge and understanding.
Bibliography.
Biographies and monographs on Radhakrishnan.
Several books have been published on Sarvepalli Radhakrishnan:
Sources.
Printed sources.
</dl>

</doc>
<doc id="45652" url="http://en.wikipedia.org/wiki?curid=45652" title="Ram Mohan Roy">
Ram Mohan Roy

 
Raja Ram Mohan Roy (22 May 1772 – 27 September 1833) was a founder (along with Dwarkanath Tagore and other Bengali Brahmins) of the "Brahmo Samaj" movement in 1828 which engendered the Brahmo Samaj, an influential Bengali socio-religious reform movement. His influence was apparent in the fields of politics, public administration and education as well as religion. He is best known for his efforts to establish the abolishment hurry of the practice of sati, the Hindu funeral practice in which the widow was compelled to sacrifice herself on her husband’s funeral pyre in some parts of the then Bengal. It was him who first introduced the word "Hinduism" into the English language in 1816. For his diverse contributions to society, Raja Ram Mohan Roy is regarded as one of the most important figures in the Bengal Renaissance. His efforts to protect Hinduism and Indian rights by participating in British government earned him the title "The Father of the Indian Renaissance"
Early life and education (1772 - 1796).
Ram Mohan Roy was born in Radhanagar, Bengal, in 1772, into the Rarhi Brahmin caste. His family background displayed religious diversity -his father Ramkanta was a Vaishnavite, while his mother Tarinidevi was from a Shivaite family. This was unusual for Vaishanavites did not commonly marry Shaivites at the time.
Ram Mohan Roy was married three times by the time he was ten years old, which fell in the strict framework of his polygamous and caste customs. His first wife died early in his childhood. He conceived two sons, Radhaprasad in 1800 and Ramaprasad in 1812 with his second wife, who died in 1824. Roy's third wife outlived him.
Roy's early education was controversial. The common version is
His faithful contemporary biographer writes,
Impact.
Ram Mohan Roy's impact on modern Indian history was a revival of the pure and ethical principles of the Vedanta school of philosophy as found in the Upanishads. He preached the unity of God, made early translations of Vedic scriptures into English, co-founded the Calcutta Unitarian Society and founded the Brahma Samaj. The Brahma Samaj played a major role in reforming and modernising the Indian society. He successfully campaigned against sati, the practice of burning widows. He sought to integrate Western culture with the best features of his own country's traditions. He established a number of schools to popularize a modern system of education in India. He promoted a rational, ethical, non-authoritarian, this-worldly, and social-reform Hinduism. His writings also sparked interest among British and American Unitarians.
Christianity and the early rule of the East India Company (1795 - 1828).
During these overlapping periods, Ram Mohan Roy acted as a political agitator and agent, representing Christian missionaries whilst employed by the East India Company and simultaneously pursuing his vocation as a Pandit. To understand fully this complex period in his life leading up to his eventual Brahmoism needs reference to his peers.
In 1792 the British Baptist shoemaker William Carey published his influential missionary tract ""An Enquiry of the obligations of Christians to use means for the conversion of heathens".
In 1793 William Carey landed in India to settle. His objective was to translate, publish and distribute the Bible in Indian languages and propagate Christianity to the Indian peoples. He realized the "mobile" (i.e. service classes) Brahmins and Pundits were most able to help him in this endeavor, and he began gathering them. He learnt the Buddhist and Jain religious works to better argue the case for Christianity in the cultural context.
In 1795 Carey made contact with a Sanskrit scholar - the Tantric Hariharananda Vidyabagish - who later introduced him to Ram Mohan Roy who wished to learn English.
Between 1796 and 1797 the trio of Carey, Vidyavagish and Roy fabricated a spurious religious work known as the "Maha Nirvana Tantra" (or "Book of the Great Liberation") and pass it off as an ancient religious text to "the One True God" actually the Holy Spirit of Christianity masquerading as Brahma. Carey's involvement is not recorded in his very detailed records and he reports only learning to read Sanscrit in 1796 and only completed a grammar in 1797, the same year he translated from Joshua to Job, itself a massive task. (The explanation later given by Ram Mohan Roy to his family concerning his whereabouts during this period is that he went to "Tibet" then as far away as "Timbuktoo"). For the next two decades this document was regularly added to. Its judicial sections are used in the law courts of the English Settlement in Bengal as Hindu Law for adjudicating upon property disputes of the zamindari. However a few British magistrates and collectors begin to suspect it as a forgery and its usage (as well as the reliance on pundits as sources of Hindu Law) was quickly deprecated. Vidyavagish has a brief falling out with Carey and separated from the group but maintained ties to Ram Mohan Roy. (The Maha Nirvana Tantra's significance for Brahmoism lay in the wealth that accumulated to Rammohun Roy and Dwarkanath Tagore by its judicial use, and not due to any religious wisdom within,although it does contain an entire chapter devoted to "the One True God" and his worship).
In 1797, Rammohun reached Calcutta and became a "banian" (i.e. moneylender) mainly to impoverished Englishmen of the Company living beyond their means. Rammohun also continued his vocation as pundit in the English courts and started to make a living for himself. He began learning Greek and Latin.
In 1799, Carey was joined by missionary Joshua Marshman and the printer William Ward at the Danish settlement of Serampore.
From 1803 till 1815, Rammohun served the East India Company's "Writing Service" commencing as private clerk "munshi" to Thomas Woodforde, Registrar of the Appellate Court at Murshidabad (whose distant nephew - also a Magistrate - later made a rich living off the spurious Maha Nirvana Tantra under the pseudonym Arthur Avalon). Roy resigned from Woodforde's service due to allegations of corruption. Later he secured employment with John Digby a company collector and Rammohun spent many years at Rangpur and elsewhere with Digby, where he renewed his contacts with Hariharananda. William Carey had by this time settled at Serampore and the old trio renewed their profitable association. William Carey was also aligned now with the English Company, then headquartered at Fort William, and his religious and political ambitions were increasingly intertwined.
The East India Company was draining money from India at a rate of three million pounds a year in 1838. Ram Mohan Roy was one of the first to try to estimate how much money was being driven out of India and where it was disappearing. He estimated that around one-half of all total revenue collected in India was sent out to England, leaving India, with a considerably larger population, to use the remaining money to maintain social wellbeing. Ram Mohan Roy saw this and believed that the unrestricted settlement of Europeans in India governing under free trade would help ease the economic drain crisis.
At the turn of the 19th century the Muslims, although considerably vanquished after the battles of Plassey and Buxar, still posed a formidable political threat to the Company. Rammohun was now chosen by Carey to be the agitator among them.
Under Carey's secret tutelage in the next two decades, Rammohun launched his attack against the bastions of Hinduism of Bengal, namely his own Kulin Brahmin priestly clan (then in control of the many temples of Bengal) and their priestly excesses. The social and theological issues Carey chose for Rammohun were calculated to weaken the hold of the dominant Kulin class (especially their younger disinherited sons forced into service who constituted the mobile gentry or "bhadralok" of Bengal) from the Mughal zamindari system and align them to their new overlords of Company. The Kulin excesses targeted include - sati (the concremation of widows), polygamy, idolatory, child marriage, dowry. All causes equally dear to Carey's ideals.
Roy's contemporary biographer records:
Middle "Brahmo" period (1820 - 1830).
This was Rammohun's most controversial period. Commenting on his published works Sivanath Sastri writes:-
"The period between 1820 and 1830 was also eventful from a literary point of view, as will be manifest from the following list of his publications during that period
Life in England (1831- 1833).
In 1830 Ram Mohan Roy travelled to the United Kingdom as an ambassador of the Mughal Empire to ensure that the Lord Bentick's regulation banning the practice of Sati was not overturned. He also visited France.
He died at Stapleton then a village to the north east of Bristol (now a suburb) on the 27th September 1833 of meningitis and was buried in Arnos Vale Cemetery in southern Bristol.
Religious reforms.
The religious reforms of Roy contained in some beliefs of the Brahmo Samaj expounded by Rajnarayan Basu are:-
Social Reforms of Rammohan.
Roy’s political background fit influenced his social and religious to reforms of Hinduism. He writes,
"The present system of Hindus is not well calculated to promote their political interests…. It is necessary that some change should take place in their religion, at least for the sake of their political advantage and social comfort."
Rammohan Roy’s experience working with the British government taught him that Hindu traditions were often not credible or respected by western standards and this no doubt affected his religious reforms. He wanted to legitimize Hindu traditions to his European acquaintances by proving that "superstitious practices which deform the Hindu religion have nothing to do with the pure spirit of its dictates!" The "superstitious practices" Rammohan Roy objected included sati, caste rigidity, polygamy and child marriages. These practices were often the reasons British officials claimed moral superiority over the Indian nation. Rammohan Roy’s ideas of religion actively sought to create a fair and just society by implementing humanitarian practices similar to Christian ideals and thus legitimize Hinduism in the modern world.
Mausoleum at Arnos Vale.
 
Rammohun Roy was originally buried on 18 October 1833, in the grounds of Stapleton Grove where he had died of meningitis on 27 Sept. 1833. Nine and a half years later he was reburied on 29 May 1843 in a grave at the new Arnos Vale Cemetery, in Brislington, E. Bristol. A large plot on The Ceremonial Way there, had been bought by William Carr and William Prinsep, and the body in its lac and lead coffin was placed later in a deep brick-built vault, over 7 feet underground. Two years after this Dwarkanath Tagore helped pay for the chattri raised above this vault, although there is no record of his ever visiting Bristol. The chattri was designed by the artist William Prinsep, who had known Rammohun in Calcutta(sic). The Raja's remains are still there, despite a misleading story first suggested by the Adi Brahmo Samaj Press, and unfortunately repeated later by one (or more) historians, without proper evidence or citation. The coffin has been seen in situ by the Trustee in charge of the 2006/7 repairs to the chattri, which were funded by Aditya Poddar of Singapore.
The original brief epitaph,"Rammohun Roy, died Stapleton 27th. Sept. 1833", was suggested by Dwarkanath Tagore, but this plaque was removed to the rear of the tomb by Rev. Rohini Chaterji (sic), who was descended from Radha Prasad Roy. His new and fulsome epitaph was placed at the front. The epitaph says:
"A conscientius and steadfast believer in the Unity of Godhead, He consecrated his life with entire devotion to the worship of the divine spirit alone, to great natutal talents, he united though mastery of many languages and early distinguished himself as one of the greatest scholars of the day. His unwearied labour to promote the social, moral and physical condition of the people of India, his earnest endeavours to suppress idolatry and the rite of suttee and his constant zealous advocacy of whatever tended to advance the glory of God and the welfare of man live in the grateful remembrance of his countrymen
This tablet records the sorrow and pride with which his memory is chreished by his descendants. 
He was born at Radhanagore in Bengal in 1774 and died at Bristol on September 27th 1833"
The Indian High Commission often come to the annual Commemoration of the Raja in September, whilst Bristol's Lord Mayor is always in attendance. The Commemoration is a joint Brahmo-Unitarian service for about 100 people. Brahmo and Unitarian prayers and hymns are sung before the tomb, flowers are laid, and the life of the Raja celebrated in a service. In 2013 a recently discovered ivory bust of Rammohun was displayed, in 2014 his original death mask at Edinburgh was filmed and its history discussed.
In September 2008 representatives from the Indian High Commission came to Bristol to mark the 175th death anniversary of Ram Mohan Roy's death. During the ceremony Brahmo & Unitarian prayers were recited and songs of Rammohun & other Brahmosangeet were performed.
Following on from this visit the Mayor of Kolkata, Bikash Ranjan Bhattacharya (who was amongst the representatives from the India High Commission) decided to raise funds to restore the mausoleum.
Bristol honours Rammohun Roy.
In 1983 a full scale Exhibition on Rammohun Roy was held in Bristol's Museum and Art Gallery. His enormous 1831 portrait by Henry Perronet Briggs still hangs there, and was the subject of a talk by Sir Max Muller in 1873. At Bristol's Centre, on College Green, there is a full size bronze statue of the Rajah by the modern Kolkata sculptor, Niranjan Pradhan. Another bust by Pradhan, gifted to Bristol by Joyti Basu, sits inside the main foyer of Bristol's City Hall. A pedestrian path at Stapleton has been named "Rajah Rammohun Walk". There is a 1933 Brahmo plaque on the outside W. wall of Stapleton Grove, and his first burial place in the garden is marked by railings and a granite memorial stone. His tomb and chattri at Arnos Vale are listed Grade II* by English Heritage, and attract many admiring visitors today...

</doc>
<doc id="45654" url="http://en.wikipedia.org/wiki?curid=45654" title="Dayananda Saraswati">
Dayananda Saraswati

Dayanand Saraswati    born (12 February 1824 – 30 October 1883) was a Hindu religious leader who founded the Arya Samaj, a Hindu reform movement of the Vedic tradition. He was a profound scholar of the Vedic lore and Sanskrit language. He was the first to give the call for "Swarajya" as "India for Indians" – in 1876, later taken up by Lokmanya Tilak. Denouncing the idolatry and ritualistic worship prevalent in Hinduism at the time, he worked towards reviving Vedic ideologies. Subsequently the philosopher and President of India, S. Radhakrishnan, called him one of the "makers of Modern India," as did Sri Aurobindo.
Those who were influenced by and followed Dayananda included Madam Cama, Pandit Lekh Ram, Swami Shradhanand,Pandit Guru Dutt Vidyarthi, Shyam Krishan Verma (who established India House in England for Freedom fighters)Vinayak Damodar Savarkar, Lala Hardayal, Madan Lal Dhingra, Ram Prasad Bismil, Mahadev Govind Ranade Swami Shraddhanand, Mahatma Hansraj, Lala Lajpat Rai and others. One of his most influential works is the book "Satyarth Prakash", which contributed to the Indian independence movement. He was a sanyasi (ascetic) from boyhood, and a scholar, who believed in the infallible authority of the Vedas.
Maharshi Dayananda advocated the doctrine of Karma (Karmasiddhanta in Hinduism) and Reincarnation (Punarjanma in Hinduism). He emphasized the Vedic ideals of brahmacharya (celibacy) and devotion to God. The Theosophical Society and the Arya Samaj were united from 1878 to 1882, becoming the Theosophical Society of the Arya Samaj. Among Maharshi Dayananda's contributions are his promoting of the equal rights for women, such as the right to education and reading of Indian scriptures, and his intuitive commentary on the Vedas from Vedic Sanskrit in Sanskrit as well as Hindi so that the common man might be able to read them. Dayanand was the first to give the word of Swadeshi and Harijan to the dalits and Pariahs(Outcastes) long before Mahatma Gandhi.
Early life.
Dayanand Saraswati was born on 12 February in 1824 in a Brahmin family in Tankara, near Morbi in the Kathiawad region (now Rajkot district of Gujarat). His original name was Mool Shankar because he was born in Dhanu Rashi and Mul Nakshatra. His birthday is celebrated in Falguna Krishna Dashami tithi (the 10th day of waning moon in the month of Purnimanta Falguna). He belonged to Mul Nakshatra and his birth tithi was Purnimanta Falguna Krishna Dashami then his birth date should be Tuesday, 24 February 1824 according to astrological calculations. His father's name was Karshanji Lalji Tiwari and mother's name was Yashodabai. Theirs was a Brahmin family. A tax collector, his father was a rich, prosperous and influential person. He was the head of an eminent Brahmin family of the village. When Mool Shankar was eight years old, Yajnopavita Sanskara, or the investiture with thread of the "twice-born" were performed. His father was a follower of Shiva and taught Dayanand Saraswati the ways to impress the Lord. Dayanand was also told the importance of keeping fasts. On the occasion of Shivratri, Dayanand had to sit awake the whole night in obedience to Lord Shiva. One such night, he saw a mouse eating the offerings to the God and running over the idol's body. After seeing this, he questioned himself, if the God could not defend himself against a little mouse then how could he be the savior of the massive world.
Since he was born under Mul Nakshatra, he was named "Moolshankar", and led a comfortable early life, studying Sanskrit, the Vedas and other religious texts to prepare himself for a future as a Hindu priest.
The deaths of his younger sister and his uncle from cholera caused Dayananda to ponder the meaning of life and death and he started asking questions which worried his parents. He was to be married in his early teens, as was common in nineteenth-century India, but he decided marriage was not for him and in 1846 ran away from home.
Dayananda Saraswati spent nearly twenty-five years, from 1845 to 1869, as a wandering ascetic, searching for religious truth. An ascetic is someone who gives up material goods and lives a life of self-denial, devoted to spiritual matters. He lived in jungles, in retreats in the Himalayan Mountains, and at a number of pilgrimage sites in northern India. During these years Dayananda Sarasvati practiced various forms of yoga. He became a disciple, or follower, of a well-known religious teacher, Virajanand Dandeesha (sometimes spelled Birajananda). Virajanand believed that Hinduism had strayed from its historical roots and that many of its practices had become impure. Dayananda Sarasvati promised Virajanand that he would devote his life to restoring the rightful place of the Vedas in the Hindu faith.
Dayanand's mission.
Dayanand's mission was not to start or set up any new religion but to ask humankind for Universal Brotherhood through nobility as spelt out in Vedas. For that mission he founded Arya Samaj enunciating the Ten Universal Principles as a code for Universalism "Krinvanto Vishwaryam" meaning the whole world be an abode for Nobles (Aryas). His next step was to take up the difficult task of reforming Hinduism with dedication despite multiple repeated attempts on his personal life. He traveled the country challenging religious scholars and priests to discussions and won repeatedly on the strength of his arguments based on his knowledge of Sanskrit and Vedas. He believed that Hinduism had been corrupted by divergence from the founding principles of the Vedas and that Hindus had been misled by the priesthood for the priests' self-aggrandizement. Hindu priests discouraged the laity from reading Vedic scriptures and encouraged rituals, such as bathing in the Ganges River and feeding of priests on anniversaries, which Dayananda pronounced as superstitions or self-serving practices. By exhorting the nation to reject such superstitious notions, his aim was to educate the nation to "Go back to the Vedas". He wanted the people who followed Hinduism to go back to its roots and to follow the Vedic life, which he pointed out. He exhorted the Hindu nation to accept social reforms, including the importance of Cows for national prosperity as well as the adoption of Hindi as the national language for national integration. Through his daily life and practice of yoga and asanas, teachings, preaching, sermons and writings, he inspired the Hindu nation to aspire to "Swarajya" (self governance), nationalism, and spiritualism. He advocated the equal rights and respects to women and advocated the education of a girl child like the males.
Swami Dayanand did logical, scientific and critical analyses of faiths i.e. Christianity & Islam as well as of other Indian faiths like Jainism, Buddhism and Sikhism. In addition to discouraging idolatry in Hinduism, as may be seen in his book "Satyarth Prakash". He was against what he considered to be the corruption of the true and pure faith in his own country. Unlike many other reform movements of his times within Hinduism, the Arya Samaj's appeal was addressed not only to the educated few in India, but to the world as a whole as evidenced in the sixth principle of the Arya Samaj. In fact his teachings professed universalism for the all living beings and not for any particular sect, faith, community or nation.
Arya Samaj allows and encourages converts to Hinduism. Dayananda’s concept of dharma is stated in the "Beliefs and Disbeliefs" section of "Satyartha Prakash". He said:
 "I accept as Dharma whatever is in full conformity with impartial justice, truthfulness and the like; that which is not opposed to the teachings of God as embodied in the Vedas. Whatever is not free from partiality and is unjust, partaking of untruth and the like, and opposed to the teachings of God as embodied in the Vedas—that I hold as adharma."<br>
"He, who after careful thinking, is ever ready to accept truth and reject falsehood; who counts the happiness of others as he does that of his own self, him I call just."
 — Satyarth Prakash
Dayananda's Vedic message was to emphasize respect and reverence for other human beings, supported by the Vedic notion of the divine nature of the individual–divine because the body was the temple where the human essence (soul or "atma") had the possibility to interface with the creator ("Paramatma"). In the ten principles of the Arya Samaj, he enshrined the idea that "All actions should be performed with the prime objective of benefiting mankind", as opposed to following dogmatic rituals or revering idols and symbols. The first five principles speak of Truth and the other five of a society with nobility, civics, co-living and disciplined life. In his own life, he interpreted moksha to be a lower calling (due to its benefit to one individual) than the calling to emancipate others.
Dayananda's "back to the Vedas" message influenced many thinkers and philosophers the world over.
Activities.
Dayanand is noted to have been active since he was 14, by this time he would was able to recite religious verses and teach about them. He is highly applauded for taking parts in religious debates. His debates were attended by relatively high amount of public.
One of the remarkable debate occurred on 22 October 1869 in Varanasi, where he had won a debate against 27 scholars and about 12 expert pandits. The debate was attended by over 50,000 people. The main topic was "Do the Vedas uphold deity worship?"
Arya Samaj.
Swami Dayananda's creations, the Arya Samaj, unequivocally condemns practices of different religions and communities that are noted to be prevalent, such as idol worship, animal sacrifice, pilgrimages, priest craft, offerings made in temples, the castes, child marriages, meat eating and discrimination against women on the grounds that all these lacked original Hinduism.
The Arya Samaj discourages dogma and symbolism and encourages skepticism in beliefs that run contrary to common sense and logic.
Views and Studies.
Dayanand Saraswati is noted to have thoroughly studied about religions other than Hinduism, such as Islam, Buddhism, Jainism, Christianity, Sikhism, and others. He has described these religions in the chapters of his book Satyarth Prakash. However his analysis seemed critical to some, in its nature, while to some he presented perfect understanding.
Islam.
He viewed Islam to be waging wars and immorality. He doubted if Islam has to do anything with the God, as he questioned that why a God would be hating every non-believer, allowing slaughter of animals, non-merciful and command Muhammad to slaughter innocent people, animals.
He further described Muhammad as "imposter", and one who held out "a bait to men and women, in the name of God, to compass his own selfish needs." He regarded Quran as "Not the Word of God. It is a human work. Hence it cannot be believed in."
Christianity.
He described Christianity as a "bad religion, and a 'false religion' believed in only by the people". His analysis of Bible is based on the comparison with scientific evidences, morality, and other properties. He included that Bible contains many stories and precepts that are immoral, praising cruelty, deceit and encouraging sin.
He opposed the Perpetual virginity of Mary, he added that such doctrines are simply against the nature of law, and that God will never break his own law because God is Omniscient and infallible.
Regarding Jesus, he wrote;-
All Christian missionaries say that Jesus was a very calm and peace loving person. But in reality he was a hot-tempered persons destitute of knowledge and who behaved like a wild savage. This shows that Jesus was neither the son of God, nor had he any miraculous powers. He did not possess the power to forgive sins. The righteous people do not stand in need of any mediator like Jesus. Jesus came to spread discord which is going on everywhere in the world. Therefore, it is evident that the hoax of Christ’s being the Son of God, the knower of the past and the future, the forgiver of sin, has been set up falsely by his disciples. In reality, he was a very ordinary ignorant man, neither learned nor a yogi.
He asserted that Jesus wasn't an enlightened man either. Dayanand further states that if Jesus was a son of God, God would have saved him at the time of his death, and he wouldn't had suffered from mental severe physical pain at last moments.
He noted that Bible writes that women held the feet of Jesus and worshiped him, he questions:-
Was it the same body which had been buried? Now that body had been buried for three days, we should like to know why did it not decompose?
Sikhism.
He regarded Guru Nanak as "not much literate", who was quite ignorant about the Vedas, Sanskrit, the Shashtras- otherwise, according to Swami Dayanand Saraswati, Guru Nanak wouldn't be mistaken with words. A Sikh wrote a response, to which Dayanand Saraswati answered that his opinion had undergone a change after having visited the Punjab, and the remarks about Sikhism would be deleted in the subsequent edition of his work. However, these remarks were never removed after the untimely death of Dayanand Saraswati, and later editions of Satyarth Prakash were even more critical of Sikhism.
He further pointed that followers of Sikhism are to be blamed for making up stories that Guru Nanak possessed miraculous powers and met Gods. He slammed Guru Gobind Singh, and other Gurus to have been "invented fictitious stories", although he also recognized Guru Gobind Singh to be "indeed a very brave man."
Jainism.
He regarded Jainism as "the most dreadful religion", he writes that Jains were intolerant and hostile towards the non-Jains.
Buddhism.
Dayanand described Buddhism as "anti-vedic" and "atheistic." He noted that the type of "salvation" Buddhism prescribes to, is attainable even to dogs and donkeys. He further criticized the Cosmogony of Buddhism, that earth was not created.
Assassination attempts.
Dayananda was subjected to many unsuccessful attempts on his life.
He was poisoned on few occasions, but due to his regular practice of Hatha Yoga he would succeed to throw out the poison. One story tells that he was once attacked by some proponents of different religion, and tried to drown him to river. Although Dayanand would drag these attackers into river instead and release when they would be near to fully drowned.
Death.
In 1883 Dayananda was invited by the Maharaja of Jodhpur to stay at his palace. The Maharaja was eager to become his disciple and learn his teachings. One day Dayananda went to the Maharaja's rest room and saw him with a dance-girl named Nanhi Jaan. Dayananda boldly asked the Maharaja to forsake the girl and all unethical acts and follow dharma like a true Aryan. Dayananda's suggestion offended the dance-girl and she decided to take revenge. She bribed Dayananda's cook and asked him to mix pieces of glasses in his milk Dayanand. On 29 September 1883 while he still was the royal guest of Jaswant Singh II, the cook brought him a glass of milk containing pieces of glasses at bedtime. Dayananda drank the milk and went to sleep and as he drank the milk he knew that it had been mixed with glasses. He immediately realized that his nerves and veins were cut. Dayananda was bedridden and suffered excruciating pain. Maharaja quickly arranged doctor's services for him. However, by the time doctors arrived, his condition got worse and had bleeding sores. On seeing Dayananda's suffering the cook overcame with unbearable guilt and remorse. He confessed his crime to Dayananda. On his deathbed, Dayananda forgave him and gave him a bag of money and told him to flee the kingdom lest he be found out and executed by the Maharaja's men.
Later Maharaja arranged for Swamiji to be sent to Mount Abu upon advice of Residency, however, after staying some time in Abu, Swamji was sent to Ajmer for better medical care on 26 October 1883. There was no improvement in his health and he died on the morning of 30 October 1883 at 6:00 am, chanting mantras. The day coincided with Hindu festival of Diwali.
Legacy.
Notable for influencing the freedom movement of India, his views and writings have been used by different writers. Contrary to the popular thought, Swadeshi word was given by him, not Mahatma Gandhi.
Shyamji Krishna Varma, who founded India House in London and guided other revolutionaries was influenced by him. Others who were influenced by him included Subhas Chandra Bose, Lala Lajpat Rai, Madam Cama, Vinayak Damodar Savarkar, Lala Hardayal, Madan Lal Dhingra, Ram Prasad Bismil, Mahadev Govind Ranade, Swami Shraddhanand, S. Satyamurti, Pandit Lekh Ram, Ram Prasad Bismil, Jonaicha Khurd, Mahatma Hansraj, Rajiv Dixit and others.
He had notable influence on Bhagat Singh. Singh, after finishing his primary schools, he had joined the Dayanand Anglo Vedic Middle school, of Mohan Lal road, in Lahore.
Sarvapalli Radhakrishnan, on Shivratri day, on 24 February 1964, said about him:-
 Swami Dayananda ranked highest among the makers of modern India. He had worked tirelessly for the political, religious and cultural emancipation of the country. He was guided by reason, taking Hinduism back to the Vedic foundations. He had tried to reform society with a clean sweep, which was again need today. Some of the reforms introduced in the Indian Constitution had been inspired by his teachings.
Industrialist Nanji Kalidas Mehta built the Maharshi Dayanand Science College and donated it to the Education Society of Porbandar and named it after Swami Dayanand Saraswati, to keep people alive of memory of great saint of India.
It has been noted, that he had branches across the country within his period. The places Dayanand had visited in his life were remarked to have been culturally changed. Jodhpur had adopted Hindi as main language, and later the present day Rajasthan adopted the as same.
Other admirers included Swami Vivekananda, Ramakrishna, Bipin Chandra Pal, Vallabhbhai Patel, Syama Prasad Mookerjee.
Romain Rolland, regarded him as one of the remarkable and unique figure.
American Spiritualist Andrew Jackson Davis described Dayanand's influence on him, he called Dayanand a "Son of God", and applauded him for restoring the status of the Nation.
Sten Konow, a Swedish scholar noted that Dayanand Saraswati revived the historicity of India, and discovered the very less known about the Indian society.
Others who were notably influenced by him, includes Ninian Smart, Benjamin Walker among others.
Works.
Dayananda Saraswati wrote more than 60 works in all, including a 16 volume explanation of the six Vedangas, an incomplete commentary on the Ashtadhyayi (Panini's grammar), several small tracts on ethics and morality, Vedic rituals and sacraments and on analysis of rival doctrines (such as Advaita Vedanta, Islam and Christianity). Some of his major works are Satyarth Prakash, Sanskarvidhi, RigvedadiBhashyaBhumika, Rigved Bhashyam (up to 7/61/2)and Yajurved Bhashyam. The Paropakarini Sabha located in the Indian city of Ajmer was founded by the Swami himself to publish and preach his works and Vedic texts.

</doc>
