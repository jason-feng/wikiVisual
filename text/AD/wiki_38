<doc id="49324" url="http://en.wikipedia.org/wiki?curid=49324" title="Unit interval">
Unit interval

In mathematics, the unit interval is the closed interval [0,1], that is, the set of all real numbers that are greater than or equal to 0 and less than or equal to 1. It is often denoted "I" (capital letter I).
In addition to its role in real analysis, the unit interval is used to study homotopy theory in the field of topology.
In the literature, the term "unit interval" is sometimes applied to the other shapes that an interval from 0 to 1 could take: (0,1], [0,1), and (0,1). However, the notation "I" is most commonly reserved for the closed interval [0,1].
Properties.
The unit interval is a complete metric space, homeomorphic to the extended real number line. As a topological space it is compact, contractible, path connected and locally path connected. The Hilbert cube is obtained by taking a topological product of countably many copies of the unit interval. 
In mathematical analysis, the unit interval is a one-dimensional analytical manifold whose boundary consists of the two points 0 and 1. Its standard orientation goes from 0 to 1. 
The unit interval is a totally ordered set and a complete lattice (every subset of the unit interval has a supremum and an infimum).
Cardinality.
The size or cardinality of a set is the number of elements it contains.
The unit interval is a subset of the real numbers formula_1. However, it has the same size as the whole set: the cardinality of the continuum. Since the real numbers can be used to represent points along an infinitely long line, this implies that a line segment of length 1, which is a part of that line, has the same number of points as the whole line. Moreover, it has the same number of points as a square of area 1, as a cube of volume 1, and even as an unbounded "n"-dimensional Euclidean space formula_2 (see Space filling curve).
The number of elements (either real numbers or points) in all the above-mentioned sets is uncountable, as it is strictly greater than the number of natural numbers.
Generalizations.
The interval [−1,1], with length two, demarcated by the positive and negative units, occurs frequently, such as in the range of the trigonometric functions sine and cosine and the hyperbolic function tanh. This interval may be used for the domain of inverse functions. For instance, when θ is restricted to [−π/2, π/2] then sin(θ) is in this interval and arcsine is defined there.
Sometimes, the term "unit interval" is used to refer to objects that play a role in various branches of mathematics analogous to the role that [0,1] plays in homotopy theory. For example, in the theory of quivers, the (analogue of the) unit interval is the graph whose vertex set is {0,1} and which contains a single edge "e" whose source is 0 and whose target is 1. One can then define a notion of homotopy between quiver homomorphisms analogous to the notion of homotopy between continuous maps.
Fuzzy logic.
In logic, the unit interval [0,1] can be interpreted as a generalization of the Boolean domain {0,1}, in which case rather than only taking values 0 or 1, any value between and including 0 and 1 can be assumed. Algebraically, negation (NOT) is replaced with formula_3 conjunction (AND) is replaced with multiplication (formula_4), and disjunction (OR) is defined via De Morgan's law.
Interpreting these values as logical truth values yields a multi-valued logic, which forms the basis for fuzzy logic and probabilistic logic. In these interpretations, a value is interpreted as the "degree" of truth – to what extent a proposition is true, or the probability that the proposition is true.

</doc>
<doc id="49325" url="http://en.wikipedia.org/wiki?curid=49325" title="Houyhnhnm">
Houyhnhnm

Houyhnhnms are a race of intelligent horses described in the last part of Jonathan Swift's satirical "Gulliver's Travels". The name is pronounced either or . (Swift apparently intended all words of the Houyhnhnm language to echo the neighing of horses.)
Description.
Gulliver's visit to the Land of the Houyhnhnm's is described in Part IV of his "Travels", and its location illustrated on the map at the start of Part IV. 
The map shows Houyhnhnms Land to be south of Australia, it indicates Edels Land and Lewins Land to the north, and Nuyts Land to the north-east, on the mainland with the islands of St Francis and St Pieter further east, and Sweers, Maatsuyker and De Wit islands to the east. 
The map is somewhat careless with the scale, however; Edels Land to Lewins Land are shown adjacent, while in reality they are some 1000 km apart, while the sweep of the Great Australian Bight, from Cape Leeuwin, Australia's south-westerly point to the Maatsuyker Islands, off the southern tip of Tasmania, is over 3000 km.
Gulliver describes the land as "divided by long rows of trees, not regularly planted but naturally growing", with a "great plenty of grass, and several fields of oats". 
The Houyhnhnms are rational, equine beings and are masters of the land, contrasting strongly with the Yahoos, savage humanoid creatures who are no better than beasts of burden, or livestock. Whereas the Yahoos represent all that is bad about humans, Houyhnhnms have a settled, calm, reliable and rational society. Gulliver much prefers the Houyhnhnms' company to the Yahoos', even though the latter are biologically closer to him.
Interpretation.
Interpretation of the Houyhnhnms has been vexatious. One might possibly, for example, regard them as a veiled criticism by Swift of the British Empire's treatment of non-whites as lesser humans, or one could regard Gulliver's preference (and his immediate division of Houyhnhnms into color-based hierarchies) as absurd and the sign of his self-deception. In a modern context the story might be seen as presenting an early example of animal rights concerns, especially in Gulliver's account of how horses are cruelly treated in his society and the reversal of roles. The story is a possible inspiration for Pierre Boulle's novel "Planet of the Apes".
Book IV of "Gulliver's Travels" is the keystone, in some ways, of the entire work, and critics have traditionally answered the question whether Gulliver is insane (and thus just another victim of Swift's satire) by questioning whether or not the Houyhnhnms are truly admirable. Gulliver loves the land and is obedient to a race that is not like his own. The Houyhnhnm society is based upon reason, and only upon reason, and therefore the horses practice eugenics based on their analyses of benefit and cost. They have no religion and their sole morality is the defence of reason, and so they are not particularly moved by pity or a belief in the intrinsic value of life. Gulliver himself, in their company, builds the sails of his skiff from "Yahoo skins". The Houyhnhnms' lack of passion surfaces mainly during their annual meeting. A visitor apologises for being late for the meeting as her husband had just died and she had to make the proper arrangements for the funeral, which consists of burial at sea. She eats her lunch like all the other Houyhnhnms and is not affected at all by her loss, rationalising that gone is gone. A further example of the lack of humanity and emotion in the Houyhnhnms is that their laws demand that each couple produce two children, one male and one female. In the event that a marriage produced two offspring of the same sex, the parents would take their children to the annual meeting and trade one with a couple who produced two children of the opposite sex. This latter part was viewed by some Swift scholars as his spoofing and or criticising the notion that the "ideal" family produces children of both sexes.
On one hand, the Houyhnhnms have an orderly and peaceful society. They have philosophy and a language that is entirely free of political and ethical nonsense. They have no word for a "lie" (and must substitute a circumlocution: "to say a thing which is not"). They also have a form of art that is derived from nature. Outside "Gulliver's Travels", Swift had expressed longstanding concern over the corruption of the English language, and he had proposed language reform. He had also, in "Battle of the Books" and in general in "A Tale of a Tub", expressed a preference for the Ancients (Classical authors) because their art was based directly upon nature, and not upon other art.
On the other hand, Swift was profoundly mistrustful of attempts at reason that resulted in either hubris (for example, the Projectors satirised in "A Tale of a Tub" or in Book III of "Gulliver's Travels") or immorality (such as the speaker of "A Modest Proposal", who offers an entirely logical and wholly immoral proposal for cannibalism). The Houyhnhnms embody both the good and the bad side of reason, for they have the pure language Swift wished for and the amorally rational approach to solving the problems of humanity (Yahoos); the extirpation of the Yahoo population by the horses is very like the speaker of "A Modest Proposal".
In the shipping lanes he is rescued by a Portuguese sea captain, a level-headed individual albeit full of concern for others, whose temperament at one level appears intermediate between the calm, rational Houyhnhnms of Houyhnhnmland and the norm of corrupt, European humanity, which Gulliver no longer distinguishes from Houyhnhnmland's wild Yahoos. Gulliver can speak with him, and though now disaffected from all humanity, has a residual sense he would have esteemed him. Gulliver is returned to his English home and family, finds their smell and look intolerable and all his countrymen no better than "Yahoos", purchases and converses with two stabled horses, tolerating the stable boy, and assures the reader of his account's utter veracity.

</doc>
<doc id="49326" url="http://en.wikipedia.org/wiki?curid=49326" title="KStars">
KStars

KStars is a planetarium program using the KDE Platform. It can be used on most Unix-like computer operating systems, as well as on the Microsoft Windows platform using 'KDE for Windows'. It provides an accurate graphical representation of the night sky, from any location on Earth, at any date and time. The display includes up to 100 million stars (with additional addons), 13,000 deep sky objects, constellations from different cultures, all 8 planets, the Sun and Moon, and thousands of comets and asteroids. It has features to appeal to users of all levels, from informative hypertext articles about astronomy, to robust control of telescopes and CCD cameras, and logging of observations of specific objects.
Included with KStars is astrophotography suite, a complete astrophotography solution that can control all INDI devices including numerous telescopes, CCDs, DSLRs, focusers, filters, and a lot more. Ekos supports highly accurate tracking using online and offline astrometry solver, auto-focus and auto-guiding capabilities, and capture of single or multiple images using the powerful built in sequence manager.
KStars has been packaged by many Linux/BSD distributions, including Red Hat Linux, openSUSE, Mandriva Linux, and Debian GNU/Linux. Some distributions package KStars as a separate application, some just provide a kdeedu package, which includes KStars. KStars is distributed with the KDE Software Compilation as part of the kdeedu "Edutainment" module.
KStars participated in Google Summer of Code in 2008, 2009, 2010, 2011 and 2012. It has also participated in the first run of ESA's Summer of Code in Space in 2011.
It has been identified as one of the three best "Linux stargazing apps" in a Linux.com review.
The latest version of KStars is 2.2.0, released with KDE Applications 14.12. Released under the GNU General Public License, KStars is free software.

</doc>
<doc id="49329" url="http://en.wikipedia.org/wiki?curid=49329" title="Strait of Juan de Fuca">
Strait of Juan de Fuca

The Strait of Juan de Fuca (officially named Juan de Fuca Strait in Canada) is a large body of water about 95 mi long that is the Salish Sea's outlet to the Pacific Ocean. The international boundary between Canada and the United States runs down the center of the Strait.
It was named in 1787 by the maritime fur trader Charles William Barkley, captain of the "Imperial Eagle", for Juan de Fuca, the Greek navigator who sailed in a Spanish expedition in 1592 to seek the fabled Strait of Anián. Barkley was the first non-indigenous person to find the strait, unless Juan de Fuca's story was true. The strait was explored in detail between 1789 and 1791 by Manuel Quimper, José María Narváez, Juan Carrasco, Gonzalo López de Haro, and Francisco de Eliza.
Definition.
The USGS defines the Strait of Juan de Fuca as a channel. It extends east from the Pacific Ocean between Vancouver Island, British Columbia, and the Olympic Peninsula, Washington, to Haro Strait, San Juan Channel, Rosario Strait, and Puget Sound. The Pacific Ocean boundary is formed by a line between Cape Flattery and Tatoosh Island, Washington, and Carmanah Point (Vancouver Island), British Columbia. Its northern boundary follows the shoreline of Vancouver Island from Carmanah Point to Gonzales Point, then follows a continuous line east to Seabird Point (Discovery Island), British Columbia, Cattle Point (San Juan Island), Washington, Iceberg Point (Lopez Island), Point Colville (Lopez Island), and then to Rosario Head (Fidalgo Island). The eastern boundary runs south from Rosario Head across Deception Pass to Whidbey Island, then along the western coast of Whidbey Island to Point Partridge, then across Admiralty Inlet to Point Wilson (Quimper Peninsula). The northern coast of the Olympic Peninsula forms the southern boundary of the strait. In the eastern entrance to the Strait, the Race Rocks Archipelago is located in the high current zone half way between Port Angeles, Washington, and Victoria, BC.
Climate.
Like the rest of the Salish Sea and surrounding regions, the climate of the Strait is disputed, with the Köppen system classifying it as Mediterranean, but most regional climatologists preferring oceanic. While the climate is mostly oceanic in nature, the dry summers result in the Mediterranean classification in the Köppen system. Rainfall ranges from over 100 in (temperate rainforest) conditions at the west end to as little as 16 in at the east end, near Sequim.
Because it is exposed to the generally westerly winds and waves of the Pacific, seas and weather in Juan de Fuca Strait are, on average, rougher than in the more protected waters inland, thereby resulting in a number of small-craft advisories. A weather station provides live data from Race Rocks at http://racerocks.ca/racerock/data/weatherlink/Current_Vantage_Pro.htm
Ferries.
An international vehicle ferry crosses the Strait from Port Angeles, Washington to Victoria, British Columbia several times each day, as do passenger ferries of the Washington State Ferry system, a seasonal private ferry connecting Port Angeles with Victoria and a private high-speed ferry between Victoria and Seattle.
Boundary dispute.
This strait remains the subject of a maritime boundary dispute between Canada and the United States. The dispute is only over the seaward boundary extending 200 mi west from the mouth of the strait. The maritime boundary within the strait is not in dispute. Both governments have proposed a boundary based on the principle of equidistance, but with different basepoint selections, resulting in small differences in the line. Resolution of the issue should be simple, but has been hindered because it might influence maritime boundary issues between Canada and the United States. In addition, the government of British Columbia has rejected both equidistant proposals, instead arguing that the Juan de Fuca submarine canyon is the appropriate "geomorphic and physiogeographic boundary". The proposed equidistant boundary currently marks the northern boundary of the Olympic Coast National Marine Sanctuary. British Columbia's position is based on the principle of natural prolongation which developed in international law. It poses a dilemma for the federal government of Canada. If Canada holds that the principle of natural prolongation applies to the Juan de Fuca Canyon on its Pacific Ocean coast, the assertion could undermine Canada's argument in the "Gulf of Maine" boundary dispute. In this Atlantic Ocean context, Canada favors an outcome based on the principle of equidistance.
Salish Sea.
In March 2008, the Chemainus First Nation proposed renaming the strait the "Salish Sea", an idea that reportedly met with approval by British Columbia's Aboriginal Relations Minister Mike de Jong, who pledged to put it before the B.C. cabinet for discussion. Making "Salish Sea" official required a formal application to the Geographical Names Board of Canada. A parallel American movement promoting the name had a different definition, combining of the Strait of Juan de Fuca and Puget Sound as well as the Strait of Georgia and related waters under the more general name "Salish Sea". This latter definition was made official in 2009 by geographic boards of Canada and the United States.
In October 2009, the Washington State Board of Geographic Names approved the Salish Sea toponym, not to replace the names of the Strait of Georgia, Puget Sound, and Strait of Juan de Fuca, but instead as a collective term for all three. The British Columbia Geographical Names Office passed a resolution only recommending that the name be adopted by the Geographical Names Board of Canada, should its US counterpart approve the name-change. The United States Board on Geographic Names approved the name on November 12, 2009.
Counties and regional districts.
Counties along the Strait of Juan de Fuca:
Regional districts along the Strait of Juan de Fuca:
Fauna.
Certain groups of seabirds called common murre migrate north by swimming. Some Pacific Coast murres paddle north to the sheltered bays of the Strait of Juan de Fuca to feed on herring and other small fish.
Other.
Some scenes from the movie "The Hunt for Red October" were filmed in the Strait of Juan de Fuca in 1989.

</doc>
<doc id="49330" url="http://en.wikipedia.org/wiki?curid=49330" title="Spellevator">
Spellevator

Spellevator is an educational computer game for the Apple II computer, published by MECC (now part of Mattel's The Learning Company). It was one of the first commercial games to use the ProDOS operating system.
Summary.
The player controls a dust bunny, which is chased by several vacuum cleaners with different movement patterns. The objective of the level is to grab all the letters and exit through the upper left corner. The player can pass through an unoccupied elevator (some vacuum cleaners use elevators also) by correctly answering a spelling or vocabulary question. Once one completes a level, the player can receive a bonus by correctly unscrambling the letters one grabbed into a word).
"Spellevator" had a utility on the disk's flipside that let a user create a word list and save it to any ProDOS formatted floppy disk. This way, teachers could customize the game to fit their own particular vocabulary lists.

</doc>
<doc id="49331" url="http://en.wikipedia.org/wiki?curid=49331" title="Patchwork">
Patchwork

Patchwork or "pieced work" is a form of needlework that involves sewing together pieces of fabric into a larger design. The larger design is usually based on repeat patterns built up with different fabric shapes (which can be different colors). These shapes are carefully measured and cut, basic geometric shapes making them easy to piece together. 
Uses.
Patchwork is most often used to make quilts, but it can also be used to make bags, wall-hangings, warm jackets, cushion covers, skirts, waistcoats and other items of clothing. Some textile artists work with patchwork, often combining it with embroidery and other forms of stitchery.
When used to make a quilt, this larger patchwork or pieced design becomes the "top" of a three-layered quilt, the middle layer being the batting, and the bottom layer the backing. To keep the batting from shifting, a patchwork or pieced quilt is often quilted by hand or machine using a running stitch in order to outline the individual shapes that make up the pieced top, or the quilting stitches may be random or highly ordered overall patterns that contrast with the patchwork composition.
History.
Evidence of patchwork—piecing small pieces of fabric together to create a larger piece and quilting layers of textile fabrics together—has been found throughout history. The earliest examples have been located in Egyptian tombs and also in early age of China about 5000 years ago. Further finds have been dated from the early Middle Ages, where layers of quilted fabric were used in the construction of armor—this kept the soldiers warm and protected. Japanese armor was made in a similar fashion.
Using this technique, quilts began to appear in households of the 11th to 13th centuries. As the European climate became colder around this time, the incidence of the use bed quilts rose, and so developed the practice of embellishing a simple cloth through the creation of pattern and design, alongside the development of decorative quilting. The tradition of making quilts in this fashion was taken to America by the Pilgrims.
Americas.
Patchwork enjoyed a widespread revival during the Great Depression as a way to recycle worn clothing into warm quilts. Even very small and worn pieces of material are suitable for use in patchwork, although crafters today more often use new 100% cotton fabrics as the basis for their designs. In the US, patchwork declined after World War II, but was again revived during the American bicentennial. In the past, hand quilting was often done in a group around a frame. Instead of quilting, the layers are sometimes tied together at regular intervals with pieces of yarn, a practice known as tying or knotting, and which produces a "comforter".
Popularity.
The 2003 Quilting in America survey estimated that the total value of the American quilting industry was $2.7 billion. International quilting exhibitions attract thousands of visitors, while countless smaller exhibitions are held every weekend in local regions. Active cyber-quilting communities abound on the web; books and magazines on the subject are published in the hundreds every year; and there are many active local quilting guilds and shops in different countries. "Quilt Art" is established as a legitimate artistic medium, with quilted works of art selling for thousands of dollars to corporate buyers and galleries. Quilt historians and quilt appraisers are re-evaluating the heritage of traditional quilting and antique quilts, while superb examples of antique quilts are purchased for large sums by collectors and museums. The American Quilt Study Group is active in promotion of research on the history of quilting.
Asia.
In Indian stitching blanket using different small pieces of cloth is an art. It is popularly known as Kaudhi in Karnataka. Such blankets are given as gifts to newborn babies in some parts of Karnataka. Lambani tribes wear skirts with such art
Structure.
There are three traditional structures used to construct a patchwork or pieced composition: 1) the block, 2) overall, and 3) strip piecing. Traditional patchwork has identifying names based on the arrangement of colors and shapes.
Blocks.
Patchwork blocks are pieced squares made up of colored shapes that repeat specific shapes to create patterns within the square or block, of, say, light and dark, or contrasting colors (motif (textile arts)). The blocks can all repeat the same pattern, or blocks can have several different patterns. The patchwork blocks are typically around 8–10" square (20 cm to 25 cm). They are sewn together in stacked rows to make a larger composition. Often strips of contrasting fabric forming a lattice separate the patchwork blocks from each other. Some common patchwork block names are Log Cabin, Drunkard's Path, Bear's Paw, Tulip, and Nine Patch.
A unique form of patchwork quilt is the crazy quilt. Crazy quilting was popular during the Victorian era (mid–late 19th century). The crazy quilt is made up of random shapes of luxurious fabric such as velvets, silks, and brocades and buttons, lace, and other embellishments left over from the gowns they had made for themselves. The patchwork pieces are stitched together forming "crazy" or non-repeat, asymmetric compositions. Fancy embroidery embellishes the seam lines between the individual, pieced shapes. The crazy quilt was a status symbol, as only well-to-do women had a staff to do all the household work, and had the time to sew their crazy quilt. Traditionally, the top was left without lining or batting. Many surviving crazy quilts still have the newspaper and other foundation papers used for piecing.
Overall.
Overall patchwork designs are incrementally pieced geometric shapes stitched together to form a larger random or composed design. The colored shapes can be randomly pieced or follow a strict order to create a specific effect, e.g. value (light to dark) progressions, or checkerboard effects. Names such as Hit or Miss, Clamshell, back-stitch, needle weave, criss-cross and Starburst identify some overall patchwork structures.
Strip piecing.
Strip piecing involves stitching together pieces of fabric in repeat patterns into long strips and then stitching the strips together lengthwise. The patchwork strips can be alternated with strips of contrasting colors. A typical strip patchwork quilt is the Flying Geese pattern.
Jelly Rolls and other Pre-Cuts.
Pre-cut fabrics come in many varietys and can be used to make various styles of quilts. Pre-cuts include Jelly Rolls which were brought onto the market by the fabric company Moda. Each Jelly Roll consists of 40 strips of 44" long fabric, all in various different styles and patterns. Quilters, Pam and Nicky Lintott, have written a book on how to get the most out of a Jelly Roll. Their designs include cot quilts, play mats and wall hangings.
Forms.
Specialised forms of patchwork include:
Trends.
Today, many things are quilted using a Longarm quilting system. The system consists of a frame and a sewing machine. The patchwork, batting and backing are loaded onto the frame and in some systems each layer can be tensioned independently. No basting is usually necessary. The frames can be up to 14' long which is big enough for a king size quilt to be tensioned ready for quilting. The sewing machine known as the Longarm machine has an extended throat space, up to 36", and it can be moved on a two-axis rail system—left and right, forwards and backwards, enabling a 360-degree movement over the surface of the quilt.
Until recently, most longarm machines were hand-guided which meant the operator had to synchronise the speed of their hands with that of the machine motor. Fast hands and slow motor meant big stitches. Slow hands and fast motor meant small stitches. Since just after the turn of the century, most longarm machines are now sold with stitch-regulation, which means that the operator no longer has to synchronize hand speed with that of the motor. Electronics in the machine ensures the stitch length remains constant. More recently, fully computerized machines are being sold. Fully computerized machines have been available for over 12 years. They were invented by Paul Statler but have only recently become popular. These machines use specialised machine-driver software and CAD-type drawing packages to enable pattern digitisation and automatic quilting. An operator is still required to mind the machine and set the pattern onto the quilt.
It is thought that over 10,000 longarm quilting machines are in use today. In the US, there are many brands available and many places to obtain training and few distributors and trainers in other countries where business quilters are more likely to travel to the States for ongoing longarm training.

</doc>
<doc id="49332" url="http://en.wikipedia.org/wiki?curid=49332" title="Needlework">
Needlework

Needlework is a broad term for the handicrafts of decorative sewing and textile arts. Anything that uses a needle for construction can be called needlework. The definition may expand to include related textile crafts such as a crochet hook or tatting shuttles.
Similar abilities often transfer well between different varieties of needlework, such as fine motor skill and a knowledge of textile fibers. Some of the same tools may be used in several different varieties of needlework. For instance, a needle threader is useful in nearly all needlecrafts.

</doc>
<doc id="49333" url="http://en.wikipedia.org/wiki?curid=49333" title="Cultural bias">
Cultural bias

Cultural bias is the phenomenon of interpreting and judging phenomena by standards inherent to one's own culture. The phenomenon is sometimes considered a problem central to social and human sciences, such as economics, psychology, anthropology, and sociology. Some practitioners of the aforementioned fields have attempted to develop methods and theories to compensate for or a culture make assumptions about conventions, including conventions of language, notation, proof and evidence. They are then accused of mistaking these assumptions for laws of logic or nature. Numerous such biases exist, concerning cultural norms for color, location of body parts, mate selection, concepts of justice, linguistic and logical validity, acceptability of evidence, and taboos. Cultural bias extends on many more fields in the globalizing world. Ordinary people may tend to imagine other people as basically the same, not significantly more or less valuable, probably attached emotionally to different groups and different land.
Examples.
People who read English often assume that it is natural to scan a visual field from left to right and from top to bottom. In the United States it is typical for the "on" position of a toggle switch to be "up", whereas in the UK, Australia, and New Zealand it is "down." Also, in these countries, North is the top of a map, up is usually the larger quantity and better, as well. As another example, Japanese do not place an X in a check-box to indicate acceptance—this indicates refusal.
These conventions are generally useful, as once one is used to light switches behaving a certain way one does not need to learn a per-light switch rule but just a general rule. Unfortunately, when people move between cultures or design something for a different group they often do not attend to which conventions remain and which change.
Linguistic and ethnic groups often do not share these notational assumptions. Notational and operative assumptions can change control systems if the users implement, from a different culture than the designers, funnel interpretations from their original world view. Safety-critical systems, according to Seidner (pp. 5–7), become responses to threats of control. Through the emergence of majority and minority categories in society, cultural biases ensue.
Cultural bias has been observed against African American minorities in standardized SAT tests.
Cultural bias in health care.
Many studies show that minorities receive less adequate and less intensive health care than whites. Such disparities persist even after taking into account health insurance status, age, sex, income, and education. Racial and ethnic disparities in health care access and quality have been extensively documented. Cultural bias in health care are of great concern, with much attention focused on the potential for unconscious (implicit) bias. Disparities in health care does not exist only in differences in access and patient preferences, they exist in the broader historical and contemporary context of social and economic inequality, prejudice, and systematic bias. Researches and works show that ethnic minorities rate the quality of interpersonal care by physicians and within the healthcare system in general more negatively than whites.
Moreover, Black, Hispanic, and Asian people are more likely than White people to say that they would receive better medical treatment if they were from different background. Medical personnel treat them with less respect because of their race or broken English.
There is also a relationship between cultural bias and mental health care as well. According to a study found in the Journal of Counseling and Development, diagnoses of clients change when the race between counselor and client do not match. When addressing a minority client, the counselor seemed to diagnose them with a more serious prognosis than with a white client. Also, when discussing a prognosis, many counselors did not take into account cultural factors, which in turn changed their clinical-decision.
Cultural bias in testing.
Cultural bias in testing refers to a situation in which a given test is inappropriate for a certain audience as it does not test the student's actual knowledge of a taught subject or includes details tied to a culture that the student is unfamiliar with. Typically, test biases are based on group membership of the test-takers, such as gender, race and ethnicity. Bias is evident both in the examiner and in testing materials. SAT scores are consistently correlated with race, ethnicity, and class.The test bias controversy and debate has its origins in the observed differences in average IQ scores between various racial groups and ethnic groups in the early 1900s. Studies showed that minorities usually score lower than white students and higher-class students. One of the reason is that higher-class students are able to pay for preparation in order to increase scores. In fact, evidence exists that negative stereotypes of minority groups play an important role in explaining at least some of the differences in test scores— this is called stereotype threat, a particular form of test anxiety. For example, the stereotype that African Americans perform poorly on standardized tests hinders many African Americans' testing ability. Researchers have identified three types of test bias that have an effect on the accuracy of the test result: construct bias, method bias and item bias. Construct Bias happens when the results are significantly different for test-takers from the original culture for which the test was developed and test-takers from a different culture. Method Bias refers to all factors surrounding the administration of the test that may influence the results of the test(length of test, temperature in the testing room, testing environment etc.) Item Bias can occur because of poor use of grammar, choice of cultural phrases and poorly written assessment items.
Culturally biased assumptions.
Assumptions that people tend to make about people from different ethnic, racial and cultural backgrounds may bias perception of other cultures. Such assumptions can be just general bias assumptions about people who are not from one’s own background. This is related to prejudice, discrimination, and stereotyping, however, cultural biases are different in that these other practices are usually applied and not just thought. Also, cultural biased assumptions can be related to a particular cultural group. There are two different types of biases, according to the Journal of Law Enforcement, and they are subtle and blatant biases.
Subtle biases are found when. An individual may appear to be unbiased and even consider him or herself to be unbiased. However, when an issue becomes personal he or she is likely to have at least slight preferences on the matter. This means although an individual may not seem or realize they have a bias against a certain culture, they could have engrained biases that change their perceptions on certain topics. For example, many Americans accepted Barack Obama as our president despite his race, however a portion of the population becomes apprehensive about a minority moving into a suburban area.
Blatant biases announce an individual’s displeasure with someone or something. These biases are founded on the core social motive of belonging. In order to fit in with the "in" group, individuals must display the same beliefs as the group, no matter who is hurt by the display. This is found in many discriminatory groups, such as the KKK, where initiation is through the blatant hatred of cultures other than their own. Blatant biases are often found in action with hate crimes.
Cultural bias in the classroom.
Cultural bias in teaching can be described as teachers and administrators holding the belief that the dominant or mainstream (presumably European and North American) cultural ways of learning and knowing are superior to ways of learning and knowing that do not reflect such a culture. (http://www.education.com/reference/article/cultural-bias-in-teaching/) A culture gap can develop in some classrooms where a white teacher has a majority of students of color. Cultural bias beliefs sanction as appropriate certain forms of classroom behavior, including the manner in which a student is to perform and learn during class time.
Most elementary and secondary U.S. history textbooks offer a romanticized view of the Europeans’ experience in the United States whereas most of the experiences of Native Americans and/or Africans in these same lands are either misrepresented or underrepresented. 
What results from these culturally biased beliefs is an in-school cultural socialization process in which ethnically and culturally diverse students are exposed to instructional practices and learning activities that do not reflect their cultural-laden modes of learning and knowing.
Linguistic discrimination.
The unfair treatment of an individual based solely on their use of language. This use of language may include the individual's native language or other characteristics of the person's speech, such as an accent, the size of vocabulary (whether the person uses complex and varied words), and syntax.
Cultural bias in psychology.
Most psychological research is carried out on Americans. Smith and Bond (1998) analysed one textbook, which showed that 66% of the studies were American, 32% European and 2% the rest of the world. Sears (1986) reported that 82% of psychological research studies used undergraduates and 51% were Psychology students.
Many psychological theories are hampered by cultural bias, which can ultimately negate their validity. Cultural bias can appear in two forms; ethnocentrism and eurocentrism
Ethnocentrism- the belief that ones own cultural group is superior. Ethnocentric individuals judge other groups relative to their own ethnic group or culture, especially with concern for language, behavior, customs, and religion.
Eurocentrism- is the practice of viewing the world from a European perspective and with an implied belief, either consciously or subconsciously, in the preeminence of European culture. Western research is then applied to other cultures to create a supposedly universal view of human behaviour
Research studies can be culturally biased in several ways:

</doc>
<doc id="49338" url="http://en.wikipedia.org/wiki?curid=49338" title="Bipolar junction transistor">
Bipolar junction transistor

Schematic symbols forPNP- and NPN-typeBJTs.
A bipolar junction transistor (BJT or bipolar transistor) is a type of transistor that relies on the contact of two types of semiconductor for its operation. BJTs can be used as amplifiers, switches, or in oscillators. BJTs can be found either as individual discrete components, or in large numbers as parts of integrated circuits.
"Bipolar" transistors are so named because their operation involves both electrons and holes. These two kinds of charge carriers are characteristic of the two kinds of doped semiconductor material; electrons are majority charge carriers in n-type semiconductors, whereas holes are majority charge carriers in p-type semiconductors. In contrast, unipolar transistors such as the field-effect transistors have only one kind of charge carrier.
Charge flow in a BJT is due to diffusion of charge carriers across a junction between two regions of different charge concentrations. The regions of a BJT are called "emitter", "collector", and "base". A discrete transistor has three leads for connection to these regions. Typically, the emitter region is heavily doped compared to the other two layers, whereas the majority charge carrier concentrations in base and collector layers are about the same. By design, most of the BJT collector current is due to the flow of charges injected from a high-concentration emitter into the base where there are minority carriers that diffuse toward the collector, and so BJTs are classified as minority-carrier devices.
Introduction.
BJTs come in two types, or polarities, known as PNP and NPN based on the doping types of the three main terminal regions. An NPN transistor comprises two semiconductor junctions that share a thin p-doped anode region, and a PNP transistor comprises two semiconductor junctions that share a thin n-doped cathode region.
In typical operation, the base–emitter junction is forward biased, which means that the p-doped side of the junction is at a more positive potential than the n-doped side, and the base–collector junction is reverse biased. In an NPN transistor, when positive bias is applied to the base–emitter junction, the equilibrium is disturbed between the thermally generated carriers and the repelling electric field of the n-doped emitter depletion region. This allows thermally excited electrons to inject from the emitter into the base region. These electrons diffuse through the base from the region of high concentration near the emitter towards the region of low concentration near the collector. The electrons in the base are called "minority carriers" because the base is doped p-type, which makes holes the "majority carrier" in the base.
To minimize the percentage of carriers that recombine before reaching the collector–base junction, the transistor's base region must be thin enough that carriers can diffuse across it in much less time than the semiconductor's minority carrier lifetime. In particular, the thickness of the base must be much less than the of the electrons. The collector–base junction is reverse-biased, and so little electron injection occurs from the collector to the base, but electrons that diffuse through the base towards the collector are swept into the collector by the electric field in the depletion region of the collector–base junction. The thin "shared" base and asymmetric collector–emitter doping is what differentiates a bipolar transistor from two "separate" and oppositely biased diodes connected in series.
Voltage, current, and charge control.
The collector–emitter current can be viewed as being controlled by the base–emitter current (current control), or by the base–emitter voltage (voltage control). These views are related by the current–voltage relation of the base–emitter junction, which is just the usual exponential current–voltage curve of a p-n junction (diode).
The physical explanation for collector current is the amount of minority carriers in the base region. Due to low level injection (in which there are much fewer excess carriers than normal majority carriers) the ambipolar transport rates (in which the excess majority and minority carriers flow at the same rate) is in effect determined by the excess minority carriers.
Detailed transistor models of transistor action, such as the Gummel–Poon model, account for the distribution of this charge explicitly to explain transistor behaviour more exactly. The charge-control view easily handles phototransistors, where minority carriers in the base region are created by the absorption of photons, and handles the dynamics of turn-off, or recovery time, which depends on charge in the base region recombining. However, because base charge is not a signal that is visible at the terminals, the current- and voltage-control views are generally used in circuit design and analysis.
In analog circuit design, the current-control view is sometimes used because it is approximately linear. That is, the collector current is approximately formula_1 times the base current. Some basic circuits can be designed by assuming that the emitter–base voltage is approximately constant, and that collector current is beta times the base current. However, to accurately and reliably design production BJT circuits, the voltage-control (for example, Ebers–Moll) model is required. The voltage-control model requires an exponential function to be taken into account, but when it is linearized such that the transistor can be modelled as a transconductance, as in the Ebers–Moll model, design for circuits such as differential amplifiers again becomes a mostly linear problem, so the voltage-control view is often preferred. For translinear circuits, in which the exponential I–V curve is key to the operation, the transistors are usually modelled as voltage controlled with transconductance proportional to collector current. In general, transistor level circuit design is performed using SPICE or a comparable analog circuit simulator, so model complexity is usually not of much concern to the designer.
Turn-on, turn-off, and storage delay.
The Bipolar transistor exhibits a few delay characteristics when turning on and off. Most transistors, and especially power transistors, exhibit long base-storage times that limit maximum frequency of operation in switching applications. One method for reducing this storage time is by using a Baker clamp.
Transistor parameters: alpha (α) and beta (β).
The proportion of electrons able to cross the base and reach the collector is a measure of the BJT efficiency. The heavy doping of the emitter region and light doping of the base region causes many more electrons to be injected from the emitter into the base than holes to be injected from the base into the emitter.
The "common-emitter current gain" is represented by βF or the h-parameter hFE; it is approximately the ratio of the DC collector current to the DC base current in forward-active region. It is typically greater than 50 for small-signal transistors but can be smaller in transistors designed for high-power applications.
Another important parameter is the "common-base current gain", αF. The common-base current gain is approximately the gain of current from emitter to collector in the forward-active region. This ratio usually has a value close to unity; between 0.98 and 0.998. It is less than unity due to recombination of charge carriers as they cross the base region.
Alpha and beta are more precisely related by the following identities (NPN transistor):
Structure.
A BJT consists of three differently doped semiconductor regions: the "emitter" region, the "base" region and the "collector" region. These regions are, respectively, "p" type, "n" type and "p" type in a PNP transistor, and "n" type, "p" type and "n" type in an NPN transistor. Each semiconductor region is connected to a terminal, appropriately labeled: "emitter" (E), "base" (B) and "collector" (C).
The "base" is physically located between the "emitter" and the "collector" and is made from lightly doped, high resistivity material. The collector surrounds the emitter region, making it almost impossible for the electrons injected into the base region to escape without being collected, thus making the resulting value of α very close to unity, and so, giving the transistor a large β. A cross section view of a BJT indicates that the collector–base junction has a much larger area than the emitter–base junction.
The bipolar junction transistor, unlike other transistors, is usually not a symmetrical device. This means that interchanging the collector and the emitter makes the transistor leave the forward active mode and start to operate in reverse mode. Because the transistor's internal structure is usually optimized for forward-mode operation, interchanging the collector and the emitter makes the values of α and β in reverse operation much smaller than those in forward operation; often the α of the reverse mode is lower than 0.5. The lack of symmetry is primarily due to the doping ratios of the emitter and the collector. The emitter is heavily doped, while the collector is lightly doped, allowing a large reverse bias voltage to be applied before the collector–base junction breaks down. The collector–base junction is reverse biased in normal operation. The reason the emitter is heavily doped is to increase the emitter injection efficiency: the ratio of carriers injected by the emitter to those injected by the base. For high current gain, most of the carriers injected into the emitter–base junction must come from the emitter.
The low-performance "lateral" bipolar transistors sometimes used in CMOS processes are sometimes designed symmetrically, that is, with no difference between forward and backward operation.
Small changes in the voltage applied across the base–emitter terminals causes the current that flows between the "emitter" and the "collector" to change significantly. This effect can be used to amplify the input voltage or current. BJTs can be thought of as voltage-controlled current sources, but are more simply characterized as current-controlled current sources, or current amplifiers, due to the low impedance at the base.
Early transistors were made from germanium but most modern BJTs are made from silicon. A significant minority are also now made from gallium arsenide, especially for very high speed applications (see HBT, below).
NPN.
NPN is one of the two types of bipolar transistors, consisting of a layer of P-doped semiconductor (the "base") between two N-doped layers. A small current entering the base is amplified to produce a large collector and emitter current. That is, when there is a positive potential difference measured from the emitter of an NPN transistor to its base (i.e., when the base is high relative to the emitter) as well as positive potential difference measured from the base to the collector, the transistor becomes active. In this "on" state, current flows between the collector and emitter of the transistor. Most of the current is carried by electrons moving from emitter to collector as minority carriers in the P-type base region. To allow for greater current and faster operation, most bipolar transistors used today are NPN because electron mobility is higher than hole mobility.
A mnemonic device for the NPN transistor symbol is ""n"ot "p"ointing i"n"", based on the arrows in the symbol and the letters in the name.
PNP.
The other type of BJT is the PNP, consisting of a layer of N-doped semiconductor between two layers of P-doped material. A small current leaving the base is amplified in the collector output. That is, a PNP transistor is "on" when its base is pulled low relative to the emitter.
The arrows in the NPN and PNP transistor symbols are on the emitter legs and point in the direction of the conventional current flow when the device is in forward active mode.
A mnemonic device for the PNP transistor symbol is ""p"ointing i"n" ("p"roudly/"p"ermanently)", based on the arrows in the symbol and the letters in the name.
Heterojunction bipolar transistor.
The heterojunction bipolar transistor (HBT) is an improvement of the BJT that can handle signals of very high frequencies up to several hundred GHz. It is common in modern ultrafast circuits, mostly RF systems.
Heterojunction transistors have different semiconductors for the elements of the transistor. Usually the emitter is composed of a larger bandgap material than the base. The figure shows that this difference in bandgap allows the barrier for holes to inject backward from the base into the emitter, denoted in the figure as Δφp, to be made large, while the barrier for electrons to inject into the base Δφn is made low. This barrier arrangement helps reduce minority carrier injection from the base when the emitter-base junction is under forward bias, and thus reduces base current and increases emitter injection efficiency.
The improved injection of carriers into the base allows the base to have a higher doping level, resulting in lower resistance to access the base electrode. In the more traditional BJT, also referred to as homojunction BJT, the efficiency of carrier injection from the emitter to the base is primarily determined by the doping ratio between the emitter and base, which means the base must be lightly doped to obtain high injection efficiency, making its resistance relatively high. In addition, higher doping in the base can improve figures of merit like the Early voltage by lessening base narrowing.
The grading of composition in the base, for example, by progressively increasing the amount of germanium in a SiGe transistor, causes a gradient in bandgap in the neutral base, denoted in the figure by ΔφG, providing a "built-in" field that assists electron transport across the base. That drift component of transport aids the normal diffusive transport, increasing the frequency response of the transistor by shortening the transit time across the base.
Two commonly used HBTs are silicon–germanium and aluminum gallium arsenide, though a wide variety of semiconductors may be used for the HBT structure. HBT structures are usually grown by epitaxy techniques like MOCVD and MBE.
Regions of operation.
Bipolar transistors have five distinct regions of operation, defined by BJT junction biases.
The modes of operation can be described in terms of the applied voltages (this description applies to NPN transistors; polarities are reversed for PNP transistors):
In terms of junction biasing:
Although these regions are well defined for sufficiently large applied voltage, they overlap somewhat for small (less than a few hundred millivolts) biases. For example, in the typical grounded-emitter configuration of an NPN BJT used as a pulldown switch in digital logic, the "off" state never involves a reverse-biased junction because the base voltage never goes below ground; nevertheless the forward bias is close enough to zero that essentially no current flows, so this end of the forward active region can be regarded as the cutoff region.
Active-mode NPN transistors in circuits.
The diagram shows a schematic representation of an NPN transistor connected to two voltage sources. To make the transistor conduct appreciable current (on the order of 1 mA) from C to E, "V"BE must be above a minimum value sometimes referred to as the cut-in voltage. The cut-in voltage is usually about 650 mV for silicon BJTs at room temperature but can be different depending on the type of transistor and its biasing. This applied voltage causes the lower P-N junction to 'turn on', allowing a flow of electrons from the emitter into the base. In active mode, the electric field existing between base and collector (caused by "V"CE) will cause the majority of these electrons to cross the upper P-N junction into the collector to form the collector current "I"C. The remainder of the electrons recombine with holes, the majority carriers in the base, making a current through the base connection to form the base current, "I"B. As shown in the diagram, the emitter current, "I"E, is the total transistor current, which is the sum of the other terminal currents, (i.e., "I"E = "I"B + "I"C).
In the diagram, the arrows representing current point in the direction of conventional current – the flow of electrons is in the opposite direction of the arrows because electrons carry negative electric charge. In active mode, the ratio of the collector current to the base current is called the "DC current gain". This gain is usually 100 or more, but robust circuit designs do not depend on the exact value (for example see op-amp). The value of this gain for DC signals is referred to as formula_6, and the value of this gain for small signals is referred to as formula_7. That is, when a small change in the currents occurs, and sufficient time has passed for the new condition to reach a steady state formula_7 is the ratio of the change in collector current to the change in base current. The symbol formula_9 is used for both formula_6 and formula_7.
The emitter current is related to formula_12 exponentially. At room temperature, an increase in formula_12 by approximately 60 mV increases the emitter current by a factor of 10. Because the base current is approximately proportional to the collector and emitter currents, they vary in the same way.
Active-mode PNP transistors in circuits.
The diagram shows a schematic representation of a PNP transistor connected to two voltage sources. To make the transistor conduct appreciable current (on the order of 1 mA) from E to C, formula_14 must be above a minimum value sometimes referred to as the cut-in voltage. The cut-in voltage is usually about 650 mV for silicon BJTs at room temperature but can be different depending on the type of transistor and its biasing. This applied voltage causes the upper P-N junction to 'turn-on' allowing a flow of holes from the emitter into the base. In active mode, the electric field existing between the emitter and the collector (caused by formula_15) causes the majority of these holes to cross the lower p-n junction into the collector to form the collector current formula_16. The remainder of the holes recombine with electrons, the majority carriers in the base, making a current through the base connection to form the base current, formula_17. As shown in the diagram, the emitter current, formula_18, is the total transistor current, which is the sum of the other terminal currents (i.e., "I"E = "I"B + "I"C).
In the diagram, the arrows representing current point in the direction of conventional current – the flow of holes is in the same direction of the arrows because holes carry positive electric charge. In active mode, the ratio of the collector current to the base current is called the "DC current gain". This gain is usually 100 or more, but robust circuit designs do not depend on the exact value. The value of this gain for DC signals is referred to as formula_6, and the value of this gain for AC signals is referred to as formula_7. However, when there is no particular frequency range of interest, the symbol formula_9 is used.
It should also be noted that the emitter current is related to formula_14 exponentially. At room temperature, an increase in formula_14 by approximately 60 mV increases the emitter current by a factor of 10. Because the base current is approximately proportional to the collector and emitter currents, they vary in the same way.
History.
The bipolar point-contact transistor was at the Bell Telephone Laboratories by John Bardeen and Walter Brattain under the direction of William Shockley. The junction version known as the bipolar junction transistor, , enjoyed three decades as the device of choice in the design of discrete and integrated circuits. Nowadays, the use of the BJT has declined in favor of CMOS technology in the design of digital integrated circuits. The incidental low performance BJTs inherent in CMOS ICs, however, are often utilized as bandgap voltage reference, silicon bandgap temperature sensor and to handle electrostatic discharge.
Germanium transistors.
The germanium transistor was more common in the 1950s and 1960s, and while it exhibits a lower "cut off" voltage, typically around 0.2 V, making it more suitable for some applications, it also has a greater tendency to exhibit thermal runaway.
Early manufacturing techniques.
Various methods of manufacturing bipolar transistors were developed.
Theory and modeling.
Transistors can be thought of as two diodes (P–N junctions) sharing a common region that minority carriers can move through. A PNP BJT will function like two diodes that share an N-type cathode region, and the NPN like two diodes sharing a P-type anode region. Connecting two diodes with wires will not make a transistor, since minority carriers will not be able to get from one P–N junction to the other through the wire.
Both types of BJT function by letting a small current input to the base control an amplified output from the collector. The result is that the transistor makes a good switch that is controlled by its base input. The BJT also makes a good amplifier, since it can multiply a weak input signal to about 100 times its original strength. Networks of transistors are used to make powerful amplifiers with many different applications. In the discussion below, focus is on the NPN bipolar transistor. In the NPN transistor in what is called active mode, the base–emitter voltage formula_12 and collector–base voltage formula_25 are positive, forward biasing the emitter–base junction and reverse-biasing the collector–base junction. In the active mode of operation, electrons are injected from the forward biased n-type emitter region into the p-type base where they diffuse as minority carriers to the reverse-biased n-type collector and are swept away by the electric field in the reverse-biased collector–base junction. For a figure describing forward and reverse bias, see semiconductor diodes.
Large-signal models.
In 1954 Jewell James Ebers and John L. Moll introduced their mathematical model of transistor currents:
Ebers–Moll model.
The DC emitter and collector currents in active mode are well modeled by an approximation to the Ebers–Moll model:
The base internal current is mainly by diffusion (see Fick's law) and
where
The formula_38 and forward formula_9 parameters are as described previously. A reverse formula_9 is sometimes included in the model.
The unapproximated Ebers–Moll equations used to describe the three currents in any operating region are given below. These equations are based on the transport model for a bipolar junction transistor.
where
Base-width modulation.
As the collector–base voltage (formula_53) varies, the collector–base depletion region varies in size. An increase in the collector–base voltage, for example, causes a greater reverse bias across the collector–base junction, increasing the collector–base depletion region width, and decreasing the width of the base. This variation in base width often is called the "Early effect" after its discoverer James M. Early.
Narrowing of the base width has two consequences:
Both factors increase the collector or "output" current of the transistor in response to an increase in the collector–base voltage.
In the forward-active region, the Early effect modifies the collector current (formula_44) and the forward common emitter current gain (formula_1) as given by:
where:
Punchthrough.
When the base–collector voltage reaches a certain (device specific) value, the base–collector depletion region boundary meets the base–emitter depletion region boundary. When in this state the transistor effectively has no base. The device thus loses all gain when in this state.
Gummel–Poon charge-control model.
The Gummel–Poon model is a detailed charge-controlled model of BJT dynamics, which has been adopted and elaborated by others to explain transistor dynamics in greater detail than the terminal-based models typically do . This model also includes the dependence of transistor formula_9-values upon the direct current levels in the transistor, which are assumed current-independent in the Ebers–Moll model.
Small-signal models.
hybrid-pi model.
The hybrid-pi model is a popular circuit model used for analyzing the small signal behavior of bipolar junction and field effect transistors. Sometimes it is also called "Giacoletto model" because it was introduced by L.J. Giacoletto in 1969. The model can be quite accurate for low-frequency circuits and can easily be adapted for higher frequency circuits with the addition of appropriate inter-electrode capacitances and other parasitic elements.
h-parameter model.
Another model commonly used to analyze BJT circuits is the "h-parameter" model, closely related to the hybrid-pi model and the y-parameter two-port, but using input current and output voltage as independent variables, rather than input and output voltages. This two-port network is particularly suited to BJTs as it lends itself easily to the analysis of circuit behaviour, and may be used to develop further accurate models. As shown, the term "x" in the model represents a different BJT lead depending on the topology used. For common-emitter mode the various symbols take on the specific values as:
and the h-parameters are given by:
As shown, the h-parameters have lower-case subscripts and hence signify AC conditions or analyses. For DC conditions they are specified in upper-case. For the CE topology, an approximate h-parameter model is commonly used which further simplifies the circuit analysis. For this the "h"oe and "h"re parameters are neglected (that is, they are set to infinity and zero, respectively). It should also be noted that the h-parameter model as shown is suited to low-frequency, small-signal analysis. For high-frequency analyses the inter-electrode capacitances that are important at high frequencies must be added.
Etymology of hFE.
The 'h' refers to its being an h-parameter, a set of parameters named for their origin in a "hybrid equivalent circuit" model. 'F' is from" forward current amplification" also called the current gain. 'E' refers to the transistor operating in a "common emitter" (CE) configuration. Capital letters used in the subscript indicate that hFE refers to a direct current circuit.
Industry models.
The Gummel Poon SPICE model is often used, but it suffers from several limitations. These have been addressed in various more advanced models: Mextram, VBIC, HICUM, Modella.
Applications.
The BJT remains a device that excels in some applications, such as discrete circuit design, due to the very wide selection of BJT types available, and because of its high transconductance and output resistance compared to MOSFETs. The BJT is also the choice for demanding analog circuits, especially for very-high-frequency applications, such as radio-frequency circuits for wireless systems. Bipolar transistors can be combined with MOSFETs in an integrated circuit by using a BiCMOS process of wafer fabrication to create circuits that take advantage of the application strengths of both types of transistor.
Amplifiers.
The α and β characterizes the current gain of the BJT. It is this gain that allow BJTs to be used as the building blocks of electronic amplifiers. The three main BJT amplifier topologies are
Temperature sensors.
Because of the known temperature and current dependence of the forward-biased base–emitter junction voltage, the BJT can be used to measure temperature by subtracting two voltages at two different bias currents in a known ratio .
Logarithmic converters.
Because base–emitter voltage varies as the log of the base–emitter and collector–emitter currents, a BJT can also be used to compute logarithms and anti-logarithms. A diode can also perform these nonlinear functions but the transistor provides more circuit flexibility.
Vulnerabilities.
Exposure of the transistor to ionizing radiation causes radiation damage. Radiation causes a buildup of 'defects' in the base region that act as recombination centers. The resulting reduction in minority carrier lifetime causes gradual loss of gain of the transistor.
Power BJTs are subject to a failure mode called secondary breakdown, in which excessive current and normal imperfections in the silicon die cause portions of the silicon inside the device to become disproportionately hotter than the others. The doped silicon has a negative temperature coefficient, meaning that it conducts more current at higher temperatures. Thus, the hottest part of the die conducts the most current, causing its conductivity to increase, which then causes it to become progressively hotter again, until the device fails internally. The thermal runaway process associated with secondary breakdown, once triggered, occurs almost instantly and may catastrophically damage the transistor package.
If the emitter-base junction is reverse biased into avalanche or Zener mode and current flows for a short period of time, the current gain of the BJT will be permanently degraded.

</doc>
<doc id="49340" url="http://en.wikipedia.org/wiki?curid=49340" title="Doping">
Doping

Doping may refer to:

</doc>
<doc id="49351" url="http://en.wikipedia.org/wiki?curid=49351" title="Trucker's hitch">
Trucker's hitch

The trucker's hitch is a compound knot commonly used for securing loads on trucks or trailers. This general arrangement, using loops and turns in the rope itself to form a crude block and tackle, has long been used to tension lines and is known by multiple names. Knot author Geoffrey Budworth claims the knot can be traced back to the days when carters and hawkers used horse-drawn conveyances to move their wares from place to place.
Variations.
The portion of the trucker's hitch which differs in the following variations is the method used to form the loop which the working end slides through to produce the mechanical advantage. The different methods of forming the loop affect the ease and speed of tying and releasing and the stability of the final product.
The variations are presented in order of increasing stability.
Sheepshank style loop.
This version of the knot uses a sheepshank-like construction, in this kind of application also known as a "bell ringer's knot", to form the loop. It is quicker to make than a fixed loop, but is less dependable. It is avoided in critical applications (such as securing a load on a truck) as it can fall apart under too little load or too much load, and can capsize if not dressed properly. However, this knot may be made secure by adding a Half Hitch by using the top bight of the Sheepshank. This form of the trucker's hitch is least likely to jam, coming apart easily once tension is released. Different sources show slight variations in the way the sheepshank portion is formed and dressed.
Slipped overhand loop.
The loop formed in this version is a simple Slipped Overhand Loop or a variation using multiple turns of rope to form the eye of the loop. If extra loops are used to form the eye it tends to ease untying. In order to prevent the closing of the loop under load, the loop must be formed by the working end of the rope (which will later pass through the loop). If the standing end goes through the loop, it will close under load.
Fixed loop.
The most reliable common variation uses a fixed loop, such as an alpine butterfly loop, artillery loop, figure-eight loop, or another of many suitable loop knots. If a fixed loop is used repeatedly for tying the trucker's hitch in the same portion of rope, excessive wear or other damage may be suffered by the portion of the loop which working end slides against.
Finishing the hitch.
In tightening the trucker's hitch, tension can be effectively increased by repeatedly pulling sideways while preventing the tail end from slipping through the loop, and then cinching the knot tighter as the sideways force is released. This is called "sweating a line".
Once tight, the trucker's hitch is often secured with a half hitch, usually slipped for easy releasing and to avoid the necessity of access to the end of the rope, though a more secure finish, such as two half-hitches, may be called for. Under large loads, the finishing half hitch can jam, especially if it is not slipped; the difficulty of releasing it can be compounded by the fact that the knot is typically still under tension when it is untied.
Mechanical advantage and friction.
All common variations of the trucker's hitch use a loop in the standing part of the rope and the anchor point as makeshift pulleys in order to "theoretically" obtain a 3 to 1 mechanical advantage while pulling on the working end.
There is sometimes confusion about how much theoretical mechanical advantage is provided by the trucker's hitch. If the trucker's hitch were to be used as in the pulley diagram at right, to lift a weight off the floor, the theoretical mechanical advantage would be only 2:1. However in the common use of the trucker's hitch, a static hook, ring, or rail, serves as the lower pulley, and the rope across the top of the load is the portion being tensioned. Thus, the standing part of the rope is represented by the top anchor point in the diagram, and the theoretical ratio is indeed 3:1 when the working end is tensioned. That is, in a frictionless system, every unit of force exerted on the working end would produce 3 units in the standing part of the rope over the load. In the typical use of the trucker's hitch, where it is used to tighten a rope over a load, when the end is secured to the loop of the Truckers hitch and let go, the tension in the two segments of rope around the ring will rise 50%, unless the rope slackens when it is being tied off, in which case the tension may drop to any value or even zero if enough slack is allowed. But when the trucker's hitch is used as in the diagram, after tying off, the load on the attachment point above the top pulley will drop to 400 lb and the tension in the two lines going to the lower pulley will not change.
Theoretical considerations aside, in real world use the mechanical advantage of the trucker's hitch is significantly less than the ideal case due to the effects of friction. Friction has been reported to reduce the mechanical advantage from 3 to 1, to well less than 2 to 1 in many cases. One advantage of the friction within the trucker's hitch, compared to a hypothetical pulley-based system, is that it allows the hitch to be held taut with less force while the working end is secured.

</doc>
<doc id="49352" url="http://en.wikipedia.org/wiki?curid=49352" title="Hangman's knot">
Hangman's knot

The hangman's knot or hangman's noose (also known as a collar during the Elizabethan era) is a well-known knot most often associated with its use in hanging a person. For a hanging, the knot of the rope is typically placed under or just behind the left ear. When the condemned drops to the end of the rope, the force is supposed to break the neck. The knot is non-jamming but tends to resist attempts to loosen it. 
Number of coils.
Each additional coil adds friction to the knot, which makes the noose harder to pull closed or open. The number of coils should therefore be adjusted depending on the intended use, the type and thickness of rope, and environmental conditions such as wet or greasy rope. Six to eight loops are normal when using natural ropes. One coil makes it equivalent to the simple slip knot.
The number thirteen was thought to be unlucky. Consequently, thirteen coils were found in a hangman’s noose, a foreboding sign for those convicted to be hanged.
Woody Guthrie sings of the hangman using thirteen coils:
<poem>Did you ever see a hangman tie a hangknot?
I've seen it many a time and he winds, he winds,
After thirteen times he's got a hangknot.</poem>
Other uses.
A variation of this knot is used in fishing and is called the Uni-knot. It is used to tie fishing line to terminal tackle, join two pieces of line, or for snelling hooks. It is especially useful when used with slick braided line as more coils can be added to increase the friction of the knot and will not let the knot pull out. It is also useful in that the knot can be pulled down tight to the lure or it can be left with a larger loop that gives the lure more freedom of movement. The hangman's noose can also be used in boating to secure an eyelet on a rope or sheet without splicing it.

</doc>
<doc id="49355" url="http://en.wikipedia.org/wiki?curid=49355" title="Thief knot">
Thief knot

The Thief knot resembles the reef knot except that the free, or bitter ends are on opposite sides. It is said that sailors would secure their belongings in a using the thief knot, often with the ends hidden. If another sailor went through the bag, the odds were high the thief would tie the bag back using the more common reef knot, revealing the tampering, hence the name. It is difficult to tie by mistake, unlike the granny knot. 
The thief knot is much less secure than the already insecure reef knot. It unties itself if the lines are pulled when the same action would seize a reef knot. 

</doc>
<doc id="49364" url="http://en.wikipedia.org/wiki?curid=49364" title="Turner syndrome">
Turner syndrome

Turner syndrome (TS) also known as Ullrich–Turner syndrome, gonadal dysgenesis, and 45,X, is a condition in which a female is partly or completely missing an X chromosome. Signs and symptoms vary among those affected. Often there is a short and webbed neck, low-set ears, low hairline at the back of the neck, short stature, and swollen hands and feet at birth. Typically they are without menstrual periods, do not develop breasts, and are unable to have children. Heart defects, diabetes, and low thyroid hormone occur more frequently. Most people with TS have normal intelligence. Many, however, have troubles with spatial visualization such as that needed for mathematics. Vision and hearing problems occur more often.
Turner syndrome is not usually inherited from a person's parents. There are no known environmental risks and the mother's age does not play a role. Turner syndrome is due to a chromosomal abnormality in which all or part of one of the X chromosomes is missing or altered. While most people have 46 chromosomes, people with TS usually only have 45. The chromosomal abnormality may be present in just some cells in which case it is known as TS with mosaicism. In these cases the symptoms are usually fewer and possibly there are none at all. Diagnosis is based on physical signs and genetic testing.
There is no cure for Turner syndrome. Treatment, however, may help with symptoms. Human growth hormone injections during childhood may increase adult height. Estrogen replacement therapy can promote development of the breasts and hips. Medical care is often required to manage other health problems with which TS is associated.
Turner syndrome occurs in between 1 in 2000 to 1 in 5000 females at birth. All regions of the world and cultures are affected about equally. People with TS have a shorter life expectancy, mostly due to heart problems and diabetes. Henry Turner first described the condition in 1938. In 1964 it was determined to be due to a chromosomal abnormality.
Signs and symptoms.
The following is a list of common symptoms of Turner syndrome. It is important to note that an individual may have any combination of symptoms and is unlikely to have all symptoms. 
Other features may include a small lower jaw (micrognathia), cubitus valgus, soft upturned nails, palmar crease, and drooping eyelids. Less common are pigmented moles, hearing loss, and a high-arch palate (narrow maxilla). Turner syndrome manifests itself differently in each female affected by the condition; therefore, no two individuals will share the same features.
While most of the physical findings are harmless, there can be significant medical problems associated with the syndrome.
Prenatal.
Despite the excellent postnatal prognosis, 99% of Turner-syndrome conceptions are thought to end in spontaneous abortion or stillbirth, and as many as 15% of all spontaneous abortions have the 45,X karyotype. Among cases that are detected by routine amniocentesis or chorionic villus sampling, one study found that the prevalence of Turner syndrome among tested pregnancies was 5.58 and 13.3 times higher respectively than among live neonates in a similar population.
Cardiovascular.
Prevalence of cardiovascular malformations.
The prevalence of cardiovascular malformations among patients with Turner syndrome ranges from 17% (Landin-Wilhelmsen et al., 2001) to 45% (Dawson-Falk et al., 1992).
The variations found in the different studies are mainly attributable to variations in non-invasive methods used for screening and the types of lesions that they can characterize (Ho et al., 2004). However, Sybert, 1998 suggests that it could be simply attributable to the small number of subjects in most studies.
Different karyotypes may have differing prevalence of cardiovascular malformations. Two studies found a prevalence of cardiovascular malformations of 30% and 38% in a group of pure 45,X monosomy. But, considering other karyotype groups, they reported a prevalence of 24.3% and 11% in patients with mosaic X monosomy, and a prevalence of 11% in patients with X chromosomal structural abnormalities.
The higher prevalence in the group of pure 45,X monosomy is primarily due to a significant difference in the prevalence of aortic valve abnormalities and coarctation of the aorta, the two most common cardiovascular malformations.
Congenital heart disease.
The most commonly observed are congenital obstructive lesions of the left side of the heart, leading to reduced flow on this side of the heart. This includes bicuspid aortic valve and coarctation (narrowing) of the aorta. Sybert, 1998 found that more than 50% of the cardiovascular malformations observed in her study of individuals with Turner syndrome were bicuspid aortic valves or coarctation of the aorta, alone or in combination.
Other congenital cardiovascular malformations, such as partial anomalous venous drainage and aortic valve stenosis or aortic regurgitation, are also more common in Turner syndrome than in the general population. Hypoplastic left heart syndrome represents the most severe reduction in left-sided structures
Bicuspid aortic valve.
Up to 15% of adults with Turner syndrome have bicuspid aortic valves, meaning that there are only two, instead of three, parts to the valves in the main blood vessel leading from the heart. Since bicuspid valves are capable of regulating blood flow properly, this condition may go undetected without regular screening. However, bicuspid valves are more likely to deteriorate and later fail. Calcification also occurs in the valves, which may lead to a progressive valvular dysfunction as evidenced by aortic stenosis or regurgitation.
With a prevalence from 12.5% to 17.5% (Dawson-Falk et al., 1992), bicuspid aortic valve is the most common congenital malformation affecting the heart in this syndrome. It is usually isolated but it may be seen in combination with other anomalies, particularly coarctation of the aorta.
Coarctation of the aorta.
Between 5% and 10% of those born with Turner syndrome have coarctation of the aorta, a congenital narrowing of the descending aorta, usually just distal to the origin of the left subclavian artery (the artery that branches off the arch of the aorta to the left arm) and opposite to the duct (and so termed "juxtaductal"). Estimates of the prevalence of this malformation in patients with Turner syndrome ranges from 6.9% to 12.5% . A coarctation of the aorta in a female is suggestive of Turner syndrome, and suggests the need for further tests, such as a karyotype.
Partial anomalous venous drainage.
This abnormality is a relatively rare congenital heart disease in the general population. The prevalence of this abnormality also is low (around 2.9%) in Turner syndrome. However, its relative risk is 320 in comparison with the general population. Strangely, Turner syndrome seems to be associated with unusual forms of partial anomalous venous drainage.
In the management of a patient with Turner syndrome it is essential to keep in mind that these left-sided cardiovascular malformations in Turner syndrome result in an increased susceptibility to bacterial endocarditis. Therefore prophylactic antibiotics should be considered when procedures with high risk endocarditis are performed, such as dental cleaning.
Turner syndrome is often associated with persistent hypertension, sometimes in childhood. In the majority of Turner syndrome patients with hypertension, there is no specific cause. In the remainder, it is usually associated with cardiovascular or kidney abnormalities, including coarctation of the aorta.
Aortic dilation, dissection, and rupture.
Two studies have suggested aortic dilatation in Turner syndrome, typically involving the root of the ascending aorta and occasionally extending through the aortic arch to the descending aorta, or at the site of previous coarctation of the aorta repair.
Sybert, 1998 points out that it remains unproven that aortic root diameters that are relatively large for body surface area but still well within normal limits imply a risk for progressive dilatation.
Prevalence of aortic abnormalities.
The prevalence of aortic root dilatation ranges from 8.8% to 42% in patients with Turner syndrome. Even if not every aortic root dilatation necessarily goes on to an aortic dissection (circumferential or transverse tear of the intima), complications such as dissection, aortic rupture resulting in death may occur. The natural history of aortic root dilatation is still unknown, but it is a fact that it is linked to aortic dissection and rupture, which has a high mortality rate.
Aortic dissection affects 1% to 2% of patients with Turner syndrome. As a result any aortic root dilatation should be seriously taken into account as it could become a fatal aortic dissection. Routine surveillance is highly recommended.
Risk factors for aortic rupture.
It is well established that cardiovascular malformations (typically bicuspid aortic valve, coarctation of the aorta and some other left-sided cardiac malformations) and hypertension predispose to aortic dilatation and dissection in the general population. At the same time it has been shown that these risk factors are common in Turner syndrome. Indeed these same risk factors are found in more than 90% of patients with Turner syndrome who develop aortic dilatation. Only a small number of patients (around 10%) have no apparent predisposing risk factors. It is important to note that the risk of hypertension is increased 3-fold in patients with Turner syndrome. Because of its relation to aortic dissection blood pressure needs to be regularly monitored and hypertension should be treated aggressively with an aim to keep blood pressure below 140/80 mmHg. It has to be noted that as with the other cardiovascular malformations, complications of aortic dilatation is commonly associated with 45,X karyotype.
Pathogenesis of aortic dissection and rupture.
The exact role that all these risk factors play in the process leading to such fatal complications is still quite unclear. Aortic root dilatation is thought to be due to a mesenchymal defect as pathological evidence of cystic medial necrosis has been found by several studies. The association between a similar defect and aortic dilatation is well established in such conditions such as Marfan syndrome. Also, abnormalities in other mesenchymal tissues (bone matrix and lymphatic vessels) suggests a similar primary mesenchymal defect in patients with Turner syndrome. However, there is no evidence to suggest that patients with Turner syndrome have a significantly higher risk of aortic dilatation and dissection in absence of predisposing factors. So the risk of aortic dissection in Turner syndrome appears to be a consequence of structural cardiovascular malformations and hemodynamic risk factors rather than a reflection of an inherent abnormality in connective tissue (Sybert, 1998). The natural history of aortic root dilatation is unknown, but because of its lethal potential, this aortic abnormality needs to be carefully followed.
Skeletal.
Normal skeletal development is inhibited due to a large variety of factors, mostly hormonal. The average height of a woman with Turner syndrome, in the absence of growth hormone treatment, is 4 ft 7 in (140 cm). Patients with Turner's mosaicism can reach normal average height.
The fourth metacarpal bone (fourth toe and ring finger) may be unusually short, as may the fifth.
Due to inadequate production of estrogen, many of those with Turner syndrome develop osteoporosis. This can decrease height further, as well as exacerbate the curvature of the spine, possibly leading to scoliosis. It is also associated with an increased risk of bone fractures.
Kidney.
Approximately one-third of all women with Turner syndrome have one of three kidney abnormalities:
Some of these conditions can be corrected surgically. Even with these abnormalities, the kidneys of most women with Turner syndrome function normally. However, as noted above, kidney problems may be associated with hypertension.
Thyroid.
Approximately one-third of all women with Turner syndrome have a thyroid disorder. Usually it is hypothyroidism, specifically Hashimoto's thyroiditis. If detected, it can be easily treated with thyroid hormone supplements.
Diabetes.
Women with Turner syndrome are at a moderately increased risk of developing type 1 diabetes in childhood and a substantially increased risk of developing type 2 diabetes by adult years. The risk of developing type 2 diabetes can be substantially reduced by maintaining a healthy weight.
Cognitive.
Turner syndrome does not typically cause intellectual disability or impair cognition. However, learning difficulties are common among women with Turner syndrome, particularly a specific difficulty in perceiving spatial relationships, such as nonverbal learning disorder. This may also manifest itself as a difficulty with motor control or with mathematics. While it is non-correctable, in most cases it does not cause difficulty in daily living. Most Turner Syndrome patients are employed as adults and lead productive lives.
There is also a rare variety of Turner Syndrome, known as "Ring-X Turner Syndrome", which has an approximate 60 percent association with intellectual disability. This variety accounts for approximately 2–4% of all Turner Syndrome cases.
Reproductive.
Women with Turner syndrome are almost universally infertile. While some women with Turner syndrome have successfully become pregnant and carried their pregnancies to term, this is very rare and is generally limited to those women whose karyotypes are not 45,X. Even when such pregnancies do occur, there is a higher than average risk of miscarriage or birth defects, including Turner Syndrome or Down Syndrome. Some women with Turner syndrome who are unable to conceive without medical intervention may be able to use IVF or other fertility treatments.
Usually estrogen replacement therapy is used to spur growth of secondary sexual characteristics at the time when puberty should onset. While very few women with Turner Syndrome menstruate spontaneously, estrogen therapy requires a regular shedding of the uterine lining ("withdrawal bleeding") to prevent its overgrowth. Withdrawal bleeding can be induced monthly, like menstruation, or less often, usually every three months, if the patient desires. Estrogen therapy does not make a woman with nonfunctional ovaries fertile, but it plays an important role in assisted reproduction; the health of the uterus must be maintained with estrogen if an eligible woman with Turner Syndrome wishes to use IVF (using donated oocytes).
Turner syndrome is a cause of primary amenorrhea, premature ovarian failure (hypergonadotropic hypogonadism), streak gonads and infertility. Failure to develop secondary sex characteristics (sexual infantilism) is typical.
Especially in mosaic cases of Turner syndrome that contains Y-chromosome (e.g. 45,X/46,XY) due to the risk of development of ovarian malignancy (most common is gonadoblastoma) gonadectomy is recommended.
Turner syndrome is characterized by primary amenorrhoea, premature ovarian failure, streak gonads and infertility. However, technology (especially oocyte donation) provides the opportunity of pregnancy in these patients.
As more women with Turner syndrome complete pregnancy thanks to modern techniques to treat infertility, it has to be noted that pregnancy may be a risk of cardiovascular complications for the mother. Indeed several studies had suggested an increased risk for aortic dissection in pregnancy. Three deaths have even been reported. The influence of estrogen has been examined but remains unclear. It seems that the high risk of aortic dissection during pregnancy in women with Turner syndrome may be due to the increased hemodynamic load rather than the high estrogen rate. Of course these findings are important and need to be remembered while following a pregnant patient with Turner syndrome.
Cause.
Turner syndrome is caused by the absence of two complete copies of the X chromosome in some or all the cells. The abnormal cells may have only one X (monosomy) (45,X) or they may be affected by one of several types of partial monosomy like a deletion of the short p arm of one X chromosome (46,X,del(Xp)) or the presence of an isochromosome with two q arms (46,X,i(Xq)) In mosaic individuals, cells with X monosomy (45,X) may occur along with cells that are normal (46,XX), cells that have partial monosomies, or cells that have a Y chromosome (46,XY). The presence of mosaicism is estimated to be relatively common in affected individuals (67-90%).
Inheritance.
In the majority of cases where monosomy occurs, the X chromosome comes from the mother. This may be due to a nondisjunction in the father. Meiotic errors that lead to the production of X with p arm deletions or abnormal Y chromosomes are also mostly found in the father. Isochromosome X or ring chromosome X on the other hand are formed equally often by both parents. Overall, the functional X chromosome mostly comes from the mother.
In most cases, Turner syndrome is a sporadic event, and for the parents of an individual with Turner syndrome the risk of recurrence is not increased for subsequent pregnancies. Rare exceptions may include the presence of a balanced translocation of the X chromosome in a parent, or where the mother has 45,X mosaicism restricted to her germ cells.
Diagnosis.
Prenatal.
Turner syndrome may be diagnosed by amniocentesis or chorionic villus sampling during pregnancy.
Usually, fetuses with Turner syndrome can be identified by abnormal ultrasound findings ("i.e.", heart defect, kidney abnormality, cystic hygroma, ascites). In a study of 19 European registries, 67.2% of prenatally diagnosed cases of Turner Syndrome were detected by abnormalities on ultrasound. 69.1% of cases had one anomaly present, and 30.9% had two or more anomalies.
An increased risk of Turner syndrome may also be indicated by abnormal triple or quadruple maternal serum screen. The fetuses diagnosed through positive maternal serum screening are more often found to
have a mosaic karyotype than those diagnosed based on ultrasonographic abnormalities, and
conversely those with mosaic karyotypes are less likely to have associated ultrasound abnormalities.
Although the recurrence risk is not increased, genetic counseling is often recommended for families who have had a pregnancy or child with Turner syndrome.
Postnatal.
Turner syndrome can be diagnosed postnatally at any age. Often, it is diagnosed at birth due to heart problems, an unusually wide neck or swelling of the hands and feet. However, it is also common for it to go undiagnosed for several years, typically until the girl reaches the age of puberty/adolescence and she fails to develop properly (the changes associated with puberty do not occur). In childhood, a short stature can be indicative of Turner syndrome.
A test, called a karyotype or a chromosome analysis, analyzes the chromosomal composition of the individual. This is the test of choice to diagnose Turner syndrome.
Treatment.
As a chromosomal condition, there is no cure for Turner syndrome. However, much can be done to minimize the symptoms. For example:
Epidemiology.
Approximately 99 percent of all fetuses with Turner syndrome result in spontaneous termination during the first trimester. Turner syndrome accounts for about 10 percent of the total number of spontaneous abortions in the United States. The incidence of Turner syndrome in live female births is believed to be around 1 in 2000.
History.
The syndrome is named after Henry Turner, an endocrinologist from Illinois, who described it in 1938. In Europe, it is often called Ullrich–Turner syndrome or even Bonnevie–Ullrich–Turner syndrome to acknowledge that earlier cases had also been described by European doctors.
The first published report of a female with a 45,X karyotype was in 1959 by Dr. Charles Ford and colleagues in Harwell, Oxfordshire and Guy's Hospital in London. It was found in a 14-year-old girl with signs of Turner syndrome.

</doc>
<doc id="49365" url="http://en.wikipedia.org/wiki?curid=49365" title="LGM-30 Minuteman">
LGM-30 Minuteman

The LGM-30 Minuteman is a US land-based intercontinental ballistic missile (ICBM), in service with the Air Force Global Strike Command. As of 2014, the LGM-30G Minuteman-III version is the only land-based ICBM in service in the United States. It is one component of the US nuclear triad—the other two parts of the triad being the Trident submarine-launched ballistic missile (SLBM), and nuclear weapons carried by long-range strategic bombers. Each missile carries up to three nuclear warheads, which have a yield in the range of 300 to 500 kilotons. The Minuteman was the first MIRV-capable missile.
The name "Minuteman" comes from the Revolutionary War's Minutemen. It also refers to its quick reaction time; the missile can be launched within minutes after the receipt of a valid launch order. The Air Force plans to keep the missile in service until at least 2030.
The current US force consists of 450 Minuteman-III missiles in missile silos around Malmstrom AFB, Montana; Minot AFB, North Dakota; and F.E. Warren AFB, Wyoming. This will slowly be reduced to 400 armed missiles, with 50 unarmed missiles in reserve, and four non-deployed test launchers to comply with the New START treaty.
History.
Edward Hall and solid fuels.
Minuteman owes its existence largely to the efforts of then Air Force Colonel Edward N. Hall. In 1956, Hall was put in charge of the solid fuel propulsion division of General Schriever's Western Development Division, which had originally been formed to lead development of the Atlas and Titan ICMBs. Solid fuels were already commonly used in rockets, but strictly for short-range uses. Hall's superiors were interested in short and medium range missiles with solids, especially for use in Europe, but Hall was convinced that they could be used for a true ICBM with 5500 nmi range.
To achieve the required energy, Hall began funding research at Boeing and Thiokol into the use of ammonium perchlorate composite propellant. Adapting a concept developed in the UK, they cast the fuel into large cylinders with a star-shaped hole running along the inner axis. This allowed the fuel to burn along the entire length of the cylinder, rather than just the end as in earlier designs, increasing thrust. This also meant the heat was spread across the entire motor and did not reach the wall of the missile fuselage until the engine was finished burning.
Guidance of an ICBM is based not only on the direction the missile is travelling, but the precise instant that thrust is cut off. Too much thrust and the warhead will overshoot its target, too little and it will fall short. Solids are normally very hard to predict in terms of burning time and their instantaneous thrust during the burn, which made them questionable for the sort of accuracy required to hit a target at intercontinental range. This appeared at first to be an insurmountable problem, but in the end was solved in almost trivial fashion. A series of ports were added inside the rocket nozzle that were opened when the guidance systems called for engine cut-off. The reduction in pressure was so abrupt that the last burning fuel ejected itself and the flame was snuffed out.
Rapid success in the development program, combined with Edward Teller's promise of much lighter nuclear warheads during Project Nobska, led the Navy to abandon their work with the US Army's liquid fuel Jupiter missile and begin development of a solid fuel missile of their own. They felt that liquid fuels were too dangerous to use onboard ships, and especially submarines. Aerojet's work with Hall would be adapted for their Polaris missile starting in December 1956.
Missile farm concept.
The Air Force, however, saw no pressing need for a solid fuel ICBM. Atlas and Titan were progressing, and "storable" liquids were being developed that would allow the missiles to be left in a ready-to-shoot form for extended periods. But Hall saw solid fuels not only as a way to improve launch times or safety, but part of a radical plan to greatly reduce the cost of ICBMs so that thousands could be built. He was aware that new computerized assembly lines would allow continual production, and that similar equipment would allow a small team to oversee operations for dozens or hundreds of missiles. A solid fuel design would be much simpler to build, and easier to maintain in service.
His ultimate plan was to build a number of integrated missile "farms" that included factories, missile silos, transport and even recycling. Each farm would support between 1,000 and 1,500 missiles being produced in a continual low rate cycle. Systems in the missiles would detect failures, at which point it would be removed and recycled, while a newly built missile was put into the silo. The missile design itself was based purely on lowest possible cost, reducing its size and complexity because "the basis of the weapon's merit was its low cost per completed mission; all other factors - accuracy, vulnerability and reliability - were secondary."
Hall's plan did not go unopposed, especially by the more established names in the ICBM field. Ramo-Wooldridge pressed for a system with higher accuracy, but Hall countered that the missile's role was to attack Soviet cities, and that "a force which provides numerical superiority over the enemy will provide a much stronger deterrent than a numerically inferior force of greater accuracy." Hall was known for his "friction with others" and in 1958 Schriever removed him from the Minuteman project and sent him to the UK to oversee deployment of the Thor ICBM. On his return to the US in 1959, Hall retired from the Air Force, but received his second Legion of Merit in 1960 for his work on solid fuels.
Although he was removed from the Minuteman project, Hall's work on cost reduction had already produced a new design of 71 in diameter, much smaller than the Atlas and Titan at 120 in, which would mean much smaller and cheaper silos. Hall's goal of dramatic cost reduction was a success, although many of the other concepts of his missile farm was abandoned.
Missile gap.
In 1957 a series of intelligence reports suggested the Soviets were far ahead in the missile race and would be able to overwhelm the US by the early 1960s. It was later demonstrated that this "missile gap" was just as fictional as the "bomber gap" of a few years earlier, but through the late 1950s it was a serious concern. The Air Force, concerned about the survivability of its striking force in the short term, began the WS-199 program to develop a survivable strategic missile, and pushed Minuteman for crash development starting in September 1958.
Advanced surveying of the potential silo sites had already begun in late 1957. Fears of a Soviet anti-ballistic missile system, which was known to be under development at Sary Shagan, led to calls for the adoption of a maneuvering reentry vehicle (MARV), which greatly complicates the problem of shooting down a warhead. Development of MARV systems began under the Alpha Draco and Boost Glide Reentry Vehicle programs in 1957. These used long and skinny arrow-like shapes that required more room on the front of the missile. To address this, the Minuteman silos were revised to be built 13 feet deeper. Although Minuteman would not deploy a boost-glide warhead, the extra space proved invaluable in the future as it allowed the missile to be extended and carry more fuel and payload.
Guidance system.
Previous long-range missiles were liquid fueled and required considerable time, 30 minutes to an hour or more, to be fueled. During this time other crewmembers would be spinning up the inertial guidance system, setting its initial position, and programming in the target coordinates. This normally took about as long as the fueling process, so it was not considered a problem that needed to be solved. Minuteman was designed from the outset to be launched in minutes. While the use of solid fuel eliminated the delays fueling up, it did nothing for the delays in erecting and aligning the guidance system. For quick launch, the guidance system would have to be kept running and aligned at all times, a serious problem for the mechanical systems of the era, especially the gyroscopes which used ball bearings.
After considerable deliberation, a design by Autonetics using air bearings was selected, after they pointed out that their experimental set had been running continually from 1952 to 1957. Autonetics further advanced the state of the art by building their bearing not in the form of a single spindle but a ball. This allowed the gyros to precess in two directions instead of along a single axis, meaning that only two gyros instead of three would be needed for the inertial platform.
The last major advance in the Minuteman development was the decision to use a general purpose digital computer in place of the analog or custom designed digital computers of earlier missile designs. This was not chosen to improve the guidance accuracy "per se", but a side effect of wishing to reduce the total number of parts in the missile. Previous missile designs had an autopilot that kept the missile flying in a straight line, and a separate guidance system that provided inputs to the autopilot to adjust its trajectory. Using a single more powerful computer would eliminate the need for two separate units.
Since the guidance computer would otherwise be doing nothing while the missile sat in the silo, using a general purpose computer and simply running a different program on it allowed it to handle the monitoring of the various sensors and test equipment. With older designs this had been handled externally, requiring miles of extra wiring and many connectors. In order to store multiple programs, the computer was built in the form of a drum machine but used a hard disk in place of the drum.
Building a computer with the required performance, size and weight demanded the use of transistors, which were at that time very expensive and not very reliable. Earlier efforts to use transistorized computers for guidance, BINAC and the system on the SM-64 Navaho, had failed to work and were abandoned. The Air Force and Autonetics spent millions on a program to improve transistor and component reliability 100 times. This program led to the "Minuteman high-rel parts" that had enormous spin-off effects in the electronics industry.
The use of a general purpose computer would have long-lasting effects on the Minuteman program, and the US's nuclear stance in general. Earlier ICBMs using custom wired computers were capable of attacking a single target, the precise trajectory information hard coded directly in the system's logic. With Minuteman, the targeting could be easily changed by loading new trajectory information into the computer's memory, a somewhat time consuming process, but one that could be completed in a few hours.
Much more importantly, this reprogrammability meant that the information could be continually updated in the field, allowing the system to gain accuracy as improving estimates of the Earth's gravitational field were fed into the system. Initially deployed with an estimated best-case circular error probable (CEP) of 1.1 nmi, Minuteman underwent several in-field updates that roughly halved this to 0.6 nmi by about 1965. The was accomplished without any mechanical changes to the missile or its navigation system.
The Puzzle of Polaris.
During Minuteman's early development, the Air Force maintained the policy that the manned strategic bomber was the primary weapon of nuclear war. Blind bombing accuracy on the order of 1500 feet was expected, and the weapons sized to ensure even the hardest targets would be destroyed as long as the weapon fell within this range. The USAF had enough bombers to attack every military and industrial target in the USSR and were confident that their bombers would survive in great enough numbers that such a strike would utterly destroy the country.
Soviet ICBMs upset this equation to a degree. Their accuracy was known to be low, on the order of 4 nmi, but they carried large warheads that would be useful against Strategic Air Command's bombers, which parked in the open. Since there was no system to detect the ICBMs being launched, the possibility was raised that the Soviets could launch a sneak attack with a few dozen missiles that would take out a significant portion of SACs bomber fleet. In this environment, the Air Force saw their own ICBMs not as a primary weapon of war, but as a way to ensure that the Soviets would not risk a sneak attack. Missiles, especially later models housed in silos, could be expected to survive a sneak attack in sufficient numbers to ensure destruction of all major Soviet cities. In such an environment, the Soviets would not risk an attack.
An attack of "400 equivalent megatons" aimed at the largest Soviet cities would promptly kill 30% of their population and destroy 50% of their industry. Larger attacks raised these numbers only slightly. This suggested that there was a "finite deterrent" level around 400 megatons that would be enough to prevent a Soviet attack no matter how many missiles they had of their own. All that had to be ensured was that the US missiles survived, which seemed likely given the low accuracy of the Soviet weapons.
This presented a serious problem for the Air Force. While still pressing for development of their bombers as the weapon of choice against military targets, at that time represented by the supersonic B-70, it appeared the missile role was served perfectly well by the Navy's Polaris. Polaris was essentially invulnerable, and the Navy's intended fleet of 41 submarines carrying 16 missiles each meant the Navy held a finite deterrent that was unassailable. A February 1960 memo by RAND entitled "The Puzzle of Polaris" was passed around among high-ranking Air Force officials, suggesting that Polaris negated any need for Air Force ICBMs if they were also being aimed at Soviet cities. This would have long-lasting effects on the future of the Minuteman program, which, by 1961, was firmly evolving towards a counterforce capability.
Kennedy and Minuteman.
Minuteman was entering final testing just as John Kennedy was entering the White House. His new Secretary of Defense, Robert McNamara, was tasked with the seemingly impossible mission of producing the world's best defense while at the same time limiting spending. McNamara began to apply cost/benefit analysis to the problem, and Minuteman's low production cost made its selection as the basis for a US buildout natural. Atlas and Titan were soon scrapped, and the storable liquid fueled Titan II deployment was severely curtailed. Perhaps a foregone conclusion, McNamara also cancelled the B-70.
Minuteman's low cost also had spin-off effects on non-ICBM programs. Another way to prevent a sneak attack was provided by the Army's Nike Zeus, an interceptor missile that was capable of shooting down the Soviet warheads. The Army argued that upgraded Soviet missiles might be able to attack US missiles in their silos, and Zeus would be able to blunt such an attack. Zeus was expensive, however, and the Air Force pointed out that it was less expensive to build another Minuteman missile than the Zeus system needed to protect it. Given the large size and complexity of the Soviet liquid-fueled missiles, an ICBM building race was one the Soviets could not afford. Zeus was cancelled in 1963.
Minuteman and counterforce.
Minuteman's selection as the primary Air Force ICBM was initially based on the same logic as their earlier missiles, that the weapon was primarily one designed to ride out any potential Soviet attack and ensure they would be hit in return. But Minuteman had a combination of features that led to its rapid evolution into the US's primary weapon of nuclear war.
Primary among these qualities was its digital computer. This could be updated in the field with new targets and better information about the flight paths with relative ease, gaining accuracy for little cost. One of the unavoidable effects on the warhead's trajectory was the mass of the Earth, which is not even, and contains many mass concentrations that pull on the warhead. Through the 1960s, the Defense Mapping Agency (now part of National Geospatial-Intelligence Agency) mapped these with increasing accuracy, feeding that information back into the Minuteman fleet. The Minuteman was deployed with a circular error probable (CEP) of about 1.1 nmi, but this had improved to about 0.6 nmi by 1965.
At those levels, the ICBM begins to approach the manned bomber in terms of accuracy. A small upgrade, roughly doubling the accuracy of the INS, would give it the same 1500 feet CEP as the manned bomber. Autonetics began such development even before the original Minuteman entered fleet service, and the Minuteman-II had a CEP of 0.26 nmi. Additionally, the computers were upgraded with more memory, allowing them to store information for eight targets, which the missile crews could select among almost instantly, greatly increasing their flexibility. From that point, Minuteman became the US's primary deterrent weapon, until its performance was matched by the Navy's Trident missile of the 1980s.
Questions about the need for the manned bomber were quickly raised. The Air Force began to offer a number of reasons why the bomber offered value, in spite of costing more money to buy and being much more expensive to operate and maintain. Newer bombers with better survivability, like the B-70, cost many times that of the Minuteman, and in spite of great efforts through the 1960s this was never addressed. The B-1 of the early 1970s eventually emerged with a price tag around $200 million ($<br>{Inflation} - Amount must not have "" prefix: 200.   million today) while the Minuteman-III's built during the 1970s cost only $7 million ($<br>{Inflation} - Amount must not have "" prefix: 7.   million today).
The Air Force countered that having a variety of platforms complicated the defense; if the Soviets built an effective anti-ballistic missile system of some sort, the ICBM and SLBM fleet might be rendered useless, while the bombers would remain. This became the nuclear triad concept, which survives into the 2000s. Although this argument was successful, the numbers of manned bombers has been repeatedly cut and the deterrent role increasingly passed to missiles.
Minuteman-I (LGM-30A/B or SM-80/HSM-80A).
Deployment.
The LGM-30A Minuteman-I was first test-fired on 1 February 1961, and entered into the Strategic Air Command's arsenal in 1962, at Malmstrom Air Force Base, Montana; the "improved" LGM-30B became operational at Ellsworth Air Force Base, South Dakota, Minot Air Force Base, North Dakota, F.E. Warren Air Force Base, Wyoming, and Whiteman Air Force Base, Missouri in 1963. All 800 Minuteman-I missiles were delivered by June 1965. Each of the bases had 150 missiles emplaced. F.E. Warren AFB had 200 of the Minuteman-IB missiles. Malmstrom AFB had 150 of the Minuteman-I and about five years later added 50 of the Minuteman-II similar to those installed at Grand Forks AFB, ND.
Guidance.
The Minuteman-I Autonetics D-17 flight computer used a rotating air bearing magnetic disk holding 2,560 "cold-stored" words in 20 tracks (write heads disabled after program fill) of 24 bits each and one alterable track of 128 words. The time for a D-17 disk revolution was 10 ms. The D-17 also used a number of short loops for faster access of intermediate results storage. The D-17 computational minor cycle was three disk revolutions or 30 ms. During that time all recurring computations were performed. For ground operations the inertial platform was aligned and gyro correction rates updated. During flight, filtered command outputs were sent by each minor cycle to the engine nozzles. Unlike modern computers, which use descendants of that technology for secondary storage on hard disk, the disk was the active computer memory. The disk storage was considered hardened to radiation from nearby nuclear explosions, making it an ideal storage medium. To improve computational speed, the D-17 borrowed an instruction look-ahead feature from the Autonetics-built Field Artillery Data Computer (M18 FADAC) that permitted simple instruction execution every word time.
The D-17B and the D-37C guidance and control computers were integral components of the Minuteman-I and Minuteman-II missiles, respectively, which formed a part of the United States ICBM arsenal. The Minuteman-III missiles, which use D-37D computers, complete the 1000 missile deployment of this system. The initial cost of these computers ranged from about $139,000 (D-37C) to $250,000 (D-17B).
Minuteman-II (LGM-30F).
The LGM-30F Minuteman-II was an improved version of the Minuteman-I missile. Development on the Minuteman-II began in 1962 as the Minuteman-I entered the Strategic Air Command's nuclear force. Minuteman-II production and deployment began in 1965 and completed in 1967. It had an increased range, a greater throw weight and guidance system with better azimuthal coverage, providing military planners with better accuracy and a wider range of targets. Some missiles also carried penetration aids, allowing higher probability of kill against Moscow's anti-ballistic missile system. The payload consisted of a single Mk-11C reentry vehicle containing a W56 nuclear warhead with a yield of 1.2 megatons of TNT (5 PJ).
The major new features provided by Minuteman-II were:
System modernization was concentrated on launch facilities and command and control facilities. This provided decreased reaction time and increased survivability when under nuclear attack. Final changes to the system were performed to increase compatibility with the expected LGM-118A Peacekeeper. These newer missiles were later deployed into modified Minuteman silos.
The Minuteman-II program was the first mass-produced system to use a computer constructed from integrated circuits (the Autonetics D-37C). The Minuteman-II integrated circuits were diode-transistor logic and diode logic made by Texas Instruments. The other major customer of early integrated circuits was the Apollo Guidance Computer, which had similar weight and ruggedness constraints. The Apollo integrated circuits were resistor-transistor logic made by Fairchild Semiconductor. The Minuteman-II flight computer continued to use rotating magnetic disks for primary storage.
Minuteman-III (LGM-30G): the current model.
The LGM-30G Minuteman-III program started in 1966, and included several improvements over the previous versions. It was first deployed in 1970. Most modifications related to the final stage and reentry system (RS). The final (third) stage was improved with a new fluid-injected motor, giving finer control than the previous four-nozzle system.
Performance improvements realized in Minuteman-III include increased flexibility in reentry vehicle (RV) and penetration aids deployment, increased survivability after a nuclear attack, and increased payload capacity. The missile retains a gimballed inertial guidance system.
Minuteman-III originally contained the following distinguishing features:
The existing Minuteman-III missiles have been further improved over the decades in service, with more than $7 billion spent in the last decade to upgrade the 450 missiles.
Guidance Replacement Program (GRP).
The Guidance Replacement Program (GRP) replaces the NS20A Missile Guidance Set with the NS50A Missile Guidance Set. The newer system extends the service life of the Minuteman missile beyond the year 2030 by replacing aging parts and assemblies with current, high reliability technology while maintaining the current accuracy performance. The replacement program was completed 25 February 2008.
Propulsion Replacement Program (PRP).
Beginning in 1998 and continuing through 2009, the Propulsion Replacement Program extends the life and maintains the performance by replacing the old solid propellant boosters (downstages).
Single Reentry Vehicle (SRV).
The Single Reentry Vehicle (SRV) modification enabled the United States ICBM force to abide by the now-vacated START II treaty requirements by reconfiguring Minuteman-III missiles from three reentry vehicles down to one. Though it was eventually ratified by both parties, START II never entered into force and was essentially superseded by follow-on agreements such as SORT and New START, which do not limit MIRV capability.
Safety Enhanced Reentry Vehicle (SERV).
Beginning in 2005, Mk-21/W87 RVs from the deactivated Peacekeeper missile will be placed on the Minuteman-III force under the Safety Enhanced Reentry Vehicle (SERV) program. The older W78 does not have many of the safety features of the newer W87, such as insensitive high explosive, as well as more advanced safety devices. In addition to implementing these safety features in at least a portion of the future Minuteman-III force, the decision to transfer W87s onto the missile is based on two features that will improve the targeting capabilities of the weapon: more fuzing options which will allow for greater targeting flexibility and the most accurate reentry vehicle available which provides a greater probability of damage to the designated targets. The first SERV-modded Minuteman-III was put on alert status at FE Warren AFB, Wyoming, in 2006.
Current and future deployment.
The Minuteman-III missile entered service in 1970, with weapon systems upgrades included during the production run from 1970 to 1978 to increase accuracy and payload capacity. s of 2008[ [update]], the USAF plans to operate it until at least 2030.
The LGM-118A Peacekeeper (MX) ICBM, which was to have replaced the Minuteman, was retired in 2005 as part of START II.
A total of 450 LGM-30G missiles are emplaced at F.E. Warren Air Force Base, Wyoming (90th Missile Wing), Minot Air Force Base, North Dakota (91st Missile Wing), and Malmstrom Air Force Base, Montana (341st Missile Wing). All Minuteman-I and Minuteman-II missiles have been retired. The United States prefers to keep its MIRV deterrents on submarine-launched Trident Nuclear Missiles. Fifty of these will be put into "warm" unarmed status, taking up half the 100 slots in America's allowable nuclear reserve.
Testing.
Minuteman-III missiles are regularly tested with launches from Vandenberg Air Force Base in order to validate the effectiveness, readiness, and accuracy of the weapon system, as well as to support the system's primary purpose, nuclear deterrence. The safety features installed on the Minuteman-III for each test launch allow the flight controllers to terminate the flight at any time if the systems indicate that its course may take it unsafely over inhabited areas. Since these flights are for test purposes only, even terminated flights can send back valuable information to correct a potential problem with the system.
The 576th Flight Test Squadron is responsible for planning, preparing, conducting, and assessing all ICBM ground and flight tests.
Advanced Maneuverable Reentry Vehicle.
When defending hardened targets, it is possible for a defensive ABM system to accurately track incoming warheads and choose to ignore those that will fall outside the lethal range of the target. This can, depending on the accuracy of the warheads, greatly reduce the number of defensive missiles that have to be fired in response to an attack. The simplest way to counter this possibility is to make a reentry vehicle that can maneuver, approaching its target along a trajectory that looks like it is going to miss, and then correcting at the last possible moment, leaving too little time for the defensive missile to launch. This concept is known as a maneuverable reentry vehicle, or MARV.
The "Advanced Maneuverable Reentry Vehicle" (AMaRV) was a prototype MARV built by McDonnell-Douglas Corp.. Four AMaRVs were made and represented a significant leap in Reentry Vehicle sophistication. Three of the AMaRVs were launched by surplus Minuteman-1s on 20 December 1979, 8 October 1980 and 4 October 1981. AMaRV had an entry mass of approximately 470 kg, a nose radius of 2.34 cm, a forward frustum half-angle of 10.4°, an inter-frustum radius of 14.6 cm, aft frustum half angle of 6°, and an axial length of 2.079 meters. No accurate diagram or picture of AMaRV has ever appeared in the open literature. However, a schematic sketch of an AMaRV-like vehicle along with trajectory plots showing hairpin turns has been published.
AMaRV's attitude was controlled through a split body flap (also called a "split-windward flap") along with two yaw flaps mounted on the vehicle's sides. Hydraulic actuation was used for controlling the flaps. AMaRV was guided by a fully autonomous navigation system designed for evading anti-ballistic missile (ABM) interception.
Influences.
The Minuteman Missile National Historic Site in South Dakota preserves a Launch Control Facility (D-01) and a launch facility (D-09) under the control of the National Park Service.
Appearances in media.
Footage of Minuteman-III ICBM test launches have been featured in several theatrical films and television movies where missile launch footage is needed. The Department of Defense film released for use was mainly drawn from Vandenberg Air Force Base test shots in 1966, including from a "salvo launch" (more than one ICBM launched simultaneously).
Theatrically released films using the footage include (most notably), the 1978 film "Superman" (which features the "twin shot"), and more extensively, the 1977 nuclear war film "Damnation Alley." The made for TV film "The Day After" also features the same footage, although the first stage of flight is completed via special effects. "Terminator 3" uses computer generated images of Minuteman missiles launching from the Plains on "Judgment Day". Minutemen also feature in "Eagle Strike", by Anthony Horowitz, in which fictional power-crazed multimillionaire Damian Cray orders their release from Air Force One. In the film "WarGames" a failed Minuteman launch simulation exercise caused by a conflicted launch control officer is the impetus for the conversion of the missiles to full automatic control by the computer system that Matthew Broderick's character later hacks into.
Other roles.
Mobile Minuteman.
Mobile Minuteman was a program for rail-based ICBMs to help increase survivability and for which the USAF released details on 12 October 1959. The Operation Big Star performance test was from 20 June to 27 August 1960 at Hill Air Force Base, and the 4062nd Strategic Missile Wing (Mobile) was organized 1 December 1960 for 3 planned missile train squadrons, each with 10 trains carrying 3 missiles per train. During the Kennedy/McNamara cutbacks, the DoD announced "that it has abandoned the plan for a mobile Minuteman ICBM. The concept called for 600 to be placed in service—450 in silos and 150 on special trains, each train carrying 5 missiles." After Kennedy announced on 18 March 1961, that the 3 squadrons were to be replaced with "fixed-base squadrons", Strategic Air Command discontinued the 4062nd Strategic Missile Wing on 20 February 1962.
Air Launched ICBM.
Air Launched ICBM was a STRAT-X proposal in which SAMSO successfully conducted an Air Mobile Feasibility Test that airdropped a Minuteman 1b from a C-5A Galaxy aircraft from 20000 ft over the Pacific Ocean. The missile fired at 8000 ft, and the 10-second engine burn carried the missile to 20,000 feet again before it dropped into the ocean. Operational deployment was discarded due to engineering and security difficulties, and the capability was a negotiating point in the Strategic Arms Limitation Talks.
Emergency Rocket Communications System (ERCS).
An additional part of the National Command Authority communication relay system was called the Emergency Rocket Communication System (ERCS). Specially designed rockets called BLUE SCOUT carried radio-transmitting payloads high above the continental United States, to relay messages to units within line-of-sight. In the event of a nuclear attack, ERCS payloads would relay pre-programmed messages giving the "go-order" to SAC units. BLUE SCOUT launch sites were located at Wisner, West Point and Tekamah, Nebraska. These locations were vital for ERCS effectiveness due to their centralized position in the US, within range of all missile complexes. Later ERCS configurations were placed on the top of modified Minuteman-II ICBMs (LGM-30Fs) under the control of the 510th Strategic Missile Squadron located at Whiteman Air Force Base, Missouri.
The Minuteman ERCS may have been assigned the designation LEM-70A.
Satellite launching role.
The U.S. Air Force has considered using some decommissioned Minuteman missiles in a satellite launching role. These missiles would be stored in silos, for launch upon short notice. The payload would be variable, and would have the ability to be replaced quickly. This would allow a surge capability in times of emergency.
During the 1980s, surplus Minuteman missiles were used to power the Conestoga rocket produced by Space Services Inc. of America. It was the first privately developed rocket, but only saw three flights and was discontinued due to a lack of business. More recently, converted Minuteman missiles have been used to power the Minotaur line of rockets produced by Orbital Sciences.
Ground and air launch targets.
L-3 Communications is currently using SR-19 SRBs, Minuteman-II Second Stage Solid Rocket Boosters, as delivery vehicles for a range of different re-entry vehicles as targets for the THAAD and ASIP interceptor missile programs as well as radar testing.
Operator.
 United States: The United States Air Force has been the only operator of the Minuteman ICBM weapons system, currently with three operational wings and one test squadron operating the LGM-30G. The active inventory in FY 2009 is 450 missiles and 45 Missile Alert Facilities (MAF).
Operational units.
The basic tactical unit of a Minuteman wing is the squadron, consisting of five flights. Each flight consists of ten unmanned launch facilities (LFs) which are remotely controlled by a manned launch control center (LCC). The five flights are interconnected and status from any LF may be monitored by any of the five LCCs. Each LF is located at least three nautical miles (5.6 km) from any LCC. Control does not extend outside the squadron (thus the 319th Missile Squadron's five LCCs cannot control the 320th Missile Squadron's 50 LFs even though they are part of the same Space Launch Wing). Each Minuteman wing is assisted logistically by a nearby Missile Support Base (MSB).
Notes.
^i All available descriptions of GIGANTIC CHARGE use the identical language shown here, so it's not clear whether the "strategic" was instead supposed to be "single" to match the normal meaning of the SIOP acronym (Single Integrated Operational Plan), or whether this was intentionally referring to a separate plan. Without any further context, the phrasing doesn't give enough detail to distinguish.
References.
Bibliograby.
</dl>

</doc>
<doc id="49367" url="http://en.wikipedia.org/wiki?curid=49367" title="Laurent-Désiré Kabila">
Laurent-Désiré Kabila

Laurent-Désiré Kabila (  ) (November 27, 1939 – January 16, 2001), or simply Laurent Kabila, was President of the Democratic Republic of the Congo from May 17, 1997, when he overthrew Mobutu Sese Seko, until his assassination by one of his bodyguards on January 16, 2001. He was succeeded by his son Joseph eight days later.
Early life.
Kabila was born to a member of the Luba tribe in Baudoinville, Katanga Province, (now Moba, Tanganyika District) in the Belgian Congo. His father was a Luba and his mother was a Lunda. He studied political philosophy in France, and in Yugoslavia at the University of Belgrade. Later he attended the University of Dar es Salaam in Tanzania.
Political activities.
Congo Crisis.
When the Congo gained independence from Belgium on June 30, 1960 and the Congo Crisis began, Kabila had a role as a "deputy commander" in the Jeunesses Balubakat, the youth wing of the Patrice Lumumba-aligned General Association of the Baluba People of Katanga (Balubakat), actively fighting the secessionist forces of Moise Tshombe. Within months, Joseph Mobutu overthrew Lumumba, and in 1962 Kabila was appointed to the provincial assembly for North Katanga and was chief of cabinet for Minister of Information Ferdinand Tumba.
Kabila established himself as a supporter of hard-line Lumumbist Prosper Mwamba Ilunga. When the Lumumbists formed the Conseil National de Libération, he was sent to eastern Congo to help organize a revolution, in particular in the Kivu and North Katanga provinces. In 1965, Kabila set up a cross-border rebel operation from Kigoma, Tanzania, across Lake Tanganyika.
Che Guevara.
Che Guevara assisted Kabila for a short time in 1965. Guevara had appeared in the Congo with approximately 100 men who planned to bring about a Cuban-style revolution. Guevara judged Kabila (then 26) as "not the man of the hour" he had alluded to, being too distracted. This, in Guevara's opinion, accounted for Kabila showing up days late at times to provide supplies, aid, or backup to Guevara's men. The lack of cooperation between Kabila and Guevara contributed to the suppression of the revolt that same year.
In Guevara's view, of all of the people he met during his campaign in Congo, only Kabila had "genuine qualities of a mass leader"; but Guevara castigated Kabila for a lack of "revolutionary seriousness". After the failure of the rebellion, Kabila turned to smuggling gold and timber on Lake Tanganyika. He also ran a bar in Tanzania.
Marxist mini-state (1967–1988).
In 1967, Kabila and his remnant of supporters moved their operation into the mountainous Fizi – Baraka area of South Kivu in the Congo, and founded the People's Revolutionary Party (PRP). With the support of the People's Republic of China, the PRP created a secessionist Marxist state in South Kivu province, west of Lake Tanganyika.
The PRP state came to an end in 1988 and Kabila disappeared and was widely believed to be dead. While in Kampala, Kabila reportedly met Yoweri Museveni, the future president of Uganda. Museveni and former Tanzanian President Julius Nyerere later introduced Kabila to Paul Kagame, who would become president of Rwanda. These personal contacts became vital in mid-1990s, when Uganda and Rwanda sought a Congolese face for their intervention in Zaire.
First Congo War.
Kabila returned in October 1996, leading ethnic Tutsis from South Kivu against Hutu forces, marking the beginning of the First Congo War. With support from Uganda, Rwanda, and Burundi, Kabila pushed his forces into a full-scale rebellion against Mobutu as the Alliance of Democratic Forces for the Liberation of Congo-Zaire (ADFL).
By mid-1997, the ADFL had almost completely overrun the country and the remains of Mobutu's army. Only the country's decrepit infrastructure slowed Kabila's forces down; in many areas, the only means of transit were irregularly used dirt paths. Following failed peace talks held on board the South African ship SAS "Outeniqua", Mobutu fled into exile on May 16.
The next day, from his base in Lubumbashi, Kabila proclaimed himself president. Kabila suspended the Constitution, and changed the name of the country from Zaire to the Democratic Republic of the Congo—the country's official name from 1964 to 1971. He made his grand entrance into Kinshasa on May 20 and was sworn in on May 31, officially commencing his term as president.
Presidency (1997–2001).
Kabila had been a committed Marxist, but his policies at this point were a mix of capitalism and collectivism. He declared that elections would not be held for two years, since it would take him at least that long to restore order. While some in the West hailed Kabila as representing a "new breed" of African leadership, critics charged that Kabila's policies differed little from his predecessor's, being characterised by authoritarianism, corruption, and human rights abuses. As early as late 1997, Kabila was being denounced as "another Mobutu."
Kabila was also accused of self-aggrandizing tendencies, including trying to set up a personality cult, with the help of Mobutu's former minister of information, Dominique Sakombi Inongo. Sakombi Inongo branded Kabila as "the Mzee," and posters reading "Here is the man we needed" (French: "Voici l'homme que nous avions besoin") appeared all over the country.
By 1998, Kabila's former allies in Uganda and Rwanda had turned against him and backed a new rebellion of the Rally for Congolese Democracy (RCD), the Second Congo War. Kabila found new allies in Angola, Namibia, and Zimbabwe, and managed to hold on in the south and west of the country and by July 1999, peace talks led to the withdrawal of most foreign forces.
Assassination.
Kabila was shot during the afternoon of January 16, 2001 by one of his bodyguards, Rashidi Muzele, who was killed as he attempted to flee the scene. His assassination was committed by some of his bodyguards and masterminded by Rwanda, according to a Rwandan former intelligence chief and allegations made by DRCongo's officials. A Lebanese diamond dealer allegedly organised the logistics of the assassination, according to the documentary film "Murder in Kinshasa", made by Marlène Rabaud and Arnaud Zajtman.
Eleven Lebanese nationals were executed in the evening of the assassination as part of a punitive campaign by the DRC's authorities who managed to keep power, despite the assassination of their President. The exact circumstances are still disputed. Kabila reportedly died on the spot, according to DRC's then health minister Dr Mashako Mamba, who was in the next door office when Kabila was shot and arrived immediately after the assassination. The government claimed that Kabila was still alive, however, when he was flown to a hospital in Zimbabwe after he was shot so that DRC authorities could organise the tense succession.
The Congolese government announced that he had died of his wounds on January 18. One week later, his body was returned to Congo for a state funeral and his son, Joseph, became president eight days later. By doing so, DRC officials were accomplishing the "verbal testimony" of the deceased President. Then Justice Minister Mwenze Kongolo and Laurent-Désiré Kabila's aide de camp Eddy Kapend have reported that Laurent Kabila had told them that his son Joseph, then number two of the army, should take over, if Laurent-Désiré Kabila was to pass away.
Aftermath.
The investigation into Kabila's assassination led to 135 people – including 4 children – being tried before a special military tribunal. The alleged ringleader, Colonel Eddy Kapend (one of Kabila's cousins), and 25 others were sentenced to death in January 2003, but not executed. Of the other defendants 64 were jailed, with sentences from six months to life, and 45 were exonerated. Some individuals were also accused of being involved in a plot to overthrow his son. Among them was Kabila's special advisor Emmanuel Dungia, former ambassador to South Africa. Many people believe the trial was flawed and the convicted defendants are innocent.

</doc>
<doc id="49369" url="http://en.wikipedia.org/wiki?curid=49369" title="1928 Winter Olympics">
1928 Winter Olympics

The 1928 Winter Olympics, officially known as the II Olympic Winter Games (French: Les "IIes Jeux olympiques d'hiver") (German: "Olympische Winterspiele 1928") (Italian: "II Giochi olimpici invernali") (Romansch: "Gieus olimpics d'enviern 1928"), were a winter multi-sport event which was celebrated February 11–19, 1928 in St. Moritz, Switzerland. The 1928 Games were the first true "Winter Olympics" held on its own as they were not in conjunction with a "Summer Olympics". The preceding 1924 Games were retroactively renamed the inaugural Winter Olympics, though they had been in fact part of the 1924 Summer Olympics. All preceding Winter Events of the Olympic Games were the winter sports part of the schedule of the Summer Games, and not held as a separate Winter Games. These games also replaced the now redundant Nordic Games, that were held quadrennially since early in the century.
Fluctuating weather conditions made these Olympics memorable. The opening ceremony was held in a blizzard.[10] In contrast, warm weather conditions plagued the Olympics for the remainder of the Games, requiring cancellations of one event with temperatures as high as 25 °C (77 °F). (See further description at the Wikipedia main article on Winter Olympic Games.)
Events.
Medals were awarded in 14 events contested in 4 sports (8 disciplines).
Participating nations.
Athletes from 25 nations competed at these Games, up from 16 in 1924. Nations making their first appearance at the Winter Olympic Games were Argentina (first participation of a delegation coming from a country belonging to the Southern Hemisphere), Estonia, Germany, Japan, Lithuania, Luxembourg, Mexico, the Netherlands, and Romania.

</doc>
<doc id="49370" url="http://en.wikipedia.org/wiki?curid=49370" title="Anthony Burgess">
Anthony Burgess

John Anthony Burgess Wilson, FRSL (; 25 February 1917 – 22 November 1993) – who published under the pen name Anthony Burgess – was an English writer and composer. From relatively modest beginnings in a Catholic family in Manchester, he eventually became one of the best known English literary figures of the latter half of the twentieth century.
Although Burgess was predominantly a comic writer, his dystopian satire "A Clockwork Orange" remains his best known novel. In 1971 it was adapted into a highly controversial film by Stanley Kubrick, which Burgess said was chiefly responsible for the popularity of the book. Burgess produced numerous other novels, including the Enderby quartet, and "Earthly Powers", regarded by most critics as his greatest novel. He wrote librettos and screenplays, including for the 1977 TV mini-series "Jesus of Nazareth". He worked as a literary critic, including for "The Observer" and "The Guardian", and wrote studies of classic writers, notably James Joyce. A versatile linguist, Burgess lectured in phonetics, and translated "Cyrano de Bergerac", "Oedipus the King" and the opera "Carmen", among others.
Burgess also composed over 250 musical works; he sometimes claimed to consider himself as much a composer as an author, although he enjoyed considerably more success in writing.
Biography.
Early life.
Burgess was born at 91 Carisbrook Street in Harpurhey, a suburb of Manchester, to Catholic parents (his mother was a convert), Joseph and Elizabeth Wilson. He described his background as lower middle class; growing up during the Great Depression, the Wilsons were fairly well off, as the demand for their tobacco and alcohol wares remained constant . He was known in childhood as Jack, Little Jack, and Johnny Eagle. At his confirmation, the name Anthony was added and he became John Anthony Burgess Wilson. He began using the pen name Anthony Burgess upon the publication of his 1956 novel "Time for a Tiger".
His mother Elizabeth (née Burgess) died at the age of 30 at home on 19 November 1918, during the 1918 flu pandemic. The causes listed on her death certificate were influenza, acute pneumonia, and cardiac failure. His sister Muriel had died four days earlier on 15 November from influenza, broncho-pneumonia, and cardiac failure, aged eight. Burgess believed he was resented by his father, Joseph Wilson, for having survived, when his mother and sister did not.
After the death of his mother, Burgess was raised by his maternal aunt, Ann Bromley, in Crumpsall with her two daughters. During this time, Burgess's father worked as a bookkeeper for a beef market by day, and in the evening played piano at a public house in Miles Platting. After he married the landlady of this pub, Margaret Dwyer, in 1922, Burgess was raised by his father and stepmother. By 1924 the couple had established a tobacconist and off-licence business with four properties. On 18 April 1938, Joseph Wilson died from cardiac failure, pleurisy, and influenza at the age of 55, leaving no inheritance despite his apparent business success. Burgess' stepmother died of a heart attack in 1940.
Burgess has said of his largely solitary childhood: "I was either distractedly persecuted or ignored. I was one despised ... Ragged boys in gangs would pounce on the well-dressed like myself." He attended St. Edmund's Elementary School before moving on to Bishop Bilsborrow Memorial Elementary School, both Catholic schools, in Moss Side. He later reflected: "When I went to school I was able to read. At the Manchester elementary school I attended, most of the children could not read, so I was ... a little apart, rather different from the rest." Good grades resulted in a place at Xaverian College (1928–1937). As a young child he did not care about music, until he heard on his home-built radio "a quite incredible flute solo", which he characterised as "sinuous, exotic, erotic," and became spellbound. Eight minutes later the announcer told him he had been listening to "Prélude à l'après-midi d'un faune" by Claude Debussy. He referred to this as a "psychedelic moment ... a recognition of verbally inexpressible spiritual realities." When Burgess announced to his family that he wanted to be a composer, they objected as "there was no money in it." Music was not taught at his school, but at about age 14 he taught himself to play the piano. Burgess had originally hoped to study music at university, but the music department at the Victoria University of Manchester turned down his application because of poor grades in physics. So instead he studied English language and literature there between 1937 and 1940, graduating with a Bachelor of Arts. His thesis concerned Marlowe's "Doctor Faustus", and he graduated with an upper second-class honours, which he found disappointing. When grading one of Burgess's term papers, the historian A.J.P. Taylor, wrote: "Bright ideas insufficient to conceal lack of knowledge."
Burgess met Llewela "Lynne" Isherwood Jones at the University where she was studying economics, politics and modern history, graduating in 1942 with an upper second-class. She reportedly claimed to be a distant relative of Christopher Isherwood, although the Lewis and Biswell biographies dispute this. Burgess and Jones were married on 22 January 1942.
Military service.
Burgess spent six weeks in 1940 as an army recruit in Eskbank before becoming a Nursing Orderly Class 3 in the Royal Army Medical Corps. During his service he was unpopular and was involved in incidents such as knocking off a corporal's cap and polishing the floor of a corridor to make people slip. In 1941 Burgess was pursued by military police of the British Armed Forces for desertion after overstaying his leave from Morpeth military base with his future bride Lynne. In 1942 he asked to be transferred to the Army Educational Corps and despite his loathing of authority he was promoted to sergeant. During the blackout his pregnant wife Lynne was beaten and raped by four American deserters in her home and perhaps as a result she lost the child. Burgess, stationed at the time in Gibraltar, was denied leave to see her.
At his stationing in Gibraltar, which he later wrote about in "A Vision of Battlements", he worked as a training college lecturer in speech and drama, teaching alongside Ann McGlinn in German, French and Spanish. McGlinn's communist ideology would have a major influence on his later novel "A Clockwork Orange". Burgess played a key role in "The British Way and Purpose" programme, designed to reintroduce members of the forces to the peacetime socialism of the post-war years in Britain. He was an instructor for the Central Advisory Council for Forces Education of the Ministry of Education. Burgess' flair for languages was noticed by army intelligence and he took part in debriefings of Dutch expatriates and Free French who found refuge in Gibraltar during the war. In the neighbouring Spanish town of La Línea de la Concepción he was arrested for insulting General Franco but released from custody shortly after the incident.
Early teaching career.
Burgess left the army in 1946 with the rank of sergeant-major and was for the next four years a lecturer in speech and drama at the Mid-West School of Education near Wolverhampton and at the Bamber Bridge Emergency Teacher Training College near Preston. Burgess taught in the extramural department of Birmingham University (1946–50).
In late 1950 he began working as a secondary school teacher at Banbury Grammar School (now Banbury School) teaching English literature. In addition to his teaching duties he supervised sports and ran the school's drama society. He organised a number of amateur theatrical events in his spare time. These involved local people and students and included productions of T. S. Eliot's "Sweeney Agonistes". Reports from his former students and colleagues indicate that he cared deeply about teaching.
With financial assistance provided by Lynne's father the couple were able to put a down payment on a cottage in the village of Adderbury, close to Banbury. He named the cottage "Little Gidding" after one of Eliot's "Four Quartets". Burgess cut his journalistic teeth in Adderbury, writing several articles for the local newspaper, the "Banbury Guardian".
Malaya.
In 1954, Burgess joined the British Colonial Service as a teacher and education officer in Malaya, initially stationed at Kuala Kangsar in Perak, in what were then known as the Federated Malay States. Here he taught at the "Malay College" (now Malay College Kuala Kangsar – MCKK), modeled on English public school lines. In addition to his teaching duties, he was a housemaster in charge of students of the preparatory school, who were housed at a Victorian mansion known as "King's Pavilion". A variety of the music he wrote there was influenced by the country, notably Sinfoni Melayu for orchestra and brass band, which included cries of Merdeka (independence) from the audience. No score, however, is extant.
Burgess and his wife had occupied a noisy apartment where privacy was minimal, and this caused resentment. Following a dispute with the Malay College's principal about this, Burgess was reposted to the Malay Teachers' Training College at Kota Bharu, Kelantan. Burgess attained fluency in Malay, spoken and written, achieving distinction in the examinations in the language set by the colonial office. He was rewarded with a salary increase for his proficiency in the language.
He devoted some of his free time in Malaya to creative writing "as a sort of gentlemanly hobby, because I knew there wasn't any money in it," and published his first novels: "Time for a Tiger", "The Enemy in the Blanket" and "Beds in the East". These became known as "The Malayan Trilogy" and were later published in one volume as "The Long Day Wanes".
Brunei.
After a brief period of leave in Britain during 1958, Burgess took up a further Eastern post, this time at the Sultan Omar Ali Saifuddin College in Bandar Seri Begawan, Brunei. Brunei had been a British protectorate since 1888, and was not to achieve independence until 1984. In the sultanate, Burgess sketched the novel that, when it was published in 1961, was to be entitled "Devil of a State" and, although it dealt with Brunei, for libel reasons the action had to be transposed to an imaginary East African territory similar to Zanzibar, named Dunia. In his autobiography "Little Wilson and Big God" (1987) Burgess wrote:"This novel was, is, about Brunei, which was renamed Naraka, Malay-Sanskrit for 'hell.' Little invention was needed to contrive a large cast of unbelievable characters and a number of interwoven plots. Though completed in 1958, the work was not published until 1961, for what it was worth it was made a choice of the book society. Heinemann, my publisher, was doubtful about publishing it: it might be libelous. I had to change the setting from Brunei to an East African one. Heinemann was right to be timorous. In early 1958, "The Enemy in the Blanket" appeared and at once provoked a libel suit."
About this time Burgess collapsed in a Brunei classroom while teaching history and was diagnosed as having an inoperable brain tumour. Burgess was given just a year to live, prompting him to write several novels to get money to provide for his widow. He gave a different account, however, to Jeremy Isaacs in a "Face to Face" interview on the BBC "The Late Show" (21 March 1989). He said "Looking back now I see that I was driven out of the Colonial Service. I think possibly for political reasons that were disguised as clinical reasons." He alluded to this in an interview with Don Swaim, explaining that his wife Lynne had said something "obscene" to the British Queen's consort, the Duke of Edinburgh, during an official visit, and the colonial authorities turned against him. He had already earned their displeasure, he told Swaim, by writing articles in the newspaper in support of the revolutionary opposition party the Parti Rakyat Brunei, and for his friendship with its leader Dr. Azahari. Burgess' biographers attribute the incident to the author's notorious mythomania. Geoffrey Grigson writes, He was, however, suffering from the effects of prolonged heavy drinking (and associated poor nutrition), of the often oppressive Southeast Asian climate, of chronic constipation, and of overwork and professional disappointment. As he put it, the scions of the sultans and of the elite in Brunei "did not wish to be taught", because the free-flowing abundance of oil guaranteed their income and privileged status. He may also have wished for a pretext to abandon teaching and get going full-time as a writer, having made a late start.
Repatriate years.
Burgess was invalided home in 1959 and relieved of his position in Brunei. He spent some time in the neurological ward of a London hospital (see "The Doctor is Sick") where he underwent cerebral tests that found no illness. On discharge, benefiting from a sum of money which Lynne Burgess had inherited from her father, together with their savings built up over six years in the East, he decided to become a full-time writer. The couple lived first in an apartment in Hove, near Brighton. They later moved to a semi-detached house called "Applegarth" in Etchingham, approximately a mile from the Jacobean house where Rudyard Kipling had lived in Burwash, and one mile from the Robertsbridge home of Malcolm Muggeridge. Upon the death of Burgess's father-in-law, the couple used their inheritance to decamp to a terraced town house in Chiswick. This provided convenient access to the White City BBC television studios where he later became a frequent guest. During these years Burgess became a regular drinking partner of the novelist William S. Burroughs. Their meetings took place in London and Tangiers.
A sea voyage the couple took with the Baltic Line from Tilbury to Leningrad in June 1961 resulted in the novel "Honey for the Bears". He wrote in his autobiographical "You've Had Your Time" (1990), that in re-learning Russian at this time, he found inspiration for the Russian-based slang Nadsat that he created for "A Clockwork Orange", going on to note "I would resist to the limit any publisher's demand that a glossary be provided."
Liliana Macellari, an Italian translator twelve years younger than Burgess, came across his novels "Inside Mr Enderby" and "A Clockwork Orange", while writing about English fiction. The two first met in 1963 over lunch in Chiswick and began an affair. In 1964, Liana gave birth to Burgess' son, Paolo Andrea. The affair was hidden from Burgess's now-alcoholic wife, whom he refused to leave for fear of offending his cousin (by Burgess's stepmother, Margaret Dwyer Wilson), George Patrick Dwyer, then the Roman Catholic Bishop of Leeds.
Lynne Burgess died from cirrhosis of the liver, on 20 March 1968. Six months later, in September 1968, Burgess married Liana, acknowledging her four-year-old boy as his own, although the birth certificate listed Roy Halliday, Liana's former partner, as the father. Paolo Andrea (also known as Andrew Burgess Wilson) died in London in 2002, aged 37. Liana died in 2007.
Tax exile.
Burgess was a Conservative (though, as he clarified in an interview with "The Paris Review", his political views could be considered "a kind of anarchism" since his ideal of a "Catholic Jacobite imperial monarch" wasn't practicable), a (lapsed) Catholic and Monarchist, harbouring a distaste for all republics. He believed that socialism for the most part was "ridiculous" but did "concede that socialized medicine is a priority in any civilized country today." To avoid the 90% tax the family would have incurred because of their high income, they left Britain and toured Europe in a Bedford Dormobile motor-home. During their travels through France and across the Alps, Burgess wrote in the back of the van as Liana drove. In this period, he wrote novels and produced film scripts for Lew Grade and Franco Zeffirelli. His first place of residence after leaving England was Lija, Malta (1968–70). The negative reaction from a lecture that Burgess delivered to an audience of Catholic priests in Malta precipitated a move by the couple to Italy. The Burgesses maintained a flat in Rome, a country house in Bracciano, and a property in Montalbuccio. On hearing rumours of a mafia plot to kidnap Paolo-Andrea while the family was staying in Rome, Burgess decided to move to Monaco in 1975. Burgess was also motivated to move to the tax haven of Monaco as the country did not level income tax and widows were exempt from death duties, a form of taxation on their husband's estates.
The couple also had a villa in Provence, in Callian, Var, France, and an apartment just off Baker Street, London. 
Burgess lived for two years in the United States, working as a visiting professor at Princeton University with the creative writing program (1970) and as a distinguished professor at the City College of New York (1972). At City College he was a close colleague and friend of Joseph Heller. He went on to teach creative writing at Columbia University and was writer-in-residence at the University of North Carolina at Chapel Hill (1969) and at the University at Buffalo (1976). He lectured on the novel at the University of Iowa in 1975. Eventually he settled in Monaco in 1976, where he was active in the local community, becoming a co-founder in 1984 of the Princess Grace Irish Library, a centre for Irish cultural studies.
Although Burgess lived not far from Graham Greene, whose house was in Antibes, Greene became aggrieved shortly before his death by comments in newspaper articles by Burgess, and broke off all contact. Gore Vidal revealed in his 2006 memoir "Point to Point Navigation" that Greene disapproved of Burgess's appearance on various European television stations to discuss his (Burgess') books. Vidal recounts that Greene apparently regarded a willingness to appear on television as something that ought to be beneath a writer's dignity. "He talks about his books", Vidal quotes an exasperated Greene as saying.
During this time, Burgess spent much time at his chalet two kilometres outside Lugano, Switzerland.
Death.
Burgess wrote: "I shall die somewhere in the Mediterranean lands, with an inaccurate obituary in the Nice-Matin, unmourned, soon forgotten." In fact he died in the country of his birth. He returned to Twickenham, an outer suburb of London, where he owned a house, to await death. Burgess died on 22 November 1993 from lung cancer, at the Hospital of St John & St Elizabeth in London. His ashes were inurned at the cemetery in Monaco. The epitaph on Burgess's marble memorial stone, reads "Abba Abba." The phrase has several connotations. It means "Father, father" in Aramaic, Arabic, Hebrew and other Semitic languages. It is Burgess's initials forwards and backwards; part of the rhyme scheme for the Petrarchan sonnet; and the title of Burgess's 22nd novel, concerning the death of Keats. Eulogies at his memorial service at St Paul's, Covent Garden, London in 1994 were delivered by the journalist Auberon Waugh and the novelist William Boyd. "The Times" obituary heralded the author as "a great moralist." At his death he was worth $3 million, and left a large European property portfolio of houses and apartments.
Life in music.
An accomplished musician, Burgess composed regularly throughout his life, and once said, "I wish people would think of me as a musician who writes novels, instead of a novelist who writes music on the side." Several of his pieces were broadcast during his lifetime on BBC Radio. His Symphony No. 3 in C was premiered by the University of Iowa orchestra in Iowa City in 1975. Burgess described his "Sinfoni Melayu" as an attempt to "combine the musical elements of the country into a synthetic language which called on native drums and xylophones." The structure of "Napoleon Symphony: A Novel in Four Movements" (1974) was modelled on Beethoven's Eroica symphony, while "Mozart and the Wolf Gang" (1991) mirrors the sound and rhythm of Mozartian composition, among other things attempting a fictional representation of Symphony No.40. Beethoven's Symphony No. 9 features prominently in "A Clockwork Orange" (and in Stanley Kubrick's film version of the novel). Many of his unpublished compositions are listed in "This Man and Music". He wrote a good deal of music for recorder as his son played the instrument. Several of his pieces for recorder and piano including the Sonata No. 1, Sonatina and 'Tre Pezzetti' have been included on a major CD release from recorder player John Turner and pianist Harvey Davies; the double album also includes related music from 15 other composers and is titled 'Anthony Burgess – The Man and his Music' (Metier records, release September 2013).
Burgess produced a translation of Bizet's "Carmen" which was performed by the English National Opera, and wrote for the 1973 Broadway musical "Cyrano", using his own adaptation of the original Rostand play as his basis. He created "Blooms of Dublin" in 1982, an operetta based on James Joyce's "Ulysses" (televised for the BBC) and wrote a libretto for Weber's "Oberon", performed by the Edinburgh-based Scottish Opera.
On the BBC's "Desert Island Discs" radio programme in 1966, Burgess chose as his favourite music Purcell's "Rejoice in the Lord Alway"; Bach's "Goldberg Variations" No. 13; Elgar's Symphony No. 1 in A-flat major; Wagner's "Walter's Trial Song" from "Die Meistersinger von Nürnberg"; Debussy's "Fêtes" from "Nocturnes"; Lambert's "The Rio Grande"; Walton's Symphony No. 1 in B-flat minor; and Vaughan Williams' "On Wenlock Edge".
Linguistics.
"Burgess's linguistic training", wrote Raymond Chapman and Tom McArthur in "The Oxford Companion to the English Language", "is shown in dialogue enriched by distinctive pronunciations and the niceties of register." During his years in Malaya, and after he had mastered Jawi, the Arabic script adapted for Malay, Burgess taught himself the Persian language, after which he produced a translation of Eliot's "The Waste Land" into Persian (unpublished). He worked on an anthology of the best of English literature translated into Malay, which failed to achieve publication. Burgess's published translations include two different versions of "Cyrano de Bergerac", "Oedipus the King" and "Carmen".
Burgess's interest in language was reflected in the invented, Anglo-Russian teen slang of "A Clockwork Orange" (Nadsat), and in the movie "Quest for Fire" (1981), for which he invented a prehistoric language ("Ulam") for the characters. His interest is reflected in his characters. In "The Doctor is Sick", Dr Edwin Spindrift is a lecturer in linguistics who escapes from a hospital ward which is peopled, as the critic Saul Maloff put it in a review, with "brain cases who happily exemplify varieties of English speech." Burgess, who had lectured on phonetics at the University of Birmingham in the late 1940s, investigates the field of linguistics in "Language Made Plain" and "A Mouthful of Air".
The depth of Burgess's multilingual proficiency came under discussion in Roger Lewis's . Lewis claimed that during production in Malaysia of the BBC documentary "A Kind of Failure" (1982), Burgess's supposedly fluent Malay was not understood by waitresses at a restaurant where they were filming. It was claimed that the documentary's director deliberately kept these moments intact in the film to expose Burgess's linguistic pretensions. A letter from David Wallace that appeared in the magazine of the London "Independent on Sunday" newspaper on 25 November 2002 shed light on the affair. Wallace's letter read, in part:
... the tale was inaccurate. It tells of Burgess, the great linguist, "bellowing Malay at a succession of Malayan waitresses" but "unable to make himself understood". The source of this tale was a 20-year-old BBC documentary ... [The suggestion was] that the director left the scene in, in order to poke fun at the great author. Not so, and I can be sure, as I was that director ... The story as seen on television made it clear that Burgess knew that these waitresses were not Malay. It was a Chinese restaurant and Burgess's point was that the ethnic Chinese had little time for the government-enforced national language, Bahasa Malaysia [i.e. Malay]. Burgess may well have had an accent, but he did speak the language; it was the girls in question who did not.
Lewis may not have been fully aware of the fact that a quarter of Malaysia's population is made up of Hokkien- and Cantonese-speaking Chinese. However, Malay had been installed as the National Language with the passing of the Language Act of 1967. By 1982 all national primary and secondary schools in Malaysia would have been teaching with Bahasa Melayu as a base language (see Harold Crouch, "Government and Society in Malaysia", Ithaca and London: Cornell University Press, 1996).
Works.
Novels.
His Malayan trilogy "The Long Day Wanes" was Burgess's first published fiction. Its three books are "Time for a Tiger," "The Enemy in the Blanket" and "Beds in the East." It was Burgess's ambition to become "the true fictional expert on Malaya." In these works, Burgess was working in the tradition established by Kipling for British India, and Conrad and Maugham for Southeast Asia. Burgess operated more in the mode of Orwell, who had a good command of Urdu and Burmese (necessary for Orwell's work as a police officer) and Kipling, who spoke Hindi (having learnt it as a child). Like his fellow English expats in Asia, Burgess had excellent spoken and written command of his operative language(s), both as a novelist and speaker, including Malay.
Burgess's repatriate years (c. 1960–69) produced "Enderby" and "The Right to an Answer," which touches on the theme of death and dying, and "One Hand Clapping," a satire on the vacuity of popular culture. "The Worm and the Ring" (1961) had to be withdrawn from circulation under the threat of libel action from one of Burgess's former colleagues, a school secretary.
His dystopian novel "A Clockwork Orange" was published in 1962. It was inspired initially by an incident during the Second World War in which his wife Lynne was robbed, assaulted and violated by deserters from the US Army in London during the blackout. The event may have contributed to her subsequent miscarriage. The book was an examination of free will and morality. The young anti-hero, Alex, captured after a short career of violence and mayhem, undergoes a course of aversion therapy treatment to curb his violent tendencies. This results in making him defenceless against other people and unable to enjoy some of his favourite music that, besides violence, had been an intense pleasure for him. In the non-fiction book "Flame into Being" (1985) Burgess described "A Clockwork Orange" as "a jeu d'esprit knocked off for money in three weeks, it became known as the raw material for a film which seemed to glorify sex and violence." He added "the film made it easy for readers of the book to misunderstand what it was about, and the misunderstanding will pursue me till I die." Near the time of publication the final chapter was cut from the American edition of the book. Burgess had written "A Clockwork Orange" with twenty-one chapters, meaning to match the age of majority. "21 is the symbol of human maturity, or used to be, since at 21 you got to vote and assumed adult responsibility," Burgess wrote in a foreword for a 1986 edition. Needing a paycheck and thinking that the publisher was "being charitable in accepting the work at all," Burgess accepted the deal and allowed "A Clockwork Orange" to be published in the US with the twenty-first chapter omitted. Stanley Kubrick's film adaptation of "A Clockwork Orange" was based on the American edition, and thus helped to perpetuate the loss of the last chapter.
In Martin Seymour-Smith's "Novels and Novelists: A Guide to the World of Fiction," Burgess related that he would often prepare a synopsis with a name-list before beginning a project. Seymour-Smith wrote: "Burgess believes overplanning is fatal to creativity and regards his unconscious mind and the act of writing itself as indispensable guides. He does not produce a draft of a whole novel but prefers to get one page finished before he goes on to the next, which involves a good deal of revision and correction."
"" is a fictional recreation of Shakespeare's love-life and an examination of the supposedly partly syphilitic sources of the bard's imaginative vision. The novel, which drew on Edgar I. Fripp's 1938 biography "Shakespeare, Man and artist," won critical acclaim and placed Burgess among the first rank novelists of his generation. "M/F" (1971) was listed by the writer himself as one of the works of which he was most proud. "Beard's Roman Women" was revealing on a personal level, dealing with the death of his first wife, his bereavement, and the affair that led to his second marriage. In "Napoleon Symphony," Burgess brought Bonaparte to life by shaping the novel's structure to Beethoven's "Eroica" symphony. The novel contains a portrait of an Arab and Muslim society under occupation by a Christian western power (Egypt by Catholic France). In the 1980s, religious themes began to feature heavily ("The Kingdom of the Wicked," "Man of Nazareth," "Earthly Powers"). Though Burgess lapsed from Catholicism early in his youth, the influence of the Catholic "training" and worldview remained strong in his work all his life. This is notable in the discussion of free will in "A Clockwork Orange," and in the apocalyptic vision of devastating changes in the Catholic Church – due to what can be understood as Satanic influence – in "Earthly Powers" (1980).
Burgess kept working through his final illness and was writing on his deathbed. The late novel "Any Old Iron" is a generational saga of two families, one Russian-Welsh, the other Jewish, encompassing the sinking of the Titanic, World War I, the Russian Revolution, the Spanish Civil War, World War II, the early years of the State of Israel, and the rediscovery of Excalibur. "A Dead Man in Deptford," about Christopher Marlowe, is a companion novel to "." The verse novel "" was published posthumously.
Critical studies.
Burgess started his career as a critic. His "English Literature, A Survey for Students", was aimed at newcomers to the subject. He followed this with "The Novel To-day" (Longmans, 1963) and "The Novel Now: A Student's Guide to Contemporary Fiction" (New York: W.W. Norton and Company, 1967). He wrote the Joyce studies "Here Comes Everybody: An Introduction to James Joyce for the Ordinary Reader" (also published as "Re Joyce") and "Joysprick: An Introduction to the Language of James Joyce". Also published was "A Shorter 'Finnegans Wake",' Burgess's abridgement. His 1970 "Encyclopædia Britannica" entry on the novel (under "Novel, the") is regarded as a classic of the genre. Burgess wrote full-length critical studies of William Shakespeare, Ernest Hemingway and D. H. Lawrence, as well as "Ninety-nine Novels: The Best in English since 1939".
Screenwriting.
Burgess wrote the screenplays for "Moses the Lawgiver" (Gianfranco De Bosio 1974), "Jesus of Nazareth" (Franco Zeffirelli 1977), and "A.D." (Stuart Cooper, 1985). Burgess was co-writer of the script for the TV series "Sherlock Holmes and Doctor Watson" (1980). The film treatments he produced include "Amundsen", "Attila", "The Black Prince", "Cyrus the Great", "Dawn Chorus", "The Dirty Tricks of Bertoldo", "Eternal Life", "Onassis", "Puma", "Samson and Delilah", "Schreber", "The Sexual Habits of the English Middle Class", "Shah", "That Man Freud" and "Uncle Ludwig". Burgess devised a Stone Age language for "La Guerre du Feu" ("Quest for Fire"; Jean-Jacques Annaud, 1981).
Burgess penned many unpublished scripts, including "Will!" or "The Bawdy Bard" about Shakespeare, based on the novel "Nothing Like The Sun". Encouraged by the success of "" (a parody of James Bond adventures), Burgess wrote a screenplay for "The Spy Who Loved Me", also rejected, although the huge submarine silo seen in the finished film was reportedly Burgess's inspiration.

</doc>
<doc id="49373" url="http://en.wikipedia.org/wiki?curid=49373" title="Grid computing">
Grid computing

Grid computing is the collection of computer resources from multiple locations to reach a common goal. The grid can be thought of as a distributed system with non-interactive workloads that involve a large number of files. Grid computing is distinguished from conventional high performance computing systems such as cluster computing in that grid computers have each node set to perform a different task/application. Grid computers also tend to be more heterogeneous and geographically dispersed (thus not physically coupled) than cluster computers. Although a single grid can be dedicated to a particular application, commonly a grid is used for a variety of purposes. Grids are often constructed with general-purpose grid middleware software libraries. Grid sizes can be quite large. 
Grids are a form of distributed computing whereby a “super virtual computer” is composed of many networked loosely coupled computers acting together to perform large tasks. For certain applications, “distributed” or “grid” computing, can be seen as a special type of parallel computing that relies on complete computers (with onboard CPUs, storage, power supplies, network interfaces, etc.) connected to a computer network (private or public) by a conventional network interface, such as Ethernet. This is in contrast to the traditional notion of a supercomputer, which has many processors connected by a local high-speed computer bus.
Overview.
Grid computing combines computers from multiple administrative domains to reach a common goal, to solve a single task, and may then disappear just as quickly.
One of the main strategies of grid computing is to use middleware to divide and apportion pieces of a program among several computers, sometimes up to many thousands. Grid computing involves computation in a distributed fashion, which may also involve the aggregation of large-scale clusters.
The size of a grid may vary from small—confined to a network of computer workstations within a corporation, for example—to large, public collaborations across many companies and networks. "The notion of a confined grid may also be known as an intra-nodes cooperation whilst the notion of a larger, wider grid may thus refer to an inter-nodes cooperation".
Grids are a form of distributed computing whereby a “super virtual computer” is composed of many networked loosely coupled computers acting together to perform very large tasks. This technology has been applied to computationally intensive scientific, mathematical, and academic problems through volunteer computing, and it is used in commercial enterprises for such diverse applications as drug discovery, economic forecasting, seismic analysis, and back office data processing in support for e-commerce and Web services.
Coordinating applications on Grids can be a complex task, especially when coordinating the flow of information across distributed computing resources. Grid workflow systems have been developed as a specialized form of a workflow management system designed specifically to compose and execute a series of computational or data manipulation steps, or a workflow, in the Grid context.
Comparison of grids and conventional supercomputers.
“Distributed” or “grid” computing in general is a special type of parallel computing that relies on complete computers (with onboard CPUs, storage, power supplies, network interfaces, etc.) connected to a network (private, public or the Internet) by a conventional network interface producing commodity hardware, compared to the lower efficiency of designing and constructing a small number of custom supercomputers. The primary performance disadvantage is that the various processors and local storage areas do not have high-speed connections. This arrangement is thus well-suited to applications in which multiple parallel computations can take place independently, without the need to communicate intermediate results between processors. The high-end scalability of geographically dispersed grids is generally favorable, due to the low need for connectivity between nodes relative to the capacity of the public Internet.
There are also some differences in programming and deployment. It can be costly and difficult to write programs that can run in the environment of a supercomputer, which may have a custom operating system, or require the program to address concurrency issues. If a problem can be adequately parallelized, a “thin” layer of “grid” infrastructure can allow conventional, standalone programs, given a different part of the same problem, to run on multiple machines. This makes it possible to write and debug on a single conventional machine, and eliminates complications due to multiple instances of the same program running in the same shared memory and storage space at the same time.
Design considerations and variations.
One feature of distributed grids is that they can be formed from computing resources belonging to multiple individuals or organizations (known as multiple administrative domains). This can facilitate commercial transactions, as in utility computing, or make it easier to assemble volunteer computing networks.
One disadvantage of this feature is that the computers which are actually performing the calculations might not be entirely trustworthy. The designers of the system must thus introduce measures to prevent malfunctions or malicious participants from producing false, misleading, or erroneous results, and from using the system as an attack vector. This often involves assigning work randomly to different nodes (presumably with different owners) and checking that at least two different nodes report the same answer for a given work unit. Discrepancies would identify malfunctioning and malicious nodes. However, due to the lack of central control over the hardware, there is no way to guarantee that nodes will not drop out of the network at random times. Some nodes (like laptops or dialup Internet customers) may also be available for computation but not network communications for unpredictable periods. These variations can be accommodated by assigning large work units (thus reducing the need for continuous network connectivity) and reassigning work units when a given node fails to report its results in expected time.
The impacts of trust and availability on performance and development difficulty can influence the choice of whether to deploy onto a dedicated cluster, to idle machines internal to the developing organization, or to an open external network of volunteers or contractors. In many cases, the participating nodes must trust the central system not to abuse the access that is being granted, by interfering with the operation of other programs, mangling stored information, transmitting private data, or creating new security holes. Other systems employ measures to reduce the amount of trust “client” nodes must place in the central system such as placing applications in virtual machines.
Public systems or those crossing administrative domains (including different departments in the same organization) often result in the need to run on heterogeneous systems, using different operating systems and hardware architectures. With many languages, there is a trade off between investment in software development and the number of platforms that can be supported (and thus the size of the resulting network). Cross-platform languages can reduce the need to make this trade off, though potentially at the expense of high performance on any given node (due to run-time interpretation or lack of optimization for the particular platform). There are diverse scientific and commercial projects to harness a particular associated grid or for the purpose of setting up new grids. BOINC is a common one for various academic projects seeking public volunteers; more are listed at the end of the article.
In fact, the middleware can be seen as a layer between the hardware and the software. On top of the middleware, a number of technical areas have to be considered, and these may or may not be middleware independent. Example areas include SLA management, Trust and Security, Virtual organization management, License Management, Portals and Data Management. These technical areas may be taken care of in a commercial solution, though the cutting edge of each area is often found within specific research projects examining the field.
Market segmentation of the grid computing market.
For the segmentation of the grid computing market, two perspectives need to be considered: the provider side and the user side:
The provider side.
The overall grid market comprises several specific markets. These are the grid middleware market, the market for grid-enabled applications, the utility computing market, and the software-as-a-service (SaaS) market.
Grid middleware is a specific software product, which enables the sharing of heterogeneous resources, and Virtual Organizations. It is installed and integrated into the existing infrastructure of the involved company or companies, and provides a special layer placed among the heterogeneous infrastructure and the specific user applications. Major grid middlewares are Globus Toolkit, gLite, and UNICORE.
Utility computing is referred to as the provision of grid computing and applications as service either as an open grid utility or as a hosting solution for one organization or a VO. Major players in the utility computing market are Sun Microsystems, IBM, and HP.
Grid-enabled applications are specific software applications that can utilize grid infrastructure. This is made possible by the use of grid middleware, as pointed out above.
Software as a service (SaaS) is “software that is owned, delivered and managed remotely by one or more providers.” (Gartner 2007) Additionally, SaaS applications are based on a single set of common code and data definitions. They are consumed in a one-to-many model, and SaaS uses a Pay As You Go (PAYG) model or a subscription model that is based on usage. Providers of SaaS do not necessarily own the computing resources themselves, which are required to run their SaaS. Therefore, SaaS providers may draw upon the utility computing market. The utility computing market provides computing resources for SaaS providers.
The user side.
For companies on the demand or user side of the grid computing market, the different segments have significant implications for their IT deployment strategy. The IT deployment strategy as well as the type of IT investments made are relevant aspects for potential grid users and play an important role for grid adoption.
CPU scavenging.
CPU-scavenging, cycle-scavenging, or shared computing creates a “grid” from the unused resources in a network of participants (whether worldwide or internal to an organization). Typically this technique uses desktop computer instruction cycles that would otherwise be wasted at night, during lunch, or even in the scattered seconds throughout the day when the computer is waiting for user input or slow devices. In practice, participating computers also donate some supporting amount of disk storage space, RAM, and network bandwidth, in addition to raw CPU power.
Many volunteer computing projects, such as BOINC, use the CPU scavenging model. Since nodes are likely to go "offline" from time to time, as their owners use their resources for their primary purpose, this model must be designed to handle such contingencies.
History.
The term "grid computing" originated in the early 1990s as a metaphor for making computer power as easy to access as an electric power grid. The power grid metaphor for accessible computing quickly became canonical when Ian Foster and Carl Kesselman published their seminal work, "The Grid: Blueprint for a new computing infrastructure" (1999).
CPU scavenging and volunteer computing were popularized beginning in 1997 by distributed.net and later in 1999 by SETI@home to harness the power of networked PCs worldwide, in order to solve CPU-intensive research problems.
The ideas of the grid (including those from distributed computing, object-oriented programming, and Web services) were brought together by Ian Foster, Carl Kesselman, and Steve Tuecke, widely regarded as the "fathers of the grid". They led the effort to create the Globus Toolkit incorporating not just computation management but also storage management, security provisioning, data movement, monitoring, and a toolkit for developing additional services based on the same infrastructure, including agreement negotiation, notification mechanisms, trigger services, and information aggregation. While the Globus Toolkit remains the de facto standard for building grid solutions, a number of other tools have been built that answer some subset of services needed to create an enterprise or global grid.
In 2007 the term cloud computing came into popularity, which is conceptually similar to the canonical Foster definition of grid computing (in terms of computing resources being consumed as electricity is from the power grid). Indeed, grid computing is often (but not always) associated with the delivery of cloud computing systems as exemplified by the AppLogic system from 3tera.
Projects and applications.
Grid computing offers a way to solve Grand Challenge problems such as protein folding, financial modeling, earthquake simulation, and climate/weather modeling. Grids offer a way of using the information technology resources optimally inside an organization. They also provide a means for offering information technology as a utility for commercial and noncommercial clients, with those clients paying only for what they use, as with electricity or water.
Grid computing is being applied by the National Science Foundation's National Technology Grid, NASA's Information Power Grid, Pratt & Whitney, Bristol-Myers Squibb Co., and American Express.
One cycle-scavenging network is SETI@home, which was using more than 3 million computers to achieve 23.37 sustained teraflops (979 lifetime teraflops) as of September 2001[ [update]].
As of August 2009 Folding@home achieves more than 4 petaflops on over 350,000 machines.
The European Union funded projects through the framework programmes of the European Commission. BEinGRID (Business Experiments in Grid) was a research project funded by the European Commission as an Integrated Project under the Sixth Framework Programme (FP6) sponsorship program. Started on June 1, 2006, the project ran 42 months, until November 2009. The project was coordinated by Atos Origin. According to the project fact sheet, their mission is “to establish effective routes to foster the adoption of grid computing across the EU and to stimulate research into innovative business models using Grid technologies”. To extract best practice and common themes from the experimental implementations, two groups of consultants are analyzing a series of pilots, one technical, one business. The project is significant not only for its long duration, but also for its budget, which at 24.8 million Euros, is the largest of any FP6 integrated project. Of this, 15.7 million is provided by the European commission and the remainder by its 98 contributing partner companies. Since the end of the project, the results of BEinGRID have been taken up and carried forward by .
The Enabling Grids for E-sciencE project, based in the European Union and included sites in Asia and the United States, was a follow-up project to the European DataGrid (EDG) and evoled into the European Grid Infrastructure. This, along with the LHC Computing Grid (LCG), was developed to support experiments using the CERN Large Hadron Collider. A list of active sites participating within LCG can be found online as can real time monitoring of the EGEE infrastructure. The relevant software and documentation is also publicly accessible. There is speculation that dedicated fiber optic links, such as those installed by CERN to address the LCG's data-intensive needs, may one day be available to home users thereby providing internet services at speeds up to 10,000 times faster than a traditional broadband connection. The European Grid Infrastructure has been also used for other research activities and experiments such as the simulation of oncological clinical trials.
The distributed.net project was started in 1997.
The NASA Advanced Supercomputing facility (NAS) ran genetic algorithms using the Condor cycle scavenger running on about 350 Sun Microsystems and SGI workstations.
In 2001, United Devices operated the United Devices Cancer Research Project based on its Grid MP product, which cycle-scavenges on volunteer PCs connected to the Internet. The project ran on about 3.1 million machines before its close in 2007.
As of 2011, over 6.2 million machines running the open-source Berkeley Open Infrastructure for Network Computing (BOINC) platform are members of the World Community Grid, which tops the processing power of the current fastest supercomputer system (China's Tianhe-I).
Definitions.
Today there are many definitions of "grid computing":

</doc>
<doc id="49374" url="http://en.wikipedia.org/wiki?curid=49374" title="978">
978

Year 978 (CMLXXVIII) was a common year starting on Tuesday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By place.
Asia.
</onlyinclude>

</doc>
<doc id="49375" url="http://en.wikipedia.org/wiki?curid=49375" title="Larynx">
Larynx

The larynx (plural "larynges"; from the Greek λάρυγξ "lárynx"), commonly called the voice box, is an organ in the neck of amphibians, reptiles, and mammals involved in breathing, sound production, and protecting the trachea against food aspiration. It manipulates pitch and volume. The larynx houses the vocal folds (vocal cords), which are essential for phonation. The vocal folds are situated just below where the tract of the pharynx splits into the trachea and the esophagus.
Structure.
Cartilages.
There are 6 cartilages, three unpaired and three paired, that support the mammalian larynx and form its skeleton.
Unpaired cartilages:
Paired cartilages:
Muscles.
The muscles of the larynx are divided into "intrinsic" and "extrinsic" muscles.
The intrinsic muscles are divided into respiratory and the phonatory muscles (the muscles of phonation). The respiratory muscles move the vocal cords apart and serve breathing. The phonatory muscles move the vocal cords together and serve the production of voice. The extrinsic, passing between the larynx and parts around; and intrinsic, confined entirely. The main respiratory muscles are the posterior cricoarytenoid muscles. The phonatory muscles are divided into adductors (lateral cricoarytenoid muscles, arytenoid muscles) and tensors (cricothyroid muscles, thyroarytenoid muscles).
Intrinsic.
The intrinsic laryngeal muscles are responsible for controlling sound production. 
Notably, the only muscle capable of separating the vocal cords for normal breathing is the posterior cricoarytenoid. If this muscle is incapacitated on both sides, the inability to pull the vocal folds apart (abduct) will cause difficulty breathing. Bilateral injury to the recurrent laryngeal nerve would cause this condition. It is also worth noting that all muscles are innervated by the recurrent laryngeal branch of the vagus except the cricothyroid muscle, which is innervated by the external laryngeal branch of the superior laryngeal nerve (a branch of the vagus).
Extrinsic.
The extrinsic laryngeal muscles support and position the larynx within the trachea.
Innervation.
The larynx is innervated by branches of the vagus nerve on each side. Sensory innervation to the glottis and laryngeal vestibule is by the internal branch of the superior laryngeal nerve. The external branch of the superior laryngeal nerve innervates the cricothyroid muscle. Motor innervation to all other muscles of the larynx and sensory innervation to the subglottis is by the recurrent laryngeal nerve. While the sensory input described above is (general) visceral sensation (diffuse, poorly localized), the vocal fold also receives general somatic sensory innervation (proprioceptive and touch) by the superior laryngeal nerve.
Injury to the external laryngeal nerve causes weakened phonation because the vocal folds cannot be tightened. Injury to one of the recurrent laryngeal nerves produces hoarseness, if both are damaged the voice may or may not be preserved, but breathing becomes difficult.
Development.
In adult humans, the larynx is found in the anterior neck at the level of the C3–C6 vertebrae. It connects the inferior part of the pharynx (hypopharynx) with the trachea. The laryngeal skeleton consists of nine cartilages: three single (epiglottic, thyroid and cricoid) and three paired (arytenoid, corniculate, and cuneiform). The hyoid bone is not part of the larynx, though the larynx is suspended from the hyoid. The larynx extends vertically from the tip of the epiglottis to the inferior border of the cricoid cartilage. Its interior can be divided in supraglottis, glottis and subglottis.
In newborn infants, the larynx is initially at the level of the C2–C3 vertebrae, and is further forward and higher relative to its position in the adult body. The larynx descends as the child grows.
Function.
Sound generation.
Sound is generated in the larynx, and that is where pitch and volume are manipulated. The strength of expiration from the lungs also contributes to loudness.
Fine manipulation of the larynx is used to generate a source sound with a particular fundamental frequency, or pitch. This source sound is altered as it travels through the vocal tract, configured differently based on the position of the tongue, lips, mouth, and pharynx. The process of altering a source sound as it passes through the filter of the vocal tract creates the many different vowel and consonant sounds of the world's languages as well as tone, certain realizations of stress and other types of linguistic prosody. The larynx also has a similar function to the lungs in creating pressure differences required for sound production; a constricted larynx can be raised or lowered affecting the volume of the oral cavity as necessary in glottalic consonants.
The vocal folds can be held close together (by adducting the arytenoid cartilages) so that they vibrate (see phonation). The muscles attached to the arytenoid cartilages control the degree of opening. Vocal fold length and tension can be controlled by rocking the thyroid cartilage forward and backward on the cricoid cartilage (either directly by contracting the cricothyroids or indirectly by changing the vertical position of the larynx), by manipulating the tension of the muscles within the vocal folds, and by moving the arytenoids forward or backward. This causes the pitch produced during phonation to rise or fall. In most males the vocal folds are longer and with a greater mass than most females' vocal folds, producing a lower pitch.
The vocal apparatus consists of two pairs of mucosal folds. These folds are false vocal folds (vestibular folds) and true vocal folds (folds). The false vocal folds are covered by respiratory epithelium, while the true vocal folds are covered by stratified squamous epithelium. The false vocal folds are not responsible for sound production, but rather for resonance. The exceptions to this are found in Tibetan Chant and Kargyraa, a style of Tuvan throat singing. Both make use of the false vocal folds to create an undertone. These false vocal folds do not contain muscle, while the true vocal folds do have skeletal muscle.
Other.
The most important role of the larynx is its protecting function; the prevention of foreign objects from entering the lungs by coughing and other reflexive actions. A cough is initiated by a deep inhalation through the vocal folds, followed by the elevation of the larynx and the tight adduction (closing) of the vocal folds. The forced expiration that follows, assisted by tissue recoil and the muscles of expiration, blows the vocal folds apart, and the high pressure expels the irritating object out of the throat. Throat clearing is less violent than coughing, but is a similar increased respiratory effort countered by the tightening of the laryngeal musculature. Both coughing and throat clearing are predictable and necessary actions because they clear the respiratory passageway, but both place the vocal folds under great strain and can be catastrophic to a trained voice.
Another important role of the larynx is abdominal fixation, a kind of Valsalva maneuver in which the lungs are filled with air in order to stiffen the thorax so that forces applied for lifting can be translated down to the legs. This is achieved by a deep inhalation followed by the adduction of the vocal folds. Grunting while lifting heavy objects is the result of some air escaping through the adducted vocal folds ready for phonation.
Abduction of the vocal folds is important during physical exertion. The vocal folds are separated by about 8 mm during normal respiration, but this width is doubled during forced respiration.
During swallowing, the backward motion of the tongue forces the epiglottis over the glottis' opening to prevent swallowed material from entering the larynx which leads to the lungs; the larynx is also pulled upwards to assist this process. Stimulation of the larynx by ingested matter produces a strong cough reflex to protect the lungs.
Clinical significance.
Disorders.
There are several things that can cause a larynx to not function properly. Some symptoms are hoarseness, loss of voice, pain in the throat or ears, and breathing difficulties. Larynx transplant is a rare procedure. The world's first successful operation took place in 1998 at the Cleveland Clinic, and the second took place in October 2010 at the University of California Davis Medical Center in Sacramento.
Other animals.
Pioneering work on the structure and evolution of the larynx was carried out in the 1920s by the British comparative anatomist Victor Negus, culminating in his monumental work "The Mechanism of the Larynx" (1929). Negus, however, pointed out that the descent of the larynx reflected the reshaping and descent of the human tongue into the pharynx. This process is not complete until age six to eight years. Some researchers, such as Philip Lieberman, Dennis Klatt, Brant de Boer and Kenneth Stevens using computer-modeling techniques have suggested that the species-specific human tongue allows the vocal tract (the airway above the larynx) to assume the shapes necessary to produce speech sounds that enhance the robustness of human speech. Sounds such as the vowels of the words see and do, [i] and [u], (in phonetic notation) have been shown to be less subject to confusion in classic studies such as the 1950 Peterson and Barney investigation of the possibilities for computerized speech recognition.
In contrast, though other species have low larynges their tongues remains anchored in their mouths and their vocal tracts cannot produce the range of speech sounds of humans. The ability to lower the larynx transiently in some species extends the length of their vocal tract, which as Fitch showed creates the acoustic illusion that they are larger. Research at Haskins Laboratories in the 1960s showed that speech allows humans to achieve a vocal communication rate that exceeds the fusion frequency of the auditory system by fusing sounds together into syllables and words. The additional speech sounds that the human tongue enables us to produce, particularly [i], allow humans to unconsciously infer the length of the vocal tract of the person who is talking, a critical element in recovering the phonemes that make up a word.
Non-mammals.
Most tetrapod species possess a larynx, but its structure is typically simpler than that found in mammals. The cartilages surrounding the larynx are apparently a remnant of the original gill arches in fish, and are a common feature, but not all are always present. For example, the thyroid cartilage is found only in mammals. Similarly, only mammals possess a true epiglottis, although a flap of non-cartilagenous mucosa is found in a similar position in many other groups. In modern amphibians, the laryngeal skeleton is considerably reduced; frogs have only the cricoid and arytenoid cartilages, while salamanders possess only the arytenoids.
Vocal folds are found only in mammals, and a few lizards. As a result, many reptiles and amphibians are essentially voiceless; frogs use ridges in the trachea to modulate sound, while birds have a separate sound-producing organ, the syrinx.
History.
Roman physician Galen first described the larynx, describing it as the "first and supremely most important instrument of the voice"
References.
Sources.
</dl>

</doc>
<doc id="49378" url="http://en.wikipedia.org/wiki?curid=49378" title="924">
924

Year 924 (CMXXIV) was a leap year starting on Thursday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By place.
Asia.
</onlyinclude>

</doc>
<doc id="49379" url="http://en.wikipedia.org/wiki?curid=49379" title="925">
925

Year 925 (CMXXV) was a common year starting on Saturday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By place.
Asia.
</onlyinclude>

</doc>
<doc id="49380" url="http://en.wikipedia.org/wiki?curid=49380" title="929">
929

Year 929 (CMXXIX) was a common year starting on Thursday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="49382" url="http://en.wikipedia.org/wiki?curid=49382" title="928">
928

Year 928 (CMXXVIII) was a leap year starting on Tuesday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="49383" url="http://en.wikipedia.org/wiki?curid=49383" title="576">
576

Year 576 (DLXXVI) was a leap year starting on Wednesday (link will display the full calendar) of the Julian calendar. The denomination 576 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Asia.
</onlyinclude>

</doc>
<doc id="49386" url="http://en.wikipedia.org/wiki?curid=49386" title="927">
927

Year 927 (CMXXVII) was a common year starting on Monday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By place.
Religion.
</onlyinclude>

</doc>
<doc id="49387" url="http://en.wikipedia.org/wiki?curid=49387" title="Deep Blue (chess computer)">
Deep Blue (chess computer)

Deep Blue was a chess-playing computer developed by IBM. It is known for being the first piece of artificial intelligence to win both a chess game and a chess match against a reigning world champion under regular time controls.
Deep Blue won its first game against a world champion on February 10, 1996, when it defeated Garry Kasparov in game one of a six-game match. However, Kasparov won three and drew two of the following five games, defeating Deep Blue by a score of 4–2. Deep Blue was then heavily upgraded, and played Kasparov again in May 1997. Deep Blue won game six, therefore winning the six-game rematch 3½–2½ and becoming the first computer system to defeat a reigning world champion in a match under standard chess tournament time controls. Kasparov accused IBM of cheating and demanded a rematch. IBM refused and retired Deep Blue.
Development for Deep Blue began in 1985 with the ChipTest project at Carnegie Mellon University. This project eventually evolved into Deep Thought, at which point the development team was hired by IBM. The project evolved once more with the new name Deep Blue in 1989. Grandmaster Joel Benjamin was also signed on to the development team by IBM.
Origins.
The project was started as ChipTest at Carnegie Mellon University by Feng-hsiung Hsu, followed by its successor, Deep Thought. After their graduation from Carnegie Mellon, Hsu, Thomas Anantharaman, and Murray Campbell from the Deep Thought team were hired by IBM Research to continue their quest to build a chess machine that could defeat the world champion. Hsu and Campbell joined IBM in autumn 1989, with Anantharaman following later. Anantharaman subsequently left IBM for Wall Street and Arthur Joseph Hoane joined the team to perform programming tasks. Jerry Brody, a long-time employee of IBM Research, was recruited for the team in 1990.
The team was managed first by Randy Moulic, followed by Chung-Jen (C J) Tan.
After Deep Thought's 1989 match against Kasparov, IBM held a contest to rename the chess machine and it became "Deep Blue", a play on IBM's nickname, "Big Blue". After a scaled down version of Deep Blue, Deep Blue Jr., played Grandmaster Joel Benjamin, Hsu and Campbell decided that Benjamin was the expert they were looking for to develop Deep Blue's opening book, and Benjamin was signed by IBM Research to assist with the preparations for Deep Blue's matches against Garry Kasparov.
In 1995 "Deep Blue prototype" (actually Deep Thought II, renamed for PR reasons) played in the 8th World Computer Chess Championship. Deep Blue prototype played the computer program Wchess to a draw while Wchess was running on a personal computer. In round 5 Deep Blue prototype had the white pieces and lost to the computer program Fritz 3 in 39 moves while Fritz was running on an Intel Pentium 90Mhz personal computer. In the end of the championship Deep Blue prototype was tied for second place with the computer program Junior while Junior was running on a personal computer.
Deep Blue versus Kasparov.
Deep Blue and Kasparov played each other on two occasions. The first match began on February 10, 1996, in which Deep Blue became the first machine to win a chess game against a reigning world champion (Garry Kasparov) under regular time controls. However, Kasparov won three and drew two of the following five games, beating Deep Blue by a score of 4–2 (wins count 1 point, draws count ½ point). The match concluded on February 17, 1996.
Deep Blue was then heavily upgraded (unofficially nicknamed "Deeper Blue") and played Kasparov again in May 1997, winning the six-game rematch 3½–2½, ending on May 11. Deep Blue won the deciding game six after Kasparov made a mistake in the opening, becoming the first computer system to defeat a reigning world champion in a match under standard chess tournament time controls.
The system derived its playing strength mainly out of brute force computing power. It was a massively parallel, RS/6000 SP Thin P2SC-based system with 30 nodes, with each node containing a 120 MHz P2SC microprocessor, enhanced with 480 special purpose VLSI chess chips. Its chess playing program was written in C and ran under the AIX operating system. It was capable of evaluating 200 million positions per second, twice as fast as the 1996 version. In June 1997, Deep Blue was the 259th most powerful supercomputer according to the TOP500 list, achieving 11.38 GFLOPS on the High-Performance LINPACK benchmark.
The Deep Blue chess computer that defeated Kasparov in 1997 would typically search to a depth of between six and eight moves to a maximum of twenty or even more moves in some situations. Levy and Newborn estimate that one additional ply (half-move) increases the playing strength 50 to 70 Elo points.
Deep Blue's evaluation function was initially written in a generalized form, with many to-be-determined parameters (e.g. how important is a safe king position compared to a space advantage in the center, etc.). The optimal values for these parameters were then determined by the system itself, by analyzing thousands of master games. The evaluation function had been split into 8,000 parts, many of them designed for special positions. In the opening book there were over 4,000 positions and 700,000 grandmaster games. The endgame database contained many six piece endgames and five or fewer piece positions. Before the second match, the chess knowledge of the program was fine tuned by grandmaster Joel Benjamin. The opening library was provided by grandmasters Miguel Illescas, John Fedorowicz, and Nick de Firmian. When Kasparov requested that he be allowed to study other games that Deep Blue had played so as to better understand his opponent, IBM refused. However, Kasparov did study many popular PC computer games to become familiar with computer game play in general.
Writer Nate Silver suggests that a bug in Deep Blue's software led to a seemingly random move (the 44th in the first game) which Kasparov misattributed to "superior intelligence". Subsequently, Kasparov experienced a drop in performance due to anxiety in the following game.
Aftermath.
After the loss, Kasparov said that he sometimes saw deep intelligence and creativity in the machine's moves, suggesting that during the second game, human chess players had intervened on behalf of the machine, which would be a violation of the rules. IBM denied that it cheated, saying the only human intervention occurred between games. The rules provided for the developers to modify the program between games, an opportunity they said they used to shore up weaknesses in the computer's play that were revealed during the course of the match. Kasparov requested printouts of the machine's log files but IBM refused, although the company later published the logs on the Internet. Kasparov demanded a rematch, but IBM refused and dismantled Deep Blue. Owing to an insufficient sample of games between Deep Blue and officially rated chess players, a chess rating for Deep Blue was not established.
In 2003 a documentary film was made that explored these claims. Entitled "", the film interviews some people who suggest that Deep Blue's victory was a ploy by IBM to boost its stock value.
One of the cultural impacts of Deep Blue was the creation of a new game called Arimaa designed to be much more difficult for computers than chess.
One of the two racks that made up Deep Blue is on display at the National Museum of American History in their exhibit about the Information Age ; the other rack appears at the Computer History Museum in the "Artificial Intelligence and Robotics" gallery of the Revolution exhibit. (Reports that Deep Blue was sold to United Airlines appear to originate from confusion between Deep Blue itself and other RS6000/SP2 systems.)
Feng-hsiung Hsu later claimed in his book "Behind Deep Blue" that he had the rights to use the Deep Blue design to build a bigger machine independently of IBM to take Kasparov's rematch offer, but Kasparov refused a rematch.
Deep Blue, with its capability of evaluating 200 million positions per second, was the fastest computer that ever faced a world chess champion. Today, in computer chess research and matches of world class players against computers, the focus of play has often shifted to software , rather than using dedicated chess hardware. Modern chess programs like Houdini, Rybka, Deep Fritz, or Deep Junior are more efficient than the programs during Deep Blue's era. In a November 2006 match between Deep Fritz and world chess champion Vladimir Kramnik, the program ran on a personal computer containing two Intel Core 2 Duo CPUs, capable of evaluating only 8 million positions per second, but searching to an average depth of 17 to 18 plies in the middlegame thanks to heuristics.
References.
Notes
Bibliography
</dl>
Further reading.
</dl>

</doc>
<doc id="49388" url="http://en.wikipedia.org/wiki?curid=49388" title="932">
932

Year 932 (CMXXXII) was a leap year starting on Sunday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>

</doc>
<doc id="49390" url="http://en.wikipedia.org/wiki?curid=49390" title="Cello (web browser)">
Cello (web browser)

Cello was an early graphical web browser for Windows 3.1, developed by Thomas R. Bruce of the Legal Information Institute at Cornell Law School, and released as shareware in 1993. While other browsers ran on various Unix machines, Cello was the first web browser for Microsoft Windows, using the winsock system to access the Internet. In addition to the basic Windows, Cello worked on Windows NT 3.5 and with small modifications on OS/2.
Cello was created because of a demand for Web access by lawyers, who were more likely to use Microsoft Windows than the Unix operating systems supporting earlier Web browsers, including the first release of Mosaic. The lack of a Windows browser meant many legal experts were unable to access legal information made available in hypertext on the World Wide Web. Cello was popular during 1993/1994, but fell out of favor following the release of Mosaic for Windows and Netscape, after which Cello development was abandoned.
Cello was first publicly released on 8 June 1993. A version 2.0 was announced, but development was abandoned. Version 1.01a, 16 April 1994, was the last public release. Since then, the Legal Information Institute at Cornell Law School has licensed the Cello 2.0 source code, which has been used to develop commercial software.
The browser is no longer available from its original homepage. However, it can still be downloaded from mirror sites.
Development and history.
The development of Cello started in 1992, with beta versions planned for June 1993 and a release for July 1993. It was publicly announced on 12 April 1993.
The Legal Information Institute at Cornell Law School created the first law site on the
Internet in 1992 and the first legal website in 1993. However, at the time, there were no web browsers for the Microsoft Windows operating system, which was used by most lawyers. Thus, to allow lawyers to use their website, the Legal Information Institute developed the first Windows-based Web browser. This was made possible by a grant from the National Center for Automated Information Research.
Although other browsers at the time were based on CERN's WWW libraries called libwww, PCs of the time were not powerful enough to run the UNIX-oriented code. As a result, Thomas Bruce had to rewrite most of the WWW libraries to work on Microsoft Windows. It should also be noted that unlike most commercial browsers at that time, Cello didn't utilize any of Mosaic's source code and thus had a different look and feel.
Steven Sinofsky, president of the Windows division at Microsoft wrote in a June 1994 email: "We do not currently plan on any other client software [in the upcoming release of Windows 95], especially something like Mosaic or Cello." Nonetheless, on 11 January 1995, Microsoft announced that it had licensed the Mosaic technology from Spyglass, which it would use to create Internet Explorer. On 15 August 1995, Microsoft debuted its own web browser Internet Explorer 1 for Windows 95. While it did not ship with the original release of Windows 95, it shipped with Microsoft Plus! for Windows 95.
Usage.
When released in 1993, Cello was the only browser for the Microsoft Windows platform. Shortly after launch, Cello was being downloaded at a rate of 500 copies per day. As such, it achieved a fair amount of use and recognition within the legal community, including a number of PC users with between 150,000 to 200,000 users. In 1994, most websites were visited using either the Cello browser or the Mosaic browser. Despite having fewer features than Mosaic, Cello continued to be used due to its simpler interface and lower system requirements. Cello was praised for being easy to install, because it isn't needed to install Win32s or a TCP/IP stack for Windows 3.1. Following the release of Windows 95, which offered a much better TCP/IP interface, Cello fell into disuse and was abandoned.
By 1995, Cello, like the Mosaic browser, was overshadowed by two newer browsers: Netscape and Internet Explorer and fell into disuse. By 1999, Cello was considered to be a "historical" browser.
Cello is considered to be one of the early casualties of the Browser wars.
Features.
Cello had the following features:
Unlike Mosaic, Cello did not have toolbar buttons, and instead commands were accessed through pull-down menus.
Cello supported the following protocols: HTTP 1.0, Gopher (not Gopher+), read-only FTP, SMTP mailing, Telnet, Usenet, CSO/ph/qi directly and WAIS, HyTelnet, TechInfo, Archie, X.500, TN3270 and a number of others through public gateways.
Cello supported the following FTP servers: most Unix servers(including SunOS, System V, and Linux),IBM VM, IBM VM, VMS systems, Windows NT, QVTNet, NCSA/CUTCP/Rutgers PC servers,FTP Software PC server, HellSoft NLM for Novell.
Cello works best with a direct Ethernet connection, but it also supports SLIP and PPP dialup connections through the use of asynchronous sockets. Cello has an integrated TCP/IP runtime stack.
Release history.
The following versions were released:
Although Cello 2.0 had been announced, development ceased before a public release.
IBM released a fix for their TCP/IP V2.0 stack so that Cello would work with OS/2 WinOS/2 on 9 February 1994.
Browser Comparison Table.
The following table shows how Cello compared to browsers of its time.
Technical.
The user agent for Cello is: codice_1 so the latest one is codice_2
DDE support.
Cello featured DDE support. OLE support and DDE client support were planned, but never released.
An example of how to invoke Cello from a Microsoft Word macro.
System requirements.
Cello has the following system requirements:
Criticism.
Cello was not very stable and its development halted early.
Cello did not render graphics well and required that the user reload the webpage when resizing the window. Like most browsers at the time, Cello also did not support any web security protocols. It was also said that Cello rendered html "crudely" and pages would appear jaggedly.
Cello also had sub-par performance in accessing the Internet and processing hypermedia documents.

</doc>
<doc id="49392" url="http://en.wikipedia.org/wiki?curid=49392" title="Affirmative action">
Affirmative action

Affirmative action or positive discrimination (known as employment equity in Canada, reservation in India and Nepal, and positive action in the UK) is the policy of favoring members of a disadvantaged group who suffer from discrimination within a culture.
The nature of positive discrimination policies varies from region to region. Some countries, such as India, use a quota system, whereby a certain percentage of jobs or school vacancies must be reserved for members of a certain group. In some other regions, specific quotas do not exist; instead, members of minorities are given preference in selection processes.
Origins.
The term "affirmative action" was first used in the United States in Executive Order 10925 and was signed by President John F. Kennedy on 6 March 1961. It was used to promote actions that achieve non-discrimination. In 1965, President Lyndon B. Johnson issued Executive Order 11246 which required government employers to take "affirmative action" to "hire without regard to race, religion and national origin". This prevented employers from discriminating against members of disadvantaged groups.
In 1967, gender was added to the anti-discrimination list.
Affirmative action is intended to promote the opportunities of defined minority groups within a society to give them equal access to that of the majority population.
It is often instituted for government and educational settings to ensure that certain designated "minority groups" within a society are able to participate in all provided opportunities including promotional, educational, and training opportunities.
The stated justification for affirmative action by its proponents is that it helps to compensate for past discrimination, persecution or exploitation by the ruling class of a culture, and to address existing discrimination.
Women.
Several different studies investigated the effect of affirmative action on women. Kurtulus (2012) in her review of affirmative action and the occupational advancement of minorities and women during 1973-2003 showed that the effect of affirmative action on advancing black, Hispanic, and white women into management, professional, and technical occupations occurred primarily during the 1970s and early 1980s. During this period, contractors grew their shares of these groups more rapidly than noncontractors because of the implementation of affirmative action. But the positive effect of affirmative action vanished entirely in the late 1980s, which Kurtulus says may be due to the slowdown into advanced occupation for women and minorities because of the political shift of affirmative action that started by President Reagan. Becoming a federal contractor increased white women's share of professional occupations by 0.183 percentage points, or 7.3 percent, on average during these three decades, and increased black women's share by 0.052 percentage points (or by 3.9 percent). Becoming a federal contractor also increased Hispanic women's and black men's share of technical occupations on average by 0.058 percent and 0.109 percentage points respectively (or by 7.7 and 4.2 percent). These represent a substantial contribution of affirmative action to overall trends in the occupational advancement of women and minorities over the three decades under the study.
Quotas.
Law regarding quotas and affirmative action varies widely from nation to nation. Caste based quotas are used in Reservation in India. However, they are illegal in the United States, where no employer, university, or other entity may create a set number required for each race.
In 2012, the European Union Commission approved a plan for women to constitute 40% of non-executive board directorships in large listed companies in Europe by the year 2020. In Sweden, the Supreme Court has ruled that "affirmative action" ethnic quotas in universities are discrimination and hence unlawful. It said that the requirements for the intake should be the same for all. The Justice Chancellor said that the decision left no room for uncertainty.
Unzueta et al. in their study of how believing in affirmative action quotas affects White women's self-image showed that white women who do not think affirmative action benefits personally may derive a self-image benefit from believing that affirmative action entails quotas. Such belief may enable White women to believe that their past achievements were attained despite the influence of discriminatory quota policies while their past failures occurred because of such policies. Study 2: Those who were led to believe that affirmative action entails quotas reported higher state self-esteem relative to those who were informed that affirmation action does not entail this controversial procedure. White women's self-image can benefit from affirmative action quota beliefs so long as they do not think of themselves as beneficiaries of such a policy. But those who are beneficiaries may become motivated to protect their self-image when presented with descriptions of affirmative action that threaten to discount their competence.
National approaches.
In some countries that have laws on racial equality, affirmative action is rendered illegal because it does not treat all races equally. This approach of equal treatment is sometimes described as being "color blind", in hopes that it is effective against discrimination without engaging in reverse discrimination.
In such countries, the focus tends to be on ensuring equal opportunity and, for example, targeted advertising campaigns to encourage ethnic minority candidates to join the police force. This is sometimes described as "positive action."
Africa.
South Africa.
Apartheid.
The Apartheid government, as a matter of state policy, favoured white-owned, especially Afrikaner-owned companies. The aforementioned policies achieved the desired results, but in the process they marginalised and excluded black people. Skilled jobs were also reserved for white people, and blacks were largely used as unskilled labour, enforced by legislation including the Mines and Works Act, the Job Reservations Act, the Native Building Workers Act, the Apprenticeship Act and the Bantu Education Act, creating and extending the "colour bar" in South African labour. For example, in early 20th century South Africa mine owners preferred hiring black workers because they were cheaper. Then the whites successfully persuaded the government to enact laws that highly restricted the blacks' employment opportunities.
Since the 1960s the Apartheid laws had been weakened. Consequently, from 1975 to 1990 the real wages of black manufacturing workers rose by 50 %, that of whites by 1 %.
The economic and politically structured society during the apartheid ultimately caused disparities in employment, occupation and income within labour markets, which provided advantages to certain groups and characteristics of people. This in due course was the motivation to introduce affirmative action in South Africa, following the end of Apartheid.
Post-apartheid Employment Equity.
Following the transition to democracy in 1994, the African National Congress-led government chose to implement affirmative action legislation to correct previous imbalances (a policy known as Employment Equity). As such, all employers were compelled by law to employ previously disenfranchised groups (blacks, Indians, and Coloureds). A related, but distinct concept is Black Economic Empowerment.
The Employment Equity Act and the Broad Based Black Economic Empowerment Act aim to promote and achieve equality in the workplace (in South Africa termed "equity"), by advancing people from designated groups. The designated groups who are to be advanced include all people of colour, women (including white women) and people with disabilities (including whites). Employment Equity legislation requires companies employing more than 50 people to design and implement plans to improve the representativity of workforce demographics, and report them to the Department of Labour.
Employment Equity also forms part of a company's Black Economic Empowerment scorecard: in a relatively complex scoring system, which allows for some flexibility in the manner in which each company meets its legal commitments, each company is required to meet minimum requirements in terms of representation by previously disadvantaged groups. The matters covered include equity ownership, representation at employee and management level (up to board of director level), procurement from black-owned businesses and social investment programs, amongst others.
The policies of Employment Equity and, particularly, Black Economic empowerment have been criticised both by those who view them as discriminatory against white people, and by those who view them as ineffectual.
These laws cause disproportionally high costs for small companies and reduce economic growth and employment. The laws may give the black middle-class some advantage but can make the worse-off blacks even poorer. Moreover, the Supreme Court has ruled that in principle blacks may be favored, but in practice this should not lead to unfair discrimination against the others. Yet it is impossible to favor somebody without discriminating against others.
Affirmative Action Purpose.
As mentioned previously affirmative action was introduced through the Employment Equality Act, 55 in 1998, 4 years after the end of Apartheid. This act was passed to promote the constitutional right of equality and exercise true democracy. This idea was to eliminate unfair discrimination in employment, to ensure the implementation of employment equity to redress the effects of discrimination, to achieve a diverse workforce broadly representative of our people, to promote economic development and efficiency in the workforce and to give effects to the obligations of the Republic as a member of the International Labour Organisation.
Many embraced the Act; however some concluded that the act contradicted itself. The act eliminates unfair discrimination in certain sectors of the national labour market by imposing similar constraints on another.
With the introduction of Affirmative Action, Black Economic Empowerment (BEE) rose additionally in South Africa. The BEE was not a moral initiative to redress the wrongs of the past but to promote growth and strategies that aim to realize a country's full potential. The idea was targeting the weakest link in economics, which was inequality and which would help develop the economy. This is evident in the statement by the Department of Trade and Industry, "As such, this strategy stresses a BEE process that is associated with growth, development and enterprise development, and not merely the redistribution of existing wealth". Similarities between the BEE and affirmative action are apparent; however there is a difference. BEE focuses more on employment equality rather than taking wealth away from the skilled white labourers.
The main goal of Affirmative Action is for a country to reach its full potential. This occurrence would result in a completely diverse workforce in economic and social sectors. Thus broadening the economic base and therefore stimulating economic growth.
Outcomes.
Once applied within the country, many different outcomes arose, some positive and some negative. This depended on the approach and the view of The Employment Equality Act and Affirmative Action.
Positive:
Pre Democracy, the Apartheid discriminated against non-white races, so with affirmative action, the country started to redress past discriminations. Affirmative Action also focused on combating structural racism and racial inequality, hoping to maximize diversity in all levels of society and sectors. Achieving this would elevate the status of the perpetual underclass and to restore equal access to the benefits of society.
Negative:
Though Affirmative Action had its positives, negatives arose. A quota system was implemented, which aimed to achieve targets of diversity in a work force. This target affected the hiring and level of skill in the work force, ultimately affecting the free market. Affirmative action created marginalization for coloured and Indian races in South Africa, as well as developing and aiding the middle and elite classes, leaving the lower class behind. This created a bigger gap between the lower and middle class, which led to class struggles and a greater segregation. Entitlement began to arise with the growth of the middle and elite classes, as well as race entitlement. Many believe that affirmative action is discrimination in reverse. With all these negatives, much talent started to leave the country. Many negative consequences of affirmative action, specifically the quota system, drive skilled labour away, resulting in bad economic growth. This is due to very few international companies wanting to invest in South Africa.
With these negative and positive outcomes of Affirmative Action it is evident that the concept of affirmative action is a continuous and learning idea.
Asia.
China.
There is affirmative action in education for minority nationalities. This may equate to lowering minimum requirements for the National University Entrance Examination, which is a mandatory exam for all students to enter university. Some universities would set ratios between ethnic minorities and Han Chinese (the majority group in China at 92%) applicants for their student intake. Further, minority students enrolled in ethnic minority-oriented specialties are provided with scholarships and/or pay no tuition, and are granted a monthly stipend.
In the labour market, "preferential policies" require some of the top positions in governments be distributed to ethnic minorities and women.
Israel.
A class-based affirmative action policy was incorporated into the admission practices of the four most selective universities in Israel during the early to mid-2000s. In evaluating the eligibility of applicants, neither their financial status nor their national or ethnic origins are considered. The emphasis, rather, is on structural disadvantages, especially neighborhood socioeconomic status and high school rigor, although several individual hardships are also weighed. This policy made the four institutions, especially the echelons at the most selective departments, more diverse than they otherwise would have been. The rise in geographic, economic and demographic diversity of a student population suggests that the plan's focus on structural determinants of disadvantage yields broad diversity dividends.
Israeli citizens who are; Women, Arabs, Blacks or people with disabilities are entitled to Affirmative Action in the civil service employment. Also Israeli citizens who are Arabs, Blacks or people with disabilities are entitled for Affirmative Actions are entitled for full University tuition scholarships by the state.
Izraeli in her study of gender Politics in Israel showed that the paradox of affirmative action for women directors is that the legitimation for legislating their inclusion on boards also resulted in the exclusion of women's interested as a legitimate issue on the boards' agendas. "The new culture of the men's club is seductive token women are under the pressure to become "social males" and prove that their competence as directors, meaning that they are not significantly different from men. In the negotiation for status as worthy peers, emphasizing gender signals that a woman is an "imposter," someone who does not rightfully belong in the position she is claiming to fill." And once affirmative action for women is fulfilled, and then affirmative action shares the element, as Izareli put it, the "group equality discourse," making it easier for other groups to claim for a fairer distribution of resources. This suggests that affirmative action can have applications for different groups in Israel.
India.
Reservation in India is a form of affirmative action designed to improve the well-being of backward and under-represented communities defined primarily by their caste.
Sri Lanka.
In 1971 the Standardization policy of Sri Lankan universities was introduced as an affirmative action program for students from areas which had lower rates of education than other areas due to missionary activity in the north and east, which essentially were the Tamil areas. Successive governments cultivated a historical myth after the colonial powers had left that the British had practised communal favouritism towards Christians and the minority Tamil community for the entire 200 years they had controlled Sri Lanka. However, the Sinhalese in fact benefitted from trade and plantation cultivations over the rest of the other groups and their language and culture as well as the religion of Buddhism was fostered and made into mediums for schools over the Tamil language, which did not have the same treatment and Tamils learned English instead as there was no medium for Tamil until near independence. Tamils' knowledge of English and education came from the very American missionary activity by overseas Christians that the British were concerned will anger the Sinhalese and destroy their trading relationships, so they sent them to the Tamil areas instead to teach, thinking it would have no consequences and due to their small numbers. The British sending the missionaries to the north and east was for the protection of the Sinhalese and in fact showed favouritism to the majority group instead of the minorities to maintain trading relationships and benefits from them. The Tamils, out of this random benefit from learning English and basic education excelled and flourished and were able to take many civil service jobs to the chagrin of the Sinhalese. The myth of Divide and Rule is untrue. The 'policy of standardisation' was typical of affirmative action policies, in that it required drastically lower standards for Sinhalese students than for the more academic Tamils who had to get about ten more marks to enter into universities. The policy, were it not implemented would have prevented the civil wars ahead as the policies had no basis and in fact is an example of discrimination against the Tamil ethnic group.
Malaysia.
The Malaysian New Economic Policy or NEP serves as a form of affirmative action. Malaysia provides affirmative action to the majority because in general, the Malays have lower income than the Chinese who have traditionally been involved in businesses and industries. Malaysia is a multi-ethnic country, with Malays making up the majority of close to 52% of the population. About 23% of the population are Malaysians of Chinese descent, while Malaysians of Indian descent comprise about 7% of the population. During more than 100 years of British colonization, the Malays were discriminated against employment because the British preferred to bring in influx of migrant workers from China and India.
("See also Bumiputra") The mean income for Malays, Chinese and Indians in 1957/58 were 134, 288 and 228 respectively. In 1967/68 it was 154, 329 and 245, and in 1970 it was 170, 390 and 300. Mean income disparity ratio for Chinese/Malays rose from 2.1 in 1957/58 to 2.3 in 1970, whereas for Indians/Malays the disparity ratio also rose from 1.7 to 1.8 in the same period. The Malays viewed Independence as restoring their proper place in their own country's socioeconomic order while the non-Malays were opposing government efforts to advance Malay political primacy and economic welfare.
Europe.
Finland.
In certain university education programs, including legal and medical education, there are quotas for persons who reach a certain standard of skills in the Swedish language; for students admitted in these quotas, the education is partially arranged in Swedish. The purpose of the quotas is to guarantee that a sufficient number of professionals with skills in Swedish are educated for nation-wide needs. The quota system has met with criticism from the Finnish speaking majority, some of whom consider the system unfair. In addition to these linguistic quotas, women may get preferential treatment in recruitment for certain public sector jobs if there is a gender imbalance in the field.
France.
No distinctions based on race, religion or sex are allowed under the 1958 French Constitution. Since the 1980s, a French version of affirmative action based on neighborhood is in place for primary and secondary education. Some schools, in neighborhoods labeled "Priority Education Zones", are granted more funds than the others. Students from these schools also benefit from special policies in certain institutions (such as Sciences Po).
The French Ministry of Defence tried in 1990 to give more easily higher ranks and driving licenses to young French soldiers with North-African ancestry. After a strong protest by a young French lieutenant in the Ministry of Defence newspaper ("Armées d'aujourd'hui"), this driving license and rank project was cancelled. After the Sarkozy election, a new attempt in favour of Arabian-French students was made but Sarkozy did not gain enough political support to change the French constitution. However, highly ranked French schools do implement affirmative action in that they are obligated to take a certain number of students from impoverished families. <br> Additionally, following the Norwegian example, after 27 January 2014, women must represent at least 20% of board members in all stock exchange listed or state owned companies. After 27 January 2017, the proportion will increase to 40%. All male director nominations will be invalid as long as the condition is not met, and financial penalties may apply for other directors.
Germany.
Article 3 of the German Basic Law provides for equal rights of all people regardless of sex, race or social background. There are programs stating that if men and women have equal qualifications, women have to be preferred for a job; moreover, the handicapped should be preferred to healthy people. This is typical for all positions in state and university service as of 2007[ [update]], typically using the phrase "We try to increase diversity in this line of work". In recent years, there has been a long public debate about whether to issue programs that would grant women a privileged access to jobs in order to fight discrimination. Germany's "Left Party" brought up the discussion about affirmative action in Germany's school system. According to Stefan Zillich, quotas should be "a possibility" to help working class children who did not do well in school gain access to a "Gymnasium" (University-preparatory school). Headmasters of "Gymnasien" have objected, saying that this type of policy would "be a disservice" to poor children.
In 2009, the Berlin Senate decided that Berlin's Gymnasium should no longer be allowed to handpick all of their students. It was ruled that while Gymnasien should be able to pick 70% to 65% of their students, the other places at the Gymnasien are to be allocated by lottery. Every child will be able to enter the lottery, no matter how he or she performed in primary school. It is hoped that this policy will increase the number of working class students attending a Gymnasium.
The Left proposed that Berlin Gymnasien should no longer be allowed to expel students who perform poorly so that the students who won a Gymnasium place in the lottery have a fair chance of graduating from that school. It is not clear yet if Berlin's senate will decide in favour of The Lefts proposal. There is also a discussion going on if affirmative action should be employed to help the children and grandchildren of the so-called "Gastarbeiter" gain better access to German universities. One prominent proponent of this was Ralf Dahrendorf. It is argued that the Gastarbeiter willingly came to Germany to help build the industry and this should be honored.
Norway.
In all public stock companies (ASA) boards, either gender should be represented by at least 40%. This affects roughly 400 companies of over 300,000 in total.
Seierstad & Opsahl in their study of the effects of affirmative action on presence, prominence, and social capital of women directors in Norway found that there are few boards chaired by a woman, from the beginning of the implementation of affirmative action policy period to August 2009, the proportion of boards led by a woman has increased from 3.4% to 4.3%. This suggests that the law has had a marginal effect on the sex of the chair and the boards remain internally segregated. Although at the beginning of our observation period, only 7 of 91 prominent directors were women. The gender balance among prominent directors has changed considerable through the period, and at the end of the period, 107 women and 117 men were prominent directors. Interestingly, by applying more restrictive definitions of prominence, the proportion of directors who are women generally increases. If only considering directors with at least three directorships, 61.4% of them are women. When considering directors with seven or more directorships, all of them are women. Thus, affirmative action increase the female population in the director position.
Romania.
Romani people are allocated quotas for access to public schools and state universities. There is evidence that some ethnic Romanians exploit the system so they can be themselves admitted to universities, which has drawn criticism from Roma representatives.
Russia and the former Soviet Union.
Quota systems existed in the USSR for various social groups including ethnic minorities, women and factory workers. Quotas for access to university education, offices in the Soviet system and the Communist Party existed: for example, the position of First Secretary of a Soviet Republic's (or Autonomous Republic's) Party Committee was always filled by a representative of this republic's "titular" ethnicity.
Modern Russia retains this system partially. Some quotas (such as those for factory workers) are abolished, however, the quotas for women and ethnic minorities remain.
Slovakia.
The Constitutional Court declared in October 2005 that affirmative action i.e. "providing advantages for people of an ethnic or racial minority group" as being against its Constitution.
United Kingdom.
In the UK, any discrimination, quotas or favouritism due to sex, race and ethnicity among other "protected characteristics" is generally illegal for any reason in education, employment, during commercial transactions, in a private club or association, and while using public services. The Equality Act 2010 established the principles of equality and their implementation in the UK.
Specific exemptions include:
North America.
Canada.
The equality section of the Canadian Charter of Rights and Freedoms explicitly permits affirmative action type legislation, although the Charter does not "require" legislation that gives preferential treatment. Subsection 2 of Section 15 states that the equality provisions do "not preclude any law, program or activity that has as its object the amelioration of conditions of disadvantaged individuals or groups including those that are disadvantaged because of race, national or ethnic origin, colour, religion, sex, age or mental or physical disability."
The Canadian Employment Equity Act requires employers in federally-regulated industries to give preferential treatment to four designated groups: Women, people with disabilities, aboriginal people, and visible minorities. In most Canadian Universities, people of Aboriginal background normally have lower entrance requirements and are eligible to receive exclusive scholarships. Some provinces and territories also have affirmative action-type policies. For example, in Northwest Territories in the Canadian north, aboriginal people are given preference for jobs and education and are considered to have P1 status. Non-aboriginal people who were born in the NWT or have resided half of their life there are considered a P2, as well as women and people with disabilities.
United States.
The concept of affirmative action was introduced in the early 1960s in the United States, as a way to combat racial discrimination in the hiring process and, in 1967, the concept was expanded to include sex. Affirmative action was first created from Executive Order 10925, which was signed by President John F. Kennedy on 6 March 1961 and required that government employers "not discriminate against any employee or applicant for employment because of race, creed, color, or national origin" and "take affirmative action to ensure that applicants are employed, and that employees are treated during employment, without regard to their race, creed, color, or national origin".
On 24 September 1965, President Lyndon B. Johnson signed Executive Order 11246, thereby replacing Executive Order 10925 and affirming Federal Government's commitment "to promote the full realization of equal employment opportunity through a positive, continuing program in each executive department and agency". Affirmative action was extended to women by Executive Order 11375 which amended Executive Order 11246 on 13 October 1967, by adding "sex" to the list of protected categories. In the U.S. affirmative action's original purpose was to pressure institutions into compliance with the nondiscrimination mandate of the Civil Rights Act of 1964. The Civil Rights Acts do not cover veterans, people with disabilities, or people over 40. These groups are protected from discrimination under different laws.
Affirmative action has been the subject of numerous court cases, and has been questioned upon its constitutional legitimacy. In 2003, a Supreme Court decision regarding affirmative action in higher education ("Grutter v. Bollinger", 539 US 244 – Supreme Court 2003) permitted educational institutions to consider race as a factor when admitting students. Alternatively, some colleges use financial criteria to attract racial groups that have typically been under-represented and typically have lower living conditions. Some states such as California (California Civil Rights Initiative), Michigan (Michigan Civil Rights Initiative), and Washington (Initiative 200) have passed constitutional amendments banning public institutions, including public schools, from practicing affirmative action within their respective states. Conservative activists have alleged that colleges quietly use illegal quotas and have launched numerous lawsuits to stop them.
Oceania.
New Zealand.
Individuals of Māori or other Polynesian descent are often afforded improved access to university courses, or have scholarships earmarked specifically for them. Affirmative action is provided for under section 73 of the Human Rights Act 1993 and section 19(2) of the New Zealand Bill of Rights Act 1990.
South America.
Brazil.
Some Brazilian Universities (State and Federal) have created systems of preferred admissions (quotas) for racial minorities (blacks and native Brazilians), the poor and people with disabilities. There are also quotas of up to 20% of vacancies reserved for people with disabilities in the civil public services. The Democrats party, accusing the board of directors of the University of Brasília of "Nazism", appealed to the Supreme Federal Court the constitutionality of the quotas the University reserves for minorities. The Supreme Court unanimously approved their constitutionality on 26 April 2012.
International organizations.
United Nations.
The International Convention on the Elimination of All Forms of Racial Discrimination stipulates (in Article 2.2) that affirmative action programs may be required of countries that ratified the convention, in order to rectify systematic discrimination. It states, however, that such programs "shall in no case entail as a consequence the maintenance of unequal or separate rights for different racial groups after the objectives for which they were taken have been achieved."
The United Nations Human Rights Committee states that "the principle of equality sometimes requires States parties to take affirmative action in order to diminish or eliminate conditions which cause or help to perpetuate discrimination prohibited by the Covenant. For example, in a State where the general conditions of a certain part of the population prevent or impair their enjoyment of human rights, the State should take specific action to correct those conditions. Such action may involve granting for a time to the part of the population concerned certain preferential treatment in specific matters as compared with the rest of the population. However, as long as such action is needed to correct discrimination, in fact, it is a case of legitimate differentiation under the Covenant."
Support.
The principle of affirmative action is to promote societal equality through the preferential treatment of socioeconomically disadvantaged people. Often, these people are disadvantaged for historical reasons, such as oppression or slavery.
Historically and internationally, support for affirmative action has sought to achieve a range of goals: bridging inequalities in employment and pay; increasing access to education; enriching state, institutional, and professional leadership with the full spectrum of society; redressing apparent past wrongs, harms, or hindrances, in particular addressing the apparent social imbalance left in the wake of slavery and slave laws.
Polls.
According to a poll taken by "USA Today" in 2005, majority of Americans support affirmative action for women, while views on minority groups were more split. Men are only slightly more likely to support affirmative action for women; though a majority of both do. However, a slight majority of Americans do believe that affirmative action goes beyond ensuring access and goes into the realm of preferential treatment. More recently, a Quinnipiac poll from June 2009 finds that 55% of Americans feel that affirmative action in general should be discontinued, though 55% support it for people with disabilities. A Gallup poll from 2005 showed that 72% of black Americans and 44% of white Americans supported racial affirmative action (with 21% and 49% opposing), with support and opposition among Hispanics falling between those of blacks and whites. Support among blacks, unlike among whites, had almost no correlation with political affiliation.
A 2009 Quinnipiac University Polling Institute survey found 65% of American voters opposed the application of affirmative action to gay people, with 27% indicating they supported it.
A Leger poll taken in 2010 finds 59% of Canadians oppose considering race, gender, or ethnicity when hiring for government jobs.
Criticism.
Opponents of affirmative action such as George Sher believe that affirmative action devalues the accomplishments of people who are chosen based on the social group to which they belong rather than their qualifications, thus rendering affirmative action counterproductive. Opponents, who sometimes say that affirmative action is "reverse discrimination", further claim that affirmative action has undesirable side-effects in addition to failing to achieve its goals. They argue that it hinders reconciliation, replaces old wrongs with new wrongs, undermines the achievements of minorities, and encourages individuals to identify themselves as disadvantaged, even if they are not. It may increase racial tension and benefit the more privileged people within minority groups at the expense of the least fortunate within majority groups (such as lower-class white people). They claim that cases such as "Fisher v. University of Texas" are few of the many examples that show how reverse discrimination can take place. In 2008, Abigail Fisher, who is a native to Texas, sued the University of Texas at Austin, claiming that she was denied admission to the university because she was "white". The students that are of top 10% in the applicants of the University of Texas are admitted and there are students that compete to barely make it in on the threshold, such as Abigail Fisher. In such cases, race becomes an important factor in deciding who gets admitted to the university, and Fisher argued that discriminating and accepting students according to their race is a violation of the Equal Protection Clause of the Fourteenth Amendment, which ensures equal protection of the law and the citizen's privilege as a citizen of United States. The constitutionality of affirmative action in college admissions is now before the Supreme Court in the 2013 landmark case "Fisher v. University of Texas".
American economist, social and political commentator, Dr. Thomas Sowell identified some negative results of race-based affirmative action in his book, "Affirmative Action Around the World: An Empirical Study". Sowell writes that affirmative action policies encourage non-preferred groups to designate themselves as members of preferred groups (i.e., primary beneficiaries of affirmative action) to take advantage of group preference policies; that they tend to benefit primarily the most fortunate among the preferred group (e.g., upper and middle class blacks), often to the detriment of the least fortunate among the non-preferred groups (e.g., poor whites or Asians); that they reduce the incentives of both the preferred and non-preferred to perform at their best – the former because doing so is unnecessary and the latter because it can prove futile – thereby resulting in net losses for society as a whole; and that they increase animosity toward preferred groups.
Mismatching.
Mismatching is the term given to the negative effect that affirmative action has when it places a student into a college that is too difficult for him or her. For example, according to the theory, in the absence of affirmative action, a student will be admitted to a college that matches his or her academic ability and have a good chance of graduating. However, according to the mismatching theory, affirmative action often places a student into a college that is too difficult, and this increases the student's chance of dropping out. Thus, according to the theory, affirmative action hurts its intended beneficiaries, because it increases their dropout rate.
Evidence in support of the mismatching theory was presented by Gail Heriot, a professor of law at the University of San Diego and a member of the U.S. Commission on Civil Rights, in an 24 August 2007 article published in the "Wall Street Journal". The article reported on a 2004 study that was conducted by UCLA law professor Richard Sander and published in the "Stanford Law Review". The study concluded that there were 7.9% fewer black attorneys than there would have been if there had been no affirmative action. The study was titled, "A Systemic Analysis of Affirmative Action in American Law Schools." The article also states that because of mismatching, blacks are more likely to drop out of law school and fail bar exams.
Sander's paper on mismatching has been criticized by several law professors, including Ian Ayres and Richard Brooks from Yale who argue that eliminating affirmative action would actually reduce the number of black lawyers by 12.7%.

</doc>
<doc id="49393" url="http://en.wikipedia.org/wiki?curid=49393" title="Office of National Assessments">
Office of National Assessments

The Office of National Assessments (ONA) is an Australian intelligence agency. ONA was established by the as an independent body directly accountable to the Prime Minister of Australia. ONA provides all-source assessments on international political, strategic and economic developments to the Prime Minister and senior ministers in the National Security Committee of Cabinet. It also coordinates and evaluates the work and performance of Australia’s foreign intelligence agencies. ONA is not an intelligence collection agency.
ONA is in the Prime Minister and Cabinet portfolio of the Australian Public Service. Its financial framework is governed by the Financial Management and Accountability Act 1997 (FMA Act).
Locations.
In October 2011, ONA moved into the Robert Marsden Hope Building, a refurbished building in the Parliamentary Triangle. The building is named for Justice Hope, who led two Royal Commissions into Australia's intelligence and security agencies and operations, the first of which led to the creation of ONA. Before its move, ONA had been a sub-tenant in the Central Office building of the Australian Security Intelligence Organisation (ASIO) in Russell, Canberra.
The Director-General of ONA is an independent statutory officer who is not subject to external direction on the content of ONA assessments. ONA has about 150 staff, including 100 analysts. The current Director-General of ONA is Allan Gyngell, a distinguished Australian diplomat, former senior advisor to former Prime Minister Paul Keating, and founder of the Lowy Institute.
Role.
The Office of National Assessments is not a producer of intelligence; it collates intelligence data generated by DIO, ASIS, ASIO, DIGO and DSD to create analytical products. The Department of Prime Minister and Cabinet is the primary consumer of these products, which are designed to assist the Australian Government in strategic decision making and ensure that government is fully briefed on emergent threats both in the region and globally. Its independence from the Defence and Foreign Affairs portfolios is a deliberate attempt to generate balanced and fair analysis.
In the media.
Although not a secret organisation, ONA usually attracts little attention. However, a striking exception occurred in 2001 when the former Prime Minister, John Howard, publicly relied upon an ONA assessment to support his claims about asylum seekers on the MV "Tampa", in an incident which became known as the "Tampa affair". The ONA assessment was later leaked to the public in its entirety, showing that the assessment was ultimately based on nothing more than press releases from various government ministers.
In 2003, in the lead-up to the 2003 invasion of Iraq, an ONA intelligence officer named Andrew Wilkie resigned from the agency, citing ethical concerns in relation to selective and exaggerated use of intelligence by the Australian Government on the matter of Iraq and weapons of mass destruction.
Flood report.
ONA has experienced substantial growth since the release of the report into intelligence agencies by Philip Flood which recommended a doubling of the agency's budget and staffing resources and formalisation of the agency's role as a coordinator and evaluator of the other Australian foreign intelligence agencies. The only ONA specific recommendation not implemented from the Flood report was the renaming of ONA to the Australian Foreign Intelligence Assessment Agency (AFIAA).
Branches.
The ONA is divided into branches: Atlantic Branch; Corporate and I.T. Services; Executive & Foreign Intelligence Coordination; International Economy Branch; South Asia & Middle East Branch; North Asia Branch; Oceania Branch; Open Source Centre; Southeast Asia Branch; Strategic Analysis Branch; Transnational Issues Branch.

</doc>
<doc id="49396" url="http://en.wikipedia.org/wiki?curid=49396" title="Volunteer (botany)">
Volunteer (botany)

In gardening and agronomic terminology, a volunteer is a plant that grows on its own, rather than being deliberately planted by a farmer or gardener. Volunteers often grow from seeds that float in on the wind, are dropped by birds, or are inadvertently mixed into compost. Unlike weeds, which are unwanted plants, a volunteer may be encouraged by gardeners once it appears, being watered, fertilized, or otherwise cared for. The action of such plants – to sprout or grow in this fashion – may also be described as volunteering.
Volunteers that grow from the seeds of specific cultivars are not reliably identical or similar to their parent, and often differ significantly from it. Such open pollinated plants, if they show desirable characteristics, may be selected to become new cultivars.
Agriculture.
In agricultural rotations, self-set plants from the previous year's crop may become established as weeds in the current crop. For example, volunteer winter wheat will germinate to quite high levels in a following oilseed rape crop, usually requiring chemical control measures. In agricultural research, high purity of a harvested crop is usually desirable. To achieve this, typically a group of temporary workers will walk the crop rows looking for volunteer plants, or "rogue" plants in an exercise often referred to as "roguing."

</doc>
<doc id="49397" url="http://en.wikipedia.org/wiki?curid=49397" title="Battle of Chosin Reservoir">
Battle of Chosin Reservoir

The Battle of Chosin Reservoir, also known as the Chosin Reservoir Campaign or the Changjin Lake Campaign (Korean: 장진호 전투; ), was a decisive battle in the Korean War. "Chosin" is the Japanese pronunciation of the Korean name, "Changjin". Reportedly, updated maps in Korean were unavailable, hence the use of maps reflecting the Japanese pronunciation (Korea had been liberated from Japanese colonial rule only five years prior, in 1945). Shortly after the People's Republic of China entered the conflict, the People's Volunteer Army 9th Army infiltrated the northeastern part of North Korea and surprised the US X Corps at the Chosin Reservoir area. A brutal 17 day battle in freezing weather soon followed. In the period between 27 November and 13 December 1950, 30,000 United Nations (UN) troops (nicknamed "The Chosin Few," a play on words) under the command of Major General Edward Almond were encircled by approximately 67,000 Chinese troops under the command of Song Shi-Lun. Although Chinese troops managed to surround and outnumber the UN forces, they were able to break out of the encirclement while inflicting crippling losses on the Chinese, allowing them to successfully withdraw in good order. The evacuation of the X Corps from the port of Hungnam marked the complete withdrawal of UN troops from North Korea.
Background.
By mid-1950 after the successful landing at Inchon by the US X Corps and the subsequent destruction of the Korean People's Army, the Korean War appeared to be all but over. United Nations (UN) forces advanced rapidly into North Korea with the intention of reuniting North and South Korea before the end of 1950. North Korea is divided through the center by the impassable Taebaek Mountains, which separated the UN forces into two groups. The US Eighth Army advanced north through the western coast of the Korean Peninsula, while the Republic of Korea (ROK) I Corps and the US X Corps advanced north on the eastern coast.
At the same time the People's Republic of China entered the conflict after issuing several warnings to the United Nations. On 19 October 1950, large formations of Chinese troops, dubbed the People's Volunteer Army (PVA), secretly crossed the border and into North Korea. One of the first Chinese units to reach the Chosin Reservoir area was the PVA 42nd Corps, and it was tasked with stopping the eastern UN advances. On 25 October, the advancing ROK I Corps made contact with the Chinese and halted at Funchilin Pass, south of the Chosin Reservoir. After the landing at Wonsan, the US 1st Marine Division of the X Corps engaged the defending PVA 124th Division on 2 November, and the ensuing battle caused heavy casualties among the Chinese. On 6 November, the PVA 42nd Corps ordered a retreat to the north with the intention of luring the UN forces into the Chosin Reservoir. By 24 November, the 1st Marine Division occupied both Sinhung-ni on the eastern side of the reservoir, and Yudami-ni on the west side of the reservoir.
Faced with the sudden attacks by Chinese forces in the Eighth Army sector, General Douglas MacArthur ordered the Eighth Army to launch the Home-by-Christmas Offensive. To support the offensive, MacArthur ordered the X Corps to attack west from the Chosin Reservoir and to cut the vital Manpojin—Kanggye—Huichon supply line. As a response, Major General Edward M. Almond, commander of the US X Corps, formulated a plan on 21 November. It called for the US 1st Marine Division to advance west through Yudami-ni, while the US 7th Infantry Division would provide a regimental combat team to protect the right flank at Sinhung-ni. The US 3rd Infantry Division would also protect the left flank while providing security in the rear area. By then the X Corps was stretched thin along a 400 mi front.
Surprised by the Marine landing at Wonsan, China's Chairman Mao Zedong called for the immediate destruction of the ROK Capital Division, ROK 3rd Infantry Division, US 1st Marine Division, and US 7th Infantry Division in a telegraph to Commander Song Shi-Lun of the PVA 9th Army on 31 October. Under Mao's urgent orders, the 9th Army was rushed into North Korea on 10 November. Undetected by UN intelligence, the 9th Army quietly entered the Chosin Reservoir area on 17 November, with the 20th Corps of the 9th Army relieving the 42nd Corps near Yudami-ni.
Prelude.
Location, terrain and weather.
Chosin Reservoir is a man-made lake located in the northeast of the Korean peninsula. The name Chosin is the Japanese pronunciation of the Korean place name Changjin, and the name stuck due to the outdated Japanese maps used by UN forces. The battle's main focus was around the 78 mi long road that connects Hungnam and Chosin Reservoir, which served as the only retreat route for the UN forces. Through these roads, Yudami-ni and Sinhung-ni, located at the west and east side of the reservoir respectively, are connected at Hagaru-ri. From there, the road passes through Koto-ri and eventually leads to the port of Hungnam. The area around the Chosin Reservoir was sparsely populated.
The battle was fought over some of the roughest terrain during some of the harshest winter weather conditions of the Korean War. The road was created by cutting through the hilly terrain of Korea, with steep climbs and drops. Dominant peaks, such as the Funchilin Pass and the Toktong Pass, overlook the entire length of the road. The road's quality was poor, and in some places it was reduced to a one lane gravel trail. On 14 November, a cold front from Siberia descended over the Chosin Reservoir, and the temperature plunged to as low as -35 °F. The cold weather was accompanied by frozen ground, creating considerable danger of frostbite casualties, icy roads, and weapon malfunctions. Medical supplies froze; morphine syrettes had to be defrosted in a medic's mouth before they could be injected; frozen blood plasma was useless on the battlefield. Even cutting off clothing to deal with a wound risked gangrene and frostbite. Batteries used for the Jeeps and radios did not function properly in the temperature and quickly ran down. The lubrication in the guns gelled and rendered them useless in battle. Likewise, the springs on the firing pins would not strike hard enough to fire the round, or would jam.
Forces and strategies.
Although the 1st Marine Division landed at Wonsan as part of Almond's US X Corps, Almond and Major General Oliver P. Smith of the 1st Marine Division shared a mutual loathing of each other that dated back to a meeting before the landing at Inchon, during which Almond had spoken of how easy amphibious landings are even though he had never been involved in one. Smith believed that there were large numbers of Chinese forces in North Korea despite the fact that higher headquarters in Tokyo had said otherwise, while Almond felt Smith was overly cautious. The mutual distrust between the two commanders made Smith slow the 1st Marine Division's advance towards the Chosin Reservoir against Almond's instructions. Along the way Smith established supply points and airfields at Hagaru-ri and Koto-ri.
While the US X Corps was pushing towards the reservoir, the Chinese formulated their strategy based on their experiences in the Chinese Civil War. Working from the assumption that only a light UN presence would be at the reservoir, the 9th Army was to first destroy the UN garrisons at Yudami-ni and Sinhung-ni, then push towards Hagaru-ri. Believing that the bulk of the US X Corps would scramble to rescue the destroyed units, the 9th Army would then block and trap the main UN forces on the road between Hagaru-ri and Hungnam. The 9th Army initially committed six divisions for the battle, with most of the forces concentrated at Yudami-ni and Sinhung-ni.
The flaw in the Chinese plan was a lack of accurate intelligence on the UN forces. Although the US X Corps was stretched thin over northeast Korea, the slow Marine advance allowed the bulk of the US 1st Marine Division, including the 5th, 7th and 11th Marines, to be concentrated at Yudami-ni. Conversely, the strategically important Hagaru-ri, which contained an airfield and a supply dump, was not a priority for the Chinese despite being lightly defended by the 1st and the 7th Marines. Only the Regimental Combat Team 31, an understrength and hastily formed regimental combat team of the US 7th Infantry Division, was thinly spread along the eastern bank of the reservoir. Those units would later take the brunt of the Chinese assaults. As for the UN strength, the 1st Marine Division had an effective strength of 25,473 men at the start of the battle, and it was further reinforced by the British 41 Royal Marine Commando and the equivalent strength of two regiments from the 3rd and the 7th Infantry Divisions. Thus the UN forces had an approximate strength of 30,000 during the course of the battle. The UN forces at Chosin were also supported by one of the greatest concentrations of air power during the Korean War, in which the 1st Marine Air Wing stationed at Yonpo Airfield and five aircraft carriers from the US Navy Task Force 77 were able to launch 230 sorties daily to provide close air support during the battle, while the US Air Force Far East Combat Cargo Command in Japan reached the capacity of airdropping 250 tons of supplies per day to resupply the trapped UN forces.
Although the 9th Army was one of China's elite formations composed of veterans and former POWs from the Huaihai Campaign, several deficiencies hampered its ability during the battle. Initially the 9th Army was intended to be outfitted in Manchuria during November, but Mao suddenly ordered it into Korea before that could happen. As the result, the 9th Army received almost no winter gear for the harsh Korean winter. Similarly, poor logistics forced the 9th Army to abandon heavy artillery, while working with little food and ammunition. The food shortage forced the 9th Army to station a third of its strength away from the Chosin Reservoir, and starvation and exposure soon broke out among the Chinese units as foraging was not an option at the sparsely populated reservoir. By the end of the battle, more Chinese troops died from the cold than from combat and air raids. As for the Chinese strength, it is normally assumed that the Chinese had 120,000 troops for the battle, due to the fact that the 9th Army were composed of 12 divisions with a nominal strength of 10,000 men per division. But during the course of the battle, the 9th Army employed only 10 divisions, while all divisions were at 65 to 70 percent strength at the start of the battle. Thus the actual Chinese strength for the battle was approximately 67,000.
Battle.
On the night of 27 November, the PVA 20th and 27th Corps of the 9th Army launched multiple attacks and ambushes along the road between the Chosin Reservoir and Koto-ri. At Yudam-ni, the 5th, 7th and 11th Marines were surrounded and attacked by the PVA 59th, 79th and 89th Division. Similarly, RCT-31 was isolated and ambushed at Sinhung-ni by the PVA 80th and the 81st Division. Finally, the PVA 60th Division surrounded elements of the 1st Marines at Kotor-ri from the north. Caught by complete surprise, the UN forces were cut off at Yudam-ni, Sinhung-ni, Hagaru-ri and Kotor-ri by 28 November.
Actions at Yudam-ni.
Acting on Almond's instruction, Smith ordered the 5th Marines to attack west toward Mupyong-ni on 27 November. The attack was soon stalled by the PVA 89th Division and forced the Marines to dig in on the ridges surrounding Yudam-ni. As night came, three Chinese regiments of the 79th Division attacked the ridges on the north and northwest of Yudam-ni, hoping to annihilate the garrison in one stroke. Close range fighting soon developed as the attackers infiltrated Marine positions, but the 5th and 7th Marines held the line while inflicting heavy casualties to the Chinese. As day broke on 28 November, the Chinese forces and the American defenders were locked in a stalemate around the Yudam-ni perimeter.
While the battle was underway at Yudam-ni, the PVA 59th Division blocked the road between Yudam-ni and Hagaru-ri by attacking the defending Charlie and Fox Companies of the 7th Marines. The successful assault forced Charlie Company to retreat into Yudam-ni which left Fox Company trapped in Toktong Pass, a vital pass that controlled the road. On 29 November, several efforts by the 7th Marines failed to rescue Fox Company despite inflicting heavy casualties on the Chinese. Aided by artillery from Hagaru-ri and Marine Corsair fighters, Fox Company managed to hold out for five days while enduring constant attacks by the PVA 59th Division.
After the heavy losses suffered by the PVA 79th Division at Yudam-ni, 9th Army headquarters realized that the bulk of the 1st Marine Division was stationed at Yudam-ni, with a garrison strength that was double the initial estimate. Believing that any further assaults would be futile, Song Shi-Lun ordered the 9th Army to switch their main attacks toward Sinhung-ni and Hagaru-ri, leaving Yudam-ni alone from 28 November to 30 November. At the same time, the US Eighth Army on the Korean western front was forced into full retreat at the Battle of the Ch'ongch'on River, and MacArthur ordered Almond to withdraw the US X Corps to the port of Hungnam. Acting on the instruction of Almond and Smith, Lieutenant Colonel Raymond L. Murray and Colonel Homer L. Litzenberg, commanders of the 5th and 7th Marines, respectively, issued a joint order to break out from Yudam-ni to Hagaru-ri on 30 November. Faced with tough fighting between the blocking Chinese divisions and the withdrawing Marines, Smith remarked: "Retreat, hell! We're not retreating, we're just advancing in a different direction."
For the breakout, the Marines formed into a convoy with a single M4A3 Sherman tank as the lead. The plan was to have 3rd Battalion, 5th Marines (3/5) as the vanguard of the convoy, with three battalions covering the rear. At the same time, 1st Battalion, 7th Marines (1/7) would attack towards Fox Company in order to open the road at Toktong Pass. To start the breakout, 3rd Battalion, 7th Marines (3/7) had to first attack south and capture Hill 1542 and Hill 1419 in order to cover the road from Chinese attacks. The breakout was carried out under the air cover of the 1st Marine Air Wing.
On the morning of 1 December, 3rd Battalion, 7th Marines (3/7) engaged the PVA 175th Regiment of the 59th Division at Hill 1542 and Hill 1419. The tenacious Chinese defenders soon forced the Marines to dig in on the slopes between the road and the peaks when the convoy passed 3/7's position by the afternoon. With Hagaru-ri still not captured, the PVA High Command scrambled the 79th Division to resume attacks on Yudam-ni while the 89th Division rushed south towards Koto-ri. The Chinese struck at night, and the ferocious fighting forced the rear covering forces to call in night fighters to suppress the attacks. The fighting lasted well into the morning of 2 December until all the Marines managed to withdraw from Yudam-ni.
At the same time, 1st Battalion, 7th Marines (1/7) also tried to break the Chinese blockade at Hill 1419 on 1 December. Despite being badly reduced by combat, hunger and frostbite, the PVA 59th Division sent in its last five platoons and refused to yield. As night approached, 1/7 finally captured the peak and started to march through the hills on the east side of the road. Relying on the element of surprise, they managed to destroy several Chinese positions along the road. On the morning of 2 December, a joint attack by Fox Company and 1/7 secured the Toktong Pass, thus opening the road between Yudam-ni and Hagaru-ri.
Although the road had been opened between Yudam-ni and Hagaru-ri, the convoy still had to fight through the numerous Chinese positions on the hills overlooking the road. On the first night of the retreat, the Chinese struck the convoy in force and inflicted heavy casualties upon 3rd Battalion, 5th Marines (3/5). Although strong air cover suppressed most of the Chinese forces for the rest of the march, the cold weather, harassing fire, raiding parties, and road blocks slowed the retreat to a crawl while inflicting numerous casualties. Despite those difficulties, the convoy reached Hagaru-ri in an orderly fashion on the afternoon of 3 December, with the withdrawal completed on 4 December.
East of the reservoir.
Regimental Combat Team 31 (RCT-31), later known as "Task Force Faith", was a hastily formed regimental combat team from the 7th Infantry Division that guarded the right flank of the Marine advance towards Mupyong-ni. Before the battle, RCT-31 was spread thin with main elements separated on the hills north of Sinhung-ni, the inlet west of Sinhung-ni, and the town of Hudong-ni south of Sinhung-ni. Although the Chinese believed RCT-31 to be a reinforced regiment, the task force was actually under strength with one battalion missing, due to the bulk of the 7th Infantry Division being scattered over northeast Korea.
On the night of 27 November, three regiments from the 80th Division attacked the northern hills and the inlets, completely surprising the defenders. The ensuing battle inflicted heavy casualties on the 1st Battalion, 32nd Infantry to the north of Sinhung-ni, while the 57th Field Artillery Battalion and the 3rd Battalion, 31st Infantry were almost overrun at the inlet. The Chinese also sent the 242nd Regiment of the 81st Division towards Hill 1221, an undefended hill that controlled the road between Sinhung-ni and Hudong-ni. As the night's fighting ended, RCT-31 was separated into three elements.
Believing that the defenders were completely destroyed at the inlet, the Chinese stopped their attacks and proceeded to loot the US positions for food and clothing. As the morning came on 28 November, the 3rd Battalion, 31st Infantry counterattacked the PVA 239th Regiment at the inlet, sending the surprised Chinese back in a complete rout. In the afternoon, Almond flew into the perimeter of RCT-31, convinced that RCT-31 was strong enough to begin its attack north and deal with whatever "remnants" of Chinese forces that were in their way. Almond ordered Colonel Allan D. Maclean, the commander of RCT-31, to resume the offensive north while presenting Silver Stars to three of Maclean's officers. In disgust, Lieutenant Colonel Don C. Faith, Jr., the commander of the 1st Battalion, 32nd Infantry, threw his medal into the snow.
On the night of 28 November, the PVA 80th Division attacked again with three regiments. At the inlet, the Chinese assault became a disaster as communications broke down while devastating fire from the anti-aircraft (AA) guns attached to the 57th Field Artillery Battalion swept the Chinese ranks. In the aftermath of the fighting, the PVA 238th and the 239th Regiment together had less than 600 soldiers. The attacks by PVA 240th Regiment, on the other hand, forced Maclean to order a retreat from the northern hills towards the inlet. On 29 November, the 1st Battalion managed to break through the Chinese blockade and reached the inlet, but Maclean disappeared as he mistook some Chinese soldiers as American. The Chinese finally stopped their attacks on the night of 29 November while waiting for fresh reinforcements.
While RCT-31 was under siege, Almond finally instructed the 1st Marine Division to rescue RCT-31 by breaking out of Yudam-ni—an impossible order for Smith to implement. Only the 31st Tank Company tried to rescue RCT-31 by attacking Hill 1221, but without infantry support, the two armored attacks on 28 and 29 November were stalled by slippery roads, rough terrain, and close infantry assaults. By 30 November the US forces evacuated Hudong-ni in order to defend Hagaru-ri, leaving the rest of RCT-31 completely stranded.
On 30 November, Major General David G. Barr, the commander of the 7th Infantry Division, flew into the Sinhung-ni inlet and met with Faith, who by now had assumed command of RCT-31. Faith expressed the difficulties for a breakout, particularly the 500 wounded that RCT-31 had to carry. On the same day, the PVA 94th Division arrived as reinforcements for the 80th Division. By midnight, four Chinese regiments renewed their attacks and Zhan Danan, the commander of the 80th Division, ordered the complete destruction of RCT-31 before dawn. Again, the 57th Battalion's AA guns held the Chinese at bay, but the shell supplies were running desperately low. On the day of 1 December, Faith finally ordered RCT-31 to breakout from Sinhung-ni and withdraw to Hagaru-ri.
The breakout began as soon as the weather allowed the 1st Marine Air Wing to provide air cover on 1 December. As the soldiers formed a convoy and tried to leave the perimeter, the PVA 241st Regiment immediately swarmed over the American forces, with three other regiments closing in. Left with no choice, the covering aircraft dropped napalm right in front of RCT-31, causing casualties among both Chinese and US troops. The resulting firestorm wiped out the blocking Chinese company, allowing the convoy to advance. As the front of RCT-31 made their way forward, heavy small arms fire caused many members of the rear guard to seek shelter below the road instead of protecting the trucks. Chinese fire also killed or wounded those already in the trucks as well as the drivers, who viewed the job as a form of suicide. Slowly, the convoy approached a roadblock under Hill 1221 in the late afternoon. Several parties tried to clear Hill 1221, but after taking part of the hill, the leaderless soldiers continued out onto the frozen reservoir instead of returning to the column. As Faith led an assault on the roadblock, he was hit by a Chinese grenade and subsequently died of his wounds. The convoy managed to fight past the first road block, but as it reached the second at Hudong-ni, RCT-31 disintegrated under Chinese attacks. About 1,050 soldiers out of the original 2,500 managed to reach Hagaru-ri, and only 385 survivors were deemed able-bodied. The remnants of RCT-31 were formed into a provisional army battalion for the rest of the battle.
Actions at Hagaru-ri.
To support the Marine attack towards Mupyong-ni, Hagaru-ri became an important supply dump with an airfield under construction. Smith and 1st Marine Division headquarters were also located at Hagaru-ri. With the bulk of the 1st Marine Division gathered at Yudam-ni, Hagaru-ri was lightly defended by two battalions from the 1st and 7th Marines, the rest of the garrison being composed of engineers and rear support units from both the Army and the Marine Corps.
The original Chinese plan called for the 58th Division to attack Hagaru-ri on the night of 27 November, but the division became lost in the countryside due to the outdated Japanese maps it used. It was not until the dawn of 28 November that the 58th Division arrived at Hagaru-ri. Meanwhile, from the fighting and ambushes that had occurred the previous night, the garrison at Hagaru-ri noticed the Chinese forces around them. Lieutenant Colonel Thomas L. Ridge, commander of 3rd Battalion, 1st Marines (3/1), predicted the Chinese attack would come on the night of 28 November. Almost everyone, including rear support units with little combat training, was pressed into the front line due to the manpower shortage, and the entire perimeter was on full alert by 21:30.
It was not long before the PVA 173rd Regiment attacked the western and the southern perimeter, while the 172nd Regiment struck the hills on the northern perimeter. Despite the preparations, the understrength garrison was overwhelmed, with the Chinese opening several gaps in the defenses and reaching the rear areas. The resulting chaos, however, caused a breakdown in discipline among the Chinese soldiers, who began looting food and clothing instead of exploiting the situation. The defending Americans managed to destroy the Chinese forces in counterattacks, while a breakdown of communications between the Chinese regiments allowed the gaps to close. When the fighting stopped, the Chinese had only gained the East Hill on the northern perimeter. Another attack was planned for the night of 29 November, but air raids by VMF-542 broke up the Chinese formations before it could be carried out.
Given the critical manpower shortage at Hagaru-ri, on November 29, Smith ordered Colonel Lewis "Chesty" Puller of the First Marine Regiment to assemble a task force to be sent north from Koto-ri to open the road south of Hagaru-ri. In response, a task force was formed with 921 troops from the 41 Royal Marine Commando, G Company of the 1st Marines and B Company of the 31st Infantry. The task force was dubbed "Task Force Drysdale" after its commander Lieutenant Colonel Douglas B. Drysdale, who also commanded 41 Commando. On the afternoon of 29 November, Task Force Drysdale pushed north from Koto-ri while under constant attack from the PVA 60th Division. The task force's harrowing experience later earned the road the nickname "Hell Fire Valley". As the Chinese attacks dragged on, the task force became disorganized, and a destroyed truck in the convoy later split the task force into two segments. Although the lead segment of the task force fought its way into Hagaru-ri on the night of 29 November, the rear segment was destroyed. Despite suffering 159 wounded and 162 dead and missing, the task force managed to bring in 300 badly needed infantrymen for the defense at Hagaru-ri.
As more reinforcements arrived from Hudong-ni on 30 November, the garrisons attempted to recapture the East Hill. All efforts failed despite the destruction of a Chinese company. When darkness settled, the PVA 58th Division gathered its remaining 1,500 soldiers in a last-ditch attempt to capture Hagaru-ri. The reinforced defenders annihilated most of the attacking forces, with only the defences around the East Hill giving way. As the Chinese tried to advance from the East Hill, they were cut down by the 31st Tank Company.
By 1 December, the PVA 58th Division was virtually destroyed, with the remainder waiting for reinforcements from the 26th Corps of the 9th Army. But much to the frustration of Song Shi-Lun, the 26th Corps did not arrive before the Marines broke out of Yudam-ni. The airfield was opened to traffic on 1 December, allowing UN forces to bring in reinforcements and to evacuate the dead and the wounded. With the Marines at Yudam-ni completing their withdrawal on 4 December, the trapped UN forces could finally start their breakout towards the port of Hungnam.
Breakout.
After a short period of rest, the breakout began on 6 December with the 7th Marines as the vanguard of the retreating column while the 5th Marines covered the rear. At the same time, the much-delayed PVA 26th Corps arrived at Hagaru-ri with its 76th and 77th Division relieving the 58th and 60th Divisions. As the 7th Marines pushed aside the PVA 76th Division south of Hagaru-ri, the 5th Marines took over the Hagaru-ri perimeter and recaptured the East Hill from the 76th Division. In a last effort to stop the breakout, the customary Chinese night attack returned with the 76th and 77th Division striking the Hagaru-ri perimeter from all directions. The Marines repulsed the Chinese attacks, inflicting heavy casualties.
Meanwhile, the 7th Marines opened the road between Hagaru-ri and Koto-ri by capturing the high ground surrounding the road. But as soon as the Marines pulled out, the 77th Division returned to the peaks and attacked the column. Chaotic fighting broke out within the column and the retreat was slowed to a crawl. The Marine night fighters, however, returned to subdue the Chinese forces, and the fighting destroyed most of the blocking troops. On 7 December, the rest of the column managed to reach Koto-ri with little difficulty with the last elements arrived at Koto-ri that night.
After the failure of the 26th Corps at Hagaru-ri, the PVA High Command ordered the 26th and the 27th Corps to chase the escaping UN force with the 20th Corps blocking the escape route. But with most of the 20th Corps destroyed at Yudam-ni and Hagaru-ri, the only forces between Koto-ri and Hungnam were the remnants of the 58th and the 60th Divisions. In desperation, Song Shi-Lun ordered these troops to dig in at Funchilin Pass while blowing up the vital treadway bridge, hoping the terrain and obstacles would allow the 26th and the 27th Corps to catch up with the retreating UN forces. The PVA 180th Regiment that occupied Hill 1081 blew up the original concrete bridge and two improvised replacements in succession, believing the bridge was rendered irreparable. In response, 1st Battalion, 1st Marines (1/1) attacked Hill 1081 from the south, and the hill was captured on 9 December after the defenders fought to the last man. At the same time, the 7th Marines and RCT-31 attacked the treadway bridge from the north, only to encounter defenders that were already frozen in their foxholes.
With the path to Hungnam blocked at Funchilin Pass, eight C-119 Flying Boxcars flown by the US 314th Troop Carrier Wing were used to drop portable bridge sections by parachute. The bridge, consisting of eight separate 18 ft long, 2900 lb sections, was dropped one section at a time, using a 48 ft parachute on each section. Four of these sections, together with additional wooden extensions were successfully reassembled into a replacement bridge by Marine Corps combat engineers and the US Army 58th Engineer Treadway Bridge Company on 9 December, enabling UN forces to proceed. Outmaneuvered, the PVA 58th and 60th Divisions still tried to slow the UN advance with ambushes and raids, but after weeks of non-stop fighting, the two Chinese divisions combined had only 200 soldiers left. The last UN forces left Funchilin Pass by 11 December.
One of the last engagements during the withdrawal was an ambush at Sudong by the pursuing PVA 89th Division, which Task Force Dog of the 3rd Infantry Division repulsed with little difficulty. The trapped UN forces finally reached the Hungnam perimeter by 21:00 on 11 December.
Evacuation at Hungnam.
By the time the UN forces arrived at Hungnam, MacArthur had already ordered the evacuation of the US X Corps on 8 December in order to reinforce the US Eighth Army, which by then was badly depleted and retreating rapidly towards the 38th parallel. Following his orders, the ROK I Corps, the ROK 1st Marine Regiment, the US 3rd Infantry Division and the US 7th Infantry Division had also set up defensive positions around the port. Some skirmishes broke out between the defending US 7th, 17th and 65th Infantry and the pursuing PVA 27th Corps, but against the strong naval fire support provided by US Navy Task Force 90, the badly mauled 9th Army was in no shape to approach the Hungnam perimeter.
In what US historians called the "greatest evacuation movement by sea in US military history", a 193-ship armada assembled at the port and evacuated not only the UN troops, but also their heavy equipment and roughly a third of the Korean refugees. One Victory ship, the "SS Meredith Victory" evacuated 14,000 refugees. The last UN unit left at 14:36 on 24 December, and the port was destroyed to deny its use to the Chinese and North Korean forces. The PVA 27th Corps entered Hungnam on the morning of 25 December.
Aftermath.
...Casualties had reached a 40,000 high. The Central [Government] expresses its deepest sorrow...
Mao Zedong
While the US X Corps was being evacuated from the eastern front, the US Eighth Army had already retreated to the 38th parallel on the western front in the aftermath of the Battle of the Ch'ongch'on River. With the entire UN front collapsing, the race to the Yalu was ended with the communist forces of China recapturing much of North Korea. The Korean War would drag on for another two and a half years before the armistice was signed on 27 July 1953. Besides the loss of North Korea, the US X Corps and the ROK I Corps later reported a total of 10,495 battle casualties, of which 4,385 were from the US Marines, 3,163 were from the US Army, 2,812 were from South Koreans attached to American formations and 78 were from the British Royal Marines. Outside of the combat losses, the 1st Marine Division also reported 7,338 non-battle casualties due to the cold weather.
Despite the losses, the US X Corps preserved much of its strength. About 105,000 soldiers, 98,000 civilians, 17,500 vehicles, and 350,000 tons of supplies were shipped from Hungnam to Pusan, and they would later rejoin the war effort in Korea. Smith was credited for saving the US X Corps from destruction, while the 1st Marine Division, the 41 Royal Marine Commando and the Army's RCT-31 were awarded the Presidential Unit Citation for their tenacity during the battle. Fourteen Marines, two Soldiers and one Navy pilot received the Medal of Honor, and all of the UN troops that served at Chosin were later honored with the nickname "The Chosin Few". On 15 September 2010, the Veterans of the Korean War Chosin Reservoir Battle memorial was unveiled by the United States Marine Corps Commandant General James T. Conway at Camp Pendleton.
China was also catapulted into the status of a major military power following the victory at Chosin, but the victory came with a staggering cost. With the escape of the US X Corps and the ROK I Corps, Mao's vision for Chosin was not realized, and the failure caused Song Shi-Lun to offer his resignation. At the same time, heavy casualties caused by both combat and poor logistical support destroyed much of the eight elite divisions under the 20th and the 27th Corps. Of those eight divisions, two divisions were forced to disband, and not until March 1951 did the 9th Army return to its normal strength and become combat effective. With the absence of nearly 40 percent of the Chinese forces in Korea in early 1951, the heavy Chinese losses at Chosin ultimately enabled the UN forces to maintain a foothold in Korea.
Operation Glory.
During the battle, UN casualties were buried at temporary grave sites along the road. Operation Glory took place from July to November 1954, during which the dead of each side were exchanged. The remains of 4,167 US Soldiers and Marines were exchanged for 13,528 North Korean and Chinese dead. In addition, 546 civilians who died in UN prisoner of war camps were turned over to the South Korean government. After Operation Glory, 416 Korean War "unknowns" were buried in the National Memorial Cemetery of the Pacific. According to a Defense Prisoner of War/Missing Personnel Office (DPMO) white paper, 1,394 names were also transmitted during "Operation Glory" from the Chinese and North Koreans, of which 858 proved to be correct. The 4,167 returned remains were found to be 4,219 individuals, of whom 2,944 were found to be Americans, with all but 416 identified by name. Of the 239 Korean War unaccounted for, 186 are not associated with the Punchbowl unknowns. From 1990 to 1994 North Korea excavated and returned more than 208 sets of remains which possibly include 200 to 400 US servicemen, but very few have been identified due to the co-mingling of remains. From 2001 to 2005, more remains were recovered from the Chosin Battle site, and around 220 were recovered near the Chinese border between 1996 to 2006.
Notes.
Footnotes
Citations
References.
</dl>
Further reading.
</dl>

</doc>
<doc id="49399" url="http://en.wikipedia.org/wiki?curid=49399" title="XY sex-determination system">
XY sex-determination system

The XY sex-determination system is the sex-determination system found in humans, most other mammals, some insects ("Drosophila"), and some plants ("Ginkgo"). In this system, the sex of an individual is determined by a pair of sex chromosomes (gonosomes). Females have two of the same kind of sex chromosome (XX), and are called the homogametic sex. Males have two distinct sex chromosomes (XY), and are called the heterogametic sex.
This system is in contrast with the ZW sex-determination system found in birds, some insects, many reptiles, and other animals, in which the heterogametic sex is female.
A temperature-dependent sex determination system is found in some reptiles.
Mechanisms.
All animals have a set of DNA coding for genes present on chromosomes. In humans, most mammals, and some other species, two of the chromosomes, called the X chromosome and Y chromosome, code for sex. In these species, one or more genes present on their Y-chromosome that determine maleness. In this process, an X chromosome and a Y chromosome act to determine the sex of offspring, often due to genes located on the Y chromosome that code for maleness. Offspring have two sex chromosomes: an offspring with two X chromosomes will develop female characteristics, and an offspring with an X and a Y chromosome will develop male characteristics.
Humans.
In humans, a single gene ("SRY") present on the Y chromosome acts as a signal to set the developmental pathway towards maleness. Presence of this gene starts off the process of virilization. This and other factors result in the sex differences in humans. The cells in females, with two X chromosomes, undergo X-inactivation, in which one of the two X chromosomes is inactivated. The inactivated X chromosome remains within a cell as a Barr body.
Humans, as well as some other organisms, can have a chromosomal arrangement that is contrary to their phenotypic sex; for example, XX males or XY females (see androgen insensitivity syndrome). Additionally, an abnormal number of sex chromosomes (aneuploidy) may be present, such as Turner's syndrome, in which a single X chromosome is present, and Klinefelter's syndrome, in which two X chromosomes and a Y chromosome are present, XYY syndrome and XXYY syndrome. Other less common chromosomal arrangements include: triple X syndrome, 48, XXXX, and 49, XXXXX.
Other animals.
XY system in mammals: Sex is determined by presence of Y. "Female" is the default sex; due to the absence of the Y. In the 1930s, Alfred Jost determined that the presence of testosterone was required for Wolffian duct development in the male rabbit.
SRY is an intronless sex-determining gene on the Y chromosome in the therians (placental mammals and marsupials). Non-human mammals use several genes on the Y-chromosome. Not all male-specific genes are located on the Y-chromosome. Other species (including most "Drosophila" species) use the presence of two X chromosomes to determine femaleness. One X chromosome gives putative maleness. The presence of Y-chromosome genes is required for normal male development.
Other systems.
Birds and many insects have a similar system of sex determination ("ZW sex-determination system"), in which it is the females that are heterogametic (ZW), while males are homogametic (ZZ).
Many insects of the order Hymenoptera instead have a system (the "haplo-diploid sex-determination system"), where the males are haploid individuals (which just one chromosome of each type), while the females are diploid (with chromosomes appearing in pairs). Some other insects have the "X0 sex-determination system", where just one chromosome type appears in pairs for the female but alone in the males, while all other chromosomes appear in pairs in both sexes.
Influences.
Genetic.
For a long time, biologists believed that the female form was the default template for the mammalian fetuses of both sexes. After the discovery of the testis-determining gene SRY, many scientists shifted to the theory that the genetic mechanism that determines a fetus to develop into a male form was initiated by the SRY gene, which was thought to be responsible for the production of testosterone and its overall effects on body and brain development. This perspective still shared the classical way of thinking; that in order to produce two sexes, nature has developed a default female pathway and an active pathway by which male genes would initiate the process of determining a male sex, as something that is developed in addition to and based on the default female form. This view is no longer considered accurate by most scientists who study the genetics of sex. In an interview for the "Rediscovering Biology" website, researcher Eric Vilain described how the paradigm changed since the discovery of the SRY gene:
In mammals, including humans, the SRY gene is responsible with triggering the development of non-differentiated gonads into testes, rather than ovaries. However, there are cases in which testes can develop in the absence of an SRY gene (see sex reversal). In these cases, the SOX9 gene, involved in the development of testes, can induce their development without the aid of SRY. In the absence of SRY and SOX9, no testes can develop and the path is clear for the development of ovaries. Even so, the absence of the SRY gene or the silencing of the SOX9 gene are not enough to trigger sexual differentiation of a fetus in the female direction. A recent finding indicates that ovary development and maintenance is an active process, regulated by the expression of a "pro-female" gene, FOXL2. In an interview for the "TimesOnline" edition, study co-author Robin Lovell-Badge explained the significance of the discovery:
Implications for human health and social policy.
Looking into the genetic determinants of human sex can have wide-ranging consequences. Scientists have been studying different sex determination systems in fruit flies and animal models to attempt an understanding of how the genetics of sexual differentiation can influence biological processes like reproduction, ageing and disease. Since many of the same genetic mechanisms involved in determining sexually dimorphic traits have been preserved during evolution to this day in fruit flies, mice, and humans, understanding how these genetic mechanisms work can lead to improved healthcare that takes into account sex differences. The research could also lead to changes in how people understand and perceive sex differences.
History.
Ancient ideas on sex determination.
Since ancient times, people have believed that the sex of an infant is determined by how much heat a man's sperm had during insemination. Aristotle wrote that:
Aristotle claimed that the male principle was the driver behind sex determination, such that if the male principle was insufficiently expressed during reproduction, the fetus would develop as a female. In contrast, modern genetics has developed a view on sex determination in which no one single factor is responsible for determining sex; a number of pro-male, anti-male and pro-female genes being responsible, though the largest factor is whether the male's gamete carries an X or Y chromosome.
Beginnings of genetics of sex determination.
Edmund Beecher Wilson and Nettie Stevens are credited with discovering, in 1905, the chromosomal XY sex-determination system; the fact that males have XY sex chromosomes and females have XX sex chromosomes. 
The first clues to the existence of a factor that determines the development of testis in mammalians came from experiments carried out by Alfred Jost, who castrated embryonic rabbits in utero and noticed that they all developed as female. 
In 1959, C. E. Ford and his team, in the wake of Jost's experiments, discovered that the Y chromosome was needed for a fetus to develop as male when they examined patients with Turner's syndrome, who grew up as phenotypic females, and found them to be X0 (hemizygous for X and no Y). At the same time, Jacob & Strong described a case of a patient with Klienfelter's syndrome (XXY), which implicated the presence of a Y chromosome in development of maleness.
All these observations lead to a consensus that a dominant gene that determines testis development (TDF) must exist on the human Y chromosome. The search for this testis-determining factor (TDF) led a team of scientists in 1990 to discover a region of the Y chromosome that is necessary for the male sex determination, which was named SRY (Sex-determining Region of the Y chromosome).

</doc>
<doc id="49400" url="http://en.wikipedia.org/wiki?curid=49400" title="Window">
Window

A window is an opening in a wall, door, roof or vehicle that allows the passage of light and, if not closed or sealed, air and sound. Modern windows are usually glazed or covered in some other transparent or translucent material. Windows are held in place by frames. Many glazed windows may be opened, to allow ventilation, or closed, to exclude inclement weather. Windows often have a latch or similar mechanism to lock the window shut.
Types include the eyebrow window, fixed windows, single-hung and double-hung sash windows, horizontal sliding sash windows, casement windows, awning windows, hopper windows, tilt and slide windows (often door-sized), tilt and turn windows, transom windows, sidelight windows, 
jalousie or louvered windows, clerestory windows, skylights, roof windows, roof lanterns, bay windows, 
oriel windows, thermal, or Diocletian, windows, picture windows, emergency exit windows, stained glass windows, French windows, and double- and triple paned windows.
The Romans were the first known to use glass for windows, a technology likely first produced in Roman Egypt—In Alexandria ca. 100 AD. Paper windows were economical and widely used in ancient China, Korea and Japan. In England, glass became common in the windows of ordinary homes only in the early 17th century whereas windows made up of panes of flattened animal horn were used as early as the 14th century. Modern-style floor-to-ceiling windows became possible only after the industrial plate glass making processes were perfected.
Etymology.
The word "window" originates from the Old Norse 'vindauga', from 'vindr – wind' and 'auga – eye', i.e., "wind eye". In Norwegian Nynorsk and Icelandic the Old Norse form has survived to this day (in Icelandic only as a less used synonym to "gluggi"), in Swedish the word "vindöga" remains as a term for a hole through the roof of a hut, and in the Danish language 'vindue' and Norwegian Bokmål 'vindu', the direct link to 'eye' is lost, just like for 'window'. The Danish (but not the Bokmål) word is pronounced fairly similarly to "window".
" Window" is first recorded in the early 13th century, and originally referred to an unglazed hole in a roof. "Window" replaced the Old English "eagþyrl", which literally means 'eye-hole,' and 'eagduru' 'eye-door'. Many Germanic languages however adopted the Latin word 'fenestra' to describe a window with glass, such as standard Swedish 'fönster', or German 'Fenster'. The use of "window" in English is probably because of the Scandinavian influence on the English language by means of loanwords during the Viking Age. In English the word "fenester" was used as a parallel until the mid-18th century and "fenestration" is still used to describe the arrangement of windows within a façade. Also, words such as "defenestration" are in use, meaning to throw something out of a window.
From Webster's 1828 Dictionary: "Window", n. [G. The vulgar pronunciation is windor, as if from the Welsh gwyntdor, wind-door.] However, 20th and 21st century etymology shows that the word came from Old Norse vindauga, from vindr ‘wind’ + auga ‘eye.’.
History.
The earliest windows were just holes in a wall. Later, windows were covered with animal hide, cloth, or wood. Shutters that could be opened and closed came next. Over time, windows were built that both protected the inhabitants from the elements and transmitted light, using multiple small pieces of translucent material (such as flattened pieces of translucent animal horn, thin slices of marble, or pieces of glass) set in frameworks of wood, iron or lead. In the Far East, paper was used to fill windows.
The Romans were the first known to use glass for windows, a technology likely first produced in Roman Egypt—In Alexandria ca. 100 AD, cast glass windows, albeit with poor optical properties, began to appear—but these were small thick productions, little more than blown glass jars (cylindrical shapes) flattened out into sheets with circular striation patterns throughout. It would be over a millennium before a window glass became transparent enough to see through clearly, as we think of it now.
Over the centuries techniques were developed to shear through one side of a blown glass cylinder and produce thinner rectangular window 
panes from the same amount of glass material. This gave rise to tall narrow windows, usually separated by a vertical support called a mullion. Mullioned glass windows were the windows of choice among European well-to-do, whereas paper windows were economical and widely used in ancient China, Korea and Japan. In England, glass became common in the windows of ordinary homes only in the early 17th century whereas windows made up of panes of flattened animal horn were used as early as the 14th century. Noted science historian, author and television show host/producer James Burke attributes the rapid deforestation of Great Britain in the late 1500s to the uptick in production of glazed windows as well as iron cannon production (1st Cast in 1547). He writes further this gave rise to coal for fuel, which spurred iron production, requiring more coal, and more iron, then steam engine pumps, canals... and more iron; all because windows became a middle class commodity in the latter days of the little ice age, one large factor among several leading to the deforesting English woodlands, and the switch over to a coal economy.
Modern-style floor-to-ceiling windows became possible only after the industrial plate glass making processes were perfected. Modern windows are usually filled with glass, although a few are transparent plastic.
Types.
Eyebrow.
The term eyebrow window is used in two ways: a curved top window in a wall or in an eyebrow dormer; and a row of small windows usually under the front eaves such as the James-Lorah House in Pennsylvania.
Fixed.
A window that cannot be opened, whose function is limited to allowing light to enter (unlike an unfixed window, which can open and close). Clerestory windows are often fixed. Transom windows may be fixed or operable. This type of window is used in situations where light or vision alone is needed as no ventilation is possible windows without the use of trickle vents or overglass vents.
Single-hung sash.
One sash is movable (usually the bottom one) and the other fixed. This is the earlier form of sliding sash window, and is also cheaper.
Double-hung sash.
A sash window is the traditional style of window in the United Kingdom, and many other places that were formerly colonized by the UK, with two parts (sashes) that overlap slightly and slide up and down inside the frame. The two parts are not necessarily the same size. Currently most new double-hung sash windows use spring balances to support the sashes, but traditionally, counterweights held in boxes on either side of the window were used. These were and are attached to the sashes using pulleys of either braided cord or, later, purpose-made chain. Three types of spring balances are called a tape or clock spring balance; channel or block-and-tackle balance; and a spiral or tube balance.
Double-hung sash windows were traditionally often fitted with shutters. Sash windows can be fitted with simplex hinges that let the window be locked into hinges on one side, while the rope on the other side is detached—so the window can be opened for fire escape or cleaning.
Horizontal sliding sash.
Has two or more sashes that overlap slightly but slide horizontally within the frame. In the UK, these are sometimes called "Yorkshire" sash windows, presumably because of their traditional use in that county.
Casement.
A window with a hinged sash that swings in or out like a door comprising either a side-hung, top-hung (also called "awning window"; see below), or occasionally bottom-hung sash or a combination of these types, sometimes with fixed panels on one or more sides of the sash. In the USA, these are usually opened using a crank, but in parts of Europe they tend to use projection friction stays and espagnolette locking. Formerly, plain hinges were used with a casement stay. Handing applies to casement windows to determine direction of swing; a casement window may be left-handed, right-handed, or double. The casement window is the dominant type now found in the UK and parts of Europe.
Awning.
An awning window is a casement window that is hung horizontally, hinged on top, so that it swings outward like an awning. Emilie Poisson designed this window.
Hopper.
A hopper window is a bottom-pivoting casement window that opens by tilting vertically, typically to the inside.
Tilt and slide.
A window (more usually a door-sized window) where the sash tilts inwards at the top and then slides horizontally behind the fixed pane.
Tilt and turn.
A "tilt and turn" window can both tilt inwards at the top or open inwards from hinges at the side. This is the most common type of window in Germany, its country of origin. It is also widespread in many other European countries.
Transom.
A window above a door; in an exterior door the "transom window is often fixed, in an interior door it can open either by hinges at top or bottom, or rotate on hinges. It provided ventilation before forced air heating and cooling. A fan-shaped transom is known as a fanlight, especially in the British Isles.
Side light.
Windows beside a door or window are called "side"-, "wing"-, and "margen-lights" and "flanking windows".
Jalousie Window.
Also known as a louvered window, the jalousie window consists of parallel slats of glass or acrylic that open and close like a Venetian blind, usually using a crank or a lever. They are used extensively in tropical architecture. A jalousie door is a door with a jalousie window.
Clerestory.
A window set in a roof structure or high in a wall, used for daylighting.
Skylight.
A flat or slope window used for daylighting, built into a roof structure that is out of reach.
Roof.
A sloped window used for daylighting, built into a roof structure. It is one of the few windows that could be used as an exit. Larger roof windows meet building codes for emergency evacuation.
Roof lantern.
A roof lantern is a multi-paned glass structure, resembling a small building, built on a roof for day or moon light. Sometimes includes an additional clerestory. May also be called a cupola.
Bay.
A multi-panel window, with at least three panels set at different angles to create a protrusion from the wall line.
Oriel.
This form of bay window most often appears in Tudor-style houses and monasteries. It projects from the wall and does not extend to the ground. Originally a form of porch, they are often supported by brackets or corbels.
Thermal.
Thermal, or Diocletian, windows are large semicircular windows (or niches) which are usually divided into three lights (window compartments) by two mullions. The central compartment is often wider than the two side lights on either side of it.
Picture.
A picture window is a large fixed window in a wall, typically without glazing bars, or glazed with only perfunctory glazing bars near the edge of the window. Picture windows provide an unimpeded view, as if framing a picture.
Multi-lit.
A window glazed with small panes of glass separated by wooden or lead "glazing bars", or "muntins", arranged in a decorative "glazing pattern" often dictated by the building's architectural style. Due to the historic unavailability of large panes of glass, the multi-lit (or "lattice window") was the most common window style until the beginning of the 20th century, and is still used in traditional architecture.
Emergency exit/egress.
A window big enough and low enough so that occupants can escape through the opening in an emergency, such as a fire. In many countries, exact specifications for emergency windows in bedrooms are given in many building codes. Specifications for such windows may also allow for the entrance of emergency rescuers. Vehicles, such as buses and aircraft, frequently have emergency exit windows as well.
Stained glass.
A window composed of pieces of colored glass, transparent, translucent or opaque, frequently portraying persons or scenes. Typically the glass in these windows is separated by lead glazing bars. Stained glass windows were popular in Victorian houses and some Wrightian houses, and are especially common in churches.
French.
A French window (when hinged "French door") is a large door-sized lattice light, typically set in pairs or multiples thereof. Known as "porte-fenêtre" in France and "portafinestra" in Italy, they often overlook a terrace and are commonly used in modern houses.
Double paned.
Double paned windows have two parallel panes (slabs of glass) with a separation of typically about 1 cm; this space is permanently sealed and filled at the time of manufacture with dry air or other dry nonreactive gas. Such windows provide a marked improvement in thermal insulation (and usually in acoustic insulation as well) and are resistant to fogging and frosting caused by temperature differential. They are widely used for residential and commercial construction in intemperate climates. Triple-paned windows have been commercially manufactured and marketed with claims of additional benefit but have not become common.
Terms.
EN 12519 is the European norm that describes windows terms officially used in EU Member States.
The main terms are:
Labeling.
The United States NFRC Window Label lists the following terms:
The European harmonised standard hEN 14351-1, which deals with doors and windows, defines 23 characteristics (divided into "essential" and "non "essential". Two other, preliminary European Norms that are under development deal with internal pedestrian doors (prEN 14351-2), smoke and fire resisting doors, and openable windows (prEN 16034).
Construction.
Windows can be a significant source of heat transfer. Therefore, insulated glazing units consist of two or more panes to reduce the transfer of heat.
Grids or muntins.
These are the pieces of framing that separate a larger window into smaller panes. In older windows, large panes of glass were quite expensive, so muntins let smaller panes fill a larger space. In modern windows, light-colored muntins still provide a useful function by reflecting some of the light going through the window, making the window itself a source of diffuse light (instead of just the surfaces and objects illuminated within the room). By increasing the indirect illumination of surfaces near the window, muntins tend to brighten the area immediately around a window and reduce the contrast of shadows within the room.
Frame and sash construction.
Frames and sashes can be made of the following materials:
Composites may combine materials to obtain aesthetics of one material with the functional benefits of another.
A special class of PVC window frames, uPVC window frames, became widespread since the late 20th century, particularly in Europe: there were 83.5 million installed by 1998 with numbers still growing as of 2012.
Glazing and filling.
Low-emissivity coated panes reduce heat transfer by radiation, which, depending on which surface is coated, helps prevent heat loss (in cold climates) or heat gains (in warm climates).
High thermal resistance can be obtained by evacuating or filling the insulated glazing units with gases such as argon or krypton, which reduces conductive heat transfer due to their low thermal conductivity. Performance of such units depends on good window seals and meticulous frame construction to prevent entry of air and loss of efficiency.
Modern double-pane and triple-pane windows often include one or more low-e coatings to reduce the window's U-factor (its insulation value, specifically its rate of heat loss). In general, soft-coat low-e coatings tend to result in a lower solar heat gain coefficient (SHGC) than hard-coat low-e coatings.
Modern windows are usually glazed with one large sheet of glass per sash, while windows in the past were glazed with multiple panes separated by "glazing bars", or "muntins", due to the unavailability of large sheets of glass. Today, glazing bars tend to be decorative, separating windows into small panes of glass even though larger panes of glass are available, generally in a pattern dictated by the architectural style at use. Glazing bars are typically wooden, but occasionally lead glazing bars soldered in place are used for more intricate glazing patterns.
Other construction details.
Many windows have movable window coverings such as blinds or curtains to keep out light, provide additional insulation, or ensure privacy.
Windows allow natural light to enter, but too much can have negative effects such as glare and heat gain. Additionally, while windows let the user see outside, there must be a way to maintain privacy on in the inside. Window coverings are practical accommodations for these issues.
Windows and the sun.
Sun incidence angle.
Historically, windows are designed with surfaces parallel to vertical building walls. Such a design allows considerable solar light and heat penetration due to the most commonly occurring incidence of sun angles. In passive solar building design, an extended eave is typically used to control the amount of solar light and heat entering the window(s).
An alternative method is to calculate an optimum window mounting angle that accounts for summer sun load minimization, with consideration of actual latitude of the building. This process has been implemented, for example, in the Dakin Building in Brisbane, California—in which most of the fenestration is designed to reflect summer heat load and help prevent summer interior over-illumination and glare, by canting windows to nearly a 45 degree angle.
Solar window.
Photovoltaic windows not only provide a clear view and illuminate rooms, but also convert sunlight to electricity for the building. In most cases, translucent photovoltaic cells are used.
Passive solar.
Passive solar windows allow light and solar energy into a building while minimizing air leakage and heat loss. Properly positioning these windows in relation to sun, wind, and landscape—while properly shading them to limit excess heat gain in summer and shoulder seasons, and providing thermal mass to absorb energy during the day and release it when temperatures cool at night—increases comfort and energy efficiency. Properly designed in climates with adequate solar gain, these can even be a building's primary heating system.
Window coverings.
A window covering is a shade or screen that provides multiple functions. For example, some window coverings control solar heat gain and glare. There are external shading devices and internal shading devices. Low-e window film is a low-cost alternative to window replacement to transform existing poorly-insulating windows into energy-efficient windows. For high-rise buildings, smart glass can provide an alternative.

</doc>
<doc id="49401" url="http://en.wikipedia.org/wiki?curid=49401" title="Hall">
Hall

In architecture, a hall is a relatively large space enclosed by a roof and walls. In the Iron Age, a mead hall was such a simple building and was the residence of a lord and his retainers. Later, rooms were partitioned from it, so that today the hall of a house is the space inside the front door through which the rooms are reached. Where the hall inside the front door of a house is elongated, it may be called a passage, corridor, or hallway.
The term "hall" is often used to designate a British or Irish country house such as a hall house, or specifically a Wealden hall house, and manor houses.
In later medieval Europe, the main room of a castle or manor house was the great hall. In a medieval building, the hall was where the fire was kept. With time, its functions as dormitory, kitchen, parlour and so on were divided off to separate rooms or, in the case of the kitchen, a separate building.
The Hall and parlor house was found in England and was a fundamental, historical floor plan in parts of the United States from 1620 to 1860.
Many buildings at colleges and universities are formally titled "_______ Hall", typically being named after the person who endowed it, for example, King's Hall, Cambridge. Others, such as Lady Margaret Hall, Oxford, commemorate respected people. Between these in age, Nassau Hall at Princeton University began as the single building of the then college. In medieval origin, these were the halls in which the members of the university lived together during term time. In many cases, some aspect of this community remains.
At colleges in the universities of Oxford and Cambridge, Hall is the dining hall for students, with High Table at one end for fellows. Typically, at "Formal Hall", gowns are worn for dinner during the evening, whereas for "informal Hall" they are not.
A hall is also a building consisting largely of a principal room, that is rented out for meetings and social affairs. It may be privately or government-owned, such as a function hall owned by one company used for weddings and cotillions (organized and run by the same company on a contractual basis) or a community hall available for rent to anyone.
In religious architecture, as in Islamic architecture, the prayer hall is a large room dedicated to the practice of the worship. (example : the prayer hall of the Great Mosque of Kairouan in Tunisia). A hall church is a church with nave and side aisles of approximately equal height.
Following a line of similar development, in office buildings and larger buildings (theatres, cinemas etc.), the entrance hall is generally known as the foyer (the French for fireplace). The atrium, a name sometimes used in public buildings for the entrance hall, was the central courtyard of a Roman house.
Types.
In architecture, the head "double-loaded" describe corridors that connects to rooms on both sides. Conversely, a single-loaded corridor only has rooms on one side (and possible windows on the other). A blind corridor doesn't lead anywhere.

</doc>
<doc id="49402" url="http://en.wikipedia.org/wiki?curid=49402" title="Closet">
Closet

A closet (especially in North American usage) is an enclosed space, small, and is most likely not bigger than a garage, or basement, etc. so to say this, a closet in North America is about the size of a cupboard and no bigger. Rooms like the attic, basement, and garage do not apply to be a closet. A cabinet, or a cupboard in a house or building are used for general storage or hanging or storing clothes.
Modern closets can be built into the walls of the house during construction so that they take up no apparent space in the bedroom, or they can be large, free-standing pieces of furniture designed for clothing storage, in which case they are often called wardrobes or armoires. Closets are often built under stairs, thereby using awkward space that would otherwise go unused.
In current British and Pakistan usage, a "wardrobe" can also be built-in, and the words "cupboard" or walk-in-wardrobe can be used to refer to a closet. In Elizabethan and Middle English, "closet" referred to a larger room in which a person could sit and read in private, but now refers to a small room in general.
In Indian usage, a closet often refers to a toilet. This probably originated from the word "water closet", which refers to a flush toilet.
In North America, chests, trunks and wall-mounted pegs typically provided storage prior to World War II. Built-in wall closets were uncommon and where they did exist, they tended to be small and shallow. Following World War II, however, deeper, more generously sized closets were introduced to new housing designs, which proved to be very attractive to buyers. It has even been suggested that the closet was a major factor in people's migration to the suburbs.
Closet tax question in colonial America.
Though some sources claim that colonial American houses often lacked closets because of a "closet tax" imposed by the British crown, others argue that closets were absent in most houses simply because their residents had few possessions.
In popular culture.
Figuratively, a closet is a place where one hides things; "having skeletons in one's closet" is a figure of speech for having particularly sensitive secrets. Thus, "closet" as an adjective means "secret"—usually with a connotation of vice or shame, as in "a closet alcoholic" or "a closet homosexual," though sometimes used as a humorous exaggeration for any potential embarrassment, as in "a closet comic book fan." To "come out of the closet" is to admit your secrets publicly, but this is now used almost exclusively in reference to homosexuality. The documentary film "The Celluloid Closet" uses this reference to gay people in its examination of how Hollywood films have depicted homosexuals on the screen. This is also extensively used in a controversial episode of "South Park".
Psychologically, bedroom closets are the center of many childhood fears. Children fear during the night that a monster or any other paranormal creature hides inside the closet, and is destined to frighten the child. The Bogeyman is one prominent example. This is a common theme in films. In the first "Poltergeist" movie, the closet was where ghosts hid. The "monster in the closet" fear was developed for comedic possibilities in film "Monsters, Inc.". In the newspaper comic "Bloom County", the character Binkley had an "anxiety closet" in his bedroom, from which his fears would manifest themselves, while he was sleeping. Similarly, the strip "Opus" also has a closet which houses his worries. Recently a closet was one of the focuses of the film "Sex and the City".
Closet organizers.
Closet organizers are fully integrated shelving systems either constructed by contractors or residents. There are a few different types of closet organizers on the market, each with its own advantages and disadvantages:
Types.
Broom closet: A closet with top to buttom space used for storing brooms, mops, vacuums, cleaning supplies, buckets, etc.
Coat closet: A closet located near the front door. Usually used to store coats, jackets, hoodies, sweatshirts, gloves, hats, scarfs, and boots/shoes. This kind of closet does not have shelving. It only has a rod and some button space used for clothes stored in boxes.
Linen closet: A tall, narrow closet with shelves in a bathroom used for storing towels,sheets,washcloths, and toiletries.
Storage closet: A storage closet is any closet that you can use for storage or clothes without using its correct purpose or use.
Uility closet: A utility closet is a closet most commonly used to house appliances and cleaning supplies. Most of the time, you may find a hot water heater and possibly the furnace. The closet may have shelves for storing appliances on top where they are out of the way.
Walk-in closet: A walk-in closet is a closet where someone walks in to store things. They may have lighting, walls, and a floor from other spaces. The walk-in closet can have hinged, bi-fold, or sliding doors.
Wall closet: A wall closet is a closet in a bedroom that is built in to the wall. It may be closed by curtains or folding doors, which clothes can be stored folded on shelves.
Wardrobe: A wardrobe is a small closet used for storing clothes.
Pantry: A pantry is a closet or cabinet in a kitchen used for storing food, dishes, linens, and provisions. The closet may have shelves for putting food on.

</doc>
<doc id="49404" url="http://en.wikipedia.org/wiki?curid=49404" title="Kitchen">
Kitchen

A kitchen is a room or part of a room used for cooking and food preparation. In the West, a modern residential kitchen is typically equipped with a stove, a sink with hot and cold running water, a refrigerator and kitchen cabinets arranged according to a modular design. Many households have a microwave oven, a dishwasher and other electric appliances. The main function of a kitchen is cooking or preparing food but it may also be used for dining, food storage, entertaining, dishwashing, laundry.
History.
The evolution of the kitchen is linked to the invention of the cooking range or stove and the development of water infrastructure capable of supplying water to private homes. Until the 18th century, food was cooked over an open fire. Technical advances in heating food in the 18th and 19th centuries, changed the architecture of the kitchen. Before the advent of modern pipes, water was brought from an outdoor source such as wells, pumps or springs.
Antiquity.
The houses in Ancient Greece were commonly of the atrium-type: the rooms were arranged around a central courtyard for women. In many such homes, a covered but otherwise open patio served as the kitchen. Homes of the wealthy had the kitchen as a separate room, usually next to a bathroom (so that both rooms could be heated by the kitchen fire), both rooms being accessible from the court. In such houses, there was often a separate small storage room in the back of the kitchen used for storing food and kitchen utensils.
In the Roman Empire, common folk in cities often had no kitchen of their own; they did their cooking in large public kitchens. Some had small mobile bronze stoves, on which a fire could be lit for cooking. Wealthy Romans had relatively well-equipped kitchens. In a Roman villa, the kitchen was typically integrated into the main building as a separate room, set apart for practical reasons of smoke and sociological reasons of the kitchen being operated by slaves. The fireplace was typically on the floor, placed at a wall—sometimes raised a little bit—such that one had to kneel to cook. There were no chimneys.
Middle Ages.
Early medieval European longhouses had an open fire under the highest point of the building. The "kitchen area" was between the entrance and the fireplace. In wealthy homes there was typically more than one kitchen. In some homes there were upwards of three kitchens. The kitchens were divided based on the types of food prepared in them. In place of a chimney, these early buildings had a hole in the roof through which some of the smoke could escape. Besides cooking, the fire also served as a source of heat and light to the single-room building. A similar design can be found in the Iroquois longhouses of North America.
In the larger homesteads of European nobles, the kitchen was sometimes in a separate sunken floor building to keep the main building, which served social and official purposes, free from indoor smoke.
The first known stoves in Japan date from about the same time. The earliest findings are from the Kofun period (3rd to 6th century). These stoves, called "kamado", were typically made of clay and mortar; they were fired with wood or charcoal through a hole in the front and had a hole in the top, into which a pot could be hanged by its rim. This type of stove remained in use for centuries to come, with only minor modifications. Like in Europe, the wealthier homes had a separate building which served for cooking. A kind of open fire pit fired with charcoal, called "irori", remained in use as the secondary stove in most homes until the Edo period (17th to 19th century). A "kamado" was used to cook the staple food, for instance rice, while "irori" served both to cook side dishes and as a heat source.
The kitchen remained largely unaffected by architectural advances throughout the Middle Ages; open fire remained the only method of heating food. European medieval kitchens were dark, smoky, and sooty places, whence their name "smoke kitchen". In European medieval cities around the 10th to 12th centuries, the kitchen still used an open fire hearth in the middle of the room. In wealthy homes, the ground floor was often used as a stable while the kitchen was located on the floor above, like the bedroom and the hall. In castles and monasteries, the living and working areas were separated; the kitchen was sometimes moved to a separate building, and thus could not serve anymore to heat the living rooms. In some castles the kitchen was retained in the same structure, but servants were strictly separated from nobles, by constructing separate spiral stone staircases for use of servants to bring food to upper levels. An extant example of such a medieval kitchen with servants' staircase is at Muchalls Castle in Scotland. In Japanese homes, the kitchen started to become a separate room within the main building at that time.
With the advent of the chimney, the hearth moved from the center of the room to one wall, and the first brick-and-mortar hearths were built. The fire was lit on top of the construction; a vault underneath served to store wood. Pots made of iron, bronze, or copper started to replace the pottery used earlier. The temperature was controlled by hanging the pot higher or lower over the fire, or placing it on a trivet or directly on the hot ashes. Using open fire for cooking (and heating) was risky; fires devastating whole cities occurred frequently.
Leonardo da Vinci invented an automated system for a rotating spit for spit-roasting: a propeller in the chimney made the spit turn all by itself. This kind of system was widely used in wealthier homes. Beginning in the late Middle Ages, kitchens in Europe lost their home-heating function even more and were increasingly moved from the living area into a separate room. The living room was now heated by tiled stoves, operated from the kitchen, which offered the huge advantage of not filling the room with smoke.
Freed from smoke and dirt, the living room thus began to serve as an area for social functions and increasingly became a showcase for the owner's wealth. In the upper classes, cooking and the kitchen were the domain of the servants, and the kitchen was set apart from the living rooms, sometimes even far from the dining room. Poorer homes often did not have a separate kitchen yet; they kept the one-room arrangement where all activities took place, or at the most had the kitchen in the entrance hall.
The medieval smoke kitchen (or Farmhouse kitchen) remained common, especially in rural farmhouses and generally in poorer homes, until much later. In a few European farmhouses, the smoke kitchen was in regular use until the middle of the 20th century. These houses often had no chimney, but only a smoke hood above the fireplace, made of wood and covered with clay, used to smoke meat. The smoke rose more or less freely, warming the upstairs rooms and protecting the woodwork from vermin.
Colonial America.
In the Connecticut, as in other colonies of New England during Colonial America, kitchens were often built as separate rooms and were located behind the parlor and keeping room or dining room. One early record of a kitchen is found in the 1648 inventory of the estate of a John Porter of Windsor, Connecticut. The inventory lists goods in the house "over the kittchin" and "in the kittchin". The items listed in the kitchen were: silver spoons, pewter, brass, iron, arms, ammunition, hemp, flax and "other implements about the room". Separate summer kitchens were also common on large farms in the north; these were used to prepare meals for harvest workers and tasks such as canning during the warm summer months.
In the southern states, where the climate and sociological conditions differed from the north, the kitchen was often relegated to an outbuilding, separate from the big house or mansion, for much of the same reasons as in the feudal kitchen in medieval Europe: the kitchen was operated by slaves, and their working place had to be separated from the living area of the masters by the social standards of the time.
Technological advances.
Technological advances during industrialization brought major changes to the kitchen. Iron stoves, which enclosed the fire completely and were more efficient, appeared. Early models included the Franklin stove around 1740, which was a furnace stove intended for heating, not for cooking. Benjamin Thompson in England designed his "Rumford stove" around 1800. This stove was much more energy efficient than earlier stoves; it used one fire to heat several pots, which were hung into holes on top of the stove and were thus heated from all sides instead of just from the bottom. However, his stove was designed for large kitchens; it was too big for domestic use. The "Oberlin stove" was a refinement of the technique that resulted in a size reduction; it was patented in the U.S. in 1834 and became a commercial success with some 90,000 units sold over the next 30 years. These stoves were still fired with wood or coal. Although the first gas street lamps were installed in Paris, London, and Berlin at the beginning of the 1820s and the first U.S. patent on a gas stove was granted in 1825, it was not until the late 19th century that using gas for lighting and cooking became commonplace in urban areas.
Before and after the beginning of the 20th century, kitchens were frequently not equipped with built-in cabinetry, and the lack of storage space in the kitchen became a real problem. The Hoosier Manufacturing Co. of Indiana adapted an existing furniture piece, the baker's cabinet, which had a similar structure of a table top with some cabinets above it (and frequently flour bins beneath) to solve the storage problem. By rearranging the parts and taking advantage of (then) modern metal working, they were able to produce a well-organized, compact cabinet which answered the home cook's needs for storage and working space. A distinctive feature of the Hoosier cabinet is its accessories. As originally supplied, they were equipped with various racks and other hardware to hold and organize spices and various staples. One useful feature was the combination flour-bin/sifter, a tin hopper that could be used without having to remove it from the cabinet. A similar sugar bin was also common.
The urbanization in the second half of the 19th century induced other significant changes that would ultimately change the kitchen. Out of sheer necessity, cities began planning and building water distribution pipes into homes, and built sewers to deal with the waste water. Gas pipes were laid; gas was used first for lighting purposes, but once the network had grown sufficiently, it also became available for heating and cooking on gas stoves. At the turn of the 20th century, electricity had been mastered well enough to become a commercially viable alternative to gas and slowly started replacing the latter. But like the gas stove, the electric stove had a slow start. The first electrical stove had been presented in 1893 at the World's Columbian Exposition in Chicago, but it was not until the 1930s that the technology was stable enough and began to take off.
Industrialization.
Industrialization also caused social changes. The new factory working class in the cities was housed under generally poor conditions. Whole families lived in small one or two-room apartments in tenement buildings up to six stories high, badly aired and with insufficient lighting. Sometimes, they shared apartments with "night sleepers", unmarried men who paid for a bed at night. The kitchen in such an apartment was often used as a living and sleeping room, and even as a bathroom. Water had to be fetched from wells and heated on the stove. Water pipes were laid only towards the end of the 19th century, and then often only with one tap per building or per story. Brick-and-mortar stoves fired with coal remained the norm until well into the second half of the century. Pots and kitchenware were typically stored on open shelves, and parts of the room could be separated from the rest using simple curtains.
In contrast, there were no dramatic changes for the upper classes. The kitchen, located in the basement or the ground floor, continued to be operated by servants. In some houses, water pumps were installed, and some even had kitchen sinks and drains (but no water on tap yet, except for some feudal kitchens in castles). The kitchen became a much cleaner space with the advent of "cooking machines", closed stoves made of iron plates and fired by wood and increasingly charcoal or coal, and that had flue pipes connected to the chimney. For the servants the kitchen continued to also serve as a sleeping room; they slept either on the floor, or later in narrow spaces above a lowered ceiling, for the new stoves with their smoke outlet no longer required a high ceiling in the kitchen. The kitchen floors were tiled; kitchenware was neatly stored in cupboards to protect them from dust and steam. A large table served as a workbench; there were at least as many chairs as there were servants, for the table in the kitchen also doubled as the eating place for the servants.
World War II cooking and dining trends.
The urban middle class imitated the luxurious dining styles of the upper class as best as they could. Living in smaller apartments, the kitchen was the main room—here, the family lived. The study or living room was saved for special occasions such as an occasional dinner invitation. Because of this, these middle-class kitchens were often more homely than those of the upper class, where the kitchen was a work-only room occupied only by the servants. Besides a cupboard to store the kitchenware, there were a table and chairs, where the family would dine, and sometimes—if space allowed—even a fauteuil or a couch.
Gas pipes were first laid in the late 19th century, and gas stoves started to replace the older coal-fired stoves. Gas was more expensive than coal, though, and thus the new technology was first installed in the wealthier homes. Where workers' apartments were equipped with a gas stove, gas distribution would go through a coin meter.
In rural areas, the older technology using coal or wood stoves or even brick-and-mortar open fireplaces remained common throughout. Gas and water pipes were first installed in the big cities; small villages were connected only much later.
Rationalization.
The trend to increasing gasification and electrification continued at the turn of the 20th century. In industry, it was the phase of work process optimization. Taylorism was born, and time-motion studies were used to optimize processes. These ideas also spilled over into domestic kitchen architecture because of a growing trend that called for a professionalization of household work, started in the mid-19th century by Catharine Beecher and amplified by Christine Frederick's publications in the 1910s.
A stepstone was the kitchen designed in Frankfurt by Margarethe Schütte-Lihotzky. Working class women frequently worked in factories to ensure the family's survival, as the men's wages often did not suffice. Social housing projects led to the next milestone: the Frankfurt Kitchen. Developed in 1926, this kitchen measured 1.9 m by 3.4 m (approximately 6 ft 2 inby 11 ft 2 in, with a standard layout). It was built for two purposes: to optimize kitchen work to reduce cooking time and lower the cost of building decently equipped kitchens. The design, created by Margarete Schütte-Lihotzky, was the result of detailed time-motion studies and interviews with future tenants to identify what they needed from their kitchens. Schütte-Lihotzky's fitted kitchen was built in some 10,000 apartments in the housing projects erected in Frankfurt in the 1930s.
The initial reception was critical: it was so small that only one person could work in it; some storage spaces intended for raw loose food ingredients such as flour were reachable by children. But the Frankfurt kitchen embodied a standard for the rest of the 20th century in rental apartments: the "work kitchen". It was criticized as "exiling the women in the kitchen", but post-World War II economic reasons prevailed. The kitchen once more was seen as a work place that needed to be separated from the living areas. Practical reasons also played a role in this development: just as in the bourgeois homes of the past, one reason for separating the kitchen was to keep the steam and smells of cooking out of the living room.
Unit/fitted.
The idea of standardized was first introduced locally with the Frankfurt kitchen, but later defined new in the "Swedish kitchen" (Svensk köksstandard, Swedish kitchen standard). The equipment used remained a standard for years to come: hot and cold water on tap and a kitchen sink and an electrical or gas stove and oven. Not much later, the refrigerator was added as a standard item. The concept was refined in the "Swedish kitchen" using unit furniture with wooden fronts for the kitchen cabinets. Soon, the concept was amended by the use of smooth synthetic door and drawer fronts, first in white, recalling a sense of cleanliness and alluding to sterile lab or hospital settings, but soon after in more lively colors, too. Some years after the Frankfurt Kitchen, Poggenpohl presented the "reform kitchen" in 1928 with interconnecting cabinets and functional interiors. The reform kitchen was a forerunner to the later unit kitchen and fitted kitchen.
Unit construction since its introduction has defined the development of the modern kitchen. Pre-manufactured modules, using mass manufacturing techniques developed during World War II, greatly brought down the cost of a kitchen. Units which are kept on the floor are called "floor units", "floor cabinets", or "base cabinets" on which a kitchen worktop – originally often formica and often now made of granite, marble, tile or wood – is placed. The units which are held on the wall for storage purposes are termed as "wall units" or "wall cabinets". In small areas of kitchen in an apartment, even a "tall storage unit" is available for effective storage. In cheaper brands, all cabinets are kept a uniform color, normally white, with interchangeable doors and accessories chosen by the customer to give a varied look. In more expensive brands, the cabinets are produced matching the doors' colors and finishes, for an older more bespoke look.
Technicalization.
A trend began in the 1940s in the United States to equip the kitchen with electrified small and large kitchen appliances such as blenders, toasters, and later also microwave ovens. Following the end of World War II, massive demand in Europe for low-price, high-tech consumer goods led to Western European kitchens being designed to accommodate new appliances such as refrigerators and electric/gas cookers.
Parallel to this development in tenement buildings was the evolution of the kitchen in homeowner's houses. There, the kitchens usually were somewhat larger, suitable for everyday use as a dining room, but otherwise the ongoing technicalization was the same, and the use of unit furniture also became a standard in this market sector.
In the former Eastern bloc countries, the official doctrine viewed cooking as a mere necessity, and women should work "for the society" in factories, not at home. Also, housing had to be built at low costs and quickly, which led directly to the standardized apartment block using prefabricated slabs. The kitchen was reduced to its minimums and the "work kitchen" paradigm taken to its extremes: in East Germany for instance, each flat in the standard P2 tenement block had a tiny 4 m2 kitchen on the inside of the building (no window), connected to the living room of the 55 m2 apartment and separated from the latter by a pass-through or a window.
Open kitchens.
Starting in the 1980s, the perfection of the extractor hood allowed an open kitchen again, integrated more or less with the living room without causing the whole apartment or house to smell. Before that, only a few earlier experiments, typically in newly built upper-middle-class family homes, had open kitchens. Examples are Frank Lloyd Wright's "House Willey" (1934) and "House Jacobs" (1936). Both had open kitchens, with high ceilings (up to the roof) and were aired by skylights. The extractor hood made it possible to build open kitchens in apartments, too, where both high ceilings and skylights were not possible.
The re-integration of the kitchen and the living area went hand in hand with a change in the perception of cooking: increasingly, cooking was seen as a creative and sometimes social act instead of work. And there was a rejection by younger home-owners of the standard suburban model of separate kitchens and dining rooms found in most 1900-1950 houses. Many families also appreciated the trend towards open kitchens, as it made it easier for the parents to supervise the children while cooking and to clean up spills. The enhanced status of cooking also made the kitchen a prestige object for showing off one's wealth or cooking professionalism. Some architects have capitalized on this "object" aspect of the kitchen by designing freestanding "kitchen objects". However, like their precursor, Colani's "kitchen satellite", such futuristic designs are exceptions.
Another reason for the trend back to open kitchens (and a foundation of the "kitchen object" philosophy) is changes in how food is prepared. Whereas prior to the 1950s most cooking started out with raw ingredients and a meal had to be prepared from scratch, the advent of frozen meals and pre-prepared convenience food changed the cooking habits of many people, who consequently used the kitchen less and less. For others, who followed the "cooking as a social act" trend, the open kitchen had the advantage that they could be with their guests while cooking, and for the "creative cooks" it might even become a stage for their cooking performance.
The "Trophy Kitchen" is equipped with very expensive and sophisticated appliances which are used primarily to impress visitors and to project social status, rather than for actual cooking.
Ventilation.
The ventilation of a kitchen, in particular a large restaurant kitchen, poses certain difficulties that are not present in the ventilation of other kinds of spaces. In particular, the air in a kitchen differs from that of other rooms in that it typically contains grease, smoke and odours.
Materials.
The Frankfurt Kitchen of 1926 was made of several materials depending on the application. The built-in kitchens of today use particle boards or MDF, decorated with veneers, in some cases also wood. Very few manufacturers produce home built-in kitchens from stainless-steel. Until the 1950s, steel kitchens were used by architects, but this material was displaced by the cheaper particle board panels sometimes decorated with a steel surface.
Domestic kitchen planning.
Domestic (or residential) kitchen design "per se" is a relatively recent discipline. The first ideas to optimize the work in the kitchen go back to Catharine Beecher's "A Treatise on Domestic Economy" (1843, revised and republished together with her sister Harriet Beecher Stowe as "The American Woman's Home" in 1869). Beecher's "model kitchen" propagated for the first time a systematic design based on early ergonomics. The design included regular shelves on the walls, ample work space, and dedicated storage areas for various food items. Beecher even separated the functions of preparing food and cooking it altogether by moving the stove into a compartment adjacent to the kitchen.
Christine Frederick published from 1913 a series of articles on "New Household Management" in which she analyzed the kitchen following Taylorist principles, presented detailed time-motion studies, and derived a kitchen design from them. Her ideas were taken up in the 1920s by architects in Germany and Austria, most notably Bruno Taut,Erna Meyer, and Margarete Schütte-Lihotzky. A social housing project in Frankfurt (the "Römerstadt" of architect Ernst May) realized in 1927/8 was the breakthrough for her Frankfurt kitchen, which embodied this new notion of efficiency in the kitchen.
While this "work kitchen" and variants derived from it were a great success for tenement buildings, home owners had different demands and did not want to be constrained by a 6.4 m² kitchen. Nevertheless, kitchen design was mostly ad-hoc following the whims of the architect. In the U.S., the "Small Homes Council", since 1993 the "Building Research Council", of the School of Architecture of the University of Illinois at Urbana-Champaign was founded in 1944 with the goal to improve the state of the art in home building, originally with an emphasis on standardization for cost reduction. It was there that the notion of the "kitchen work triangle" was formalized: the three main functions in a kitchen are storage, preparation, and cooking (which Catharine Beecher had already recognized), and the places for these functions should be arranged in the kitchen in such a way that work at one place does not interfere with work at another place, the distance between these places is not unnecessarily large, and no obstacles are in the way. A natural arrangement is a triangle, with the refrigerator, the sink, and the stove at a vertex each.
This observation led to a few common kitchen forms, commonly characterized by the arrangement of the kitchen cabinets and sink, stove, and refrigerator:
In the 1980s, there was a backlash against industrial kitchen planning and cabinets with people installing a mix of work surfaces and free standing furniture, led by kitchen designer Johnny Grey and his concept of the "Unfitted Kitchen".
Modern kitchens often have enough informal space to allow for people to eat in it without having to use the formal dining room. Such areas are called "breakfast areas", "breakfast nooks" or "breakfast bars" if the space is integrated into a kitchen counter. Kitchens with enough space to eat in are sometimes called "eat-in kitchens".
Other kitchen types.
Restaurant and canteen kitchens found in hotels, hospitals, educational and work place facilities, army barracks, and similar establishments are generally (in developed countries) subject to public health laws. They are inspected periodically by public-health officials, and forced to close if they do not meet hygienic requirements mandated by law.
Canteen kitchens (and castle kitchens) were often the places where new technology was used first. For instance, Benjamin Thompson's "energy saving stove", an early-19th century fully closed iron stove using one fire to heat several pots, was designed for large kitchens; another thirty years passed before they were adapted for domestic use.
Today's western restaurant kitchens typically have tiled walls and floors and use stainless steel for other surfaces (workbench, but also door and drawer fronts) because these materials are durable and easy to clean. Professional kitchens are often equipped with gas stoves, as these allow cooks to regulate the heat more quickly and more finely than electrical stoves. Some special appliances are typical for professional kitchens, such as large installed deep fryers, steamers, or a bain-marie. (s of 2004[ [update]], steamers — not to be confused with a pressure cooker — are beginning to find their way into domestic households, sometimes as a combined appliance of oven and steamer.)
The fast food and convenience food trends have also changed the way restaurant kitchens operate. There's a trend for restaurants to only "finish" delivered convenience food or even just re-heat completely prepared meals, maybe at the utmost grilling, a hamburger, or a steak.
The kitchens in railway dining cars present special challenges: space is constrained, and, nevertheless, the personnel must be able to serve a great number of meals quickly. Especially in the early history of railways this required flawless organization of processes; in modern times, the microwave oven and prepared meals have made this task much easier. Galleys are kitchens aboard ships or aircraft (although the term "galley" is also often used to refer to a railroad dining car's kitchen). On yachts, galleys are often cramped, with one or two burners fueled by an LP gas bottle, but kitchens on cruise ships or large warships are comparable in every respect with restaurants or canteen kitchens. On passenger airliners, the kitchen is reduced to a mere pantry, the only function reminiscent of a kitchen is the heating of in-flight meals delivered by a catering company. An extreme form of the kitchen occurs in space, "e.g.", aboard a Space Shuttle (where it is also called the "galley") or the International Space Station. The astronauts' food is generally completely prepared, dehydrated, and sealed in plastic pouches, and the kitchen is reduced to a rehydration and heating module.
Outdoor areas in which food is prepared are generally not considered to be kitchens, even though an outdoor area set up for regular food preparation, for instance when camping, might be called an "outdoor kitchen". Military camps and similar temporary settlements of nomads may have dedicated kitchen tents.
In schools where home economics, food technology (previously known as "domestic science"), or culinary arts are taught, there will be a series of kitchens with multiple equipment (similar in some respects to laboratories) solely for the purpose of teaching. These will consist of multiple workstations, each with their own oven, sink, and kitchen utensils.
Belfast sinks are making a bit of a resurgence and can look really stylish in a new kitchen. Many people may remember these sinks from their childhood and could not wait to get rid of them as they looked unsightly things at the time, sticking out like sore thumbs, but not any more. There are specially designed units used to house these types of sinks now that help them to blend in with the overall fitted kitchen design.
Kitchen types by region.
China.
Kitchens in China are called （厨房,膳所）. More than 3000 years ago, the ancient Chinese used (ding 鼎） for cooking food. The ding was developed into the wok and pot used today. The earthenware pot, which was commonly used by most people, (砂锅）sand pot is still used in all kinds of cooking or medicine brewing. 
Many Chinese people believe that there is a kitchen god (灶神) who watches over the kitchen for the family. The kitchen god returns to heaven to give a report to the Jade Emperor (玉皇大帝) annually about this family behavior. Every Chinese New Year Eve, families will gather together to pray for the kitchen god to give a good report to heaven and wish him to bring back good news on the fifth day of the New Year. 
The most common cooking equipment in Chinese family kitchens and restaurant kitchens are woks, steamer basket, and pots. The fuel or heating resource was also important technique to practice the cooking skills. Traditionally Chinese were using wood or straw as the fuel to cook food. A Chinese chef had to master flaming and heat radiation to reliably prepare traditional recipes. Chinese cooking will use a pot or wok for pan frying, stir frying, deep frying or boiling.
Japan.
Kitchens in Japan are called Daidokoro (台所; lit. "kitchen"). Daidokoro is the place where food is prepared in a Japanese house. Until the Meiji era, a kitchen was also called "kamado" (かまど; lit. stove) and there are many sayings in the Japanese language that involve kamado as it was considered the symbol of a house and the term could even be used to mean "family" or "household" (similar to the English word "hearth"). When separating a family, it was called "Kamado wo wakeru", which means "divide the stove". "Kamado wo yaburu" (lit. "break the stove") means that the family was bankrupt.
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="49407" url="http://en.wikipedia.org/wiki?curid=49407" title="Pliny the Younger">
Pliny the Younger

Gaius Plinius Caecilius Secundus, born Gaius Caecilius or Gaius Caecilius Cilo (61 – c. 113), better known as Pliny the Younger (), was a lawyer, author, and magistrate of Ancient Rome. Pliny's uncle, Pliny the Elder, helped raise and educate him. Both Pliny the Elder and Younger were witnesses to the eruption of Vesuvius on August 24, 79 AD, during which the former died.
Pliny wrote hundreds of letters, many of which still survive, that are of great historical value for the time period. Some are addressed to reigning emperors or to notables such as the historian Tacitus. Pliny served as an imperial magistrate under Trajan (reigned 98–117), and his letters to Trajan provide one of the few surviving records of the relationship between the imperial office and provincial governors.
Pliny was considered an honest and moderate man. He rose through a series of Imperial civil and military offices, the "cursus honorum". He was a friend of the historian Tacitus and employed the biographer Suetonius on his staff. Pliny also came into contact with other well-known men of the period, including the philosophers Artemidorus and Euphrates the Stoic during his time in Syria.
Background.
Childhood.
Pliny the Younger was born in "Novum Comum" (Como, Northern Italy) around 61 AD, the son of Lucius Caecilius Cilo, born there, and wife Plinia Marcella, a sister of Pliny the Elder. He was the grandson of Senator and landowner Gaius Caecilius. He revered his uncle, Pliny the Elder (who at this time was extremely famous around the Roman Empire), and provides sketches of how his uncle worked on the "Naturalis Historia".
Pliny's father died at an early age when Pliny was still young. As a result, Pliny probably lived with his mother. His guardian and preceptor in charge of his education was Lucius Verginius Rufus, famed for quelling a revolt against Nero in 68 AD.
After being first tutored at home, Pliny went to Rome for further education. There he was taught rhetoric by Quintilian, a great teacher and author, and Nicetes Sacerdos of Smyrna. It was at this time that Pliny became closer to his uncle Pliny the Elder. When Pliny the Younger was 18, his uncle Pliny the Elder died attempting to rescue victims of the Vesuvius eruption, and the terms of the Elder Pliny's will passed his estate to his nephew. In the same document the younger Pliny was adopted by his uncle. As a result, Pliny the Younger changed his name from "Gaius Caecilius Cilo" to "Gaius Plinius Caecilius Secundus" (his official title was "Gaius Plinius Luci filius Caecilius Secundus").
There is some evidence that Pliny had a sibling. Although Pliny the Younger uses "Secundus" as part of his name, this does not mean he is the second son: adopted sons took over the name of their adoption father.
The word "contubernalis" describing Lutulla is the military term meaning "tent-mate", which can only mean that she was living with Lucius, not as his wife. The first man mentioned, L. Caecilius Valens, is probably the older son. Pliny the Younger confirms that he was a trustee for the largess "of my ancestors". It seems unknown to Pliny the Elder, so Valens' mother was probably not his sister Plinia; perhaps Valens was Lutulla's son from an earlier relationship.
Adult life.
Pliny the Younger married three times, firstly when he was very young, about eighteen, to a stepdaughter of Veccius Proculus, of whom he became a widower at age 37; secondly to the daughter of Pompeia Celerina, at an unknown date; and thirdly to Calpurnia, daughter of Calpurnius and granddaughter of Calpurnius Fabatus of Comum. Letters survive in which Pliny records this latter marriage taking place, as well as his attachment to Calpurnia and his sadness when she miscarries their child.
Pliny is thought to have died suddenly during his appointment in Bithynia-Pontus, around 113 AD, since no events referred to in his letters date later than that.
Career.
Pliny was by birth of equestrian rank, that is, a member of the aristocratic order of "equites" (knights), the lower (beneath the senatorial order) of the two Roman aristocratic orders that monopolised senior civil and military offices during the early Empire. His career began at the age of eighteen and initially followed a normal equestrian route. But, unlike most equestrians, he achieved entry into the upper order by being elected Quaestor in his late twenties. (See Career summary below.)
Pliny was active in the Roman legal system, especially in the sphere of the Roman centumviral court, which dealt with inheritance cases. Later, he was a well-known prosecutor and defender at the trials of a series of provincial governors, including Baebius Massa, governor of Baetica, Marius Priscus, the governor of Africa, Gaius Caecilius Classicus, governor of Baetica and most ironically in light of his later appointment to this province, Gaius Julius Bassus and Varenus Rufus, both governors of Bithynia-Pontus.
Pliny's career is commonly considered as a summary of the main Roman public charges and is the best-documented example from this period, offering proof for many aspects of imperial culture. Effectively, Pliny crossed all the principal fields of the organization of the early Roman Empire. It is an achievement for a man to have not only survived the reigns of several disparate emperors, especially the much-detested Domitian, but also to have risen in rank throughout.
Writings.
As an author, Pliny started writing at the age of 14, penning a tragedy in Greek. In the course of his life he wrote a quantity of poetry, most of which is lost. Also known as a notable "orator", he professed himself a follower of Cicero, but his prose was certainly more magniloquent and less direct than Cicero's. The only oration that now survives is the "Panegyricus Traiani". This was pronounced in the Senate in 100 and is a description of Trajan's figure and actions in an adulatory and emphatic form, especially contrasting him with the Emperor Domitian. It is, however, a relevant document that allows us to know many details about the Emperor's actions in several fields of his administrative power such as taxes, justice, military discipline, and commerce. Recalling the speech in one of his letters, Pliny shrewdly defines his own motives thus:
I hoped in the first place to encourage our Emperor in his virtues by a sincere tribute and, secondly, to show his successors what path to follow to win the same renown, not by offering instruction but by setting his example before them. To proffer advice on an Emperor's duties might be a noble enterprise, but it would be a heavy responsibility verging on insolence, whereas to praise an excellent ruler ("optimum principem") and thereby shine a beacon on the path posterity should follow would be equally effective without appearing presumptuous.
"Epistulae".
The largest surviving body of Pliny's work is his "Epistulae" ("Letters"), a series of personal missives directed to his friends and associates. These letters are a unique testimony of Roman administrative history and everyday life in the 1st century AD. Especially noteworthy among the letters are two in which he describes the eruption of Mount Vesuvius in August 79, during which his uncle Pliny the Elder died ("Epistulae" VI.16, VI.20), and one in which he asks the Emperor for instructions regarding official policy concerning Christians ("Epistulae" X.96).
Epistles concerning the eruption of Mount Vesuvius.
The two Letters describing the eruption of Mount Vesuvius were written by Pliny approximately 25 years after the event, and both were sent in response to the request of his friend the historian Tacitus, who wanted to know more about Pliny the Elder's death. The two letters have great historical value due to the accurate description of Vesuvius' eruption: Pliny's attention to detail in the letters about Vesuvius is so keen that modern volcanologists describe that type as Plinian eruptions.
Epistle concerning the Christian Religion.
As the Roman governor of Bithynia-Pontus (now in modern Turkey) Pliny wrote a letter to Emperor Trajan around 112 AD and asked for counsel on dealing with Christians. In the letter ("Epistulae" X.96) Pliny detailed an account of how he conducted trials of suspected Christians who appeared before him as a result of anonymous accusations and asked for the Emperor's guidance on how they should be treated. Pliny had never performed a legal investigation of Christians, and thus consulted Trajan in order to be on solid ground regarding his actions, and saved his letters and Trajan's replies. Neither Pliny nor Trajan mention the crime that Christians had committed, except for being a Christian; Trajan's response to Pliny makes it clear that being known as a "Christian" was sufficient for judicial action. The correspondence between Pliny and Emperor Trajan shows that the Roman Empire, as a government entity, did not at this time “seek out” Christians for prosecution or persecution. Pliny's letter is the earliest surviving Roman document to refer to early Christians.
Manuscripts.
In France Giovanni Giocondo discovered a manuscript of Pliny the Younger's letters containing his correspondence with Trajan. He published it in Paris dedicating the work to Louis XII. Two Italian editions of Pliny's Epistles were published by Giocondo, one printed in Bologna in 1498 and one from the press of Aldus Manutius in 1508.
Villas.
Pliny loved villas, and, being wealthy, owned many, such as the one in Lake Como named "Tragedy" because of its situation high on a hill. Another, on the shore of the lake, was named "Comedy" because it was sited low down.
Pliny's main estate in Italy was in the north of Umbria, under the passes of Bocca Trabaria and Bocca Serriola, where wood was cut for Roman ships and sent to Rome via the Tiber.

</doc>
<doc id="49414" url="http://en.wikipedia.org/wiki?curid=49414" title="Sex-determination system">
Sex-determination system

A sex-determination system is a biological system that determines the development of sexual characteristics in an organism. Most sexual organisms have two sexes. Occasionally, there are hermaphrodites in place of one or both sexes. There are also some species that are only one sex due to parthenogenesis, the act of a female reproducing without fertilization.
In many species, sex determination is genetic: males and females have different alleles or even different genes that specify their sexual morphology. In animals this is often accompanied by chromosomal differences, generally through combinations of XY, ZW, XO, ZO chromosomes, or haplodiploidy. The sexual differentiation is generally triggered by a main gene (a "sex locus"), with a multitude of other genes following in a domino effect.
In other cases, sex is determined by environmental variables (such as temperature) or social variables (e.g. the size of an organism relative to other members of its population). Environmental sex determination preceded the genetically determined systems of birds and mammals; it is thought that a temperature-dependent amniote was the common ancestor of amniotes with sex chromosomes.
Some species do not have a fixed sex, and instead change sex based on certain cues. The details of some sex-determination systems are not yet fully understood.
Chromosomal determination.
XX/XY sex chromosomes.
The XX/XY sex-determination system is the most familiar, as it is found in humans. In the system, females have two of the same kind of sex chromosome (XX), while males have two distinct sex chromosomes (XY). The XY sex chromosomes are different in shape and size from each other, unlike the autosomes, and are termed allosomes. Some species (including humans) have a gene SRY on the Y chromosome that determines maleness; others (such as the fruit fly) use the presence of two X chromosomes to determine femaleness. Because the fruit fly, as well as other species, use the number of Xs to determine sex, they are nonviable with an extra X. SRY-reliant species can have conditions such as XXY and still live. Human sex is determined by containing SRY or not. Once SRY is activated, cells create testosterone and anti-müllerian hormone to turn the genderless sex organs into male. With females, their cells excrete estrogen, driving the body down the female pathway. Not all organisms remain gender indifferent for a time after they're created; for example, fruit flies differentiate into specific sexes as soon as the egg is fertilized. In Y-centered sex determination, the SRY gene is not the only gene to have an influence on sex. Despite the fact that SRY seems to be the main gene in determining male characteristics, it requires the action of multiple genes to develop testes. In XY mice, lack of the gene DAX1 on the X chromosome results in sterility, but in humans it causes adrenal hypoplasia congenita. However, when an extra DAX1 gene is placed on the X, the result is a female, despite the existence of SRY. Also, even when there are normal sex chromosomes in XX females, duplication or expression of SOX9 causes testes to develop. Gradual sex reversal in developed mice can also occur when the gene FOXL2 is removed from females. Even though the gene DMRT1 is used by birds as their sex locus, species who have XY chromosomes also rely upon DMRT1, contained on chromosome 9, for sexual differentiation at some point in their formation.
The XX/XY system is also found in most other mammals, as well as some insects. Some fish also have variants of this, as well as the regular system. For example, while it has an XY format, Xiphophorus nezahualcoyotl and X. milleri also have a second Y chromosome, known as Y', that creates XY' females and YY' males. At least one monotreme, the platypus, presents a particular sex determination scheme that in some ways resembles that of the ZW sex chromosomes of birds, and also lacks the SRY gene, whereas some rodents, such as several Arvicolinae (voles and lemmings), are also noted for their unusual sex determination systems. The platypus has ten sex chromosomes; males have an XYXYXYXYXY pattern while females have ten X chromosomes. Although it is an XY system, the platypus' sex chromosomes share no homologues with eutherian sex chromosomes. Instead, homologues with eutherian sex chromosomes lie on the platypus chromosome 6, which means that the eutherian sex chromosomes were autosomes at the time that the monotremes diverged from the therian mammals (marsupials and eutherian mammals). However, homologues to the avian DMRT1 gene on platypus sex chromosomes X3 and X5 suggest that it is possible the sex-determining gene for the platypus is the same one that is involved in bird sex-determination. More research must be conducted in order to determine the exact sex determining gene of the platypus.
XX/X0 sex determination.
In this variant of the XY system, females have two copies of the sex chromosome (XX) but males have only one (X0). The "0" denotes the absence of a second sex chromosome. Generally in this method, the sex is determined by amount of genes expressed across the two chromosomes. This system is observed in a number of insects, including the grasshoppers and crickets of order Orthoptera and in cockroaches (order Blattodea). A small number of mammals also lack a Y chromosome. These include the Amami spiny rat ("Tokudaia osimensis") and the Tokunoshima spiny rat ("Tokudaia tokunoshimensis") and "Sorex araneus", a shrew species. Transcaucasian mole voles ("Ellobius lutescens") also have a form of XO determination, in which both genders lack a second sex chromosome. The mechanism of sex determination is not yet understood.
The nematode "C. elegans" is male with one sex chromosome (X0); with a pair of chromosomes (XX) it is a hermaphrodite. Its main sex gene is XOL, which encodes XOL-1 and also controls the expression of the genes TRA-2 and HER-1. These genes reduce male gene activation and increase it, respectively.
ZW sex chromosomes.
The ZW sex-determination system is found in birds, some reptiles, and some insects and other organisms. The ZW sex-determination system is reversed compared to the XY system: females have two different kinds of chromosomes (ZW), and males have two of the same kind of chromosomes (ZZ). In the chicken, this was found to be dependent on the expression of DMRT1. In birds, the genes FET1 and ASW are found on the W chromosome for females, similar to how the Y chromosome contains SRY. However, not all species depend upon the W for their sex. For example, there are moths and butterflies that are ZW, but some have been found female with ZO, as well as female with ZZW. Also, while mammals inactivate one of their extra X chromosomes when female, it appears that in the case of Lepidoptera, the males produce double the normal amount of enzymes, due to having two Z's. Because the use of ZW sex determination is varied, it is still unknown how exactly most species determine their sex. However, reportedly, the silkworm "Bombyx mori" uses a single female-specific piRNA as the primary determiner of sex. Despite the similarities between ZW and XY, the sex chromosomes do not line up correctly and evolved separately. In the case of the chicken, their Z chromosome is more similar to humans' autosome 9. The chicken's Z chromosome also seems to be related to the X chromosomes of the platypus. When a ZW species, such as the Komodo Dragon, reproduce parthenogenetically, usually only males are produced. This is due to the fact that the haploid eggs double their chromosomes, resulting in ZZ or WW. The ZZ become males, but the WW are not viable and are not brought to term.
UV sex chromosomes.
In some Bryophyte and some algae species, the gametophyte stage of the life cycle, rather than being hermaphrodite, occurs as separate male or female individuals that produce male and female gametes respectively. When meiosis occurs in the sporophyte generation of the life cycle, the sex chromosomes known as U and V assort in spores that carry either the U chromosome and give rise to female gametophytes, or the V chromosome and give rise to male gametophytes.
Haplodiploidy.
Haplodiploidy is found in insects belonging to Hymenoptera, such as ants and bees. Unfertilized eggs develop into haploid individuals, which are the males. Diploid individuals are generally female but may be sterile males. Males cannot have sons or fathers. If a queen bee mates with one drone, her daughters share ¾ of their genes with each other, not ½ as in the XY and ZW systems. This is believed to be significant for the development of eusociality, as it increases the significance of kin selection, but it is debated. Most females in the Hymenoptera order can decide the sex of their offspring by holding received sperm in their spermatheca and either releasing it into their oviduct or not. This allows them to create more workers, depending on the status of the colony.
Non-genetic sex-determination systems.
Temperature-dependent sex determination.
Many other sex-determination systems exist. In some species of reptiles, including alligators, some turtles, and the tuatara, sex is determined by the temperature at which the egg is incubated during a temperature-sensitive period. There are no examples of temperature-dependent sex determination (TSD) in birds. Megapodes had formerly been thought to exhibit this phenomenon, but actually exhibit temperature-dependent embryo mortality. For some species with TSD, sex determination is achieved by exposure to hotter temperatures resulting in the offspring being one sex and cooler temperatures resulting in the other. For others species using TSD, it is exposure to temperatures on both extremes that results in offspring of one sex, and exposure to moderate temperatures that results in offspring of the opposite sex. These systems are known as Pattern I and Pattern II, respectively. The specific temperatures required to produce each sex are known as the female-promoting temperature and the male-promoting temperature. When the temperature stays near the threshold during the temperature sensitive period, the sex ratio is varied between the two sexes. Some species' temperature standards are based on when a particular enzyme is created. These species that rely upon temperature for their sex determination do not have the SRY gene, but have other genes such as DAX1, DMRT1, and SOX9 that are expressed or not expressed depending on the temperature. The sex of some species, such as the Nile Tilapia, Australian skink lizard, and Australian dragon lizard, is initially determined by chromosomes, but can later be changed by the temperature of incubation. 
It is unknown how exactly temperature-dependent sex determination evolved. It could have evolved through certain sexes being more suited to certain areas that fit the temperature requirements. For example, a warmer area could be more suitable for nesting, so more females are produced to increase the amount that nest next season.
Other sex-determination systems.
Although temperature-dependent sex determination is relatively common, there are many other environmental systems. Some species, such as some snails, practice sex change: adults start out male, then become female (See also sex reversal). In tropical clown fish, the dominant individual in a group becomes female while the other ones are male, and bluehead wrasses ("Thalassoma bifasciatum") are the reverse. In the marine worm ("Bonellia viridis"), larvae become males if they make physical contact with a female, and females if they end up on the bare sea floor. This is triggered by the presence of a chemical produced by the females, bonellin. Some species, however, have no sex-determination system. Hermaphrodite species include the common earthworm and certain species of snails. A few species of fish, reptiles, and insects reproduce by parthenogenesis and are female altogether. There are some reptiles, such as the boa constrictor and komodo dragon that can reproduce both sexually and asexually, depending on whether a mate is available.
Other unusual systems:
Evolution of sex-determination systems.
Origin of sex chromosomes.
The accepted hypothesis of XY and ZW sex chromosome evolution is that they evolved at the same time, in two different branches. However, there is some evidence to suggest that there could have been transitions between ZW and XY, such as in "Xiphophorus maculatus", which have both ZW and XY systems in the same population, despite the fact that ZW and XY have different gene locations. A recent theoretical model raises the possibility of both transitions between the XY/XX and ZZ/ZW system and environmental sex determination The platypus' genes also back up the possible evolutionary link between XY and ZW, because they have the DMRT1 gene possessed by birds on their X chromosomes. Regardless, XY and ZW follow a similar route. All sex chromosomes started out as an original autosome of an original amniote that relied upon temperature to determine the sex of offspring. After the mammals separated, the branch further split into Lepidosauria and Archosauromorpha. These two groups both evolved the ZW system separately, as evidenced by the existence of different sex chromosomal locations. In mammals, one of the autosome pair, now Y, mutated its SOX3 gene into the SRY gene, causing that chromosome to designate sex. After this mutation, the SRY-containing chromosome inverted and was no longer completely homologous with its partner. The regions of the X and Y chromosomes that are still homologous to one another are known as the pseudoautosomal region. Once it inverted, the Y chromosome became unable to remedy deleterious mutations, and thus degenerated. There is some concern that the Y chromosome will shrink further and stop functioning in 10 million years, but other evidence has shown that the Y chromosome has been strictly conserved after its initial rapid gene loss.
There are some species, such as the medaka fish, that evolved sex chromosomes separately; their Y chromosome never inverted and can still swap genes with the X. These species are still in an early phase of evolution with regard to their sex chromosomes. Because the Y does not have male-specific genes and can interact with the X, XY and YY females can be formed as well as XX males.

</doc>
<doc id="49416" url="http://en.wikipedia.org/wiki?curid=49416" title="NMOS logic">
NMOS logic

N-type metal-oxide-semiconductor logic uses n-type metal-oxide-semiconductor field effect transistors (MOSFETs) to implement logic gates and other digital circuits. NMOS transistors have four modes of operation: cut-off (or sub-threshold), triode, saturation (sometimes called active), and velocity saturation.
The n-type MOSFETs are arranged in a so-called "pull-down network" (PDN) between the logic gate output and negative supply voltage, while a resistor is placed between the logic gate output and the positive supply voltage. The circuit is designed such that if the desired output is low, then the PDN will be active, creating a current path between the negative supply and the output.
As an example, here is a NOR gate in NMOS logic. If either input A or input B is high (logic 1, = True), the respective MOS transistor acts as a very low resistance between the output and the negative supply, forcing the output to be low (logic 0, = False). When both A and B are high, both transistors are conductive, creating an even lower resistance path to ground. The only case where the output is high is when both transistors are off, which occurs only when both A and B are low, thus satisfying the truth table of a NOR gate:
A MOSFET can be made to operate as a resistor, so the whole circuit can be made with n-channel MOSFETs only. For many years, this made NMOS circuits much faster than comparable PMOS and CMOS circuits, which had to use much slower p-channel transistors. It was also easier to manufacture NMOS than CMOS, as the latter has to implement p-channel transistors in special n-wells on the p-substrate. The major problem with NMOS (and most other logic families) is that a DC current must flow through a logic gate even when the output is in a steady state (low in the case of NMOS). This means static power dissipation, i.e. power drain even when the circuit is not switching. This is a similar situation to the modern high speed, high density CMOS circuits (microprocessors etc.) which also has significant static current draw, although this is due to leakage, not bias. However, older and/or slower static CMOS circuits used for ASICs, SRAM etc., typically have very low static power consumption.
Also, NMOS circuits are slow to transition from low to high. When transitioning from high to low, the transistors provide low resistance, and the capacitative charge at the output drains away very quickly (similar to discharging a capacitor through a very low resistor). But the resistance between the output and the positive supply rail is much greater, so the low to high transition takes longer (similar to charging a capacitor through a high value resistor). Using a resistor of lower value will speed up the process but also increases static power dissipation. However, a better (and the most common) way to make the gates faster is to use depletion-mode transistors instead of enhancement-mode transistors as loads. This is called depletion-load NMOS logic.
Additionally, just like in DTL, TTL and ECL etc., the asymmetric input logic levels make NMOS circuits somewhat susceptible to noise. These disadvantages are why the CMOS logic now has supplanted most of these types in most high-speed digital circuits such as microprocessors (despite the fact that CMOS was originally very slow compared to logic gates built with bipolar transistors).

</doc>
<doc id="49417" url="http://en.wikipedia.org/wiki?curid=49417" title="Extinction">
Extinction

In biology and ecology, extinction is the end of an organism or of a group of organisms (taxon), normally a species. The moment of extinction is generally considered to be the death of the last individual of the species, although the capacity to breed and recover may have been lost before this point. Because a species' potential range may be very large, determining this moment is difficult, and is usually done retrospectively. This difficulty leads to phenomena such as Lazarus taxa, where a species presumed extinct abruptly "reappears" (typically in the fossil record) after a period of apparent absence.
Although more than ninety-nine per cent of all species that ever lived on the planet are estimated to be extinct, there are currently 10–14 million species of life on Earth. Through evolution, new species arise through the process of speciation—where new varieties of organisms arise and thrive when they are able to find and exploit an ecological niche—and species become extinct when they are no longer able to survive in changing conditions or against superior competition. The relationship between animals and their ecological niches has been firmly established. A typical species becomes extinct within 10 million years of its first appearance, although some species, called living fossils, survive with virtually no morphological change for hundreds of millions of years. Most extinctions have occurred naturally, prior to "Homo sapiens" walking on Earth: it is estimated that 99.9% of all species that have ever existed are now extinct.
Mass extinctions are relatively rare events; however, isolated extinctions are quite common. Only recently have extinctions been recorded and scientists have become alarmed at the current high rate of extinctions. Most species that become extinct are never scientifically documented. Some scientists estimate that up to half of presently existing plant and animal species may become extinct by 2100. It is difficult to estimate the trajectory that biodiversity might have taken without human impact but scientists at the University of Bristol estimate that biodiversity might increase exponentially without human influence.
Definition.
A species is extinct when the last existing member dies. Extinction therefore becomes a certainty when there are no surviving individuals that can reproduce and create a new generation. A species may become functionally extinct when only a handful of individuals survive, which cannot reproduce due to poor health, age, sparse distribution over a large range, a lack of individuals of both sexes (in sexually reproducing species), or other reasons.
Pinpointing the extinction (or pseudoextinction) of a species requires a clear definition of that species. If it is to be declared extinct, the species in question must be uniquely distinguishable from any ancestor or daughter species, and from any other closely related species. Extinction of a species (or replacement by a daughter species) plays a key role in the punctuated equilibrium hypothesis of Stephen Jay Gould and Niles Eldredge.
In ecology, "extinction" is often used informally to refer to local extinction, in which a species ceases to exist in the chosen area of study, but may still exist elsewhere. This phenomenon is also known as extirpation. Local extinctions may be followed by a replacement of the species taken from other locations; wolf reintroduction is an example of this. Species which are not extinct are termed extant. Those that are extant but threatened by extinction are referred to as threatened or endangered species.
Currently an important aspect of extinction is human attempts to preserve critically endangered species. These are reflected by the creation of the conservation status "extinct in the wild" (EW). Species listed under this status by the International Union for Conservation of Nature (IUCN) are not known to have any living specimens in the wild, and are maintained only in zoos or other artificial environments. Some of these species are functionally extinct, as they are no longer part of their natural habitat and it is unlikely the species will ever be restored to the wild. When possible, modern zoological institutions try to maintain a viable population for species preservation and possible future reintroduction to the wild, through use of carefully planned breeding programs.
The extinction of one species' wild population can have knock-on effects, causing further extinctions. These are also called "chains of extinction". This is especially common with extinction of keystone species.
Pseudoextinction.
Descendants may or may not exist for extinct species. Daughter species that evolve from a parent species carry on most of the parent species' genetic information, and even though the parent species may become extinct, the daughter species lives on. Extinction of a parent species where daughter species or subspecies are still extant is called pseudoextinction. In other cases, species have produced no new variants, or none that are able to survive the parent species' extinction.
Pseudoextinction is difficult to demonstrate unless one has a strong chain of evidence linking a living species to members of a pre-existing species. For example, it is sometimes claimed that the extinct "Hyracotherium", which was an early horse that shares a common ancestor with the modern horse, is pseudoextinct, rather than extinct, because there are several extant species of "Equus", including zebra and donkey. However, as fossil species typically leave no genetic material behind, one cannot say whether "Hyracotherium" evolved into more modern horse species or merely evolved from a common ancestor with modern horses. Pseudoextinction is much easier to demonstrate for larger taxonomic groups.
Lazarus taxa.
The coelacanth, a fish related to lungfish and tetrapods, was considered to have been extinct since the end of the Cretaceous Period until 1938 when a specimen was found, off the Chalumna River (now Tyolomnqa) on the east coast of South Africa. Museum curator Marjorie Courtenay-Latimer discovered the fish among the catch of a local angler, Captain Hendrick Goosen, on December 23, 1938. A local chemistry professor, JLB Smith, confirmed the fish's importance with a famous cable: "MOST IMPORTANT PRESERVE SKELETON AND GILLS = FISH DESCRIBED".
Far more recent possible or presumed extinctions of species which may turn out still to exist include the thylacine, or Tasmanian tiger ("Thylacinus cynocephalus"), the last known example of which died in Hobart Zoo in Tasmania in 1936; the Japanese wolf ("Canis lupus hodophilax"), last sighted over 100 years ago; the ivory-billed woodpecker ("Campephilus principalis"), last sighted for certain in 1944; and the slender-billed curlew ("Numenius tenuirostris"), not seen since 2007.
Causes.
As long as species have been evolving, species have been going extinct. It is estimated that over 99.9% of all species that ever lived are extinct. The average life-span of a species is 10 million years, although this varies widely between taxa.
There are a variety of causes that can contribute directly or indirectly to the extinction of a species or group of species. "Just as each species is unique", write Beverly and Stephen C. Stearns, "so is each extinction ... the causes for each are varied—some subtle and complex, others obvious and simple". Most simply, any species that cannot survive and reproduce in its environment and cannot move to a new environment where it can do so, dies out and becomes extinct. Extinction of a species may come suddenly when an otherwise healthy species is wiped out completely, as when toxic pollution renders its entire habitat unliveable; or may occur gradually over thousands or millions of years, such as when a species gradually loses out in competition for food to better adapted competitors. Extinction may occur a long time after the events that set it in motion, a phenomenon known as extinction debt.
Assessing the relative importance of genetic factors compared to environmental ones as the causes of extinction has been compared to the debate on nature and nurture. The question of whether more extinctions in the fossil record have been caused by evolution or by catastrophe is a subject of discussion; Mark Newman, the author of "Modeling Extinction", argues for a mathematical model that falls between the two positions. By contrast, conservation biology uses the extinction vortex model to classify extinctions by cause. When concerns about human extinction have been raised, for example in Sir Martin Rees' 2003 book "Our Final Hour", those concerns lie with the effects of climate change or technological disaster.
Currently, environmental groups and some governments are concerned with the extinction of species caused by humanity, and they try to prevent further extinctions through a variety of conservation programs. Humans can cause extinction of a species through overharvesting, pollution, habitat destruction, introduction of invasive species (such as new predators and food competitors), overhunting, and other influences. Explosive, unsustainable human population growth is an essential cause of the extinction crisis. According to the International Union for Conservation of Nature (IUCN), 784 extinctions have been recorded since the year 1500, the arbitrary date selected to define "recent" extinctions, up to the year 2004; with many more likely to have gone unnoticed. Several species have also been listed as extinct since 2004.
Genetics and demographic phenomena.
Population genetics and demographic phenomena affect the evolution, and therefore the risk of extinction, of species. Limited geographic range is the most important determinant of genus extinction at background rates but becomes increasingly irrelevant as mass extinction arises.
Natural selection acts to propagate beneficial genetic traits and eliminate weaknesses. But a deleterious mutation can also be spread throughout a population through genetic drift.
Because traits are selected and not genes, the relationship between genetic diversity and extinction risk can be complex: factors such as balancing selection, cryptic genetic variation, phenotypic plasticity, and degeneracy all potentially play roles.
A diverse or deep gene pool gives a population a higher chance of surviving an adverse change in conditions. Effects that cause or reward a loss in genetic diversity can increase the chances of extinction of a species. Population bottlenecks can dramatically reduce genetic diversity by severely limiting the number of reproducing individuals and make inbreeding more frequent. The founder effect can cause rapid, individual-based speciation and is the most dramatic example of a population bottleneck.
Genetic pollution.
Purebred wild species evolved to a specific ecology can be threatened with extinction through the process of genetic pollution—i.e., uncontrolled hybridization, introgression genetic swamping which leads to homogenization or out-competition from the introduced (or hybrid) species. Endemic populations can face such extinctions when new populations are imported or selectively bred by people, or when habitat modification brings previously isolated species into contact. Extinction is likeliest for rare species coming into contact with more abundant ones; interbreeding can swamp the rarer gene pool and create hybrids, depleting the purebred gene pool (for example, the endangered wild water buffalo is most threatened with extinction by genetic pollution from the abundant domestic water buffalo). Such extinctions are not always apparent from morphological (non-genetic) observations. Some degree of gene flow is a normal evolutionarily process, nevertheless, hybridization (with or without introgression) threatens rare species' existence.
The gene pool of a species or a population is the variety of genetic information in its living members. A large gene pool (extensive genetic diversity) is associated with robust populations that can survive bouts of intense selection. Meanwhile, low genetic diversity (see inbreeding and population bottlenecks) reduces the range of adaptions possible. Replacing native with alien genes narrows genetic diversity within the original population, thereby increasing the chance of extinction.
Habitat degradation.
Habitat degradation is currently the main anthropogenic cause of species extinctions. The main cause of habitat degradation worldwide is agriculture, with urban sprawl, logging, mining and some fishing practices close behind. The degradation of a species' habitat may alter the fitness landscape to such an extent that the species is no longer able to survive and becomes extinct. This may occur by direct effects, such as the environment becoming toxic, or indirectly, by limiting a species' ability to compete effectively for diminished resources or against new competitor species.
Habitat degradation through toxicity can kill off a species very rapidly, by killing all living members through contamination or sterilizing them. It can also occur over longer periods at lower toxicity levels by affecting life span, reproductive capacity, or competitiveness.
Habitat degradation can also take the form of a physical destruction of niche habitats. The widespread destruction of tropical rainforests and replacement with open pastureland is widely cited as an example of this; elimination of the dense forest eliminated the infrastructure needed by many species to survive. For example, a fern that depends on dense shade for protection from direct sunlight can no longer survive without forest to shelter it. Another example is the destruction of ocean floors by bottom trawling.
Diminished resources or introduction of new competitor species also often accompany habitat degradation. Global warming has allowed some species to expand their range, bringing unwelcome competition to other species that previously occupied that area. Sometimes these new competitors are predators and directly affect prey species, while at other times they may merely outcompete vulnerable species for limited resources. Vital resources including water and food can also be limited during habitat degradation, leading to extinction.
Predation, competition, and disease.
In the natural course of events, species become extinct for a number of reasons, including but not limited to: extinction of a necessary host, prey or pollinator, inter-species competition, inability to deal with evolving diseases and changing environmental conditions (particularly sudden changes) which can act to introduce novel predators, or to remove prey. Recently in geological time, humans have become an additional cause of extinction (many people would say premature extinction) of some species, either as a new mega-predator or by transporting animals and plants from one part of the world to another. Such introductions have been occurring for thousands of years, sometimes intentionally (e.g. livestock released by sailors on islands as a future source of food) and sometimes accidentally (e.g. rats escaping from boats). In most cases, the introductions are unsuccessful, but when an invasive alien species does become established, the consequences can be catastrophic. Invasive alien species can affect native species directly by eating them, competing with them, and introducing pathogens or parasites that sicken or kill them; or indirectly by destroying or degrading their habitat. Human populations may themselves act as invasive predators. According to the "overkill hypothesis", the swift extinction of the megafauna in areas such as Australia (40,000 years before present), North and South America (12,000 years before present), Madagascar, Hawaii (300-1000 CE), and New Zealand (1300-1500 CE), resulted from the sudden introduction of human beings to environments full of animals that had never seen them before, and were therefore completely unadapted to their predation techniques.
Coextinction.
Coextinction refers to the loss of a species due to the extinction of another; for example, the extinction of parasitic insects following the loss of their hosts. Coextinction can also occur when a species loses its pollinator, or to predators in a food chain who lose their prey. "Species coextinction is a manifestation of the interconnectedness of organisms in complex ecosystems ... While coextinction may not be the most important cause of species extinctions, it is certainly an insidious one". Coextinction is especially common when a keystone species goes extinct.
Models suggest that coextinction is the most common form of biodiversity loss. There may be a cascade of coextinction across the trophic levels. Such effects are most severe in mutualistic and parasitic relationships. An example of coextinction is the Haast's eagle and the moa: the Haast's eagle was a predator that became extinct because its food source became extinct. The moa were several species of flightless birds that were a food source for the Haast's eagle.
Climate change.
Extinction as a result of climate change has been confirmed by fossil studies. Particularly, the extinction of amphibians during the Carboniferous Rainforest Collapse, 305 million years ago. A 2003 review across 14 biodiversity research centers predicted that, because of climate change, 15–37% of land species would be "committed to extinction" by 2050. The ecologically rich areas that would potentially suffer the heaviest losses include the Cape Floristic Region, and the Caribbean Basin. These areas might see a doubling of present carbon dioxide levels and rising temperatures that could eliminate 56,000 plant and 3,700 animal species.
Mass extinctions.
 <imagemap>
Image:Extinction intensity.svg
Marine extinction intensity during the Phanerozoic
Millions of years ago
K–Pg
Tr–J
P–Tr
Late D
O–S
<imagemap>
Image:Extinction intensity.svg
The blue graph shows the apparent "percentage" (not the absolute number) of marine animal genera becoming extinct during any given time interval. It does not represent all marine species, just those that are readily fossilized. The labels of the "Big Five" extinction events are clickable hyperlinks; see Extinction event for more details. "()"
There have been at least five mass extinctions in the history of life on earth, and four in the last 3.5 billion years in which many species have disappeared in a relatively short period of geological time. A massive eruptive event is considered to be one likely cause of the "Permian–Triassic extinction event" about 250 million years ago, which is estimated to have killed 90% of species then existing. There is also evidence to suggest that this event was preceded by another mass extinction, known as Olson's Extinction. The Cretaceous–Paleogene extinction event (K-Pg) occurred 66 million years ago, at the end of the Cretaceous period, and is best known for having wiped out non-avian dinosaurs, among many other species.
Modern extinctions.
According to a 1998 survey of 400 biologists conducted by New York's American Museum of Natural History, nearly 70% believed that the Earth is currently in the early stages of a human-caused extinction, known as the Holocene extinction. In that survey, the same proportion of respondents agreed with the prediction that up to 20% of all living populations could become extinct within 30 years (by 2028). Biologist E. O. Wilson estimated in 2002 that if current rates of human destruction of the biosphere continue, one-half of all plant and animal species of life on earth will be extinct in 100 years. More significantly, the current rate of global species extinctions is estimated as 100 to 1000 times "background" rates (the average extinction rates in the evolutionary time scale of planet Earth), while future rates are likely 10,000 times higher. However, some groups are going extinct much faster. Amphibians, for example, are disappearing at as much as 45,000 times their extinction rate at K-Pg. Despite such extreme losses on a global scale, the public's interest in extinction is in a world-wide decline.
History of scientific understanding.
When it was first described in the 1750s, the idea of extinction was threatening to those who held a belief in the great chain of being, a theological position that did not allow for "missing links".
The possibility of extinction was not widely accepted before the 1800s. The devoted naturalist Carl Linnaeus could "hardly entertain" the idea that humans could cause the extinction of a species. When parts of the world had not been thoroughly examined and charted, scientists could not rule out that animals found only in the fossil record were not simply "hiding" in unexplored regions of the Earth. Georges Cuvier is credited with establishing extinction as a fact in a 1796 lecture to the French Institute. Cuvier's observations of fossil bones convinced him that they did not originate in extant animals. This discovery was critical for the spread of uniformitarianism, and led to the first book publicizing the idea of evolution though Cuvier himself strongly opposed the theories of evolution advanced by Lamarck and others.
Human attitudes and interests.
Extinction is an important research topic in the field of zoology, and biology in general, and has also become an area of concern outside the scientific community. A number of organizations, such as the Worldwide Fund for Nature, have been created with the goal of preserving species from extinction. Governments have attempted, through enacting laws, to avoid habitat destruction, agricultural over-harvesting, and pollution. While many human-caused extinctions have been accidental, humans have also engaged in the deliberate destruction of some species, such as dangerous viruses, and the total destruction of other problematic species has been suggested. Other species were deliberately driven to extinction, or nearly so, due to poaching or because they were "undesirable", or to push for other human agendas. One example was the near extinction of the American bison, which was nearly wiped out by mass hunts sanctioned by the United States government, to force the removal of Native Americans, many of whom relied on the bison for food.
Biologist Bruce Walsh of the University of Arizona states three reasons for scientific interest in the preservation of species; genetic resources, ecosystem stability, and ethics; and today the scientific community "stress[es] the importance" of maintaining biodiversity.
In modern times, commercial and industrial interests often have to contend with the effects of production on plant and animal life. However, some technologies with minimal, or no, proven harmful effects on "Homo sapiens" can be devastating to wildlife (for example, DDT). Biogeographer Jared Diamond notes that while big business may label environmental concerns as "exaggerated", and often cause "devastating damage", some corporations find it in their interest to adopt good conservation practices, and even engage in preservation efforts that surpass those taken by national parks.
Governments sometimes see the loss of native species as a loss to ecotourism, and can enact laws with severe punishment against the trade in native species in an effort to prevent extinction in the wild. Nature preserves are created by governments as a means to provide continuing habitats to species crowded by human expansion. The 1992 Convention on Biological Diversity has resulted in international Biodiversity Action Plan programmes, which attempt to provide comprehensive guidelines for government biodiversity conservation. Advocacy groups, such as The Wildlands Project and the Alliance for Zero Extinctions, work to educate the public and pressure governments into action.
People who live close to nature can be dependent on the survival of all the species in their environment, leaving them highly exposed to extinction risks. However, people prioritize day-to-day survival over species conservation; with human overpopulation in tropical developing countries, there has been enormous pressure on forests due to subsistence agriculture, including slash-and-burn agricultural techniques that can reduce endangered species's habitats.
The American philosopher Michael Levin argues, "The very fact that a species is near extinction implies that its final demise will have negligible impact."
Planned extinction.
Proposed.
Biologist Olivia Judson has advocated the deliberate extinction of certain species. In a September 25, 2003 "New York Times" article, she advocated "specicide" of thirty mosquito species by introducing a genetic element which can insert itself into another crucial gene, to create recessive "knockout genes". She says that the "Anopheles" mosquitoes (which spread malaria) and "Aedes" mosquitoes (which spread dengue fever, yellow fever, elephantiasis, and other diseases) represent only 30 species; eradicating these would save at least one million human lives per annum, at a cost of reducing the genetic diversity of the family Culicidae by only 1%. She further argues that since species become extinct "all the time" the disappearance of a few more will not destroy the ecosystem: "We're not left with a wasteland every time a species vanishes. Removing one species sometimes causes shifts in the populations of other species — but different need not mean worse." In addition, anti-malarial and mosquito control programs offer little realistic hope to the 300 million people in developing nations who will be infected with acute illnesses this year. Although trials are ongoing, she writes that if they fail: "We should consider the ultimate swatting."
Cloning.
Some, such as Harvard geneticist George M. Church believe that ongoing technological advances will let us "bring back to life" an extinct species by cloning, using DNA from the remains of that species. Proposed targets for cloning include the mammoth, the thylacine, and the Pyrenean ibex. For this to succeed, enough individuals would have to be cloned, from the DNA of different individuals (in the case of sexually reproducing organisms) to create a viable population. Though bioethical and philosophical objections have been raised, the cloning of extinct creatures seems theoretically possible.
In 2003, scientists tried to clone the extinct Pyrenean ibex ("C. p. pyrenaica"). This attempt failed: of the 285 embryos reconstructed, 54 were transferred to 12 mountain goats and mountain goat-domestic goat hybrids, but only two survived the initial two months of gestation before they too died. In 2009, a second attempt was made to clone the Pyrenean ibex: one clone was born alive, but died seven minutes later, due to physical defects in the lungs.
In fiction, the concept of cloning extinct species is thought to have been first popularized by the successful 1990 Michael Crichton novel and subsequent film "Jurassic Park", though the idea may have been first used in John Brosnan's 1984 novel "Carnosaur", then in F. Paul Wilson's 1989 novel "Dydeetown World", and later in Piers Anthony's 1990 novel "Balook", which featured the resurrection of a "Baluchitherium", though Pat Mills' Judge Dredd story "The Cursed Earth" – in which the titular lawman battles tyrannosaurs who live wild in post-apocalyptic America after they escape from the theme park where they have been cloned to be used as attractions - precedes these examples, published as it was in 1978.

</doc>
<doc id="49418" url="http://en.wikipedia.org/wiki?curid=49418" title="MidasWWW">
MidasWWW

MidasWWW was one of the earliest web browsers, developed at the Stanford Linear Accelerator Center (SLAC). It ran under Unix and VMS. The last release was version 2.2.

</doc>
<doc id="49420" url="http://en.wikipedia.org/wiki?curid=49420" title="CMOS">
CMOS

Complementary metal–oxide–semiconductor (CMOS) is a technology for constructing integrated circuits. CMOS technology is used in microprocessors, microcontrollers, static RAM, and other digital logic circuits. CMOS technology is also used for several analog circuits such as image sensors (CMOS sensor), data converters, and highly integrated transceivers for many types of communication. In 1963, while working for Fairchild Semiconductor, Frank Wanlass patented CMOS ().
CMOS is also sometimes referred to as complementary-symmetry metal–oxide–semiconductor (or COS-MOS).
The words "complementary-symmetry" refer to the fact that the typical design style with CMOS uses complementary and symmetrical pairs of p-type and n-type metal oxide semiconductor field effect transistors (MOSFETs) for logic functions.
Two important characteristics of CMOS devices are high noise immunity and low static power consumption. Since one transistor of the pair is always off, the series combination draws significant power only momentarily during switching between on and off states. Consequently, CMOS devices do not produce as much waste heat as other forms of logic, for example transistor–transistor logic (TTL) or NMOS logic, which normally have some standing current even when not changing state. CMOS also allows a high density of logic functions on a chip. It was primarily for this reason that CMOS became the most used technology to be implemented in VLSI chips.
The phrase "metal–oxide–semiconductor" is a reference to the physical structure of certain field-effect transistors, having a metal gate electrode placed on top of an oxide insulator, which in turn is on top of a semiconductor material. Aluminium was once used but now the material is polysilicon. Other metal gates have made a comeback with the advent of high-k dielectric materials in the CMOS process, as announced by IBM and Intel for the 45 nanometer node and beyond.
Technical details.
"CMOS" refers to both a particular style of digital circuitry design and the family of processes used to implement that circuitry on integrated circuits (chips). CMOS circuitry dissipates less power than logic families with resistive loads. Since this advantage has increased and grown more important, CMOS processes and variants have come to dominate, thus the vast majority of modern integrated circuit manufacturing is on CMOS processes.
As of 2010, CPUs with the best performance per watt each year have been CMOS static logic since 1976.
CMOS circuits use a combination of p-type and n-type metal–oxide–semiconductor field-effect transistors (MOSFETs) to implement logic gates and other digital circuits. Although CMOS logic can be implemented with discrete devices for demonstrations, commercial CMOS products are integrated circuits composed of up to billions of transistors of both types, on a rectangular piece of silicon of between 10 and 400 mm2.
Inversion.
CMOS circuits are constructed in such a way that all PMOS transistors must have either an input from the voltage source or from another PMOS transistor. Similarly, all NMOS transistors must have either an input from ground or from another NMOS transistor. The composition of a PMOS transistor creates low resistance between its source and drain contacts when a low gate voltage is applied and high resistance when a high gate voltage is applied. On the other hand, the composition of an NMOS transistor creates high resistance between source and drain when a low gate voltage is applied and low resistance when a high gate voltage is applied. CMOS accomplishes current reduction by complementing every nMOSFET with a pMOSFET and connecting both gates and both drains together. A high voltage on the gates will cause the nMOSFET to conduct and the pMOSFET to not conduct, while a low voltage on the gates causes the reverse. This arrangement greatly reduces power consumption and heat generation. However, during the switching time, both MOSFETs conduct briefly as the gate voltage goes from one state to another. This induces a brief spike in power consumption and becomes a serious issue at high frequencies.
The image on the right shows what happens when an input is connected to both a PMOS transistor (top of diagram) and an NMOS transistor (bottom of diagram). When the voltage of input A is low, the NMOS transistor's channel is in a high resistance state. This limits the current that can flow from Q to ground. The PMOS transistor's channel is in a low resistance state and much more current can flow from the supply to the output. Because the resistance between the supply voltage and Q is low, the voltage drop between the supply voltage and Q due to a current drawn from Q is small. The output therefore registers a high voltage.
On the other hand, when the voltage of input A is high, the PMOS transistor is in an OFF (high resistance) state so it would limit the current flowing from the positive supply to the output, while the NMOS transistor is in an ON (low resistance) state, allowing the output from drain to ground. Because the resistance between Q and ground is low, the voltage drop due to a current drawn into Q placing Q above ground is small. This low drop results in the output registering a low voltage.
In short, the outputs of the PMOS and NMOS transistors are complementary such that when the input is low, the output is high, and when the input is high, the output is low. Because of this behavior of input and output, the CMOS circuit's output is the inverse of the input.
The power supplies for CMOS are called VDD and VSS, or VCC and Ground(GND) depending on the manufacturer. VDD and VSS are carryovers from conventional MOS circuits and stand for the drain and source supplies. These do not apply directly to CMOS, since both supplies are really source supplies. VCC and Ground are carryovers from TTL logic and that nomenclature has been retained with the introduction of the 54C/74C line of CMOS.
Duality.
An important characteristic of a CMOS circuit is the duality that exists between its PMOS transistors and NMOS transistors. A CMOS circuit is created to allow a path always to exist from the output to either the power source or ground. To accomplish this, the set of all paths to the voltage source must be the complement of the set of all paths to ground. This can be easily accomplished by defining one in terms of the NOT of the other. Due to the De Morgan's laws based logic, the PMOS transistors in parallel have corresponding NMOS transistors in series while the PMOS transistors in series have corresponding NMOS transistors in parallel.
Logic.
More complex logic functions such as those involving AND and OR gates require manipulating the paths between gates to represent the logic. When a path consists of two transistors in series, both transistors must have low resistance to the corresponding supply voltage, modelling an AND. When a path consists of two transistors in parallel, either one or both of the transistors must have low resistance to connect the supply voltage to the output, modelling an OR.
Shown on the right is a circuit diagram of a NAND gate in CMOS logic. If both of the A and B inputs are high, then both the NMOS transistors (bottom half of the diagram) will conduct, neither of the PMOS transistors (top half) will conduct, and a conductive path will be established between the output and "V"ss (ground), bringing the output low. If both of the A and B inputs are low, then neither of the NMOS transistors will conduct, while both of the PMOS transistors will conduct, establishing a conductive path between the output and "V"dd (voltage source), bringing the output high. If either of the A or B inputs is low, one of the NMOS transistors will not conduct, one of the PMOS transistors will, and a conductive path will be established between the output and "V"dd (voltage source), bringing the output high. As the only configuration of the two inputs that results in a low output is when both are high, this circuit implements a NAND (NOT AND) logic gate.
An advantage of CMOS over NMOS is that both low-to-high and high-to-low output transitions are fast since the pull-up transistors have low resistance when switched on, unlike the load resistors in NMOS logic. In addition, the output signal swings the full voltage between the low and high rails. This strong, more nearly symmetric response also makes CMOS more resistant to noise.
See Logical effort for a method of calculating delay in a CMOS circuit.
Example: NAND gate in physical layout.
This example shows a NAND logic device drawn as a physical representation as it would be manufactured. The physical layout perspective is a "bird's eye view" of a stack of layers. The circuit is constructed on a P-type substrate. The polysilicon, diffusion, and n-well are referred to as "base layers" and are actually inserted into trenches of the P-type substrate. The contacts penetrate an insulating layer between the base layers and the first layer of metal (metal1) making a connection.
The inputs to the NAND (illustrated in green color) are in polysilicon. The CMOS transistors (devices) are formed by the intersection of the polysilicon and diffusion; N diffusion for the N device & P diffusion for the P device (illustrated in salmon and yellow coloring respectively). The output ("out") is connected together in metal (illustrated in cyan coloring). Connections between metal and polysilicon or diffusion are made through contacts (illustrated as black squares). The physical layout example matches the NAND logic circuit given in the previous example.
The N device is manufactured on a P-type substrate while the P device is manufactured in an N-type well (n-well). A P-type substrate "tap" is connected to VSS and an N-type n-well tap is connected to VDD to prevent latchup.
Power: switching and leakage.
CMOS logic dissipates less power than NMOS logic circuits because CMOS dissipates power only when switching ("dynamic power"). On a typical ASIC in a modern 90 nanometer process, switching the output might take 120 picoseconds, and happens once every ten nanoseconds. NMOS logic dissipates power whenever the transistor is on, because there is a current path from Vdd to Vss through the load resistor and the n-type network.
Static CMOS gates are very power efficient because they dissipate nearly zero power when idle. Earlier, the power consumption of CMOS devices was not the major concern while designing chips. Factors like speed and area dominated the design parameters. As the CMOS technology moved below sub-micron levels the power consumption per unit area of the chip has risen tremendously.
Broadly classifying, power dissipation in CMOS circuits occurs because of two components:
Static dissipation.
Subthreshold conduction when the transistors are off.
Both NMOS and PMOS transistors have a gate–source threshold voltage, below which the current (called "sub threshold" current) through the device drops exponentially. Historically, CMOS designs operated at supply voltages much larger than their threshold voltages (Vdd might have been 5 V, and Vth for both NMOS and PMOS might have been 700 mV). A special type of the CMOS transistor with near zero threshold voltage is the native transistor.
Tunnelling current through gate oxide.
SiO2 is a very good insulator, but at very small thickness levels electrons can tunnel across the very thin insulation; the probability drops off exponentially with oxide thickness. Tunnelling current becomes very important for transistors below 130 nm technology with gate oxides of 20 Å or thinner.
Leakage current through reverse-biased diodes.
Small reverse leakage currents are formed due to formation of reverse bias between diffusion regions and wells (for e.g., p-type diffusion vs. n-well), wells and substrate (for e.g., n-well vs. p-substrate). In modern process diode leakage is very small compared to sub threshold and tunnelling currents, so these may be neglected during power calculations.
Dynamic dissipation.
Charging and discharging of load capacitances.
CMOS circuits dissipate power by charging the various load capacitances (mostly gate and wire capacitance, but also drain and some source capacitances) whenever they are switched. In one complete cycle of CMOS logic, current flows from VDD to the load capacitance to charge it and then flows from the charged load capacitance (CL) to ground during discharge. Therefore in one complete charge/discharge cycle, a total of Q=CLVDD is thus transferred from VDD to ground. Multiply by the switching frequency on the load capacitances to get the current used, and multiply by the average voltage again to get the characteristic switching power dissipated by a CMOS device: formula_1.
Since most gates do not operate/switch at every clock cycle, they are often accompanied by a factor formula_2, called the activity factor. Now, the dynamic power dissipation may be re-written as formula_3.
A clock in a system has an activity factor α=1, since it rises and falls every cycle. Most data has an activity factor of 0.1. If correct load capacitance is estimated on a node together with its activity factor, the dynamic power dissipation at that node can be calculated effectively.
Short-circuit power dissipation.
Since there is a finite rise/fall time for both pMOS and nMOS, during transition, for example, from off to on, both the transistors will be on for a small period of time in which current will find a path directly from VDD to ground, hence creating a short-circuit current. Short-circuit power dissipation increases with rise and fall time of the transistors.
An additional form of power consumption became significant in the 1990s as wires on chip became narrower and the long wires became more resistive. CMOS gates at the end of those resistive wires see slow input transitions. During the middle of these transitions, both the NMOS and PMOS logic networks are partially conductive, and current flows directly from VDD to VSS. The power thus used is called "crowbar" power. Careful design which avoids weakly driven long skinny wires ameliorates this effect, but crowbar power can be a substantial part of dynamic CMOS power.
To speed up designs, manufacturers have switched to constructions that have lower voltage thresholds but because of this a modern NMOS transistor with a Vth of 200 mV has a significant subthreshold leakage current. Designs (e.g. desktop processors) which include vast numbers of circuits which are not actively switching still consume power because of this leakage current. Leakage power is a significant portion of the total power consumed by such designs. Multi-threshold CMOS (MTCMOS), now available from foundries, is one approach to managing leakage power. With MTCMOS, high Vth transistors are used when switching speed is very important, while low Vth transistors are used in speed sensitive paths. Further technology advances that use even thinner gate dielectrics have an additional leakage component because of current tunnelling through the extremely thin gate dielectric. Using high-k dielectrics instead of silicon dioxide that is the conventional gate dielectric allows similar device performance, but with a thicker gate insulator, thus avoiding this current. Leakage power reduction using new material and system designs is critical to sustaining scaling of CMOS.
Analog CMOS.
Besides digital applications, CMOS technology is also used in analog applications. For example, there are CMOS operational amplifier ICs available in the market. Transmission gates may be used instead of signal relays. CMOS technology is also widely used for RF circuits all the way to microwave frequencies, in mixed-signal (analog+digital) applications.
Temperature range.
Conventional CMOS devices work over a range of −55 °C to +125 °C. There were theoretical indications as early as August 2008 that silicon CMOS will work down to −233 °C (40 K). Functioning temperatures near 40 K have since been achieved using overclocked AMD Phenom II processors with a combination of liquid nitrogen and liquid helium cooling.
Single-electron CMOS transistors.
Ultra small (L = 20 nm, W = 20 nm) CMOS transistors achieve the single-electron limit when operated at cryogenic temperature over a range of −269 °C (4 K) to about −258 °C (15 K). The transistor displays Coulomb blockade due to progressive charging of electrons one by one. The number of electrons confined in the channel is driven by the gate voltage, starting from an occupation of zero electrons, and it can be set to 1 or many.

</doc>
<doc id="49430" url="http://en.wikipedia.org/wiki?curid=49430" title="Robert Abercromby (missionary)">
Robert Abercromby (missionary)

Father Robert Abercromby (1536 – 27 April 1613), whose surname was also spelled as Abrecromby and Abercrombie, and was known by such pseudonyms as Robert Sandiesoun and Sanders Robertson, was a Scottish Jesuit missionary.
Early life.
He was born and educated in Scotland, and studied in the Collegium Romanum in Rome, where on 19 August 1563 he became a Jesuit. From 1564 he lived in Braunsberg (then in Royal Prussia; present-day Braniewo, Poland) where he was professor of grammar in the biggest Polish Jesuit "collegium" and a novice master.
In 1565 he was ordained a priest. In Braniewo he was in constant contact with Cardinal Stanislaus Hosius. He was considered a good priest, but learning Polish was difficult for him, and he had some problems with the finances of the school. Due to these problems he was permitted to leave Poland in 1580, when he met the Scottish king for the first time. In September 1580 he went back to Poland - from 1580–87 he performed similar tasks in Kraków, Poznań and Wilno. In 1587 he left Poland and returned to Scotland. During the journey to Scotland in 1580 and during his second stay there he was organizing transports of Scottish Catholic novices to be trained in Polish schools and seminaries.
Contact with Anne of Denmark.
Abercromby claimed that he had reconciled Anne of Denmark, queen of James VI of Scotland, to the Catholic Church. James apparently allowed Abercromby to meet her at Holyroodhouse circa 1599. She made no outward sign of a change of religion.
Later life.
Abercromby remained in Scotland for some time, but a price of 10,000 crowns was put upon his head. He spent the period 1601–06 under the protection of George Gordon, 1st Marquess of Huntly.
Abercromby went back to Braunsberg in 1606. His name was connected to the allegiance oath controversy when a pamphlet "pasquil", "Exetasis epistolæ nomine regis", written under the pseudonym Bartholus Pacenius against James I was traced to Braunsberg; but the investigation by Patrick Gordon was inconclusive. He died there on 27 April 1613.
References.
 incorporates text from a publication now in the public domain: 

</doc>
<doc id="49431" url="http://en.wikipedia.org/wiki?curid=49431" title="Francis Pharcellus Church">
Francis Pharcellus Church

Francis Pharcellus Church (February 22, 1839 – April 11, 1906) was an American publisher and editor. He was a member of the Century Association.
Biography.
He was born in Rochester, New York and graduated from Columbia College of Columbia University in New York City in 1859. 
With his brother William Conant Church he established "The Army and Navy Journal" in 1863, and "Galaxy" magazine in 1866 (merged with "Atlantic Monthly" after 10 years). He was a lead editorial writer on his brother's newspaper, the "New York Sun", and it was in that capacity that in 1897 he wrote his most famous editorial, "Yes, Virginia, there is a Santa Claus". In this editorial he responds to a young girl's question if there truly is a Santa Claus, writing that he definitely exists and placing himself within Christmas' history forever. 
Church died in New York City, aged 67, and was buried in Sleepy Hollow Cemetery in Sleepy Hollow, New York. He had no children.

</doc>
<doc id="49434" url="http://en.wikipedia.org/wiki?curid=49434" title="Conjunction (astronomy)">
Conjunction (astronomy)

A conjunction occurs when two astronomical objects have either the same right ascension or the same ecliptical longitude, normally when observed from the Earth.
In the case of two objects that always appear close to the ecliptic – such as two planets, or the Moon and a planet, or the Sun and a planet – this implies an apparent close approach between the objects as seen on the sky.
In contrast, the term appulse is defined as the minimum apparent separation on the sky of two astronomical bodies.
Conjunctions therefore involve two Solar System bodies, or one Solar System body and one more distant object such as a star. A conjunction is an apparent phenomenon caused by perspective only: there is no close physical approach in space between the two objects involved. Conjunctions between two bright objects close to the ecliptic, such as two bright planets, can be easily seen with the naked eye and can attract some public interest.
The astronomical symbol of conjunction is ☌ (in Unicode U+260C) and handwritten:
. However, this symbol is never used in modern astronomy and is of historical interest only.
Passing close.
More generally, in the particular case of two planets, it means that they merely have the same right ascension (and hence the same hour angle). This is called conjunction in right ascension. However, there is also the term conjunction in ecliptical longitude. At such conjunction both objects have the same ecliptical longitude. Conjunction in right ascension and conjunction in ecliptical longitude do not normally take place at the same time, but in most cases nearly at the same time. However, at triple conjunctions, it is possible that a conjunction only in right ascension (or ecliptical length) occur. At the time of conjunction – it does not matter if in right ascension or in ecliptical longitude – the involved planets are close together upon the celestial sphere. In the vast majority of such cases, one of the planets will appear to pass north or south of the other.
Passing closer.
However, if two celestial bodies attain the same declination at the time of a conjunction in right ascension (or the same ecliptical latitude at a conjunction in ecliptical longitude), the one that is closer to the Earth will pass in front of the other. In such a case, a syzygy takes place. If one object moves into the shadow of another, the event is an eclipse. For example, if the Moon passes into the shadow of Earth and disappears from view, this event is called a lunar eclipse. If the visible disk of the nearer object is considerably smaller than that of the farther object, the event is called a transit. When Mercury passes in front of the Sun, it is a transit of Mercury, and when Venus passes in front of the Sun, it is a transit of Venus. When the nearer object appears larger than the farther one, it will completely obscure its smaller companion; this is called an occultation. An example of an occultation is when the Moon passes between Earth and the Sun, causing the Sun to disappear either entirely or partially. This phenomenon is commonly known as a solar eclipse. Occultations in which the larger body is neither the Sun nor the Moon are very rare. More frequent, however, is an occultation of a planet by the Moon. Several such events are visible every year from various places on Earth.
Position of the observer.
A conjunction, as a phenomenon of perspective, is an event that involves two astronomical bodies seen by an observer on the Earth. Times and details depend only very slightly on the observer's location on the Earth's surface, with the differences being greatest for conjunctions involving the Moon because of its relative closeness, but even for the Moon the time of a conjunction never differs by more than a few hours.
Superior and inferior.
As seen from a planet that is superior, if an inferior planet is on the opposite side of the Sun, it is in superior conjunction with the Sun. An inferior conjunction occurs when the two planets lie in a line on the same side of the Sun. In an inferior conjunction, the superior planet is "in opposition" to the Sun as seen from the inferior planet.
The terms "inferior conjunction" and "superior conjunction" are used in particular for the planets Mercury and Venus, which are inferior planets as seen from the Earth. However, this definition can be applied to any pair of planets, as seen from the one farther from the Sun.
A planet (or asteroid or comet) is simply said to be in conjunction, when it is in conjunction with the Sun, as seen from the Earth. The Moon is in conjunction with the Sun at New Moon.
Quasiconjunction.
In a quasiconjunction, a planet in retrograde motion — always either Mercury or Venus, from the point of view of the Earth — will "drop back" in right ascension until it almost allows another planet to overtake it, but then the former planet will resume its forward motion and thereafter appear to draw away from it again. This will occur in the morning sky, before dawn. The reverse may happen in the evening sky after dusk, with Mercury or Venus entering retrograde motion just as it is about to overtake another planet (often Mercury "and" Venus are "both" of the planets involved, and when this situation arises they may remain in very close visual proximity for several days or even longer). The quasiconjunction is reckoned as occurring at the time the distance in right ascension between the two planets is smallest, even though, when declination is taken into account, they may appear closer together shortly before or after this.
Notable conjunctions.
1899.
In early December 1899 the Sun and the naked-eye planets appeared to lie within a band 35 degrees wide along the ecliptic as seen from the Earth. As a consequence, over the period 1-4 December 1899, the Moon reached conjunction with, in order, Jupiter, Uranus, the Sun, Mercury, Mars, Saturn and Venus. Most of these conjunctions would not have been visible because of the glare of the Sun.
1962.
Over the period 4-6 February 1962, in a rare series of events, Mercury and Venus reached conjunction as observed from the Earth, followed by Venus and Jupiter, then by Mars and Saturn. Conjunctions took place between the Moon and, in turn, Mars, Saturn, the Sun, Mercury, Venus and Jupiter. Mercury also reached inferior conjunction with the Sun. The conjunction between the Moon and the Sun at new Moon produced a total solar eclipse visible in Indonesia and the Pacific Ocean,
when these five naked-eye planets would have been visible in the vicinity of the Sun in the sky.
1987.
Mercury, Venus and Mars separately reached conjunction with one other, and each separately with the Sun, within a 7-day period in August 1987 as seen from the Earth. The Moon also reached conjunction with each of these bodies on 24 August. However, none of these conjunctions would have been observable due to the glare of the Sun.
2000.
In May 2000, in a very rare event, several planets lay in the vicinity of the Sun in the sky as seen from the Earth, and a series of conjunctions took place. Jupiter, Mercury and Saturn each reached conjunction with the Sun in the period 8-10 May. These three planets in turn were in conjunction with each other and with Venus over a period of a few weeks. However, most of these conjunctions would not have been visible from the Earth because of the glare from the Sun.
2002.
Venus, Mars and Saturn appeared close together in the evening sky in early May 2002, with a conjunction of Mars and Saturn occurring on 4 May. This was followed by a conjunction of Venus and Saturn on 7 May, and another of Venus and Mars on 10 May when their angular separation was only 18 arcminutes. A series of conjunctions between the Moon and, in order, Saturn, Mars and Venus took place on 14 May, although it would not have been possible to observe all these in darkness from any single location on the Earth.
2007.
A conjunction of the Moon and Mars took place on 24 December 2007, very close to the time of the full Moon and at the time when Mars was at opposition to the Sun. Mars and the full Moon would have appeared close together in the sky worldwide, with an occultation of Mars occurring for observers in some far northern locations.
Jupiter was at conjunction with the Sun a day earlier, when it would have been invisible due to the Sun's glare in the daytime sky.
2008.
A conjunction of Venus and Jupiter occurred on 1 December 2008, and several hours later both planets separately reached conjunction with the crescent Moon. An occultation of Venus by the Moon was visible from some locations. The three objects appeared close together in the sky from any location on the Earth.

</doc>
<doc id="49435" url="http://en.wikipedia.org/wiki?curid=49435" title="Axel Oxenstierna">
Axel Oxenstierna

Axel Gustafsson Oxenstierna af Södermöre (]; 1583 – 1654), Count of Södermöre, was a Swedish statesman. He became a member of the Swedish Privy Council in 1609 and served as Lord High Chancellor of Sweden from 1612 until his death. He was a confidant of first Gustavus Adolphus and then Queen Christina.
Oxenstierna is widely considered one of the most influential people in Swedish history. He played an important role during the Thirty Years' War and was appointed Governor-General of occupied Prussia; he is also credited for having laid the foundations of the modern central administrative structure of the State, including the creation of counties (Swedish: "län").
Early life and education.
Oxenstierna was born on 16 June 1583, at Fånö in Uppland, the son of Gustaf Gabrielsson Oxenstierna (1551–1597) and Barbro Axelsdotter Bielke (1556–1624). He was the oldest of nine siblings. After the death of her husband Gustaf, Axel's mother Barbro decided to let Axel and his brothers Christer and Gustaf finish their studies abroad. Thus, the brothers received their education at the universities of Rostock, Wittenberg and Jena. On returning home in 1603 he took up an appointment as "kammarjunkare" to King Charles IX of Sweden.
One of Oxenstierna's more unusual intellectual qualifications was his knowledge of the Scots language, reflecting the importance of the Scottish expatriate community in Sweden at that time. As Chancellor, he would regularly receive correspondence in Scots from his agent Sir James Spens, and he ventured into the language himself for an official letter to his Scottish counterpart, the Earl of Loudoun.
Career.
1606–1611: Diplomat and Privy Councillor.
In 1606 he undertook his first diplomatic mission, to Mecklenburg and other German royal courts. While on diplomatic duty abroad, Oxenstierna gained appointment to the Privy Council ("Riksrådet"). Henceforth, Oxenstierna became one of the king's most trusted servants. In 1609 he travelled to Reval (present day Tallinn, on King Charles's behalf, to receive tributes from the city of Reval and the Estonian knighthood. Together with other councillors, Oxenstierna tried to warn the king of Denmark and the intentions of Danish King Christian IV. In 1610, Oxenstierna travelled to Copenhagen with the aim of preventing war with the neighbours, but unsuccessfully. The following year, Danish forces crossed the border, initiating the Kalmar War. In the fall of 1611, King Charles died. Around New Year 1611–12, the parliament had to deal with the situation. According to the rules, the 17-year-old Gustavus Adolphus had not reached the proper age to be considered adult enough to rule as king. However, the estates agreed to disregard those rules. In return, the young king agreed to ensure the nobles further privileges and appoint Axel Oxenstierna Lord High Chancellor.
1612–1629: Lord High Chancellor and Governor-General.
On 6 January 1612 Oxenstierna became Lord High Chancellor ("Rikskansler") of the Privy Council. His controlling, organizing hand soon became apparent in every branch of the administration. Sweden was at the time troubled by three wars against Denmark (Kalmar War), Poland-Lithuania (Polish-Swedish War) and Russia (Ingrian War). Oxenstierna's first big task as Chancellor was to achieve peace in some of the wars. The war against Denmark was considered the most dangerous of the three as the enemy-controlled parts of Sweden itself. Negotiations began in Knäred and Oxenstierna was first Swedish plenipotentiary. The negotiations led to the Treaty of Knäred in 1613. For his efforts regarding these negotiations, Oxenstierna received the title of district judge in the hundred of Snävringe and, eventually, the barony of Kimito.
During the frequent absences of Gustavus in Livonia and in Finland (1614–1616) Oxenstierna acted as his viceroy. One assignment Oxenstierna received while the king was in Livonia, was the task to finalize the negotiations regarding the marriage of John Casimir and the king's sister, Princess Catharina. At the coronation of Gustavus Adolphus, in October 1617, Oxenstierna was knighted. In 1620 he headed the embassy dispatched to Berlin to arrange the nuptial contract between Gustavus and Maria Eleonora of Brandenburg. During the king's Russian and Polish wars he had the principal duty of supplying the armies and the fleets with everything necessary, including men and money. Oxenstierna's ways of carrying out his assignments apparently gained King Gustavus's appreciation, since the king, in 1622, asked Oxenstierna to accompany him to Livonia and appointed him Governor-General and commandant of Riga, a strategically important town during the ongoing war against Poland. His services in Livonia gained him the reward of four castles (among others Burtnieki and Valmiera) and the whole bishopric of Wenden. Entrusted with the peace negotiations which led to the truce with Poland in 1623, he succeeded in averting a threatened rupture with Denmark in 1624. The Polish-Swedish War was reinitiated in 1626, and on 7 October that year, Oxenstierna became Governor-General in the newly acquired Swedish possession of Prussia. In 1629 he concluded the advantageous Truce of Altmark with Poland-Lithuania. Prior to this, in September 1628, he arranged a joint occupation of Stralsund with Denmark in order to prevent that important fortress from falling into the hands of the Imperialists.
Oxenstierna was not only highly successful within the diplomacy. During these years, he was entrusted with various important assignments in which he succeeded, such as gathering money and troops for the attack in Prussia in 1626. He played the leading organizational and administrational role in Prussia, as he had done earlier in Livonia. He was in charge of, for example, tolls, fortifications and the entire state grain trade. During the latter part of the 1620s, Elbląg (German: "Elbing"), where Oxenstierna resided and from where he governed the Swedish parts of Prussia, became a major Swedish centre of power, second only to Stockholm.
1630–1636: Oxenstierna in the Thirty Years' War.
When Sweden entered the Thirty Years' War in the summer of 1630, tolls from Oxenstierna-controlled Prussia, as well as food supplies acquired by Oxenstierna, were pivotal assets. He had also obtained credits from foreign businessmen, ensuring large sums of money making it possible to buy mercenary soldiers to the army used in Germany.
After the Battle of Breitenfeld on 7 September 1631, Oxenstierna received a summons to assist the king with his counsels and co-operation in Germany. During the king's absence in Franconia and Bavaria in 1632 he held the appointment of "legatus" in the Rhineland, with plenipotentiary authority over all the German generals and princes in the Swedish service. Although he never fought a battle, he frustrated all the efforts of the Spanish troops by using strategically successful regulations. He managed to conduct large reinforcements to King Gustavus through the heart of Germany in the summer of 1632.
In the Battle of Lützen (1632), on 6 November 1632, Gustavus Adolphus died. This meant that Oxenstierna became supreme commander of the Swedish troops in Germany, although he let his subordinate generals be responsible for the military operations on a lower level. He moved his headquarters to Mainz, which in practice became the new Swedish capital. Oxenstierna was now absolute ruler of the significant area that the Swedish army had conquered in Germany. He was offered the position as prince-elector of Mainz, but, after serious considerations, the offer was turned down.
When King Gustavus died in November 1632, his only legitimate and surviving child, Christina, was almost six years old. Until her declaration of majority at 18, a regency council ruled Sweden. This council was headed by Lord High Chancellor Oxenstierna, who wrote Instrument of Government (1634), a new constitution. During the years after the king's death, it became apparent that differences of opinion existed within the council. Some of Oxenstierna's colleagues recommended that Sweden should seek peace and withdraw from the war in Germany, not least after the defeat at Nördlingen in 1634. However, Oxenstierna's opinion, that Sweden should remain in the war to ensure compensation for the sacrifices made, prevailed. The, for the Swedish side, disastrous outcome at Nördlingen brought him, for an instant, to the verge of ruin and compelled him for the first time so far to depart from his policy of independence as to solicit direct assistance from France. But, well aware that Richelieu needed the Swedish armies as much as he himself needed money, he refused at the Conference of Compiègne in 1635 to bind his hands in the future for the sake of some slight present relief. In 1636, nevertheless, he concluded a fresh subsidy-treaty with France at Wismar. Swedish troops remained in Germany all the way until 1648 and the Thirty Years' War's end. Oxenstierna, however, left Germany and returned to Stockholm in 1636, after ten years duty as premier Swedish representative in Prussia and Germany.
1636–1654: Back in Sweden.
Oxenstierna more directly claimed his place within the regency of Queen Christina and became the young queen's teacher in statesmanship. His presence at home dominated all opposition, and such was the general confidence for Oxenstierna, that for the next nine years his voice, especially as regarding foreign affairs, remained omnipotent in the Privy Council.
The Torstenson War.
In May 1643, the Swedish Privy Council decided to attack Denmark. The Torstenson War was at large parts the work of Oxenstierna. The purpose was to gain territories from Denmark and be released from the Danish Sound Dues. Other factors might have been a will to revenge the tough peace treaty of Knäred in 1613. Whatever the reason, Oxenstierna considered the time was right to finally settle the score with Denmark. Swedish troops led by Field Marshal Lennart Torstensson attacked Danish Jutland from Germany, while Field Marshal Gustav Horn was in charge of the troops that attacked Scania. The outcome of the war was decided in the naval battle of Fehmarn Belt in 1644 where the Royal Swedish Navy decisively defeated the Danish Navy. The defeat of the Danish Navy left the Danish isles open to a Swedish invasion, and Denmark sued for peace. Oxenstierna was personally involved in the negotiations leading to the Treaty of Brömsebro, with which Sweden gained Gotland, Saaremaa (Ösel), Jämtland, Härjedalen and for thirty years Halland. Shortly after the peace treaty, Oxenstierna was created Count of Södermöre.
Queen Christina and her abdication.
When Christina came of age, she tried to push Oxenstierna, her old mentor, aside. The relations between the two were not good and Oxenstierna always attributed the exiguousness of Sweden's gains by the Peace of Westphalia following the conference in Osnabrück to Christina's undue interference, which merely gave Sweden Pomerania, Usedom, Wollin, Wismar and Bremen-Verden. When the queen a few years later wanted to abdicate, Oxenstierna at first opposed this because he feared mischief to Sweden from the unruly and adventurous disposition of her appointed successor, Charles X Gustav. The chancellor changed his mind about Charles Gustav, and decided to give Christina the help she needed to go through with her abdication. A couple of months after the ascent of the new king, Oxenstierna died.
Death.
Oxenstierna died in Stockholm on 28 August 1654. He was interred in Storkyrkan, Stockholm on 18 March 1655. His body was then moved to Jäders kyrka in Eskilstuna Municipality, where a vault had been built in accordance with his wishes. In the vault, "Oxenstiernska gravvalvet", several members of the Oxenstierna family have been buried, including Axel and his spouse Anna.
Personal life.
Family.
On 5 June 1608 Axel Oxenstierna married Anna Åkesdotter Bååt, the daughter of nobleman Åke Johansson Bååt and Christina Trolle. The wedding took place at Fiholm Castle, owned by the Oxenstierna family. They had 13 children, of which five survived their childhoods:
Axel Oxenstierna's wife Anna died in 1649.
Properties.
Oxenstierna was in possession of large estates and many mansions. During his life he owned palaces in, among others, Estonian Otepää, in Latvian Burtnieki, Ropaži and Valmiera, in Finnish Nousiainen (Nousis) and in Stockholm (Oxenstiernska Palace). The foremost of the mansions was Tidö Palace in Västmanland.
Impact and legacy.
The modernization of Sweden.
Axel Oxenstierna is perhaps most remembered for the establishment of a uniform administrative system. He was ever-present during the vast reforms of the 1610s and 1620s, when the Swedish government was hugely modernized and made more effective. This was necessary for the war policies that would build the Swedish Empire. Among the areas reformed were army and navy organization and recruiting, trade and industrial policies, regional and local administration, the system of higher education, and the judicial system.
Relationship with King Gustavus Adolphus.
Oxenstierna would not have had such an impact unless he had won the king's trust. From 1612, when Oxenstierna was appointed Lord High Chancellor, until 1632, when King Gustavus Adolphus died, the two men struck a long and successful partnership. They seem to have complemented each other. With Oxenstierna's own words, his "cool" balanced the king's "heat". More than once, the chancellor had to realize plans of the king, plans that sometimes were highly spontaneous and far from ready to be implemented in reality. When it came to entering the Thirty Years' War, Oxenstierna was not as enthusiastic as the king, but since the king's will was decisive, Oxenstierna accommodated himself to Gustavus's wish. At times, Oxenstierna stepped in to ease tense relations that the harsh behaviour of the king had caused. He regularly received the highest praise for his work from the king and there was almost no area in which King Gustavus did not consult his Lord High Chancellor Oxenstierna.
The mind behind the Instrument of Government of 1634.
The Chancellor made large contributions to the Standing orders of the House of Knights ("riddarhusordning") of 1626. After the death of Gustavus Adolphus, Oxenstierna was the mind behind the Instrument of Government of 1634, in which, for example, the organization of the five Great Officers of the Realm was clarified. Five governmental branches, of which the Great Officers became heads, were established. Oxenstierna pushed through the Instrument of Government, but not without opposition. He claimed that the new form of government reflected the will of the late King Gustavus, making himself the interpreter of the king's thoughts and wishes, and leaving the opposition no possibility to control the truth in this.
Opinions.
Oxenstierna is regarded as a brilliant pragmatist, willing to reconsider his positions. There are examples of discussions within the Privy Council when Oxenstierna rejected laws he himself had earlier introduced, admitting that he knew better now. His way of examining, reconsidering, testing, and sometimes rejecting his earlier opinions constitutes his legacy more than his ideas on particular points of policy.
When he discovered that there were too few young noblemen to staff governmental positions, he worked to make it easier for boys outside the noble families to gain higher education, and gave them the possibility, eventually, to be raised to the nobility themselves. He could therefore be considered the father of Swedish meritocracy.
Oxenstierna was also a supporter of mercantilism and a believer in immigration and free enterprise.
In Germany, Oxenstierna became a fear-evoking character in a derived version of the popular German lullaby , in which he is referred to as "Ochsenstern".
Opinions about Oxenstierna.
Dutch jurist and philosopher Hugo Grotius considered Oxenstierna "the greatest man of the century". French Cardinal Richelieu called him "an inexhaustible source of fine advice", while Richelieu's successor, Cardinal Mazarin, said that if all ministers of Europe were on the same ship, the helm would be handed to Oxenstierna. Pope Urban VIII claimed that Oxenstierna was one of the most excellent men the world had seen.
Quotation.
"Do you not know, my son, with how little wisdom the world is governed?" (in a letter to his son Johan written in 1648, in the original Latin "An nescis, mi fili, quantilla prudentia mundus regatur"?). Although attributed to Cardinal Richelieu as well, this is probably the most famous Swedish quotation in the English-speaking world. The words were intended to encourage his son, a delegate to the negotiations that would lead to the Peace of Westphalia, who worried about his ability to hold his own amidst experienced and eminent statesmen and diplomats.

</doc>
<doc id="49443" url="http://en.wikipedia.org/wiki?curid=49443" title="Religious humanism">
Religious humanism

Religious humanism is an integration of humanist ethical philosophy with religious rituals and beliefs that center on human needs, interests, and abilities.
Origins.
Humanism as it was conceived in the early 20th century rejected revealed knowledge, theism-based morality and the supernatural. In the late 20th century the Humanist movement that affirms the dignity and worth of all people came into conflict with conservative Christian groups in the United States and "Secular Humanism" became the most popular element of organized Humanism. Though practitioners of religious humanism did not officially organize under the name of "humanism" until the late 19th and early 20th centuries, non-theistic religions paired with human-centered ethical philosophy date to the Enlightenment era.
French Revolution.
The Cult of Reason (French: "Culte de la Raison") was a religion based on deism devised during the French Revolution by Jacques Hébert, Pierre Gaspard Chaumette and their supporters.
In 1793 during the French Revolution, the cathedral Notre Dame de Paris was turned into a Temple to Reason and for a time Lady Liberty replaced the Virgin Mary on several altars.
Positivism.
In the 1850s, Auguste Comte, the Father of Sociology, founded Positivism, a "religion of humanity". Auguste Comte was a student and secretary for Claude Henri de Rouvroy, Comte de Saint-Simon, the Father of French Socialism. Auguste Comte coined the term "altruism".
Humanistic Religious Association.
One of the earliest forerunners of contemporary chartered humanist organizations was the Humanistic Religious Association formed in 1853 in London. This early group was democratically organized, with male and female members participating in the election of the leadership and promoted knowledge of the sciences, philosophy, and the arts.
Ethical Culture.
The Ethical Culture movement was founded in 1876. The movement's founder, Felix Adler, a former member of the Free Religious Association, conceived of Ethical Culture as a new religion that would strip away the accumulated unscientific dogmas of traditional religions while retaining and elevating the ethical message at the heart of all religions. Adler believed that traditional religions would ultimately prove to be incompatible with a scientific worldview. He felt that the vital aspects of religion should not be allowed to fall by the wayside. Religions provided vital functions in encouraging good works. And religions taught important truths about the world, albeit these truths were expressed through metaphors that were not always suited to modern understandings of the world. For example, monotheistic religions were based on a metaphor of an authoritarian monarchy, whereas democratic relationships were now understood to be the ideal.
Initially, Ethical Culture involved little in the way of ceremony and ritual. Rather, Ethical Culture was religious in the sense of playing a defining role in people's lives and addressing issues of ultimate concern. Some Ethical Societies have subsequently added a degree of ritual as a means of marking special times or providing a tangible reminder of humanistic ideals.
United States.
Before the term "humanism" was ever coined or even thought of being integrated into religion it had existed in America in at least an ideological sense for a very long time. Groups like the Free Religious Association (FRA) which was formed in 1867 and other less radical groups mainly consisting of extreme forms of early American Protestants such as the Unitarians and Quakers had existed from the very first landings of the Europeans in the Western Hemisphere. In 1915, a Positivist defined the term "humanism" in a magazine for the British Ethical Societies. Another Unitarian Minister John H. Dietrich read the magazine and adopted the term to describe his own religion. Dietrich is considered by some to be the "Father of Religious Humanism" (Olds 1996) particularly for his sermons while serving the First Unitarian Society of Minneapolis.
In 1929 Charles Francis Potter founded the First Humanist Society of New York whose advisory board included Julian Huxley, John Dewey, Albert Einstein and Thomas Mann. Potter was a minister from the Unitarian tradition and in 1930 he and his wife, Clara Cook Potter, published "". Throughout the 1930s Potter was a well known advocate of women’s rights, access to birth control, "civil divorce laws", and an end to capital punishment.
A Humanist Manifesto, also known as Humanist Manifesto I to distinguish it from later Humanist Manifestos, was written in 1933 primarily by Raymond Bragg and was published with thirty-four signatories. Unlike the later ones, the first manifesto talked of a new "religion", and referred to humanism as a religious movement meant to transcend and replace previous, deity-based religions. However, it is careful not to outline a creed or dogma. The document outlines a fifteen-point belief system, which, in addition to a secular outlook, opposes "acquisitive and profit-motivated society" and outlines a worldwide egalitarian society based on voluntary mutual cooperation.
The Fellowship of Humanity was founded in 1935 by Reverend A. D. Faupel as one of a handful of "humanist churches" seeded in the early 20th century as part of the American Religious Humanism movement. It was the only such organization to survive into the 21st century and is the first and oldest affiliate of the American Humanist Association.
In 1961, Webster's Third New International Unabridged Dictionary defined religious humanism as "A modern American movement composed chiefly of non-theistic humanists and humanist churches and dedicated to achieving the ethical goals of religion without beliefs and rites resting upon superstition."
American Religious Humanist organizations that have survived into the 21st century include the HUUmanists, formerly the Friends of Religious Humanism, and the Humanist Society, formerly the Humanist Society of Friends.
A declining number of members of Unitarian Universalist congregations today identify themselves as humanists. The UU Humanist Association is the main representation of religious humanism within the Unitarian Universalist Association.
Related or similar traditions.
Some distinguish religious humanism from Jewish humanism, Christian humanism, and secular humanism.
In the past, humanist versions of major religions, such as Christian humanism, have arisen. In addition, many Indian religions like Hinduism, Buddhism and other Asian religions and belief systems like Confucianism, Taoism, Mohism, Shenism, and Zoroastrianism, that focus on human nature and action more than theology, were always primarily humanistic. Currently, however, humanism is dominated almost exclusively by secular humanism. This has given rise to a newer version of humanist religions which are similar in philosophy to secular humanism. Secular humanists and revealed religious humanists primarily differ in their definition of religion and their positions on supernatural beliefs. They can also diverge in practice since religious humanists endorse religious ceremonies, rituals, and rites.
Buddhist approaches.
The humanist approach to Buddhism shares the fundamental principle of analysing and evaluating the tradition according to natural, human values, but the particular interpretations and results various Buddhist humanists come up with will naturally vary. An early exponent, U Dhammaloka, combined western freethought and atheist positions with orthodox Burmese ritual practice and a strong critique of missionary theism. Most Buddhist groups are more or less humanistic anyway, but there is also a particular modern Chinese Buddhist organisation that calls itself 'Humanistic Buddhism'.
The teachings of the modern Chinese Buddhist thought of Humanistic Buddhism encompass all of the Buddhist teachings from the time of Gautama Buddha to the present. The goal of Humanistic Buddhism is the bodhisattva way, which means to be an energetic, enlightened, and endearing person who strives to help all sentient beings liberate themselves. Humanistic Buddhism focuses more on issues of the world rather than on how to leave the world behind; on caring for the living, rather than the dead; on benefiting others, rather than benefiting oneself; and on universal salvation, rather than salvation for only oneself.
Other Buddhist scholars are exploring a humanist method of analysis and evaluation of the Buddha's teachings based exclusively on the pre-sectarian early texts, which were probably mainly composed pre-300BCE. The focus of this form of humanistic Buddhism is analysis of the implicit authority theories contained in the different stages of evolution of Buddhist tradition, and critiquing the misunderstanding and misuse of religious 'authority' to justify abuse of individuals. It also re-emphasises value-pluralism, which is a humanistic way of reasoning about ethics.
Abrahamically-derived approaches.
Another approach, Christian Existential Humanism, related to the work of the Danish philosopher Søren Kierkegaard, features a humanist perspective grounded in Christian religious belief; where humanity is something to be excited about, but not as a replacement for the divine.
Many medieval Muslim thinkers pursued humanistic, rational and scientific discourses in their search for knowledge, meaning and values. A wide range of Islamic writings on love poetry, history and philosophical theology show that medieval Islamic thought was open to the humanistic ideas of individualism, occasional secularism, skepticism and liberalism. Certain aspects of Renaissance humanism has its roots in the medieval Islamic world, including the "art of "dictation", called in Latin, "ars dictaminis"," and "the humanist attitude toward classical language."
Humanistic Judaism is a movement that holds that Jewish culture and Jewish history, rather than religion, are the source of Jewish identity.
Humanistic Mormonism is a movement and a form of religious humanism that holds that Mormon history, Mormon culture, and those who self-identity as Mormons based on their personal life experiences rather than religion, are the key sources of Mormon identity.

</doc>
<doc id="49444" url="http://en.wikipedia.org/wiki?curid=49444" title="Racial quota">
Racial quota

Racial quotas in employment and education are numerical requirements for hiring, promoting, admitting and/or graduating members of a particular racial group. Racial quotas are often established as means of diminishing racial discrimination, addressing under-representation and evident racism against those racial groups. However, it has been argued that such quotas are in themselves a form of racial discrimination; and therefore they are a contentious subject.
These quotas may be determined by governmental authority and backed by governmental sanctions. When the total number of jobs or enrollment slots is fixed, this proportion may get translated to a specific number. In education, this kind of quota is also known as "Numerus clausus".
History.
Racial quotas in the United States began to be implemented with government approval after the Civil Rights Act of 1964, especially during the 1970s. Richard Nixon's Labor Secretary George P. Schultz demanded that anti-black construction unions allow a certain number of black people into the unions. The Department of Labor began enforcing these quotas across the country. After a Supreme Court case, "Griggs v. Duke Power Company", found that neutral application tests and procedures that still resulted in "de facto" segregation of employees (if previous discrimination had existed) were illegal, more companies began implementing quotas on their own.
In a 1973 court case, a federal judge created one of the first mandated quotas when he ruled that half of the Bridgeport, Connecticut Police Department's new employees must be either black or Puerto Rican. In 1974, the Department of Justice and the United Steelworkers of America came to an agreement on the largest-to-then quota program, for steel unions.
In 1978, the Supreme Court ruled in "Regents of the University of California v. Bakke" that public universities (and other government institutions) could not set specific numerical targets based on race for admissions or employment. The Court said that "goals" and "timetables" for diversity could be set instead. A 1979 Supreme Court case, "United Steelworkers v. Weber", found that private employers could set rigid numerical quotas, if they chose to do so. In 1980, the Supreme Court found that a 10% racial quota for federal contractors was permitted.
Then in 1991, President George H. W. Bush made an attempt to abolish affirmative action altogether, maintaining that “any regulation, rule, enforcement practice or other aspect of these programs that mandates, encourages, or otherwise involves the use of quotas, preferences, set-asides or other devices on the basis of race, sex, religion or national origin are to be terminated as soon as is legally feasible." This claim led up to the creation of the Civil Rights Act of 1991, however the document was not able to implement these changes. It only covered the terms for settling cases where discrimination has been confirmed to have occurred.
Opposition.
Opponents of quotas object that one group is favored at the expense of another whenever a quota is invoked rather than factors such as grade point averages or test scores. They argue that using quotas displaces individuals that would normally be favored based on their individual achievements. Opponents of racial quotas believe that qualifications should be the only determining factor when competing for a job or admission to a school. It is argued this causes "reverse discrimination" where individuals in the majority to lose out to a minority. Another critique of racial quotas is the process of reevaluating quota percentages after changes of racial ratios in a society.
Alternatives.
Advocates of affirmative action programs often deny that these programs involve quotas, although some openly do, such as the admission program of the Universidade Federal do Rio Grande do Sul. Advocates may regard the term "racial quotas" as particularly divisive in that it is assumed to be backed by the force of law to enable or disable certain linked programs or benefits based solely upon attainment of the one quota measure.
The law student organization Building a Better Legal Profession has developed a method to encourage politically liberal students to avoid law firms whose racial makeup is markedly different than that of the population as a whole. In an October 2007 press conference reported in "The Wall Street Journal", and the New York Times the group released data publicizing the numbers of African-Americans, Hispanics, and Asian-Americans at America's top law firms. The group has sent information to top law schools around the country, encouraging students who agree with this viewpoint to take the demographic data into account when choosing where to work after graduation. As more students choose where to work based on the firms' diversity rankings, firms face an increasing market pressure to change theirs.
See also.
Examples:
Related:

</doc>
<doc id="49448" url="http://en.wikipedia.org/wiki?curid=49448" title="Horus">
Horus

Horus is one of the oldest and most significant deities in ancient Egyptian religion, who was worshipped from at least the late Predynastic period through to Greco-Roman times. Different forms of Horus are recorded in history and these are treated as distinct gods by Egypt specialists. These various forms may possibly be different perceptions of the same multi-layered deity in which certain attributes or syncretic relationships are emphasized, not necessarily in opposition but complementary to one another, consistent with how the Ancient Egyptians viewed the multiple facets of reality. He was most often depicted as a falcon, most likely a lanner or peregrine, or as a man with a falcon head.
The earliest recorded form of Horus is the patron deity of Nekhen in Upper Egypt, who is the first known national god, specifically related to the king who in time came to be regarded as a manifestation of Horus in life and Osiris in death. The most commonly encountered family relationship describes Horus as the son of Isis and Osiris but in another tradition Hathor is regarded as his mother and sometimes as his wife. Horus served many functions in the Egyptian pantheon, most notably being a god of the sun, war and protection.
Etymology.
Horus is recorded in Egyptian hieroglyphs as ḥr.w; the pronunciation has been reconstructed as *Ḥāru, meaning "falcon". Additional meanings are thought to have been "the distant one" or "one who is above, over". By Coptic times, the name became "Hōr". It was adopted into Greek as Ὧρος "Hōros". The original name also survives in later Egyptian names such as Har-si-ese literally "Horus, son of Isis".
Horus was also known as "Nekheny", meaning "falcon". Some have proposed that Nekheny may have been another falcon-god, worshipped at Nekhen (city of the falcon), with which Horus was identified from early on. Horus may be shown as a falcon on the Narmer Palette dating from about the 31st century BC.
Note of changes over time.
In early Egypt, Horus was the brother of Isis, Osiris, Set and Nephthys. As different cults formed, he became the son of Isis and Osiris. Isis remained the sister of Osiris, Set and Nephthys.
Horus and the pharaoh.
Pyramid texts ca. 2400–2300 BC describe the nature of the Pharaoh in different characters as both Horus and Osiris. The Pharaoh as Horus in life became the Pharaoh as Osiris in death, where he was united with the rest of the gods. New incarnations of Horus succeeded the deceased pharaoh on earth in the form of new Pharaohs.
The lineage of Horus, the eventual product of unions between the children of Atum, may have been a means to explain and justify Pharaonic power; The gods produced by Atum were all representative of cosmic and terrestrial forces in Egyptian life; by identifying Horus as the offspring of these forces, then identifying him with Atum himself, and finally identifying the Pharaoh with Horus, the Pharaoh theologically had dominion over all the world.
The notion of Horus as the Pharaoh seems to have been superseded by the concept of the Pharaoh as the son of Ra during the Fifth Dynasty of Egypt.
Origin mythology.
Horus was born to the goddess Isis after she retrieved all the dismembered body parts of her murdered husband Osiris, except his penis which was thrown into the Nile and eaten by a catfish, or sometimes by a crab, and according to Plutarch's account (see Osiris) used her magic powers to resurrect Osiris and fashion a golden phallus to conceive her son (older Egyptian accounts have the penis of Osiris surviving).
Once Isis knew she was pregnant with Horus, she fled to the Nile Delta marshlands to hide from her brother Set who jealously killed Osiris and who she knew would want to kill their son. There Isis bore a divine son, Horus.
Mythological roles.
Sky god.
Since Horus was said to be the sky, he was considered to also contain the sun and moon. It became said that the sun was his right eye and the moon his left, and that they traversed the sky when he, a falcon, flew across it. Later, the reason that the moon was not as bright as the sun was explained by a tale, known as the "The Contendings of Horus and Seth". In this tale, it was said that Set, the patron of Upper Egypt, and Horus, the patron of Lower Egypt, had battled for Egypt brutally, with neither side victorious, until eventually the gods sided with Horus.
As Horus was the ultimate victor he became known as Harsiesis, Heru-ur or Har-Wer (ḥr.w wr 'Horus the Great'), but more usually translated as Horus the Elder. In the struggle Set had lost a testicle, explaining why the desert, which Set represented, is infertile. Horus' left eye had also been gouged out, then a new eye was created by part of Khonsu, the moon god, and was replaced.
Horus was occasionally shown in art as a naked boy with a finger in his mouth sitting on a lotus with his mother. In the form of a youth, Horus was referred to as Neferhor. This is also spelled Nefer Hor, Nephoros or Nopheros (nfr ḥr.w) meaning 'The Good Horus'.
The Eye of Horus is an ancient Egyptian symbol of protection and royal power from deities, in this case from Horus or Ra. The symbol is seen on images of Horus' mother, Isis, and on other deities associated with her.
In the Egyptian language, the word for this symbol was "Wedjat". It was the eye of one of the earliest of Egyptian deities, Wadjet, who later became associated with Bast, Mut, and Hathor as well. Wedjat was a solar deity and this symbol began as her eye, an all seeing eye. In early artwork, Hathor is also depicted with this eye. Funerary amulets were often made in the shape of the Eye of Horus. The Wedjat or Eye of Horus is "the central element" of seven "gold, faience, carnelian and lapis lazuli" bracelets found on the mummy of Shoshenq II. The Wedjat "was intended to protect the king [here] in the afterlife" and to ward off evil. Ancient Egyptian and Near Eastern sailors would frequently paint the symbol on the bow of their vessel to ensure safe sea travel.
God of war and hunting.
Horus was also said to be a god of war and hunting. The Horus falcon is shown upon a standard on the predynastic Hunters Palette in the "lion hunt".
Thus he became a symbol of majesty and power as well as the model of the pharaohs. The Pharaohs were said to be Horus in human form.
Furthermore Nemty, another war god, was later identified as Horus.
Conflict between Horus and Set.
Horus was told by his mother, Isis, to protect the people of Egypt from Set, the god of the desert, who had killed his father Osiris.
Horus had many battles with Set, not only to avenge his father, but to choose the rightful ruler of Egypt. In these battles, Horus came to be associated with Lower Egypt, and became its patron.
According to Papyrus Chester-Beatty I, Set is depicted as trying to prove his dominance by seducing Horus and then having intercourse with him. However, Horus places his hand between his thighs and catches Set's semen, then subsequently throws it in the river, so that he may not be said to have been inseminated by Set. Horus then deliberately spreads his own semen on some lettuce, which was Set's favorite food. After Set had eaten the lettuce, they went to the gods to try to settle the argument over the rule of Egypt. The gods first listened to Set's claim of dominance over Horus, and call his semen forth, but it answered from the river, invalidating his claim. Then, the gods listened to Horus' claim of having dominated Set, and call his semen forth, and it answered from inside Set.
However, Set still refused to relent, and the other gods were getting tired from over eighty years of fighting and challenges. Horus and Set challenged each other to a boat race, where they each raced in a boat made of stone. Horus and Set agreed, and the race started. But Horus had an edge: his boat was made of wood painted to resemble stone, rather than true stone. Set's boat, being made of heavy stone, sank, but Horus's did not. Horus then won the race, and Set stepped down and officially gave Horus the throne of Egypt. But after the New Kingdom, Set still was considered Lord of the desert and its oases.
This myth, along with others, could be seen as an explanation of how the two kingdoms of Egypt (Upper and Lower) came to be united. Horus was seen as the God of Upper Egypt, and Set as the God of Lower Egypt. In this myth, the respective Upper and Lower deities have a fight, through which Horus is the victor. However, some of Horus (representing Upper Egypt) enters into Set (Lower Egypt) thus explaining why Upper Egypt is dominant over Lower Egypt. Set's regions were then considered to be of the desert.
Heru-pa-khered (Horus the Younger).
Horus the Younger, Harpocrates to the Ptolemaic Greeks, is represented in the form of a youth wearing a lock of hair (a sign of youth) on the right of his head while sucking his finger. In addition, he usually wears the united crowns of Egypt, the crown of Upper Egypt and the crown of Lower Egypt. He is a form of the rising sun, representing its earliest light.
Her-ur (Horus the Elder).
In this form he represented the god of light and the husband of Hathor. He was one of the oldest gods of ancient Egypt. He became the patron of Nekhen (Hierakonpolis) and the first national god (God of the Kingdom). Later, he also became the patron of the pharaohs, and was called the son of truth. – signifying his role as an important upholder of Maat. He was seen as a great falcon with outstretched wings whose right eye was the sun and the left one was the moon. In this form, he was sometimes given the title Kemwer, meaning "(the) great black (one)".
The Greek form of Her-ur (or Har wer) is Haroeris. Other variants include Hor Merti 'Horus of the two eyes' and Horkhenti Irti.

</doc>
<doc id="49451" url="http://en.wikipedia.org/wiki?curid=49451" title="Bayeux Tapestry">
Bayeux Tapestry

The Bayeux Tapestry (French: "Tapisserie de Bayeux", ], Norman: "La telle du conquest") is an embroidered cloth nearly 70 m long and 50 cm tall, which depicts the events leading up to the Norman conquest of England concerning William, Duke of Normandy, and Harold, Earl of Wessex, later King of England, and culminating in the Battle of Hastings.
According to Sylvette Lemagnen, conservator of the tapestry,
The Bayeux tapestry is one of the supreme achievements of the Norman Romanesque ... Its survival almost intact over nine centuries is little short of miraculous ... Its exceptional length, the harmony and freshness of its colors, its exquisite workmanship, and the genius of its guiding spirit combine to make it endlessly fascinating.
The tapestry consists of some fifty scenes with Latin "tituli", embroidered on linen with coloured woollen yarns. It is likely that it was commissioned by Bishop Odo, William's half-brother, and made in England—not Bayeux—in the 1070s. In 1729 the hanging was rediscovered by scholars at a time when it was being displayed annually in Bayeux Cathedral. The tapestry is now exhibited at the Musée de la Tapisserie de Bayeux in Bayeux, Normandy, France ().
The designs on the Bayeux Tapestry are embroidered rather than woven, so that it is not technically a tapestry. Nevertheless, it is always referred to as such.
Origins.
The earliest known written reference to the tapestry is a 1476 inventory of Bayeux Cathedral, but its origins have been the subject of much speculation and controversy.
French legend maintained the tapestry was commissioned and created by Queen Matilda, William the Conqueror's wife, and her ladies-in-waiting. Indeed, in France it is occasionally known as "La Tapisserie de la Reine Mathilde" (Tapestry of Queen Matilda). However, scholarly analysis in the 20th century concluded it was probably commissioned by William's half-brother, Bishop Odo, who, after the Conquest, became Earl of Kent and, when William was absent in Normandy, regent of England. 
The reasons for the Odo commission theory include: 1) three of the bishop's followers mentioned in the "Domesday Book" appear on the tapestry; 2) it was found in Bayeux Cathedral, built by Odo; and 3) it may have been commissioned at the same time as the cathedral's construction in the 1070s, possibly completed by 1077 in time for display on the cathedral's dedication.
Assuming Odo commissioned the tapestry, it was probably designed and constructed in England by Anglo-Saxon artists (Odo's main power base being by then in Kent); the Latin text contains hints of Anglo-Saxon; other embroideries originate from England at this time; and the vegetable dyes can be found in cloth traditionally woven there. The actual physical work of stitching was most likely undertaken by skilled seamsters. Anglo-Saxon needlework, or Opus Anglicanum, was famous across Europe.
Alternative theories exist. Carola Hicks has suggested it could possibly have been commissioned by Edith of Wessex; and Howard B. Clarke has proposed that the designer of the tapestry was Scolland, the abbot of St Augustine's Abbey, because of his previous position as head of the scriptorium at Mont Saint-Michel (famed for its illumination), his travels to Trajan's Column, and his connections to Wadard and Vital, two individuals identified in the tapestry. Wolfgang Grape has challenged the consensus that the embroidery is Anglo-Saxon, distinguishing between Anglo-Saxon and other Northern European techniques; Medieval material authority Elizabeth Coatsworth contradicted this: "The attempt to distinguish Anglo-Saxon from other Northern European embroideries before 1100 on the grounds of technique cannot be upheld on the basis of present knowledge." George Beech suggests the tapestry was executed at the Abbey of St. Florent in the Loire Valley, and says the detailed depiction of the Breton campaign argues for additional sources in France. Andrew Bridgeford has suggested that the tapestry was actually of English design and encoded with secret messages meant to undermine Norman rule.
Construction, design and technique.
In common with other embroidered hangings of the early medieval period, this piece is conventionally referred to as a "tapestry", although it is not a true tapestry in which the design is woven into the cloth; it is in fact an embroidery.
The Bayeux tapestry is embroidered in crewel (wool yarn) on a tabby-woven linen ground 68.38 metres long and 0.5 metres wide (68.38 x) and using two methods of stitching: outline or stem stitch for lettering and the outlines of figures, and couching or laid work for filling in figures. Nine linen panels, between fourteen and three metres in length, were sewn together after each was embroidered and the joins were disguised with subsequent embroidery. At the first join (start of scene 14) the borders do not line up properly but the technique was improved so that the later joins are practically invisible. The design involved a broad central zone with narrow decorative borders top and bottom. By inspecting the woollen threads behind the linen it is apparent all these aspects were embroidered together at a session and the awkward placing of the "tituli" is not due to them being added later. Later generations have patched the hanging in numerous places and some of the embroidery (especially in the final scene) has been reworked. The tapestry may well have maintained much of its original appearance—it now compares closely with a careful drawing made in 1730.
The main yarn colours are terracotta or russet, blue-green, dull gold, olive green, and blue, with small amounts of dark blue or black and sage green. Later repairs are worked in light yellow, orange, and light greens. Laid yarns are couched in place with yarn of the same or contrasting colour.
The tapestry's central zone contains most of the action, which sometimes overflows into the borders either for dramatic effect or because depictions would otherwise be very cramped (for example at Edward's death scene). Events take place in a long series of scenes which are generally separated by highly stylised trees. However, the trees are not placed consistently and the greatest scene shift, between Harold's audience with Edward after his return to England and Edward's burial scene, is not marked in any way at all.
The "tituli" are normally in the central zone but occasionally use the top border. The borders are otherwise mostly purely decorative and only sometimes does the decoration complement the action in the central zone. The decoration consists of birds, beasts, fish and scenes from fables, agriculture and hunting. There are frequent oblique bands separating the vignettes. There are nude figures, some of corpses from battle, others of a ribald nature. A harrow, a newly invented implement, is depicted (scene 10) and this is the earliest known depiction. The picture of Halley's Comet, which appears in the upper border (scene 32), is the first known picture of this comet.
The end of the tapestry has been missing from time immemorial and the final "titulus" "Et fuga verterunt Angli" ("and the English left fleeing") is said to be "entirely spurious", added shortly before 1814 at a time of anti-English sentiment. Musset speculates the hanging was originally about 1.5 metres longer. At the last section still remaining the embroidery has been almost completely restored but this seems to have been done with at least some regard to the original stitching. The stylised tree is quite unlike any other tree in the tapestry. The start of the tapestry has also been restored but to a much lesser extent
In 1724 a linen backing cloth was sewn on comparatively crudely and, in around the year 1800, large ink numerals were written on the backing which broadly enumerate each scene and which are still commonly used for reference.
 The entire Bayeux Tapestry. Individual images of each scene are at Bayeux Tapestry tituli.
Background to the events depicted.
In a series of pictures supported by a written commentary the tapestry tells the story of the events of 1064–1066 culminating in the Battle of Hastings. The two main protagonists are Harold Godwinson, recently crowned King of England, leading the Anglo-Saxon English, and William, Duke of Normandy, leading a mainly Norman army, sometimes called the companions of William the Conqueror.
William was the illegitimate son of Robert the Magnificent, Duke of Normandy, and Herleva (or Arlette), a tanner's daughter. William became Duke of Normandy at the age of seven and was in control of Normandy by the age of nineteen. His half brother was Bishop Odo of Bayeux.
King Edward the Confessor, king of England and about sixty years old at the time the tapestry starts its narration, had no children or any clear successor. Edward's mother, Emma of Normandy, was William's great aunt. At that time succession to the English throne was not by primogeniture but was decided jointly by the king and by an assembly of nobility, the Witenagemot.
Harold Godwinson, earl of Wessex and the most powerful noble in England, was Edward's brother-in-law. The Norman chronicler William of Poitiers recorded that Edward sent Harold to tell William that Edward had decided William should succeed him as king of England upon his (Edward's) death. However, other sources dispute this claim.
Events depicted in the tapestry.
The tapestry begins with a panel of Edward the Confessor sending Harold to Normandy.(scene 1) Later Norman sources say that the mission was for Harold to pledge loyalty to William but the tapestry does not suggest any specific purpose. By mischance, Harold arrives at the wrong location in France and is taken prisoner by Guy, Count of Ponthieu.(scene 7) After exchanges of messages borne by mounted messengers, Harold is released to William who then invites Harold to come on a campaign against Conan II, Duke of Brittany. On the way, just outside the monastery of Mont Saint-Michel, the army become mired in quicksand and Harold saves two Norman soldiers.(scene 17) William's army chases Conan from Dol de Bretagne to Rennes, and Conan finally surrenders at Dinan.(scene 20) William gives Harold arms and armour (possibly knighting him) and Harold takes an oath on saintly relics.(scene 23) Although the writing on the tapestry explicitly states an oath is taken there is no clue as to what is being promised.
Harold leaves for home and meets again with the old king Edward, who appears to be remonstrating with him.(scene 25) Harold is in a somewhat submissive posture and seems to be in disgrace. However, possibly deliberately, the king's intentions are not made clear. The scene then shifts by about one year to when Edward has become mortally ill and the tapestry strongly suggests that, on his deathbed, he bequeaths the crown to Harold. What is probably the coronation ceremony is attended by Stigand, whose position as Archbishop of Canterbury was controversial.(scene 31) Stigand is performing a liturgical function, possibly not the crowning itself. The tapestry labels the celebrant as "Stigant Archieps" (Stigand the archbishop) although by that time he had been excommunicated by the papacy who considered his appointment unlawful.
A star with a streaming tail then appears: Halley's Comet. Comets, in the beliefs of the Middle Ages, were a bad omen. At this point the lower border of the tapestry shows a fleet of ghost-like ships thus hinting at a future invasion.(scene 33) The news of Harold's coronation is taken to Normandy, whereupon we are told that William is ordering a fleet of ships to be built although it is Bishop Odo shown issuing the instructions.(scene 35) The invaders reach England, and land unopposed. William orders his men to find food, and a meal is cooked.(scene 43) A house is burnt, which may indicate some ravaging of the local countryside on the part of the invaders.(scene 47) News is brought to William. The Normans build a motte and bailey at Hastings to defend their position. Messengers are sent between the two armies, and William makes a speech to prepare his army for battle.(scene 51)
The Battle of Hastings was fought on 14 October 1066 less than three weeks after the Battle of Stamford Bridge but the tapestry does not provide this context. The English fight on foot behind a shield wall, whilst the Normans are on horses. Two fallen knights are named as Leofwine and Gyrth, Harold's brothers, but both armies are shown fighting bravely. Bishop Odo brandishes his baton or mace and rallies the Norman troops in battle.(scene 54) To reassure his knights that he is still alive and well, William raises his helmet to show his face. The battle becomes very bloody with troops being slaughtered and dismembered corpses littering the ground. King Harold is killed.(scene 57) This scene can be interpreted in different ways, as the name "Harold" appears above a number of knights, making it difficult to identify which character is Harold. The final remaining scene shows unarmoured English troops fleeing the battlefield. The last part of the tapestry is missing but it is thought that story contained only one additional scene.
Latin text.
"Tituli" are included on many scenes to point out names of people and places or to explain briefly the event being depicted. The text is in Latin but at times the style of words and spelling shows an English influence. A dark blue wool, almost black, is mostly used but towards the end of the tapestry other colours are used, sometimes for each word and other times for each letter. The complete text and English translation are displayed beside images of each scene at Bayeux Tapestry tituli.
History of the tapestry.
The first reference to the tapestry is from 1476 when it was listed in an inventory of the treasures of Bayeux Cathedral. It survived the sack of Bayeux by the Huguenots in 1562; and the next certain reference is from 1724. Antoine Lancelot sent a report to the "Académie Royale des Inscriptions et Belles-Lettres" concerning a sketch he had received about a work concerning William the Conqueror. He had no idea whether the original was a sculpture or painting though he mooted it could be a tapestry. Despite his further enquiries he discovered no more. However, the Benedictine scholar Bernard de Montfaucon made more successful investigations and found that the sketch was of a small portion of a tapestry preserved at Bayeux Cathedral.
In 1729 and 1730 he published drawings and a detailed description of the complete work in the first two volumes of his "Les Monuments de la Monarchie française". The drawings were by Antoine Benoît, one of the ablest draughtsmen of that time.
The tapestry was first briefly noted in English in 1743 by William Stukeley, in his "Palaeographia Britannica". The first detailed account in English was written by Smart Lethieullier, who was living in Paris in 1732–3, and was acquainted with Lancelot and de Montfaucon: it was not published, however, until 1767, as an appendix to Andrew Ducarel's "Anglo-Norman Antiquities".
During the French Revolution, in 1792, the tapestry was confiscated as public property to be used for covering military wagons. It was rescued from a wagon by a local lawyer who stored it in his house until the troubles were over, when he sent it to the city administrators for safekeeping. After the Terror the Fine Arts Commission, set up to safeguard national treasures, in 1803 required it to be removed to Paris for display at the Musée Napoléon. When Napoleon abandoned his planned invasion of Britain its propaganda value was lost and it was returned to Bayeux where the council displayed it on a winding apparatus of two cylinders. Despite scholars' concern that the tapestry was becoming damaged the council refused to return it to the Cathedral.
In 1816 the Society of Antiquaries of London commissioned its historical draughtsman, Charles Stothard, to visit Bayeux to make an accurate hand-coloured facsimile of the tapestry. His drawings were subsequently engraved by James Basire jr., and published by the Society in 1819–23. Stothard's images are still of value as a record of the tapestry as it was before 19th-century restoration.
By 1842 the tapestry was displayed in a special-purpose room in the Bibliothèque Publique. It required special storage in 1870 with the threatened invasion of Normandy in the Franco-Prussian War and again in 1939–1944 by the Ahnenerbe during the German Occupation of France and the Normandy landings. On 27 June 1944 the Gestapo took the tapestry to the Louvre and on 18 August, three days before the Wehrmacht withdrew from Paris, Himmler sent a message (intercepted by Bletchley Park) ordering it to be taken to "a place of safety", thought to be Berlin. It was only on 22 August that the SS attempted to take possession of the tapestry by which time the Louvre was again in French hands. After the liberation of Paris, on 25 August, the tapestry was again put on public display in the Louvre, and in 1945 it was returned to Bayeux where it is exhibited at "Musée de la Tapisserie de Bayeux".
Critical reception.
The inventory listing of 1476 shows that the tapestry was being hung annually in Bayeux Cathedral for the week of the Feast of St. John the Baptist; and this was still the case in 1728, although by that time the purpose was merely to air the hanging, which was otherwise stored in a chest. Clearly, the work was being well cared for. In the eighteenth century the artistry was regarded as crude or even barbarous—red and yellow multi-coloured horses upset some critics. It was thought to be unfinished because the linen was not covered with embroidery. However, its exhibition in the Louvre in 1797 caused a sensation, with "Le Moniteur", which normally dealt with foreign affairs, reporting on it on its first two pages. It inspired a popular musical, "La Tapisserie de la Reine Mathilde". It was because the tapestry was regarded as an antiquity rather than a work of art that in 1804 it was returned to Bayeux, where in 1823 one commentator, A. L. Léchaudé d'Anisy, reported that "there is a sort of purity in its primitive forms, especially considering the state of the arts in the eleventh century".
The tapestry was becoming a tourist attraction, with Robert Southey complaining of the need to queue to see the work. In the 1843 "Hand-book for Travellers in France" by John Murray III, a visit was included on "Recommended Route 26 (Caen to Cherbourg via Bayeux)", and this guidebook led John Ruskin to go there; he would describe the tapestry as "the most interesting thing in its way conceivable". Charles Dickens, however, was not impressed: "It is certainly the work of amateurs; very feeble amateurs at the beginning and very heedless some of them too."
During the Second World War Heinrich Himmler coveted the work, regarding it as "important for our glorious and cultured Germanic history".
Mysteries.
The tapestry contains several mysteries:
Reliability.
The Bayeux Tapestry was probably commissioned by the House of Normandy and essentially depicts a Norman viewpoint. However, Harold is shown as brave and his soldiers are not belittled. Throughout, William is described as "dux" (duke) whereas Harold, also called dux up to his coronation, is subsequently called "rex" (king). The fact that the narrative extensively covers Harold's activities in Normandy (in 1064) indicates that the intention was to show a strong relationship between that expedition and the Norman Conquest starting two years later. It is for this reason that the tapestry is generally seen by modern scholars as an apologia for the Norman Conquest.
The tapestry's narration seems to place stress on Harold's oath to William although its rationale is not made clear. Norman sources claimed that the English succession was being pledged to William but English sources gave varied accounts. Today it is thought the Norman sources are to be preferred. Both the tapestry and Norman sources named Stigand, the excommunicated Archbishop of Canterbury, as the man who crowned Harold, possibly to discredit Harold's kingship; English sources suggested that he was crowned by Ealdred, Archbishop of York and favoured by the papacy, making Harold's position as legitimate king more secure. Contemporary scholarship has not decided the matter although it is generally thought Ealdred performed the coronation.
While political propaganda or personal emphasis may have somewhat distorted the historical accuracy of the story, the Bayeux tapestry presents a unique visual document of medieval arms, apparel, and other objects unlike any other artifact surviving from this period. There is no attempt at continuity between scenes either for individuals' appearance or clothing. The knights carry shields, but show no system of hereditary coats of arms—the beginnings of modern heraldic structure were in place, but would not become standard until the middle of the 12th century. It has been noted that the warriors are depicted fighting with bare hands, while other sources indicate the general use of gloves in battle and hunt.
Artistic context.
Tapestry fragments have been found in Scandinavia dating from the ninth century and it is thought that Norman and Anglo-Saxon embroidery developed from this sort of work. Examples are to be found in the grave goods of the Oseberg ship and the Överhogdal tapestries.
A monastic text from Ely, the "Liber Eliensis", mentions a woven narrative wall-hanging commemorating the deeds of Byrhtnoth, killed in 991. Wall-hangings were common by the tenth century with English and Norman texts particularly commending the skill of Anglo-Saxon seamstresses. Mural paintings imitating draperies still exist in France and Italy and there are twelfth century mentions of other wall-hangings in Normandy and France. A poem by Baldric of Dol might even be describing the Bayeux Tapestry itself.
Therefore, the Bayeux Tapestry was not unique at the time it was created—rather it is remarkable for being the sole surviving example of Middle Ages' narrative needlework.
Replicas & continuations.
There are a number of replicas of the Bayeux Tapestry in existence. Through the collaboration of William Morris with textile manufacturer Thomas Wardle, Wardle's wife Elizabeth, who was an accomplished seamstress, embarked on creating a reproduction in 1885. She organised some 37 women in her "Leek School of Art Embroidery" to collaborate working from a full-scale water-colour facsimile drawing provided by the South Kensington Museum The full-size replica was finished in 1886 and is now exhibited in the Museum of Reading in Reading, Berkshire, England. The naked figure in the original tapestry (in the border below the Ælfgyva figure) is depicted wearing a brief garment because the drawing which was worked from was similarly bowdlerised.
Ray Dugan of University of Waterloo, Ontario, Canada, completed a stitched replica in 1996. Since its completion, it has been displayed in various museums and galleries in Canada and the United States.
Starting in 2000, the Bayeux Group, part of the Viking Group Lindholm Høje, has been making an accurate replica of the Bayeux Tapestry in Denmark, using the original sewing technique.
Dr. E. D. Wheeler, former judge and former dean at Oglethorpe University, commissioned a hand-painted, full-size replica of the Bayeux Tapestry and donated it to the University of West Georgia in Carrollton in 1994. In 2014, the replica was acquired by the University of North Georgia in Dahlonega.
An approximately half scale mosaic version of the Bayeux Tapestry is on display at Geraldine, New Zealand. The mosaic of 1.5 million pieces of spring steel was created by Michael Linton over a period of twenty years from 1979.
The 7 mm2 steel pieces are off-cuts from patterning disks of knitting machines and are affixed to a blackened background. The pieces have been enamel-painted one by one over several years in one of eight colors in keeping with the original tapestry's color scheme. The complete mosaic is displayed as 32 sections, weighing a total of 230 kg. The work includes a hypothetical reconstruction of the missing final section of the Tapestry up to William The Conqueror's coronation at Westminster Abbey on Christmas Day, 1066.
Similarly, other modern artists have attempted to complete the work by creating panels depicting subsequent events up to William's coronation, though the actual content of the missing panels is unknown. In 1997, the embroidery artist Jan Messent completed a reconstruction showing William accepting the surrender of English nobles at Berkhamsted ("Beorcham"), Hertfordshire, and his coronation. In early 2013, 416 residents of Alderney in the Channel Islands finished a continuation including William's coronation and the building of the Tower of London.
In popular culture.
Because it resembles a modern comic strip or movie storyboard, is widely recognised, and is so distinctive in its artistic style, the Bayeux Tapestry has frequently been used or reimagined in a variety of different popular culture contexts. It has been cited by Scott McCloud in "Understanding Comics" as an example of early narrative art; and Bryan Talbot, a British comic book artist, has called it "the first known British comic strip".
It has inspired many modern political and other cartoons, including:
The tapestry has also inspired modern embroideries, notably:
A number of films have used sections of the tapestry in their opening credits or closing titles, including Disney's "Bedknobs and Broomsticks", Anthony Mann's "El Cid", Zeffirelli's "Hamlet", Frank Cassenti's "La Chanson de Roland", "", and Richard Fleischer's "The Vikings".
The tapestry is referenced in Tony Kushner's play "". The apocryphal account of Queen Matilda's creation of the tapestry is used, perhaps in order to demonstrate that Louis, one of the main characters, holds himself to mythological standards.

</doc>
<doc id="49453" url="http://en.wikipedia.org/wiki?curid=49453" title="Seth">
Seth

Seth (Hebrew: שֵׁת, "Šet", "Šēṯ"; Arabic: شِيث‎ "Šīṯ"; placed; appointed"), in Judaism, Christianity and Islam, was the third son of Adam and Eve and brother of Cain and Abel, who were the only other of their children mentioned by name in the Tanakh (Old Testament). According to , Seth was born after Abel's murder, and Eve believed God had appointed him as a replacement for Abel.
Genesis.
According to Genesis, Seth was born when Adam was 130 years old "a son in his likeness and image." The genealogy is repeated at . states that Adam fathered "sons and daughters" before his death, aged 930 years. According to the Bible, Seth lived to the age of 912.
Jewish tradition.
Seth figures in the Life of Adam and Eve, also known in its Greek version as the Apocalypse of Moses, a Jewish pseudepigraphical group of writings. It recounts the lives of Adam and Eve from after their expulsion from the Garden of Eden to their deaths. The surviving versions were composed from the early 3rd to the 5th century,:252 the literary units in the work are considered to be older and predominantly of Jewish origin. There is wide agreement that the original was composed in a Semitic language:251 in the 1st century AD/CE.:252 In the Greek versions Seth and Eve travel to the doors of the Garden to beg for some oil of the tree of mercy (i.e. the Tree of Life). On the way Seth is attacked and bitten by a wild beast, which goes away when ordered by Seth. Michael refuses to give them the oil at that time, but promises to give it at the end of time, when all flesh will be raised up, the delights of paradise will be given to the holy people and God will be in their midst. On their return, Adam says to Eve: "What hast thou done? Thou hast brought upon us great wrath which is death." (chapters 5-14) Later only Seth can witness the taking-up of Adam at his funeral in a divine chariot, which deposits him in the Garden of Eden.
Rashi (Rabbi Shlomo Yitzhaqi) refers to Seth as the ancestor of Noah and hence the father of all mankind, all other humans having perished in the Great Flood.
In gnosticism, Seth is seen as a replacement given by God for Abel, whom Cain had slain. It is said that late in life, Adam gave Seth secret teachings that would become the kabbalah. The Zohar refers to Seth as "ancestor of all the generations of the tzaddikim" (Hebrew: righteous ones).
According to Seder Olam Rabbah, based on Jewish reckoning, he was born in 130 AM. According to Aggadah, he had 33 sons and 23 daughters. According to the Seder Olam Rabbah, he died in 1042 AM.
Islam.
Islamic tradition reveres Seth as the third son of Adam and Eve. It views him as a righteous son and sees him as the gift bestowed upon Adam after the death of Abel. Although Seth is not mentioned in the Qur'an, Muslim tradition generally regards him as a prophet like his father, and the one who continued teaching mankind the laws of God after the death of Adam. Islamic lore gives Seth an exalted position among the Antediluvian Patriarchs of the Generations of Adam, and some sources even cite Seth as the receiver of a scripture.
Islamic literature holds that Seth was born when Adam was past 100 and that, by the time Adam died, Adam had already made Seth the heir to him in guiding his people. Muslims hold that Seth was given wisdom in several different aspects of life, including knowledge of time, a prophetic vision of the future Great Flood, and inspiration on the methods of night prayer. Islam, like Judaism and Christianity, traces the genealogy of mankind back to Seth, since Abel did not leave any heirs and Cain's heirs, according to tradition, were destroyed by the Great Flood. In Muslim tradition, many of the traditional Islamic crafts are also traced back to Seth, such as the making of horn combs. Seth has also played a role in Islamic mysticism, known as Sufism, and Ibn Arabi included a chapter in his "Bezels of Wisdom" on Seth, titled "The Wisdom of Expiration in the Word of Seth".
Some Muslims believe that Seth's tomb is located in the village of Al-Nabi Shiyth (literally meaning "The Prophet Seth") in Lebanon, where a mosque is named after him. A rival tradition, mentioned by Arab geographers from the 13th century onwards, placed the tomb of "Nabi Shith" ("Prophet Seth") in the Palestinian village of Bashsheet southwest of Ramla village. Indeed, according to the Palestine Exploration Fund, Bashshit stands for "Beit Shith", meaning "House of Seth". The village was depopulated with the establishment of the State of Israel in 1948, but the three-domed structure considered Seth's tomb still exists in the Israeli moshav Aseret built on the site.
Abu l-Hasan al-Masudi writes, "One of the two pyramids (of Giza) is the tomb of Agathodaimon (Seth), the other one is the tomb of Hermes, (Idris, Enoch). Between the two 1000 years elapsed, Agathodaimon was the older one". Additionally, Jean Doresse, in The Secret Books of the Egyptian Gnostics writes, "Seth... is known in Islam, and usually assimilated to Agathodaimon, who is one of the great figures of Hermetic literature. The prophetic prestige with which the Gnostics endowed him, he still possesses, especially in the traditions of various Shi'ite groups, therefore chiefly in Mesopotamia or in Iran. In these particular doctrines the survival of Gnostic themes is ubiquitous and seems immense..."
According to Josephus.
In the "Antiquities of the Jews", Josephus refers to Seth as virtuous and of excellent character, and reports that his descendants invented the wisdom of the heavenly bodies, and built the "pillars of the sons of Seth", two pillars inscribed with many scientific discoveries and inventions, notably in astronomy. They were built by Seth's descendants based on Adam's prediction that the world would be destroyed at one time by fire and another time by global flood, in order to protect the discoveries and be remembered after the destruction. One was composed of brick, and the other of stone, so that if the pillar of brick should be destroyed, the pillar of stone would remain, both reporting the ancient discoveries, and informing men that a pillar of brick was also erected. Josephus reports that the pillar of stone remained in the land of Siriad in his day.
William Whiston, a 17/18th century translator of the "Antiquities", stated in a footnote that he believed Josephus mistook Seth for Sesostris, king of Egypt, the erector of the pillar in Siriad (being a contemporary name for the territories in which Sirius was venerated (i.e., Egypt). He stated that there was no way for any pillars of Seth to survive the deluge, because the deluge buried all such pillars and edifices far underground in the sediment of its waters. The Perennialist writer Nigel Jackson identifies the land of Siriad in Josephus' account with Syria, citing related Mandaean legends regarding the 'Oriental Land of Shyr' in connection with the visionary mytho-geography of the prophetic traditions surrounding Seth.('On the Prophethood of Seth in the Abrahamic Traditions', Sacred Web volume 25, Summer 2010)
Christianity.
The 2nd century BC Book of Jubilees, regarded as non-canonical except in the Alexandrian Rite, also dates his birth to 130 AM. According to it, in 231 AM Seth married his sister, Azura, who was four years younger than he was. In the year 235 AM, Azura gave birth to Enos.
Seth is commemorated as one of the Holy Forefathers in the Calendar of Saints of the Armenian Apostolic Church, along with Adam, Abel, and others, with a feast day on July 26. He is also included in the Genealogy of Jesus, according to Luke 3:23–38.
The Sethians were a Christian Gnostic sect who may date their existence to before Christianity. Their influence spread throughout the Mediterranean into the later systems of the Basilideans and the Valentinians. Their thinking, though it is predominantly Judaic in foundation, is arguably strongly influenced by Platonism. Sethians are so called for their veneration of the biblical Seth, who is depicted in their myths of creation as a divine incarnation; consequently, the offspring or 'posterity' of Seth are held to comprise a superior elect within human society.
Shrine in Mosul.
On July 26, 2014, forces of the Islamic State of Iraq and the Levant, blew up Nabi Shiyt (Prophet Seth) shrine in Mosul. Sami al-Massoudi, the deputy head of the Shiite endowment agency overseeing holy sites, confirmed that destruction. He added, ISIL took some of the artifacts to an unknown location.
As a name.
"Seth" is used as male first name, and sometimes as a surname, particularly in the US —- see Seth (disambiguation).
The High Priest Annas, mentioned in the New Testament, was the son of a man named "Seth", but the name was not commonly taken up by Jews.

</doc>
<doc id="49457" url="http://en.wikipedia.org/wiki?curid=49457" title="Jefferson National Expansion Memorial">
Jefferson National Expansion Memorial

The Jefferson National Expansion Memorial is in St. Louis, Missouri, near the starting point of the Lewis and Clark Expedition. It was designated as a National Memorial by Executive Order 7523, on December 21, 1935, and is maintained by the National Park Service (NPS).
The park was established to commemorate:
The memorial consists of a 91 acre park along the Mississippi River on the site of the earliest buildings of St. Louis; the Old Courthouse, a former state and federal courthouse that saw the origins of the "Dred Scott" case; the 45000 sqft Museum of Westward Expansion; and most notably, the Gateway Arch, a steel catenary arch that has become the definitive icon of the city.
Components.
The Gateway Arch.
The Gateway Arch is known as the "Gateway to the West". It was designed by Finnish-American architect Eero Saarinen and structural engineer Hannskarl Bandel in 1947 and built between 1963 and October 1965. It stands 630 ft tall and 630 ft wide at its base. The legs are 54 ft wide at the base, narrowing to 17 ft at the arch. There is a unique tram system to carry passengers to the observation room at the top of the arch.
Old Courthouse.
The Old Courthouse is built on land originally deeded by St. Louis founder Auguste Chouteau. It marks the location over which the arch reaches. Its dome was built during the American Civil War and is similar to the dome on the United States Capitol which was also built during the Civil War. It was the site of the local trials in the Dred Scott case.
The courthouse is the only portion of the memorial west of Interstate 44. To the west of the Old Courthouse is a Greenway between Market and Chestnut Streets which is only interrupted by the Civil Courts Building which features a pyramid model of the Mausoleum of Mausolus (which was one of the Seven Wonders of the Ancient World) on its roof. When the Civil Courts building was built in the 1920s, the Chouteau family sued to regain the property belonging to the Old Courthouse because it had been deeded in perpetuity to be a courthouse.
Museum of Westward Expansion.
Underneath the Arch is a visitor center, entered from a descending outdoor ramp starting at either base. Within the center is the Museum of Westward Expansion, exhibits on the history of the St. Louis riverfront, and tram loading and unloading areas. Tucker Theater, finished in 1968 and renovated 30 years later, has about 285 seats and shows a documentary ("Monument to the Dream") on the Arch's construction. Odyssey Theater, designed by Cox/Croslin Architects, Robert Cox and Charles Croslin, was completed in 1993 and has 255 seats. It was the first 70 mm film theater to be located on National Park Service grounds and operated by the NPS. The theater runs films from a rotating play list. Also located in the visitor center are retail operations run by the Jefferson National Parks Association, a not-for-profit partner.
A bronze standing full-length portrait of Thomas Jefferson, done by A. Lloyd Lillie, a nationally renown sculptor, stands inside the entrance of the Museum. The casual pose represents the ease with which Jefferson was able to move seamlessly among his many interests and areas of expertise. He was a president, statesman, ambassador, architect, farmer, and a framer of the Declaration of Independence. The standing pose illustrates his tireless efforts for his young, developing, and independent country.
History.
1930s.
The memorial was developed largely through the efforts of St. Louis civic booster Luther Ely Smith who first pitched the idea in 1933, was the long-term chairman of the committee that selected the area and persuaded Franklin Roosevelt in 1935 to make it a national park after St. Louis passed a bond issue to begin building it, and who partially financed the 1947 architectural contest that selected the Arch.
In the early 1930s the United States began looking for a suitable memorial for Thomas Jefferson (the Washington Monument and the newly built Lincoln Memorial were the only large Presidential memorials at the time).
Shortly after Thanksgiving in 1933 Smith who had been on the commission to build the George Rogers Clark National Historical Park in Indiana, was returning via train when he noticed the poor condition of the original platted location of St. Louis along the Mississippi. He thought that the memorial to Jefferson should be on the actual location that was symbolic of one of Jefferson's greatest triumphs—the Louisiana Purchase.
The originally platted area of St. Louis included:
Almost all of the historic buildings associated with this period had been replaced by newer buildings. His idea was to raze all of the buildings in the original St. Louis platted area and replace it with a park with "a central feature, a shaft, a building, an arch, or something which would symbolize American culture and civilization."
Smith pitched the idea to Bernard Dickmann who quickly assembled a meeting of St. Louis civic leaders on December 15, 1933 at the Jefferson Hotel and they endorsed the plan and Smith became chairman of what would become the Jefferson National Expansion Memorial Association (a position he would hold until 1949 with a one-year exception).
The Commission then defined the area, got cost estimates of $30 million to buy the land, clear the buildings and erect a park and monument. With promises from the federal government (via the United States Territorial Expansion Memorial Commission) to join if the City of St. Louis could raise money.
The area to be included in the park was the Eads Bridge/Washington Avenue on the north and Poplar Street on the south, the Mississippi River on the east and Third Street (now Interstate 44) on the west. The Old Courthouse, just west of Third Street, was added in 1940.
The only building in this area not included was the Old Cathedral, which is on the site of St. Louis first church and was opposite the home of St. Louis founder Auguste Chouteau. The founders of the city were buried in its graveyard (but were moved in 1849 to Bellefontaine Cemetery during a cholera outbreak).
Taking away 40 blocks in the center of St. Louis was bitterly fought by some sources—particularly the "St. Louis Post-Dispatch". On September 10, 1935, the voters of St. Louis approved a $7.5 million bond issue to buy the property.
The buildings were bought for $7 million by the federal government via Eminent domain and was subject to considerable litigation but were ultimately bought at 131.99 percent of assessed valuation. Roosevelt inspected the memorial area on October 14, 1936 during the dedication of the St. Louis Soldiers Memorial. Included in the party was then Senator Harry S. Truman.
1940s.
The land was to be cleared by 1942. Among the buildings razed was the "Old Rock House" 1818 home of fur trader Manuel Lisa (now occupied by the stairs on the north side of the Arch) and the 1819 home of original St. Louis pioneer Jean Pierre Chouteau at First and Washington.
The architectural competition for a monument was delayed by World War II. Interest in the monument was fed after the war as it was to be the first big monument in the post-World War II era.
The estimated cost of the competition was $225,000 and Smith personally donated $40,000. Civic leaders held the nation-wide competition in 1947 to select a design for the main portion of the Memorial space.
Architect Eero Saarinen won this competition with plans for a 590 ft catenary arch to be placed on the banks of the Mississippi River. However, these plans were modified over the next 15 years, placing the arch on higher ground and adding 40 ft in height and width.
The central architectural feature at the base of the arch is the Old Courthouse, which was once the tallest building in Missouri and has a dome similar to the United States Capitol and was placed on the building during the American Civil War at the same time as that on the U.S. Capitol.
Saarinen developed the shape with the help of architectural engineer Hannskarl Bandel. It is not a pure inverted catenary. Saarinen preferred a shape that was slightly elongated and thinner towards the top, a shape that produces a subtle soaring effect, and transfers more of the structure's weight downward rather than outward at the base.
When Saarinen won the competition, the official notification was sent to "E. Saarinen", thinking it to be the architect's father Eliel Saarinen, who had also submitted an entry. The family celebrated with a bottle of champagne, and two hours later an embarrassed official called to say the winner was, in fact, the younger Saarinen. The elder Saarinen then broke out a second bottle of champagne to celebrate his son's success.
Among the five finalists was local St. Louis architect Harris Armstrong.
1950s.
Land for the memorial was formally dedicated on June 10, 1950 by Harry S. Truman. However the Korean War began and the project was put on hold.
On June 23, 1959, work begins on covering railroad tracks that cut across the memorial grounds.
1960s.
On February 11, 1961, excavation began, and that September 1, Saarinen died. On February 12, 1963, the first stainless steel triangle that formed the first section of the arch was set in place on the south leg.
On October 28, 1965, it was completed, costing approximately $15 million to build. The adjacent park was designed by landscape artise Dan Kiley. Along with all other historical areas of the National Park Service, the memorial was listed on the National Register of Historic Places on October 15, 1966. Vice President Hubert Humphrey and Secretary of the Interior Stewart Udall dedicated the Arch on May 25, 1968.
1980s.
In 1984, Congress authorized the enlargement of the Memorial to include up to 100 acre on the east bank of the Mississippi River in East St. Louis, Illinois. Funds were authorized to begin land acquisition, but Congress placed a moratorium upon NPS land acquisitions in fiscal year 1998. The moratorium continued into the 21st century, with expansion becoming less likely because of the construction of a riverboat gambling facility and related amenities.
1990s.
During the Great Flood of 1993, Mississippi flood waters reached halfway up the Grand Staircase on the east.
In 1999, the Arch tram queue areas were renovated at a cost of about $2.2 million. As well, the Ulysses S. Grant National Historic Site in St. Louis County, Missouri, was put under the jurisdiction of the Superintendent of the Memorial.
2000s.
The arch was featured on the Missouri state quarter in 2003.
In 2007 St. Louis Mayor Francis Slay and former Missouri Senator John Danforth asked the National Park Service to create a more "active" use of the grounds of the memorial and model it on Millennium Park in Chicago including the possibility of an amphitheater, cafes and restaurants, fountains, bicycle rentals, sculptures and an aquarium. The National Park Service was not in favor of the plan noting that the only other overt development pressure on National Park property has been at the Jackson Hole Airport in Grand Teton National Park
2010s.
The Memorial is separated from the rest of Downtown St. Louis by a sunken section of I-70. The city is considering a $90 million proposal to cover the interstate. The NPS, as part of their Centennial Initiative celebrating its 100th anniversary in 2016, is considering a plan to complete Saarinen's original master plan. The intention is to build the Gateway Arch Connector to link the Old Courthouse with the grounds of the Arch. In September 2010 Michael Van Valkenburgh Associates won a design contest to "re-envision the visitor experience" of the grounds. The projected completion of the project is planned for 2015 to coincide with the 50th anniversary of the completion of the Arch. The plan includes:
Although it was originally planned for completion to coincide with the 50th anniversary of the completion of the arch, the renovation is now 14 months behind schedule. It is now scheduled for a December 2016 completion. 
In 2010, officials stated that they were seeking to replace 1,000 ash trees on the grounds that may be destroyed by emerald ash borers.
References.
</dl>

</doc>
<doc id="49460" url="http://en.wikipedia.org/wiki?curid=49460" title="Boole's syllogistic">
Boole's syllogistic

Boolean logic is a system of syllogistic logic invented by 19th-century British mathematician George Boole, which attempts to incorporate the "empty set", that is, a class of non-existent entities, such as round squares, without resorting to uncertain truth values.
In Boolean logic, the universal statements "all S is P" and "no S is P" (contraries in the traditional Aristotelian schema) are compossible provided that the set of "S" is the empty set. "All S is P" is construed to mean that "there is nothing that is both S and not-P"; "no S is P", that "there is nothing that is both S and P". For example, since there is nothing that is a round square, it is true both that nothing is a round square and purple, and that nothing is a round square and "not"-purple. Therefore, both universal statements, that "all round squares are purple" and "no round squares are purple" are true.
Similarly, the subcontrary relationship is dissolved between the existential statements "some S is P" and "some S is not P". The former is interpreted as "there is some S such that S is P" and the latter, "there is some S such that S is not P", both of which are clearly false where S is nonexistent.
Thus, the subaltern relationship between universal and existential also does not hold, since for a nonexistent S, "All S is P" is true but does not entail "Some S is P", which is false. Of the Aristotelian square of opposition, only the contradictory relationships remain intact.

</doc>
<doc id="49463" url="http://en.wikipedia.org/wiki?curid=49463" title="Upper and Lower Egypt">
Upper and Lower Egypt

Ancient Egypt was divided into two regions, namely Upper Egypt and Lower Egypt. To the north was Lower Egypt, where the Nile stretched out with its several branches to form the Nile Delta. To the south was Upper Egypt, stretching to Syene. The two kingdoms of Upper and Lower Egypt were united c. 3000 BC, but each maintained its own regalia: the "hedjet" or White Crown for Upper Egypt and the "deshret" or Red Crown for Lower Egypt. Thus, the pharaohs were known as the rulers of the Two Lands, and wore the "pschent", a double crown, each half representing sovereignty of one of the kingdoms. Ancient Egyptian tradition credited Menes, now believed to be the same as Narmer, as the king who united Upper and Lower Egypt. To represent the union of the two lands, the double crown was formed. 
The terminology "Upper" and "Lower" derives from the flow of the Nile from the highlands of East Africa northwards to the Mediterranean Sea, so Upper Egypt lies to the south of Lower Egypt. Lower Egypt mostly consists of the Nile Delta. 

</doc>
<doc id="49471" url="http://en.wikipedia.org/wiki?curid=49471" title="Osiris myth">
Osiris myth

The Osiris myth is the most elaborate and influential story in ancient Egyptian mythology. It concerns the murder of the god Osiris, a primeval king of Egypt, and its consequences. Osiris's murderer, his brother Set, usurps his throne. Meanwhile, Osiris's wife Isis restores her husband's body, allowing him to posthumously conceive a son with her. The remainder of the story focuses on Horus, the product of the union of Isis and Osiris, who is at first a vulnerable child protected by his mother and then becomes Set's rival for the throne. Their often violent conflict ends with Horus's triumph, which restores order to Egypt after Set's unrighteous reign and completes the process of Osiris's resurrection. The myth, with its complex symbolism, is integral to the Egyptian conceptions of kingship and succession, conflict between order and disorder, and especially death and the afterlife. It also expresses the essential character of each of the four deities at its center, and many elements of their worship in ancient Egyptian religion were derived from the myth.
The Osiris myth reached its basic form in or before the 24th century BCE. Many of its elements originated in religious ideas, but the conflict between Horus and Set may have been partly inspired by a regional struggle in Egypt's early history or prehistory. Scholars have tried to discern the exact nature of the events that gave rise to the story, but they have reached no definitive conclusions.
Parts of the myth appear in a wide variety of Egyptian texts, from funerary texts and magical spells to short stories. The story is, therefore, more detailed and more cohesive than any other ancient Egyptian myth. Yet no Egyptian source gives a full account of the myth, and the sources vary widely in their versions of events. Greek and Roman writings, particularly "De Iside et Osiride" by Plutarch, provide more information but may not always accurately reflect Egyptian beliefs. Through these writings, the Osiris myth persisted after knowledge of most ancient Egyptian beliefs was lost, and it is still well known today.
Sources.
The myth of Osiris was deeply influential in ancient Egyptian religion and was popular among ordinary people. One reason for this popularity is the myth's primary religious meaning, which implies that any dead person can reach a pleasant afterlife. Another reason is that the characters and their emotions are more reminiscent of the lives of real people than those in most Egyptian myths, making the story more appealing to the general populace. In particular, the myth conveys a "strong sense of family loyalty and devotion", as the Egyptologist J. Gwyn Griffiths put it, in the relationships between Osiris, Isis, and Horus.
With this widespread appeal, the myth appears in more ancient texts than any other myth and in an exceptionally broad range of Egyptian literary styles. These sources also provide an unusual amount of detail. Ancient Egyptian myths are fragmentary and vague; the religious metaphors contained within the myths were more important than coherent narration. Each text that contains a myth, or a fragment of one, may adapt the myth to suit its particular purposes, so different texts can contain contradictory versions of events. Because the Osiris myth was used in such a variety of ways, different versions often conflict with each other. Nevertheless, the fragmentary versions, taken together, give it a greater resemblance to a cohesive story than most Egyptian myths.
The earliest mentions of the Osiris myth are in the "Pyramid Texts", the first Egyptian funerary texts, which appeared on the walls of burial chambers in pyramids at the end of the Fifth Dynasty, during the 24th century BCE. These texts, made up of disparate spells or "utterances", contain ideas that are presumed to date from still earlier times. The texts are concerned with the afterlife of the king buried in the pyramid, so they frequently refer to the Osiris myth, which is deeply involved with kingship and the afterlife. Major elements of the story, such as the death and restoration of Osiris and the strife between Horus and Set, appear in the utterances of the "Pyramid Texts".
The same elements from the myth that appear in the "Pyramid Texts" recur in funerary texts written in later times, such as the "Coffin Texts" from the Middle Kingdom (c. 2055–1650 BCE) and the "Book of the Dead" from the New Kingdom (c. 1550–1070 BCE). Most of these writings were made for the general populace, so unlike the "Pyramid Texts", they link Osiris with all deceased souls, not exclusively with royalty.
Other types of religious texts give evidence for the myth, such as two Middle Kingdom texts, the Dramatic Ramesseum Papyrus, and the Ikhernofret Stela. The papyrus describes the coronation of Senusret I, whereas the stela alludes to events in the annual festival of Khoiak. Rituals in both these festivals reenacted elements of the Osiris myth. The most complete ancient Egyptian account of the myth is the Great Hymn to Osiris, an inscription from the Eighteenth Dynasty (c. 1550–1292 BCE) that gives the general outline of the entire story but includes little detail. Another important source is the Memphite Theology, a religious narrative that includes an account of Osiris's death as well as the resolution of the dispute between Horus and Set. This narrative associates the kingship that Osiris and Horus represent with Ptah, the creator deity of Memphis. The text was long thought to date back to the Old Kingdom (c. 2686–2181 BCE) and was treated as a source for information about the early stages in the development of the myth. Since the 1970s, however, Egyptologists have concluded that the text dates to the New Kingdom at the earliest. 
Rituals in honor of Osiris are another major source of information. Some of these texts are found on the walls of temples that date from the New Kingdom, the Ptolemaic period (323–30 BCE), or the Roman era (30 BCE to the fourth century CE). Some of these late ritual texts, in which Isis and Nephthys lament their brother's death, were adapted into funerary texts. In these texts, the goddesses' pleas were meant to rouse Osiris—and thus the deceased person—to live again.
Magical healing spells, which were used by Egyptians of all classes, are the source for an important portion of the myth, in which Horus is poisoned or otherwise sickened, and Isis heals him. The spells identify a sick person with Horus so that he or she can benefit from the goddess's efforts. The spells are known from papyrus copies, which serve as instructions for healing rituals, and from a specialized type of inscribed stone stela called a "cippus". People seeking healing poured water over these cippi, an act that was believed to imbue the water with the healing power contained in the text, and then drank the water in hope of curing their ailments. The theme of an endangered child protected by magic also appears on inscribed ritual wands from the Middle Kingdom, which were made centuries before the more detailed healing spells that specifically connect this theme with the Osiris myth.
Episodes from the myth were also recorded in writings that may have been intended as entertainment. Prominent among these texts is "The Contendings of Horus and Set", a humorous retelling of several episodes of the struggle between the two deities, which dates to the Twentieth Dynasty (c. 1190–1070 BCE). It vividly characterizes the deities involved; as the Egyptologist Donald B. Redford says, "Horus appears as a physically weak but clever Puck-like figure, Seth [Set] as a strong-man buffoon of limited intelligence, Re-Horakhty [Ra] as a prejudiced, sulky judge, and Osiris as an articulate curmudgeon with an acid tongue." Despite its atypical nature, "Contendings" includes many of the oldest episodes in the divine conflict, and many events appear in the same order as in much later accounts, suggesting that a traditional sequence of events was forming at the time that the story was written.
Ancient Greek and Roman writers, who described Egyptian religion late in its history, recorded much of the Osiris myth. Herodotus, in the 5th century BCE, mentioned parts of the myth in his description of Egypt in "The Histories", and four centuries later, Diodorus Siculus provided a summary of the myth in his "Bibliotheca historica". In the early 2nd century AD, Plutarch wrote the most complete ancient account of the myth in "De Iside et Osiride", an analysis of Egyptian religious beliefs. Plutarch's account of the myth is the version that modern popular writings most frequently retell. The writings of these classical authors may give a distorted view of Egyptian beliefs. For instance, "De Iside et Osiride" includes many interpretations of Egyptian belief that are influenced by various Greek philosophies, and its account of the myth contains portions with no known parallel in Egyptian tradition. Griffiths concluded that several elements of this account were taken from Greek mythology, and that the work as a whole was not based directly on Egyptian sources. His colleague John Baines, on the other hand, says that temples may have kept written accounts of myths, which later were lost, and that Plutarch could have drawn on such sources to write his narrative.
Synopsis.
Death and resurrection of Osiris.
At the start of the story, Osiris rules Egypt, having inherited the kingship from his ancestors in a lineage stretching back to the creator of the world, Ra or Atum. His queen is Isis, who, along with Osiris and his murderer Set, is one of the children of the earth god Geb and the sky goddess Nut. Little information about the reign of Osiris appears in Egyptian sources; the focus is on his death and the events that follow. Osiris is connected with life-giving power, righteous kingship, and the rule of "maat", the ideal natural order whose maintenance was a fundamental goal in ancient Egyptian culture. Set is closely associated with violence and chaos. Therefore, the slaying of Osiris symbolizes the struggle between order and disorder, and the disruption of life by death.
Some versions of the myth provide Set's motive for killing Osiris. According to a spell in the "Pyramid Texts", Set is taking revenge for a kick Osiris gave him, whereas in a Late Period text, Set's grievance is that Osiris had sex with Nephthys, who is Set's consort and the fourth child of Geb and Nut. The murder itself is frequently alluded to, but never clearly described. The Egyptians believed that written words had the power to affect reality, so they avoided writing directly about profoundly negative events such as Osiris's death. Sometimes they denied his death altogether, even though the bulk of the traditions about him make it clear that he has been murdered. In some cases the texts suggest that Set takes the form of a wild animal, such as a crocodile or bull, to slay Osiris; in others they imply that Osiris's corpse is thrown in the water or that he is drowned. This latter tradition is the origin of the Egyptian belief that people who had drowned in the Nile were sacred. Even the identity of the victim is changeable in texts, as it is sometimes the god Haroeris, an elder form of Horus, who is murdered by Set and then avenged by another form of Horus, who is Haroeris's son by Isis.
By the end of the New Kingdom, a tradition had developed that Set had cut Osiris's body into pieces and scattered them across Egypt. Cult centers of Osiris all over the country claimed that the corpse, or particular pieces of it, were found near them. The dismembered parts could be said to number as many as forty-two, each piece being equated with one of the forty-two nomes, or provinces, in Egypt. Thus, the god of kingship becomes the embodiment of his kingdom.
Osiris's death is followed either by an interregnum or by a period in which Set assumes the kingship. Meanwhile, Isis searches for her husband's body with the aid of Nephthys. When searching for or mourning Osiris, the two goddesses are often likened to falcons or kites, possibly because kites travel far in search of carrion, because the Egyptians associated their plaintive calls with cries of grief, or because of the goddesses' connection with Horus, who is often represented as a falcon. In the New Kingdom, when Osiris's death and renewal came to be associated with the annual flooding of the Nile that fertilized Egypt, the waters of the Nile were equated with Isis's tears of mourning, or with Osiris's bodily fluids. Osiris thus represented the life-giving divine power that was present in the river's water and in the plants that grew after the flood.
The goddesses find and restore Osiris's body, often with the help of other deities, including Thoth, a deity credited with great magical and healing powers, and Anubis, the god of embalming and funerary rites. Their efforts are the mythological basis for Egyptian embalming practices, which, by mummifying the body, sought to prevent and reverse the decay that follows death. This part of the story is often extended with episodes in which Set or his followers try to damage the corpse, and Isis and her allies must protect it. Once Osiris is made whole, Isis conceives his son and rightful heir, Horus. One ambiguous spell in the Coffin Texts may indicate that Isis is impregnated by a flash of lightning, while in other sources, Isis, still in bird form, fans breath and life into Osiris's body with her wings and copulates with him. Osiris's revival is apparently not permanent, and after this point in the story he is only mentioned as the ruler of the Duat, the distant and mysterious realm of the dead. Although he lives on only in the Duat, he and the kingship he stands for will, in a sense, be reborn in his son.
The cohesive account by Plutarch, which deals mainly with this portion of the myth, differs in many respects from the known Egyptian sources. Set—whom Plutarch, using Greek names for many of the Egyptian deities, refers to as "Typhon"—conspires against Osiris seventy-two unspecified accomplices, as well as a queen from Ethiopia (Nubia). Set has an elaborate chest made to fit Osiris's exact measurements and then, at a banquet, declares that he will give the chest as a gift to whoever fits inside it. The guests, in turn, lie inside the coffin, but none fit inside except Osiris. When he lies down in the chest, Set and his accomplices slam the cover shut, seal it, and throw it into the Nile. With Osiris's corpse inside, the chest floats out into the sea, arriving at the city of Byblos, where a tree grows around it. The king of Byblos has the tree cut down and made into a pillar for his palace, still with the chest inside. Isis must remove the chest from within the tree in order to retrieve her husband's body. Having taken the chest, she leaves the tree in Byblos, where it becomes an object of worship for the locals. This episode, which is not known from Egyptian sources, gives an etiological explanation for a cult of Isis and Osiris that existed in Byblos in Plutarch's time and possibly as early as the New Kingdom.
Plutarch also states that Set steals and dismembers the corpse only after Isis has retrieved it. Isis then finds and buries each piece of her husband's body, with the exception of the penis, which she has to reconstruct with magic, because the original was eaten by fish in the river. According to Plutarch, this is the reason the Egyptians had a taboo against eating fish. In Egyptian accounts, however, the penis of Osiris is found intact, and the only close parallel with this part of Plutarch's story is in "The Tale of Two Brothers", a folk tale from the New Kingdom with similarities to the Osiris myth.
A final difference in Plutarch's account is Horus's birth. The form of Horus that avenges his father has been conceived and born before Osiris's death. It is a premature and weak second child, Harpocrates, who is born from Osiris's posthumous union with Isis. Here, two of the separate forms of Horus that exist in Egyptian tradition have been given distinct positions within Plutarch's version of the myth.
Birth and childhood of Horus.
In Egyptian accounts, the pregnant Isis hides from Set, to whom the unborn child is a threat, in a thicket of papyrus in the Nile Delta. This place is called "Akh-bity", meaning "papyrus thicket of the king of Lower Egypt" in Egyptian. Greek writers call this place "Khemmis" and indicate that it is near the city of Buto, but in the myth, the physical location is less important than its nature as an iconic place of seclusion and safety. The thicket's special status is indicated by its frequent depiction in Egyptian art; for most events in Egyptian mythology, the backdrop is minimally described or illustrated. In this thicket, Isis gives birth to Horus and raises him, and hence it is also called the "nest of Horus". The image of Isis nursing her child is a very common motif in Egyptian art.
There are texts in which Isis travels in the wider world. She moves among ordinary humans who are unaware of her identity, and she even appeals to these people for help. This is another unusual circumstance, for in Egyptian myth, gods and humans are normally separate. As in the first phase of the myth, she often has the aid of other deities, who protect her son in her absence. According to one magical spell, seven minor scorpion deities travel with and guard Isis as she seeks help for Horus. They even take revenge on a wealthy woman who has refused to help Isis by stinging the woman's son, making it necessary for Isis to heal the blameless child. This story conveys a moral message that the poor can be more virtuous than the wealthy and illustrates Isis's fair and compassionate nature.
In this stage of the myth, Horus is a vulnerable child beset by dangers. The magical texts that use Horus's childhood as the basis for their healing spells give him different ailments, from scorpion stings to simple stomachaches, adapting the tradition to fit the malady that each spell was intended to treat. Most commonly, the child god has been bitten by a snake, reflecting the Egyptians' fear of snakebite and the resulting poison. Some texts indicate that these hostile creatures are agents of Set. Isis may use her own magical powers to save her child, or she may plead with or threaten deities such as Ra or Geb, so they will cure him. As she is the archetypal mourner in the first portion of the story, so during Horus's childhood she is the ideal devoted mother. Through the magical healing texts, her efforts to heal her son are extended to cure any patient.
Conflict of Horus and Set.
The next phase of the myth begins when the adult Horus challenges Set for the throne of Egypt. The contest between them is often violent but is also described as a legal judgment before the Ennead, an assembled group of Egyptian deities, to decide who should inherit the kingship. The judge in this trial may be Geb, who, as the father of Osiris and Set, held the throne before they did, or it may be the creator gods Ra or Atum, the originators of kingship. Other deities also take important roles: Thoth frequently acts as a conciliator in the dispute or as an assistant to the divine judge, and in "Contendings", Isis uses her cunning and magical power to aid her son.
The rivalry of Horus and Set is portrayed in two contrasting ways. Both perspectives appear as early as the "Pyramid Texts", the earliest source of the myth. In some spells from these texts, Horus is the son of Osiris and nephew of Set, and the murder of Osiris is the major impetus for the conflict. The other tradition depicts Horus and Set as brothers. This incongruity persists in many of the subsequent sources, where the two gods may be called brothers or uncle and nephew at different points in the same text.
The divine struggle involves many episodes. "Contendings" describes the two gods appealing to various other deities to arbitrate the dispute and competing in different types of contests, such as racing in boats or fighting each other in the form of hippopotami, to determine a victor. In this account, Horus repeatedly defeats Set and is supported by most of the other deities. Yet the dispute drags on for eighty years, largely because the judge, the creator god, favors Set. In late ritual texts, the conflict is characterized as a great battle involving the two deities' assembled followers. The strife in the divine realm extends beyond the two combatants. At one point Isis attempts to harpoon Set as he is locked in combat with her son, but she strikes Horus instead, who then cuts off her head in a fit of rage. Thoth replaces Isis's head with that of a cow; the story gives a mythical origin for the cow-horn headdress that Isis commonly wears.
In a key episode in the conflict, Set sexually abuses Horus. Set's violation is partly meant to degrade his rival, but it also involves homosexual desire, in keeping with one of Set's major characteristics, his forceful and indiscriminate sexuality. In the earliest account of this episode, in a fragmentary Middle Kingdom papyrus, the sexual encounter begins when Set asks to have sex with Horus, who agrees on the condition that Set will give Horus some of his strength. The encounter puts Horus in danger, because in Egyptian tradition semen is a potent and dangerous substance, akin to poison. According to some texts, Set's semen enters Horus's body and makes him ill, but in "Contendings", Horus thwarts Set by catching Set's semen in his hands. Isis retaliates by putting Horus's semen on lettuce-leaves that Set eats. Set's defeat becomes apparent when this semen appears on his forehead as a golden disk. He has been impregnated with his rival's seed and as a result "gives birth" to the disk. In "Contendings", Thoth takes the disk and places it on his own head; in earlier accounts, it is Thoth who is produced by this anomalous birth.
Another important episode concerns mutilations that the combatants inflict upon each other: Horus injures or steals Set's testicles and Set damages or tears out one, or occasionally both, of Horus's eyes. Sometimes the eye is torn into pieces. Set's mutilation signifies a loss of virility and strength. The removal of Horus's eye is even more important, for this stolen Eye of Horus represents a wide variety of concepts in Egyptian religion. One of Horus's major roles is as a sky deity, and for this reason his right eye was said to be the sun and his left eye the moon. The theft or destruction of the Eye of Horus is therefore equated with the darkening of the moon in the course of its cycle of phases, or during eclipses. Horus may take back his lost Eye, or other deities, including Isis, Thoth, and Hathor, may retrieve or heal it for him. The Egyptologist Herman te Velde argues that the tradition about the lost testicles is a late variation on Set's loss of semen to Horus, and that the moon-like disk that emerges from Set's head after his impregnation is the Eye of Horus. If so, the episodes of mutilation and sexual abuse would form a single story, in which Set assaults Horus and loses semen to him, Horus retaliates and impregnates Set, and Set comes into possession of Horus's Eye when it appears on Set's head. Because Thoth is a moon deity in addition to his other functions, it would make sense, according to te Velde, for Thoth to emerge in the form of the Eye and step in to mediate between the feuding deities.
In any case, the restoration of the Eye of Horus to wholeness represents the return of the moon to full brightness, the return of the kingship to Horus, and many other aspects of "maat". Sometimes the restoration of Horus's eye is accompanied by the restoration of Set's testicles, so that both gods are made whole near the conclusion of their feud.
Resolution.
As with so many other parts of the myth, the resolution is complex and varied. Often, Horus and Set divide the realm between them. This division can be equated with any of several fundamental dualities that the Egyptians saw in their world. Horus may receive the fertile lands around the Nile, the core of Egyptian civilization, in which case Set takes the barren desert or the foreign lands that are associated with it; Horus may rule the earth while Set dwells in the sky; and each god may take one of the two traditional halves of the country, Upper and Lower Egypt, in which case either god may be connected with either region. Yet in the Memphite Theology, Geb, as judge, first apportions the realm between the claimants and then reverses himself, awarding sole control to Horus. In this peaceable union, Horus and Set are reconciled, and the dualities that they represent have been resolved into a united whole. Through this resolution, order is restored after the tumultuous conflict.
A different view of the myth's end focuses on Horus's sole triumph. In this version, Set is not reconciled with his rival, but utterly defeated, and sometimes he is exiled from Egypt or even destroyed. His defeat and humiliation is more pronounced in sources from later periods of Egyptian history, when he was increasingly equated with disorder and evil, and the Egyptians no longer saw him as an integral part of natural order.
With great celebration among the gods, Horus takes the throne, and Egypt at last has a rightful king. The divine decision that Set is in the wrong corrects the injustice created by Osiris's murder and completes the process of his restoration after death. Sometimes Set is made to carry Osiris's body to its tomb as part of his punishment. The new king performs funerary rites for his father and gives food offerings to sustain him—often including the Eye of Horus, which in this instance represents life and plenty. According to some sources, only through these acts can Osiris be fully enlivened in the afterlife and take his place as king of the dead, paralleling his son's role as king of the living. Thereafter, Osiris is deeply involved with natural cycles of death and renewal, such as the annual growth of crops, that parallel his own resurrection.
Origins.
As the Osiris myth first appears in the "Pyramid Texts", most of its essential features must have taken shape sometime before the texts were written down. The distinct segments of the story—Osiris's death and restoration, Horus's childhood, and Horus's conflict with Set—may originally have been independent mythic episodes. If so, they must have begun to coalesce into a single story by the time of the "Pyramid Texts", which loosely connect those segments. In any case, the myth was inspired by a variety of influences. Much of the story is based in religious ideas and the general nature of Egyptian society: the divine nature of kingship, the succession from one king to another, the struggle to maintain "maat", and the effort to overcome death. For instance, the lamentations of Isis and Nephthys for their dead brother may represent an early tradition of ritualized mourning.
There are, however, important points of disagreement. The origins of Osiris are much debated, and the basis for the myth of his death is also somewhat uncertain. One influential hypothesis was given by the anthropologist James Frazer, who in 1906 said that Osiris, like other "dying and rising gods" across the ancient Near East, began as a personification of vegetation. His death and restoration, therefore, were based on the yearly death and re-growth of plants. Many Egyptologists adopted this explanation. But in the late 20th century, J. Gwyn Griffiths, who extensively studied Osiris and his mythology, argued that Osiris originated as a divine ruler of the dead, and his connection with vegetation was a secondary development. Meanwhile, scholars of comparative religion have criticized the overarching concept of "dying and rising gods", or at least Frazer's assumption that all these gods closely fit the same pattern. More recently, the Egyptologist Rosalie David maintains that Osiris originally "personified the annual rebirth of the trees and plants after the [Nile] inundation."
Another continuing debate concerns the opposition of Horus and Set, which Egyptologists have often tried to connect with political events early in Egypt's history or prehistory. The cases in which the combatants divide the kingdom, and the frequent association of the paired Horus and Set with the union of Upper and Lower Egypt, suggest that the two deities represent some kind of division within the country. Egyptian tradition and archaeological evidence indicate that Egypt was united at the beginning of its history when an Upper Egyptian kingdom, in the south, conquered Lower Egypt in the north. The Upper Egyptian rulers called themselves "followers of Horus", and Horus became the patron god of the unified nation and its kings. Yet Horus and Set cannot be easily equated with the two halves of the country. Both deities had several cult centers in each region, and Horus is often associated with Lower Egypt and Set with Upper Egypt. One of the better-known explanations for these discrepancies was proposed by Kurt Sethe in 1930. He argued that Osiris was originally the human ruler of a unified Egypt in prehistoric times, before a rebellion of Upper Egyptian Set-worshippers. The Lower Egyptian followers of Horus then forcibly reunified the land, inspiring the myth of Horus's triumph, before Upper Egypt, now led by Horus worshippers, became prominent again at the start of the Early Dynastic Period.
In the late 20th century, Griffiths focused on the inconsistent portrayal of Horus and Set as brothers and as uncle and nephew. He argued that, in the early stages of Egyptian mythology, the struggle between Horus and Set as siblings and equals was originally separate from the murder of Osiris. The two stories were joined into the single Osiris myth sometime before the writing of the "Pyramid Texts". With this merging, the genealogy of the deities involved and the characterization of the Horus–Set conflict were altered so that Horus is the son and heir avenging Osiris's death. Traces of the independent traditions remained in the conflicting characterizations of the combatants' relationship and in texts unrelated to the Osiris myth, which make Horus the son of the goddess Nut or the goddess Hathor rather than of Isis and Osiris. Griffiths therefore rejected the possibility that Osiris's murder was rooted in historical events. This hypothesis has been accepted by more recent scholars such as Jan Assmann and George Hart.
Griffiths sought a historical origin for the Horus–Set rivalry, and he posited two distinct predynastic unifications of Egypt by Horus worshippers, similar to Sethe's theory, to account for it. Yet the issue remains unresolved, partly because other political associations for Horus and Set complicate the picture further. Before even Upper Egypt had a single ruler, two of its major cities were Nekhen, in the far south, and Naqada, many miles to the north. The rulers of Nekhen, where Horus was the patron deity, are generally believed to have unified Upper Egypt, including Naqada, under their sway. Set was associated with Naqada, so it is possible that the divine conflict dimly reflects an enmity between the cities in the distant past. Much later, at the end of the Second Dynasty (c. 2890–2686 BCE), King Peribsen used the Set animal in writing his "serekh"-name, in place of the traditional falcon hieroglyph representing Horus. His successor Khasekhemwy used both Horus and Set in the writing of his "serekh". This evidence has prompted conjecture that the Second Dynasty saw a clash between the followers of the Horus-king and the worshippers of Set led by Peribsen. Khasekhemwy's use of the two animal symbols would then represent the reconciliation of the two factions, as does the resolution of the myth.
Noting the uncertainty surrounding these events, Herman te Velde argues that the historical roots of the conflict are too obscure to be very useful in understanding the myth and are not as significant as its religious meaning. He says that "the origin of the myth of Horus and Seth is lost in the mists of the religious traditions of prehistory."
Influence.
The effect of the Osiris myth on Egyptian culture was greater and more widespread than that of any other myth. In literature, the myth was not only the basis for a retelling such as "Contendings"; it also provided the basis for more distantly related stories. "The Tale of Two Brothers", a folk tale with human protagonists, includes elements similar to the myth of Osiris. One character's penis is eaten by a fish, and he later dies and is resurrected. Another story, "The Tale of Truth and Falsehood", adapts the conflict of Horus and Set into an allegory, in which the characters are direct personifications of truth and lies rather than deities associated with those concepts.
Osiris and funerary ritual.
From at least the time of the "Pyramid Texts", kings hoped that after their deaths they could emulate Osiris's restoration to life and his rule over the realm of the dead. By the early Middle Kingdom (c. 2055–1650 BCE), non-royal Egyptians believed that they, too, could overcome death as Osiris had, by worshipping him and receiving the funerary rites that were partly based on his myth. Osiris thus became Egypt's most important afterlife deity. The myth also influenced the notion, which grew prominent in the New Kingdom, that only virtuous people could reach the afterlife. As the assembled deities judged Osiris and Horus to be in the right, undoing the injustice of Osiris's death, so a deceased soul had to be judged righteous in order for his or her death to be undone. As ruler of the land of the dead and as a god connected with "maat", Osiris became the judge in this posthumous trial, offering life after death to those who followed his example.
As the importance of Osiris grew, so did his popularity. By late in the Middle Kingdom, the centuries-old tomb of the First Dynasty ruler Djer, near Osiris's main center of worship in the city of Abydos, was seen as Osiris's tomb. Accordingly, it became a major focus of Osiris worship. For the next 1,500 years, an annual festival procession traveled from Osiris's main temple to the tomb site. Kings and commoners from across Egypt built chapels, which served as cenotaphs, near the processional route. In doing so they sought to strengthen their connection with Osiris in the afterlife.
Another major funerary festival, a national event spread over several days in the month of Khoiak in the Egyptian calendar, became linked with Osiris during the Middle Kingdom. During Khoiak the "djed" pillar, an emblem of Osiris, was ritually raised into an upright position, symbolizing Osiris's restoration. By Ptolemaic times (305–30 BCE), Khoiak also included the planting of seeds in an "Osiris bed", a mummy-shaped bed of soil, connecting the resurrection of Osiris with the seasonal growth of plants.
Horus, the Eye of Horus, and kingship.
The myth's religious importance extended beyond the funerary sphere. Mortuary offerings, in which family members or hired priests presented food to the deceased, were logically linked with the mythological offering of the Eye of Horus to Osiris. By analogy, this episode of the myth was eventually equated with other interactions between a human and a being in the divine realm. In temple offering rituals, the officiating priest took on the role of Horus, the gifts to the deity became the Eye of Horus, and whichever deity received these gifts was momentarily equated with Osiris.
The myth influenced popular religion as well. One example is the magical healing spells based on Horus's childhood. Another is the use of the Eye of Horus as a protective emblem in personal apotropaic amulets. Its mythological restoration made it appropriate for this purpose, as a general symbol of well-being.
The ideology surrounding the living king was also affected by the Osiris myth. The Egyptians envisioned the events of the Osiris myth as taking place sometime in Egypt's dim prehistory, and Osiris, Horus, and their divine predecessors were included in Egyptian lists of past kings such as the Turin Royal Canon. Horus, as a primeval king and as the personification of kingship, was regarded as the predecessor and exemplar for all Egyptian rulers. His assumption of his father's throne and pious actions to sustain his spirit in the afterlife were the model for all pharaonic successions to emulate. Each new king was believed to renew "maat" after the death of the preceding king, just as Horus had done. In royal coronations, rituals alluded to Osiris's burial, and hymns celebrated the new king's accession as the equivalent of Horus's own.
Set.
The Osiris myth contributed to the frequent characterization of Set as a disruptive, enemy god. Although other elements of Egyptian tradition credit Set with positive traits, in the Osiris myth the sinister aspects of his character predominate. He and Horus were often juxtaposed in art to represent opposite principles, such as good and evil, intellect and instinct, and the different regions of the world that they rule in the myth. Egyptian wisdom texts contrast the character of the ideal person with the opposite type—the calm and sensible "Silent One" and the impulsive, disruptive "Hothead"—and one description of these two characters calls them the Horus-type and the Set-type. Yet the two gods were often treated as part of a harmonious whole. In some local cults they were worshipped together; in art they were often shown tying together the emblems of Upper and Lower Egypt to symbolize the unity of the nation; and in funerary texts they appear as a single deity with the heads of Horus and Set, apparently representing the mysterious, all-encompassing nature of the Duat.
Overall Set was viewed with ambivalence, until during the first millennium BCE he came to be seen as a totally malevolent deity. This transformation was prompted more by his association with foreign lands than by the Osiris myth. Nevertheless, in these late times, the widespread temple rituals involving the ceremonial annihilation of Set were often connected with the myth.
Isis, Nephthys, and the Greco-Roman world.
Both Isis and Nephthys were seen as protectors of the dead in the afterlife because of their protection and restoration of Osiris's body. Khoiak celebrations made reference to, and may have ritually reenacted, Isis's and Nephthys's mourning, restoration, and revival of their murdered brother. Isis, as Horus's mother, was also the mother of every king according to royal ideology, and kings were said to have nursed at her breast as a symbol of their divine legitimacy. Her appeal to the general populace was based in her protective character, as exemplified by the magical healing spells. In the Late Period, she was credited with ever greater magical power, and her maternal devotion was believed to extend to everyone. By Roman times she had become the most important goddess in Egypt. The image of the goddess holding her child was used prominently in her worship—for example, in panel paintings that were used in household shrines dedicated to her. Isis's iconography in these paintings closely resembles and may have influenced the earliest Christian icons of Mary holding Jesus.
In the late centuries BCE, the worship of Isis spread from Egypt across the Mediterranean world, and she became one of the most popular deities in the region. Although this new, multicultural form of Isis absorbed characteristics from many other deities, her original mythological nature as a wife and mother was key to her appeal. Horus and Osiris, being central figures in her story, spread along with her. It was to a Greek priestess of Isis that Plutarch wrote his account of the myth of Osiris.
Through the work of classical writers such as Plutarch, knowledge of the Osiris myth was preserved even after the middle of the first millennium AD, when Egyptian religion ceased to exist and knowledge of the writing systems that were originally used to record the myth were lost. The myth remained a major part of Western impressions of ancient Egypt. In modern times, when understanding of Egyptian beliefs is informed by the original Egyptian sources, the story continues to influence and inspire new ideas, from works of fiction to scholarly speculation and new religious movements.
Works cited.
</dl>

</doc>
<doc id="49482" url="http://en.wikipedia.org/wiki?curid=49482" title="Das Rheingold">
Das Rheingold

Das Rheingold (  ; The Rhine Gold), WWV 86A, is the first of the four operas that constitute Richard Wagner's "Der Ring des Nibelungen" ('The Ring of the Nibelung'). It was originally written as an introduction to the tripartite "Ring", but the cycle is now generally regarded as consisting of four individual operas.
"Das Rheingold" premiered at the National Theatre Munich on 22 September 1869, with August Kindermann in the role of Wotan, Heinrich Vogl as Loge, and Karl Fischer as Alberich. Wagner wanted this opera to premiere as part of the entire cycle, but was forced to allow the performance at the insistence of his patron King Ludwig II of Bavaria. The opera was first performed as part of the complete cycle on 13 August 1876, in the Bayreuther Festspielhaus.
Composition history.
Although "Das Rheingold" comes first in the sequence of "Ring" operas, it was the last to be conceived. Wagner's plans for the cycle grew backwards from the tale of the death of the hero Siegfried, to include his youth and then the story of the events around his conception and of how the Valkyrie Brünnhilde was punished for trying to save his parents against Wotan's instructions. So in August 1851, Wagner wrote in "Eine Mittheilung an meine Freunde" ("A Communication to My Friends"), "I propose to produce my myth in three complete dramas...". However, by October, he had decided that this trilogy required a prelude and the text of "Eine Mittheilung" was duly altered to reflect the change. To the sentence quoted above he added the words, "which will be preceded by a great prelude".
He started work on the prelude producing a three paragraph prose sketch that month, although he remained uncertain of the name, considering in turn "Der Raub: Vorspiel" (The Theft: Prelude), "Der Raub des Rheingoldes" (The Theft of the Rhinegold) and "Das Rheingold (Vorspiel)" (The Rhinegold (Prelude)). A letter Wagner wrote to Theodor Uhlig confirms that at this time the opera was intended to have three acts. Wagner continued to develop the text and storyline of the prelude in parallel with those of "Die Walküre". The prose draft of "Das Rheingold" was completed between 21 March and 23 March 1852 and its verse draft between 15 September and 3 November. A fair copy of the text was finished by 15 December.
During the early years of the 1850s Wagner produced some musical sketches for parts of the "Ring" and noted down various motifs that were to be used in the work. Of particular note is 5 September 1853; Wagner claimed in his autobiography "Mein Leben" that on this date the musical idea came to him while he was half asleep in a hotel in La Spezia in Italy, but this has been disputed by John Deathridge and others.
There also exist three sets of isolated musical sketches for "Das Rheingold" which were composed between 15 September 1852 and November 1853. The first of these was entered into the verse draft of the text, the second into Wagner's copy of the 1853 printing of the text; the third was written on an undated sheet of music paper. All three were subsequently used by Wagner.
Proper sequential development of the score started on 1 November 1853. By 14 January, Wagner had completed the first draft of the opera on between two and three staves. The next stage involved the development of a more detailed draft that indicated most of the vocal and instrumental details. This was completed by 28 May. In parallel with this, Wagner started work on a fair copy of the score on 15 February, a task he completed on 26 September 1854, by which time he had also started work on the sketches of "Die Walküre".
Performance history.
"Das Rheingold" was first performed at Munich on 22 September 1869. Its first performance as part of the complete Ring cycle took place at Bayreuth on 13 August 1876. It continues to be performed on a regular basis both in Bayreuth and elsewhere either as part of a complete Ring or separately.
Synopsis.
"Das Rheingold", considerably shorter than its three successors, consists of four scenes performed without a break.
Scene 1.
The scale of the whole work is established in the prelude, over 136 bars, beginning with a low E flat, and building in more and more elaborate figurations of the chord of E flat major, to portray the motion of the river Rhine. It is considered the best-known drone piece in the concert repertory, lasting approximately four minutes.
The curtain rises to show, at the bottom of the Rhine, the three Rhine maidens, Woglinde, Wellgunde, and Flosshilde, playing together. The key shifts to A flat as Woglinde begins an innocent song whose melody is frequently used to characterise the Rhine maidens later in the cycle. Alberich, a Nibelung dwarf, appears from a deep chasm and tries to woo them. Struck by Alberich's ugliness, the Rhine maidens mock his advances and he grows angry. As the sun begins to rise, the maidens praise the golden glow atop a nearby rock; Alberich asks what it is. The Rhine maidens tell him about the Rhine gold, which their father has ordered them to guard: it can be made into a magic ring which will let its bearer rule the world, but only by someone who first renounces love. They think they have nothing to fear from the lustful dwarf, but Alberich, embittered by their mockery, curses love, seizes the gold and returns to his chasm, leaving them screaming in dismay.
Scene 2.
Wotan, ruler of the gods, is asleep on a mountaintop with Fricka, his wife. Fricka awakes and sees a magnificent castle behind them. She wakes Wotan and points out that their new home has been completed. The giants Fasolt and Fafner built the castle; in exchange Wotan has promised to give them Fricka's sister Freia, the goddess of youth and beauty and feminine love. Fricka is worried for her sister, but Wotan is confident that they will not have to give Freia away, because he has dispatched his clever servant Loge to search the world for something else to give the giants instead.
Freia rushes onstage in a panic, followed by Fasolt and Fafner. Fasolt demands payment for their finished work. He points out that Wotan's authority is sustained by the treaties carved into his spear, including his contract with the giants, which Wotan therefore cannot violate. Donner (god of thunder) and Froh (god of spring) arrive to defend their sister Freia, but Wotan stops them; as ruler of the gods, he cannot permit the use of force to break the agreement. Hoping Loge will arrive with the alternative payment he promised, Wotan tries to stall.
Loge finally returns with a discouraging report: there is nothing that men will accept in exchange for feminine love, and, by extension, nothing the giants would accept in exchange for Freia. Loge tells them that he was able to find only one instance where someone willingly gave up love for something else: Alberich the dwarf has renounced love, stolen the Rheingold and made a powerful magic ring out of it. A general discussion of the ring ensues and everyone finds good reasons for wanting it. Fafner makes a counteroffer: the giants will accept the Nibelung's treasure in payment, instead of Freia. When Wotan tries to haggle, the giants depart, taking Freia with them as hostage.
Freia's golden apples had kept the Gods eternally young; in her absence, they begin to age and weaken. In order to win Freia back, Wotan resolves to follow Loge down to earth, in pursuit of the gold.
An orchestral interlude follows: it "paints" the descent of Loge and Wotan into Nibelheim. As the orchestra fades, it gives way to a choir of 18 tuned anvils (indicated in the score with specific size, quantity and pitch) beating out the dotted rhythm of the Nibelung theme to give a stark depiction of the toiling of the enslaved dwarves.
Scene 3.
In Nibelheim, Alberich has enslaved the rest of the Nibelung dwarves with the power of the ring. He has forced his brother Mime, the most skillful smith, to create a magic helmet, the Tarnhelm. Alberich demonstrates the Tarnhelm's power by making himself invisible, the better to torment his subjects. (The Tarnhelm can also change the wearer's shape, and teleport him long distances.)
Wotan and Loge arrive and happen upon Mime, who tells them about Alberich's forging of the ring and the misery of the Nibelungs under his rule. Alberich returns, driving his slaves to pile up a huge mound of gold. When they have finished, he dismisses them and turns his attention to the two visitors. He boasts to them about his plans to conquer the world. Loge asks how he can protect himself against a thief while he sleeps. Alberich says the Tarnhelm would hide him, by allowing him to turn invisible or change his form. Loge says he doesn't believe it and requests a demonstration. Alberich complies, turning into a giant snake (or dragon, depending on the translation). Loge acts suitably impressed and then he asks if he can also reduce his size, which would be very useful for hiding. Alberich transforms himself into a toad. The two gods quickly seize him, tie him up, and drag him up to the mountain top.
Scene 4.
On the mountaintop, Wotan and Loge force Alberich to exchange his wealth for his freedom. They untie his right hand, and he uses the ring to summon his Nibelung slaves, who bring the hoard of gold. After the gold has been delivered, he asks for the return of the Tarnhelm, but Loge says that it is part of his ransom. Finally, Wotan demands the ring. Alberich refuses, but Wotan seizes it from his finger and puts it on his own. Alberich is crushed by his loss, and before he leaves he lays a curse on the ring: until it should return to him, whoever does not possess it will desire it, and whoever possesses it will live in anxiety and will eventually be killed and robbed of it by its next owner. Alberich's discordant "Death-Curse" leitmotif is one of the few leitmotifs which occur regularly and unchanged in all four parts of the "Ring Cycle".
The gods reconvene. Fasolt and Fafner return, carrying Freia. Reluctant to release Freia, Fasolt insists that the gold be heaped high enough to hide her from view. They pile up the gold, and Wotan is forced to relinquish the Tarnhelm to help cover Freia completely. However, Fasolt spots a remaining crack in the gold, through which Freia's eye can be seen. He demands that Wotan fill the crack by yielding the ring. Loge reminds all present that the ring rightly belongs to the Rhine maidens. Wotan angrily and defensively declares that he will keep it for his own. The giants seize Freia and start to leave, this time forever.
Suddenly, Erda the earth goddess, a primeval goddess older than Wotan, appears out of the ground. She warns Wotan of impending doom and urges him to give up the cursed ring. Troubled, Wotan calls the giants back and surrenders the ring. The giants release Freia and begin dividing the treasure, but they quarrel over the ring itself. Fafner clubs Fasolt to death (the orchestra repeats the "Death-Curse" leitmotif). Wotan, horrified, realizes that Alberich's curse has terrible power. Loge remarks that Wotan is indeed a lucky fellow: his enemies are killing each other for the gold he gave up.
At last, the gods prepare to enter their new home. Donner summons a thunderstorm to clear the air. After the storm has ended, Froh creates a rainbow bridge that stretches to the gate of the castle. Wotan leads them across the bridge to the castle, which he names Valhalla. Fricka asks him about the name, and he replies enigmatically that its meaning will become clear when his plans come to fruition.
Loge, who knows that the end of the gods is coming, does not follow the others into Valhalla; he tells the audience that he is tempted to destroy the gods and all they have deceitfully acquired. Far below, the Rhine maidens mourn the loss of their gold and proclaim that the glory of the gods is only an illusion. The curtain falls.

</doc>
<doc id="49484" url="http://en.wikipedia.org/wiki?curid=49484" title="Volatilisation">
Volatilisation

Volatilization is the process whereby a dissolved sample is vaporised. In atomic spectroscopy this is usually a two-step process. The analyte is turned into small droplets in a nebuliser which are entrained in a gas flow which is in turn volatilised in a high temperature flame in the case of AAS or volatilised in a gas plasma torch in the case of ICP spectroscopy.
Herbicide volatilisation.
Herbicide volatilisation refers to evaporation or sublimation of a volatile herbicide. The effect of gaseous chemical is lost at its intended place of application and may move downwind and affect other plants not intended to be affected causing crop damage. Herbicides vary in their susceptibility to volatilisation. Prompt incorporation of the herbicide into the soil may reduce or prevent volatilisation. Wind, temperature, and humidity also affect the rate of volatilisation with humidity reducing in. 2,4-D and dicamba are commonly used chemicals that are known to be subject to volatilisation but there are many others. Application of herbicides later in the season to protect herbicide-resistant genetically modified plants increases the risk of volatilisation as the temperature is higher and incorporation into the soil impractical.
Herbicide applied as a powder or a mist can also Pesticide drift in the wind in solid form as dust or liquid form as tiny drops. While drifting or after drifting by volatilisation it can become vapor, gas. Obviously, volatilisation also affects other substances, a commonplace example is water and ice.

</doc>
<doc id="49491" url="http://en.wikipedia.org/wiki?curid=49491" title="Kings Cross">
Kings Cross

Kings Cross or King's Cross can refer to:
Also:

</doc>
<doc id="49492" url="http://en.wikipedia.org/wiki?curid=49492" title="Divisor">
Divisor

In mathematics a divisor of an integer formula_1, also called a factor of formula_1, is an integer that can be multiplied by some other integer to produce formula_1.
Definition.
Two versions of the definition of a divisor are commonplace:
In the remainder of this article, which definition is applied is indicated where this is significant.
General.
Divisors can be negative as well as positive, although sometimes the term is restricted to positive divisors. For example, there are six divisors of 4; they are 1, 2, 4, −1, −2, and −4, but only the positive ones (1, 2, and 4) would usually be mentioned.
1 and −1 divide (are divisors of) every integer. Every integer (and its negation) is a divisor of itself. Every integer is a divisor of 0. Integers divisible by 2 are called even, and numbers not divisible by 2 are called odd.
1, −1, "n" and −"n" are known as the trivial divisors of "n". A divisor of "n" that is not a trivial divisor is known as a non-trivial divisor. A non-zero integer with at least one non-trivial divisor is known as a composite number, while the units −1 and 1 and prime numbers have no non-trivial divisors.
There are divisibility rules which allow one to recognize certain divisors of a number from the number's digits.
The generalization can be said to be the concept of "divisibility" in any integral domain.
Further notions and facts.
There are some elementary rules: 
If formula_39, and gcdformula_40, then formula_25. This is called Euclid's lemma.
If formula_42 is a prime number and formula_43 then formula_44 or formula_45.
A positive divisor of formula_1 which is different from formula_1 is called a proper divisor or an aliquot part of formula_1. A number that does not evenly divide formula_1 but leaves a remainder is called an aliquant part of formula_1.
An integer formula_51 whose only proper divisor is 1 is called a prime number. Equivalently, a prime number is a positive integer which has exactly two positive factors: 1 and itself.
Any positive divisor of formula_1 is a product of prime divisors of formula_1 raised to some power. This is a consequence of the fundamental theorem of arithmetic.
A number formula_1 is said to be perfect if it equals the sum of its proper divisors, deficient if the sum of its proper divisors is less than formula_1, and abundant if this sum exceeds formula_1.
The total number of positive divisors of formula_1 is a multiplicative function formula_58, meaning that when two numbers formula_4 and formula_1 are relatively prime, then formula_61. For instance, formula_62; the eight divisors of 42 are 1, 2, 3, 6, 7, 14, 21 and 42. However the number of positive divisors is not a totally multiplicative function: if the two numbers formula_4 and formula_1 share a common divisor, then it might not be true that formula_61. The sum of the positive divisors of formula_1 is another multiplicative function formula_67 (e.g. formula_68). Both of these functions are examples of divisor functions.
If the prime factorization of formula_1 is given by
then the number of positive divisors of formula_1 is
and each of the divisors has the form
where formula_74 for each formula_75
For every natural formula_1, formula_77.
Also,
where formula_79 is Euler–Mascheroni constant.
One interpretation of this result is that a randomly chosen positive integer "n" has an expected
number of divisors of about formula_80.
In abstract algebra.
Given the definition for which formula_15 holds, the relation of divisibility turns the set formula_82 of non-negative integers into a partially ordered set: a complete distributive lattice. The largest element of this lattice is 0 and the smallest is 1. The meet operation ∧ is given by the greatest common divisor and the join operation ∨ by the least common multiple. This lattice is isomorphic to the dual of the lattice of subgroups of the infinite cyclic group formula_83.

</doc>
<doc id="49497" url="http://en.wikipedia.org/wiki?curid=49497" title="Pascal's triangle">
Pascal's triangle

In mathematics, Pascal's triangle is a triangular array of the binomial coefficients. In much of the Western world it is named after French mathematician Blaise Pascal, although other mathematicians studied it centuries before him in India, Iran, China, Germany, and Italy.
The rows of Pascal's triangle (sequence in OEIS) are conventionally enumerated starting with row "n" = 0 at the top (the 0th row). The entries in each row are numbered from the left beginning with "k" = 0 and are usually staggered relative to the numbers in the adjacent rows. Having the indices of both rows and columns start at zero makes it possible to state that the binomial coefficient formula_1 appears in the "n"th row and "k"th column of Pascal's triangle. A simple construction of the triangle proceeds in the following manner. In row 0, the topmost row, the entry is formula_2 (the entry in the zeroth row and zeroth column). Then, to construct the elements of the following rows, add the number above and to the left with the number above and to the right of a given position to find the new value to place in that position. If either the number to the right or left is not present, substitute a zero in its place. For example, the initial number in the first (or any other) row is 1 (the sum of 0 and 1), whereas the numbers 1 and 3 in the third row are added to produce the number 4 in the fourth row.
This construction is related to the binomial coefficients by Pascal's rule, which says that if
then
for any non-negative integer "n" and any integer "k" between 0 and "n". 
Pascal's triangle has higher dimensional generalizations. The three-dimensional version is called "Pascal's pyramid" or "Pascal's tetrahedron", while the general versions are called "Pascal's simplices".
History.
The set of numbers that form Pascal's triangle was known well before Pascal's time. Pascal does innovate many previously unattested uses of the triangle's numbers, uses he describes comprehensively in what is perhaps the earliest known mathematical treatise to be specially devoted to the triangle, his "Traité du triangle arithmétique" (1653). Still, discussion of the numbers had centuries before arisen in the context of Indian studies of combinatorics and of binomial numbers and Greeks' study of figurate numbers.
From later commentary, it appears that the binomial coefficients and the additive formula for generating them, formula_5, were known to Pingala in or before the 2nd century BC. While Pingala's work only survives in fragments, the commentator Varāhamihira, around 505, gave a clear description of the additive formula, and a more detailed explanation of the same rule was given by Halayudha, around 975. Halayudha also explained obscure references to "Meru-prastaara", the "Staircase of Mount Meru", giving the first surviving description of the arrangement of these numbers into a triangle. In approximately 850, the Jain mathematician Mahāvīra gave a different formula for the binomial coefficients, using multiplication, equivalent to the modern formula formula_6. In 1068, four columns of the first sixteen rows were given by the mathematician Bhattotpala, who was the first recorded mathematician to equate the additive and multiplicative formulas for these numbers.
At around the same time, it was discussed in Persia (Iran) by the Persian mathematician, Al-Karaji (953–1029). It was later repeated by the Persian poet-astronomer-mathematician Omar Khayyám (1048–1131); thus the triangle is referred to as the Khayyam-Pascal triangle or Khayyam triangle in Iran. Several theorems related to the triangle were known, including the binomial theorem. Khayyam used a method of finding "n"th roots based on the binomial expansion, and therefore on the binomial coefficients.
Pascal's triangle was known in China in the early 11th century through the work of the Chinese mathematician Jia Xian (1010–1070). In the 13th century, Yang Hui (1238–1298) presented the triangle and hence it is still called Yang Hui's triangle in China.
In the west, the binomial coefficients were calculated by Gersonides in the early 14th century, using the multiplicative formula for them.
Petrus Apianus (1495–1552) published the full triangle on the frontispiece of his book on business calculations in 1527. This is the first record of the triangle in Europe. Michael Stifel published a portion of the triangle (from the second to the middle column in each row) in 1544, describing it as a table of figurate numbers. In Italy, Pascal's triangle is referred to as Tartaglia's triangle, named for the Italian algebraist Niccolò Fontana Tartaglia (1500–77), who published six rows of the triangle in 1556. 
Gerolamo Cardano, also, published the triangle as well as the additive and multiplicative rules for constructing it in 1570.
Pascal's "Traité du triangle arithmétique" ("Treatise on Arithmetical Triangle") was published posthumously in 1665. In this, Pascal collected several results then known about the triangle, and employed them to solve problems in probability theory. The triangle was later named after Pascal by Pierre Raymond de Montmort (1708) who called it "Table de M. Pascal pour les combinaisons" (French: Table of Mr. Pascal for combinations) and Abraham de Moivre (1730) who called it "Triangulum Arithmeticum PASCALIANUM" (Latin: Pascal's Arithmetic Triangle), which became the modern Western name.
Binomial expansions.
Pascal's triangle determines the coefficients which arise in binomial expansions. For an example, consider the expansion
Notice the coefficients are the numbers in row two of Pascal's triangle: 1, 2, 1.
In general, when a binomial like "x" + "y" is raised to a positive integer power we have:
where the coefficients "a""i" in this expansion are precisely the numbers on row "n" of Pascal's triangle. In other words,
This is the binomial theorem.
Notice that the entire right diagonal of Pascal's triangle corresponds to the coefficient of "y""n" in these binomial expansions, while the next diagonal corresponds to the coefficient of "xy""n"−1 and so on.
To see how the binomial theorem relates to the simple construction of Pascal's triangle, consider the problem of calculating the coefficients of the expansion of ("x" + 1)"n"+1 in terms of the corresponding coefficients of ("x" + 1)"n" (setting "y" = 1 for simplicity). Suppose then that
Now
The two summations can be reorganized as follows:
(because of how raising a polynomial to a power works, "a"0 = "a""n" = 1).
We now have an expression for the polynomial ("x" + 1)"n"+1 in terms of the coefficients of ("x" + 1)"n" (these are the "a""i"s), which is what we need if we want to express a line in terms of the line above it. Recall that all the terms in a diagonal going from the upper-left to the lower-right correspond to the same power of "x", and that the a-terms are the coefficients of the polynomial ("x" + 1)"n", and we are determining the coefficients of ("x" + 1)"n"+1. Now, for any given "i" not 0 or "n" + 1, the coefficient of the "x""i" term in the polynomial ("x" + 1)"n"+1 is equal to "a""i" (the figure above and to the left of the figure to be determined, since it is on the same diagonal) + "a""i"−1 (the figure to the immediate right of the first figure). This is indeed the simple rule for constructing Pascal's triangle row-by-row.
It is not difficult to turn this argument into a proof (by mathematical induction) of the binomial theorem. Since
("a" + "b")"n" = "b""n"("a"/"b" +  1)"n", the coefficients are identical in the expansion of the general case.
An interesting consequence of the binomial theorem is obtained by setting both variables "x" and "y" equal to one. In this case, we know that (1 + 1)"n" = 2"n", and so
In other words, the sum of the entries in the "n"th row of Pascal's triangle is the "n"th power of 2.
Combinations.
A second useful application of Pascal's triangle is in the calculation of combinations. For example, the number of combinations of "n" things taken "k" at a time (called "n choose k") can be found by the equation
But this is also the formula for a cell of Pascal's triangle. Rather than performing the calculation, one can simply look up the appropriate entry in the triangle. For example, suppose a basketball team has 10 players and wants to know how many ways there are of selecting 8. Provided we have the first row and the first entry in a row numbered 0, the answer is entry 8 in row 10: 45. That is, the solution of 10 choose 8 is 45.
Relation to binomial distribution and convolutions.
When divided by 2"n", the "n"th row of Pascal's triangle becomes the binomial distribution in the symmetric case where "p" = 1/2. By the central limit theorem, this distribution approaches the normal distribution as "n" increases. This can also be seen by applying Stirling's formula to the factorials involved in the formula for combinations.
This is related to the operation of discrete convolution in two ways. First, polynomial multiplication exactly corresponds to discrete convolution, so that repeatedly convolving the sequence {..., 0, 0, 1, 1, 0, 0, ...} with itself corresponds to taking powers of 1 + "x", and hence to generating the rows of the triangle. Second, repeatedly convolving the distribution function for a random variable with itself corresponds to calculating the distribution function for a sum of "n" independent copies of that variable; this is exactly the situation to which the central limit theorem applies, and hence leads to the normal distribution in the limit.
Patterns and properties.
Pascal's triangle has many properties and contains many patterns of numbers.
Diagonals.
The diagonals of Pascal's triangle contain the figurate numbers of simplices:
The symmetry of the triangle implies that the "n"th d-dimensional number is equal to the "d"th "n"-dimensional number.
An alternative formula that does not involve recursion is as follows:
The geometric meaning of a function P"d" is: P"d"(1) = 1 for all "d". Construct a "d"-dimensional triangle (a 3-dimensional triangle is a tetrahedron) by placing additional dots below an initial dot, corresponding to P"d"(1) = 1. Place these dots in a manner analogous to the placement of numbers in Pascal's triangle. To find P"d"("x"), have a total of "x" dots composing the target shape. P"d"("x") then equals the total number of dots in the shape. A 0-dimensional triangle is a point and a 1-dimensional triangle is simply a line, and therefore P0("x") = "1" and P1("x") = "x", which is the sequence of natural numbers. The number of dots in each layer corresponds to P"d" − 1("x").
Calculating a row or diagonal by itself.
There are simple algorithms to compute all the elements in a row or diagonal without computing other elements or factorials.
To compute row formula_24 with the elements formula_25, formula_26, ..., formula_27, begin with formula_28. For each subsequent element, the value is determined by multiplying the previous value by a fraction with slowly changing numerator and denominator:
For example, to calculate row 5, the fractions are  formula_30,  formula_31,  formula_32,  formula_33 and formula_34, and hence the elements are  formula_35,   formula_36,   formula_37, etc. (The remaining elements are most easily obtained by symmetry.)
To compute the diagonal containing the elements formula_25, formula_39, formula_40, ..., we again begin with formula_28 and obtain subsequent elements by multiplication by certain fractions:
For example, to calculate the diagonal beginning at formula_43, the fractions are  formula_44,  formula_45,  formula_46, ..., and the elements are formula_35,   formula_48,   formula_49, etc. By symmetry, these elements are equal to formula_50, formula_51, formula_52, etc.
Construction as matrix exponential.
Due to its simple construction by factorials, a very basic representation of Pascal's triangle in terms of the matrix exponential can be given: Pascal's triangle is the exponential of the matrix which has the sequence 1, 2, 3, 4, … on its subdiagonal and zero everywhere else.
Number of elements of polytopes.
Pascal's triangle can be used as a lookup table for the number of elements (such as edges and corners) within a polytope (such as a triangle, a tetrahedron, a square and a cube).
Let's begin by considering the 3rd line of Pascal's triangle, with values 1, 3, 3, 1. A 2-dimensional triangle has one 2-dimensional element (itself), three 1-dimensional elements (lines, or edges), and three 0-dimensional elements (vertices, or corners). The meaning of the final number (1) is more difficult to explain (but see below). Continuing with our example, a tetrahedron has one 3-dimensional element (itself), four 2-dimensional elements (faces), six 1-dimensional elements (edges), and four 0-dimensional elements (vertices). Adding the final 1 again, these values correspond to the 4th row of the triangle (1, 4, 6, 4, 1). Line 1 corresponds to a point, and Line 2 corresponds to a line segment (dyad). This pattern continues to arbitrarily high-dimensioned hyper-tetrahedrons (known as simplices).
To understand why this pattern exists, one must first understand that the process of building an "n"-simplex from an ("n" − 1)-simplex consists of simply adding a new vertex to the latter, positioned such that this new vertex lies outside of the space of the original simplex, and connecting it to all original vertices. As an example, consider the case of building a tetrahedron from a triangle, the latter of whose elements are enumerated by row 3 of Pascal's triangle: 1 face, 3 edges, and 3 vertices (the meaning of the final 1 will be explained shortly). To build a tetrahedron from a triangle, we position a new vertex above the plane of the triangle and connect this vertex to all three vertices of the original triangle.
The number of a given dimensional element in the tetrahedron is now the sum of two numbers: first the number of that element found in the original triangle, plus the number of new elements, "each of which is built upon elements of one fewer dimension from the original triangle". Thus, in the tetrahedron, the number of cells (polyhedral elements) is 0 (the original triangle possesses none) + 1 (built upon the single face of the original triangle) = 1; the number of faces is 1 (the original triangle itself) + 3 (the new faces, each built upon an edge of the original triangle) = 4; the number of edges is 3 (from the original triangle) + 3 (the new edges, each built upon a vertex of the original triangle) = 6; the number of new vertices is 3 (from the original triangle) + 1 (the new vertex that was added to create the tetrahedron from the triangle) = 4. This process of summing the number of elements of a given dimension to those of one fewer dimension to arrive at the number of the former found in the next higher simplex is equivalent to the process of summing two adjacent numbers in a row of Pascal's triangle to yield the number below. Thus, the meaning of the final number (1) in a row of Pascal's triangle becomes understood as representing the new vertex that is to be added to the simplex represented by that row to yield the next higher simplex represented by the next row. This new vertex is joined to every element in the original simplex to yield a new element of one higher dimension in the new simplex, and this is the origin of the pattern found to be identical to that seen in Pascal's triangle. Alternately, the "extra" 1 in a row can be thought of as the empty space, or field, in which the vertices, edges, faces, etc., exist.
A similar pattern is observed relating to squares, as opposed to triangles. To find the pattern, one must construct an analog to Pascal's triangle, whose entries are the coefficients of ("x" + 2)Row Number, instead of ("x" + 1)Row Number. There are a couple ways to do this. The simpler is to begin with Row 0 = 1 and Row 1 = 1, 2. Proceed to construct the analog triangles according to the following rule:
That is, choose a pair of numbers according to the rules of Pascal's triangle, but double the one on the left before adding. This results in:
 1
 1 2
 1 4 4
 1 6 12 8
 1 8 24 32 16
 1 10 40 80 80 32
 1 12 60 160 240 192 64
 1 14 84 280 560 672 448 128
The other way of manufacturing this triangle is to start with Pascal's triangle and multiply each entry by 2k, where k is the position in the row of the given number. For example, the 2nd value in row 4 of Pascal's triangle is 6 (the slope of 1s corresponds to the zeroth entry in each row). To get the value that resides in the corresponding position in the analog triangle, multiply 6 by 2Position Number = 6 × 22 = 6 × 4 = 24. Now that the analog triangle has been constructed, the number of elements of any dimension that compose an arbitrarily dimensioned cube (called a hypercube) can be read from the table in a way analogous to Pascal's triangle. For example, the number of 2-dimensional elements in a 2-dimensional cube (a square) is one, the number of 1-dimensional elements (sides, or lines) is 4, and the number of 0-dimensional elements (points, or vertices) is 4. This matches the 2nd row of the table (1, 4, 4). A cube has 1 cube, 6 faces, 12 edges, and 8 vertices, which corresponds to the next line of the analog triangle (1, 6, 12, 8). This pattern continues indefinitely.
To understand why this pattern exists, first recognize that the construction of an "n"-cube from an ("n" − 1)-cube is done by simply duplicating the original figure and displacing it some distance (for a regular "n"-cube, the edge length) orthogonal to the space of the original figure, then connecting each vertex of the new figure to its corresponding vertex of the original. This initial duplication process is the reason why, to enumerate the dimensional elements of an "n"-cube, one must double the first of a pair of numbers in a row of this analog of Pascal's triangle before summing to yield the number below. The initial doubling thus yields the number of "original" elements to be found in the next higher "n"-cube and, as before, new elements are built upon those of one fewer dimension (edges upon vertices, faces upon edges, etc.). Again, the last number of a row represents the number of new vertices to be added to generate the next higher "n"-cube.
In this triangle, the sum of the elements of row "m" is equal to 3"m" − 1. Again, to use the elements of row 5 as an example: formula_54, which is equal to formula_55.
Fourier transform of sin("x")"n"+1/"x".
As stated previously, the coefficients of ("x" + 1)"n" are the nth row of the triangle. Now the coefficients of ("x" − 1)"n" are the same, except that the sign alternates from +1 to −1 and back again. After suitable normalization, the same pattern of numbers occurs in the Fourier transform of sin("x")"n"+1/"x". More precisely: if "n" is even, take the real part of the transform, and if "n" is odd, take the imaginary part. Then the result is a step function, whose values (suitably normalized) are given by the "n"th row of the triangle with alternating signs. For example, the values of the step function that results from:
compose the 4th row of the triangle, with alternating signs. This is a generalization of the following basic result (often used in electrical engineering):
is the boxcar function. The corresponding row of the triangle is row 0, which consists of just the number 1.
If n is congruent to 2 or to 3 mod 4, then the signs start with −1. In fact, the sequence of the (normalized) first terms corresponds to the powers of i, which cycle around the intersection of the axes with the unit circle in the complex plane:
Elementary cellular automaton.
The pattern produced by an elementary cellular automaton using rule 60 is exactly Pascal's triangle of binomial coefficients reduced modulo 2 (black cells correspond to odd binomial coefficients). Rule 102 also produces this pattern when trailing zeros are omitted. Rule 90 produces the same pattern but with an empty cell separating each entry in the rows.
Extensions.
Pascal's Triangle can be extended to negative row numbers.
First write the triangle in the following form:
Next, extend the column of 1s upwards:
Now the rule:
can be rearranged to:
which allows calculation of the other entries for negative rows:
This extension preserves the property that the values in the "m"th column viewed as a function of "n" are fit by an order "m" polynomial, namely
This extension also preserves the property that the values in the "n"th row correspond to the coefficients of (1 + "x")"n":
For example:
When viewed as a series, the rows of negative "n" diverge. However, they are still Abel summable, which summation gives the standard values of 2"n". (In fact, the "n" = -1 row results in Grandi's series which "sums" to 1/2, and the "n" = -2 row results in another well-known series which has an Abel sum of 1/4.)
Another option for extending Pascal's triangle to negative rows comes from extending the "other" line of 1s:
Applying the same rule as before leads to
Note that this extension also has the properties that just as
we have
Also, just as summing along the lower-left to upper-right diagonals of the Pascal matrix yields the Fibonacci numbers, this second type of extension still sums to the Fibonacci numbers for negative index.
Either of these extensions can be reached if we define
and take certain limits of the Gamma function, formula_67.

</doc>
<doc id="49503" url="http://en.wikipedia.org/wiki?curid=49503" title="Inductively coupled plasma mass spectrometry">
Inductively coupled plasma mass spectrometry

Inductively coupled plasma mass spectrometry (ICP-MS) is a type of mass spectrometry which is capable of detecting metals and several non-metals at concentrations as low as one part in 1015 (part per quadrillion, ppq) on non-interfered low-background isotopes. This is achieved by ionizing the sample with inductively coupled plasma and then using a mass spectrometer to separate and quantify those ions.
Compared to atomic absorption techniques, ICP-MS has greater speed, precision, and sensitivity. However, compared with other types of mass spectrometry, such as TIMS and Glow Discharge, ICP-MS introduces a lot of interfering species: argon from the plasma, component gasses of air that leak through the cone orifices, and contamination from glassware and the cones.
The variety of applications exceeds that of inductively coupled plasma atomic emission spectroscopy and includes isotopic speciation. Due to possible applications in nuclear technologies, ICP-MS hardware is a subject for special exporting regulations.
Components.
Inductively coupled plasma.
An inductively coupled plasma is a plasma that is energized (ionized) by inductively heating the gas with an electromagnetic coil, and contains a sufficient concentration of ions and electrons to make the gas electrically conductive. Even a partially ionized gas in which as little as 1% of the particles are ionized can have the characteristics of a plasma (i.e., response to magnetic fields and high electrical conductivity). The plasmas used in spectrochemical analysis are essentially electrically neutral, with each positive charge on an ion balanced by a free electron. In these plasmas the positive ions are almost all singly charged and there are few negative ions, so there are nearly equal amounts of ions and electrons in each unit volume of plasma.
An inductively coupled plasma (ICP) for spectrometry is sustained in a torch that consists of three concentric tubes, usually made of quartz, although the inner tube (injector) can be sapphire if hydrofluoric acid is being used. The end of this torch is placed inside an induction coil supplied with a radio-frequency electric current. A flow of argon gas (usually 13 to 18 liters per minute) is introduced between the two outermost tubes of the torch and an electric spark is applied for a short time to introduce free electrons into the gas stream. These electrons interact with the radio-frequency magnetic field of the induction coil and are accelerated first in one direction, then the other, as the field changes at high frequency (usually 27.12 million cycles per second). The accelerated electrons collide with argon atoms, and sometimes a collision causes an argon atom to part with one of its electrons. The released electron is in turn accelerated by the rapidly changing magnetic field. The process continues until the rate of release of new electrons in collisions is balanced by the rate of recombination of electrons with argon ions (atoms that have lost an electron). This produces a ‘fireball’ that consists mostly of argon atoms with a rather small fraction of free electrons and argon ions. The temperature of the plasma is very high, of the order of 10,000 K. The plasma also produces ultraviolet light, so for safety should not be viewed directly.
The ICP can be retained in the quartz torch because the flow of gas between the two outermost tubes keeps the plasma away from the walls of the torch. A second flow of argon (around 1 liter per minute) is usually introduced between the central tube and the intermediate tube to keep the plasma away from the end of the central tube. A third flow (again usually around 1 liter per minute) of gas is introduced into the central tube of the torch. This gas flow passes through the centre of the plasma, where it forms a channel that is cooler than the surrounding plasma but still much hotter than a chemical flame. Samples to be analyzed are introduced into this central channel, usually as a mist of liquid formed by passing the liquid sample into a nebulizer.
To maximise plasma temperature (and hence ionisation efficiency) and stability, the sample should be introduced through the central tube with as little liquid (solvent load) as possible, and with consistent droplet sizes. A nebuliser can be used for liquid samples, followed by a spray chamber to remove larger droplets, or a desolvating nebuliser can be used to evaporate most of the solvent before it reaches the torch. Solid samples can also be introduced using laser ablation. The sample enters the central channel of the ICP, evaporates, molecules break apart, and then the constituent atoms ionise. At the temperatures prevailing in the plasma a significant proportion of the atoms of many chemical elements are ionized, each atom losing its most loosely bound electron to form a singly charged ion. The plasma temperature is selected to maximise ionisation efficiency for elements with a high first ionisation energy, while minimising second ionisation (double charging) for elements that have a low second ionisation energy.
Mass spectrometry.
For coupling to mass spectrometry, the ions from the plasma are extracted through a series of cones into a mass spectrometer, usually a quadrupole. The ions are separated on the basis of their mass-to-charge ratio and a detector receives an ion signal proportional to the concentration.
The concentration of a sample can be determined through calibration with certified reference material such as single or multi-element reference standards. ICP-MS also lends itself to quantitative determinations through isotope dilution, a single point method based on an isotopically enriched standard.
Other mass analyzers coupled to ICP systems include double focusing magnetic-electrostatic sector systems with both single and multiple collector, as well as time of flight systems (both axial and orthogonal accelerators have been used).
Applications.
One of the largest volume uses for ICP-MS is in the medical and forensic field, specifically, toxicology. A physician may order a metal assay for a number of reasons, such as suspicion of heavy metal poisoning, metabolic concerns, and even hepatological issues. Depending on the specific parameters unique to each patient's diagnostic plan, samples collected for analysis can range from whole blood, urine, plasma, serum, to even packed red blood cells. Another primary use for this instrument lies in the environmental field. Such applications include water testing for municipalities or private individuals all the way to soil, water and other material analysis for industrial purposes.
In recent years, industrial and biological monitoring has presented another major need for metal analysis via ICP-MS. Individuals working in plants where exposure to metals is likely and unavoidable, such as a battery factory, are required by their employer to have their blood or urine analyzed for metal toxicity on a regular basis. This monitoring has become a mandatory practice implemented by OSHA, in an effort to protect workers from their work environment and ensure proper rotation of work duties (i.e. rotating employees from a high exposure position to a low exposure position).
Regardless of the sample type, blood, water, etc., it is important that it be free of clots or other particulate matter, as even the smallest clot can disrupt sample flow and block or clog the sample tips within the spray chamber. Very high concentrations of salts, e.g. sodium chloride in sea water, can eventually lead to blockages as some of the ions reunite after leaving the torch and build up around the orifice of the skimmer cone. This can be avoided by diluting samples whenever high salt concentrations are suspected, though at a cost to detection limits.
ICP-MS is also used widely in the geochemistry for radiometric dating, in which it is used to analyze relative abundance of different isotopes, in particular uranium and lead. ICP-MS is more suitable for this application than the previously used thermal ionization mass spectrometry, as species with high ionization energy such as osmium and tungsten can be easily ionized. For high precision ratio work, multiple collector instruments are normally used to reduce the effect noise on the calculated ratios.
In the field of flow cytometry, a new technique uses ICP-MS to replace the traditional fluorochromes. Briefly, instead of labelling antibodies (or other biological probes) with fluorochromes, each antibody is labelled with a distinct combinations of lanthanides. When the sample of interest is analysed by ICP-MS in a specialised flow cytometer, each antibody can be identified and quantitated by virtue of a distinct ICP "footprint". In theory, hundreds of different biological probes can thus be analysed in an individual cell, at a rate of ca. 1,000 cells per second. Because elements are easily distinguished in ICP-MS, the problem of compensation in multiplex flow cytometry is effectively eliminated.
In the pharmaceutical industry, ICP-MS is used for detecting inorganic impurities in pharmaceuticals and their ingredients. New and reduced maximum permitted exposure levels of heavy metals form dietary supplements, introduced in USP (United States Pharmacopeia) <232>Elemental Impurities—Limits and USP <233> Elemental Impurities—Procedures, will increase the need for ICP-MS technology, where, previously, other analytic methods have been sufficient.
Metal speciation.
A growing trend in the world of elemental analysis has revolved around the speciation of certain metals such as chromium and arsenic. One of the primary techniques to achieve this is to use an ICP-MS in combination with high-performance liquid chromatography (HPLC) or field flow fractionation (FFF).
Quantification of proteins and biomolecules.
There is an increasing trend of using ICP-MS as a tool in speciation analysis, which normally involves a front end chromatograph separation and an elemental selective detector, such as AAS and ICP-MS. For example, ICP-MS may be combined with size exclusion chromatography and quantitative preparative native continuous polyacrylamide gel electrophoresis (QPNC-PAGE) for identifying and quantifying native metal cofactor containing proteins in biofluids. Also the phosphorylation status of proteins can be analyzed.
In 2007, a new type of protein tagging reagents called metal-coded affinity tags (MeCAT) were introduced to label proteins quantitatively with metals, especially lanthanides. The MeCAT labelling allows relative and absolute quantification of all kind of proteins or other biomolecules like peptides. MeCAT comprises a site-specific biomolecule tagging group with at least a strong chelate group which binds metals. The MeCAT labelled proteins can be accurately quantified by ICP-MS down to low attomol amount of analyte which is at least 2–3 orders of magnitude more sensitive than other mass spectrometry based quantification methods. By introducing several MeCAT labels to a biomolecule and further optimization of LC-ICP-MS detection limits in the zeptomol range are within the realm of possibility. By using different lanthanides MeCAT multiplexing can be used for pharmacokinetics of proteins and peptides or the analysis of the differential expression of proteins (proteomics) e.g. in biological fluids. Breakable PAGE SDS-PAGE (DPAGE, dissolvable PAGE), two-dimensional gel electrophoresis or chromatography is used for separation of MeCAT labelled proteins. Flow-injection ICP-MS analysis of protein bands or spots from DPAGE SDS-PAGE gels can be easily performed by dissolving the DPAGE gel after electrophoresis and staining of the gel. MeCAT labelled proteins are identified and relatively quantified on peptide level by MALDI-MS or ESI-MS.
Elemental analysis.
The ICP-MS allows determination of elements with atomic mass ranges 7 to 250 (Li to U), and sometimes higher. Some masses are prohibited such as 40 due to the abundance of argon in the sample. Other blocked regions may include mass 80 (due to the argon dimer), and mass 56 (due to ArO), the latter of which greatly hinders Fe analysis unless the instrumentation is fitted with a reaction chamber. Such interferences can be reduced by using a high resolution ICP-MS (HR-ICP-MS) which uses two or more slits constrict the beam and distinguish between nearby peaks. This comes at the cost of transmission, for example to distinguish Iron from Argon by take a resolving power of 10,000, which may reduce the Iron transmission by around 99%.
A single collector ICP-MS may use a multiplier in pulse counting mode to amplify very low signals, an attenuation grid or a multiplier in anologue mode to detect medium signals, and a Faraday cup/bucket to detect larger signals. A multi-collector ICP-MS may have more than one of any of these, normally Faraday buckets which are much less expensive. With this combination, a dynamic range of 12 orders of magnitude, form 1 ppq to 100 ppm is possible.
ICP-MS is a method of choice for the determination of cadmium in biological samples.
Unlike atomic absorption spectroscopy, which can only measure a single element at a time, ICP-MS has the capability to scan for all elements simultaneously. This allows rapid sample processing. A simultaneous ICP-MS that can record the entire analytical spectrum from lithium to uranium in every analysis won the Silver Award at the 2010 Pittcon Editors' Awards. An ICP-MS may use multiple scan modes, each one striking a different balance between speed and precision. Using the magnet alone to scan is slow, due to hysteresis, but is precise. Electrostatic plates can be used in addition to the magnet to increase the speed, and this, combined with multiple collectors, can allow a scan of every element from Lithium 6 to Uranium Oxide 256 in less than a quarter of a second. For low detection limits, interfering species and high precision, the counting time can increase substantially. The rapid scanning, large dynamic range and large mass range is ideally suited to measuring multiple unknown concentrations and isotope ratios in samples that have had minimal preparation (an advantage over TIMS), for example seawater, urine, and digested whole rock samples. It also lends well to laser ablated rock samples, where the scanning rate is so quick that a real time plot of any number of isotopes is possible.This also allows easy spacial mapping of mineral grains.
Hardware.
In terms of input and output, ICP-MS instrument consumes prepared sample material and translates it into mass-spectral data. Actual analytical procedure takes some time; after that time the instrument can be switched to work on the next sample. Series of such sample measurements requires the instrument to have plasma ignited, meanwhile a number of technical parameters has to be stable in order for the results obtained to have feasibly accurate and precise interpretation. Maintaining the plasma requires a constant supply of carrier gas (usually, pure argon) and increased power consumption of the instrument. When these additional running costs are not considered justified, plasma and most of auxiliary systems can be turned off. In such standby mode only pumps are working to keep proper vacuum in mass-spectrometer.
The constituents of ICP-MS instrument are designed to allow for reproducible and/or stable operation.
Sample introduction.
The first step in analysis is the introduction of the sample. This has been achieved in ICP-MS through a variety of means.
The most common method is the use of "analytical nebulizers." Nebulizer converts liquids into an aerosol, and that aerosol can then be swept into the plasma to create the ions. Nebulizers work best with simple liquid samples (i.e. solutions). However, there have been instances of their use with more complex materials like a slurry. Many varieties of nebulizers have been coupled to ICP-MS, including pneumatic, cross-flow, Babington, ultrasonic, and desolvating types. The aerosol generated is often treated to limit it to only smallest droplets, commonly by means of a Peltier cooled double pass or cyclonic spray chamber. Use of autosamplers makes this easier and faster, especially for routine work and large numbers of samples. A Desolvating Nebuliser (DSN) may also be used; this uses a long heated capillary, coated with a fluoropolymer membrane, to remove most of the solvent and reduce the load on the plasma. Matrix removal introduction systems are sometimes used for samples, such as seawater, where the species of interest are at trace levels, and are surrounded by much more abundant contaminants.
Laser ablation is another method. While being less common in the past, is rapidly becoming popular has been used as a means of sample introduction, thanks to increased ICP-MS scanning speeds. In this method, a pulsed UV laser is focused on the sample and creates a plume of ablated material which can be swept into the plasma. This allows geochemists to spacially map the isotope composition in cross-sections of rock samples, a tool which is lost if the rock is digested and introduced as a liquid sample. Lasers for this task are built to have highly controllable power outputs and uniform radial power distributions, to produce craters which are flat bottomed and of a chosen diameter and depth.
For both Laser Ablation and Desolvating Nebulisers, a small flow of Nitrogen may also be introduced into the Argon flow. Nitrogen exists as a dimer, so has more vibrational modes and is more efficient as receiving energy from the RF coil around the torch.
Other methods of sample introduction are also utilized. Electrothermal vaporization (ETV) and in torch vaporization (ITV) use hot surfaces (graphite or metal, generally) to vaporize samples for introduction. These can use very small amounts of liquids, solids, or slurries. Other methods like vapor generation are also known.
Plasma torch.
The plasma used in an ICP-MS is made by partially ionizing argon gas (Ar → Ar+ + e−). The energy required for this reaction is obtained by pulsing an alternating electrical current in wires that surround the argon gas.
After the sample is injected, the plasma's extreme temperature causes the sample to separate into individual atoms (atomization). Next, the plasma ionizes these atoms (M → M+ + e−) so that they can be detected by the mass spectrometer.
An inductively coupled plasma (ICP) for spectrometry is sustained in a torch that consists of three concentric tubes, usually made of quartz. The two major designs are the Fassel and Greenfield torches. The end of this torch is placed inside an induction coil supplied with a radio-frequency electric current. A flow of argon gas (usually 14 to 18 liters per minute) is introduced between the two outermost tubes of the torch and an electrical spark is applied for a short time to introduce free electrons into the gas stream. These electrons interact with the radio-frequency magnetic field of the induction coil and are accelerated first in one direction, then the other, as the field changes at high frequency (usually 27.12 MHz). The accelerated electrons collide with argon atoms, and sometimes a collision causes an argon atom to part with one of its electrons. The released electron is in turn accelerated by the rapidly changing magnetic field. The process continues until the rate of release of new electrons in collisions is balanced by the rate of recombination of electrons with argon ions (atoms that have lost an electron). This produces a ‘fireball’ that consists mostly of argon atoms with a rather small fraction of free electrons and argon ions.
Advantage of argon.
Making the plasma from argon, instead of other gases, has several advantages. First, argon is abundant (in the atmosphere, as a result of the radioactive decay of potassium) and therefore cheaper than other noble gases. Argon also has a higher first ionization potential than all other elements except He, F, and Ne. Because of this high ionization energy, the reaction (Ar+ + e− → Ar) is less energetically favorable than the reaction (M+ + e− → M). This ensures that the sample remains ionized (as M+) so that the mass spectrometer can detect it.
Argon can be purchased for use with the ICP-MS in either a refrigerated liquid or a gas form. However it is important to note that whichever form of argon purchased, it should have a guaranteed purity of 99.9% Argon at a minimum. It is important to determine which type of argon will be best suited for the specific situation. Liquid argon is typically cheaper and can be stored in a greater quantity as opposed to the gas form, which is more expensive and takes up more tank space. If the instrument is in an environment where it gets infrequent use, then buying argon in the gas state will be most appropriate as it will be more than enough to suit smaller run times and gas in the cylinder will remain stable for longer periods of time, whereas liquid argon will suffer loss to the environment due to venting of the tank when stored over extended time frames. However if the ICP-MS is to be used routinely and is on and running for eight or more hours each day for several days a week, then going with liquid argon will be the most suitable. If there are to be multiple ICP-MS instruments running for long periods of time, then it will most likely be beneficial for the laboratory to install a bulk or micro bulk argon tank which will be maintained by a gas supply company, thus eliminating the need to change out tanks frequently as well as minimizing loss of argon that is left over in each used tank as well as down time for tank changeover.
There are rare ICP-MS solutions that utilize helium for plasma generation.
Transfer of ions into vacuum.
The carrier gas is sent through the central channel and into the very hot plasma. The sample is then exposed to radio frequency which converts the gas into a plasma. The high temperature of the plasma is sufficient to cause a very large portion of the sample to form ions. This fraction of ionization can approach 100% for some elements (e.g. sodium), but this is dependent on the ionization potential. A fraction of the formed ions passes through a ~1 mm hole (sampler cone) and then a ~0.4 mm hole (skimmer cone). The purpose of which is to allow a vacuum that is required by the mass spectrometer.
The vacuum is created and maintained by a series of pumps. The first stage is usually based on a roughing pump, most commonly a standard rotary vane pump. This removes most of the gas and typically reaches a pressure of around 133 Pa. Later stages have their vacuum generated by more powerful vacuum systems, most often turbomolecular pumps. Older instruments may have used oil diffusion pumps for high vacuum regions.
Ion optics.
Before mass separation, a beam of positive ions has to be extracted from the plasma and focused into the mass-analyzer. It is important to separate the ions from UV photons, energetic neutrals and from any solid particles that may have been carried into the instrument from the ICP. Traditionally, ICP-MS instruments have used transmitting ion lens arrangements for this purpose. Examples include the Einzel lens, the Barrel lens, Agilent's Omega Lens and Perkin-Elmer's Shadow Stop. Another approach is to use ion guides (quadrupoles, hexapoles, or octopoles) to guide the ions into mass analyzer along a path away from the trajectory of photons or neutral particles. Yet another approach is Varian patented used by Analytik Jena ICP-MS 90 degrees reflecting parabolic "Ion Mirror" optics, which are claimed to provide more efficient ion transport into the mass-analyzer, resulting in better sensitivity and reduced background. Baffled flight tubes and off-axis detectors are also used. Analytik Jena ICP-MS is the most sensitive instrument on the market.
A sector ICP-MS will commonly have four sections: an extraction acceleration region, steering lenses, an electrostatic sector and a magnetic sector. The first region takes ions from the plasma and accelerates them using a high voltage. The second uses may use a combination of parallel plates, rings, quadropoles, hexapoles and octopoles to steer, shape and focus the beam so that the resulting peaks are symmetrical, flat topped and have high transmission. The electrostatic sector may be before or after the magnetic sector depending on the particular instrument, and reduces the spread in kinetic energy caused by the plasma. This spread is particularly large for ICP-MS, being larger than Glow Discharge and much larger than TIMS. The geometry of the instrument is chosen so that the instrument the combined focal point of the electrostatic and magnetic sectors is at the collector, known as Double Focussing (or Double Foccussing).
If the mass of interest has a low sensitivity and is just below a much larger peak, the low mass tail from this larger peak can intrude onto the mass of interest. A Retardation Filter might be used to reduce this tail. This sits near the collector, and applies a voltage equal but opposite to the accelerating voltage; any ions that have lost energy while flying around the instrument will be decelerated to rest by the filter.
Collision reaction cell and CRI.
The collision/reaction cell is used to remove interfering ions through ion/neutral reactions. Collision/reaction cells are known under several names. The dynamic reaction cell is located before the quadrupole in the ICP-MS device. The chamber has a quadrupole and can be filled with reaction (or collision) gases (ammonia, methane, oxygen or hydrogen), with one gas type at a time or a mixture of two of them, which reacts with the introduced sample, eliminating some of the interference.
The collisional reaction interface (CRI) is a mini-collision cell installed in front of the parabolic ion mirror optics that removes interfering ions by injecting a collisional gas (He), or a reactive gas (H2), or a mixture of the two, directly into the plasma as it flows through the skimmer cone and/or the sampler cone. The CRI removed interfering ions using a collisional kinetic energy discrimination (KED) phenomenon and chemical reactions with interfering ions similarly to traditionally used larger collision cells.
Routine maintenance.
As with any piece of instrumentation or equipment, there are many aspects of maintenance that need to be encompassed by daily, weekly and annual procedures. The frequency of maintenance is typically determined by the sample volume and cumulative run time that the instrument is subjected to.
One of the first things that should be carried out before the calibration of the ICP-MS is a sensitivity check and optimization. This ensures that the operator is aware of any possible issues with the instrument and if so, may address them before beginning a calibration. Typical indicators of sensitivity are Rhodium levels, Cerium/Oxide ratios and DI water blanks.
One of the most frequent forms of routine maintenance is replacing sample and waste tubing on the peristaltic pump, as these tubes can get worn fairly quickly resulting in holes and clogs in the sample line, resulting in skewed results. Other parts that will need regular cleaning and/or replacing are sample tips, nebulizer tips, sample cones, skimmer cones, injector tubes, torches and lenses. It may also be necessary to change the oil in the interface roughing pump as well as the vacuum backing pump, depending on the workload put on the instrument.
Sample preparation.
For most clinical methods using ICP-MS, there is a relatively simple and quick sample prep process. The main component to the sample is an internal standard, which also serves as the diluent. This internal standard consists primarily of deionized water, with nitric or hydrochloric acid, and Indium and/or Gallium. Depending on the sample type, usually 5 ml of the internal standard is added to a test tube along with 10–500 microliters of sample. This mixture is then vortexed for several seconds or until mixed well and then loaded onto the autosampler tray.
For other applications that may involve very viscous samples or samples that have particulate matter, a process known as sample digestion may have to be carried out, before it can be pipetted and analyzed. This adds an extra first step to the above process, and therefore makes the sample prep more lengthy.

</doc>
<doc id="49508" url="http://en.wikipedia.org/wiki?curid=49508" title="Die Walküre">
Die Walküre

Die Walküre (The Valkyrie), WWV 86B, is an opera in three acts by Richard Wagner with a German libretto by the composer. It is the second of the four operas that form Wagner's cycle "Der Ring des Nibelungen" ("The Ring of the Nibelung").
The story of the opera is based on the Norse mythology told in the "Volsunga Saga" and the "Poetic Edda". In Norse mythology, a valkyrie is one in a group of female figures who decide which soldiers die in battle and which live. "Die Walküre"'s best-known excerpt is the "Ride of the Valkyries".
It received its premiere at the Königliches Hof- und National-Theater in Munich on 26 June 1870. Wagner originally intended the opera to be premiered as part of the entire cycle, but was forced to allow the performance at the insistence of his patron King Ludwig II of Bavaria. It was first presented as part of the complete cycle on 14 August 1876 at Wagner's Bayreuth Festival. The opera made its United States premiere at the Academy of Music in New York on 2 April 1877.
Composition.
Although "Die Walküre" is the second of the "Ring" operas, it was the third in order of conception. Wagner worked backwards from planning an opera about Siegfried's death, then deciding he needed another opera to tell of Siegfried's youth, then deciding he needed to tell the tale of Siegfried's conception and of Brünnhilde's attempts to save Siegfried's parents, and finally deciding he also needed a prelude that told of the original theft of the Rheingold and the creation of the ring.
Wagner intermingled development of the text of these last two planned operas, i.e. "Die Walküre", originally entitled "Siegmund und Sieglinde: der Walküre Bestrafung" ("Siegmund and Sieglinde: the Valkyrie's Punishment") and what became "Das Rheingold". Wagner had first written of his intention to create a trilogy of operas in the August 1851 draft of "Eine Mittheilung an meine Freunde" (A Communication to My Friends), but did not produce any sketches of the plot of "Siegmund and Sieglinde" until November. The following summer, Wagner and his wife rented the Pension Rinderknecht, a pied-à-terre on the Zürichberg (now Hochstrasse 56–58 in Zürich). There he worked on the prose draft of "Die Walküre", an extended description of the story including dialogue between 17 and 26 May 1852 and the verse draft between 1 June and 1 July. It was between these drafts that Wagner made the decision not to introduce Wotan in act 1, instead leaving the sword the god had been going to bring on stage already embedded in the tree before the action starts. The fair copy of the text was completed by 15 December 1852.
Even before the text of the "Ring" was finalised, Wagner had begun to sketch some of the music. On 23 July 1851 he wrote down on a loose sheet of paper what was to become the best-known leitmotif in the entire cycle: the theme from the "Ride of the Valkyries" ("Walkürenritt"). Other early sketches for "Die Walküre" were made in the summer of 1852. But it was not until 28 June 1854 that Wagner began to transform these into a complete draft of all three acts of the opera. This preliminary draft ("Gesamtentwurf") was completed by 27 December 1854. Much of the work of this stage of development of the opera overlapped with work on the final orchestral version of "Das Rheingold".
As Wagner had included some indication of the orchestration in the draft, he decided to move straight on to developing a full orchestral score in January 1855 without bothering to write an intermediate instrumentation draft as he had done for "Das Rheingold". This was a decision he was soon to regret, as numerous interruptions including a four-month visit to London made the task of orchestrating more difficult than he had expected. If he allowed too much time to elapse between the initial drafting of a passage and its later elaboration, he found that he could not remember how he had intended to orchestrate the draft. Consequently some passages had to be composed again from scratch. Wagner, nevertheless, persevered with the task and the full score was finally completed on 20 March 1856. The fair copy was begun on 14 July 1855 in the Swiss resort of Seelisberg, where Wagner and his wife spent a month. It was completed in Zürich on 23 March 1856, just three days after the completion of the full score.
Synopsis.
Act 1.
During a raging storm, Siegmund seeks shelter at the house of the warrior Hunding. Hunding is not present, and Siegmund is greeted by Sieglinde, Hunding's unhappy wife. Siegmund tells her that he is fleeing from enemies. After taking a drink of mead, he moves to leave, claiming to be cursed by misfortune. But Sieglinde bids him stay, saying he can bring no misfortune to the "house where ill luck lives".
Returning, Hunding reluctantly offers Siegmund the hospitality demanded by custom. Sieglinde, increasingly fascinated by the visitor, urges him to tell his tale. Siegmund describes returning home with his father one day to find his mother dead and his twin sister abducted. He then wandered with his father until he was parted from him as well. One day he found a girl being forced into marriage and fought with the girl's relatives. His weapons were broken and the bride was killed, and he was forced to flee to Hunding's home. Initially Siegmund does not reveal his name, choosing to call himself Wehwalt, 'filled with woe'.
When Siegmund finishes, Hunding reveals that he is one of Siegmund's pursuers. He grants Siegmund a night's stay, but they are to do battle in the morning. Hunding leaves the room with Sieglinde, ignoring his wife's distress. Siegmund laments his misfortune, recalling his father's promise that he would find a sword when he most needed it.
Sieglinde returns, having drugged Hunding's drink to send him into a deep sleep. She reveals that she was forced into a marriage with Hunding. During their wedding feast, an old man appeared and plunged a sword into the trunk of the ash tree in the center of the room, which neither Hunding nor any of his companions could remove. She expresses her longing for the hero who could draw the sword and save her. Siegmund expresses his love for her, which she reciprocates, and as she strives to understand her recognition of him, she realises it is in the echo of her own voice, and reflection of her image, that she already knows him. When he speaks the name of his father, Wälse, she declares that he is Siegmund, and that the Wanderer left the sword for him.
Siegmund now easily draws the sword forth, and she tells him she is Sieglinde, his twin sister. He names the blade "Nothung" (or "needful", for this is the weapon that he needs for his forthcoming fight with Hunding). As the act closes he calls her "bride and sister", and draws her to him with passionate fervour.
Act 2.
Wotan is standing on a rocky mountainside with Brünnhilde, his Valkyrie daughter. He instructs Brünnhilde to protect Siegmund in his coming fight with Hunding. Fricka, Wotan's wife and the guardian of wedlock, arrives demanding the punishment of Siegmund and Sieglinde, who have committed adultery and incest. She knows that Wotan, disguised as the mortal man Wälse, fathered Siegmund and Sieglinde. Wotan protests that he requires a free hero (i.e., one not ruled by him) to aid his plans, but Fricka retorts that Siegmund is not a free hero but Wotan's creature and unwitting pawn. Backed into a corner, Wotan promises Fricka that Siegmund will die.
Fricka exits, leaving Brünnhilde with a despairing Wotan. Wotan explains his problems: troubled by the warning delivered by Erda (at the end of "Das Rheingold"), he had seduced the earth-goddess to learn more of the prophesied doom; Brünnhilde was born to him by Erda. He raised Brünnhilde and eight other daughters as the Valkyries, warrior maidens who gather the souls of fallen heroes to form an army against Alberich. Valhalla's army will fail if Alberich should ever wield the ring, which is in Fafner's possession. The giant has transformed himself into a dragon, lurking in a forest with the Nibelung treasure. Wotan cannot wrest the ring from Fafner, who is bound to him by contract; he needs a free hero to defeat Fafner in his stead. But as Fricka pointed out, he can create only thralls (i.e. servants) to himself. Bitterly, Wotan orders Brünnhilde to obey Fricka and ensure the death of his beloved child Siegmund.
Having fled Hunding's hall, Siegmund and Sieglinde enter the mountain pass, where Sieglinde faints in guilt and exhaustion. Brünnhilde approaches Siegmund and tells him of his impending death. Siegmund refuses to follow Brünnhilde to Valhalla when she tells him Sieglinde cannot accompany him there. Siegmund dismisses Brünnhilde's warning since he has Wälse's sword, which his father assured him would win victory for him, but Brünnhilde tells him it has lost its power. Siegmund draws his sword and threatens to kill both Sieglinde and himself. Impressed by his passion, Brünnhilde relents and agrees to grant victory to Siegmund instead of Hunding.
Hunding arrives and attacks Siegmund. Blessed by Brünnhilde, Siegmund begins to overpower Hunding, but Wotan appears and shatters Nothung (Siegmund's sword) with his spear. While Siegmund is thus disarmed and helpless, Hunding stabs him to death. Wotan looks down on Siegmund's body, grieving, and Brünnhilde gathers up the fragments of Nothung and flees on horseback with Sieglinde. Wotan strikes Hunding dead with a contemptuous gesture, and angrily sets out in pursuit of his disobedient daughter.
Act 3.
The other Valkyries assemble on the summit of a mountain, each with a dead hero in her saddlebag. They are astonished when Brünnhilde arrives with Sieglinde, a living woman. She begs them to help, but they dare not defy Wotan. Brünnhilde decides to delay Wotan as Sieglinde flees. She also reveals that Sieglinde is pregnant by Siegmund, and names the unborn son Siegfried.
Wotan arrives in wrath and passes judgement on Brünnhilde: she is to be stripped of her Valkyrie status and become a mortal woman, to be held in a magic sleep on the mountain, prey to any man who happens by. Dismayed, the other Valkyries flee. Brünnhilde begs mercy of Wotan for herself, his favorite child. She recounts the courage of Siegmund and her decision to protect him, knowing that was Wotan's true desire. With the words 'Der diese Liebe mir ins Herz gehaucht' ("He who breathed this love into me"), introducing the key of E major, she identifies her actions as Wotan's true will. Wotan consents to her last request: to encircle the mountaintop with magic flame, which will deter all but the bravest of heroes (who, as shown through the leitmotif, they both know will be the yet unborn Siegfried). Wotan lays Brünnhilde down on a rock and, in a long embrace, kisses her eyes closed into an enchanted sleep. He summons Loge (the Norse demigod of fire) to ignite the circle of flame that will protect her, then slowly departs in sorrow, after pronouncing: "Whosoever fears the point of my spear shall not pass through the fire." The curtain falls as the Magic Fire Music again resolves into E major.

</doc>
<doc id="49511" url="http://en.wikipedia.org/wiki?curid=49511" title="Siegfried (opera)">
Siegfried (opera)

Siegfried, WWV 86C, is the third of the four operas that constitute "Der Ring des Nibelungen" ("The Ring of the Nibelung"), by Richard Wagner. It premiered at the Bayreuth Festspielhaus on 16 August 1876, as part of the first complete performance of "The Ring". This part of the opera is primarily inspired by the story of the legendary hero Sigurd in Norse mythology.
Composition history.
Sources.
Siegfried is a man without fear, and he expresses to his foster father Mime his wish to learn to fear. Mime tells him that the wise learn fear quickly, but the stupid find it more difficult. In a letter to his friend Theodor Uhlig, Wagner recounted "The Story of the Youth Who Went Forth to Learn What Fear Was", about a boy so stupid he had never learned to be afraid. Wagner wrote that the boy and Siegfried are the same character. The boy is taught to fear by his wife, and Siegfried learns it when he discovers the sleeping Brünnhilde.
Act 3, Scene 1 (between The Wanderer and Erda) is based on the Eddic poem Baldrs draumar.
Synopsis.
Act 1.
In a cave in the forest, the Nibelung dwarf Mime, Alberich's brother, is forging a sword. Mime is plotting to obtain the ring for himself. He has raised the human boy Siegfried as a foster child, to kill the dragon, Fafner, who guards the ring and other treasures. He needs a sword for Siegfried to use, but the youth has broken every sword Mime has made. Siegfried returns from his wanderings in the forest with a wild bear that he caught and demands his new sword, which he immediately breaks. After Siegfried's tantrum and a carefully studied speech by Mime about Siegfried's ingratitude toward him, Siegfried comes to understand why he keeps coming back to Mime although he despises him: he wants to know his parentage. Mime is forced to explain how he took in Siegfried's mother, Sieglinde, who then died, giving birth to Siegfried. He shows Siegfried the broken pieces of Nothung, which Mime had obtained from her. Siegfried orders him to reforge the sword; Mime, however, has been unable to accomplish this because the metal refuses to yield to his best techniques.
Siegfried departs, leaving Mime in despair. An old man (Wotan in disguise) arrives at the door and introduces himself as the Wanderer. In return for the hospitality due a guest, he wagers his head on answering any three questions or riddles from Mime. The dwarf agrees in order to get rid of his unwelcome guest. He asks the Wanderer to name the races that live beneath the ground, on the earth, and in the skies. These are the Nibelung, the Giants, and the Gods, as the Wanderer answers correctly. Mime tells the Wanderer to be on his way but is forced to wager his own head on three more riddles for breaking the law of hospitality. The Wanderer asks him to name the race most beloved of Wotan, but most harshly treated; the name of the sword that can destroy Fafner; and the person who can repair the sword. Mime answers the first two questions: the Wälsungs and Nothung. However, he cannot answer the last. Wotan spares Mime, telling him that only "he who does not know fear" can reforge Nothung, and leaves Mime's head forfeit to that person.
Siegfried returns and is annoyed by Mime's lack of progress. Mime realizes that Siegfried is "the one who does not know fear" and that unless he can instill fear in him, Siegfried will kill him in accordance with the Wanderer's prediction. He tells Siegfried that fear is an essential craft; Siegfried is eager to learn it, and Mime promises to teach him by taking him to Fafner. Since Mime was unable to forge Nothung, Siegfried decides to do it himself. He succeeds by shredding the metal, melting it, and casting it anew. In the meantime, Mime brews a poisoned drink to offer Siegfried after the youth has defeated the dragon. After he finishes forging the sword, Siegfried demonstrates its strength by chopping the anvil in half with it.
Act 2.
The Wanderer arrives at the entrance to Fafner's cave, outside of which Alberich has been keeping vigil. The old enemies quickly recognize each other. Alberich blusters, boasting of his plans for regaining the ring and ruling the world. Wotan calmly states that he does not intend to interfere, only to observe. He even offers to awaken Fafner so that Alberich can bargain with him. Alberich warns the dragon that a hero is coming to fight him, and offers to prevent the fight in return for the ring. Fafner dismisses the threat, declines Alberich's offer, and returns to sleep. Wotan leaves and Alberich withdraws.
At daybreak, Siegfried and Mime arrive. Mime decides to draw back while Siegfried confronts the dragon. As Siegfried waits for the dragon to appear, he notices a woodbird in a tree. Befriending it, he attempts to mimic the bird's song using a reed pipe, but is unsuccessful. He then plays a tune on his horn, which brings Fafner out of his cave. After a short exchange, they fight; Siegfried stabs Fafner in the heart with Nothung.
In his last moments, Fafner learns Siegfried's name, and tells him to beware of treachery. When Siegfried draws his sword from the corpse, his hands are burned by the dragon's blood, and he instinctively puts them to his mouth. On tasting the blood, he finds that he can understand the woodbird's song. Following its instructions, he takes the ring and the Tarnhelm from Fafner's hoard. Outside the cave, Alberich and Mime quarrel loudly over the treasure. Alberich hides as Siegfried comes out of the cave. Mime greets Siegfried; Siegfried complains that he has still not learned the meaning of fear. Mime offers him the poisoned drink. However, the lingering effect of the dragon's blood allows Siegfried to read Mime's treacherous thoughts, and he stabs him to death. Alberich, observing from offstage, laughs sadistically. Siegfried then throws Mime's body into the treasure cave and places Fafner's body in the cave entrance to block it as well.
The woodbird now sings of a woman sleeping on a rock surrounded by magic fire. Siegfried, wondering if he can learn fear from this woman, heads toward the mountain.
Act 3.
The Wanderer appears on the path to Brünnhilde's rock and summons Erda, the earth goddess. Erda, appearing confused, is unable to offer any advice. Wotan informs her that he no longer fears the end of the gods; indeed, it is his desire. His heritage will be left to Siegfried the Wälsung, and their (Erda's and Wotan's) child, Brünnhilde, will "work the deed that redeems the World." Dismissed, Erda sinks back into the earth.
Siegfried arrives, and the Wanderer questions the youth. Siegfried, who does not recognize his grandfather, answers insolently and starts down the path toward Brünnhilde's rock. The Wanderer blocks his path, but Siegfried breaks his spear with a blow from Nothung. Wotan calmly gathers up the pieces and vanishes.
Siegfried enters the ring of fire, emerging on Brünnhilde's rock. At first, he thinks the armored figure is a man. However, when he removes the armor, he finds a woman beneath. At the sight of the first woman he has ever seen, Siegfried at last experiences fear. In desperation, he kisses Brünnhilde, waking her from her magic sleep. Hesitant at first, Brünnhilde is won over by Siegfried's love, and renounces the world of the gods. Together, they hail "light-bringing love, and laughing death."
Noted excerpts.
As with the rest of the "Ring", a few excerpts are heard outside the opera house. The most commonly heard excerpt from "Siegfried" is the Forest Murmurs.
Other famous excerpts include
References.
Notes

</doc>
<doc id="49513" url="http://en.wikipedia.org/wiki?curid=49513" title="Bernhard von Reesen">
Bernhard von Reesen

Bernhard von Reesen (born ca. 1490) was an art collector born to a patrician family in Danzig (Gdańsk), in Polish Royal Prussia.
Reesen received an extensive education as was the custom of that time for men of his background. As did many other well-off business men and statesmen, at the age of thirty, Reesen commissioned Albrecht Dürer of Nuremberg to paint his portrait in 1520/21. The painting can still be seen at the exhibit of Old Masters in Dresden, Germany.
The 'von Reesen' name indicates that his family hails from the city of Rees, Germany.

</doc>
<doc id="49514" url="http://en.wikipedia.org/wiki?curid=49514" title="Henri Lebesgue">
Henri Lebesgue

Henri Léon Lebesgue ForMemRS (]; June 28, 1875 – July 26, 1941) was a French mathematician most famous for his theory of integration, which was a generalization of the 17th century concept of integration—summing the area between an axis and the curve of a function defined for that axis. His theory was published originally in his dissertation "Intégrale, longueur, aire" ("Integral, length, area") at the University of Nancy during 1902.
Personal life.
Henri Lebesgue was born on 28 June 1875 in Beauvais, Oise. Lebesgue's father was a typesetter and his mother was a school teacher. His parents assembled at home a library that the young Henri was able to use. Unfortunately his father died of tuberculosis when Lebesgue was still very young and his mother had to support him by herself. As he showed a remarkable talent for mathematics in primary school, one of his instructors arranged for community support to continue his education at the Collège de Beauvais and then at Lycée Saint-Louis and Lycée Louis-le-Grand in Paris.
In 1894 Lebesgue was accepted at the École Normale Supérieure, where he continued to focus his energy on the study of mathematics, graduating in 1897. After graduation he remained at the École Normale Supérieure for two years, working in the library, where he became aware of the research on discontinuity done at that time by René-Louis Baire, a recent graduate of the school. At the same time he started his graduate studies at the Sorbonne, where he learned about Émile Borel's work on the incipient measure theory and Camille Jordan's work on the Jordan measure. In 1899 he moved to a teaching position at the Lycée Central in Nancy, while continuing work on his doctorate. In 1902 he earned his Ph.D. from the Sorbonne with the seminal thesis on "Integral, Length, Area", submitted with Borel, four years older, as advisor.
Lebesgue married the sister of one of his fellow students, and he and his wife had two children, Suzanne and Jacques.
After publishing his thesis, Lebesgue was offered in 1902 a position at the University of Rennes, lecturing there until 1906, when he moved to the Faculty of Sciences of the University of Poitiers. In 1910 Lebesgue moved to the Sorbonne as a maître de conférences, being promoted to professor starting with 1919. In 1921 he left the Sorbonne to become professor of mathematics at the Collège de France, where he lectured and did research for the rest of his life. In 1922 he was elected a member of the Académie française. Henri Lebesgue died on 26 July 1941 in Paris.
Mathematical career.
Lebesgue's first paper was published in 1898 and was titled "Sur l'approximation des fonctions". It dealt with Weierstrass' theorem on approximation to continuous functions by polynomials. Between March 1899 and April 1901 Lebesgue published six notes in "Comptes Rendus." The first of these, unrelated to his development of Lebesgue integration, dealt with the extension of Baire's theorem to functions of two variables. The next five dealt with surfaces applicable to a plane, the area of skew polygons, surface integrals of minimum area with a given bound, and the final note gave the definition of Lebesgue integration for some function f(x). Lebesgue's great thesis, "Intégrale, longueur, aire", with the full account of this work, appeared in the Annali di Matematica in 1902. The first chapter develops the theory of measure (see Borel measure). In the second chapter he defines the integral both geometrically and analytically. The next chapters expand the "Comptes Rendus" notes dealing with length, area and applicable surfaces. The final chapter deals mainly with Plateau's problem. This dissertation is considered to be one of the finest ever written by a mathematician.
His lectures from 1902 to 1903 were collected into a "Borel tract" "Leçons sur l'intégration et la recherche des fonctions primitives". The problem of integration regarded as the search for a primitive function is the keynote of the book. Lebesgue presents the problem of integration in its historical context, addressing Augustin-Louis Cauchy, Peter Gustav Lejeune Dirichlet, and Bernhard Riemann. Lebesgue presents six conditions which it is desirable that the integral should satisfy, the last of which is "If the sequence fn(x) increases to the limit f(x), the integral of fn(x) tends to the integral of f(x)." Lebesgue shows that his conditions lead to the theory of measure and measurable functions and the analytical and geometrical definitions of the integral.
He turned next to trigonometric functions with his 1903 paper "Sur les séries trigonométriques". He presented three major theorems in this work: that a trigonometrical series
representing a bounded function is a Fourier series, that the nth Fourier coefficient tends to zero (the Riemann–Lebesgue lemma), and that a Fourier series is integrable term by term. In 1904-1905 Lebesgue lectured once again at the Collège de France, this time on trigonometrical series and he went on to publish his lectures in another of the "Borel tracts". In this tract he once again treats the subject in its historical context. He expounds on Fourier series, Cantor-Riemann theory, the Poisson integral and the Dirichlet problem.
In a 1910 paper, "Représentation trigonométrique approchée des fonctions satisfaisant a une condition de Lipschitz" deals with the Fourier series of functions satisfying a Lipschitz condition, with an evaluation of the order of magnitude of the remainder term. He also proves that the Riemann–Lebesgue lemma is a best possible result for continuous functions, and gives some treatment to Lebesgue constants.
Lebesgue once wrote, "Réduites à des théories générales, les mathématiques seraient une belle forme sans contenu." ("Reduced to general theories, mathematics would be a beautiful form without content.")
In measure-theoretic analysis and related branches of mathematics, the Lebesgue–Stieltjes integral generalizes Riemann–Stieltjes and Lebesgue integration, preserving the many advantages of the latter in a more general measure-theoretic framework.
During the course of his career, Lebesgue also made forays into the realms of complex analysis and topology. He also had a disagreement with Borel (called "teilweise heftig") with regards to effective calculation. However, these minor forays pale in comparison to his contributions to real analysis; his contributions to this field had a tremendous impact on the shape of the field today and his methods have become an essential part of modern analysis. Additionally, he is claimed to be the last of the mathematicians to consider one to be a prime number.
Lebesgue's theory of integration.
Integration is a mathematical operation that corresponds to the informal idea of finding the area under the graph of a function. The first theory of integration was developed by Archimedes in the 3rd century BC with his method of quadratures, but this could be applied only in limited circumstances with a high degree of geometric symmetry. In the 17th century, Isaac Newton and Gottfried Wilhelm Leibniz discovered the idea that integration was intrinsically linked to differentiation, the latter being a way of measuring how quickly a function changed at any given point on the graph. This surprising relationship between two major geometric operations in calculus, differentiation and integration, is now known as the Fundamental Theorem of Calculus. It has allowed mathematicians to calculate a broad class of integrals for the first time. However, unlike Archimedes' method, which was based on Euclidean geometry, mathematicians felt that Newton's and Leibniz's integral calculus did not have a rigorous foundation.
In the 19th century, Augustin Cauchy developed epsilon-delta limits, and Bernhard Riemann followed up on this by formalizing what is now called the Riemann integral. To define this integral, one fills the area under the graph with smaller and smaller rectangles and takes the limit of the sums of the areas of the rectangles at each stage. For some functions, however, the total area of these rectangles does not approach a single number. As such, they have no Riemann integral.
Lebesgue invented a new method of integration to solve this problem.
Instead of using the areas of rectangles, which put the focus on the domain of the function, Lebesgue looked at the codomain of the function for his fundamental unit of area.
Lebesgue's idea was to first define measure, for both sets and functions on those sets. He then proceeded to build the integral for what he called simple functions; measurable functions that take only finitely many values.
Then he defined it for more complicated functions as the least upper bound of all the integrals of simple functions smaller than the function in question.
Lebesgue integration has the property that every function defined over a bounded interval with a Riemann integral also has a Lebesgue integral, and for those functions the two integrals agree. Furthermore, every bounded function on a closed bounded interval has a Lebesgue integral and there are many functions with a Lebesgue integral that have no Riemann integral.
As part of the development of Lebesgue integration, Lebesgue invented the concept of measure, which extends the idea of length from intervals to a very large class of sets, called measurable sets (so, more precisely, simple functions are functions that take a finite number of values, and each value is taken on a measurable set).
Lebesgue's technique for turning a measure into an integral generalises easily to many other situations, leading to the modern fielld of measure theory.
The Lebesgue integral is deficient in one respect.
The Riemann integral generalises to the improper Riemann integral to measure functions whose domain of definition is not a closed interval.
The Lebesgue integral integrates many of these functions (always reproducing the same answer when it did), but not all of them.
For functions on the real line, the Henstock integral is an even more general notion of integral (based on Riemann's theory rather than Lebesgue's) that subsumes both Lebesgue integration and improper Riemann integration.
However, the Henstock integral depends on specific ordering features of the real line and so does not generalise to allow integration in more
general spaces (say, manifolds), while the Lebesgue integral extends to such spaces quite naturally.

</doc>
<doc id="49519" url="http://en.wikipedia.org/wiki?curid=49519" title="Transportation (disambiguation)">
Transportation (disambiguation)

Transportation may refer to:

</doc>
<doc id="49522" url="http://en.wikipedia.org/wiki?curid=49522" title="Anwar Sadat">
Anwar Sadat

Muhammad Anwar El Sadat (Arabic: محمد أنور السادات‎ "Muḥammad Anwar as-Sādāt " ]; 25 December 1918 – 6 October 1981) was the third President of Egypt, serving from 15 October 1970 until his assassination by fundamentalist army officers on 6 October 1981. Sadat was a senior member of the Free Officers who overthrew King Farouk in the Egyptian Revolution of 1952, and a close confidant of President Gamal Abdel Nasser, under whom he served as Vice President twice and whom he succeeded as President in 1970.
In his eleven years as president, he changed Egypt's trajectory, departing from many of the political, and economic tenets of Nasserism, re-instituting a multi-party system, and launching the Infitah economic policy. As President, he led Egypt in the Yom Kippur War of 1973 to regain Egypt's Sinai Peninsula, which Israel had occupied since the Six-Day War of 1967, making him a hero in Egypt and, for a time, the wider Arab World. Afterwards, he engaged in negotiations with Israel, culminating in the Egypt–Israel Peace Treaty; this won him and Israeli Prime Minister Menachem Begin the Nobel Peace Prize, making Sadat the first Muslim Nobel laureate. Though reaction to the treaty—which resulted in the return of Sinai to Egypt—was generally favorable among Egyptians, it was rejected by the country's Muslim Brotherhood and leftists in particular, who felt Sadat had abandoned efforts to ensure a Palestinian state. With the exception of Sudan, the Arab world and the Palestine Liberation Organization (PLO) strongly opposed Sadat's efforts to make a separate peace with Israel without prior consultations with the Arab states. His refusal to reconcile with them over the Palestinian issue resulted in Egypt being suspended from the Arab League from 1979 to 1989. The peace treaty was also one of the primary factors that led to his assassination.
Early life and revolutionary activities.
Anwar Sadat was born on 25 December 1918 in Mit Abu al-Kum, al-Minufiyah, Egypt to a poor family, one of 13 brothers and sisters. One of his brothers, Atef Sadat, later became a pilot and was killed in action during the October War of 1973. His father, Anwar Mohammed El Sadat was an Upper Egyptian, and his mother, Sit Al-Berain, was a Sudanese from her father. Thus, he faced insults by his Arab opponents in Egypt for not looking "Egyptian enough" and "Nasser's black poodle." He spent his early childhood under the care of his grandmother, who told him stories revolving around resistance to the British occupation and drawing on contemporary history. During Sadat's childhood, he admired and was influenced greatly by four individuals. The first of his childhood heroes was Zahran, the alleged hero of the Denshawai Incident, who resisted the British occupation in a farmer protest. According to the story, a British soldier was killed, and Zahran was the first Egyptian hanged in retribution. Stories like the Ballad of Zahran introduced Sadat to Egyptian nationalism, a value he held throughout his life.
The second individual was Mustafa Kemal Atatürk, who was the leader of contemporary Turkey. Sadat admired his ability to overthrow the foreign influence, and his many social reforms. He also idolized Mahatma Gandhi and his belief in non-violent struggle or Ahimsa when facing injustice. During the period when Egypt was a protectorate of the United Kingdom, Sadat was fascinated by Hitler's Nazi German army for their quick ability to become a strategic threat to Britain.
He graduated from the Royal Military Academy in Cairo in 1938 and was appointed to the Signal Corps. He entered the army as a second lieutenant and was posted to Sudan (Egypt and Sudan were one country at the time). There, he met Gamal Abdel Nasser, and along with several other junior officers they formed the secret Free Officers, a movement committed to freeing Egypt and Sudan from British domination, and royal corruption.
During the Second World War he was imprisoned by the British for his efforts to obtain help from the Axis Powers in expelling the occupying British forces. Anwar Sadat was active in many political movements, including the Muslim Brotherhood, the fascist Young Egypt, the pro-palace Iron Guard of Egypt, and the secret military group called the Free Officers . Along with his fellow Free Officers, Sadat participated in the military coup that launched the Egyptian Revolution of 1952, which overthrew King Farouk on 23 July of that year. Sadat was assigned to announce the news of the revolution to the Egyptian people over the radio networks.
During Nasser's presidency.
During the presidency of Gamal Abdel Nasser, Sadat was appointed minister of State in 1954. He was also appointed editor of the newly founded daily "Al Gomhuria". In 1959, he assumed the position of Secretary to the National Union. Sadat was the President of the National Assembly (1960–1968) and then vice president and member of the presidential council in 1964. He was reappointed as vice president again in December 1969.
Presidency.
Some of the major events of the Sadat's presidency were his "Corrective Revolution" to consolidate power, the break with Egypt's long-time ally and aid-giver the USSR, the 1973 October War with Israel, the Camp David peace treaty with Israel, the "opening up" (or Infitah) of Egypt's economy, and finally his assassination in 1981.
Sadat succeeded Nasser as president after the latter's death in October 1970. Sadat's presidency was widely expected to be short-lived. Viewing him as having been little more than a puppet of the former president, Nasser's supporters in government settled on Sadat as someone they could manipulate easily. Sadat surprised everyone with a series of astute political moves by which he was able to retain the presidency and emerge as a leader in his own right. On 15 May 1971, Sadat announced his "Corrective Revolution", purging the government, political and security establishments of the most ardent Nasserists. Sadat encouraged the emergence of an Islamist movement, which had been suppressed by Nasser. Believing Islamists to be socially conservative he gave them "considerable cultural and ideological autonomy" in exchange for political support.
In 1971, three years into the War of Attrition in the Suez Canal zone, Sadat endorsed in a letter the peace proposals of UN negotiator Gunnar Jarring, which seemed to lead to a full peace with Israel on the basis of Israel's withdrawal to its pre-war borders. This peace initiative failed as neither Israel nor the United States of America accepted the terms as discussed then.
Corrective Revolution.
Shortly after taking office, Sadat shocked many Egyptians by dismissing and imprisoning two of the most powerful figures in the regime, Vice President Ali Sabri, who had close ties with Soviet officials, and Sharawy Gomaa, the Interior Minister, who controlled the secret police. Sadat's rising popularity would accelerate after he cut back the powers of the hated secret police, expelled Soviet military from the country and reformed the Egyptian army for a renewed confrontation with Israel. During this time, Egypt was suffering greatly from economic problems caused by the Six-Day War and the Soviet relationship also declined due to their unreliability and refusal of Sadat's requests for more military support.
Yom Kippur War.
On 6 October 1973, in conjunction with Hafez al-Assad of Syria, Sadat launched the October War, also known as the Yom Kippur War (and less commonly as the Ramadan War), a surprise attack against the Israeli forces occupying the Egyptian Sinai Peninsula, and the Syrian Golan Heights in an attempt to retake these respective Egyptian and Syrian territories that had been occupied by Israel since the Six Day War six years earlier. The Egyptian and Syrian performance in the initial stages of the war astonished both Israel, and the Arab World. The most striking achievement (Operation Badr, also known as The Crossing) was the Egyptian military's advance approximately 15 km into the occupied Sinai Peninsula after penetrating and largely destroying the Bar Lev Line. This line was popularly thought to have been an impregnable defensive chain.
As the war progressed, three divisions of the Israeli army led by General Ariel Sharon had crossed the Suez Canal, trying to encircle first the Egyptian Second Army, and, when this failed, the Egyptian Third Army. Prompted by an agreement between the United States of America, and the Soviet Union, the United Nations Security Council passed Resolution 338 on 22 October 1973, calling for an immediate ceasefire. Although agreed upon, the ceasefire was immediately broken. Alexei Kosygin, the Chairman of the USSR Council of Ministers, cancelled an official meeting with Danish Prime Minister Anker Jørgensen to travel to Egypt where he tried to persuade Sadat to sign a peace treaty. During Kosygin's two-day long stay it is unknown if he and Sadat ever met in person. The Israeli military then continued their drive to encircle the Egyptian army. The encirclement was completed on 24 October, three days after the ceasefire was broken. This development prompted superpower tension, but a second ceasefire was imposed cooperatively on 25 October to end the war. At the conclusion of hostilities, Israeli forces were 40 kilometres (25 mi) from Damascus and 101 kilometres (63 mi) from Cairo.
Peace with Israel.
The initial Egyptian and Syrian victories in the war restored popular morale throughout Egypt and the Arab World and, for many years after, Sadat was known as the "Hero of the Crossing". Efforts to make peace with Israel through diplomacy would soon gain popular support among Egyptians as well. Israel recognized Egypt as a formidable foe, and Egypt's renewed political significance eventually led to regaining and reopening the Suez Canal through the peace process. His new peace policy led to the conclusion of two agreements on disengagement of forces with the Israeli government. The first of these agreements was signed on 18 January 1974, and the second on 4 September 1975.
One major aspect of Sadat's peace policy was to gain some religious support for his efforts. Already during his visit to the US in October–November 1975, he invited Evangelical pastor Billy Graham for an official visit, which was held a few days after Sadat's visit. In addition to cultivating relations with Evangelical Christians in the US, he also built some cooperation with the Vatican. On 8 April 1976, he visited the Vatican for the first time, and got a message of support from Pope Paul VI regarding achieving peace with Israel, to include a just solution to the Palestinian issue. Sadat, on his part, extended to the Pope a public invitation to visit Cairo.
Sadat also used the media to promote his purposes. In an interview he gave to the Lebanese paper "El Hawadeth" in early February 1976, he claimed he had secret commitment from the US government to put pressure on the Israeli government for a major withdrawal in Sinai and the Golan Heights. This statement caused some concern to the Israeli government, but Kissinger denied such a promise was ever made.
In January 1977, a series of 'Bread Riots' protested Sadat's economic liberalization and specifically a government decree lifting price controls on basic necessities like bread. The riots lasted for two days and included hundreds of thousands in Cairo. 120 buses and hundreds of buildings were destroyed in Cairo alone. The riots ended with the deployment of the army and the re-institution of the subsidies/price controls. During this time, Sadat was also taking a new approach towards improving relations with the West.
The United States and the Soviet Union agreed on 1 October 1977, on principles to govern a Geneva conference on the Middle East. Syria continued to resist such a conference. Not wanting either Syria or the Soviet Union to influence the peace process, Sadat decided to take more progressive stance towards building a comprehensive peace agreement with Israel.
On 19 November 1977, Sadat became the first Arab leader to visit Israel officially when he met with Israeli Prime Minister Menachem Begin, and spoke before the Knesset in Jerusalem about his views on how to achieve a comprehensive peace to the Arab–Israeli conflict, which included the full implementation of UN Resolutions 242 and 338. He said during his visit that he hopes "that we can keep the momentum in Geneva, and may God guide the steps of Premier Begin and Knesset, because there is a great need for hard and drastic decision".
The Peace treaty was finally signed by Anwar Sadat and Israeli Prime Minister Menachem Begin in Washington, D.C., United States, on 26 March 1979, following the Camp David Accords (1978), a series of meetings between Egypt and Israel facilitated by US President Jimmy Carter. Both Sadat and Begin were awarded the Nobel Peace Prize for creating the treaty. In his acceptance speech, Sadat referred to the long awaited peace desired by both Arabs and Israelis:
Let us put an end to wars, let us reshape life on the solid basis of equity and truth. And it is this call, which reflected the will of the Egyptian people, of the great majority of the Arab and Israeli peoples, and indeed of millions of men, women, and children around the world that you are today honoring. And these hundreds of millions will judge to what extent every responsible leader in the Middle East has responded to the hopes of mankind.
The main features of the agreement were the mutual recognition of each country by the other, the cessation of the state of war that had existed since the 1948 Arab–Israeli War, and the complete withdrawal by Israel of its armed forces and civilians from the rest of the Sinai Peninsula, which Israel had captured during the 1967 Six-Day War.
The agreement also provided for the free passage of Israeli ships through the Suez Canal and recognition of the Strait of Tiran and the Gulf of Aqaba as international waterways. The agreement notably made Egypt the first Arab country to officially recognize Israel. The peace agreement between Egypt and Israel has remained in effect since the treaty was signed.
The treaty was extremely unpopular in most of the Arab World and the wider Muslim World. His predecessor Nasser had made Egypt an icon of Arab nationalism, an ideology that appeared to be sidelined by an Egyptian orientation following the 1973 war (see Egypt). The neighboring Arab countries believed that in signing the accords, Sadat had put Egypt's interests ahead of Arab unity, betraying Nasser's pan-Arabism, and destroyed the vision of a united "Arab front" for the support of the Palestinians against the "Zionist Entity". However, Sadat decided early on that peace is the solution. Sadat's shift towards a strategic relationship with the US was also seen as a betrayal by many Arabs. In the United States his peace moves gained him popularity among some Evangelical circles. He was awarded the Prince of Peace Award by Pat Robertson.
In 1979, the Arab League suspended Egypt in the wake of the Egyptian–Israel peace agreement, and the League moved its headquarters from Cairo to Tunis. Arab League member states believed in the elimination of the "Zionist Entity" and Israel at that time. It was not until 1989 that the League re-admitted Egypt as a member, and returned its headquarters to Cairo. As part of the peace deal, Israel withdrew from the Sinai Peninsula in phases, completing its withdrawal from the entire territory except the town of Taba by 25 April 1982 (withdrawal from which did not occur until 1989). The improved relations Egypt gained with the West through the Camp David Accords soon gave the country resilient economic growth. By 1980, however, Egypt's strained relations with the Arab World would result in a period of rapid inflation.
Relationship with Mohammad Reza Shah Pahlavi of Iran.
The relationship between Iran and Egypt had fallen into open hostility during Gamal Abdul Nasser's presidency. Following his death in 1970, President Sadat turned this around quickly into an open and close friendship.
In 1971, Sadat addressed the Iranian parliament in Tehran in fluent Persian, describing the 2,500-year old historic connection between the two lands.
Overnight, the Egyptian and Iranian governments were turned from bitter enemies into fast friends. The relationship between Cairo and Tehran became so friendly that the Shah of Iran, Mohammad Reza Pahlavi, called Sadat his "dear brother".
After the 1973 war with Israel, Iran assumed a leading role in cleaning up and reactivating the blocked Suez Canal with heavy investment. The country also facilitated the withdrawal of Israel from the occupied Sinai Peninsula by promising to substitute the loss of the oil to the Israelis with free Iranian oil if they withdrew from the Egyptian oil wells in western Sinai.
All these added more to the personal friendship between Sadat and the Shah of Iran. (The Shah's first wife was Princess Fawzia of Egypt. She was the eldest daughter of Sultan Fuad I of Egypt and Sudan (later King Fuad I) and his second wife Nazli Sabri.)
After his overthrow, the deposed Shah spent the last months of his life in exile in Egypt. When the Shah died, Sadat ordered that he be given a state funeral and be interred at the Al-Rifa'i Mosque in Cairo, the resting place of Egyptian Khedive Isma'il Pasha, his mother Khushyar Hanim, and numerous other members of the royal family of Egypt and Sudan.
Assassination.
The last months of Sadat's presidency were marked by internal uprising. Sadat dismissed allegations that the rioting was incited by domestic issues, believing that the Soviet Union was recruiting its regional allies in Libya and Syria to incite an uprising that would eventually force him out of power. Following a failed military coup in June 1981, Sadat ordered a major crackdown that resulted in the arrest of numerous opposition figures. Though Sadat still maintained high levels of popularity in Egypt, it has been said that he was assassinated "at the peak" of his unpopularity.
Earlier in his presidency, Islamists had benefited from the 'rectification revolution' and the release from prison of activists jailed under Nasser but Sadat's Sinai treaty with Israel enraged Islamists, particularly the radical Egyptian Islamic Jihad. According to interviews and information gathered by journalist Lawrence Wright, the group was recruiting military officers and accumulating weapons, waiting for the right moment to launch "a complete overthrow of the existing order" in Egypt. Chief strategist of El-Jihad was Abbud al-Zumar, a colonel in the military intelligence whose "plan was to kill the main leaders of the country, capture the headquarters of the army and State Security, the telephone exchange building, and of course the radio and television building, where news of the Islamic revolution would then be broadcast, unleashing—he expected—a popular uprising against secular authority all over the country".
In February 1981, Egyptian authorities were alerted to El-Jihad's plan by the arrest of an operative carrying crucial information. In September, Sadat ordered a highly unpopular roundup of more than 1500 people, including many Jihad members, but also the Coptic Pope and other Coptic clergy, intellectuals and activists of all ideological stripes. All non-government press was banned as well. The round up missed a Jihad cell in the military led by Lieutenant Khalid Islambouli, who would succeed in assassinating Anwar Sadat that October.
According to Tala'at Qasim, ex-head of the Gama'a Islamiyya interviewed in "Middle East Report", it was not Islamic Jihad but his organization, known in English as the "Islamic Group", that organized the assassination and recruited the assassin (Islambouli). Members of the Group's 'Majlis el-Shura' ('Consultative Council') – headed by the famed 'blind shaykh' – were arrested two weeks before the killing, but they did not disclose the existing plans and Islambouli succeeded in assassinating Sadat.
On 6 October 1981, Sadat was assassinated during the annual victory parade held in Cairo to celebrate Egypt's crossing of the Suez Canal. Islambouli emptied his assault rifle into Sadat's body while in the front of the grandstand, instantly killing the President. In addition to Sadat, eleven others were killed, including the Cuban ambassador, an Omani general, a Coptic Orthodox bishop and Samir Helmy, the head of Egypt's Central Auditing Agency (CAA). Twenty-eight were wounded, including Vice President Hosni Mubarak, Irish Defence Minister James Tully, and four US military liaison officers.
The assassination squad was led by Lieutenant Khalid Islambouli after a fatwā approving the assassination had been obtained from Omar Abdel-Rahman. Islambouli was tried, found guilty, sentenced to death, and executed by firing squad in April 1982.
Aftermath.
Sadat was succeeded by his vice president Hosni Mubarak, whose hand was injured during the attack. Sadat's funeral was attended by a record number of dignitaries from around the world, including a rare simultaneous attendance by three former US presidents: Gerald Ford, Jimmy Carter and Richard Nixon. Sudan's President Gaafar Nimeiry was the only Arab head of state to attend the funeral. Only 3 of 24 states in the Arab League — Oman, Somalia and Sudan — sent representatives at all. Israel's prime minister, Menachem Begin, considered Sadat a personal friend and insisted on attending the funeral. Sadat was buried in the unknown soldier memorial in Cairo, across the street from the stand where he was assassinated.
Over three hundred Islamic radicals were indicted in the trial of assassin Khalid Islambouli, including future al-Qaeda leader Ayman al-Zawahiri, Omar Abdel-Rahman and Abd al-Hamid Kishk. The trial was covered by the international press and Zawahiri's knowledge of English made him the de facto spokesman for the defendants. Zawahiri was released from prison in 1984. His brother Muhammad al-Zawahiri was imprisoned from 2000 until 17 March 2011, and then re-arrested on 20 March 2011. Abboud al-Zomor and Tareq al-Zomor, two Islamic Jihad leaders imprisoned in connection with the assassination, were released on 11 March 2011.
Despite these facts, the nephew of the late president, Talaat Sadat, claimed that the assassination was an international conspiracy. On 31 October 2006, he was sentenced to a year in prison for defaming Egypt's armed forces, less than a month after he gave the interview accusing Egyptian generals of masterminding his uncle's assassination. In an interview with a Saudi television channel, he also claimed both the United States and Israel were involved: "No one from the special personal protection group of the late president fired a single shot during the killing, and not one of them has been put on trial," he said.
Media portrayals of Anwar Sadat.
In 1983, "Sadat", a miniseries based on the life of Anwar Sadat, aired on US television with Oscar-winning actor Louis Gossett, Jr. in the title role. The film was promptly banned by the Egyptian government, as were all other movies produced and distributed by Columbia Pictures, over allegations of historical inaccuracies. A civil lawsuit was brought by Egypt's artists' and film unions against Columbia Pictures and the film's directors, producers and scriptwriters before a court in Cairo, but was dismissed; the court held, "the distortions and the slanders found in the film took place outside the country," so that "the crimes were not within the Egyptian courts' jurisdiction."
Western authors attributed the film's poor reception to racism – Gossett being African American – in the Egyptian government or Egypt in general. Either way, one Western source wrote that Sadat's portrayal by Gossett "bothered race-conscious Egyptians and wouldn't have pleased [the deceased] Sadat". – The two-part series earned Gossett an Emmy nomination in the United States.
The first Egyptian depiction of Sadat's life came in 2001, when "Ayyam El Sadat" (English: Days of Sadat) was released in Egyptian cinemas. This movie, by contrast, was a major success in Egypt, and was hailed as Ahmed Zaki's greatest performance to date.
The BBC also produced a film on Sadat titled "Why Was Cairo Calm?". Film director and blogger Adam Curtis summarizes the documentary: "It tells the story of Sadat's presidency—and how the American TV networks created a fantasy vision of him as a wise democratic leader who had opened up the Egyptian economy to the free market, and was loved by his people for making peace for Israel. As the film shows—this was a complete illusion."
The young Sadat is a major character in Ken Follett's thriller "The Key to Rebecca", taking place in World War II Cairo. Sadat, at the time a young officer in the Egyptian Army and involved in anti-British revolutionary activities, is presented quite sympathetically; his willingness to cooperate with German spies is clearly shown to derive from his wish to find allies against British domination of his country, rather than from support of Nazi ideology. Some of the scenes in the book, such as Sadat's arrest by the British, closely follow the information provided in Sadat's own autobiography.
Sadat was a recurring character on Saturday Night Live, played by Garrett Morris, who bore a resemblance to Sadat.

</doc>
<doc id="49523" url="http://en.wikipedia.org/wiki?curid=49523" title="Bayreuth">
Bayreuth

Bayreuth (]; ]) is a sizeable town in northern Bavaria, Germany, on the Red Main river in a valley between the Franconian Jura and the Fichtelgebirge Mountains. The town's roots date back to 1194 and it is nowadays the capital of Upper Franconia with a population of 72,576 (2009). It is world-famous for its annual Bayreuth Festival, at which performances of operas by the 19th-century German composer Richard Wagner are presented.
History.
Middle Ages and Early Modern Period.
The town is believed to have been founded by the Counts of Andechs probably around the mid-12th century, but was first mentioned in 1194 as "Baierrute" in a document by Bishop Otto II of Bamberg. The syllable "-rute" may mean "Rodung" or "clearing", whilst "Baier-" indicates immigrants from the Bavarian region.
Already documented earlier, were villages later merged into Bayreuth: Seulbitz (in 1035 as the royal Salian estate of Silewize in a document by Emperor Conrad II) and St. Johannis (possibly 1149 as "Altentrebgast"). Even the district of Altstadt (formerly Altenstadt) west of the town centre must be older than the town of Bayreuth itself. Even older traces of human presence were found in the hamlets of Meyernberg: pieces of pottery and wooden crockery were dated to the 9th century based on their decoration.
While Bayreuth was previously (1199) referred to as a "villa" (village), the term "civitas" ("town") appeared for the first time in a document published in 1231. One can therefore assume that Bayreuth was awarded its town charter between 1200 and 1230. The town was ruled until 1248 by the counts of Andechs-Merania. After they died out in 1260 the burgraves of Nuremberg from the House of Hohenzollern took over the inheritance. Initially, however, their residence and the centre of the territory was the castle of Plassenburg in Kulmbach. The town of Bayreuth developed slowly and was affected time and again by disasters.
As early as 1361 Emperor Charles IV had conferred on Burgrave Frederick V the right to mint coins for the towns of Bayreuth and Kulmbach.
Bayreuth was first published on a map in 1421.
In February 1430, the Hussites devastated Bayreuth and the town hall and churches were razed. Matthäus Merian described this event in 1642 as follows:"In 1430 the Hussites from Bohemia attacked / Culmbach and Barreut / and committed great acts of cruelty / like wild animals / against the common people / and certain individuals. / The priests / monks and nuns they either burnt at the stake / or took them onto the ice of lakes and rivers / (in Franconia and Bavaria) and doused them with cold water / and killed them in a deplorable way / as Boreck reported in the Bohemian Chronicle, page 450"(Source: Frühwald (Hg.): "Fränkische Städte und Burgen um 1650" based on texts and engravings by Merian, Sennfeld 1991.)
By 1528, less than ten years after the start of the Reformation, the lords of the Frankish margrave territories switched to the Lutheran faith.
In 1605 a great fire, caused by negligence, destroyed 137 of the town's 251 houses. In 1620 plague broke out and, in 1621, there was another big fire in the town. The town also suffered during the Thirty Years War.
A turning point in the town's history came in 1603 when Margrave Christian, the son of the elector, John George of Brandenburg, moved the aristocratic residence from the castle of Plassenburg above Kulmbach to Bayreuth. The first Hohenzollern palace was built in 1440-1457 under Margrave John the Alchemist. It was the forerunner of today's Old Palace ("Altes Schloss") and was expanded and renovated many times. The development of the new capital stagnated due to the Thirty Years' War, but afterwards many famous baroque buildings were added to the town. After Christian's death in 1655 his grandson, Christian Ernest, followed him, ruling from 1661 until 1712. He was an educated and well-travelled man, whose tutor had been the statesman Joachim Friedrich von Blumenthal. He founded the Christian-Ernestinum Grammar School and, in 1683, participated in the liberation of Vienna which had been besieged by the Turks. To commemorate this feat, he had the Margrave Fountain built as a monument on which he is depicted as the victor of the Turks; it now stands outside the New Palace ("Neues Schloss"). During this time, the outer ring of the town wall and the castle chapel ("Schlosskirche") were built.
18th century.
His successor, the Crown Prince and later Margrave, George William, began in 1701 to establish the then independent town of St. Georgen am See (today, the district of St. Georgen) with its castle, the so-called "Ordensschloss", a town hall, a prison and a small barracks. In 1705 he founded the Order of Sincerity (Ordre de la Sincérité), which was renamed in 1734 to the Order of the Red Eagle and had the monastery church built, which was completed in 1711. In 1716 a princely porcelain factory was established in St. Georgen.
The first 'castle' in the park of the Hermitage was built at this time by Margrave George William (1715–1719).
In 1721 the town council acquired the palace of Baroness Sponheim (today's Old Town Hall or "Altes Rathaus") as a replacement for the town hall built in 1440 in the middle of the market place and destroyed by fire.
In 1735 a nursing home, the so-called Gravenreuth Stift, was founded by a private foundation in St. Georgen. The cost of the building exceeded the funds of the foundation, but Margrave Frederick came to their aid.
Bayreuth experienced its Golden Age during the reign (1735–1763) of Margrave Frederick and Margravine Wilhelmina of Bayreuth, the favourite sister of Frederick the Great. During this time, under the direction of court architects, Joseph Saint-Pierre and Carl von Gontard, numerous courtly buildings and attractions were created: the Margravial Opera House with its richly furnished baroque theatre (1744–1748), the New 'Castle' and Sun Temple (1749–1753) at the Hermitage, the New Palace with its courtyard garden (1754 ff) to replace the Old Palace which had burned down through the carelessness of the margrave, and the magnificent row of buildings in today's "Friedrichstraße". There was even a unique version of the rococo architectural style, the so-called Bayreuth Rococo which characterised the aforementioned buildings, especially their interior design.
The old, sombre gatehouses were demolished because they impeded transport and were an outmoded form of defence. The walls were built over in places. Margrave Frederick successfully kept his principality out of the wars being waged by his brother-in-law, Frederick the Great, at this time, and, as a result, brought a time of peace to the Frankish kingdom.
1742 saw the founding of the Frederick Academy, which became a university in 1743, but was moved that same year to Erlangen after serious riots because of the adverse reaction of the population. The university has remained there to the present today. From 1756 to 1763 there was also an Academy of Arts and Sciences.
Roman Catholics were given the right to set up a prayer room and Jewish families settled here again. In 1760 the synagogue was opened and in 1787 the Jewish cemetery was dedicated.
Countess Wilhelmina died in 1758 and, although, Margrave Frederick married again, the marriage was only short-lived and without issue. After his death in 1763, many artists and craftsmen migrated to Berlin and Potsdam, to work for King Frederick the Great, because Frederick's successor, Margrave Frederick Christian had little understanding of art. He also lacked the means due to the elaborate lifestyle of his predecessor, because the buildings and the salaries of the mainly foreign artists had swallowed up a lot of money. For example the court - which under George Frederick Charles had comprised around 140 people - had grown to about 600 employees by the end of the reign of Margrave Frederick. By 1769 the principality was close to bankruptcy.
In 1769 Margrave Charles Alexander, from the Ansbach line of Frankish Hohenzollerns, followed the childless Frederick Christian and Bayreuth was reduced to a secondary residence. Charles Alexander continued to live in Ansbach and rarely came to Bayreuth.
In 1775 the Brandenburg Pond ("Brandenburger Weiher") in St.Georgen was drained.
Following the abdication of the last Margrave, Charles Alexander, from the principalities of Ansbach and Bayreuth on 2 December 1791 its territories became part of a Prussian province. The Prussian Minister Karl August von Hardenberg took over its administration at the beginning of 1792.
The town centre still possesses the typical structure of a Bavarian street market: the settlement is grouped around a road widening into a square; the Town Hall was located in the middle. The church stood apart from it and on a small hill stood the castle. Some sixty years later the town (at that time a tiny village) became subordinate to the Hohenzollern state, and when this state was divided, Bayreuth ended up in the county of Kulmbach.
19th century.
In 1804, the author Jean Paul Richter moved from Coburg to Bayreuth, where he lived until his death in 1825.
The rule of the Hohenzollerns over the Principality of Kulmbach-Bayreuth ended in 1806 after the defeat of Prussia by Napoleonic France. During the French occupation from 1806 to 1810 Bayreuth was treated as a province of the French Empire and had to pay high war contributions. It was placed under the administration of Comte Camille de Tournon, who wrote a detailed inventory of the former Principality of Bayreuth. On 30 June 1810 the French army handed over the former principality to what was now the Kingdom of Bavaria, which it had bought from Napoleon for 15 million francs.
Bayreuth became the capital of the Bavarian district of Mainkreis, which later transferred into Obermainkreis and was finally renamed as the province of Upper Franconia.
As Bavaria was opened up by the railways, the main line from Nuremberg to Hof went past Bayreuth, running via Lichtenfels, Kulmbach and Neuenmarkt-Wirsberg to Hof. Bayreuth was first given a railway connexion in 1853, when the Bayreuth–Neuenmarkt-Wirsberg railway was built at the town's expense. It was followed in 1863 by the line to Weiden, in 1877 by the railway to Schnabelwaid, in 1896 by the branch line to Warmensteinach, in 1904 by the branch to Hollfeld and in 1909 by the branch via Thurnau to Kulmbach, known as the "Thurnauer Bockala" (which means something like "Thurnau Goat").
On 17 April 1870 Richard Wagner visited Bayreuth, because he had read about the Margrave Opera House, whose great stage seemed fitting for his works. However, the orchestra pit could not accommodate the large number of musicians required, for example, for the Ring of the Nibelung and the ambience of the auditorium seemed inappropriate for his piece. So, he toyed with the idea of building his own festival hall (the "Festspielhaus") in Bayreuth. The town supported him in this project and made a piece of land available to him, an undeveloped area outside the town between the railway station and Hohe Warte, the "Grüner Hügel" ("Green Hill"). At the same time Wagner acquired a property at "Hofgarten" to build his own house, "Wahnfried". On 22 May 1872 the cornerstone for the Festival Hall was laid and, on 13 August 1876, it was officially opened (see Bayreuth Festival). Planning and construction were in the hands of the Leipzig architect, Otto Brückwald, who had already made a name for himself in the building of theatres in Leipzig and Altenburg.
In 1886, the composer Franz Liszt died in Bayreuth while visiting his daughter Cosima Liszt, Wagner's widow. Both Liszt and Wagner are buried in Bayreuth; however Wagner did not die there. Rather he died in Venice in 1883, but his family had his body brought to Bayreuth for burial.
20th century.
To the end of the Weimar Republic (1900–1933).
The new century also brought several innovations of modern technology: in 1892, the first electric street lights; in 1908 a municipal electricity station, and, in the same year, the first cinema.
In 1914–15, one section of the northern arm of the Red Main was straightened and widened after areas along the river had been flooded during a period of high water in 1909.
After the First World War had ended in 1918, the Workers' and Soldiers' Council took power briefly in Bayreuth. On 17 February 1919, there was a three-day coup, the so-called "Speckputsch", a brief interlude of excitement in the otherwise rather staid town.
In a series of "völkisch" and nationalist "Deutscher Tag" (German Days), the NSDAP organised the event in Bayreuth on 30 September 1923. More than 5,000 military and civilian people gathered (equivalent to 15% of the inhabitants), although Minister of Defence Otto Gessler had forbidden the participation of Reichswehr units. Among the guests were mayor Albert Preu as well as Siegfried and Winifred Wagner, who invited keynote speaker Adolf Hitler to Wahnfried house. There he met writer Houston Stewart Chamberlain, son-in-law of Richard Wagner and anti-semitic race theorist. Also on that day, Hans Schemm met Hitler for the first time.
In 1932, the provinces of Upper and Middle Franconia were merged and Ansbach was chosen as the seat of government. As a small compensation, Bayreuth was given the merged state insurance agency for Upper and Middle Franconia. Unlike the provincial merger, the merger of those institutions was never reversed.
The Nazi era (1933–1945).
Being a stronghold of right-wing parties since the 1920s, Bayreuth became a center of Nazi ideology. In 1933, it was made capital of the Nazi Gau of Bavarian Ostmark ("Bayerische Ostmark", in 1943 "Gau Bayreuth"). Nazi leaders often visited the Wagner festival and tried to turn Bayreuth into a Nazi model town. It was one of several places in which town planning was administered directly from Berlin, due to Hitler's special interest in the town and in the festival. Hitler loved the music of Richard Wagner, and he became a close friend of Winifred Wagner after she took over the festival. Hitler frequently attended Wagner performances in the Bayreuth Festival Hall.
Bayreuth was to have received a so-called "Gauforum", a combined government building and marching square built to symbolise the centre of power in the town. Bayreuth's first Gauleiter was Hans Schemm, who was also the head ("Reichswalter") of the National Socialist Teachers League, NSLB, which was located in Bayreuth. In 1937 the town was connected to the new "Reichsautobahn".
Under Nazi dictatorship the synagogue of the Jewish Community in "Münzgasse" was desecrated and looted on Kristallnacht but, due to its proximity to the Opera House it was not razed. Inside the building, which is once again used by a Jewish community as a synagogue, a plaque next to the Torah Shrine recalls the persecution and murder of Jews in the Shoah, which took the lives of at least 145 Jews in Bayreuth.
During the Second World War, a subcamp of the Flossenbürg concentration camp was based in the town, in which prisoners had to participate in physical experiments for the V-2. Wieland Wagner, the grandson of the composer, Richard Wagner, was the deputy civilian director there from September 1944 to April 1945. Shortly before the war's end branches of the People's Court ("Volksgerichtshof") were to have been set up in Bayreuth.
On 5, 8 and 11 April 1945 about one third of the town, including many public buildings and industrial installations were destroyed by heavy air strikes, along with 4,500 houses. 741 people were also killed. On 14 April, the U.S. Army occupied the town.
Post-war era and Reconstruction (1945–2000).
After the war Bayreuth tried to part with its ill-fated past. It became part of the American Zone. The American military government set up a DP camp to accommodate displaced persons (DP), many of whom were Ukrainian. The camp was supervised by the UNRRA.
The housing situation was very difficult at first: there were about 55,000 inhabitants in the town, many more than before the war began. This increase was primarily due to the high number of refugees and expellees. Even in 1948 more than 11,000 refugees were counted. In addition, because many homes had been destroyed due to the war, thousands of people were living in temporary shelters, even the festival restaurant next to the Festival Hall housed some 500 people.
In 1945, 1,400 men were conscripted by the town council for "essential work" (clean-up work on damaged buildings and the clearing of roads). A significant number of historic buildings were demolished post-war but cultural life was soon back on track: in 1947 Mozart festival weeks were held in the Opera House, from which the Franconian Festival Weeks developed. In 1949 the Festival Hall was used for the first time again and there was a gala concert with the Vienna Philharmonic led by Hans Knappertsbusch. In 1951, the first post-war Richard Wagner Festival took place under the leadership of Wieland and Wolfgang Wagner.
In 1949 Bayreuth became the seat of the government of Upper Franconia again.
In 1971 the Bavarian State Parliament decided to establish the University of Bayreuth and, on 3 November 1975, it opened for lectures and research. There are now about 10,000 students in the town.
In May 1972, a serious accident occurred at the folk festival in the town, when an overcrowded carriage derailed and several people were thrown out. Four died and five were injured, some seriously. At that time, it was the worst disaster on a roller coaster since the Second World War.
In 1979, US Army serviceman Roy Chung disappeared from the area and allegedly defected to North Korea via East Germany.
In 1999 the world gliding championship took place at Bayreuth municipal airport.
21st century.
In 2006, Bayreuth chose its first CSU member and mayor, the lawyer, Michael Hohl, and, in 2007, a Youth Parliament, consisting of 12 young people, aged 14–17 years, was elected for the first time. The end of October saw the opening of the long-planned bus station and its associated office building on the newly created "Hohenzollernplatz".
Richard Wagner and Bayreuth.
The town is best known for its association with the composer Richard Wagner, who lived in Bayreuth from 1872 until his death in 1883. Wagner's villa, "Wahnfried", was constructed in Bayreuth under the sponsorship of King Ludwig II of Bavaria, and was converted after World War II into a Wagner Museum. In the northern part of Bayreuth is the Festival Hall, an opera house specially constructed for and exclusively devoted to the performance of Wagner's operas. The premieres of the final two works of Wagner's "Ring Cycle" ("Siegfried" and "Götterdämmerung"); the cycle as a whole; and of "Parsifal" took place here.
Every summer, Wagner's operas are performed at the Festspielhaus during the month-long Richard Wagner Festival, commonly known as the Bayreuth Festival. The Festival draws thousands each year, and has persistently been sold out since its inauguration in 1876. Currently, waiting lists for tickets can stretch for 10 years or more.
Owing to Wagner's relationship with the then unknown philosopher Friedrich Nietzsche, the first Bayreuth festival is cited as a key turning point in Nietzsche's philosophical development. Though at first an enthusiastic champion of Wagner's music, Nietzsche ultimately became hostile, viewing the festival and its revellers as symptom of cultural decay and bourgeois decadence —an event which led him to turn his eye upon the moral values esteemed by society as a whole - "Nietzsche clearly preferred to see Bayreuth fail than succeed by mirroring a society gone wrong."
Geography.
Location.
Bayreuth lies on the Red Main river, the southern of the two headstreams of the River Main, between the Fichtelgebirge Mountains and Franconian Switzerland.
Town divisions.
The borough of Bayreuth is divided into 39 districts:
Climate.
Climate in this area has mild dfferences between highs and lows, and there is adequate rainfall year round. The Köppen Climate Classification subtype for this climate is "" (Marine West Coast Climate/Oceanic climate).
Politics.
Town council.
The results of the 2008 local elections in Bavaria were as follows (in brackets the change from the 2002 elections):
Twin towns.
The town of Bayreuth has twinning partnerships with the following towns:
 Annecy, France, since 1966
 Prague 6, Czech Republic, since 2008
Further twinnings with other European towns are planned. Under discussion are Shrewsbury, England, United Kingdom, and Tekirdağ in western Turkey.
There is also a cultural partnership with the state of Burgenland, Austria, and a university partnership between the University of Bayreuth and the Washington and Lee University in Lexington, Virginia.
Sponsorship.
In 1955 Bayreuth took on sponsorship for displaced Sudeten Germans from the town of Franzensbad in Okres Cheb.
Coat of arms.
Margrave Albert Achilles, who was also Elector of Brandenburg, presented the town Bayreuth in December 1457 which the coat of arms that it still bears today. Two fields show the black and white coat of arms of the Hohenzollerns. The black lion on gold with a red and white border was the municipal coat of arms of the burgraves of Nuremberg. Along the two diagonals are two "Reuten", small triangular shovels with a slightly bent shaft. They represent the ending "-reuth" in the town's name.“ 
Culture and places of interest.
Theatre.
The Margravial Opera House was opened in 1748 and is one of the finest Baroque theatres in Europe. It is both a museum and the oldest working "tableau" in Bayreuth.
The Festival Hall dates to the 19th century and is now used solely for the Bayreuth Festival. Only works by Richard Wagner are put on.
The Bayreuth Town House ("Stadthaus"), likewise, does not have its own ensemble. It is regularly used by the "Theater Hof" as well as the "Tourneetheater".
The only two theatres with their own ensemble are the "Studiobühne Bayreuth" and amateur dramatic society, "Brandenburg Kulturstadl". The venues of the studio theatre in Bayreuth are the domicile of the theatre in the "Röntgenstraße", the ruins of the Bayreuth Hermitage and the courtyard of Bayreuth piano manufacturer, Steingraeber & Söhne.
Public parks and cemeteries.
In the town centre is the Court Garden ("Hofgarten") of the New Palace. Near the Festival Hall is the Festival Park. On the southern edge of the town lie the Botanical Gardens of the University of Bayreuth. On the "Königsallee", east of the town centre, is the relatively small Miedel Garden.
The best known park in Bayreuth is that of the 'Eremitage' (Hermitage) in the district of St. Johannis. With a total area of almost 50 hectares it is the largest park in Bayreuth.
Bayreuth has been chosen to host the Bavarian Country Garden Show in 2016. For this reason another park is planned on the Main water meadows between the Volksfestplatz and the A9 motorway.
The oldest surviving cemetery is the Town Cemetery ("Stadtfriedhof") with a large number of gravestones of famous people. On the southern edge of the town is the Southern Cemetery ("Südfriedhof") and crematorium. The districts of St. Johannis and St. Georgen have their own cemeteries. On "Nürnberger Straße", in the east of the town, is an Israeli cemetery.
Sport.
Over 60 clubs offer just under one hundred sports. The most successful club in the town is the street hockey team, Hurricans Bayreuth, who have been German runners-up three times (1998/2004/2006) and champions five times (1996/1997/2001/2005/2007). The only other first division team is the Bayreuth Gliding Club which won the league in 2002 and were runners-up in 2005 and 2008. They were also fourth in the World League in 2008. The basketball team of BBC Bayreuth plays in the Basketball Bundesliga (division 1), the HaSpo Bayreuth handball team, the footballers of SpVgg Bayreuth and the volleyball players of BSV Bayreuth each play in their respective Bavarian League. The ice hockey team, EHC Bayreuth, has also just entered the Bavarian League.
Bayreuth had its sporting heyday in the late 1980s and early 90s. The basketball team, Steiner Bayreuth, were twice German Cup winners (1987/1988 and 1988/1989) and in the 1988/1989 season they also won the German championship. The hockey team of Bayreuth's swimming club (SCC) was twice champions of Second Division South and also played for a year in the Hockey League. At the time that the table tennis team of Steiner Bayreuth was also first class (since 1983 2nd Division, in 1984/85, 1986/87 and 1987/88 1st Division, 1988 relegated and the team has played for many years in the 2nd Football Division. The table tennis players of the 1. Bayreuth FC played in the 1st Division from 1994 to 1997.
In 1999 the World Glider Championships took place in Bayreuth.
Economy and infrastructure.
Transport.
Long-distance roads.
Motorways ("Autobahnen"):
Federal roads ("Bundesstraßen"):
Railways.
From Bayreuth Central Station ("Hauptbahnhof") railway lines run north to Neuenmarkt-Wirsberg, and from there to Bamberg and over the Schiefe Ebene to Hof, east to Weidenberg, southeast to Weiden and south to Schnabelwaid with connections to Nuremberg on the Pegnitz Valley Railway. The lines around Bayreuth are all single-tracked and non-electrified.
Since 23 May 1992 tilting Class 610 diesel multiple units have worked the Pegnitz Valley route. These were bought by the former Deutsche Bundesbahn specifically for the winding track.
Since a 2006/2007 timetable change, Bayreuth has no longer been connected to the DB's long-distance network. However, the Franken-Sachsen-Express still provides a direct connection to Dresden (since December 2007, every two hours). This service is worked by Class 612 diesel multiple units. There are also Regional Express links via Lichtenfels to Bamberg and Würzburg, and via Lichtenfels and Kronach to Saalfeld.
Local public transport.
The town bus routes are operated by Bayreuth Transport and Public Baths (BVB) ("Bayreuther Verkehrs- und Bäder GmbH"). Sometimes private bus operators run services on behalf of the transport companies. The 15 routes (lines 301-315) operate from Monday to Friday at 20 or 30-minute intervals; on Saturday and Sunday the interval is extended to 30 minutes. Late evening services (from about 20 to 12 pm during the week and to 1 am at weekends), on Sunday mornings a simplified network of six lines (lines 321-326) runs buses at 30-minute intervals. Some lines then operate like an on-call taxi service. The network is star-shaped. Originally, the central station was at the market square in "Maximilianstrasse". Since 27 October 2007 the Central Bus Station (ZOH) has been at "Hohenzollernplatz" at the junction of "Kanalstraße" on the "Hohenzollernring". At this stop there are also bus stops for local buses to facilitate transfers.
Regional rail is operated by the Omnibusverkehr Franken. From 1 January 2010 public transport from the town and district of Bayreuth was integrated into the Nuremberg Regional Transport Network ("Verkehrsverbund Großraum Nürnberg").
Cycling.
In most places there is a signed cycle path network. In the centre of Bayreuth itself, cycling is fairly straightforward due to the relatively flat topography, something which encourages the use bicycles as an everyday means of transport. Because of the proximity of the 600 kilometre long Main Cycle Path, Bayreuth is also a destination for many tourist cycle routes.
Because of the long service intervals of the Bayreuth town bus system and its long overnight pause, students use bicycles as their everyday mode of transport.
Bicycles may be carried for a fee on DB Regio trains leaving Bayreuth and in the VGN's buses.
Air transport.
The local airport supports Bayreuth's commercial aviation traffic, individual business travel, general aviation and air sports. By 2002 even the airline from Frankfurt to Hof stopped in Bayreuth three times a day.
The airfield at Bindlacher Berg is also one of the most important bases for gliding in Germany. For example, the World Championships took place here in 1999. For the air sports community in Bayreuth, the airport is a departure point for glider flights taking part in the national Bundesliga competition league. The local gliding club also provides instruction in flying gliders and light aircraft.
See also: " Bayreuth Airport".
Garrison.
For centuries Bayreuth was also a garrison town for the Prussian Army, Royal Bavarian Army, "Reichswehr", "Wehrmacht", US Army, German Army ("Bundeswehr") and the German Border Police ("Bundesgrenzschutz"). In the early 1990s, following the end of the Cold War the garrison tradition of the town came to an end when the "Bundeswehr's" Margrave Barracks ("Markgrafenkaserne") and the Röhrensee Barracks ("Röhrenseekaserne"), used by the US Army and the BGS ("Grenzschutzabteilung Süd 3"), were closed.

</doc>
<doc id="49524" url="http://en.wikipedia.org/wiki?curid=49524" title="Jula">
Jula

Jula refers to:

</doc>
<doc id="49526" url="http://en.wikipedia.org/wiki?curid=49526" title="Scale (ratio)">
Scale (ratio)

The scale ratio of a model represents the proportional ratio of a linear dimension of the model to the same feature of the original. Examples include a 3-dimensional scale model of a building or the scale drawings of the elevations or plans of a building.
In such cases the scale is dimensionless and exact throughout the model or drawing.
The scale can be expressed in four ways: in words (a lexical scale), as a ratio, as a fraction and as a graphical (bar) scale. Thus on an architect's drawing one might read
and a bar scale would also normally appear on the drawing.
General representation.
In general a representation may involve more than one scale at the same time. For example, a drawing showing a new road in elevation might use different horizontal and vertical scales. An elevation of a bridge might be annotated with arrows with a length proportional to a force loading, as in 1 cm to 1000 newtons: this is an example of a dimensional scale. A weather map at some scale may be annotated with wind arrows at a dimensional scale of 1 cm to 20 mph.
Map scales require careful discussion. A town plan may be constructed as an exact scale drawing, but for larger areas a map projection is necessary and no projection can represent the Earth's surface at a uniform scale. In general the scale of a projection depends on position and direction. The variation of scale may be considerable in small scale maps which may cover the globe. In large scale maps of small areas the variation of scale may be insignificant for most purposes but it is always present. The scale of a map projection must be interpreted as a nominal scale. (The usage "large" and "small" in relation to map scales relates to their expressions as fractions. The fraction 1/10,000 used for a local map is much "larger" than 1/100,000,000 used for a global map. There is no fixed dividing line between small and large scales.)
A scale model is a representation or copy of an object that is larger or smaller than the actual size of the object being represented. Very often the scale model is smaller than the original and used as a guide to making the object in full size.
Mathematics.
In mathematics, the idea of geometric scaling can be generalized. The scale between two mathematical objects need not be a fixed ratio but may vary in some systematic way; this is part of mathematical projection, which generally defines a point by point relationship between two mathematical objects. (Generally, these may be mathematical sets and may not represent geometric objects.)

</doc>
<doc id="49528" url="http://en.wikipedia.org/wiki?curid=49528" title="Baen Free Library">
Baen Free Library

The Baen Free Library is a digital library of the science fiction and fantasy publishing house Baen Books where (as of December 2008) 112 full books can be downloaded free in a number of formats, without copy protection. It was founded in autumn 1999 by science fiction writer Eric Flint and publisher Jim Baen to determine whether the availability of books free of charge on the Internet encourages or discourages the sale of their paper books.
The Baen Free Library represents an interesting experiment in the field of intellectual property and copyright. It appears that sales of both the books made available free and other books by the same author, even from a different publisher, increase when the electronic version is made available free of charge.
In 2002, Baen also started adding CD-ROMs into some hardcovers of newest titles in successful series. They contain the complete series of novels preceding the printed book (for those books that were the latest in a series), other works by the same author, some works by other authors, and multimedia bonuses. The CD-ROMs have a prominent permissive copyright license which expressly encourages free-of-charge copying and sharing, including over the Internet.
The books in the Free Library are available via the website for Baen Ebooks, which also sells individual e-books and a subscription-based e-book program.

</doc>
<doc id="49532" url="http://en.wikipedia.org/wiki?curid=49532" title="Battle of the Milvian Bridge">
Battle of the Milvian Bridge

The Battle of the Milvian Bridge took place between the Roman Emperors Constantine I and Maxentius on 28 October 312. It takes its name from the Milvian Bridge, an important route over the Tiber. Constantine won the battle and started on the path that led him to end the Tetrarchy and become the sole ruler of the Roman Empire. Maxentius drowned in the Tiber during the battle.
According to chroniclers such as Eusebius of Caesarea and Lactantius, the battle marked the beginning of Constantine's conversion to Christianity. Eusebius of Caesarea recounts that Constantine and his soldiers had a vision of the Christian God promising victory if they daubed the sign of the Chi-Rho, the first two letters of Christ's name in Greek, on their shields. The Arch of Constantine, erected in celebration of the victory, certainly attributes Constantine's success to divine intervention; however, the monument does not display any overtly Christian symbolism.
Historical background.
The underlying causes of the battle were the the rivalries inherent in Diocletian's Tetrarchy. After Diocletian stepped down on 1 May 305, his successors began to struggle for control of the Roman Empire almost immediately. Although Constantine was the son of the Western Emperor Constantius, the Tetrarchic ideology did not necessarily provide for hereditary succession. When Constantius died on 25 July 306, his father's troops proclaimed Constantine as Augustus in Eboracum (York). In Rome, the favorite was Maxentius, the son of Constantius' imperial colleague Maximian, who seized the title of emperor on 28 October 306. But whereas Constantine's claim was recognized by Galerius, ruler of the Eastern provinces and the senior emperor in the Empire, Maxentius was treated as a usurper. Galerius, however, recognized Constantine as holding only the lesser imperial rank of Caesar. Galerius ordered his co-Augustus, Severus, to put Maxentius down in early 307. Once Severus arrived in Italy, however, his army defected to Maxentius. Severus was captured, imprisoned, and executed. Galerius himself marched on Rome in the autumn, but failed to take the city. Constantine avoided conflict with both Maxentius and the Eastern emperors for most of this period.
By 312, however, Constantine and Maxentius were engaged in open hostility with one another, although they were brothers-in‑law through Constantine's marriage to Fausta, sister of Maxentius. In the spring of 312, Constantine gathered his forces and decided to oust Maxentius himself. He easily overran northern Italy, winning two major battles: the first near Turin, the second at Verona, where the praetorian prefect Ruricius Pompeianus, Maxentius' most senior general, was killed.
Vision of Constantine.
It is commonly stated that on the evening of 27 October with the armies preparing for battle, Constantine had a vision which led him to fight under the protection of the Christian God. The details of that vision, however, differ between the sources reporting it.
Lactantius states that, in the night before the battle, Constantine was commanded in a dream to "delineate the heavenly sign on the shields of his soldiers" ("On the Deaths of the Persecutors" 44.5). He followed the commands of his dream and marked the shields with a sign "denoting Christ". Lactantius describes that sign as a "staurogram", or a Latin cross with its upper end rounded in a P-like fashion. There is no certain evidence that Constantine ever used that sign, opposed to the better known Chi-Rho sign described by Eusebius.
From Eusebius, two accounts of the battle survive. The first, shorter one in the "Ecclesiastical History" promotes the belief that God helped Constantine but does not mention any vision. In his later "Life of Constantine", Eusebius gives a detailed account of a vision and stresses that he had heard the story from the Emperor himself. According to this version, Constantine with his army was marching (Eusebius does not specify the actual location of the event, but it clearly is not in the camp at Rome), when he looked up to the sun and saw a cross of light above it, and with it the Greek words "Εν Τούτῳ Νίκα", "En toutō níka", usually translated into Latin as "in hoc signo vinces." Both phrases have the literal meaning "In this sign,[you shall] conquer"; a more free translation would be "Through this sign [you shall] conquer". At first he was unsure of the meaning of the apparition, but in the following night he had a dream in which Christ explained to him that he should use the sign against his enemies. Eusebius then continues to describe the labarum, the military standard used by Constantine in his later wars against Licinius, showing the Chi-Rho sign.
The accounts of the two contemporary authors, though not entirely consistent, have been merged into a popular notion of Constantine seeing the Chi-Rho sign on the evening before the battle. Both authors agree that the sign was not widely understandable to denote Christ (although among the Christians, it was already being used in the catacombs along with other special symbols to mark and/or decorate Christian tombs). Its first imperial appearance is on a Constantinian silver coin from c. 317, which proves that Constantine did use the sign at that time, though not very prominently. He made more extensive use of the Chi-Rho and the Labarum later, during the conflict with Licinius.
Some have interpreted the vision in a solar context ("e.g.", as a solar halo phenomenon called a Sun dog), which may have been reshaped to fit with the Christian beliefs later expressed by Constantine. Coins of Constantine depicting him quite overtly as the companion of a solar deity were minted as late as 313, the year following the battle. The solar deity, Sol Invictus, is often pictured with a nimbus, or halo. Various emperors portrayed Sol Invictus on their official coinage, with a wide range of legends, only a few of which incorporated the epithet "invictus", such as the legend , claiming the Unconquered Sun as a companion to the emperor, used with particular frequency by Constantine. Constantine's official coinage continues to bear images of Sol until 325/6. A solidus of Constantine as well as a gold medallion from his reign depict the Emperor's bust in profile jugate with Sol Invictus, with the legend . The official cults of Sol Invictus and Sol Invictus Mithras were popular amongst the soldiers of the Roman Army. Statuettes of Sol Invictus, carried by the standard-bearers, appear in three places in reliefs on the Arch of Constantine. Constantine's triumphal arch was carefully positioned to align with the colossal statue of Sol by the Colosseum, so that Sol formed the dominant backdrop when seen from the direction of the main approach towards the arch.'
Events of the battle.
Constantine reached Rome at the end of October 312 approaching along the Via Flaminia. He camped at the location of Malborghetto near Prima Porta, where remains of a Constantinian monument, the Arch of Malborghetto, in honour of the occasion are still extant.
It was expected that Maxentius would remain within Rome and endure a siege; he had successfully employed this strategy twice before, during the invasions of Severus and Galerius. Indeed, Maxentius had organised the stockpiling of large amounts of food in the city in preparation for such an event. Surprisingly, he decided otherwise choosing to meet Constantine in open battle. Ancient sources commenting on these events attribute this decision either to divine intervention ("e.g.", Lactantius, Eusebius) or superstition ("e.g.", Zosimus). They also note that the day of the battle was the same as the day of his accession (28 October), which was generally thought to be a good omen. Additionally, Maxentius is reported to have consulted the oracular Sibylline Books, which stated that "on October 28 an enemy of the Romans would perish". Maxentius interpreted this prophecy as being favourable to himself.<Ref>Pohlsander, p.19</ref> Lactantius also reports that the populace supported Constantine with acclamations during circus games, 
Maxentius chose to make his stand in front of the Milvian Bridge, a stone bridge that carries the Via Flaminia road across the Tiber River into Rome (the bridge stands today at the same site, somewhat remodelled, named in Italian "Ponte Milvio" or sometimes "Ponte Molle", "soft bridge"). Holding it was crucial if Maxentius was to keep his rival out of Rome, where the Senate would surely favour whoever held the city. As Maxentius had probably partially destroyed the bridge during his preparations for a siege, he had a wooden or pontoon bridge constructed to get his army across the river. The sources vary as to the nature of the bridge central to the events of the battle. Zosimus mentions it, vaguely, as being constructed in two parts connected by iron fastenings, while others indicate that it was a pontoon bridge; sources are also unclear as to whether the bridge was deliberately constructed as a collapsible trap for Constantine's forces or not.
The next day, the two armies clashed, and Constantine won a decisive victory. The dispositions of Maxentius may have been faulty as his troops seem to have been arrayed with the River Tiber too close to their rear, giving them little space to allow re-grouping in the event of their formations being forced to give ground. Already known as a skillful general, Constantine first launched his cavalry at the cavalry of Maxentius and broke them. Constantine's infantry then advanced, most of Maxentius's troops fought well but they began to be pushed back toward the Tiber; Maxentius decided to retreat and make another stand at Rome itself; but there was only one escape route, via the bridge. Constantine's men inflicted heavy losses on the retreating army. Finally, the temporary bridge set up alongside the Milvian Bridge, over which many of the Maxentian troops were escaping, collapsed, and those stranded on the north bank of the Tiber were either taken prisoner or killed. Maxentius' Praetorian Guard, who had originally acclaimed him emperor, seem to have made a stubborn stand on the northern bank of the river "... in despair of pardon they covered with their bodies the place which they had chosen for combat." 
Maxentius was among the dead, having drowned in the river while trying to swim across it in a desperate bid to escape or, alternatively, he is described as having been thrown by his horse into the river. Lactantius describes the death of Maxentius in the following manner: "The bridge in his rear was broken down. At sight of that the battle grew hotter. The hand of the Lord prevailed, and the forces of Maxentius were routed. He fled towards the broken bridge; but the multitude pressing on him, he was driven headlong into the Tiber."
Aftermath.
Constantine entered Rome on 29 October. He staged a grand arrival ceremony in the city ("adventus)", and was met with popular jubilation. Maxentius' body was fished out of the Tiber and decapitated. His head was paraded through the streets for all to see. After the ceremonies, Maxentius' head was sent to Carthage as proof of his downfall, Africa then offered no further resistance. The battle gave Constantine undisputed control of the western half of the Roman Empire. The descriptions of Constantine's entry into Rome omit mention of him ending his procession at the temple of Capitoline Jupiter, where sacrifice was usually offered. Though often employed to show Constantine's Christian sensibilities, this silence cannot be taken as proof that Constantine was a Christian at this point. He chose to honour the Senatorial Curia with a visit, where he promised to restore its ancestral privileges and give it a secure role in his reformed government: there would be no revenge against Maxentius' supporters. Maxentius was condemned to "damnatio memoriae", all his legislation was invalidated and Constantine usurped all of Maxentius' considerable building projects within Rome, including the Temple of Romulus and the Basilica of Maxentius. Maxentius' strongest supporters in the military were neutralized when the Praetorian Guard and Imperial Horse Guard ("equites singulares") were disbanded. Constantine is thought to have replaced the former imperial guards with a number of cavalry units termed the Scholae Palatinae.
Significance.
Paul K. Davis writes, "Constantine’s victory gave him total control of the Western Roman Empire paving the way for Christianity to become the dominant religion for the Roman Empire and ultimately for Europe." The following year, 313, Constantine and Licinius issued the Edict of Milan, which made Christianity an officially recognised and tolerated religion in the Roman Empire.
References.
The most important ancient sources for the battle are Lactantius, "De mortibus persecutorum" 44; Eusebius of Caesarea, "Ecclesiastical History" ix, 9 and "Life of Constantine" i, 28-31 (the vision) and i, 38 (the actual battle); Zosimus ii, 15-16; and the "Panegyrici Latini" of 313 (anonymous) and 321 (by Nazarius).

</doc>
<doc id="49535" url="http://en.wikipedia.org/wiki?curid=49535" title="Thought experiment">
Thought experiment

A thought experiment or Gedankenexperiment (from German) considers some hypothesis, theory, or principle for the purpose of thinking through its consequences. Given the structure of the experiment, it may or may not be possible to actually perform it, and if it can be performed, there need be no intention of any kind to actually perform the experiment in question.
The common goal of a thought experiment is to explore the potential consequences of the principle in question: "A thought experiment is a device with which one performs an intentional, structured process of intellectual deliberation in order to speculate, within a specifiable problem domain, about potential consequents (or antecedents) for a designated antecedent (or consequent)" (Yeates, 2004, p. 150).
Famous examples of thought experiments include Schrödinger's cat, illustrating quantum indeterminacy through the manipulation of a perfectly sealed environment and a tiny bit of radioactive substance, and Maxwell's demon, which attempts to demonstrate the ability of a hypothetical finite being to violate the second law of thermodynamics.
Overview.
The ancient Greek δείκνυμι "(transl.: deiknymi)", or thought experiment, "was the most ancient pattern of mathematical proof", and existed before Euclidean mathematics, where the emphasis was on the conceptual, rather than on the experimental part of a thought-experiment. Perhaps the key experiment in the history of modern science is Galileo's demonstration that falling objects must fall at the same rate regardless of their masses. This is widely thought to have been a straightforward physical demonstration, involving climbing up the Leaning Tower of Pisa and dropping two heavy weights off it, whereas in fact, it was a logical demonstration, using the 'thought experiment' technique. The 'experiment' is described by Galileo in "Discorsi e dimostrazioni matematiche" (1638) (literally, 'Discourses and Mathematical Demonstrations') thus:
"Salviati". If then we take two bodies whose natural speeds are different, it is clear that on uniting the two, the more rapid one will be partly retarded by the slower, and the slower will be somewhat hastened by the swifter. Do you not agree with me in this opinion?
"Simplicio". You are unquestionably right.
"Salviati". But if this is true, and if a large stone moves with a speed of, say, eight while a smaller moves with a speed of four, then when they are united, the system will move with a speed less than eight; but the two stones when tied together make a stone larger than that which before moved with a speed of eight. Hence the heavier body moves with less speed than the lighter; an effect which is contrary to your supposition. Thus you see how, from your assumption that the heavier body moves more rapidly than ' the lighter one, I infer that the heavier body moves more slowly.
Although the extract does not convey the elegance and power of the 'demonstration' terribly well, it is clear that it is a 'thought' experiment, rather than a practical one. Strange then, as Cohen says, that philosophers and scientists alike refuse to acknowledge either Galileo in particular, or the thought experiment technique in general for its pivotal role in both science and philosophy. (The exception proves the rule — the iconoclastic philosopher of science, Paul Feyerabend, has also observed this methodological prejudice.)
Instead, many philosophers prefer to consider 'Thought Experiments' to be merely the use of a hypothetical scenario to help understand the way things actually are.
Variety.
Thought experiments have been used in a variety of fields, including philosophy, law, physics, and mathematics. In philosophy, they have been used at least since classical antiquity, some pre-dating Socrates. In law, they were well-known to Roman lawyers quoted in the Digest. In physics and other sciences, notable thought experiments date from the 19th and especially the 20th century, but examples can be found at least as early as Galileo.
Origins and use of the literal term.
Johann Witt-Hansen established that Hans Christian Ørsted was the first to use the Latin-German mixed term "Gedankenexperiment" (lit. thought experiment) circa 1812. Ørsted was also the first to use its entirely German equivalent, "Gedankenversuch", in 1820.
Much later, Ernst Mach used the term "Gedankenexperiment" in a different way, to denote exclusively the "imaginary" conduct of a "real" experiment that would be subsequently performed as a "real physical experiment" by his students. Physical and mental experimentation could then be contrasted: Mach asked his students to provide him with explanations whenever the results from their subsequent, real, physical experiment differed from those of their prior, imaginary experiment.
The English term "thought experiment" was coined (as a calque) from Mach's "Gedankenexperiment", and it first appeared in the 1897 English translation of one of Mach’s papers. Prior to its emergence, the activity of posing hypothetical questions that employed subjunctive reasoning had existed for a very long time (for both scientists and philosophers). However, people had no way of categorizing it or speaking about it. This helps to explain the extremely wide and diverse range of the application of the term "thought experiment" once it had been introduced into English.
Uses.
Thought experiments, which are well-structured, well-defined hypothetical questions that employ subjunctive reasoning (irrealis moods) – "What might happen (or, what might have happened) if . . . " – have been used to pose questions in philosophy at least since Greek antiquity, some pre-dating Socrates (see ). In physics and other sciences many famous thought experiments date from the 19th and especially the 20th Century, but examples can be found at least as early as Galileo.
In thought experiments we gain new information by rearranging or reorganizing already known empirical data in a new way and drawing new (a priori) inferences from them or by looking at these data from a different and unusual perspective. In Galileo’s thought experiment, for example, the rearrangement of empirical experience consists in the original idea of combining bodies of different weight.
Thought experiments have been used in philosophy (especially ethics), physics, and other fields (such as cognitive psychology, history, political science, economics, social psychology, law, organizational studies, marketing, and epidemiology). In law, the synonym "hypothetical" is frequently used for such experiments.
Regardless of their intended goal, all thought experiments display a patterned way of thinking that is designed to allow us to explain, predict and control events in a better and more productive way.
Theoretical consequences.
In terms of their theoretical consequences, thought experiments generally:
Practical applications.
Thought experiments can produce some very important and different outlooks on previously unknown or unaccepted theories. However, they may make those theories themselves irrelevant, and could possibly create new problems that are just as difficult, or possibly more difficult to resolve.
In terms of their practical application, thought experiments are generally created in order to:
In science.
Scientists tend to use thought experiments in the form of imaginary, "proxy" experiments which they conduct prior to a real, "physical" experiment (Ernst Mach always argued that these gedankenexperiments were "a necessary precondition for physical experiment"). In these cases, the result of the "proxy" experiment will often be so clear that there will be no need to conduct a physical experiment at all.
Scientists also use thought experiments when particular physical experiments are impossible to conduct (Carl Gustav Hempel labeled these sorts of experiment "theoretical experiments-in-imagination"), such as Einstein's thought experiment of chasing a light beam, leading to Special Relativity. This is a unique use of a scientific thought experiment, in that it was never carried out, but led to a successful theory, proven by other empirical means.
Relation to real experiments.
The relation to real experiments can be quite complex, as can be seen again from an example going back to Albert Einstein. In 1935, with two coworkers, he published a famous paper on a newly created subject called later the EPR effect (EPR paradox). In this paper, starting from certain philosophical assumptions, on the basis of a rigorous analysis of a certain, complicated, but in the meantime assertedly realizable model, he came to the conclusion that "quantum mechanics should be described as "incomplete"". Niels Bohr asserted a refutation of Einstein's analysis immediately, and his view prevailed. After some decades, it was asserted that feasible experiments could prove the error of the EPR paper. These experiments tested the Bell inequalities published in 1964 in a purely theoretical paper. The above-mentioned EPR philosophical starting assumptions were considered to be falsified by empirical fact (e.g. by the optical "real experiments" of Alain Aspect).
Thus "thought experiments" belong to a theoretical discipline, usually to theoretical physics, but often to theoretical philosophy. In any case, it must be distinguished from a real experiment, which belongs naturally to the experimental discipline and has "the final decision on "true" or "not true"", at least in physics.
Causal reasoning.
The first characteristic pattern that thought experiments display is their orientation
in time. They are either:
The second characteristic pattern is their movement in time in relation to “the present
moment standpoint” of the individual performing the experiment; namely, in terms of:
Seven Types.
Generally speaking, there are seven types of thought experiments in which one reasons from causes to effects, or effects to causes:
Prefactual.
"Prefactual (before the fact) thought experiments" speculate on possible future outcomes, given the present, and ask "What will be the outcome if event E occurs?"
Counterfactual.
"Counterfactual (contrary to established fact) thought experiments" speculate on the possible outcomes of a different past; and ask "What might have happened if A had happened instead of B?" (e.g., "If Isaac Newton and Gottfried Leibniz had cooperated with each other, what would mathematics look like today?").
Semifactual.
"Semifactual" "thought experiments" speculate on the extent to which things might have remained the same, despite there being a different past; and asks the question Even though X happened instead of E, would Y have still occurred? (e.g., Even if the goalie had moved left, rather than right, could he have intercepted a ball that was traveling at such a speed?).
Semifactual speculations are an important part of clinical medicine.
Prediction.
The activity of prediction attempts to project the circumstances of the present into the future. According to David Sarewitz and Roger Pielke (1999, p123), scientific prediction takes two forms:
Although they perform different social and scientific functions, the only difference between the qualitatively identical activities of "predicting", "forecasting," and "nowcasting" is the distance of the speculated future from the present moment occupied by the user. Whilst the activity of nowcasting, defined as “a detailed description of the current weather along with forecasts obtained by extrapolation up to 2 hours ahead”, is essentially concerned with describing the current state of affairs, it is common practice to extend the term “to cover very-short-range forecasting up to 12 hours ahead” (Browning, 1982, p.ix).
Hindcasting.
The activity of hindcasting involves running a forecast model after an event has happened in order to test whether the model's simulation is valid.
In 2003, Dake Chen and his colleagues “trained” a computer using the data of the surface
temperature of the oceans from the last 20 years. Then, using data that had been
collected on the surface temperature of the oceans for the period 1857 to 2003, they
went through a hindcasting exercise and discovered that their simulation not only
accurately predicted every El Niño event for the last 148 years, it also identified the (up
to 2 years) looming foreshadow of every single one of those El Niño events.
Retrodiction (or postdiction).
The activity of "retrodiction" (or "postdiction") involves moving backwards in time, step-by-step, in as many stages as are considered necessary, from the present into the speculated past, in order to establish the ultimate cause of a specific event (e.g., Reverse engineering and Forensics).
Given that retrodiction is a process in which “past observations, events and data are used as evidence to infer the process(es) the produced them” and that diagnosis “involve[s] going from visible effects such as symptoms, signs and the like to their prior causes”, the essential balance between prediction and retrodiction could be characterized as:
regardless of whether the prognosis is of the course of the disease in the absence of treatment, or of the application of a specific treatment regimen to a specific disorder in a particular patient.
Backcasting.
The activity of "backcasting" involves establishing the description of a very definite and very specific future situation. It then involves an imaginary moving backwards in time, step-by-step, in as many stages as are considered necessary, from the future to the present, in order to reveal the mechanism through which that particular specified future could be attained from the present.
Backcasting is not concerned with predicting the future:
According to Jansen (1994, p. 503:
In philosophy.
In philosophy, a thought experiment typically presents an imagined scenario with the intention of eliciting an intuitive or reasoned response about the way things are in the thought experiment. (Philosophers might also supplement their thought experiments with theoretical reasoning designed to support the desired intuitive response.) The scenario will typically be designed to target a particular philosophical notion, such as morality, or the nature of the mind or linguistic reference. The response to the imagined scenario is supposed to tell us about the nature of that notion in any scenario, real or imagined.
For example, a thought experiment might present a situation in which an agent intentionally kills an innocent for the benefit of others. Here, the relevant question is not whether the action is moral or not, but more broadly whether a moral theory is correct that says morality is determined solely by an action's consequences (See Consequentialism). John Searle imagines a man in a locked room who receives written sentences in Chinese, and returns written sentences in Chinese, according to a sophisticated instruction manual. Here, the relevant question is not whether or not the man understands Chinese, but more broadly, whether a functionalist theory of mind is correct.
It is generally hoped that there is universal agreement about the intuitions that a thought experiment elicits. (Hence, in assessing their own thought experiments, philosophers may appeal to "what we should say," or some such locution.) A successful thought experiment will be one in which intuitions about it are widely shared. But often, philosophers differ in their intuitions about the scenario.
Other philosophical uses of imagined scenarios arguably are thought experiments also. In one use of scenarios, philosophers might imagine persons in a particular situation (maybe ourselves), and ask what they would do.
For example, John Rawls asks us to imagine a group of persons in a situation where they know nothing about themselves, and are charged with devising a social or political organization (See the veil of ignorance). The use of the state of nature to imagine the origins of government, as by Thomas Hobbes and John Locke, may also be considered a thought experiment. Søren Kierkegaard explored the possible ethical and religious implications of Abraham's binding of Isaac in "Fear and Trembling" Similarly, Friedrich Nietzsche, in "On the Genealogy of Morals", speculated about the historical development of Judeo-Christian morality, with the intent of questioning its legitimacy.
An early written thought experiment was Plato's allegory of the cave. Another historic thought experiment was Avicenna's "Floating Man" thought experiment in the 11th century. He asked his readers to imagine themselves suspended in the air isolated from all in order to demonstrate human self-awareness and self-consciousness, and the substantiality of the soul.
Possibility.
The scenario presented in a thought experiment must be possible in some sense. In many thought experiments, the scenario would be nomologically possible, or possible according to the laws of nature. John Searle's Chinese room is nomologically possible.
Some thought experiments present scenarios that are not nomologically possible. In his Twin Earth thought experiment, Hilary Putnam asks us to imagine a scenario in which there is a substance with all of the observable properties of water (e.g., taste, color, boiling point), but which is chemically different from water. It has been argued that this thought experiment is not nomologically possible, although it may be possible in some other sense, such as metaphysical possibility. It is debatable whether the nomological impossibility of a thought experiment renders intuitions about it moot.
In some cases, the hypothetical scenario might be considered metaphysically impossible, or impossible in any sense at all. David Chalmers says that we can imagine that there are zombies, or persons who are physically identical to us in every way but who lack consciousness. This is supposed to show that physicalism is false. However, some argue that zombies are inconceivable: we can no more imagine a zombie than we can imagine that 1+1=3. Others have claimed that the conceivability of a scenario may not entail its possibility.
Other criticisms.
The use of thought experiments in philosophy has received other criticisms, especially in the philosophy of mind. Daniel Dennett has derisively referred to certain types of thought experiments such as the Chinese Room experiment as "intuition pumps", claiming they are simply thinly veiled appeals to intuition which fail when carefully analyzed. Another criticism that has been voiced is that some science fiction-type thought experiments are too wild to yield clear intuitions, or that any resulting intuitions could not possibly pertain to the real world.
Famous thought experiments.
Physics.
Thought experiments are popular in physics and include:
Philosophy.
The field of philosophy makes extensive use of thought experiments:

</doc>
<doc id="49544" url="http://en.wikipedia.org/wiki?curid=49544" title="Junichiro Koizumi">
Junichiro Koizumi

Junichiro Koizumi (小泉 純一郎, Koizumi Jun'ichirō, born January 8, 1942) is a Japanese politician who was the 87th Prime Minister of Japan from 2001 to 2006. He retired from politics when his term in parliament ended in 2009, and was the fifth longest serving prime minister in the history of Japan.
Widely seen as a maverick leader of the Liberal Democratic Party (LDP), he became known as an economic reformer, focusing on Japan's government debt and the privatization of its postal service. In 2005, Koizumi led the LDP to win one of the largest parliamentary majorities in modern Japanese history. Affiliated to the openly revisionist lobby Nippon Kaigi, which advocates among other the return to militarism, the denial of Japanese war crimes, and visits to Yasukuni Shrine Koizumi also attracted international attention through his deployment of the Japan Self-Defense Forces to Iraq, and through his visits to the controversial shrine that fueled diplomatic tensions with neighboring China and South Korea.
Although Koizumi maintained a low profile for several years after leaving office, he returned to national attention in 2013 as an advocate for abandoning nuclear power in the wake of the Fukushima nuclear disaster, which contrasted with the pro-nuclear views espoused by the LDP governments both during and after Koizumi's term in office.
Early life.
Koizumi is a third-generation politician. His father, Jun'ya Koizumi, was director general of the Japan Defense Agency and a member of the Diet. His grandfather, Koizumi Matajirō,called "Tattoo Minister" because of his big tattoo on his body, and the leader of Koizumi Gumi in Kanagawa (a big group of "yakuza") was Minister of Posts and Telecommunications under Prime Ministers Hamaguchi and Wakatsuki and an early advocate of postal privatization. See Koizumi family.
Born in Yokosuka, Kanagawa prefecture on January 8, 1942, Koizumi was educated at Yokosuka High School and Keio University, where he studied economics. He attended University College London before returning to Japan in August 1969 upon the death of his father.
He stood for election to the lower house in December; however, he did not earn enough votes to win election as a Liberal Democratic Party (LDP) representative. In 1970, he was hired as a secretary to Takeo Fukuda, who was Minister of Finance at the time and was elected as Prime Minister in 1976.
In the general elections of December 1972, Koizumi was elected as a member of the Lower House for the Kanagawa 11th district. He joined Fukuda's faction within the LDP. Since then, he has been re-elected ten times.
Member of House of Representatives.
Koizumi gained his first senior post in 1979 as Parliamentary Vice Minister of Finance, and his first ministerial post in 1988 as Minister of Health and Welfare under Prime Minister Noboru Takeshita. He held cabinet posts again in 1992 (Minister of Posts and Telecommunications in the Miyazawa cabinet) and 1996–1998 (Minister of Health and Welfare in the Uno and Hashimoto cabinets).
In 1994, with the LDP in opposition, Koizumi became part of a new LDP faction, Shinseiki, made up of younger and more motivated parliamentarians led by Taku Yamasaki, Koichi Kato and Koizumi, a group popularly dubbed "YKK" (after the YKK Group well known for manufacturing zippers).
After Prime Minister Morihiro Hosokawa resigned in 1994 and the LDP returned to power in a coalition government, Koizumi and Hosokawa teamed up with Shusei Tanaka of New Party Sakigake in a strategic dialogue across party lines regarding Japan becoming a permanent member of the United Nations Security Council. Although this idea was not popular within the LDP and never came to fruition, Koizumi and Hosokawa maintained a close working relationship across party lines, with Hosokawa tacitly serving as Koizumi's personal envoy to China during times of strained Sino-Japanese relations.
Koizumi competed for the presidency of the LDP in September 1995 and July 1998, but he gained little support losing decisively to Ryutaro Hashimoto and then Keizō Obuchi, both of whom had broader bases of support within the party. However, after Yamasaki and Kato were humiliated in a disastrous attempt to force a vote of no confidence against Prime Minister Yoshirō Mori in 2000, Koizumi became the last remaining credible member of the YKK trio, which gave him leverage over the reform-minded wing of the party.
On April 24, 2001, Koizumi was elected president of the LDP. He was initially considered an outside candidate against Hashimoto, who was running for his second term as Prime Minister. However, in the first poll of prefectural party organizations, Koizumi won 87 to 11 percent; in the second vote of Diet members, Koizumi won 51 to 40 percent. He defeated Hashimoto by a final tally of 298 to 155 votes. He was made Prime Minister of Japan on April 26, and his coalition secured 78 of 121 seats in the Upper House elections in July.
Prime minister.
Domestic policy.
Within Japan, Koizumi pushed for new ways to revitalise the moribund economy, aiming to act against bad debts with commercial banks, privatize the postal savings system, and reorganize the factional structure of the LDP. He spoke of the need for a period of painful restructuring in order to improve the future.
See also "Honebuto Hoshin".
In the fall of 2002, Koizumi appointed Keio University economist and frequent television commentator Heizō Takenaka as Minister of State for Financial Services and head of the Financial Services Agency (FSA) to fix the country's banking crisis. Bad debts of banks were dramatically cut with the NPL ratio of major banks approaching half the level of 2001. The Japanese economy has been through a slow but steady recovery, and the stock market has dramatically rebounded. The GDP growth for 2004 was one of the highest among G7 nations, according to the International Monetary Fund and Organization for Economic Co-operation and Development. Takenaka was appointed as a Postal Reform Minister in 2004 for the privatization of Japan Post, operator of the country's Postal Savings system.
Koizumi moved the LDP away from its traditional rural agrarian base toward a more urban, neoliberal core, as Japan's population grew in major cities and declined in less populated areas, although under current purely geographical districting, rural votes in Japan are still many times more powerful than urban ones. In addition to the privatization of Japan Post (which many rural residents fear will reduce their access to basic services such as banking), Koizumi also slowed down the LDP's heavy subsidies for infrastructure and industrial development in rural areas. These tensions made Koizumi a controversial but popular figure within his own party and among the Japanese electorate.
Foreign policy.
Although Koizumi's foreign policy was focused on closer relations with the United States and UN-centered diplomacy, which were adopted by all of his predecessors, he went further to pursue supporting the US policies in the War on Terrorism. He decided to deploy the Japan Self-Defense Forces to Iraq, which was the first military mission in active foreign war zones since the end of the World War II. Many Japanese commentators indicated that the favorable US-Japan relation was based on the Koizumi's personal friendship with the US President George W. Bush. White House officials described the first meeting between Koizumi and Bush at Camp David as "incredibly warm", with the two men playing catch with a baseball. In the North Korean abductions and nuclear development issues, Koizumi took more assertive attitudes than his predecessors.
Self-Defense Forces policy.
Although Koizumi did not initially campaign on the issue of defense reform, he approved the expansion of the Japan Self-Defense Forces (JSDF) and in October 2001 they were given greater scope to operate outside of the country. Some of these troops were dispatched to Iraq. Koizumi's government also introduced a bill to upgrade the Japan Defense Agency to ministry status; finally, the Defense Agency became the Japanese Ministry of Defense on January 9, 2007.
Visits to Yasukuni Shrine.
Koizumi has often been noted for his controversial visits to the Yasukuni Shrine, starting on August 13, 2001. He visited the shrine six times as prime minister. Because the shrine honors Japan's war dead, which also include many convicted Japanese war criminals and 14 executed Class A war criminals, these visits drew strong condemnation and protests from both Japan's neighbours, mainly China and South Korea, and many Japanese citizens. China and South Korea's people hold bitter memories of Japanese invasion and occupation during the first half of the 20th century. China and South Korea refused to have their representatives meet Koizumi in Japan and their countries. There were no mutual visits between Chinese and Japanese leaders from October 2001, and between South Korean and Japanese leaders from June 2005. The standstill ended when the next prime minister Abe visited China and South Korea in October 2006.
In China, the visits led to massive anti-Japanese riots. The president, ruling and opposition parties, and much of the media of South Korea openly condemned Koizumi's pilgrimages. Many Koreans applauded the president's speeches criticizing Japan, despite the South Korean President's low popularity. When asked about the reaction, Koizumi said the speeches were "for the domestic (audience)".
Although Koizumi signed the shrine's visitor book as "Junichiro Koizumi, the Prime Minister of Japan", he claimed that his visits were as a private citizen and not an endorsement of any political stance. China and Korea found the claims weak as excuses. Several journals and news reports in Japan, such as one published by Kyodo News Agency on August 15, 2006, questioned Koizumi's statement of private purpose, as he recorded his position on the shrine's guestbook as prime minister. He visited the shrine annually in fulfillment of a campaign pledge. Koizumi's last visit as prime minister was on August 15, 2006, fulfilling a campaign pledge to visit on the anniversary of Japan's surrender in World War II.
Eleven months after his resignation as prime minister, Koizumi revisited the shrine on August 15, 2007, to mark the 62nd anniversary of Japan's surrender in World War II. His 2007 visit attracted less attention from the media than his prior visits while he was in office.
Statements on World War II.
On August 15, 2005, the sixtieth anniversary of the end of World War II, Koizumi publicly stated that Japan was deeply saddened by the suffering it caused during World War II and vowed Japan would never again take "the path to war". However, Koizumi was criticized for actions which allegedly ran contrary to this expression of remorse (e.g. the Yasukuni visits), which resulted in worsening relations with China and South Korea.
Popularity.
Koizumi was at certain points in his tenure an extremely popular leader. Most people know him very well due to his trademark wavy grey hair. His outspoken nature and colourful past contributed to that; his nicknames included "Lionheart", due to his hair style and fierce spirit, and "Maverick". During his tenure in office, the Japanese public referred to him as "Jun-chan" (the suffix "chan" in the Japanese language is used as a term of familiarity, typically between children, "Jun" is a contraction of Junichiro). In June 2001, he enjoyed an approval rating of 85 percent.
In January 2002, Koizumi fired his popular Foreign Minister Makiko Tanaka, replacing her with Yoriko Kawaguchi. A few days before the sacking of Tanaka, when she was filmed crying after a dispute with government officials, Koizumi generated controversy with his statement "tears are women's ultimate weapons". Following an economic slump and a series of LDP scandals that claimed the career of YKK member Koichi Kato, by April Koizumi's popularity rating had fallen 40 percentage points since his nomination as prime minister.
Koizumi was re-elected in 2003 and his popularity surged as the economy recovered. His proposal to cut pension benefits as a move to fiscal reform turned out to be highly unpopular. Two visits to North Korea to solve the issue of abducted Japanese nationals only somewhat raised his popularity, as he could not secure several abductee's returns to Japan. In the House of Councilors elections in 2004, the LDP performed only marginally better than the opposition Democratic Party of Japan (DPJ).
In 2005, the House of Councilors rejected the contentious postal privatization bills. Koizumi previously made it clear that he would dissolve the lower house if the bill failed to pass. The Democratic Party, while expressing support for the privatization, made a tactical vote against the bill. Fifty-one LDP members also either voted against the bills or abstained.
On August 8, 2005, Koizumi, as promised, dissolved the House of Representatives and called for snap elections. He expelled rebel LDP members for not supporting the bill. The LDP's chances for success were initially uncertain; the secretary general of New Komeito (a junior coalition partner with Koizumi's Liberal Democratic Party) said that his party would entertain forming a coalition government with the Democratic Party of Japan if the DPJ took a majority in the House of Representatives.
Koizumi's popularity rose almost twenty points after he dissolved the House and expelled rebel LDP members. Opinion polls ranked the government's approval ratings between 51 and 59 percent. The electorate saw the election in terms of a vote for or against reform of the postal service, which the Democratic Party and rebel LDP members were seen as being against.
The September 2005 elections were the LDP's largest victory since 1986, giving the party a large majority in the House of Representatives and nullifying opposing voices in the House of Councilors. In the following Diet session, the last to be held under Koizumi's government, the LDP passed 82 of its 91 proposed bills, including postal privatization. A number of Koizumi-supported candidates known as "Koizumi Children" joined the Diet in this election and supported successive LDP governments until the 2009 elections, when most were defeated.
Retirement.
Koizumi announced that he would step down from office in 2006, per LDP rules, and would not personally choose a successor as many LDP prime ministers have in the past. On September 20, 2006, Shinzo Abe was elected to succeed Koizumi as president of the LDP. Abe succeeded Koizumi as prime minister on September 26, 2006.
Koizumi remained in the Diet through the administrations of Abe and Yasuo Fukuda, but announced his retirement from politics on September 25, 2008, shortly following the election of Taro Aso as Prime Minister. He retained his Diet seat until the next general election, when his son Shinjiro was elected into the same seat representing the Kanagawa 11th district in 2009. Koizumi supported Yuriko Koike in the LDP leadership election held earlier in September 2008, but Koike placed a distant third.
Since leaving office as prime minister, Koizumi has not granted a single request for an interview or television appearance, although he has given speeches and had private interactions with journalists.
Anti-nuclear advocacy.
Koizumi returned to the national spotlight in October 2013, after seven years of largely avoiding attention, when he gave a speech to business executives in Nagoya in which he stated: "We should aim to be nuclear-free... If the Liberal Democratic Party were to adopt a zero-nuclear policy, then we'd see a groundswell of support for getting rid of nuclear energy." He recalled Japan's reconstruction in the wake of World War II and called for the country to "unite toward a dream of achieving a society based on renewable energy."
Koizumi had been a proponent of nuclear power throughout his term as prime minister, and was one of the first pro-nuclear politicians to change his stance on the issue in the wake of the Fukushima disaster of 2011. His dramatic remarks were widely covered in the Japanese media, with some tabloids speculating that he may break away from the LDP to form a new party with his son Shinjiro. Economy Minister Akira Amari characterized Koizumi's stance as pure but simplistic, while other LDP administration officials downplayed the potential impact of Koizumi's views. Former prime minister Naoto Kan, however, expressed hope that Koizumi's status as then-Prime Minister Shinzo Abe's "boss" would help put pressure on the government to minimize or eliminate nuclear power in Japan.
Koizumi defended his change of stance, stating in November that "it is overly optimistic and much more irresponsible to think nuclear power plants can be maintained just with the completion of disposal facilities... We had failed to secure sites for final disposal even before an accident occurred," concluding that "it's better to spend money on developing natural energy resources--citizens are more likely to agree with that idea--than using such large amounts of expenses and energy to advance such a feckless project [as nuclear power]." He explained that in August, he had visited a nuclear waste disposal facility in Finland, where he learned that nuclear waste would have to be sealed up for 100,000 years. A poll by the "Asahi Shimbun" in November 2013 found that 54% of the public supported Koizumi's anti-nuclear statements. Koizumi told one reporter that he felt lied to by the Federation of Electric Power Companies of Japan, which characterized nuclear power as a safe alternative to fossil fuels, stating that "we certainly had no idea how difficult it is to control nuclear energy."
Koizumi reportedly approached leader, who served as Prime Minister in an anti-LDP coalition cabinet in the 1990s, to run for Governor of Tokyo in the February 2014 gubernatorial election on the platform of opposing the Abe government's pro-nuclear policy.
Hosokawa ran in the election with Koizumi's support, but lost to the LDP-supported candidate Yoichi Masuzoe. Koizumi and Hosokawa continued their collaboration in the wake of this defeat, organizing an anti-nuclear forum to be held in May 2014.
Personal life.
Koizumi lives in Yokosuka, Kanagawa.
Family.
Koizumi married 21-year-old university student Kayoko Miyamoto in 1978. The couple had been formally introduced to each other as potential spouses, a common practice known as "omiai". The wedding ceremony at the Tokyo Prince Hotel was attended by about 2,500 people, including Takeo Fukuda (then Prime Minister), and featured a wedding cake shaped like the National Diet Building. The marriage ended in divorce in 1982, as Kayoko was reportedly unhappy with her married life for several reasons. After this divorce, Koizumi never married again, saying that divorce consumed ten times more energy than marriage.
Koizumi had custody of two of his three sons: Kōtarō Koizumi and Shinjirō Koizumi, who were reared by one of his sisters. Shinjiro is the representative for Kanagawa's 11th district, a position his father has also filled. The youngest son, Yoshinaga Miyamoto, now a graduate of Keio University, was born following the divorce and has never met Koizumi. Yoshinaga is known to have attended one of Koizumi's rallies, but was turned away from trying to meet his father. He was also turned away from attending his paternal grandmother's funeral. Koizumi's ex-wife Kayoko Miyamoto has asked unsuccessfully several times to meet their two oldest sons.
Koizumi is known to have a cousin in Brazil, and was overwhelmed to the point of tears when he visited Brazil in 2004 and was met by a group of Japanese immigrants.
Interests.
Koizumi is a fan of Richard Wagner and has released a CD of his favorite pieces by contemporary Italian composer Ennio Morricone. He is also a fan of the heavy metal band X Japan, with the LDP having even used their song "Forever Love" in television commercials in 2001. It was also reported that he was influential in getting the museum honoring X Japan's deceased guitarist hide made.
Koizumi is also a noted fan of Elvis Presley, with whom he shares a birthday (January 8). In 2001 he released a collection of his favorite Elvis songs on CD, with his comments about each song. His brother is Senior Advisor of the Tokyo Elvis Fan Club. Koizumi and his brother helped finance a statue of Elvis in Tokyo's Harajuku district. On June 30, 2006, Koizumi visited Presley's estate, Graceland, accompanied by U.S. President George W. Bush, and First Lady Laura Bush. After arriving in Memphis aboard Air Force One, they headed to Graceland. While there, Koizumi briefly sang a few bars of his favourite Elvis tunes, whilst warmly impersonating Presley, and wearing Presley's trademark oversized golden sunglasses.
Koizumi also appreciates Finnish composer Jean Sibelius. On September 8, 2006, he and Finnish Prime Minister Matti Vanhanen visited the Sibelius' home, where Koizumi showed respect to the late composer with a moment of silence. He owns reproductions of the manuscripts of all seven symphonies by Sibelius.
In 2009, Koizumi made a voice acting appearance in an Ultraman feature film, "", playing the voice of Ultraman King. Koizumi said he took on the role at the urging of his son Shinjiro. His political career is parodied in a seinen manga, "Mudazumo Naki Kaikaku", which re-interprets his life as a mahjong master.
He has been compared many times to American actor Richard Gere, because of their similar hair style. In 2005, he used the latter as a boost for his falling popularity, by staging an "impromptu ballroom dance performance".
Koizumi cabinets.
Notes:

</doc>
<doc id="49547" url="http://en.wikipedia.org/wiki?curid=49547" title="Hyperlink">
Hyperlink

In computing, a hyperlink is a reference to data that the reader can directly follow either by clicking or by hovering. A hyperlink points to a whole document or to a specific element within a document. Hypertext is text with hyperlinks. A software system that's used for viewing and creating hypertext is a "hypertext system", and to create a hyperlink is "to hyperlink" (or simply "to link"). A user following hyperlinks is said to "navigate" or "browse" the hypertext.
A hyperlink has an "anchor", which is the location within a certain type of a document from which the hyperlink can be followed only from the homepage. The document containing a hyperlink is known as its source code document. For example, in an online reference work such as Wikipedia, many words and terms in the text are hyperlinked to definitions of those terms. Hyperlinks are often used to implement reference mechanisms, such as tables of contents, footnotes, bibliographies, indexes, letters, and glossaries.
In some hypertext, hyperlinks can be bidirectional: they can be followed in two directions, so both ends act as anchors and as targets. More complex arrangements exist, such as many-to-many links.
The effect of following a hyperlink may vary with the hypertext system and may sometimes depend on the link itself; for instance, on the World Wide Web, most hyperlinks cause the target document to replace the document being displayed, but some are marked to cause the target document to open in a new window. Another possibility is transclusion, for which the link target is a document fragment that replaces the link anchor within the source document. Not only persons browsing the document follow hyperlinks; they may also be followed automatically by programs. A program that traverses the hypertext, following each
hyperlink and gathering all the retrieved documents is known as a Web "spider" or crawler.
Types of links.
Inline links.
An inline link displays remote content without the need for embedding the content. The remote content may be accessed with or without the user selecting the link.
An inline link may display a modified version of the content; for instance, instead of an image, a thumbnail, low resolution preview, cropped section, or magnified section may be shown. The full content will then usually be available on demand, as is the case with print publishing software – e.g. with an external link. This allows for smaller file sizes and quicker response to changes when the full linked content is not needed, as is the case when rearranging a page layout.
Anchor.
An anchor hyperlink is a link bound to a portion of a document—generally text, though not necessarily. For instance, it may also be a "hot area" in an image (image map in HTML), a designated, often irregular part of an image. One way to define it is by a list of coordinates that indicate its boundaries. For example, a political map of Africa may have each country hyperlinked to further information about that country. A separate invisible hot area interface allows for swapping skins or labels within the linked hot areas without repetitive embedding of links in the various skin elements.
Hyperlinks in various technologies.
Hyperlinks in HTML.
Tim Berners-Lee saw the possibility of using hyperlinks to link any information to any other information over the Internet. Hyperlinks were therefore integral to the creation of the World Wide Web. Web pages are written in the hypertext mark-up system HTML.
Links are specified in HTML using the <a> (anchor) elements.
To see the HTML used to create a page, most browsers offer a "view page source" option. The HTML code consists of tags; the hyperlink tag starts with "", marking the start of the link. The text between the hyperlink tag and its corresponding "closing" tag ( "") is called the anchor text.
Webgraph is a graph, formed from web pages as vertices and hyperlinks, as directed edges.
XLink: hyperlinks in XML.
The W3C Recommendation called XLink describes hyperlinks that offer a far greater degree of functionality than those offered in HTML. These extended links can be "multidirectional", linking from, within, and between XML documents. It also describes "simple links", which are unidirectional and therefore offer no more functionality than hyperlinks in HTML.
Hyperlinks in other document technologies.
Hyperlinks are used in the Gopher protocol, text editors, PDF documents, help systems such as Windows Help, word processing documents, spreadsheets, Apple's HyperCard and many other places.
Hyperlinks in virtual worlds.
Hyperlinks are being implemented in various 3D virtual world networks, including those which utilize the OpenSimulator and Open Cobalt platforms.
Hyperlinks in wikis.
While wikis may use HTML-type hyperlinks, the use of wiki markup, a set of lightweight markup languages specifically for wikis, provides simplified syntax for linking pages within wiki environments, or in other words, for creating wikilinks.
The syntax and appearance of wikilinks may vary. Ward Cunningham's original wiki software, the WikiWikiWeb, used CamelCase for this purpose. CamelCase was also used in the early version of Wikipedia and is still used in some wikis, such as TiddlyWiki, Trac and PMWiki. A common markup syntax is the use of double square brackets around the term to be wikilinked. For example, the input "zebras" will be converted by wiki software to a link to the zebras article.
Hyperlinks used in wikis are commonly classified as follows:
Wikilinks are visibly distinct from other text, and if an internal wikilink leads to a page that does not yet exist, it usually has a different specific visual appearance. For example, in Wikipedia wikilinks are displayed in blue, except those which link to pages which do not yet exist, which are instead shown in red. Another possibility for linking is to display a highlighted clickable question mark after the wikilinked term.
How hyperlinks work in HTML.
A link from one domain to another is said to be "outbound" from its source anchor and "inbound" to its target.
The most common destination anchor is a URL used in the World Wide Web. This can refer to a document, e.g. a webpage, or other resource, or to a position in a webpage. The latter is achieved by means of an HTML element with a "name" or "id" attribute at that position of the HTML document. The URL of the position is the URL of the webpage with a fragment identifier — "#"id attribute" — appended.
When linking to PDF documents from an HTML page the "id attribute"" can be replaced with syntax that references a page number or another element of the PDF, for example, "#"page=386"".
Link behavior in web browsers.
A web browser usually displays a hyperlink in some distinguishing way, e.g. in a different color, font or style. The behavior and style of links can be specified using the Cascading Style Sheets (CSS) language.
In a graphical user interface, the appearance of a mouse cursor may change into a hand motif to indicate a link. In most graphical web browsers, links are displayed in underlined blue text when they have not been visited, but underlined purple text when they have. When the user activates the link (e.g. by clicking on it with the mouse) the browser will display the target of the link. If the target is not an HTML file, depending on the file type and on the browser and its plugins, another program may be activated to open the file.
The HTML code contains some or all of the five main characteristics of a link:
It uses the HTML element "a" with the attribute "href" (HREF is an abbreviation for "Hypertext REFerence") and optionally also the attributes "title", "target", and "class" or "id":
To embed a link into a web page, blogpost, or comment, it may take this form:
In a typical web browser, this would display as the underlined word "Example" in blue, which when clicked would take the user to the example.com website. This contributes to a clean, easy to read text or document.
When the cursor hovers over a link, depending on the browser and graphical user interface, some informative text about the link can be shown, popping up, not in a regular window, but in a special hover box, which disappears when the cursor is moved away (sometimes it disappears anyway after a few seconds, and reappears when the cursor is moved away and back). Mozilla Firefox, IE, Opera, and many other web browsers all show the URL. In addition, the URL is commonly shown in the status bar.
Normally, a link will open in the current frame or window, but sites that use frames and multiple windows for navigation can add a special "target" attribute to specify where the link will be loaded. If no window exists with that name, a new window will be created with the ID, which can be used to refer to the window later in the browsing session.
Creation of new windows is probably the most common use of the "target" attribute. In order to prevent accidental reuse of a window, the special window names "_blank" and "_new" are usually available, and will always cause a new window to be created. It is especially common to see this type of link when one large website links to an external page. The intention in that case is to ensure that the person browsing is aware that there is no endorsement of the site being linked to by the site that was linked from. However, the attribute is sometimes overused and can sometimes cause many windows to be created even while browsing a single site.
Another special page name is "_top", which causes any frames in the current window to be cleared away so that browsing can continue in the full window.
History.
The term "hyperlink" was coined in 1965 (or possibly 1964) by Ted Nelson at the start of Project Xanadu. Nelson had been inspired by "As We May Think", a popular 1945 essay by Vannevar Bush. In the essay, Bush described a microfilm-based machine (the Memex) in which one could link any two pages of information into a "trail" of related information, and then scroll back and forth among pages in a trail as if they were on a single microfilm reel.
In a series of books and articles published from 1964 through 1980, Nelson transposed Bush's concept of automated cross-referencing into the computer context, made it applicable to specific text strings rather than whole pages, generalized it from a local desk-sized machine to a theoretical proprietary worldwide computer network, and advocated the creation of such a network. Though Nelson's Xanadu Corporation was eventually funded by Autodesk in the 1980s, it never created this proprietary public-access network. Meanwhile, working independently, a team led by Douglas Engelbart (with Jeff Rulifson as chief programmer) was the first to implement the hyperlink concept for scrolling within a single document (1966), and soon after for connecting between paragraphs within separate documents (1968), with NLS. Ben Shneiderman working with graduate student Dan Ostroff designed and implemented the highlighted link in the in 1983. HyperTIES was used to produce the world's first electronic journal, the July 1988 Communications of ACM, which was cited as the source for the link concept in Tim Berners-Lee's Spring 1989 manifesto for the Web. In 1988, Ben Shneiderman and Greg Kearsley used HyperTIES to publish "Hypertext Hands-On!", the world's first electronic book.
A database program HyperCard was released in 1987 for the Apple Macintosh that allowed hyperlinking between various pages within a document. In 1990, Windows Help, which was introduced with Microsoft Windows 3.0, had widespread use of hyperlinks to link different pages in a single help file together; in addition, it had a visually different kind of hyperlink that caused a popup help message to appear when clicked, usually to give definitions of terms introduced on the help page. The first widely used open protocol that included hyperlinks from any Internet site to any other Internet site was the Gopher protocol from 1991. It was soon eclipsed by HTML after the 1993 release of the Mosaic browser (which could handle Gopher links as well as HTML links). HTML's advantage was the ability to mix graphics, text, and hyperlinks, unlike Gopher, which just had menu-structured text and hyperlinks.
Legal issues.
While hyperlinking among webpages is an intrinsic feature of the web, some websites object to being linked by other websites; some have claimed that linking to them is not allowed without permission.
Contentious in particular are deep links, which do not point to a site's home page or other entry point designated by the site owner, but to content elsewhere, allowing the user to bypass the site's own designated flow, and "inline links", which incorporate the content in question into the pages of the linking site, making it seem part of the linking site's own content unless an explicit attribution is added.
In certain jurisdictions it is or has been held that hyperlinks are not merely references or citations, but are devices for copying web pages. In the Netherlands, Karin Spaink was initially convicted in this way of copyright infringement by linking, although this ruling was overturned in 2003. The courts that advocate this view see the mere publication of a hyperlink that connects to illegal material to be an illegal act in itself, regardless of whether referencing illegal material is illegal. In 2004, Josephine Ho was acquitted of 'hyperlinks that corrupt traditional values' in Taiwan.
In 2000 British Telecom sued Prodigy, claiming that Prodigy infringed its patent (U.S. Patent ) on web hyperlinks. After litigation, a court found for Prodigy, ruling that British Telecom's patent did not cover web hyperlinks.
In United States "jurisprudence", there is a distinction between the mere act of linking to someone else's website, and linking to content that is illegal (i.e. gambling illegal in the US) or infringing (i.e. illegal MP3 copies). Several courts have found that merely linking to someone else's website, even if by bypassing commercial advertising, is not copyright or trademark infringement, regardless of how much someone else might object. Linking to illegal or infringing content can be sufficiently problematic to give rise to legal liability. For a summary of the current status of US copyright law as to hyperlinking, see discussion regarding the "Arriba Soft" case.
Philosophical implications.
Hypertext has the ability to separate form from content on the Internet. Once form and content have been separated, Internet users with no previous coding experience are able to upload content (text, photos, video, etc.). The advancement of the hyperlink fundamentally changes user interaction with digital media. Hypertext also has the ability to help sort information in the digital world as an increasing amount of information is being added to the Internet daily.

</doc>
<doc id="49549" url="http://en.wikipedia.org/wiki?curid=49549" title="Katsura Tarō">
Katsura Tarō

Prince Katsura Tarō (桂 太郎, January 4, 1848 – October 10, 1913), was a general in the Imperial Japanese Army, politician and three-time Prime Minister of Japan.
Early life.
Katsura was born into a "samurai" family from Hagi, Chōshū Domain (present day Yamaguchi Prefecture). As a youth, he joined the movement against the Tokugawa shogunate and participated in some of the major battles of the Boshin War that led to the Meiji Restoration.
Army career.
The new Meiji government considered that Katsura displayed great talent, and sent him to Germany to study military science. He served as military attaché at the Japanese embassy in Germany from 1875–1878 and again from 1884-1885. On his return to Japan, he was promoted to major general. He served in several key positions within the Imperial Japanese Army, and in 1886 was appointed Vice-Minister of War.
During the First Sino-Japanese War (1894–1895) Katsura commanded the IJA 3rd Division under his mentor, Field Marshal Yamagata Aritomo. During the war, his division made a memorable march in the depth of winter from the north-east shore of the Yellow Sea to Haicheng, finally occupying Niuchwang, and effecting a junction with the IJA 2nd Army which had moved up the Liaodong Peninsula.
After the war, he was elevated with the title of "shishaku" (viscount) under the "kazoku" peerage system. He was appointed 2nd Governor-General of Taiwan from June 2, 1896 to October 1896.
In successive cabinets from 1898 to 1901, he served as Minister of War.
As Prime Minister.
Katsura Tarō served as the 11th, 13th and 15th Prime Minister of Japan. He remains the longest-serving Prime Minister of Japan to date.
First Katsura Administration.
Katsura became Prime Minister for the first time on June 2, 1901 and retained the office for four and a half years to January 7, 1906, which was a record in Japan at that time. During his four year first term Japan emerged as a major imperialist power in East Asia. In terms of foreign affairs, it was marked by the Anglo-Japanese Alliance of 1902 and victory over Russia in the Russo-Japanese War of 1904-1905. During his tenure, the Taft–Katsura Agreement regarding the issue of Japanese hegemony over Korea was reached with the United States. During this term, Katsura received the Grand Cross of the Order of St Michael and St George from King Edward VII of Great Britain, and was elevated to the rank of marquess by Emperor Meiji.
In terms of domestic policy, Katsura was a strictly conservative politician who attempted to distance himself from the Diet of Japan and party politics. His political views mirrored that of Yamagata Aritomo, in that he viewed that his sole responsibility was to the Emperor. He vied for control of the government with the "Rikken Seiyūkai", the majority party of the lower house, headed by his arch-rival, Marquess Saionji Kinmochi.
In January 1906, Katsura resigned the premiership to Saionji Kinmochi over controversy and unpopularity of the Treaty of Portsmouth (1905) ending the war between Japan and Russia. However, his resignation was part of a “back door deal” brokered by Hara Takashi to alternate power between Saionji and Hara.
On April 1, 1906, he was awarded the Grand Cordon of the Supreme Order of the Chrysanthemum.
Second Katsura Administration.
Katsura returned as Prime Minister from July 14, 1908 to August 30, 1911. His second term was noteworthy for the Japan–Korea Annexation Treaty of 1910.
He also promulgated the Factory Act in 1911, which was the first act for the purpose of labor protection in Japan.
Katsura was increasingly unpopular during his second term over public perception that he was using his office to further his personal fortune, and the interests of the military "(gunbatsu)" over the welfare of the people. He also faced growing public dissatisfaction over the persistence of the "hanbatsu" domainal based politics. 
After his resignation, he became a "kōshaku" (公爵 = prince), Lord Keeper of the Privy Seal of Japan and one of the "genrō."
Third Katsura Administration.
Katsura's brief reappointment again as Prime Minister again from December 21, 1912 to February 20, 1913 sparked widespread riots in what became known as the Taisho Political Crisis. His appointment was viewed as a plot by the "genrō" to overthrown rule by the Constitution. However, rather than compromising, Katsura created his own political party, the "Rikken Dōshikai" in an effort to establish his own support base. 
However, faced with a no-confidence motion (the first successful one in Japanese history) and the loss of the support of his backers, he was forced to resign in February 1913. He was succeeded by Yamamoto Gonnohyōe and the Diet was held by his new "Rikken Dōshikai" party. 
Death.
Katsura died of stomach cancer eight months later on October 10, 1913, aged 65. His funeral was held at the temple of Zōjō-ji in Shiba, Tokyo and his grave is at the Shōin Jinja, in Setagaya, Tokyo.
Honors.
"From the corresponding article in the Japanese Wikipedia"

</doc>
<doc id="49551" url="http://en.wikipedia.org/wiki?curid=49551" title="Rugby School">
Rugby School

Rugby School is a co-educational day and boarding school located in the town of Rugby, Warwickshire, England. Rugby School is a registered charity and is one of the oldest independent schools in Britain. The influence of Rugby and its pupils and masters in the nineteenth century was enormous and in many ways the stereotype of the English public school is a reworking of Thomas Arnold's Rugby. It is one of the original nine English public schools defined by the Public Schools Act 1868, It is one of the best-known and most expensive schools in the country. "Floreat Rugbeia" is the traditional school song. Rugby School enrolls boarding and day students with a total student enrollment of 800 in day student grades 4 to 12.
Rugby School is best known as the birthplace of the sport that bears its name as well as the fable of its pupil William Webb Ellis picking up the ball in 1823. In 1845 three Rugby School pupils produced the first written rules of the "Rugby style of game."
History.
Early challenges.
Rugby School was founded in 1567 as a provision in the will of Lawrence Sheriff, who had made his fortune supplying groceries to Queen Elizabeth I of England. Since Lawrence Sheriff lived in Rugby and the neighbouring Brownsover, the school was intended to be a free grammar school for the boys of those towns. Up to 1667, the school remained in comparative obscurity. Its history during that trying period is characterised mainly by a series of lawsuits between descendants of the founder, who tried to defeat the intentions of the testator, and the masters and trustees, who tried to carry them out. A final decision was handed down in 1667, confirming the findings of a commission in favour of the trust, and henceforth the school maintained a steady growth.
Academic life.
Teaching and Learning is one of the most important aspects of Rugby School. Students at Rugby have the benefit of great facilities and highly qualified staff, providing them with the ability to succeed in life. Even after leaving school students keep valuable traits that they have acquired at their stay in rugby including responsibility, dedication and independence. Rugby being an elite boarding school expects high academic achievements from its students in terms of their work ethic and intellectual drive.
Pupils beginning Rugby in the F Block (first year) receive a wide array of subjects providing them with the best ability to succeed in the 21st century. Both the variety and balance of education is continued all the way through to D block (GCSE year). This range of subjects allows students to broaden their minds and acquire a better knowledge over more materials preparing them for university.
The Upper School at Rugby is unlike other schools. We have developed Pre U-Courses for our most talented of students and provide standard A-levels in 29 subjects. Students at this stage have the choice of taking 3 or four subjects allowing then to have higher concentration on relevant subjects. Students are also offered the opportunity to take an extended project which will allow them to showcase their talents and capabilities in certain subjects.
Through this rigorous academic course Rugby's main goal is to prepare its students for success in the future . Rugby produces top A- levels and GCSE results, two major necessities universities are looking for. Rugby also gives students the ability to join societies which play important roles in the curriculum by emphasizing cooperation and discussion among peers . We believe we have provided our students with the best education to allow them to flourish later in life.
Scholarships.
The Governing Body of Rugby School is dedicated to providing education to all those who would benefit from an education like this, by giving finical benefits with school fees to families unable to afford them. Parents of pupils who are given a Scholarship are capable of obtaining a 10% fee deduction, although more than one scholarship can be awarded to one student.
Growth.
It was no longer desirable to have only local boys attending and the nature of the school shifted, and so a new school – Lawrence Sheriff Grammar School – was founded in 1878 to continue Lawrence Sheriff's original intentions; that school receives a substantial proportion of the endowment income from Lawrence Sheriff's estate every year.
The core of the school (which contains School House, featured in "Tom Brown's Schooldays") was completed in 1815 and is built around the Old Quad (quadrangle), with its Georgian architecture. Especially notable rooms are the Upper Bench (an intimate space with a book-lined gallery), the Old Hall of School House, and the Old Big School (which makes up one side of the quadrangle and was once the location for teaching all junior pupils). Thomas Hughes (like his fictional hero, Tom Brown) once carved his name onto the hands of the school clock, situated on a tower above the Old Quad. The polychromatic school chapel, new quadrangle, Temple Reading Room, Macready Theatre and Gymnasium were designed by the well-known Victorian Gothic revival architect William Butterfield in 1875, and the smaller Memorial Chapel was dedicated in 1922.
By the 20th century Rugby expanded and new buildings were built inspired by this Edwardian Era.The Temple Speech Room, named after former Head Master and Archbishop of Canterbury Frederick Temple (1858–69) and now used for whole-School assemblies, Speech Days, concerts, musicals – and BBC Mastermind. Oak-panelled walls boast the portraits of illustrious alumni, including Neville Chamberlain holding his piece of paper. Between the wars, the Memorial Chapel, the Music Schools and a new Sanatorium appeared.
Cartel.
In 2005, Rugby School was one of fifty of the country's leading independent schools which were found guilty of running an illegal price-fixing cartel which had allowed them to drive up fees for thousands of parents. Each school was required to pay a nominal penalty of £10,000 and all agreed to make ex-gratia payments totalling three million pounds into a trust designed to benefit pupils who attended the schools during the period in respect of which fee information was shared. However, Mrs Jean Scott, the head of the Independent Schools Council, said that independent schools had always been exempt from anti-cartel rules applied to business, were following a long-established procedure in sharing the information with each other, and that they were unaware of the change to the law (on which they had not been consulted). She wrote to John Vickers, the OFT director-general, saying, "They are not a group of businessmen meeting behind closed doors to fix the price of their products to the disadvantage of the consumer. They are schools that have quite openly continued to follow a long-established practice because they were unaware that the law had changed."
Headmasters.
Thomas Arnold.
Rugby's most famous headmaster was Thomas Arnold, appointed in 1828, he executed many reforms to the school curriculum and administration. Arnold's reputation and the school's reputation was immortalised through Thomas Hughes' book "Tom Brown's School Days."
David Newsome writes about the new educational methods employed by Arnold in his book, 'Godliness and Good Learning' (Cassell 1961). He calls the morality practised at Arnold's school muscular Christianity. Dr. George Mosse, former professor of History in University of Wisconsin-Madison, lectured on Arnold's time at Rugby. According to Mosse, Thomas Arnold created an institution which fused religious and moral principles, gentlemanly conduct, and learning based on self-discipline. These morals were socially enforced through the "Gospel of work." The object of education was to produced "the Christian gentleman," a man with good outward appearance, playful but earnest, industrious, manly, honest, virginal pure, innocent, and responsible.
John Percival.
In 1888 the appointment of Marie Bethell Beauclerc by Percival was the first appointment of a female teacher in an English boys' public school and the first time shorthand had been taught in any such school. The shorthand course was popular with one hundred boys in the classes.
William Webb Ellis.
The game of Rugby football owes its name to the school. The legend of William Webb Ellis and the origin of the game is commemorated by a plaque. The story has been known to be a myth since it was first investigated by the Old Rugbeian Society (renamed the Rugbeian Society) in 1895. There were no standard rules for football during Webb Ellis's time at Rugby (1816–1825) and most varieties involved carrying the ball. The games played at Rugby were organised by the pupils and not the masters, the rules of the game played at Rugby and elsewhere were a matter of custom and were not written down. They were frequently changed and modified with each new intake of students. The sole source of the story is credited to one Matthew Bloxam (a former pupil, but not a contemporary of Webb Ellis) in October 1876 (four years after the death of Webb Ellis) in a letter to the school newspaper ("The Meteor") wherein he quotes some unknown friend relating the story to him. He elaborated on the story some three years later in another letter to "The Meteor", but shed no further light on its source. Richard Lindon is credited for the invention of the "oval" rugby ball, the rubber inflatable bladder and the brass hand pump. Lindon, a Boot and Shoemaker, had premises immediately across the street from the School's main entrance in Lawrence Sheriff Street. No doubt the boys of Rugby School had significant input into their required design.
Houses.
Rugby School has both day and boarding-pupils, the latter in the majority. Originally it was for boys only, but girls have been admitted to the sixth form since 1975. It went fully co-educational in 1995. The school community is divided into houses.
There is also a co-educational day house for 11+ admission, called Marshall House, it is much smaller than the other main school houses.
Alumni.
There have been a number of notable including the purported father of the sport of Rugby William Webb Ellis, the inventor of Australian rules football Tom Wills, the war poets Rupert Brooke and John Gillespie Magee, Jr., Prime Minister Neville Chamberlain, author and mathematician Lewis Carroll, poet and cultural critic Matthew Arnold, the author and social critic Salman Rushdie (who said of his time there: "Almost the only thing I am proud of about going to Rugby school was that Lewis Carroll went there too.") and the Irish writer and republican Francis Stuart. Matthew Arnold's father Thomas Arnold, was a headmaster of the school. An OR seven-a-side rugby team was invited to compete in the inaugural Old Boys Sevens tournament in June 2010, hosted by the Old Silhillians, the former pupils' association of Solihull School. Philip Henry Bahr (later Sir Philip Henry Manson-Bahr), a zoologist and medical doctor, World War I veteran, was President of both Royal Society of Tropical Medicine and Hygiene and Medical Society of London, and Vice-President of the British Ornithologists’ Union.
Rugbeian Society.
The Rugbeian Society is for former pupils at the School. An Old Rugbeian is sometimes referred to as an OR.
The purposes of the society are to encourage and help Rugbeians in interacting with each other and to strengthen the ties between ORs and the school.
In 2010 the Rugbeians reached the semi-finals of the Public Schools' Old Boys' Sevens tournament, hosted by the Old Silhillians to celebrate the 450th anniversary of fellow Warwickshire public school, Solihull School.
Rugby Fives.
Rugby Fives is a handball game, similar to squash, played in an enclosed court. It has similarities with Winchester Fives (a form of Wessex Fives) and Eton Fives.
It is most commonly believed to be derived from Wessex Fives, a game played by Thomas Arnold, Headmaster of Rugby, who had played Wessex Fives when a boy at Lord Weymouth's Grammer, now Warminster School. The open court of Wessex Fives, built in 1787, is still in existence at Warminster School although it has fallen out of regular use.
Rugby Fives is played between two players (singles) or between two teams of two players each (doubles), the aim being to hit the ball above a 'bar' across the front wall in such a way that the opposition cannot return it before a second bounce. The ball is slightly larger than a golf ball, leather-coated and hard. Players wear leather padded gloves on both hands, with which they hit the ball.
Rugby Fives continues to have a good following with tournaments being run nationwide, presided over by the Rugby Fives Association.

</doc>
<doc id="49553" url="http://en.wikipedia.org/wiki?curid=49553" title="Savonlinna">
Savonlinna

Savonlinna (Swedish: "Nyslott", Russian: Нейшлот, Neishlott) is a town and a municipality of () inhabitants in the southeast of Finland, in the heart of the Saimaa lake region. The Finnish name of the town means ""Castle of Savonia" and the Swedish name means "Newcastle"". The city's Russian name is a direct transliteration of the Swedish name.
History.
The city was founded in 1639, based on Olavinlinna castle. The castle was founded by Erik Axelsson Tott in 1475 in an effort to protect Savonia and to control the unstable border between the Kingdom of Sweden and its Russian adversary. During the Russo-Swedish War (1741–1743), the castle was captured by Field-Marshal Peter Lacy. It was held by Russia between 1743 and 1812, when it was granted back to Finland as a part of the "Old Finland".
In 1973 the municipality of Sääminki was consolidated with Savonlinna. In the beginning of year 2009 the municipality of Savonranta and a 31.24 km2 land strip from Enonkoski between Savonlinna and Savonranta were consolidated with Savonlinna.
Transport.
This town is 335 km away from the capital of Helsinki by road, some four hours away by train. There is an airport in the town, and the journey to Helsinki takes 40–60 minutes by plane. It is built on a chain of islands located throughout a number of large lakes.
Education.
The University of Eastern Finland has a campus in Savonlinna, primarily for teacher education.
Attractions.
The city hosts the famous annual Savonlinna Opera Festival. The operas are performed on a stage built inside the castle. It also has hosted the Mobile Phone Throwing World Championships annually since 2000.
Sports.
The ice hockey team of Savonlinna, SaPKo or Savonlinnan Pallokerho, is playing in the second tier Mestis. Notable SaPKo alumni include Jarmo Myllys, Ville Leino, Tuukka Rask (Boston Bruins) and Hannu Aravirta. One notable hockey player from the city who did not play for SaPKo is Joonas Rask of HIFK and formerly the Nashville Predators, younger brother of Tuukka.
The top-tier volleyball team Saimaa Volley plays some of its home matches in Savonlinna. The football team Savonlinnan Työväen Palloseura (STPS), is playing in Kolmonen, the fourth tier.
International relations.
Twin towns — Sister cities.
Savonlinna is twinned with:
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="49555" url="http://en.wikipedia.org/wiki?curid=49555" title="Batholith">
Batholith

A batholith (from Greek "bathos", depth + "lithos", rock) is a large emplacement of igneous intrusive (also called plutonic) rock that forms from cooled magma deep in the Earth's crust. Batholiths are almost always made mostly of felsic or intermediate rock-types, such as granite, quartz monzonite, or diorite (see also "granite dome").
Formation.
Although they may appear uniform, batholiths are in fact structures with complex histories and compositions. They are composed of multiple masses, or "plutons", bodies of igneous rock of irregular dimensions (typically at least several kilometers) that can be distinguished from adjacent igneous rock by some combination of criteria including age, composition, texture, or mappable structures. Individual plutons are crystallized from magma that traveled toward the surface from a zone of partial melting near the base of the Earth's crust.
Traditionally, these plutons have been considered to form by ascent of relatively buoyant magma in large masses called "plutonic diapirs". Because the diapirs are liquified and very hot, they tend to rise through the surrounding native "country" rock, pushing it aside and partially melting it. Most diapirs do not reach the surface to form volcanoes, but instead slow down, cool, and usually solidify 5 to 30 kilometers underground as plutons (hence the use of the word "pluton"; in reference to the Roman god of the underworld Pluto). It has also been proposed that plutons commonly are formed not by diapiric ascent of large magma diapirs, but rather by aggregation of smaller volumes of magma that ascended as dikes.
A batholith is formed when many plutons converge to form a huge expanse of granitic rock. Some batholiths are mammoth, paralleling past and present subduction zones and other heat sources for hundreds of kilometers in continental crust. One such batholith is the Sierra Nevada Batholith, which is a continuous granitic formation that makes up much of the Sierra Nevada in California. An even larger batholith, the Coast Plutonic Complex is found predominantly in the Coast Mountains of western Canada, and extends for 1,800 kilometers and reaches into southeastern Alaska.
Surface expression and erosion.
A batholith is an exposed area of (mostly) continuous plutonic rock that covers an area larger than 100 square kilometers (40 square miles). Areas smaller than 100 square kilometers are called "stocks". However, the majority of batholiths visible at the surface (via outcroppings) have areas far greater than 100 square kilometers. These areas are exposed to the surface through the process of erosion accelerated by continental uplift acting over many tens of millions to hundreds of millions of years. This process has removed several tens of square kilometers of overlying rock in many areas, exposing the once deeply buried batholiths.
Batholiths exposed at the surface are subjected to huge pressure differences between their former location deep in the earth and their new location at or near the surface. As a result, their crystal structure expands slightly and over time. This manifests itself by a form of mass wasting called exfoliation. This form of weathering causes convex and relatively thin sheets of rock to slough off the exposed surfaces of batholiths (a process accelerated by frost wedging). The result is fairly clean and rounded rock faces. A well-known result of this process is Half Dome, located in Yosemite Valley.

</doc>
<doc id="49557" url="http://en.wikipedia.org/wiki?curid=49557" title="Castle">
Castle

A castle (from Latin: "castellum") is a type of fortified structure built in Europe and the Middle East during the Middle Ages by nobility. Scholars debate the scope of the word "castle", but usually consider it to be the private fortified residence of a lord or noble. This is distinct from a palace, which is not fortified; from a fortress, which was not always a residence for nobility; and from a fortified settlement, which was a public defence – though there are many similarities among these types of construction. Usage of the term has varied over time and has been applied to structures as diverse as hill forts and country houses. Over the approximately 900 years that castles were built, they took on a great many forms with many different features, although some, such as curtain walls and arrowslits, were commonplace.
A European innovation, castles originated in the 9th and 10th centuries, after the fall of the Carolingian Empire resulted in its territory being divided among individual lords and princes. These nobles built castles to control the area immediately surrounding them, and were both offensive and defensive structures; they provided a base from which raids could be launched as well as protection from enemies. Although their military origins are often emphasised in castle studies, the structures also served as centres of administration and symbols of power. Urban castles were used to control the local populace and important travel routes, and rural castles were often situated near features that were integral to life in the community, such as mills and fertile land.
Many castles were originally built from earth and timber, but had their defences replaced later by stone. Early castles often exploited natural defences, and lacked features such as towers and arrowslits and relied on a central keep. In the late 12th and early 13th centuries, a scientific approach to castle defence emerged. This led to the proliferation of towers, with an emphasis on flanking fire. Many new castles were polygonal or relied on concentric defence – several stages of defence within each other that could all function at the same time to maximise the castle's firepower. These changes in defence have been attributed to a mixture of castle technology from the Crusades, such as concentric fortification, and inspiration from earlier defences such as Roman forts. Not all the elements of castle architecture were military in nature, and devices such as moats evolved from their original purpose of defence into symbols of power. Some grand castles had long winding approaches intended to impress and dominate their landscape.
Although gunpowder was introduced to Europe in the 14th century, it did not significantly affect castle building until the 15th century, when artillery became powerful enough to break through stone walls. While castles continued to be built well into the 16th century, new techniques to deal with improved cannon fire made them uncomfortable and undesirable places to live. As a result, true castles went into decline and were replaced by artillery forts with no role in civil administration, and country houses that were indefensible. From the 18th century onwards, there was a renewed interest in castles with the construction of mock castles, part of a romantic revival of Gothic architecture, but they had no military purpose.
Definition.
Etymology.
The word "castle" is derived from the Latin word "castellum" which is a diminutive of the word "castrum", meaning "fortified place". The Old English "castel", Old French "castel" or "chastel", French "château", Spanish "castillo", Italian "castello", and a number of words in other languages also derive from "castellum". The word "castle" was introduced into English shortly before the Norman Conquest to denote this type of building, which was then new to England.
Defining characteristics.
In its simplest terms, the definition of a castle accepted amongst academics is "a private fortified residence". This contrasts with earlier fortifications, such as Anglo Saxon burhs and walled cities such as Constantinople and Antioch in the Middle East; castles were not communal defences but were built and owned by the local feudal lords, either for themselves or for their monarch. Feudalism was the link between a lord and his vassal where, in return for military service, the lord would grant the vassal land and expect loyalty. In the late 20th century, there was a trend to refine the definition of a castle by including the criterion of feudal ownership, thus tying castles to the medieval period; however, this does not necessarily reflect the terminology used in the medieval period. During the First Crusade (1096–1099), the Frankish armies encountered walled settlements and forts that they indiscriminately referred to as castles, but which would not be considered as such under the modern definition.
Castles served a range of purposes, the most important of which were military, administrative, and domestic. As well as defensive structures, castles were also offensive tools which could be used as a base of operations in enemy territory. Castles were established by Norman invaders of England for both defensive purposes and to pacify the country's inhabitants. As William the Conqueror advanced through England, he fortified key positions to secure the land he had taken. Between 1066 and 1087, he established 36 castles such as Warwick Castle, which he used to guard against rebellion in the English Midlands. 
Towards the end of the Middle Ages, castles tended to lose their military significance due to the advent of powerful cannons and permanent artillery fortifications; as a result, castles became more important as residences and statements of power. A castle could act as a stronghold and prison but was also a place where a knight or lord could entertain his peers. Over time the aesthetics of the design became more important, as the castle's appearance and size began to reflect the prestige and power of its occupant. Comfortable homes were often fashioned within their fortified walls. Although castles still provided protection from low levels of violence in later periods, eventually they were succeeded by country houses as high status residences.
Terminology.
"Castle" is sometimes used as a catch-all term for all kinds of fortifications, and as a result has been misapplied in the technical sense. An example of this is Maiden Castle which, despite the name, is an Iron Age hill fort, which had a very different origin and purpose.
Although "castle" has not become a generic term for a manor house (like château in French and Schloss in German) many manor houses contain "castle" in their name while having few if any of the architectural characteristics, usually as their owners liked to maintain a link to the past and felt the term "castle" was a masculine expression of their power. In scholarship the castle, as defined above, is generally accepted as a coherent concept, originating in Europe and later spreading to parts of the Middle East, where they were introduced by European Crusaders. This coherent group shared a common origin, dealt with a particular mode of warfare, and exchanged influences.
In different areas of the world, analogous structures shared features of fortification and other defining characteristics associated with the concept of a castle, though they originated in different periods and circumstances and experienced differing evolutions and influences. For example "shiro" in Japan, described as castles by historian Stephen Turnbull, underwent "a completely different developmental history, were built in a completely different way and were designed to withstand attacks of a completely different nature". While European castles built from the late 12th and early 13th century onwards were generally stone, "shiro" were predominantly timber buildings into the 16th century. 
By the time Japanese and European cultures met in the late 16th century, fortification in Europe had moved beyond castles and relied on innovations such as the Italian "trace italienne" and star forts. Forts in India present a similar case; when they were encountered by the British in the 17th century castles in Europe had generally fallen out of use militarily. Like "shiro", the Indian forts, "durga" or "durg" in Sanskrit, shared features with castles in Europe such as acting as a domicile for a lord as well as being fortifications. They too developed differently from the structures known as castles that had their origins in Europe.
Common features.
Motte.
A motte was an earthen mound with a flat top. It was often artificial, although sometimes it incorporated a pre-existing feature of the landscape. The excavation of earth to make the mound left a ditch around the motte, called a moat (which could be either wet or dry). "Motte" and "moat" derive from the same Old French word, indicating that the features were originally associated and depended on each other for their construction. Although the motte is commonly associated with the bailey to form a motte-and-bailey castle, this was not always the case and there are instances where a motte existed on its own.
"Motte" refers to the mound alone, but it was often surmounted by a fortified structure, such as a keep, and the flat top would be surrounded by a palisade. It was common for the motte to be reached over a flying bridge (a bridge over the ditch from the counterscarp of the ditch to the edge of the top of the mound), as shown in the Bayeux Tapestry's depiction of Château de Dinan. Sometimes a motte covered an older castle or hall, whose rooms became underground storage areas and prisons beneath a new keep.
Bailey and enceinte.
A bailey, also called a ward, was a fortified enclosure. It was a common feature of castles, and most had at least one. The keep on top of the motte was the domicile of the lord in charge of the castle and a bastion of last defence, while the bailey was the home of the rest of the lord's household and gave them protection. The barracks for the garrison, stables, workshops, and storage facilities were often found in the bailey. Water was supplied by a well or cistern. Over time the focus of high status accommodation shifted from the keep to the bailey; this resulted in the creation of another bailey that separated the high status buildings – such as the lord's chambers and the chapel – from the everyday structures such as the workshops and barracks. 
From the late 12th century there was a trend for knights to move out of the small houses they had previously occupied within the bailey to live in fortified houses in the countryside. Although often associated with the motte-and-bailey type of castle, baileys could also be found as independent defensive structures. These simple fortifications were called ringworks. The enceinte was the castle's main defensive enclosure, and the terms "bailey" and "enceinte" are linked. A castle could have several baileys but only one enceinte. Castles with no keep, which relied on their outer defences for protection, are sometimes called enceinte castles; these were the earliest form of castles, before the keep was introduced in the 10th century.
Keep.
A keep was a great tower and usually the most strongly defended point of a castle before the introduction of concentric defence. "Keep" was not a term used in the medieval period – the term was applied from the 16th century onwards – instead "donjon" was used to refer to great towers, or "turris" in Latin. In motte-and-bailey castles, the keep was on top of the motte. "Dungeon" is a corrupted form of "donjon" and means a dark, unwelcoming prison. Although often the strongest part of a castle and a last place of refuge if the outer defences fell, the keep was not left empty in case of attack but was used as a residence by the lord who owned the castle, or his guests or representatives. 
At first this was usual only in England, when after the Norman Conquest of 1066 the "conquerors lived for a long time in a constant state of alert"; elsewhere the lord's wife presided over a separate residence ("domus", "aula" or "mansio" in Latin) close to the keep, and the donjon was a barracks and headquarters. Gradually, the two functions merged into the same building, and the highest residential storeys had large windows; as a result for many structures, it is difficult to find an appropriate term. The massive internal spaces seen in many surviving donjons can be misleading; they would have been divided into several rooms by light partitions, as in a modern office building. Even in some large castles the great hall was separated only by a partition from the lord's "chamber", his bedroom and to some extent his office.
Curtain wall.
Curtain walls were defensive walls enclosing a bailey. They had to be high enough to make scaling the walls with ladders difficult and thick enough to withstand bombardment from siege engines which, from the 15th century onwards, included gunpowder artillery. A typical wall could be 3 m thick and 12 m tall, although sizes varied greatly between castles. To protect them from undermining, curtain walls were sometimes given a stone skirt around their bases. Walkways along the tops of the curtain walls allowed defenders to rain missiles on enemies below, and battlements gave them further protection. Curtain walls were studded with towers to allow enfilading fire along the wall. Arrowslits in the walls did not become common in Europe until the 13th century, for fear that they might compromise the wall's strength.
Gatehouse.
The entrance was often the weakest part in a circuit of defences. To overcome this, the gatehouse was developed, allowing those inside the castle to control the flow of traffic. In earth and timber castles, the gateway was usually the first feature to be rebuilt in stone. The front of the gateway was a blind spot and to overcome this, projecting towers were added on each side of the gate in a style similar to that developed by the Romans. The gatehouse contained a series of defences to make a direct assault more difficult than battering down a simple gate. Typically, there were one or more portcullises – a wooden grille reinforced with metal to block a passage – and arrowslits to allow defenders to harry the enemy. The passage through the gatehouse was lengthened to increase the amount of time an assailant had to spend under fire in a confined space and unable to retaliate. 
It is a popular myth that so-called murder-holes – openings in the ceiling of the gateway passage – were used to pour boiling oil or molten lead on attackers; the price of oil and lead and the distance of the gatehouse from fires meant that this was impractical. They were most likely used to drop objects on attackers, or to allow water to be poured on fires to extinguish them. Provision was made in the upper storey of the gatehouse for accommodation so the gate was never left undefended, although this arrangement later evolved to become more comfortable at the expense of defence.
During the 13th and 14th centuries the barbican was developed. This consisted of a rampart, ditch, and possibly a tower, in front of the gatehouse which could be used to further protect the entrance. The purpose of a barbican was not just to provide another line of defence but also to dictate the only approach to the gate.
Moat.
A moat was a defensive ditch with steep sides, and could be either dry or filled with water. Its purpose was twofold; to stop devices such as siege towers from reaching the curtain wall and to prevent the walls from being undermined. Water moats were found in low-lying areas and were usually crossed by a drawbridge, although these were often replaced by stone bridges. Fortified islands could be added to the moat, adding another layer of defence. Water defences, such as moats or natural lakes, had the benefit of dictating the enemy's approach to the castle. The site of the 13th-century Caerphilly Castle in Wales covers over 30 acre and the water defences, created by flooding the valley to the south of the castle, are some of the largest in Western Europe.
Other features.
Battlements were most often found surmounting curtain walls and the tops of gatehouses, and comprised several elements: crenellations, hoardings, machicolations, and loopholes. Crenellation is the collective name for alternating crenels and merlons: gaps and solid blocks on top of a wall. Hoardings were wooden constructs that projected beyond the wall, allowing defenders to shoot at, or drop objects on, attackers at the base of the wall without having to lean perilously over the crenellations, thereby exposing themselves to retaliatory fire. Machicolations were stone projections on top of a wall with openings that allowed objects to be dropped on an enemy at the base of the wall in a similar fashion to hoardings. 
Arrowslits, also commonly called loopholes, were narrow vertical openings in defensive walls which allowed arrows or crossbow bolts to be fired on attackers. The narrow slits were intended to protect the defender by providing a very small target, but the size of the opening could also impede the defender if it was too small. A smaller horizontal opening could be added to give an archer a better view for aiming. Sometimes a sally port was included; this could allow the garrison to leave the castle and engage besieging forces. It was usual for the latrines to empty down the external walls of a castle and into the surrounding ditch.
History.
Antecedents.
According to historian Charles Coulson the accumulation of wealth and resources, such as food, led to the need for defensive structures. The earliest fortifications originated in the Fertile Crescent, the Indus Valley, Egypt, and China where settlements were protected by large walls. Northern Europe was slower than the East to develop defensive structures and it was not until the Bronze Age that hill forts developed and began to spread across Europe. In the medieval period castles were influenced by earlier forms of elite architecture, contributing to regional variations. Importantly, while castles had military aspects, they contained a recognisable household structure within their walls, reflecting the multi-functional use of these buildings.
Origins (9th and 10th centuries).
The subject of the emergence of castles is a complex matter which has led to considerable debate. Discussions have typically attributed the rise of the castle to a reaction to attacks by Magyars, Muslims, and Vikings and a need for private defence. The breakdown of the Carolingian Empire led to the privatisation of government, and local lords assumed responsibility for the economy and justice. However, while castles proliferated in the 9th and 10th centuries the link between periods of insecurity and building fortifications is not always straightforward. Some high concentrations of castles occur in secure places, while some border regions had relatively few castles.
It is likely that the castle evolved from the practice of fortifying a lordly home. The greatest threat to a lord's home or hall was fire as it was usually a wooden structure. To protect against this, and keep other threats at bay, there were several courses of action available: create encircling earthworks to keep an enemy at a distance; build the hall in stone; or raise it up on an artificial mound, known as a motte, to present an obstacle to attackers. While the concept of ditches, ramparts, and stone walls as defensive measures is ancient, raising a motte is a medieval innovation.
A bank and ditch enclosure was a simple form of defence, and when found without an associated motte is called a ringwork; when the site was in use for a prolonged period, it was sometimes replaced by a more complex structure or enhanced by the addition of a stone curtain wall. Building the hall in stone did not necessarily make it immune to fire as it still had windows and a wooden door. This led to the elevation of windows to the first floor – to make it harder to throw objects in – and to change the entrance from ground floor to first floor. These features are seen in many surviving castle keeps, which were the more sophisticated version of halls. Castles were not just defensive sites but also enhanced a lord's control over his lands. They allowed the garrison to control the surrounding area, and formed a centre of administration, providing the lord with a place to hold court.
Building a castle sometimes required the permission of the king or other high authority. In 864 the King of West Francia, Charles the Bald, prohibited the construction of "castella" without his permission and ordered them all to be destroyed. This is perhaps the earliest reference to castles, though military historian R. Allen Brown points out that the word "castella" may have applied to any fortification at the time.
In some countries the monarch had little control over lords, or required the construction of new castles to aid in securing the land so was unconcerned about granting permission – as was the case in England in the aftermath of the Norman Conquest and the Holy Land during the Crusades. Switzerland is an extreme case of there being no state control over who built castles, and as a result there were 4,000 in the country. There are very few castles dated with certainty from the mid-9th century. Converted into a donjon around 950, Château de Doué-la-Fontaine in France is the oldest standing castle in Europe.
11th century.
From 1000 onwards, references to castles in texts such as charters increased greatly. Historians have interpreted this as evidence of a sudden increase in the number of castles in Europe around this time; this has been supported by archaeological investigation which has dated the construction of castle sites through the examination of ceramics. The increase in Italy began in the 950s, with numbers of castles increasing by a factor of three to five every 50 years, whereas in other parts of Europe such as France and Spain the growth was slower. In 950 Provence was home to 12 castles, by 1000 this figure had risen to 30, and by 1030 it was over 100. Although the increase was slower in Spain, the 1020s saw a particular growth in the number of castles in the region, particularly in contested border areas between Christian and Muslim. 
Despite the common period in which castles rose to prominence in Europe, their form and design varied from region to region. In the early 11th century, the motte and keep – an artificial mound surmounted by a palisade and tower – was the most common form of castle in Europe, everywhere except Scandinavia. While Britain, France, and Italy shared a tradition of timber construction that was continued in castle architecture, Spain more commonly used stone or mud-brick as the main building material. 
The Muslim invasion of the Iberian Peninsula in the 8th century introduced a style of building developed in North Africa reliant on "tapial", pebbles in cement, where timber was in short supply. Although stone construction would later become common elsewhere, from the 11th century onwards it was the primary building material for Christian castles in Spain, while at the same time timber was still the dominant building material in north-west Europe.
Historians have interpreted the widespread presence of castles across Europe in the 11th and 12th centuries as evidence that warfare was common, and usually between local lords. Castles were introduced into England shortly before the Norman Conquest in 1066. Before the 12th century, castles were as uncommon in Denmark as they had been in England before the Norman Conquest. The introduction of castles to Denmark was a reaction to attacks from Wendish pirates, and they were usually intended as coastal defences. The motte and bailey remained the dominant form of castle in England, Wales, and Ireland well into the 12th century. At the same time, castle architecture in mainland Europe became more sophisticated. 
The donjon was at the centre of this change in castle architecture in the 12th century. Central towers proliferated, and typically had a square plan, with walls 3 to thick. Their decoration emulated Romanesque architecture, and sometimes incorporated double windows similar to those found in church bell towers. Donjons, which were the residence of the lord of the castle, evolved to become more spacious. The design emphasis of donjons changed to reflect a shift from functional to decorative requirements, imposing a symbol of lordly power upon the landscape. This sometimes led to compromising defence for the sake of display.
Innovation and scientific design (12th century).
Until the 12th century, stone-built and earth and timber castles were contemporary, but by the late 12th century the number of castles being built went into decline. This has been partly attributed to the higher cost of stone-built fortifications, and the obsolescence of timber and earthwork sites, which meant it was preferable to build in more durable stone. Although superseded by their stone successors, timber and earthwork castles were by no means useless. This is evidenced by the continual maintenance of timber castles over long periods, sometimes several centuries; Owain Glyndŵr's 11th-century timber castle at Sycharth was still in use by the start of the 15th century, its structure having been maintained for four centuries.
At the same time there was a change in castle architecture. Until the late 12th century castles generally had few towers; a gateway with few defensive features such as arrowslits or a portcullis; a great keep or donjon, usually square and without arrowslits; and the shape would have been dictated by the lay of the land (the result was often irregular or curvilinear structures). The design of castles was not uniform, but these were features that could be found in a typical castle in the mid-12th century. By the end of the 12th century or the early 13th century, a newly constructed castle could be expected to be polygonal in shape, with towers at the corners to provide enfilading fire for the walls. The towers would have protruded from the walls and featured arrowslits on each level to allow archers to target anyone nearing or at the curtain wall. 
These later castles did not always have a keep, but this may have been because the more complex design of the castle as a whole drove up costs and the keep was sacrificed to save money. The larger towers provided space for habitation to make up for the loss of the donjon. Where keeps did exist, they were no longer square but polygonal or cylindrical. Gateways were more strongly defended, with the entrance to the castle usually between two half-round towers which were connected by a passage above the gateway – although there was great variety in the styles of gateway and entrances – and one or more portcullis. 
A peculiar feature of Muslim castles in the Iberian Peninsula was the use of detached towers, called Albarrana towers, around the perimeter as can be seen at the Alcazaba of Badajoz. Probably developed in the 12th century, the towers provided flanking fire. They were connected to the castle by removable wooden bridges, so if the towers were captured the rest of the castle was not accessible.
When seeking to explain this change in the complexity and style of castles, antiquarians found their answer in the Crusades. It seemed that the Crusaders had learned much about fortification from their conflicts with the Saracens and exposure to Byzantine architecture. There were legends such as that of Lalys – an architect from Palestine who reputedly went to Wales after the Crusades and greatly enhanced the castles in the south of the country – and it was assumed that great architects such as James of Saint George originated in the East. In the mid-20th century this view was cast into doubt. Legends were discredited, and in the case of James of Saint George it was proven that he came from Saint-Georges-d'Espéranche, in France. If the innovations in fortification had derived from the East, it would have been expected for their influence to be seen from 1100 onwards, immediately after the Christians were victorious in the First Crusade (1096–1099), rather than nearly 100 years later. Remains of Roman structures in Western Europe were still standing in many places, some of which had flanking round-towers and entrances between two flanking towers. 
The castle builders of Western Europe were aware of and influenced by Roman design; late Roman coastal forts on the English "Saxon Shore" were reused and in Spain the wall around the city of Ávila imitated Roman architecture when it was built in 1091. Historian Smail in "Crusading warfare" argued that the case for the influence of Eastern fortification on the West has been overstated, and that Crusaders of the 12th century in fact learned very little about scientific design from Byzantine and Saracen defences. A well-sited castle that made use of natural defences and had strong ditches and walls had no need for a scientific design. An example of this approach is Kerak. Although there were no scientific elements to its design, it was almost impregnable, and in 1187 Saladin chose to lay siege to the castle and starve out its garrison rather than risk an assault.
After the First Crusade, Crusaders who did not return to their homes in Europe helped found the Crusader states of the principality of Antioch, the County of Edessa, the Kingdom of Jerusalem, and the County of Tripoli. The castles they founded to secure their acquisitions were designed mostly by Syrian master-masons. Their design was very similar to that of a Roman fort or Byzantine "tetrapyrgia" which were square in plan and had square towers at each corner that did not project much beyond the curtain wall. The keep of these Crusader castles would have had a square plan and generally be undecorated. 
While castles were used to hold a site and control movement of armies, in the Holy Land some key strategic positions were left unfortified. Castle architecture in the East became more complex around the late 12th and early 13th centuries after the stalemate of the Third Crusade (1189–1192). Both Christians and Muslims created fortifications, and the character of each was different. Saphadin, the 13th-century ruler of the Saracens, created structures with large rectangular towers that influenced Muslim architecture and were copied again and again, however they had little influence on Crusader castles.
13th to 15th centuries.
In the early 13th century, Crusader castles were mostly built by Military Orders including the Knights Hospitaller, Knights Templar, and Teutonic Knights. The orders were responsible for the foundation of sites such as Krak des Chevaliers, Margat, and Belvoir. Design varied not just between orders, but between individual castles, though it was common for those founded in this period to have concentric defences. 
The concept, which originated in castles such as Krak des Chevaliers, was to remove the reliance on a central strongpoint and to emphasise the defence of the curtain walls. There would be multiple rings of defensive walls, one inside the other, with the inner ring rising above the outer so that its field of fire was not completely obscured. If assailants made it past the first line of defence they would be caught in the killing ground between the inner and outer walls and have to assault the second wall. 
Concentric castles were widely copied across Europe, for instance when Edward I of England – who had himself been on Crusade – built castles in Wales in the late 13th century, four of the eight he founded had a concentric design. Not all the features of the Crusader castles from the 13th century were emulated in Europe. For instance, it was common in Crusader castles to have the main gate in the side of a tower and for there to be two turns in the passageway, lengthening the time it took for someone to reach the outer enclosure. It is rare for this bent entrance to be found in Europe.
One of the effects of the Livonian Crusade in the Baltic was the introduction of stone and brick fortifications. Although there were hundreds of wooden castles in Prussia and Livonia, the use of bricks and mortar was unknown in the region before the Crusaders. Until the 13th century and start of the 14th centuries, their design was heterogeneous, however this period saw the emergence of a standard plan in the region: a square plan, with four wings around a central courtyard. It was common for castles in the East to have arrowslits in the curtain wall at multiple levels; contemporary builders in Europe were wary of this as they believed it weakened the wall. Arrowslits did not compromise the wall's strength, but it was not until Edward I's programme of castle building that they were widely adopted in Europe. 
The Crusades also led to the introduction of machicolations into Western architecture. Until the 13th century, the tops of towers had been surrounded by wooden galleries, allowing defenders to drop objects on assailants below. Although machicolations performed the same purpose as the wooden galleries, they were probably an Eastern invention rather than an evolution of the wooden form. Machicolations were used in the East long before the arrival of the Crusaders, and perhaps as early as the first half of the 8th century in Syria. 
The greatest period of castle building in Spain was in the 11th to 13th centuries, and they were most commonly found in the disputed borders between Christian and Muslim lands. Conflict and interaction between the two groups led to an exchange of architectural ideas, and Spanish Christians adopted the use of detached towers. The Spanish Reconquista, driving the Muslims out of the Iberian Peninsula, was complete in 1492.
Although France has been described as "the heartland of medieval architecture", the English were at the forefront of castle architecture in the 12th century. French historian François Gebelin wrote: "The great revival in military architecture was led, as one would naturally expect, by the powerful kings and princes of the time; by the sons of William the Conqueror and their descendants, the Plantagenets, when they became dukes of Normandy. These were the men who built all the most typical twelfth-century fortified castles remaining to-day". Despite this, by the beginning of the 15th century, the rate of castle construction in England and Wales went into decline. The new castles were generally of a lighter build than earlier structures and presented few innovations, although strong sites were still created such as that of Raglan in Wales. At the same time, French castle architecture came to the fore and led the way in the field of medieval fortifications. Across Europe – particularly the Baltic, Germany, and Scotland – castles were built well into the 16th century.
Advent of gunpowder.
Artillery powered by gunpowder was introduced to Europe in the 1320s and spread quickly. Handguns, which were initially unpredictable and inaccurate weapons, were not recorded until the 1380s. Castles were adapted to allow small artillery pieces – averaging between 19.6 and – to fire from towers. These guns were too heavy for a man to carry and fire, but if he supported the butt end and rested the muzzle on the edge of the gun port he could fire the weapon. The gun ports developed in this period show a unique feature, that of a horizontal timber across the opening. A hook on the end of the gun could be latched over the timber so the gunner did not have to take the full recoil of the weapon. This adaptation is found across Europe, and although the timber rarely survives, there is an intact example at Castle Doornenburg in the Netherlands. Gunports were keyhole shaped, with a circular hole at the bottom for the weapon and a narrow slit on top to allow the gunner to aim. 
This form is very common in castles adapted for guns, found in Egypt, Italy, Scotland, and Spain, and elsewhere in between. Other types of port, though less common, were horizontal slits – allowing only lateral movement – and large square openings, which allowed greater movement. The use of guns for defence gave rise to artillery castles, such as that of Château de Ham in France. Defences against guns were not developed until a later stage. Ham is an example of the trend for new castles to dispense with earlier features such as machicolations, tall towers, and crenellations.
Bigger guns were developed, and in the 15th century became an alternative to siege engines such as the trebuchet. The benefits of large guns over trebuchets – the most effective siege engine of the Middle Ages before the advent of gunpowder – were those of a greater range and power. In an effort to make them more effective, guns were made ever bigger, although this hampered their ability to reach remote castles. By the 1450s guns were the preferred siege weapon, and their effectiveness was demonstrated by Mehmed II at the Fall of Constantinople. 
The response towards more effective cannons was to build thicker walls and to prefer round towers, as the curving sides were more likely to deflect a shot than a flat surface. While this sufficed for new castles, pre-existing structures had to find a way to cope with being battered by cannon. An earthen bank could be piled behind a castle's curtain wall to absorb some of the shock of impact. 
Often, castles constructed before the age of gunpowder were incapable of using guns as their wall-walks were too narrow. A solution to this was to pull down the top of a tower and to fill the lower part with the rubble to provide a surface for the guns to fire from. Lowering the defences in this way had the effect of making them easier to scale with ladders. A more popular alternative defence, which avoided damaging the castle, was to establish bulwarks beyond the castle's defences. These could be built from earth or stone and were used to mount weapons.
Bastions and star forts (16th century).
Around 1500, the innovation of the angled bastion was developed in Italy. With developments such as these, Italy pioneered permanent artillery fortifications, which took over from the defensive role of castles. From this evolved star forts, also known as "trace italienne". The elite responsible for castle construction had to choose between the new type that could withstand cannon fire and the earlier, more elaborate style. The first was ugly and uncomfortable and the latter was less secure, although it did offer greater aesthetic appeal and value as a status symbol. The second choice proved to be more popular as it became apparent that there was little point in trying to make the site genuinely defensible in the face of cannon. For a variety of reasons, not least of which is that many castles have no recorded history, there is no firm number of castles built in the medieval period. However, it has been estimated that between 75,000 and 100,000 were built in western Europe; of these around 1,700 were in England and Wales and around 14,000 in German-speaking areas.
Some true castles were built in the Americas by the Spanish and French colonies. The first stage of Spanish fort construction has been termed the "castle period", which lasted from 1492 until the end of the 16th century. Starting with Fortaleza Ozama, "these castles were essentially European medieval castles transposed to America". Among other defensive structures (including forts and citadels), castles were also built in New France towards the end of the 17th century. In Montreal the artillery was not as developed as on the battle-fields of Europe, some of the region's outlying forts were built like the fortified manor houses of France. Fort Longueuil, built from 1695–1698 by a baronial family, has been described as "the most medieval-looking fort built in Canada". The manor house and stables were within a fortified bailey, with a tall round turret in each corner. The "most substantial castle-like fort" near Montréal was Fort Senneville, built in 1692 with square towers connected by thick stone walls, as well as a fortified windmill. Stone forts such as these served as defensive residences, as well as imposing structures to prevent Iroquois incursions.
Although castle construction faded towards the end of the 16th century, castles did not necessarily all fall out of use. Some retained a role in local administration and became law courts, while others are still handed down in aristocratic families as hereditary seats. A particularly famous example of this is Windsor Castle in England which was founded in the 11th century and is home to the monarch of the United Kingdom. In other cases they still had a role in defence. Tower houses, which are closely related to castles and include pele towers, were defended towers that were permanent residences built in the 14th to 17th centuries. Especially common in Ireland and Scotland, they could be up to five storeys high and succeeded common enclosure castles and were built by a greater social range of people. While unlikely to provide as much protection as a more complex castle, they offered security against raiders and other small threats.
Later use and revival castles.
According to archaeologists Oliver Creighton and Robert Higham, "the great country houses of the seventeenth to twentieth centuries were, in a social sense, the castles of their day". Though there was a trend for the elite to move from castles into country houses in the 17th century, castles were not completely useless. In later conflicts, such as the English Civil War (1641–1651), many castles were refortified, although subsequently slighted to prevent them from being used again.
Revival or mock castles became popular as a manifestation of a Romantic interest in the Middle Ages and chivalry, and as part of the broader Gothic Revival in architecture. Examples of these castles include Chapultepec in Mexico, Neuschwanstein in Germany, and Edwin Lutyens' Castle Drogo (1911–1930) – the last flicker of this movement in the British Isles. While churches and cathedrals in a Gothic style could faithfully imitate medieval examples, new country houses built in a "castle style" differed internally to their medieval predecessors. This was because to be faithful to medieval design would have left the houses cold and dark by contemporary standards.
Artificial ruins, built to resemble remnants of historic edifices, were also a hallmark of the period. They were usually built as centre pieces in aristocratic planned landscapes. Follies were similar, although they differed from artificial ruins in that they were not part of a planned landscape, but rather seemed to have no reason for being built. Both drew on elements of castle architecture such as castellation and towers, but served no military purpose and were solely for display.
Construction.
Once the site of a castle had been selected – whether a strategic position or one intended to dominate the landscape as a mark of power – the building material had to be selected. An earth and timber castle was cheaper and easier to erect than one built from stone. The costs involved in construction are not well-recorded, and most surviving records relate to royal castles. A castle with earthen ramparts, a motte, and timber defences and buildings could have been constructed by an unskilled workforce. The source of man-power was probably from the local lordship, and the tenants would already have the necessary skills of felling trees, digging, and working timber necessary for an earth and timber castle. Possibly coerced into working for their lord, the construction of an earth and timber castle would not have been a drain on a client's funds. In terms of time, it has been estimated that an average sized motte – 5 m high and 15 m wide at the summit – would have taken 50 people about 40 working days. An exceptionally expensive motte and bailey was that of Clones in Ireland, built in 1211 for £20. The high cost, relative to other castles of its type, was because labourers had to be imported.
The cost of building a castle varied according to factors such as their complexity and transport costs for material. It is certain that stone castles cost a great deal more than those built from earth and timber. Even a very small tower, such as Peveril Castle, would have cost around £200. In the middle were castles such as Orford, which was built in the late 12th century for £1,400, and at the upper end were those such as Dover, which cost about £7,000 between 1181 and 1191. Spending on the scale of the vast castles such as Château Gaillard (an estimated £15,000 to £20,000 between 1196 and 1198) was easily supported by The Crown, but for lords of smaller areas, castle building was a very serious and costly undertaking. It was usual for a stone castle to take the best part of a decade to finish. The cost of a large castle built over this time (anywhere from £1,000 to £10,000) would take the income from several manors, severely impacting a lord's finances. Costs in the late 13th century were of a similar order, with castles such as Beaumaris and Rhuddlan costing £14,500 and £9,000 respectively. Edward I's campaign of castle-building in Wales cost £80,000 between 1277 and 1304, and £95,000 between 1277 and 1329. Renowned designer Master James of Saint George, responsible for the construction of Beaumaris, explained the cost:
In case you should wonder where so much money could go in a week, we would have you know that we have needed – and shall continue to need 400 masons, both cutters and layers, together with 2,000 less skilled workmen, 100 carts, 60 wagons and 30 boats bringing stone and sea coal; 200 quarrymen; 30 smiths; and carpenters for putting in the joists and floor boards and other necessary jobs. All this takes no account of the garrison ... nor of purchases of material. Of which there will have to be a great quantity ... The men's pay has been and still is very much in arrears, and we are having the greatest difficulty in keeping them because they have simply nothing to live on.—
Not only were stone castles expensive to build in the first place, but their maintenance was a constant drain. They contained a lot of timber, which was often unseasoned and as a result needed careful upkeep. For example, it is documented that in the late 12th century repairs at castles such as Exeter and Gloucester cost between £20 and £50 annually.
Medieval machines and inventions, such as the treadwheel crane, became indispensable during construction, and techniques of building wooden scaffolding were improved upon from Antiquity. When building in stone a prominent concern of medieval builders was to have quarries close at hand. There are examples of some castles where stone was quarried on site, such as Chinon, Château de Coucy and Château Gaillard. When it was built in 992 in France the stone tower at Château de Langeais was 16 m high, 17.5 m wide, and 10 m long with walls averaging 1.5 m. The walls contain 1200 m3 of stone and have a total surface (both inside and out) of 1600 m2. The tower is estimated to have taken 83,000 average working days to complete, most of which was unskilled labour.
Many countries had both timber and stone castles, however Denmark had few quarries, and as a result, most of its castles are earth and timber affairs, or later on built from brick. Brick-built structures were not necessarily weaker than their stone-built counterparts. Brick castles are less common in England than stone or earth and timber constructions, and often it was chosen for its aesthetic appeal or because it was fashionable, encouraged by the brick architecture of the Low Countries. For example, when Tattershall Castle was built between 1430 and 1450, there was plenty of stone available nearby, but the owner, Lord Cromwell, chose to use brick. About 700,000 bricks were used to build the castle, which has been described as "the finest piece of medieval brick-work in England". Most Spanish castles were built from stone, whereas castles in Eastern Europe were usually of timber construction.
The Castle of the Teutonic Order in Malbork, Poland, is an example of medieval fortresses and built in the typical style of northern German Brick Gothic. On its completion in 1406 it was the largest brick castle in the world.
Social centre.
Due to the lord's presence in a castle, it was a centre of administration from where he controlled his lands. He relied on the support of those below him, as without the support of his more powerful tenants a lord could expect his power to be undermined. Successful lords regularly held court with those immediately below them on the social scale, but absentees could expect to find their influence weakened. Larger lordships could be vast, and it would be impractical for a lord to visit all his properties regularly so deputies were appointed. This especially applied to royalty, who sometimes owned land in different countries. 
To allow the lord to concentrate on his duties regarding administration, he had a household of servants to take care of chores such as providing food. The household was run by a chamberlain, while a treasurer took care of the estate's written records. Royal households took essentially the same form as baronial households, although on a much larger scale and the positions were more prestigious. An important role of the household servants was the preparation of food; the castle kitchens would have been a busy place when the castle was occupied, called on to provide large meals. Without the presence of a lord's household, usually because he was staying elsewhere, a castle would have been a quiet place with few residents, focused on maintaining the castle.
As social centres castles were important places for display. Builders took the opportunity to draw on symbolism, through the use of motifs, to evoke a sense of chivalry that was aspired to in the Middle Ages amongst the elite. Later structures of the Romantic Revival would draw on elements of castle architecture such as battlements for the same purpose. Castles have been compared with cathedrals as objects of architectural pride, and some castles incorporated gardens as ornamental features. The right to crenellate, when granted by a monarch – though it was not always necessary – was important not just as it allowed a lord to defend his property but because crenellations and other accoutrements associated with castles were prestigious through their use by the elite. Licences to crenellate were also proof of a relationship with or favour from the monarch, who was the one responsible for granting permission.
Courtly love was the eroticisation of love between the nobility. Emphasis was placed on restraint between lovers. Though sometimes expressed through chivalric events such as tournaments, where knights would fight wearing a token from their lady, it could also be private and conducted in secret. The legend of Tristan and Iseult is one example of stories of courtly love told in the Middle Ages. It was an ideal of love between two people not married to each other, although the man might be married to someone else. It was not uncommon or ignoble for a lord to be adulterous – Henry I of England had over 20 bastards for instance – but for a lady to be promiscuous was seen as dishonourable.
The purpose of marriage between the medieval elites was to secure land. Girls were married in their teens, but boys did not marry until they came of age. There is a popular conception that women played a peripheral role in the medieval castle household, and that it was dominated by the lord himself. This derives from the image of the castle as a martial institution, but most castles in England, France, Ireland, and Scotland were never involved in conflicts or sieges, so the domestic life is a neglected facet. The lady was given a "marriage portion" of her husband's estates – usually about a third – which was hers for life, and her husband would inherit on her death. It was her duty to administer them directly, as the lord administered his own land. Despite generally being excluded from military service, a woman could be in charge of a castle, either on behalf of her husband or if she was widowed. Because of their influence within the medieval household, women influenced construction and design, sometimes through direct patronage; historian Charles Coulson emphasises the role of women in applying "a refined aristocratic taste" to castles due to their long term residence.
Location and landscape.
The positioning of castles was influenced by the available terrain. Whereas hill castles such as Marksburg were common in Germany, where 66 per cent of all known medieval were highland area while 34 per cent were on low-lying land, they formed a minority of sites in England. Because of the range of functions they had to fulfil, castles were built in a variety of locations. Multiple factors were considered when choosing a site, balancing between the need for a defendable position with other considerations such as proximity to resources. For instance many castles are located near Roman roads, which remained important transport routes in the Middle Ages, or could lead to the alteration or creation of new road systems in the area. Where available it was common to exploit pre-existing defences such as building with a Roman fort or the ramparts of an Iron Age hillfort. A prominent site that overlooked the surrounding area and offered some natural defences may also have been chosen because its visibility made it a symbol of power. Urban castles were particularly important in controlling centres of population and production, especially with an invading force, for instance in the aftermath of the Norman Conquest of England in the 11th century the majority of royal castles were built in or near towns.
As castles were not simply military buildings but centres of administration and symbols of power, they had a significant impact on the surrounding landscape. Placed by a frequently-used road or river, the toll castle ensured that a lord would get his due toll money from merchants. Rural castles were often associated with mills and field systems due to their role in managing the lord's estate, which gave them greater influence over resources. Others were adjacent to or in royal forests or deer parks and were important in their upkeep. Fish ponds were a luxury of the lordly elite, and many were found next to castles. Not only were they practical in that they ensured a water supply and fresh fish, but they were a status symbol as they were expensive to build and maintain.
Although sometimes the construction of a castle led to the destruction of a village, such as at Eaton Socon in England, it was more common for the villages nearby to have grown as a result of the presence of a castle. Sometimes planned towns or villages were created around a castle. The benefits of castle building on settlements was not confined to Europe. When the 13th-century Safad Castle was founded in Galilee in the Holy Land, the 260 villages benefitted from the inhabitants' newfound ability to move freely. When built, a castle could result in the restructuring of the local landscape, with roads moved for the convenience of the lord. Settlements could also grow naturally around a castle, rather than being planned, due to the benefits of proximity to an economic centre in a rural landscape and the safety given by the defences. Not all such settlements survived, as once the castle lost its importance – perhaps succeeded by a manor house as the centre of administration – the benefits of living next to a castle vanished and the settlement depopulated.
During and shortly after the Norman Conquest of England, castles were inserted into important pre-existing towns to control and subdue the populace. They were usually located near any existing town defences, such as Roman walls, although this sometimes resulted in the demolition of structures occupying the desired site. In Lincoln, 166 houses were destroyed to clear space for the castle, and in York agricultural land was flooded to create a moat for the castle. As the military importance of urban castles waned from their early origins, they became more important as centres of administration, and their financial and judicial roles. When the Normans invaded Ireland, Scotland, and Wales in the 11th and 12th centuries, settlement in those countries was predominantly non-urban, and the foundation of towns was often linked with the creation of a castle.
The location of castles in relation to high status features, such as fish ponds, was a statement of power and control of resources. Also often found near a castle, sometimes within its defences, was the parish church. This signified a close relationship between feudal lords and the Church, one of the most important institutions of medieval society. Even elements of castle architecture that have usually been interpreted as military could be used for display. The water features of Kenilworth Castle in England – comprising a moat and several satellite ponds – forced anyone approaching a water castle entrance to take a very indirect route, walking around the defences before the final approach towards the gateway. Another example is that of the 14th-century Bodiam Castle, also in England; although it appears to be a state of the art, advanced castle it is in a site of little strategic importance, and the moat was shallow and more likely intended to make the site appear impressive than as a defence against mining. The approach was long and took the viewer around the castle, ensuring they got a good look before entering. Moreover, the gunports were impractical and unlikely to have been effective.
The landscape around Leeds Castle in England has been managed since the 13th century. The castle overlooks artificial lakes and ponds and is within a medieval deer park.
Warfare.
As a static structure, castles could often be avoided. Their immediate area of influence was about 400 m and their weapons had a short range even early in the age of artillery. However, leaving an enemy behind would allow them to interfere with communications and make raids. Garrisons were expensive and as a result often small unless the castle was important. Cost also meant that in peace time garrisons were smaller, and small castles were manned by perhaps a couple of watchmen and gate-guards. Even in war, garrisons were not necessarily large as too many people in a defending force would strain supplies and impair the castle's ability to withstand a long siege. In 1403, a force of 37 archers successfully defended Caernarfon Castle against two assaults by Owain Glyndŵr's allies during a long siege, demonstrating that a small force could be effective. 
Early on, manning a castle was a feudal duty of vassals to their magnates, and magnates to their kings, however this was later replaced with paid forces. A garrison was usually commanded by a constable whose peace-time role would have been looking after the castle in the owner's absence. Under him would have been knights who by benefit of their military training would have acted as a type of officer class. Below them were archers and bowmen, whose role was to prevent the enemy reaching the walls as can be seen by the positioning of arrowslits.
If it was necessary to seize control of a castle an army could either launch an assault or lay siege. It was more efficient to starve the garrison out than to assault it, particularly for the most heavily defended sites. Without relief from an external source, the defenders would eventually submit. Sieges could last weeks, months, and in rare cases years if the supplies of food and water were plentiful. A long siege could slow down the army, allowing help to come or for the enemy to prepare a larger force for later. Such an approach was not confined to castles, but was also applied to the fortified towns of the day. On occasion, siege castles would be built to defend the besiegers from a sudden sally and would have been abandoned after the siege ended one way or another.
If forced to assault a castle, there were many options available to the attackers. For wooden structures, such as early motte-and-baileys, fire was a real threat and attempts would be made to set them alight as can be seen in the Bayeux Tapestry. Projectile weapons had been used since antiquity and the mangonel and petraria – from Roman and Eastern origins respectively – were the main two that were used into the Middle Ages. The trebuchet, which probably evolved from the petraria in the 13th century, was the most effective siege weapon before the development of cannons. These weapons were vulnerable to fire from the castle as they had a short range and were large machines. Conversely, weapons such as trebuchets could be fired from within the castle due to the high trajectory of its projectile, and would be protected from direct fire by the curtain walls. 
Ballistas or springalds were siege engines that worked on the same principles as crossbows. With their origins in Ancient Greece, tension was used to project a bolt or javelin. Missiles fired from these engines had a lower trajectory than trebuchets or mangonels and were more accurate. They were more commonly used against the garrison rather than the buildings of a castle. Eventually cannons developed to the point where they were more powerful and had a greater range than the trebuchet, and became the main weapon in siege warfare.
Walls could be undermined by a sap. A mine leading to the wall would be dug and once the target had been reached, the wooden supports preventing the tunnel from collapsing would be burned. It would cave in and bring down the structure above. Building a castle on a rock outcrop or surrounding it with a wide, deep moat helped prevent this. A counter-mine could be dug towards the besiegers' tunnel; assuming the two converged, this would result in underground hand-to-hand combat. Mining was so effective that during the siege of Margat in 1285 when the garrison were informed a sap was being dug they surrendered. Battering rams were also used, usually in the form of a tree trunk given an iron cap. They were used to force open the castle gates, although they were sometimes used against walls with less effect.
As an alternative to the time-consuming task of creating a breach, an escalade could be attempted to capture the walls with fighting along the walkways behind the battlements. In this instance, attackers would be vulnerable to arrowfire. A safer option for those assaulting a castle was to use a siege tower, sometimes called a belfry. Once ditches around a castle were partially filled in, these wooden, movable towers could be pushed against the curtain wall. As well as offering some protection for those inside, a siege tower could overlook the interior of a castle, giving bowmen an advantageous position from which to unleash missiles.
References.
</dl>
Further reading.
</dl>

</doc>
<doc id="49559" url="http://en.wikipedia.org/wiki?curid=49559" title="Yalu River">
Yalu River

The Yalu River, also called the Amnok River (]), is a river on the border between North Korea and China. Together with the Tumen River to its east, and a small portion of Paektu Mountain, the Yalu forms the border between North Korea and China and is notable as a site involved in military conflicts in the First Sino-Japanese War, the Russo-Japanese War and the Korean War.
Name.
There are two versions regarding the origin of the river name. One version is that the name derived from "Yalv ula" in Manchu language, one ethnic language in China. The Manchu word "Yalu" means "the boundary between two countries"; in Mandarin Chinese, "Yalu" phonetically approximates the original Manchu word "Yalu", but literally means "Duck Green", which was said to be the color of the river once before. The other version is that the river was named after the combination of its two upper branches, which was called "Ya"and "Lu" respectively.
Geography.
From 2,500 m above sea level on Baekdu Mountain (Changbai Mountain), on the North Korea-China border, the river flows south to Hyesan before sweeping 130 km northwest to Linjiang and then returning to a more southerly route for a further 300 km to empty into the Korea Bay between Dandong (China) and Sinuiju (North Korea). The bordering Chinese provinces are Jilin and Liaoning.
The river is 795 km (493 mi) long and receives the water from over 30,000 km² of land. The Yalu's most significant tributaries are the Changjin (장진강/長津江), the Hochon (허천강/虚川江), the Tokro (독로강/禿魯江) and the Ai (瑷河) rivers. The river is not easily navigable for most of its length.
The depth of the Yalu River varies from some of the more shallow parts on the eastern side in Hyesan (1 metre) to the deeper parts of the river near the Yellow Sea (2.5 metres). The estuary is the site of the Amrok River estuary Important Bird Area, identified as such by BirdLife International.
There are 205 islands on the Yalu. A 1962 border treaty between North Korea and China split the islands according to which ethnic group were living on each island. North Korea possesses 127 and China 78. Due to the division criteria, some islands such as Hwanggumpyong Island belong to North Korea but abut the Chinese side of the river.
History.
The river basin is the site where the ancient kingdom of Goguryeo rose to power. Many former fortresses are located along the river and the former capital of that kingdom was situated at what is now the medium-sized city of Ji'an, China along the Yalu, a site rich in Goguryeo era relics.
Wihwa Island on the river is historically famous as the place where General Yi Songgye, in 1392, decided to turn back his army southward to Kaesong in the first of a series of revolts that eventually led to the establishment of the Yi Dynasty.
The river has been the site of several battles because of its strategic location between Korea and China, including:
The Korean side of the river was heavily industrialized during Colonial Korea (1910–1945), and by 1945 almost 20% of Imperial Japan's total industrial output originated in Korea. During the Korean War the movement of United Nations troops approaching the river precipitated massive Chinese intervention from around Dandong. In the course of the conflict every bridge across the river except one was destroyed. The one remaining bridge was the Sino-Korea Friendship Bridge connecting Sinuiju, North Korea to Dandong, China. During the war the valley surrounding the western end of the river also became the focal point of a series of dogfights for air superiority over North Korea, earning the nickname "MiG Alley" in reference to the MiG-15 fighters flown by the combined North Korean, Chinese and Soviet forces.
It was the advance of UN forces during the Korean War toward the Yalu which prompted Chairman Mao Zedong to involve China in the war for fear of an American invasion, since toppling communism was one of America's stated goals and Douglas MacArthur had expressed his desire to expand the war into China.
The river has frequently been crossed by North Koreans fleeing to China since the early 1990s.
Economy.
The river is important for hydroelectric power, and one of the largest hydroelectric dams in Asia is in Sup'ung Dam, 106 m high and over 850 m long, located upstream from Sinuiju, North Korea. The dam has created an artificial lake over a portion of the river, called Sapung Lake. In addition the river is used for transportation, particularly of lumber from its forested banks. The river provides fish for the local population. Downstream of Sup'ung is the Taipingwan Dam. Upstream of Sup'ung is the Yunfeng Dam. Both dams produce hydroelectric power as well.

</doc>
<doc id="49562" url="http://en.wikipedia.org/wiki?curid=49562" title="Nicolae Ceaușescu">
Nicolae Ceaușescu

Nicolae Ceaușescu ( ; ]; 26 January 1918 – 25 December 1989) was a Romanian Communist politician. He was General Secretary of the Romanian Communist Party from 1965 to 1989, and as such was the country's second and last Communist leader. He was also the country's head of state from 1967 to 1989.
A member of the Romanian Communist youth movement, Ceaușescu rose up through the ranks of Gheorghe Gheorghiu-Dej's Socialist government and, upon the death of Gheorghiu-Dej in 1965, he succeeded to the leadership of Romania’s Communist Party as General Secretary.
After a brief period of relatively moderate rule, Ceaușescu's regime became increasingly brutal and repressive. By some accounts, his rule was the most rigidly Stalinist in the Soviet bloc. He maintained controls over speech and the media that were very strict even by Soviet-bloc standards, and internal dissent was not tolerated. His secret police, the Securitate, was one of the most ubiquitous and brutal secret police forces in the world. In 1982, with the goal of paying off Romania's large foreign debt, Ceaușescu ordered the export of much of the country’s agricultural and industrial production. The resulting extreme shortages of food, fuel, energy, medicines, and other basic necessities drastically lowered living standards and intensified unrest. Ceaușescu's regime was also marked by an extensive and ubiquitous cult of personality, nationalism, a continuing deterioration in foreign relations even with the Soviet Union, and nepotism.
Ceaușescu’s regime collapsed after he ordered his security forces to fire on anti-government demonstrators in the city of Timișoara on 17 December 1989. The demonstrations spread to Bucharest and became known as the Romanian Revolution, which was the only violent removal of a Communist government in the course of the revolutions of 1989. Ceaușescu and his wife, Elena, fled the capital in a helicopter but were captured by the armed forces. On 25 December the couple were hastily tried and convicted by a special military tribunal on charges of genocide and sabotage of the Romanian economy in an approximate one hour long court session. Ceaușescu and his wife were then shot by a firing squad.
Early life and career.
Ceaușescu was born in the village of Scornicești, Olt County, on 26 January 1918 being one of the ten children of a poor peasant family (see Ceaușescu family). His father, Andruță Ceaușescu, owned 3 ha of agricultural land, a few sheep, and he also supplemented his large family's income through tailoring. Nicolae studied at the village school until at the age of 11, when he ran away from his abusive, alcoholic father to Bucharest. He initially lived with his sister, Niculina Rusescu, and then became an apprentice shoemaker.
He worked in the workshop of Alexandru Săndulescu, a shoemaker who was an active member in the then-illegal Communist Party. Ceaușescu was soon involved in the Communist Party activities (becoming a member in early 1932), but, as a teenager, he was given only small tasks. He was first arrested in 1933, at the age of 15 for street fighting during a strike and again, in 1934, first for collecting signatures on a petition protesting the trial of railway workers and twice more for other similar activities. By the mid-1930s, he had been in missions in Bucharest, Craiova, Câmpulung, and Râmnicu Vâlcea, being arrested several times.
The profile file from the secret police, Siguranța Statului, named him "a dangerous Communist agitator" and "distributor of Communist and antifascist propaganda materials". For these charges he was convicted on 6 June 1936 by the Brașov Tribunal to 2 years in prison, an additional 6 months for contempt of court, and one year of forced residence in Scornicești. He spent most of his sentence in Doftana Prison. While out of jail in 1940, he met Elena Petrescu, whom he married in 1946 and who would play an increasing role in his political life over the years.
Soon after being freed, he was arrested again and sentenced for "conspiracy against social order", spending the time during the war in prisons and internment camps: Jilava (1940), Caransebeș (1942), Văcărești (1943), and Târgu Jiu (1943). In 1943, he was transferred to Târgu Jiu internment camp where he shared a cell with Gheorghe Gheorghiu-Dej, becoming his protégé. After World War II, when Romania was beginning to fall under Soviet influence, he served as secretary of the Union of Communist Youth (1944–1945).
After the Communists seized power in Romania in 1947, he headed the ministry of agriculture, then served as deputy minister of the armed forces under Gheorghe Gheorghiu-Dej, becoming a major-general. In 1952, Gheorghiu-Dej brought him onto the Central Committee months after the party's "Muscovite faction" led by Ana Pauker had been purged. In 1954, he became a full member of the Politburo and eventually rose to occupy the second-highest position in the party hierarchy.
Leadership of Romania.
Ceaușescu was not the obvious successor to Gheorghiu-Dej when he died on 19 March 1965, despite his closeness to the longtime leader, but amid widespread infighting among older and more connected officials the Politburo turned to Ceaușescu as a compromise candidate. He was elected general secretary on 22 March 1965, three days after Gheorghiu-Dej's death. One of his first acts was to change the name of the party from the Romanian Workers' Party back to the Communist Party of Romania, and declare the country a socialist republic rather than a people's republic. In 1967, he consolidated his power by becoming president of the State Council, making him de jure head of state.
Initially, Ceaușescu became a popular figure in Romania and also in the West, because of his independent foreign policy, challenging the authority of the Soviet Union. In the 1960s, he eased press censorship and ended Romania's active participation in the Warsaw Pact (though Romania formally remained a member). He not only refused to take part in the 1968 invasion of Czechoslovakia by Warsaw Pact forces, but actively and openly condemned that action in his 21 August 1968 speech. He even traveled to Prague a week before the invasion to offer moral support to his Czechoslovak counterpart, Alexander Dubček. Although the Soviet Union largely tolerated Ceaușescu's recalcitrance, his seeming independence from Moscow earned Romania a maverick status within the Eastern Bloc.
During the following years Ceaușescu pursued an open policy towards the United States and Western Europe. Romania was the first Warsaw Pact country to recognize West Germany, the first to join the International Monetary Fund, and the first to receive a US President, Richard Nixon. In 1971, Romania became a member of the General Agreement on Tariffs and Trade (GATT). Romania and Yugoslavia were also the only Eastern European countries that entered into trade agreements with the European Economic Community before the fall of the Eastern Bloc.
A series of official visits to Western countries (including the US, France, the United Kingdom, and Spain) helped Ceaușescu to present himself as a reforming Communist, pursuing an independent foreign policy within the Soviet Bloc. He also became eager to be seen as an enlightened international statesman, able to mediate in international conflicts, and to gain international respect for Romania. Ceaușescu negotiated in international affairs, such as the opening of US relations with China in 1969 and the visit of Egyptian president Anwar Sadat to Israel in 1977. Also Romania was the only country in the world to maintain normal diplomatic relations with both Israel and the PLO. In 1980, Romania participated in the 1980 Moscow Olympics with its other Soviet bloc allies but in 1984 was one of the few Communist countries to participate in the 1984 Summer Olympics when most of the East bloc's nations boycotted this event.
The 1966 decree.
In 1966, Ceaușescu, in an attempt to boost the country's population, made abortion illegal, and introduced Decree 770 to reverse the very low birth rate and fertility rate. Mothers of at least five children would be entitled to significant benefits, while mothers of at least ten children were declared "heroine mothers" by the Romanian state. Few women ever sought this status; instead, the average Romanian family during the time had two to three children (see Demographics of Romania).
The government also targeted rising divorce rates and made divorce much more difficult—it was decreed that a marriage could be dissolved only in exceptional cases. By the late 1960s, the population began to swell. In turn, a new problem was created by child abandonment, which swelled the orphanage population (see Cighid). Transfusions of untested blood led to Romania accounting for many of Europe's pediatric HIV/AIDS cases at the turn of the 21st century despite having a population that only makes up around 3% of Europe's total population.
Speech of 21 August 1968.
Ceaușescu's speech of 21 August 1968 represented the apogee of Ceaușescu's regime. It marked the highest point in Ceaușescu's popularity, when he openly condemned the Warsaw Pact invasion of Czechoslovakia.
July Theses.
Ceaușescu visited China, North Korea, the Mongolian People's Republic and North Vietnam in 1971. He took great interest in the idea of total national transformation as embodied in the programs of North Korea's "Juche" and China's Cultural Revolution. He was also inspired by the personality cults of North Korea's Kim Il-sung and China's Mao Zedong. Shortly after returning home, he began to emulate North Korea's system. North Korean books on "Juche" were translated into Romanian and widely distributed inside the country.
On 6 July 1971, he delivered a speech before the Executive Committee of the PCR. This quasi-Maoist speech, which came to be known as the July Theses, contained seventeen proposals. Among these were: continuous growth in the "leading role" of the Party; improvement of Party education and of mass political action; youth participation on large construction projects as part of their "patriotic work"; an intensification of political-ideological education in schools and universities, as well as in children's, youth and student organizations; and an expansion of political propaganda, orienting radio and television shows to this end, as well as publishing houses, theatres and cinemas, opera, ballet, artists' unions, promoting a "militant, revolutionary" character in artistic productions. The liberalisation of 1965 was condemned and an index of banned books and authors was re-established.
The Theses heralded the beginning of a "mini cultural revolution" in Romania, launching a Neo-Stalinist offensive against cultural autonomy, reaffirming an ideological basis for literature that, in theory, the Party had hardly abandoned. Although presented in terms of "Socialist Humanism", the Theses in fact marked a return to the strict guidelines of Socialist Realism, and attacks on non-compliant intellectuals. Strict ideological conformity in the humanities and social sciences was demanded. Competence and aesthetics were to be replaced by ideology; professionals were to be replaced by agitators; and culture was once again to become an instrument for political-ideological propaganda and hardline measures.
President of Romania.
In 1974, Ceaușescu converted his post of president of the State Council to a full-fledged executive presidency. He was first elected to this post in 1974, and would be reelected every five years until 1989.
Although Ceaușescu had been head of state since 1965, he had merely been first among equals on the State Council, with his real power coming from his status as party leader. The new post, however, made him the nation's top decision-maker both in name and in fact. He was empowered to carry out those functions of the State Council that didn't require plenums. He also appointed and dismissed the president of the Supreme Court and the prosecutor general whenever the legislature wasn't in session. In practice, from 1974 onward Ceaușescu frequently ruled by decree. For all intents and purposes, Ceaușescu now held all governing power in the nation; virtually all party and state institutions were subordinated to his will.
He continued to follow an independent policy in foreign relations—for example, in 1984, Romania was one of few communist states (notably including the People's Republic of China, and Yugoslavia) to take part in the American-organized 1984 Summer Olympics in Los Angeles.
Also, the Socialist Republic of Romania was the first of the Eastern bloc nations to have official relations with the Western bloc and the European Community: an agreement including Romania in the Community's Generalised System of Preferences was signed in 1974 and an Agreement on Industrial Products was signed in 1980. On 4 April 1975, Ceaușescu visited Japan and met with Emperor Hirohito.
Pacepa defection.
In 1978, Ion Mihai Pacepa, a senior member of the Romanian political police (Securitate, State Security), defected to the United States. A 3-star general, he was the highest ranking defector from the Eastern Bloc during the Cold War. His defection was a powerful blow against the regime, forcing Ceaușescu to overhaul the architecture of the Security. Pacepa's 1986 book, "Red Horizons: Chronicles of a Communist Spy Chief" (ISBN 0-89526-570-2), claims to expose details of Ceaușescu's regime, such as massive spying on American industry and elaborate efforts to rally Western political support.
Foreign debt.
Ceaușescu's political independence from the Soviet Union and his protest against the invasion of Czechoslovakia in 1968 drew the interest of Western powers, whose governments briefly believed that he was an anti-Soviet maverick and hoped to create a schism in the Warsaw Pact by funding him. Ceaușescu did not realise that the funding was not always favorable. Ceaușescu was able to borrow heavily (more than $13 billion) from the West to finance economic development programs, but these loans ultimately devastated the country's finances. He also secured a deal for cheap oil from Iran, but that deal fell through after the Shah was overthrown.
In an attempt to correct this, Ceaușescu decided to repay Romania's foreign debts. He organised a referendum and managed to change the constitution, adding a clause that barred Romania from taking foreign loans in the future. According to official results, the referendum yielded a nearly unanimous "yes" vote.
In the 1980s, Ceaușescu ordered the export of much of the country's agricultural and industrial production in order to repay its debts. The resulting domestic shortages made the everyday life of Romanians a fight for survival as food rationing was introduced and heating, gas and electricity blackouts became the rule. During the 1980s, there was a steady decrease in the Romanian population's standard of living, especially in the availability and quality of food and general goods in shops. During this time, all regional radio stations were closed, and television was limited to a single channel broadcasting for only two hours a day.
The debt was fully paid in the summer of 1989, shortly before Ceaușescu was overthrown. However, heavy exports continued until the revolution in December.
1984 failed coup d'état attempt.
A tentative coup d'état planned in October 1984 failed when the military unit assigned to carry out the plan was sent to harvest maize instead.
Revolution.
In November 1989, the XIVth Congress of the Romanian Communist Party (PCR) saw Ceaușescu, then aged 71, re-elected for another five years as leader of the PCR. During the Congress, Ceaușescu made a speech denouncing the anti-Communist revolutions happening throughout the rest of Eastern Europe. The following month, Ceaușescu's regime itself collapsed after a series of violent events in Timișoara and Bucharest in December 1989.
Timișoara.
Demonstrations in the city of Timișoara were triggered by the government-sponsored attempt to evict László Tőkés, an ethnic Hungarian pastor, accused by the government of inciting ethnic hatred. Members of his ethnic Hungarian congregation surrounded his apartment in a show of support.
Romanian students spontaneously joined the demonstration, which soon lost nearly all connection to its initial cause and became a more general anti-government demonstration. Regular military forces, police and Security fired on demonstrators on 17 December 1989, killing and injuring men, women and children.
On 18 December 1989, Ceaușescu departed for a state visit to Iran, leaving the duty of crushing the Timișoara revolt to his subordinates and his wife. Upon his return to Romania on the evening of 20 December, the situation became even more tense, and he gave a televised speech from the TV studio inside Central Committee Building (CC Building), in which he spoke about the events at Timișoara in terms of an "interference of foreign forces in Romania's internal affairs" and an "external aggression on Romania's sovereignty".
The country, which had little or no information of the Timișoara events from the national media, learned about the Timișoara revolt from western radio stations such as Voice of America and Radio Free Europe, and by word of mouth. On the next day, 21 December, Ceaușescu staged a mass meeting in Bucharest. Official media presented it as a "spontaneous movement of support for Ceaușescu", emulating the 1968 meeting in which Ceaușescu had spoken against the invasion of Czechoslovakia by Warsaw Pact forces.
Overthrow.
Speech on 21 December.
The mass meeting of 21 December, held in what is now Revolution Square, began like many of Ceaușescu's speeches over the years. Ceaușescu spoke of the achievements of the "Socialist revolution" and Romanian "multi-laterally developed Socialist society." He also blamed the Timișoara riots on "fascist agitators who want to destroy socialism."
However, Ceaușescu had misjudged the crowd's mood. Roughly eight minutes into his speech, several people began jeering, booing and whistling at him, others began chanting "Timișoara!". He tried to silence them by raising his right hand and calling for the crowd's attention before order was temporarily restored, then proceeded to announce social benefit reforms that included raising of the national minimum wage by 200 lei per month. Images of Ceaușescu's facial expression as the crowd began to boo and heckle him were among the most widely broadcast of the collapse of Communism in Eastern Europe, and perhaps the most intimate and stark depiction of impending doom that the media has seen in any context.
Failing to control the crowds, the Ceaușescus finally took cover inside the building that housed the Central Committee of the Romanian Communist Party, where they remained until the next day. The rest of the day saw an open revolt of Bucharest's population, which had assembled in University Square and confronted the police and army at barricades. The rioters were no match for the military apparatus concentrated in Bucharest, which cleared the streets by midnight and arrested hundreds of people in the process.
Flight on 22 December.
By the morning of 22 December, the rebellion had already spread to all major cities across the country. The suspicious death of Vasile Milea, the defense minister (later confirmed as a suicide), was announced by the media. Immediately thereafter, Ceaușescu presided over the CPEx (Political Executive Committee) meeting and assumed the leadership of the army.
Believing that Milea had been murdered, rank-and-file soldiers switched sides to the revolution almost "en masse". The commanders wrote off Ceaușescu as a lost cause and made no effort to keep their men loyal to the regime. Ceaușescu made a last desperate attempt to address the crowd gathered in front of the Central Committee building, but the people in the square began throwing stones and other projectiles at him, forcing him to take refuge in the building once more. One group of protesters forced open the doors of the building, by now left unprotected. They managed to overpower Ceaușescu's bodyguards and rushed through his office and onto the balcony. Although they did not know it, they were only a few meters from Ceaușescu, who was trapped in an elevator. He, Elena and four others managed to get to the roof and escaped by helicopter, only seconds ahead of a group of demonstrators who had followed them there. The PCR disappeared soon afterward; unlike its kindred parties in the former Soviet bloc, it has never been revived.
During the course of the revolution, the western press published estimates of the number of people killed by Securitate forces in attempting to support Ceaușescu and quell the rebellion. The count increased rapidly until an estimated 64,000 fatalities were widely reported across front pages. The Hungarian military attaché expressed doubt regarding these figures, pointing out the unfeasible logistics of killing such a large number of people in such a short period of time. After Ceaușescu's death, hospitals across the country reported a death toll of less than 1,000, and probably much lower than that.
Death.
Ceaușescu and his wife Elena fled the capital with Emil Bobu and Manea Mănescu and headed, by helicopter, for Ceaușescu's Snagov residence, whence they fled again, this time for Târgoviște. Near Târgoviște they abandoned the helicopter, having been ordered to land by the army, which by that time had restricted flying in Romania's airspace. The Ceaușescus were held by the police while the policemen listened to the radio. They were eventually turned over to the army.
On Christmas Day, 25 December, in a small room the Ceaușescus were tried before a kangaroo court convened on orders of the National Salvation Front, Romania's provisional government. They faced charges including illegal gathering of wealth and genocide. Ceaușescu repeatedly denied the court's authority to try him, and asserted he was still legally president of Romania. At the end of the quick show trial the Ceaușescus were found guilty and sentenced to death. A soldier standing guard in the proceedings was ordered to take the Ceaușescus out back one by one and shoot them, but the Ceaușescus demanded to die together. The soldiers agreed to this and began to tie their hands behind their back which the Ceaușescus protested again but were powerless to prevent.
The Ceaușescus were executed by a gathering of soldiers: Captain Ionel Boeru, Sergeant-Major Georghin Octavian and Dorin-Marian Cîrlan, while reportedly hundreds of others also volunteered. The firing squad began shooting as soon as the two were in position against a wall. A TV crew who were to film the execution only managed to catch the end of it as the Ceaușescus lay on the ground shrouded by dust kicked up by the bullets striking the wall and ground. Before his sentence was carried out, Nicolae Ceaușescu sang "The Internationale" while being led up against the wall. After the shooting, the bodies were covered with canvas.
The hasty show trial and the images of the dead Ceaușescus were videotaped and the footage promptly released in numerous western countries two days after the execution. Later that day, it was also shown on Romanian television.
The manner in which the trial was conducted was widely criticised inside and outside Romania. However, Ion Iliescu, Romania's provisional president, said in 2009 that the trial was "quite shameful, but necessary" in order to end the state of near-anarchy that had gripped the country in the three days since the Ceaușescus fled Bucharest. Similarly, Victor Stănculescu, who had been defense minister before going over to the revolution, said in 2009 that the alternative would have been seeing the Ceaușescus lynched on the streets of Bucharest.
The Ceaușescus were the last people to be executed in Romania before the abolition of capital punishment on 7 January 1990.
Their graves are located in Ghencea cemetery in Bucharest. They are buried on opposite sides of a path. The graves are unassuming, but they tend to be covered in flowers and symbols of the regime. In April 2007, their son, Valentin Ceaușescu, lost an appeal for an investigation into whether the graves were genuine. Upon his death in 1996, the younger son, Nicu, was buried nearby in the same cemetery. According to "Jurnalul Național", requests were made by the Ceaușescus' daughter Zoia and by supporters of their political views to move their remains to mausoleums or to purpose-built churches. These demands were denied by the government. On 21 July 2010, forensic scientists exhumed the bodies of the Ceaușescus to perform DNA tests. It was determined that they were, indeed, the remains of the Ceaușescus. His family organized a funeral service for the couple, and they were reburied together at Ghencea, under a modest tombstone.
Personality cult and authoritarianism.
Ceaușescu created a pervasive personality cult, giving himself such titles as "Conducător" ("Leader") and "Geniul din Carpați" ("The Genius of the Carpathians"), with inspiration from Proletarian Culture (Proletkult). After his election as President of Romania, he even had a king-like sceptre made for himself.
The most important day of the year during Ceaușescu's rule was his birthday, 26 January — a day which saw Romanian media saturated with praise for him. According to historian Victor Sebestyen, it was one of the few days of the year when the average Romanian put on a happy face, since appearing miserable on this day was too risky to contemplate.
Such excesses prompted painter Salvador Dalí to send a congratulatory telegram to the "Conducător", in which he sarcastically congratulated Ceaușescu on his "introducing the presidential sceptre". The Communist Party daily "Scînteia" published the message, unaware that it was a work of satire. To lessen the chance of further treason after Pacepa's defection, Ceaușescu also invested his wife Elena and other members of his family with important positions in the government. This led Romanians to joke that Ceaușescu was creating "socialism in one family".
Not surprisingly, Ceaușescu was greatly concerned about his public image. For years, nearly all official photographs of him showed him in his late 40s. Romanian state television was under strict orders to portray him in the best possible light. Additionally, producers had to take great care to make sure that Ceaușescu's height (he was only 1.68 m tall) was never emphasized on screen. Consequences for breaking these rules were severe; one producer showed footage of Ceaușescu blinking and stuttering, and was banned for three months.
Statesmanship.
Ceaușescu's Romania was the only Eastern Bloc country that retained diplomatic relations with Israel and did not sever diplomatic relations after Israel's pre-emptive strike against Egypt at the start of the Six-Day War in 1967. Ceaușescu made efforts to act as a mediator between the PLO and Israel. Similarly, Romania was the only Soviet bloc country to attend the 1984 Summer Olympics in Los Angeles.
He organised a successful referendum for reducing the size of the Romanian Army by 5% and held large rallies for peace.
He was a close ally and personal friend of dictator President Mobutu Sese Seko of Zaïre. Relations were in fact not just state-to-state, but party-to-party between the MPR and the Romanian Communist Party. Many believe that Ceaușescu's death played a role in influencing Mobutu to "democratise" Zaïre in 1990.
France granted Ceaușescu the Legion of Honour and in 1978 he became a Knight Grand Cross of the Order of the Bath (GCB, stripped in 1989) in the UK. Elena Ceaușescu was arranged to be "elected" to membership of a Science Academy in the USA; all of these, and more, were arranged by the Ceaușescus as a propaganda ploy through the consular cultural attachés of Romanian embassies in the countries involved.
Ceaușescu's Romania was the only Warsaw Pact country that did not sever diplomatic relations with Chile after Augusto Pinochet's coup.
In August 1976, Nicolae Ceaușescu was the first high-level Romanian visitor to Bessarabia since World War II. In December 1976, at one of his meetings in Bucharest, Ivan Bodiul said that "the good relationship was initiated by Ceaușescu's visit to Soviet Moldova".
Legacy.
Nicolae and Elena Ceaușescu had three children: Valentin Ceaușescu (born 1948), a nuclear physicist; Nicu Ceaușescu (1951–1996), also a physicist; and a daughter, Zoia Ceaușescu (1949–2006), who was a mathematician. After the death of his parents, Nicu Ceaușescu ordered the construction of an Orthodox church, the walls of which are decorated with portraits of his parents.
Praising the crimes of so-called totalitarian regimes and denigrating their victims is forbidden by law in Romania; this includes the Ceaușescu regime. Dinel Staicu was fined 25,000 lei (approx. 9,000 United States dollars) for praising Ceaușescu and displaying his pictures on his private television channel ("3TV Oltenia"). Nevertheless, according to opinion polls held in 2010, 41% of Romanians would vote for Ceaușescu and 63% think that their lives were better before 1989. In 2014, the percentage of those who would vote for Ceaușescu reached 66%.
"Ceaușism".
While the term "Ceaușism" became widely used inside Romania, usually as a pejorative, it never achieved status in academia. This can be explained by the largely crude and syncretic character of the dogma. Ceaușescu attempted to include his views in mainstream Marxist theory, to which he added his belief in a "multilaterally developed Socialist society" as a necessary stage between the Leninist concepts of Socialist and Communist societies (a critical view reveals that the main reason for the interval is the disappearance of the State and Party structures in Communism). A Romanian Encyclopedic Dictionary entry in 1978 underlines the concept as "a new, superior, stage in the Socialist development of Romania [...] begun by the 1971–1975 Five-Year Plan, prolonged over several [succeeding and projected] Five-Year Plans".
Ceaușism's main trait was a form of Romanian nationalism, one which arguably propelled Ceaușescu to power in 1965, and probably accounted for the Party leadership gathered around Ion Gheorghe Maurer choosing him over the more orthodox Gheorghe Apostol. Although he had previously been a careful supporter of the official lines, Ceaușescu came to embody Romanian society's wish for independence after what many considered years of Soviet directives and purges, during and after the SovRom fiasco. He carried this nationalist option inside the Party, manipulating it against the nominated successor Apostol. This nationalist policy had more timid precedents: for example, the Gheorghiu-Dej regime had overseen the withdrawal of the Red Army in 1958.
It had also engineered the publishing of several works that subverted the Russian and Soviet image, such as the final volumes of the official "History of Romania", no longer glossing over traditional points of tension with Russia and the Soviet Union (even alluding to an unlawful Soviet presence in Bessarabia). In the final years of Gheorghiu-Dej's rule more problems were openly discussed, with the publication of a collection of Karl Marx texts that dealt with Romanian topics, showing Marx's previously censored, politically uncomfortable views of Russia.
Ceaușescu was prepared to take a more decisive step in questioning Soviet policies. In the early years of his rule, he generally relaxed political pressures inside Romanian society, which led to the late 1960s and early 1970s being the most liberal decade in Socialist Romania. Gaining the public's confidence, Ceaușescu took a clear stand against the 1968 crushing of the Prague Spring by Leonid Brezhnev. After a visit from Charles de Gaulle earlier in the same year (during which the French President gave recognition to the incipient maverick), Ceaușescu's public speech in August deeply impressed the population, not only through its themes, but also because, uniquely, it was unscripted. He immediately attracted Western sympathies and backing, which lasted well beyond the 'liberal' phase of his regime; at the same time, the period brought forward the threat of armed Soviet invasion: significantly, many young men inside Romania joined the "Patriotic Guards" created on the spur of the moment, in order to meet the perceived threat. President Richard Nixon was invited to Bucharest in 1969, which was the first visit of a United States president to a Socialist country after the start of the Cold War.
Alexander Dubček's version of "Socialism with a human face" was never suited to Romanian Communist goals. Ceaușescu found himself briefly aligned with Dubček's Czechoslovakia and Josip Broz Tito's Yugoslavia. The latter friendship was to last until Tito's death in 1980, with Ceaușescu adapting the Titoist doctrine of "independent Socialist development" to suit his own objectives. Romania proclaimed itself a "Socialist" (in place of "People's") Republic to show that it was fulfilling Marxist goals without Moscow's overseeing.
The system's nationalist traits grew and progressively blended with "Juche" and Maoist ideals. In 1971, the Party, which had already been completely purged of internal opposition (with the possible exception of Gheorghe Gaston Marin), approved the "July Theses", expressing Ceaușescu's disdain of Western models as a whole, and the reevaluation of the recent liberalisation as "bourgeois". The 1974 11th Congress tightened the Party's grip on Romanian culture, guiding it towards Ceaușescu's nationalist principles:. Notably, it demanded that Romanian historians refer to Dacians as having "an unorganised State", part of a political continuum that culminated in the Socialist Republic. The regime continued its cultural dialogue with ancient forms, with Ceaușescu connecting his cult of personality to figures such as Mircea cel Bătrân (whom he styled "Mircea the Great") and Mihai Viteazul. It also started adding Dacian or Roman versions to the names of cities and towns ("Drobeta" to Turnu Severin, "Napoca" to Cluj). Although Ceaușescu maintained an independent, "national Communist" course, his absolute control over the country led many non-Romanian observers to describe his regime as one of the closest things to an old-style Stalinist regime. The last edition of the Country Study on Romania, for instance, referred to the PCR's "Stalinist repression of individual liberties."
A new generation of committed supporters on the outside confirmed the regime's character. Ceaușescu probably never emphasized that his policies constituted a paradigm for theorists of National Bolshevism such as Jean-François Thiriart, but there was a publicised connection between him and Iosif Constantin Drăgan, an Iron Guardist Romanian-Italian émigré millionaire (Drăgan was already committed to a Dacian Protochronism that largely echoed the official cultural policy).
Nicolae Ceaușescu had a major influence on modern-day Romanian populist rhetoric. In his final years, he had begun to rehabilitate the image of pro-Nazi dictator Ion Antonescu. Although Antonescu's image was never a fully official myth in Ceaușescu's time, today's politicians such as Corneliu Vadim Tudor have coupled the images of the two leaders into their versions of a national Pantheon. The conflict with Hungary over the treatment of the Magyar minority in Romania had several unusual aspects: not only was it a vitriolic argument between two officially Socialist states (as Hungary had not yet officially embarked on the course to a free market economy), it also marked the moment when Hungary, a state behind the Iron Curtain, appealed to the Organisation for Security and Co-operation in Europe for sanctions to be taken against Romania. This meant that the later 1980s were marked by a pronounced anti-Hungarian discourse, which owed more to nationalist tradition than to Marxism, and the ultimate isolation of Romania on the world stage.
The strong opposition of his regime to all forms of "perestroika" and "glasnost" placed Ceaușescu at odds with Mikhail Gorbachev. He was very displeased when other Warsaw Pact countries decided to try their own versions of Gorbachev's reforms. In particular, he was incensed when Poland's leaders opted for a power-sharing arrangement with the Solidarity trade union. He even went as far as to call for a Warsaw Pact invasion of Poland—a significant reversal, considering how violently he opposed the invasion of Czechoslovakia 20 years earlier. For his part, Gorbachev made no secret of his distaste for Ceaușescu, whom he called "the Romanian führer." At a meeting between the two, Gorbachev upbraided Ceaușescu for his inflexible attitude. "You are running a dictatorship here," the Soviet leader warned.
In November 1989, at the XIVth and last congress of the PCR, Ceaușescu condemned the Molotov–Ribbentrop Pact and asked for the annulment of its consequences. In effect, this amounted to a demand for the return of Bessarabia (most of which was then a Soviet republic and since 1991 has been independent Moldova) and northern Bukovina, both of which had been occupied by the Soviet Union in 1940 and again at the end of World War II.
Honours and awards.
Ceaușescu received the Danish Order of the Elephant, but this award was revoked on 23 December 1989 by the queen of Denmark, Margrethe II.
Ceaușescu was likewise stripped of his honorary GCB (Knight Grand Cross of the Most Honourable Order of the Bath) status by Queen Elizabeth II of the United Kingdom on the day before his execution. Queen Elizabeth II also returned the Romanian order Ceaușescu had bestowed upon her.
On his 70th birthday in 1988, Ceaușescu was decorated with the Karl-Marx-Orden by then Socialist Unity Party of Germany (SED) chief Erich Honecker; through this he was honoured for his rejection of Mikhail Gorbachev's reforms.
All titles and decorations were revoked by the provisional government on December 26, 1989.
Several foreign decorations were revoked at the time of the collapse of the Ceaușescu regime.
Honorary degrees from the University of Bucharest (1973), Lebanese University (1974), University of Buenos Aires (1974), Autonomous University of Yucatan (1975), University of Nice Sophia Antipolis (1975), University of Liberia (1988) and the Democratic People's Republic of Korea (1988).

</doc>
<doc id="49565" url="http://en.wikipedia.org/wiki?curid=49565" title="President pro tempore of the United States Senate">
President pro tempore of the United States Senate

The President pro tempore ( or ), also president pro tem, is the second-highest-ranking official of the United States Senate. According to the United States Constitution, the Vice President of the United States is the President of the Senate, despite not being a senator, and the Senate must choose a president "pro tempore" to act in his absence. Since 1890, the most senior senator in the majority party has generally been chosen to be president pro tempore; this tradition has been observed without interruption since 1949.
During the Vice President's absence, the president pro tempore is empowered to preside over Senate sessions. In practice, neither the Vice President nor the President pro tempore usually presides; instead, the duty of presiding officer is rotated among junior senators of the majority party to give them experience in parliamentary procedure.
The president pro tempore is third in the line of succession to the presidency, after the Vice President and the Speaker of the House of Representatives and ahead of the Secretary of State.
Orrin Hatch, a Republican and senior senator from Utah, is the president pro tempore of the Senate, having assumed office in January 2015.
Power and responsibilities.
The office of president pro tempore is created by Article I, Section 3 of the Constitution:
The Senate shall choose their other Officers, and also a President pro tempore, in the absence of the Vice President, or when he shall exercise the Office of President of the United States.
Although the position is in some ways analogous to the Speaker of the House of Representatives, the powers of the president pro tempore are far more limited. In the Senate, most power rests with party leaders and individual senators, but as the chamber's presiding officer, the president pro tempore is authorized to perform certain duties in the absence of the Vice President, including ruling on points of order. Additionally, under the 25th Amendment to the Constitution, the president pro tempore and the speaker are the two authorities to whom declarations must be transmitted that the president is unable to perform the duties of the office, or is able to resume doing so. The president pro tempore is third in the line of presidential succession, following the vice president and the speaker. Additional duties include appointment of various congressional officers, certain commissions, advisory boards, and committees and joint supervision of the congressional page school. The president pro tempore is the designated legal recipient of various reports to the Senate, including War Powers Act reports under which he or she, jointly with the speaker, may have the president call Congress back into session. The officeholder is an ex officio member of various boards and commissions. With the secretary and sergeant at arms, the president pro tempore maintains order in Senate portions of the Capitol and Senate buildings.
History.
The office of president pro tempore was established by the Constitution of the United States in 1789. The first president pro tempore, John Langdon, was elected on April 6 the same year. Originally, the president pro tempore was appointed on an intermittent basis when the vice president was not present to preside over the Senate. Until the 1960s, it was common practice for the vice president to preside over daily Senate sessions, so the president pro tempore rarely presided unless the vice presidency became vacant.
Until 1891, the president pro tempore only served until the return of the vice president to the chair or the adjournment of a session of Congress. Between 1792 and 1886, the president pro tempore was second in the line of presidential succession following the vice president and preceding the speaker.
When President Andrew Johnson, who had no vice president, was impeached and tried in 1868, Senate President pro tempore Benjamin Franklin Wade was next in line to the presidency. Wade's radicalism is thought by many historians to be a major reason why the Senate, which did not want to see Wade in the White House, acquitted Johnson. The president pro tempore and the speaker were removed from the line of succession in 1886, but were restored in 1947. This time however the president pro tempore followed the speaker.
Following the resignation (for health reasons) of President pro tempore William P. Frye, a Senate divided among progressive Republicans, conservative Republicans, and Democrats reached a compromise by which each of their candidates would rotate holding the office from 1911 to 1913 (see below, #62nd Congress).
Only three former presidents pro tempore ever became vice president: John Tyler, William R. King and Charles Curtis. Tyler is also the only one to have become president, when he succeeded William Henry Harrison in 1841.
Related officials.
Acting president pro tempore.
While the president pro tempore does have other official duties, the holders of the office have, like the vice president, over time ceased presiding over the Senate on a daily basis, owing to the mundane and ceremonial nature of the position. Furthermore, as the president pro tempore is now usually the most senior senator of the majority party, he or she most likely also chairs a major Senate committee and has other significant demands on his or her time. Therefore, the president pro tempore has less time now than in the past to preside daily over the Senate. Instead, junior senators from the majority party are designated acting president pro tempore to preside over the Senate. This allows junior senators to learn proper parliamentary procedure.
Permanent Acting President pro tempore.
In June 1963, because of the illness of president pro tempore Carl Hayden, Senator Lee Metcalf was designated permanent acting president pro tempore. No term was imposed on this designation, so Metcalf retained it until he died in office in 1978.
Deputy President pro tempore.
The ceremonial post of Deputy President pro tempore was created for Hubert Humphrey, a former vice president, in 1977 following his losing bid to become the Senate majority leader. The Senate resolution creating the position stated that any former president or former vice president serving in the Senate would be entitled to this position, though none has served since Humphrey's death in 1978, and former vice president Walter Mondale, who sought his former Senate seat in Minnesota in 2002, is the only one to have tried. Andrew Johnson is the only former president to have subsequently served in the Senate.
George J. Mitchell was elected deputy president pro tempore in 1987, because of the illness of president pro tempore John C. Stennis, similar to Metcalf's earlier designation as Permanent Acting President pro tempore. The office has remained vacant since 1988, and no senator other than Humphrey and Mitchell has held it since its creation.
The post is largely honorary and ceremonial, but comes with a salary increase. By statute, the compensation granted to the position holder equals the rate of annual compensation paid to the president pro tempore, majority leader, and minority leader. ("See"  .)
President pro tempore emeritus.
Since 2001, the honorary title of president pro tempore emeritus has been given to a Senator of the minority party who has previously served as president pro tempore. The position has been held by Strom Thurmond (R-South Carolina) (2001-2003), Robert Byrd (D-West Virginia) (2003-2007), Ted Stevens (R-Alaska) (2007-2009) and Patrick Leahy (D-Vermont) (2015–Present).
The position was created for Thurmond when the Democratic Party regained a majority in the Senate in June 2001. With the change in party control, Democrat Robert Byrd of West Virginia replaced Thurmond as president pro tempore, reclaiming a position he had previously held from 1989 to 1995 and briefly in January 2001. Thurmond's retirement from the Senate on January 3, 2003, coincided with a change from Democratic to Republican control, making Stevens president pro tempore and Byrd the second president pro tempore emeritus. Byrd returned as president pro tempore, and Stevens became the third president pro tempore emeritus, when the Democrats gained control of the Senate in 2007. While a president pro tempore emeritus has no official duties, he is entitled to an increase in staff and advises party leaders on the functions of the Senate.
The office's accompanying budget increase was removed toward the end of the 113th Congress, shortly before Patrick Leahy was to become the first holder of the title in six years. Quoted in "CQ Roll Call", Leahy commented, "They didn't keep their commitment. They want to treat us differently than we treated them, and so they've got that right. It seems kind of petty, but it really doesn't matter to me. I've got plenty of funding, plenty of good staff."
Salary.
The salary of the president pro tempore for 2012 was $193,400, equal to that of the majority leaders and minority leaders of both houses of Congress. If there is a vacancy in the office of vice president, then the salary would be the same as that of the vice president, $230,700.
List of Presidents pro tempore of the United States Senate.
Before 1890, the Senate elected a president pro tempore only for the period when the vice president would be absent.
List of Presidents pro tempore per state.
No President pro tempore has served from: California, Colorado, Florida, Idaho, Minnesota, Montana, New Mexico, Oklahoma, Oregon, South Dakota, or Wyoming.
Note.
Arthur Vandenberg (serving in 1947–1949) was the last president pro tempore not to be the senior member of the majority party, aside from the single day accorded Milton Young (serving in 1980), who was the retiring senior member of the party who had been elected to a majority in the incoming congress.

</doc>
<doc id="49568" url="http://en.wikipedia.org/wiki?curid=49568" title="Montgomery">
Montgomery

Montgomery or Montgomerie may refer to:

</doc>
<doc id="49569" url="http://en.wikipedia.org/wiki?curid=49569" title="Bayes' theorem">
Bayes' theorem

In probability theory and statistics, Bayes' theorem (alternatively Bayes' law or Bayes' rule) relates current probability to prior probability. It is important in the mathematical manipulation of conditional probabilities. 
When applied, the probabilities involved in Bayes' theorem may have different interpretations. In one of these interpretations, the theorem is used directly as part of a particular approach to statistical inference. In particular, with the Bayesian interpretation of probability, the theorem expresses how a subjective degree of belief should rationally change to account for evidence: this is Bayesian inference, which is fundamental to Bayesian statistics. However, Bayes' theorem has applications in a wide range of calculations involving probabilities, not just in Bayesian inference.
Bayes' theorem is named after Rev. Thomas Bayes (; 1701–1761), who first showed how to use new evidence to update beliefs. It was further developed by Pierre-Simon Laplace, who first published the modern formulation in his 1812 "Théorie analytique des probabilités". Sir Harold Jeffreys put Bayes' algorithm and Laplace's formulation on an axiomatic basis. Jeffreys wrote that Bayes' theorem "is to the theory of probability what Pythagoras's theorem is to geometry".
Statement of theorem.
Bayes' theorem is stated mathematically as the following equation:
where "A" and "B" are events.
Introductory example.
The entire output of a factory is produced on three machines. The three machines account for 20%, 30%, and 50% of the output, respectively. The fraction of defective items produced is this: for the first machine, 5%; for the second machine, 3%; for the third machine, 1%. If an item is chosen at random from the total output and is found to be defective, what is the probability that it was produced by the third machine?
A solution is as follows. Let "Ai" denote the event that a randomly chosen item was made by the "i"th machine (for "i" = 1,2,3). Let "B" denote the event that a randomly chosen item is defective. Then, we are given the following information:
If the item was made by machine "A"1, then the probability that it is defective is 0.05; that is, "P"("B" | "A"1) = 0.05. Overall, we have
To answer the original question, we first find "P"("B"). That can be done in the following way:
Hence 2.4% of the total output of the factory is defective.
We are given that "B" has occurred, and we want to calculate the conditional
probability of "A"3. By Bayes' theorem,
Given that the item is defective, the probability that it was made by the third
machine is only 5/24. Although machine 3 produces half of the total output, it
produces a much smaller fraction of the defective items. Hence the knowledge
that the item selected was defective enables us to replace the prior probability
"P"("A"3) = 1/2 by the smaller posterior probability "P"("A"3 | "B") = 5/24.
Interpretations.
The interpretation of Bayes' theorem depends on the interpretation of probability ascribed to the terms. The two main interpretations are described below.
Bayesian interpretation.
In the Bayesian (or epistemological) interpretation, probability measures a "degree of belief". Bayes' theorem then links the degree of belief in a proposition before and after accounting for evidence. For example, suppose it is believed with 50% certainty that a coin is twice as likely to land heads than tails. If the coin is flipped a number of times and the outcomes observed, that degree of belief may rise, fall or remain the same depending on the results.
For proposition "A" and evidence "B",
For more on the application of Bayes' theorem under the Bayesian interpretation of probability, see Bayesian inference.
Frequentist interpretation.
In the frequentist interpretation, probability measures a "proportion of outcomes". For example, suppose an experiment is performed many times. "P"("A") is the proportion of outcomes with property "A", and "P"("B") that with property "B". "P"("B" | "A") is the proportion of outcomes with property "B" "out of" outcomes with property "A", and "P"("A" | "B") the proportion of those with "A" "out of" those with "B".
The role of Bayes' theorem is best visualized with tree diagrams, as shown to the right. The two diagrams partition the same outcomes by "A" and "B" in opposite orders, to obtain the inverse probabilities. Bayes' theorem serves as the link between these different partitionings.
Forms.
Events.
Simple form.
For events "A" and "B", provided that "P"("B") ≠ 0,
In many applications, for instance in Bayesian inference, the event "B" is fixed in the discussion, and we wish to consider the impact of its having been observed on our belief in various possible events "A". In such a situation the denominator of the last expression, the probability of the given evidence "B", is fixed; what we want to vary is "A". Bayes' theorem then shows that the posterior probabilities are proportional to the numerator:
In words: posterior is proportional to prior times likelihood.
If events "A"1, "A"2, ..., are mutually exclusive and exhaustive, i.e., one of them is certain to occur but no two can occur together, and we know their probabilities up to proportionality, then we can determine the proportionality constant by using the fact that their probabilities must add up to one. For instance, for a given event "A", the event "A" itself and its complement ¬"A" are exclusive and exhaustive. Denoting the constant of proportionality by "c" we have
Adding these two formulas we deduce that
Alternative form.
Another form of Bayes' Theorem that is generally encountered when looking at two competing statements or hypotheses is:
For an epistemological interpretation:
For proposition "A" and evidence or background "B",
Extended form.
Often, for some partition {"Aj"} of the event space, the event space is given or conceptualized in terms of "P"("Aj") and "P"("B" | "Aj"). It is then useful to compute "P"("B") using the law of total probability:
In the special case where "A" is a binary variable:
Random variables.
Consider a sample space Ω generated by two random variables "X" and "Y". In principle, Bayes' theorem applies to the events "A" = {"X" = "x"} and "B" = {"Y" = "y"}. However, terms become 0 at points where either variable has finite probability density. To remain useful, Bayes' theorem may be formulated in terms of the relevant densities (see Derivation).
Simple form.
If "X" is continuous and "Y" is discrete,
If "X" is discrete and "Y" is continuous,
If both "X" and "Y" are continuous,
Extended form.
A continuous event space is often conceptualized in terms of the numerator terms. It is then useful to eliminate the denominator using the law of total probability. For "fY"("y"), this becomes an integral:
Bayes' rule.
Bayes' rule is Bayes' theorem in odds form.
where
is called the Bayes factor or likelihood ratio and the odds between two events is simply the ratio of the probabilities of the two events. Thus
So the rule says that the posterior odds are the prior odds times the Bayes factor, or in other words, posterior is proportional to prior times likelihood.
Derivation.
For events.
Bayes' theorem may be derived from the definition of conditional probability:
For random variables.
For two continuous random variables "X" and "Y", Bayes' theorem may be analogously derived from the definition of conditional density:
Examples.
Frequentist example.
An entomologist spots what might be a rare subspecies of beetle, due to the pattern on its back. In the rare subspecies, 98% have the pattern, or "P"(Pattern | Rare) = 98%. In the common subspecies, 5% have the pattern. The rare subspecies accounts for only 0.1% of the population. How likely is the beetle having the pattern to be rare, or what is "P"(Rare | Pattern)?
From the extended form of Bayes' theorem (since any beetle can be only rare or common),
formula_26
Drug testing.
Suppose a drug test is 99% sensitive and 99% specific. That is, the test will produce 99% true positive results for drug users and 99% true negative results for non-drug users. Suppose that 0.5% of people are users of the drug. If a randomly selected individual tests positive, what is the probability he or she is a user?
Despite the apparent accuracy of the test, if an individual tests positive, it is more likely that they do "not" use the drug than that they do.
This surprising result arises because the number of non-users is very large compared to the number of users; thus the number of false positives (0.995%) outweighs the number of true positives (0.495%). To use concrete numbers, if 1000 individuals are tested, there are expected to be 995 non-users and 5 users. From the 995 non-users, 0.01 × 995 ≃ 10 false positives are expected. From the 5 users, 0.99 × 5 ≃ 5 true positives are expected. Out of 15 positive results, only 5, about 33%, are genuine.
Note: The importance of specificity can be illustrated by showing that even if sensitivity is 100% and specificity is at 99% the probability of the person being a drug user is ≈33% but if the specificity is changed to 99.5% and the sensitivity is dropped down to 99% the probability of the person being a drug user rises to 49.8%.
History.
Bayes' theorem was named after the Reverend Thomas Bayes (1701–61), who studied how to compute a distribution for the probability parameter of a binomial distribution (in modern terminology). Bayes' unpublished manuscript was significantly edited by Richard Price before it was posthumously read at the Royal Society. Price edited Bayes' major work "An Essay towards solving a Problem in the Doctrine of Chances" (1763), which appeared in "Philosophical Transactions", and contains Bayes' Theorem. Price wrote an introduction to the paper which provides some of the philosophical basis of Bayesian statistics. In 1765 he was elected a Fellow of the Royal Society in recognition of his work on the legacy of Bayes. 
The French mathematician Pierre-Simon Laplace reproduced and extended Bayes' results in 1774, apparently quite unaware of Bayes' work. Stephen Stigler suggested in 1983 that Bayes' theorem was discovered by Nicholas Saunderson some time before Bayes; that interpretation, however, has been disputed.
Martyn Hooper and Sharon McGrayne have argued that Richard Price's contribution was substantial:
By modern standards, we should refer to the Bayes–Price rule. Price discovered Bayes' work, recognized its importance, corrected it, contributed to the article, and found a use for it. The modern convention of employing Bayes' name alone is unfair but so entrenched that anything else makes little sense.—

</doc>
<doc id="49571" url="http://en.wikipedia.org/wiki?curid=49571" title="Bayesian inference">
Bayesian inference

Bayesian inference is a method of statistical inference in which Bayes' rule is used to update the probability for a hypothesis as evidence is acquired. Bayesian inference is an important technique in statistics, and especially in mathematical statistics. Bayesian updating is particularly important in the dynamic analysis of a sequence of data. Bayesian inference has found application in a wide range of activities, including science, engineering, philosophy, medicine, and law. In the philosophy of decision theory, Bayesian inference is closely related to subjective probability, often called "Bayesian probability". Bayesian probability provides a rational method for updating beliefs.
Introduction to Bayes' rule.
Formal.
Bayesian inference derives the posterior probability as a consequence of two antecedents, a prior probability and a "likelihood function" derived from a statistical model for the observed data. Bayesian inference computes the posterior probability according to Bayes' theorem:
where
Note that, for different values of formula_3, only the factors formula_5 and formula_12 affect the value of formula_8. As both of these factors appear in the numerator, the posterior probability is proportional to both. In words:
Note that Bayes' rule can also be written as follows:
where the factor formula_27 represents the impact of formula_28 on the probability of formula_29.
Informal.
If the evidence does not match up with a hypothesis, one should reject the hypothesis. But if a hypothesis is extremely unlikely "a priori", one should also reject it, even if the evidence does appear to match up.
For example, imagine that I have various hypotheses about the nature of a newborn baby of a friend, including:
Then consider two scenarios:
The critical point about Bayesian inference, then, is that it provides a principled way of combining new evidence with prior beliefs, through the application of Bayes' rule. (Contrast this with frequentist inference, which relies only on the evidence as a whole, with no reference to prior beliefs.) Furthermore, Bayes' rule can be applied iteratively: after observing some evidence, the resulting posterior probability can then be treated as a prior probability, and a new posterior probability computed from new evidence. This allows for Bayesian principles to be applied to various kinds of evidence, whether viewed all at once or over time. This procedure is termed "Bayesian updating".
Bayesian updating.
Bayesian updating is widely used and computationally convenient. However, it is not the only updating rule that might be considered "rational".
Ian Hacking noted that traditional "Dutch book" arguments did not specify Bayesian updating: they left open the possibility that non-Bayesian updating rules could avoid Dutch books. Hacking wrote "And neither the Dutch book argument, nor any other in the personalist arsenal of proofs of the probability axioms, entails the dynamic assumption. Not one entails Bayesianism. So the personalist requires the dynamic assumption to be Bayesian. It is true that in consistency a personalist could abandon the Bayesian model of learning from experience. Salt could lose its savour."
Indeed, there are non-Bayesian updating rules that also avoid Dutch books (as discussed in the literature on "probability kinematics" following the publication of Richard C. Jeffrey's rule, which applies Bayes' rule to the case where the evidence itself is assigned a probability. The additional hypotheses needed to uniquely require Bayesian updating have been deemed to be substantial, complicated, and unsatisfactory.
Formal description of Bayesian inference.
Bayesian inference.
Note that this is expressed in words as "posterior is proportional to likelihood times prior", or sometimes as "posterior = likelihood times prior, over evidence".
Bayesian prediction.
Bayesian theory calls for the use of the posterior predictive distribution to do predictive inference, i.e., to predict the distribution of a new, unobserved data point. That is, instead of a fixed point as a prediction, a distribution over possible points is returned. Only this way is the entire posterior distribution of the parameter(s) used. By comparison, prediction in frequentist statistics often involves finding an optimum point estimate of the parameter(s)—e.g., by maximum likelihood or maximum a posteriori estimation (MAP)—and then plugging this estimate into the formula for the distribution of a data point. This has the disadvantage that it does not account for any uncertainty in the value of the parameter, and hence will underestimate the variance of the predictive distribution.
Note that both types of predictive distributions have the form of a compound probability distribution (as does the marginal likelihood). In fact, if the prior distribution is a conjugate prior, and hence the prior and posterior distributions come from the same family, it can easily be seen that both prior and posterior predictive distributions also come from the same family of compound distributions. The only difference is that the posterior predictive distribution uses the updated values of the hyperparameters (applying the Bayesian update rules given in the conjugate prior article), while the prior predictive distribution uses the values of the hyperparameters that appear in the prior distribution.
Inference over exclusive and exhaustive possibilities.
If evidence is simultaneously used to update belief over a set of exclusive and exhaustive propositions, Bayesian inference may be thought of as acting on this belief distribution as a whole.
General formulation.
Suppose a process is generating independent and identically distributed events formula_53, but the probability distribution is unknown. Let the event space formula_54 represent the current state of belief for this process. Each model is represented by event formula_55. The conditional probabilities formula_56 are specified to define the models. formula_57 is the degree of belief in formula_55. Before the first inference step, formula_59 is a set of "initial prior probabilities". These must sum to 1, but are otherwise arbitrary.
Suppose that the process is observed to generate formula_60. For each formula_61, the prior formula_62 is updated to the posterior formula_63. From Bayes' theorem:
Upon observation of further evidence, this procedure may be repeated.
Multiple observations.
For a set of independent and identically distributed observations formula_65, it may be shown that repeated application of the above is equivalent to
Where
This may be used to optimize practical calculations. 
Parametric formulation.
By parameterizing the space of models, the belief in all models may be updated in a single step. The distribution of belief over the model space may then be thought of as a distribution of belief over the parameter space. The distributions in this section are expressed as continuous, represented by probability densities, as this is the usual situation. The technique is however equally applicable to discrete distributions.
Let the vector formula_68 span the parameter space. Let the initial prior distribution over formula_68 be formula_70, where formula_71 is a set of parameters to the prior itself, or "hyperparameters". Let formula_65 be a set of independent and identically distributed event observations, where all formula_73 are distributed as formula_74 for some formula_68. Bayes' theorem is applied to find the posterior distribution over formula_68:
Where
Mathematical properties.
Interpretation of factor.
formula_79. That is, if the model were true, the evidence would be more likely than is predicted by the current state of belief. The reverse applies for a decrease in belief. If the belief does not change, formula_80. That is, the evidence is independent of the model. If the model were true, the evidence would be exactly as likely as predicted by the current state of belief.
Cromwell's rule.
If formula_81 then formula_82. If formula_83, then formula_84. This can be interpreted to mean that hard convictions are insensitive to counter-evidence.
The former follows directly from Bayes' theorem. The latter can be derived by applying the first rule to the event "not formula_85" in place of "formula_85", yielding "if formula_87, then formula_88", from which the result immediately follows.
Asymptotic behaviour of posterior.
Consider the behaviour of a belief distribution as it is updated a large number of times with independent and identically distributed trials. For sufficiently nice prior probabilities, the Bernstein-von Mises theorem gives that in the limit of infinite trials, the posterior converges to a Gaussian distribution independent of the initial prior under some conditions firstly outlined and rigorously proven by Joseph L. Doob in 1948, namely if the random variable in consideration has a finite probability space. The more general results were obtained later by the statistician David A. Freedman who published in two seminal research papers in 1963 and 1965 when and under what circumstances the asymptotic behaviour of posterior is guaranteed. His 1963 paper treats, like Doob (1949), the finite case and comes to a satisfactory conclusion. However, if the random variable has an infinite but countable probability space (i.e., corresponding to a die with infinite many faces) the 1965 paper demonstrates that for a dense subset of priors the Bernstein-von Mises theorem is not applicable. In this case there is almost surely no asymptotic convergence. Later in the 1980s and 1990s Freedman and Persi Diaconis continued to work on the case of infinite countable probability spaces. To summarise, there may be insufficient trials to suppress the effects of the initial choice, and especially for large (but finite) systems the convergence might be very slow.
Conjugate priors.
In parameterized form, the prior distribution is often assumed to come from a family of distributions called conjugate priors. The usefulness of a conjugate prior is that the corresponding posterior distribution will be in the same family, and the calculation may be expressed in closed form.
Estimates of parameters and predictions.
It is often desired to use a posterior distribution to estimate a parameter or variable. Several methods of Bayesian estimation select measurements of central tendency from the posterior distribution.
For one-dimensional problems, a unique median exists for practical continuous problems. The posterior median is attractive as a robust estimator.
If there exists a finite mean for the posterior distribution, then the posterior mean is a method of estimation.
Taking a value with the greatest probability defines maximum "a posteriori" (MAP) estimates:
There are examples where no maximum is attained, in which case the set of MAP estimates is empty.
There are other methods of estimation that minimize the posterior "risk" (expected-posterior loss) with respect to a loss function, and these are of interest to statistical decision theory using the sampling distribution ("frequentist statistics").
The posterior predictive distribution of a new observation formula_45 (that is independent of previous observations) is determined by
Examples.
Probability of a hypothesis.
Suppose there are two full bowls of cookies. Bowl #1 has 10 chocolate chip and 30 plain cookies, while bowl #2 has 20 of each. Our friend Fred picks a bowl at random, and then picks a cookie at random. We may assume there is no reason to believe Fred treats one bowl differently from another, likewise for the cookies. The cookie turns out to be a plain one. How probable is it that Fred picked it out of bowl #1?
Intuitively, it seems clear that the answer should be more than a half, since there are more plain cookies in bowl #1. The precise answer is given by Bayes' theorem. Let formula_93 correspond to bowl #1, and formula_94 to bowl #2.
It is given that the bowls are identical from Fred's point of view, thus formula_95, and the two must add up to 1, so both are equal to 0.5.
The event formula_28 is the observation of a plain cookie. From the contents of the bowls, we know that formula_97 and formula_98 Bayes' formula then yields
Before we observed the cookie, the probability we assigned for Fred having chosen bowl #1 was the prior probability, formula_100, which was 0.5. After observing the cookie, we must revise the probability to formula_101, which is 0.6.
Making a prediction.
An archaeologist is working at a site thought to be from the medieval period, between the 11th century to the 16th century. However, it is uncertain exactly when in this period the site was inhabited. Fragments of pottery are found, some of which are glazed and some of which are decorated. It is expected that if the site were inhabited during the early medieval period, then 1% of the pottery would be glazed and 50% of its area decorated, whereas if it had been inhabited in the late medieval period then 81% would be glazed and 5% of its area decorated. How confident can the archaeologist be in the date of inhabitation as fragments are unearthed?
The degree of belief in the continuous variable formula_102 (century) is to be calculated, with the discrete set of events formula_103 as evidence. Assuming linear variation of glaze and decoration with time, and that these variables are independent,
Assume a uniform prior of formula_108, and that trials are independent and identically distributed. When a new fragment of type formula_109 is discovered, Bayes' theorem is applied to update the degree of belief for each formula_110:
formula_111
A computer simulation of the changing belief as 50 fragments are unearthed is shown on the graph. In the simulation, the site was inhabited around 1420, or formula_112. By calculating the area under the relevant portion of the graph for 50 trials, the archaeologist can say that there is practically no chance the site was inhabited in the 11th and 12th centuries, about 1% chance that it was inhabited during the 13th century, 63% chance during the 14th century and 36% during the 15th century. Note that the Bernstein-von Mises theorem asserts here the asymptotic convergence to the "true" distribution because the probability space corresponding to the discrete set of events formula_103 is finite (see above section on asymptotic behaviour of the posterior).
In frequentist statistics and decision theory.
A decision-theoretic justification of the use of Bayesian inference was given by Abraham Wald, who proved that every unique Bayesian procedure is admissible. Conversely, every admissible statistical procedure is either a Bayesian procedure or a limit of Bayesian procedures.
Wald characterized admissible procedures as Bayesian procedures (and limits of Bayesian procedures), making the Bayesian formalism a central technique in such areas of frequentist inference as parameter estimation, hypothesis testing, and computing confidence intervals. For example:
Applications.
Computer applications.
Bayesian inference has applications in artificial intelligence and expert systems. Bayesian inference techniques have been a fundamental part of computerized pattern recognition techniques since the late 1950s. There is also an ever growing connection between Bayesian methods and simulation-based Monte Carlo techniques since complex models cannot be processed in closed form by a Bayesian analysis, while a graphical model structure "may" allow for efficient simulation algorithms like the Gibbs sampling and other Metropolis–Hastings algorithm schemes. Recently Bayesian inference has gained popularity amongst the phylogenetics community for these reasons; a number of applications allow many demographic and evolutionary parameters to be estimated simultaneously.
As applied to statistical classification, Bayesian inference has been used in recent years to develop algorithms for identifying e-mail spam. Applications which make use of Bayesian inference for spam filtering include CRM114, DSPAM, Bogofilter, SpamAssassin, SpamBayes, Mozilla, XEAMS, and others. Spam classification is treated in more detail in the article on the naive Bayes classifier.
Solomonoff's Inductive inference is the theory of prediction based on observations; for example, predicting the next symbol based upon a given series of symbols. The only assumption is that the environment follows some unknown but computable probability distribution. It is a formal inductive framework that combines two well-studied principles of inductive inference: Bayesian statistics and Occam’s Razor.
Solomonoff's universal prior probability of any prefix "p" of a computable sequence "x" is the sum of the probabilities of all programs (for a universal computer) that compute something starting with "p". Given some "p" and any computable but unknown probability distribution from which "x" is sampled, the universal prior and Bayes' theorem can be used to predict the yet unseen parts of "x" in optimal fashion.
In the courtroom.
Bayesian inference can be used by jurors to coherently accumulate the evidence for and against a defendant, and to see whether, in totality, it meets their personal threshold for 'beyond a reasonable doubt'. Bayes' theorem is applied successively to all evidence presented, with the posterior from one stage becoming the prior for the next. The benefit of a Bayesian approach is that it gives the juror an unbiased, rational mechanism for combining evidence. It may be appropriate to explain Bayes' theorem to jurors in odds form, as betting odds are more widely understood than probabilities. Alternatively, a logarithmic approach, replacing multiplication with addition, might be easier for a jury to handle.
If the existence of the crime is not in doubt, only the identity of the culprit, it has been suggested that the prior should be uniform over the qualifying population. For example, if 1,000 people could have committed the crime, the prior probability of guilt would be 1/1000.
The use of Bayes' theorem by jurors is controversial. In the United Kingdom, a defence expert witness explained Bayes' theorem to the jury in "R v Adams". The jury convicted, but the case went to appeal on the basis that no means of accumulating evidence had been provided for jurors who did not wish to use Bayes' theorem. The Court of Appeal upheld the conviction, but it also gave the opinion that "To introduce Bayes' Theorem, or any similar method, into a criminal trial plunges the jury into inappropriate and unnecessary realms of theory and complexity, deflecting them from their proper task."
Gardner-Medwin argues that the criterion on which a verdict in a criminal trial should be based is "not" the probability of guilt, but rather the "probability of the evidence, given that the defendant is innocent" (akin to a frequentist p-value). He argues that if the posterior probability of guilt is to be computed by Bayes' theorem, the prior probability of guilt must be known. This will depend on the incidence of the crime, which is an unusual piece of evidence to consider in a criminal trial. Consider the following three propositions:
Gardner-Medwin argues that the jury should believe both A and not-B in order to convict. A and not-B implies the truth of C, but the reverse is not true. It is possible that B and C are both true, but in this case he argues that a jury should acquit, even though they know that they will be letting some guilty people go free. See also Lindley's paradox.
Bayesian epistemology.
Bayesian epistemology is a movement that advocates for Bayesian inference as a means of justifying the rules of inductive logic.
Karl Popper and David Miller have rejected the alleged rationality of Bayesianism, i.e. using Bayes rule to make epistemological inferences: It is prone to the same vicious circle as any other justificationist epistemology, because it presupposes what it attempts to justify. According to this view, a rational interpretation of Bayesian inference would see it merely as a probabilistic version of falsification, rejecting the belief, commonly held by Bayesians, that high likelihood achieved by a series of Bayesian updates would prove the hypothesis beyond any reasonable doubt, or even with likelihood greater than 0.
Bayes and Bayesian inference.
The problem considered by Bayes in Proposition 9 of his essay, "An Essay towards solving a Problem in the Doctrine of Chances", is the posterior distribution for the parameter "a" (the success rate) of the binomial distribution.
History.
The term "Bayesian" refers to Thomas Bayes (1702–1761), who proved a special case of what is now called Bayes' theorem. However, it was Pierre-Simon Laplace (1749–1827) who introduced a general version of the theorem and used it to approach problems in celestial mechanics, medical statistics, reliability, and jurisprudence. Early Bayesian inference, which used uniform priors following Laplace's principle of insufficient reason, was called "inverse probability" (because it infers backwards from observations to parameters, or from effects to causes). After the 1920s, "inverse probability" was largely supplanted by a collection of methods that came to be called frequentist statistics.
In the 20th century, the ideas of Laplace were further developed in two different directions, giving rise to "objective" and "subjective" currents in Bayesian practice. In the objective or "non-informative" current, the statistical analysis depends on only the model assumed, the data analyzed, and the method assigning the prior, which differs from one objective Bayesian to another objective Bayesian. In the subjective or "informative" current, the specification of the prior depends on the belief (that is, propositions on which the analysis is prepared to act), which can summarize information from experts, previous studies, etc.
In the 1980s, there was a dramatic growth in research and applications of Bayesian methods, mostly attributed to the discovery of Markov chain Monte Carlo methods, which removed many of the computational problems, and an increasing interest in nonstandard, complex applications. Despite growth of Bayesian research, most undergraduate teaching is still based on frequentist statistics. Nonetheless, Bayesian methods are widely accepted and used, such as for example in the field of machine learning.
Further reading.
Elementary.
The following books are listed in ascending order of probabilistic sophistication:

</doc>
<doc id="49572" url="http://en.wikipedia.org/wiki?curid=49572" title="Hooliganism">
Hooliganism

Hooliganism is disruptive or unlawful behavior such as rioting, bullying, and vandalism.
Etymology.
There are several theories regarding the origin of the word "hooliganism," which is a derivative of the word hooligan. "The Compact Oxford English Dictionary" states that the word may have originated from the surname of a fictional rowdy Irish family in a music hall song of the 1890s. Clarence Rook, in his 1899 book, "Hooligan Nights", claimed that the word came from Patrick Hoolihan (or Hooligan), an Irish bouncer and thief who lived in London.
Early usage.
The first use of the term is unknown, but the word first appeared in print in London police-court reports in 1894 referring to the name of a gang of youths in the Lambeth area of London—the "Hooligan Boys", and later—the "O'Hooligan Boys".
In August 1898 a murder in Lambeth committed by a member of the gang drew further attention to the word which was immediately popularised by the press. The London newspaper "Daily Graphic" wrote in an article on 22 August 1898, "The avalanche of brutality which, under the name of 'Hooliganism' ... has cast such a dire slur on the social records of South London."
Arthur Conan Doyle wrote in his 1904 short story "The Adventure of the Six Napoleons", "It seemed to be one of those senseless acts of Hooliganism which occur from time to time, and it was reported to the constable on the beat as such." H.G. Wells wrote in his 1909 semi-autobiographical novel "Tono-Bungay", "Three energetic young men of the hooligan type, in neck-wraps and caps, were packing wooden cases with papered-up bottles, amidst much straw and confusion."
According to "Life" magazine (30 July 1941), the comic strip artist and political cartoonist Frederick Burr Opper introduced a character called Happy Hooligan in 1900; "hapless Happy appeared regularly in U.S. newspapers for more than 30 years," a "naive, skinny, baboon-faced tramp who invariably wore a tomato can for a hat." "Life" brought this up by way of criticizing the Soviet U.N. delegate Yakov A. Malik for misusing the word. Malik had indignantly referred to anti-Soviet demonstrators in New York as "hooligans". Happy Hooligan, "Life" reminded its readers, "became a national hero, not by making trouble, which Mr. Malik understands is the function of a hooligan, but by getting himself into it."
Modern usage.
Later, as the meaning of the word shifted slightly, none of the possible alternatives had precisely the same undertones of a person, usually young, who belongs to an informal group and commits acts of vandalism or criminal damage, starts fights, and who causes disturbances but is not a thief.
Violence in sports.
The word "hooliganism" and "hooligan" began to be associated with violence in sports, in particular from the 1970s in the UK with football hooliganism. The phenomenon, however, long preceded the modern term; for example, one of the earliest known instances of crowd violence at a sporting event took place in ancient Constantinople. Two chariot racing factions, the Blues and the Greens, were involved in the Nika riots which lasted around a week in 532 CE; nearly half the city was burned or destroyed, in addition to tens of thousands of deaths.
Sports crowd violence continues to be a worldwide concerning phenomenon exacting at times an inexcusable number of injuries, damage to property and casualties. No single account on its own can be used to understand or explain sports collective violence. Rather, individual, social and environmental factors interact and influence one another through a dynamic process occurring at different levels. Furthermore, any form of sport fan aggression should always be considered in reference to the wider social-structural and environmental context in which it takes place. Macro-sociological accounts suggest that structural strains, experiences of deprivation or a low socio-economic background can at times be instrumental to the acceptance and reproduction of norms that tolerate great levels of violence and territoriality, which is a common feature of football hooliganism. Furthermore, social cleavages within societies facilitate the development of strong in-groups bonds and intense feelings of antagonism towards outsiders which in turn can facilitate group identification and affect the likelihood of fan violence.
In the Soviet Union and Russia.
In the Soviet Union the word "khuligan" was used to refer to scofflaws. Hooliganism (rus. Хулига́нство) was listed as a criminal offense, similar to disorderly conduct in some other jurisdictions, and used as a catch-all charge for prosecuting unapproved behavior. Hooliganism is defined generally in the Criminal Code of Russia as an average gravity crime.
Olympic medalist Vasiliy Khmelevskiy was convicted of hooliganism for setting a masqueraded person on fire during a celebration in 1979 and sentenced to five years imprisonment. Matthias Rust was convicted of hooliganism, among other things, for his 1987 Cessna landing in Red Square. More recently, the same charge has been leveled against members of the feminist punk group Pussy Riot for which three members have each received a two-year sentence on 17 August 2012. Hooliganism charges have also been levelled against the Greenpeace protestors in October 2013.

</doc>
<doc id="49574" url="http://en.wikipedia.org/wiki?curid=49574" title="Abbie Hoffman">
Abbie Hoffman

Abbot Howard "Abbie" Hoffman (November 30, 1936 – April 12, 1989) was an American political and social activist and anarchist who co-founded the Youth International Party ("Yippies").
Hoffman was arrested and tried for conspiracy and inciting to riot as a result of his role in protests that led to violent confrontations with police during the 1968 Democratic National Convention, along with Jerry Rubin, David Dellinger, Tom Hayden, Rennie Davis, John Froines, Lee Weiner, and Bobby Seale. The group was known collectively as the "Chicago Eight"; when Seale's prosecution was separated from the others, they became known as the Chicago Seven. While the defendants were initially convicted of intent to incite a riot, the verdicts were overturned on appeal.
Hoffman arose to prominence in the 1960s, continued his activism in the 1970s, and has remained a symbol of the youth rebellion of the counterculture era.
Early life and education.
Hoffman was born November 30, 1936 in Worcester, Massachusetts, to John Hoffman and Florence Schanberg, both of Jewish descent. Hoffman was raised in a middle-class household and had two younger siblings. As a child in the 1940s–50s, he was a member of what has been described as "the transitional generation between the beatniks and hippies". He described his childhood as "idyllic" and the '40s as "a great time to grow up in." On June 3, 1954, 17-year-old Hoffman was arrested for the first time, for driving without a license. During his school days, he became known as a troublemaker who started fights, played pranks, vandalized school property, and referred to teachers by their first names. In his sophomore year, Hoffman was expelled from Classical High School, a now-closed public high school in Worcester. As an atheist, Hoffman wrote a paper declaring that "God could not possibly exist, for if he did, there wouldn't be any suffering in the world." The irate teacher ripped up the paper and called him "a Communist punk." Hoffman jumped on the teacher and started fighting him until he was restrained and removed from the school. After his expulsion, he attended Worcester Academy, graduating in 1955. Hoffman engaged in many behaviors typical of rebellious teenagers in the 1950s such as riding motorcycles, wearing leather jackets, and sporting a ducktail haircut. Upon graduating, he enrolled in Brandeis University, where he studied under professors such as noted psychologist Abraham Maslow, often considered the father of humanistic psychology. He was also a student of Marxist theorist Herbert Marcuse, whom Hoffman said had a profound effect on his political outlook. Hoffman would later cite Marcuse's influence during his activism and his theories on revolution. Hoffman graduated with a B.A. in psychology in 1959. That fall, he enrolled at the University of California, Berkeley, where he completed coursework toward a master's degree in psychology. Soon after, he married his pregnant girlfriend Sheila Karklin in May 1960.
Early protests.
Prior to his days as a leading member of the Yippie movement, Hoffman was involved with the Student Nonviolent Coordinating Committee (SNCC), and organized Liberty House, which sold items to support the Civil Rights Movement in the southern United States. During the Vietnam War, Hoffman was an anti-war activist, using deliberately comical and theatrical tactics.
In late 1966, Hoffman met with a radical community-action group called the Diggers and studied their ideology. He later returned to New York and published a book with this knowledge. Doing so was considered a violation by the Diggers. Diggers co-founder Peter Coyote explained:
Abbie, who was a friend of mine, was always a media junky. We explained everything to those guys, and they violated everything we taught them. Abbie went back, and the first thing he did was publish a book, with his picture on it, that blew the hustle of every poor person on the Lower East Side by describing every free scam then current in New York, which were then sucked dry by disaffected kids from Scarsdale.
One of Hoffman's well-known stunts was on August 24, 1967, when he led members of the movement to the gallery of the New York Stock Exchange (NYSE). The protesters threw fistfuls of real and fake dollar bills down to the traders below, some of whom booed, while others began to scramble frantically to grab the money as fast as they could.
 Accounts of the amount of money that Hoffman and the group tossed was said to be as little as $30 to $300. Hoffman claimed to be pointing out that, metaphorically, that's what NYSE traders "were already doing." "We didn't call the press", wrote Hoffman. "At that time we really had no notion of anything called a media event." Yet the press was quick to react and by evening the event was reported around the world. Since that incident, the stock exchange spent $20,000 to enclose the gallery with bulletproof glass.
In October 1967, David Dellinger of the National Mobilization Committee to End the War in Vietnam asked Jerry Rubin to help mobilize and direct a March on the Pentagon. The protesters gathered at the Lincoln Memorial as Dellinger and Dr. Benjamin Spock gave speeches to the mass of people. From there, the group marched towards the Pentagon. As the protesters neared the Pentagon, they were met by soldiers of the 82nd Airborne Division who formed a human barricade blocking the Pentagon steps. Not to be dissuaded, Hoffman vowed to levitate the Pentagon claiming he would attempt to use psychic energy to levitate the Pentagon until it would turn orange and begin to vibrate, at which time the war in Vietnam would end. Allen Ginsberg led Tibetan chants to assist Hoffman.
Hoffman's theatrics were successful at convincing many young people to become more active in the politics of the time.
Chicago Eight conspiracy trial.
Hoffman was arrested and tried for conspiracy and inciting to riot as a result of his role in anti-Vietnam War protests, which were met by a violent police response during the 1968 Democratic National Convention in Chicago. He was among the group that came to be known as the Chicago Seven (originally known as the Chicago Eight), which included fellow Yippie Jerry Rubin, David Dellinger, Rennie Davis, John Froines, Lee Weiner, future California state senator Tom Hayden and Black Panther Party co-founder Bobby Seale (before his trial was severed from the others).
Presided over by Judge Julius Hoffman (no relation to Hoffman, about which he joked throughout the trial), Abbie Hoffman's courtroom antics frequently grabbed the headlines; one day, defendants Hoffman and Rubin appeared in court dressed in judicial robes, while on another day, Hoffman was sworn in as a witness with his hand giving the finger. Judge Hoffman became the favorite courtroom target of the Chicago Seven defendants, who frequently would insult the judge to his face. Abbie Hoffman told Judge Hoffman "you are a 'shande fur de Goyim' [disgrace in front of the gentiles]. You would have served Hitler better." He later added that "your idea of justice is the only obscenity in the room." Both Davis and Rubin told the Judge "this court is bullshit." When Hoffman was asked in what state he resided, he replied the "state of mind of my brothers and sisters".
Other celebrities were called as "cultural witnesses" including Allen Ginsberg, Phil Ochs, Arlo Guthrie, Norman Mailer and others. Hoffman closed the trial with a speech in which he quoted Abraham Lincoln, making the claim that the President himself, if alive today, would also be arrested in Chicago's Lincoln Park.
On February 18, 1970, Hoffman and four of the other defendants (Rubin, Dellinger, Davis, and Hayden) were found guilty of intent to incite a riot while crossing state lines. All seven defendants were found not guilty of conspiracy. At sentencing, Hoffman suggested the judge try LSD and offered to set him up with "a dealer he knew in Florida" (the judge was known to be headed to Florida for a post-trial vacation). Each of the five was sentenced to five years in prison and a $5,000 fine.
However, all convictions were subsequently overturned by the Seventh Circuit Court of Appeals. The Walker Commission later found that in fact it had been a "police riot."
Controversy at Woodstock.
At Woodstock in 1969, Hoffman reportedly interrupted The Who's performance to attempt to speak against the jailing of John Sinclair of the White Panther Party. He grabbed a microphone and yelled, "I think this is a pile of shit while John Sinclair rots in prison ..." Pete Townshend was adjusting his amplifier between songs and turned to look at Hoffman over his left shoulder. Townshend shouted "Fuck off! Fuck off my stage!" and reportedly ran at Hoffman with his guitar and hit Hoffman in the back, although Townshend later denied attacking Hoffman. Townshend later said that while he actually agreed with Hoffman on Sinclair's imprisonment, he would have knocked him offstage regardless of the content of his message, given that Hoffman had violated the "sanctity of the stage," i.e., the right of the band to perform uninterrupted by distractions not relevant to the actual show. The incident took place during a camera change, and was not captured on film. The audio of this incident, however, can be heard on The Who's box set, "Thirty Years of Maximum R&B" (Disc 2, Track 20, "Abbie Hoffman Incident").
In 1971's "Steal This Book" in the section "Free Communication," Hoffman encourages his readership to take to the stage at rock concerts to use the pre-assembled audience and PA system to get their message out. However he mentions that "interrupting the concert is frowned upon since it is only spitting in the faces of people you are trying to reach."
In "Woodstock Nation", Hoffman mentions the incident, and says he was on a bad LSD trip at the time. Joe Shea, then a reporter for the "Times Herald-Record", a Dow Jones-Ottaway newspaper that covered the event on-site, said he saw the incident. He recalled that Hoffman was actually hit in the back of the head by Townshend's guitar and toppled directly into the pit in front of the stage. He does not recall any "shove" from Townshend, and discounts both men's accounts.
Underground.
In 1971, Hoffman published "Steal This Book", which advised readers on how to live basically for free. Many of his readers followed Hoffman's advice and stole the book, leading many bookstores to refuse to carry it. He was also the author of several other books, including "Vote!", co-written with Rubin and Ed Sanders. Hoffman was arrested August 28, 1973 on drug charges for intent to sell and distribute cocaine. He always maintained that undercover police agents entrapped him into a drug deal and planted suitcases of cocaine in his office. In the spring of 1974, Hoffman skipped bail, underwent cosmetic surgery to alter his appearance, and hid from authorities for several years.
Some believed Hoffman made himself a target. In 1998, Peter Coyote opined:
The FBI couldn't infiltrate us. We did everything anonymously, and we did everything for nothing, because we wanted our actions to be authentic. It's the mistake that Abbie Hoffman made. He came out, he studied with us, we taught him everything, and then he went back and wrote a book called "Free," and he put his name on it! He set himself up to be a leader of the counterculture, and he was undone by that. Big mistake.
Despite being "in hiding" during part of this period (Hoffman lived in Fineview, New York near Thousand Island Park, a private resort on Wellesley Island on the St. Lawrence River under the name "Barry Freed"), he helped coordinate an environmental campaign to preserve the Saint Lawrence River (Save the River organization). During his time on the run, he was also the "travel" columnist for "Crawdaddy!" magazine. On September 4, 1980, he surrendered to authorities; on the same date, he appeared on a pre-taped edition of ABC-TV's "20/20" in an interview with Barbara Walters. Hoffman received a one-year sentence, but was released after four months.
Back to visibility.
In November 1986, Hoffman was arrested along with 14 others, including Amy Carter, the daughter of former President Jimmy Carter, for trespassing at the University of Massachusetts Amherst. The charges stemmed from a protest against the Central Intelligence Agency's recruitment on the UMass campus. Since the university's policy limited campus recruitment to law-abiding organizations, the defense argued that the CIA engaged in illegal activities. The federal district court judge permitted expert witnesses, including former Attorney General Ramsey Clark and a former CIA agent who testified that the CIA carried on an illegal Contra war against the Sandinista regime in Nicaragua in violation of the Boland Amendment.
In three days of testimony, more than a dozen defense witnesses, including Daniel Ellsberg, and former Contra leader Edgar Chamorro, described the CIA's role in more than two decades of covert, illegal and often violent activities. In his closing argument, Hoffman, acting as his own attorney, placed his actions within the best tradition of American civil disobedience. He quoted from Thomas Paine, "the most outspoken and farsighted of the leaders of the American Revolution: 'Every age and generation must be as free to act for itself, in all cases, as the ages and generations which preceded it. Man has no property in man, neither has any generation a property in the generations which are to follow.'"
Hoffman concluded: "Thomas Paine was talking about this Spring day in this courtroom. A verdict of not guilty will say, 'When our country is right, keep it right; but when it is wrong, right those wrongs.'" On April 15, 1987, the jury found Hoffman and the other defendants not guilty.
After his acquittal, Hoffman acted in a cameo appearance in Oliver Stone's later-released anti-Vietnam War movie, "Born on the Fourth of July." He essentially played himself in the movie, waving a flag on the ramparts of an administration building during a campus protest that was being teargassed and crushed by state troopers.
In 1987 Hoffman summed up his views.
You are talking to a leftist. I believe in the redistribution of wealth and power in the world. I believe in universal hospital care for everyone. I believe that we should not have a single homeless person in the richest country in the world. And I believe that we should not have a CIA that goes around overwhelming governments and assassinating political leaders, working for tight oligarchies around the world to protect the tight oligarchy here at home.
Later that same year, Hoffman and Jonathan Silvers wrote "Steal This Urine Test" (published October 5, 1987), which exposed the internal contradictions of the War on Drugs and suggested ways to circumvent its most intrusive measures. He stated, for instance, that Federal Express, which received high praise from management guru Tom Peters for "empowering" workers, in fact subjected most employees to random drug tests, firing any who got a positive result, with no retest or appeal procedure, despite the fact that FedEx chose a drug lab (the lowest bidder) with a proven record of frequent false positive results.
Stone's "Born on the Fourth of July" was released on December 20, 1989, more than eight months after Hoffman's suicide on April 12, 1989. At the time of his death, Hoffman was at the height of a renewed public visibility, one of the few 1960s radicals who still commanded the attention of all kinds of mass media. He regularly lectured audiences about the CIA's covert activities, including assassinations disguised as suicides. His "Playboy" article (October, 1988) outlining the connections that constitute the "October Surprise," brought that alleged conspiracy to the attention of a wide-ranging American readership for the first time.
Personal life.
In 1960, Hoffman married Sheila Karklin and had two children: Andrew (born 1960) and Amy (1962–2007), who later went by the name Ilya. They divorced in 1966.
In 1967, Hoffman married Anita Kushner in Manhattan's Central Park. They had one son, amerika Hoffman, deliberately named using a lowercase "a" to indicate both patriotism and non-jingoistic intent. Although Hoffman and Kushner were effectively separated after Hoffman became a fugitive, starting in 1973, they were not formally divorced until 1980. He subsequently fell in love with Johanna Lawrenson in 1974, while a fugitive.
His personal life drew a great deal of scrutiny from the Federal Bureau of Investigation. By their own admission, they kept a file on him that was 13,262 pages long.
Death.
Hoffman was 52 at the time of his death on April 12, 1989, which was caused by swallowing 150 phenobarbital tablets and liquor. He had been diagnosed with bipolar disorder in 1980. At the time he had recently changed treatment medications and was reportedly depressed when his 83-year-old mother was diagnosed with cancer (she died in 1996 at the age of 90). Some close to Hoffman claimed that as a natural prankster who valued youth, he was also unhappy about reaching middle age, combined with the fact that the ideas of the 1960s had given way to a conservative backlash in the 1980s. In 1984 he had expressed dismay that the current generation of young people were not as interested in protesting and social activism as youth had been during the 1960s. Hoffman's body was found in his apartment in a converted turkey coop on Sugan Road in Solebury Township, near New Hope, Pennsylvania. At the time of his death, he was surrounded by about 200 pages of his own handwritten notes, many about his own moods.
His death was officially ruled as suicide. As reported by "The New York Times," "Among the more vocal doubters at the service today was Mr. Dellinger, who said, 'I don't believe for one moment the suicide thing.' He said he had been in fairly frequent touch with Mr. Hoffman, who had 'numerous plans for the future.'" Yet the same "New York Times" article reported that the coroner found the residue of about 150 pills and quoted the coroner in a telephone interview saying 'There is no way to take that amount of phenobarbital without intent. It was intentional and self-inflicted.
A week after Hoffman's death, a thousand friends and relatives gathered for a memorial in Worcester, Massachusetts at Temple Emanuel, the synagogue he attended as a child. Two of his colleagues from the Chicago Seven conspiracy trial were there: David Dellinger and Jerry Rubin, Hoffman's co-founder of the Yippies, by then a businessman.
As "The New York Times" reported: "Indeed, most of the mourners who attended the formal memorial at Temple Emanuel here were more yuppie than yippie and there were more rep ties than ripped jeans among the crowd..."
The "Times" report continued:
Bill Walton, the radical Celtic of basketball renown, told of a puckish Abbie, then underground evading a cocaine charge in the '70s, leaping from the shadows on a New York street to give him an impromptu basketball lesson after a loss to the Knicks. 'Abbie was not a fugitive from justice,' said Mr. Walton. 'Justice was a fugitive from him.' On a more traditional note, Rabbi Norman Mendell said in his eulogy that Mr. Hoffman's long history of protest, antic though much of it had been, was 'in the Jewish prophetic tradition, which is to comfort the afflicted and afflict the comfortable.'
Media.
Appearances in documentary films.
Hoffman is featured in interviews and archival news footage in the following documentaries:
Legacy.
Theatre Festival.
The Mary-Archie Theatre Company in Chicago started the "Abbie Hoffman Died For Our Sins" Theatre Festival in 1988. This festival runs every year for 3 consecutive days as a celebration of the Woodstock Music and Art Fair of 1969.

</doc>
<doc id="49582" url="http://en.wikipedia.org/wiki?curid=49582" title="Battle of Lewes">
Battle of Lewes

The Battle of Lewes was one of two main battles of the conflict known as the Second Barons' War. It took place at Lewes in Sussex, on 14 May 1264. It marked the high point of the career of Simon de Montfort, 6th Earl of Leicester, and made him the "uncrowned King of England". Henry III left the safety of Lewes Castle and St. Pancras Priory to engage the Barons in battle and was initially successful, his son Prince Edward routing part of the baronial army with a cavalry charge. However Edward pursued his quarry off the battlefield and left Henry's men exposed. Henry was forced to launch an infantry attack up Offham Hill where he was defeated by the barons' men, defending the hilltop. The royalists fled back to the castle and priory and the King was forced to sign the Mise of Lewes, ceding many of his powers to Montfort.
Background.
Henry III was an unpopular monarch due to his autocratic style, displays of favouritism and his refusal to negotiate with his barons. The barons eventually imposed a constitutional reform known as the Provisions of Oxford upon Henry that called for a thrice-yearly meeting led by Simon de Montfort to discuss matters of government. Henry sought to escape the restrictions of the provisions and applied to Louis IX of France to arbitrate in the dispute. Louis agreed with Henry and annulled the provisions. Montfort was angered by this and rebelled against the King along with other barons in the Second Barons' War. 
The war was not initially openly fought, each side toured the country to raise support for their army. By May the King's force had reached Lewes where they intended to halt for a while to allow reinforcements to reach them. The King encamped at St. Pancras Priory with a force of infantry, but his son, Prince Edward (later King Edward I), commanded the cavalry at Lewes Castle 500 yd to the north. De Montfort approached the King with the intention of negotiating a truce or failing that to draw him into open battle. The King rejected the negotiations and de Montfort moved his men from Fletching to Offam Hill, a mile to the north-west of Lewes, in a night march that surprised the royalist forces.
Deployment.
The royalist army was up to twice the size of de Montfort's. Henry held command of the centre, with Prince Edward, William de Valence, 1st Earl of Pembroke, and John de Warenne, 6th Earl of Surrey, on the right; and Richard, 1st Earl of Cornwall, and his son, Henry of Almain, on the left. The barons held the higher ground, overlooking Lewes and had ordered their men to wear white crosses as a distinguishing emblem. De Montfort split his forces into four parts, giving his son, Henry de Montfort command of one quarter; Gilbert de Clare with John FitzJohn and William of Montchensy another; a third portion consisting of Londoners was placed under Nicholas Seagrave whilst de Montfort himself led the fourth quarter with Thomas of Pelveston.
Battle.
The baronial forces commenced the engagement with a surprise dawn attack on foragers sent out from the royalist forces. The King then made his move. Edward led a cavalry charge against Seagrave's Londoners, placed on the left of the baronial line, that caused them to break and run to the village of Offham. Edward pursued his foe for some four miles, leaving the King unsupported. Henry was forced to launch an attack with his centre and right divisions straight up Offham Hill into the baronial line which awaited them at the defensive. Cornwall's division faltered almost immediately but Henry's men fought on until compelled to retreat by the arrival of de Montfort's men that had been held as the baronial reserve.
The King's men were forced down the hill and into Lewes where they engaged in a fighting retreat to the castle and priory. Edward returned with his weary cavalrymen and launched a counterattack but upon locating his father was persuaded that, with the town ablaze and many of the King's supporters having fled, it was time to accept de Montfort's renewed offer of negotiations. The Earl of Cornwall was captured by the barons when he was unable to reach the safety of the priory and, being discovered in a windmill, was taunted with cries of "Come down, come down, thou wicked miller."
Aftermath.
The King was forced to sign the so-called Mise of Lewes. Though the document has not survived, it is clear that Henry was forced to accept the Provisions of Oxford, while Prince Edward remained a hostage of the barons. This put Montfort in a position of ultimate power, which would last until Prince Edward's escape, and Montfort's subsequent defeat at the Battle of Evesham in August 1265.
In 1994, an archaeological survey of the cemetery of St Nicholas Hospital, in Lewes, revealed the remains of bodies that were thought to be combatants from the battle of Lewes. However, in 2014 it was revealed that some of the skeletons may actually be much older with a skeleton known as "skeleton 180", being contemporary with the Norman invasion.
Location.
There remains some uncertainty over the location of the battle with Offham Hill's eastern and lower slopes covered by modern housing. The top and southern slopes remain accessible by footpaths through agricultural land and the ruins of the priory and castle are also open to visitors.

</doc>
<doc id="49583" url="http://en.wikipedia.org/wiki?curid=49583" title="Link awareness">
Link awareness

Link awareness is defined as the ability to discover, view, search and update global hyperlink information about any resource with a URL on the World Wide Web. This global link information is a shared information resource.
Implementing link awareness is difficult. In practice, an implementation only approximates link awareness. There are at least two qualitative axes on which we can classify these implementations.

</doc>
