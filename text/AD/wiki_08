<doc id="44035" url="http://en.wikipedia.org/wiki?curid=44035" title="Franco-Prussian War">
Franco-Prussian War

The Franco-Prussian War or Franco-German War (German: "Deutsch-Französischer Krieg", lit. "German-French War", French: "Guerre franco-allemande", lit. "French-German War"), often referred to in France as the War of 1870 (19 July 1870 – 10 May 1871), was a conflict between the Second French Empire and the German states of the North German Confederation led by the Kingdom of Prussia. The conflict centered on Prussian ambitions to extend German unification. Prussian chancellor Otto von Bismarck planned to provoke a French attack in order to draw the southern German states—Baden, Württemberg, Bavaria and Hesse-Darmstadt—into an alliance with the Prussian-dominated North German Confederation. 
Bismarck adroitly created a diplomatic crisis over the succession to the Spanish throne, then rewrote a dispatch about a meeting between king William of Prussia and the French foreign minister, to make it appear that the French had been insulted. The French press and parliament demanded a war, which the generals of Napoleon III assured him that France would win. Napoleon and his Prime Minister, Émile Ollivier, for their parts sought war to solve political disunity in France. On 16 July 1870, the French parliament voted to declare war on the German Kingdom of Prussia and hostilities began three days later. The German coalition mobilised its troops much more quickly than the French and rapidly invaded northeastern France. The German forces were superior in numbers, had better training and leadership and made more effective use of modern technology, particularly railroads and artillery. 
A series of swift Prussian and German victories in eastern France, culminating at the Battle of Sedan and the Siege of Metz saw the French army decisively defeated; Napoleon III was captured at Sedan on 2 September. A Government of National Defence declared the Third Republic in Paris on 4 September and continued the war. For the next five months the German forces fought and defeated new French armies in northern France. Following the Siege of Paris, the capital fell on 28 January 1871. The German states proclaimed their union as the German Empire under the Prussian king, Wilhelm I, uniting Germany as a nation-state. The Treaty of Frankfurt of 10 May 1871 gave Germany most of Alsace and some parts of Lorraine, which became the Imperial territory of Alsace-Lorraine ("Reichsland Elsaß-Lothringen").
Following defeat, a revolutionary uprising called the Paris Commune seized power in the capital and held it for two months, until it was bloodily suppressed by the regular French army at the end of May 1871. The German conquest of France and the unification of Germany upset the European balance of power that had existed since the Congress of Vienna in 1815 and Otto von Bismarck maintained great authority in international affairs for two decades. French determination to regain Alsace-Lorraine and fear of another Franco-German war, along with British concern over the balance of power, became factors in the causes of World War I.
Causes.
The causes of the Franco–Prussian War are deeply rooted in the events surrounding the unification of Germany. In the aftermath of the Austro–Prussian War of 1866, Prussia had annexed numerous territories and formed the North German Confederation. This new power destabilized the European balance of power established by the Congress of Vienna in 1815 after the Napoleonic Wars. Napoleon III, then the emperor of France, demanded compensations in Belgium and on the left bank of the Rhine to secure France's strategic position, which the Prussian chancellor, Otto von Bismarck, flatly refused. Prussia then turned its attention towards the south of Germany, where it sought to incorporate the southern German kingdoms, Bavaria, Württemberg, Baden and Hesse-Darmstadt, into a unified Prussia-dominated Germany. France was strongly opposed to the annexation of the southern German states, which would have significantly strengthened the Prussian military.
In Prussia, some officials considered a war against France both inevitable and necessary to arouse German nationalism in those states that would allow the unification of a great German empire. This aim was epitomized by Prussian Chancellor Otto von Bismarck's later statement: "I did not doubt that a Franco-German war must take place before the construction of a United Germany could be realised." Bismarck also knew that France should be regarded as the aggressor in the conflict to bring the southern German states to side with Prussia, hence giving Germans numerical superiority. Many Germans also viewed the French as the traditional destabilizer of Europe, and sought to weaken France to prevent further breaches of the peace.
The immediate cause of the war resided in the candidacy of a Leopold of Hohenzollern-Sigmaringen, a Prussian prince, to the throne of Spain. France feared encirclement by an alliance between Prussia and Spain. The Hohenzollern prince's candidacy was withdrawn under French diplomatic pressure, but Otto von Bismarck goaded the French into declaring war by altering a telegram sent by William I. Releasing the Ems Dispatch to the public, Bismarck made it sound as if the king had treated the French envoy in a demeaning fashion, which inflamed public opinion in France. 
Some historians argue that Napoleon III also sought war, particularly for the diplomatic defeat in 1866 in leveraging any benefits from the Austro-Prussian War, and he believed he would win a conflict with Prussia. They also argue that he wanted a war to resolve growing domestic political problems. Other historians, notably French historian Pierre Milza, dispute this. On 8 May 1870, shortly before the war, French voters had overwhelmingly supported Napoleon III's program in a national plebiscite, with 7,358,000 votes yes against 1,582,000 votes no, an increase of support of two million votes since the legislative elections in 1869. The Emperor had no need for a war to increase his popularity.
The Ems telegram had exactly the effect on French public opinion that Bismarck had intended. "This text produced the effect of a red flag on the Gallic bull", Bismarck later wrote. Gramont, the French foreign minister, declared that he felt "he had just received a slap". The leader of the monarchists in Parliament, Adolphe Thiers, spoke for moderation, arguing that France had won the diplomatic battle and there was no reason for war, but he was drowned out by cries that he was a traitor and a Prussian. Napoleon's new prime minister, Emile Ollivier, declared that France had done all that it could humanly and honorably do to prevent the war, and that he accepted the responsibility "with a light heart." A crowd of 15–20,000 people, carrying flags and patriotic banners, marched through the streets of Paris, demanding war. On 19 July 1870 a declaration of war was sent to the Prussian government. The southern German states immediately sided with Prussia.
Opposing forces.
The French Army consisted in peacetime of approximately 400,000 soldiers, some of them regulars, others conscripts who until 1869 served the comparatively long period of seven years with the colours. Some of them were veterans of previous French campaigns in the Crimean War, Algeria, the Franco–Austrian War in Italy, and in the Franco–Mexican War. However, following the "Seven Weeks War" between Prussia and Austria four years earlier, it had been calculated that the French Army could field only 288,000 men to face the Prussian Army when perhaps 1,000,000 would be required. Under Marshal Adolphe Niel, urgent reforms were made. Universal conscription (rather than by ballot, as previously) and a shorter period of service gave increased numbers of reservists, who would swell the army to a planned strength of 800,000 on mobilisation. Those who for any reason were not conscripted were to be enrolled in the "Garde Mobile", a militia with a nominal strength of 400,000. However, the Franco-Prussian War broke out before these reforms could be completely implemented. The mobilisation of reservists was chaotic and resulted in large numbers of stragglers, while the "Garde Mobile" were generally untrained and often mutinous.
French infantry were equipped with the breech-loading Chassepot rifle, one of the most modern mass-produced firearms in the world at the time. With a rubber ring seal and a smaller bullet, the Chassepot had a maximum effective range of some 1500 m with a short reloading time. French tactics emphasised the defensive use of the Chassepot rifle in trench-warfare style fighting - the so-called "feu de bataillon". The artillery was equipped with rifled, muzzle-loaded La Hitte guns. The army also possessed a precursor to the machine-gun: the mitrailleuse, which could unleash significant, concentrated firepower but nevertheless lacked range and was comparatively immobile, and thus prone to being easily overrun. The mitrailleuse was mounted on an artillery gun carriage and grouped in batteries in a similar fashion to cannon.
The army was nominally led by Napoleon III, with Marshals Francois Achille Bazaine and Patrice de Mac-Mahon in command of the field armies. However, there was no previously arranged plan of campaign in place. The only campaign plan prepared between 1866 and 1870 was a defensive one.
The Prussian Army was composed not of regulars but conscripts. Service was compulsory for all of men of military age, and thus Prussia and its North and South German allies could mobilise and field some 1,000,000 soldiers in time of war. German tactics emphasised encirclement battles like Cannae and using artillery offensively whenever possible. Rather than advancing in a column or line formation, Prussian infantry moved in small groups that were harder to target by artillery or French defensive fire. The sheer number of soldiers available made encirclement "en masse" and destruction of French formations relatively easy.
The army was still equipped with the Dreyse needle gun of Battle of Königgrätz fame, which was by this time showing the age of its 25-year-old design. The rifle had a range of only 600 m and lacked the rubber breech seal that permitted aimed shots. The deficiencies of the needle gun were more than compensated for by the famous Krupp 6-pounder (3 kg) steel breech-loading cannons being issued to Prussian artillery batteries. Firing a contact-detonated shell, the Krupp gun had a longer range and a higher rate of fire than the French bronze muzzle loading cannon, which relied on faulty time fuses.
The Prussian army was controlled by the General Staff, under Field Marshal Helmuth von Moltke. The Prussian army was unique in Europe for having the only such organisation in existence, whose purpose in peacetime was to prepare the overall war strategy, and in wartime to direct operational movement and organise logistics and communications. The officers of the General Staff were hand-picked from the Prussian "Kriegsakademie" (War Academy). Moltke embraced new technology, particularly the railroad and telegraph, to coordinate and accelerate mobilisation of large forces.
French Army incursion.
Preparations for the offensive.
On 28 July 1870 Napoleon III left Paris for Metz and assumed command of the newly titled Army of the Rhine, some 202,448 strong and expected to grow as the French mobilization progressed. Marshal MacMahon took command of I Corps (4 infantry divisions) near Wissembourg, Marshal François Canrobert brought VI Corps (four infantry divisions) to Châlons-sur-Marne in northern France as a reserve and to guard against a Prussian advance through Belgium.
A pre-war plan laid out by the late Marshal Niel called for a strong French offensive from Thionville towards Trier and into the Prussian Rhineland. This plan was discarded in favour of a defensive plan by Generals Charles Frossard and Bartélemy Lebrun, which called for the Army of the Rhine to remain in a defensive posture near the German border and repel any Prussian offensive. As Austria along with Bavaria, Württemberg and Baden were expected to join in a revenge war against Prussia, I Corps would invade the Bavarian Palatinate and proceed to "free" the South German states in concert with Austro-Hungarian forces. VI Corps would reinforce either army as needed.
Unfortunately for Frossard's plan, the Prussian army was mobilizing far more rapidly than expected. The Austro-Hungarians, still smarting after their defeat by Prussia in the Austro–Prussian War, were treading carefully before stating that they would only commit to France's cause if the southern Germans viewed the French positively. This did not materialize as the South German states had come to Prussia's aid and were mobilizing their armies against France.
Occupation of Saarbrücken.
Napoleon III was under immense domestic pressure to launch an offensive before the full might of Moltke's forces was mobilized and deployed. Reconnaissance by Frossard's forces had identified only the Prussian 16th Infantry Division guarding the border town of Saarbrücken, right before the entire Army of the Rhine. Accordingly, on 31 July the Army marched forward toward the Saar River to seize Saarbrücken.
General Frossard's II Corps and Marshal Bazaine's III Corps crossed the German border on 2 August, and began to force the Prussian 40th Regiment of the 16th Infantry Division from the town of Saarbrücken with a series of direct attacks. The Chassepot rifle proved its worth against the Dreyse rifle, with French riflemen regularly outdistancing their Prussian counterparts in the skirmishing around Saarbrücken. However the Prussians resisted strongly, and the French suffered 86 casualties to the Prussian 83 casualties. Saarbrücken also proved to be a major obstacle in terms of logistics. Only one railway there led to the German hinterland but could be easily defended by a single force, and the only river systems in the region ran along the border instead of inland. While the French hailed the invasion as the first step towards the Rhineland and later Berlin, General Le Bœuf and Napoleon III were receiving alarming reports from foreign news sources of Prussian and Bavarian armies massing to the southeast in addition to the forces to the north and northeast.
Moltke had indeed massed three armies in the area—the Prussian First Army with 50,000 men, commanded by General Karl von Steinmetz opposite Saarlouis, the Prussian Second Army with 134,000 men commanded by Prince Friedrich Karl opposite the line Forbach–Spicheren, and the Prussian Third Army with 120,000 men commanded by Crown Prince Friedrich Wilhelm, poised to cross the border at Wissembourg.
Prussian Army advance.
Battle of Wissembourg.
Upon learning from captured Prussian soldiers and a local area police chief, that the Prussian Crown Prince's Third Army was just 30 mi from Saarbrücken near the town of Wissembourg, General Le Bœuf and Napoleon III decided to retreat to defensive positions. General Frossard, without instructions, hastily withdrew the elements of Army of the Rhine in Saarbrücken back to Spicheren and Forbach.
Marshal MacMahon, now closest to Wissembourg, spread his four divisions over 20 mi to react to any Prussian invasion. This organization of forces was due to a lack of supplies, forcing each division to seek out basic provisions along with the representatives of the army supply arm that was supposed to aid them. What made a bad situation much worse was the conduct of General Auguste-Alexandre Ducrot, commander of the 1st Division. He told General Abel Douay, commander of the 2nd Division, on 1 August that "The information I have received makes me suppose that the enemy has no considerable forces very near his advance posts, and has no desire to take the offensive". Two days later, he told MacMahon that he had not found "a single enemy post ... it looks to me as if the menace of the Bavarians is simply bluff". Even though Ducrot shrugged off the possibility of an attack by the Germans, MacMahon tried to warn the other divisions of his army, without success.
The first action of the Franco–Prussian War took place on 4 August 1870. This battle saw the unsupported division of General Douay of I Corps, with some attached cavalry, which was posted to watch the border, attacked in overwhelming but un-coordinated fashion by the German 3rd Army. During the day, elements of a Bavarian and two Prussian corps became engaged and were aided by Prussian artillery, which blasted holes in the defenses of the town. Douay held a very strong position initially, thanks to the accurate long-range fire of the Chassepots but his force was too thinly stretched to hold it. Douay was killed in the late morning when a caisson of the divisional mitrailleuse battery exploded near him; the encirclement of the town by the Prussians threatened the French avenue of retreat.
The fighting within the town had become extremely intense, becoming a door to door battle of survival. Despite a never-ending attack of Prussian infantry, the soldiers of the 2nd Division kept to their positions. The people of the town of Wissembourg finally surrendered to the Germans. The French troops who did not surrender retreated westward, leaving behind 1,000 dead and wounded and another 1,000 prisoners and all of their remaining ammunition. The final attack by the Prussian troops also cost  1,000 casualties. The German cavalry then failed to pursue the French and lost touch with them. The attackers had an initial superiority of numbers, a broad deployment which made envelopment highly likely but the effectiveness of French Chassepot rifle-fire inflicted costly repulses on infantry attacks, until the French infantry had been extensively bombarded by the Prussian artillery.
Battle of Spicheren.
The Battle of Spicheren, on 5 August, was the second of three critical French defeats. Moltke had originally planned to keep Bazaine's army on the Saar River until he could attack it with the 2nd Army in front and the 1st Army on its left flank, while the 3rd Army closed towards the rear. The aging General von Steinmetz made an overzealous, unplanned move, leading the 1st Army south from his position on the Moselle. He moved straight toward the town of Spicheren, cutting off Prince Frederick Charles from his forward cavalry units in the process.
On the French side, planning after the disaster at Wissembourg had become essential. General Le Bœuf, flushed with anger, was intent upon going on the offensive over the Saar and countering their loss. However, planning for the next encounter was more based upon the reality of unfolding events rather than emotion or pride, as Intendant General Wolff told him and his staff that supply beyond the Saar would be impossible. Therefore, the armies of France would take up a defensive position that would protect against every possible attack point, but also left the armies unable to support each other.
While the French army under General MacMahon engaged the German 3rd Army at the Battle of Wörth, the German 1st Army under Steinmetz finished their advance west from Saarbrücken. A patrol from the German 2nd Army under Prince Friedrich Karl of Prussia spotted decoy fires close and Frossard's army farther off on a distant plateau south of the town of Spicheren, and took this as a sign of Frossard's retreat. Ignoring Moltke's plan again, both German armies attacked Frossard's French 2nd Corps, fortified between Spicheren and Forbach.
The French were unaware of German numerical superiority at the beginning of the battle as the German 2nd Army did not attack all at once. Treating the oncoming attacks as merely skirmishes, Frossard did not request additional support from other units. By the time he realized what kind of a force he was opposing, it was too late. Seriously flawed communications between Frossard and those in reserve under Bazaine slowed down so much that by the time the reserves received orders to move out to Spicheren, German soldiers from the 1st and 2nd armies had charged up the heights. Because the reserves had not arrived, Frossard erroneously believed that he was in grave danger of being outflanked as German soldiers under General von Glume were spotted in Forbach. Instead of continuing to defend the heights, by the close of battle after dusk he retreated to the south. The German casualties were relatively high due to the advance and the effectiveness of the chassepot rifle. They were quite startled in the morning when they had found out that their efforts were not in vain–Frossard had abandoned his position on the heights.
Battle of Wörth.
The Battle of Wörth (also known as Fröschwiller or Reichshoffen) began when the two armies clashed again on 6 August near Wörth in the town of Fröschwiller, about 10 mi from Wissembourg. The Crown Prince of Prussia's 3rd army had, on the quick reaction of his Chief of Staff General von Blumenthal, drawn reinforcements which brought its strength up to 140,000 troops. The French had been slowly reinforced and their force numbered only 35,000. Although badly outnumbered, the French defended their position just outside Fröschwiller. By afternoon, the Germans had suffered  10,500 killed or wounded and the French had lost a similar number of casualties and another  9,200 men taken prisoner, a loss of about 50%. The Germans captured Fröschwiller which sat on a hilltop in the centre of the French line. Having lost any hope for victory and facing a massacre, the French army disengaged and retreated in a westerly direction towards Bitche and Saverne, hoping to join French forces on the other side of the Vosges mountains. The German 3rd army did not pursue the French but remained in Alsace and moved slowly south, attacking and destroying the French garrisons in the vicinity.
Battle of Mars-La-Tour.
About 160,000 French soldiers were besieged in the fortress of Metz following the defeats on the frontier. A retirement from Metz to link up with French forces at Châlons, was ordered on 15 August and spotted by a Prussian cavalry patrol under Major Oskar von Blumenthal. Next day a grossly outnumbered Prussian force of 30,000 men of III Corps (of the 2nd Army) under General Konstantin von Alvensleben, found the French Army near Vionville, east of Mars-la-Tour.
Despite odds of four to one, the III Corps launched a risky attack. The French were routed and the III Corps captured Vionville, blocking any further escape attempts to the west. Once blocked from retreat, the French in the fortress of Metz had no choice but to engage in a fight that would see the last major cavalry engagement in Western Europe. The battle soon erupted, and III Corps was shattered by incessant cavalry charges, losing over half its soldiers. The German Official History recorded 15,780 casualties and French casualties of 13,761 men.
On 16 August, the French had a chance to sweep away the key Prussian defense, and to escape. Two Prussian corps had attacked the French advanced guard, thinking that it was the rearguard of the retreat of the French Army of the Meuse. Despite this misjudgment the two Prussian corps held the entire French army for the whole day. Outnumbered 5 to 1, the extraordinary élan of the Prussians prevailed over gross indecision by the French. The French had lost the opportunity to win a decisive victory.
Battle of Gravelotte.
The Battle of Gravelotte, or Gravelotte–St. Privat (18 August), was the largest battle during the Franco–Prussian War. It was fought about 6 mi west of Metz, where on the previous day, having intercepted the French army's retreat to the west at the Battle of Mars-La-Tour, the Prussians were now closing in to complete the destruction of the French forces. The combined German forces, under Field Marshal Count Helmuth von Moltke, were the Prussian First and Second Armies of the North German Confederation numbering about 210 infantry battalions, 133 cavalry squadrons, and 732 heavy cannons totaling 188,332 officers and men. The French Army of the Rhine, commanded by Marshal François-Achille Bazaine, numbering about 183 infantry battalions, 104 cavalry squadrons, backed by 520 heavy cannons, totaling 112,800 officers and men, dug in along high ground with their southern left flank at the town of Rozerieulles, and their northern right flank at St. Privat.
On 18 August, the battle began when at 08:00 Moltke ordered the First and Second Armies to advance against the French positions. By 12:00, General Manstein opened up the battle before the village of Amanvillers with artillery from the 25th Infantry Division. But the French had spent the night and early morning digging trenches and rifle pits while placing their artillery and their mitrailleuses in concealed positions. Finally aware of the Prussian advance, the French opened up a massive return fire against the mass of advancing Germans. The battle at first appeared to favor the French with their superior Chassepot rifle. However, the Prussian artillery was superior with the all-steel Krupp breech-loading gun. By 14:30, General Steinmetz, the commander of the First Army, unilaterally launched his VIII Corps across the Mance Ravine in which the Prussian infantry were soon pinned down by murderous rifle and mitrailleuse fire from the French positions. At 15:00, the massed guns of the VII and VIII Corps opened fire to support the attack. But by 16:00, with the attack in danger of stalling, Steinmetz ordered the VII Corps forward, followed by the 1st Cavalry Division.
By 16:50, with the Prussian southern attacks in danger of breaking up, the Prussian 3rd Guards Infantry Brigade of the Second Army opened an attack against the French positions at St. Privat which were commanded by General Canrobert. At 17:15, the Prussian 4th Guards Infantry Brigade joined the advance followed at 17:45 by the Prussian 1st Guards Infantry Brigade. All of the Prussian Guard attacks were pinned down by lethal French gunfire from the rifle pits and trenches. At 18:15 the Prussian 2nd Guards Infantry Brigade, the last of the 1st Guards Infantry Division, was committed to the attack on St. Privat while Steinmetz committed the last of the reserves of the First Army across the Mance Ravine. By 18:30, a considerable portion of the VII and VIII Corps disengaged from the fighting and withdrew towards the Prussian positions at Rezonville.
With the defeat of the First Army, Prince Frederick Charles ordered a massed artillery attack against Canrobert's position at St. Privat to prevent the Guards attack from failing too. At 19:00 the 3rd Division of Fransecky's II Corps of the Second Army advanced across Ravine while the XII Corps cleared out the nearby town of Roncourt and with the survivors of the 1st Guards Infantry Division launched a fresh attack against the ruins of St. Privat. At 20:00, the arrival of the Prussian 4th Infantry Division of the II Corps and with the Prussian right flank on Mance Ravine, the line stabilised. By then, the Prussians of the 1st Guards Infantry Division and the XII and II Corps captured St. Privat forcing the decimated French forces to withdraw. With the Prussians exhausted from the fighting, the French were now able to mount a counter-attack. General Bourbaki, however, refused to commit the reserves of the French Old Guard to the battle because, by that time, he considered the overall situation a 'defeat'. By 22:00, firing largely died down across the battlefield for the night. The next morning, the French Army of the Rhine, rather than resume the battle with an attack of its own against the battle-weary German armies, retreated to Metz where they were besieged and forced to surrender two months later.
The casualties were horrible, especially for the attacking Prussian forces. A grand total of 20,163 German troops were killed, wounded or missing in action during the August 18 battle. The French losses were 7,855 killed and wounded along with 4,420 prisoners of war (half of them were wounded) for a total of 12,275. While most of the Prussians fell under the French Chassepot rifles, most French fell under the Prussian Krupp shells. In a breakdown of the casualties, Frossard's II Corps of the Army of the Rhine suffered 621 casualties while inflicting 4,300 casualties on the Prussian First Army under Steinmetz before the Pointe du Jour. The Prussian Guards Infantry Divisions losses were even more staggering with 8,000 casualties out of 18,000 men. The Special Guards Jäger lost 19 officers, a surgeon and 431 men out of a total of 700. The 2nd Guards Infantry Brigade lost 39 officers and 1,076 men. The 3rd Guards Infantry Brigade lost 36 officers and 1,060 men. On the French side, the units holding St. Privat lost more than half their number in the village.
Siege of Metz.
With the defeat of Marshal Bazaine's Army of the Rhine at Gravelotte, the French were forced to retire to Metz, where they were besieged by over 150,000 Prussian troops of the First and Second Armies. Napoleon III and MacMahon formed the new French Army of Châlons, to march on to Metz to rescue Bazaine. Napoleon III personally led the army with Marshal MacMahon in attendance. The Army of Châlons marched north-east towards the Belgian border to avoid the Prussians before striking south to link up with Bazaine. The Prussians, under the command of Field Marshal Count Helmuth von Moltke, took advantage of this maneuver to catch the French in a pincer grip. He left the Prussian First and Second Armies besieging Metz, except three corps detached to form the Army of the Meuse under the Crown Prince of Saxony. With this army and the Prussian Third Army, Moltke marched northward and caught up with the French at Beaumont on 30 August. After a sharp fight in which they lost 5,000 men and 40 cannons, the French withdrew toward Sedan. Having reformed in the town, the Army of Châlons was immediately isolated by the converging Prussian armies. Napoleon III ordered the army to break out of the encirclement immediately. With MacMahon wounded on the previous day, General Auguste Ducrot took command of the French troops in the field.
Battle of Sedan.
On 1 September 1870, the battle opened with the Army of Châlons, with 202 infantry battalions, 80 cavalry squadrons and 564 guns, attacking the surrounding Prussian Third and Meuse Armies totaling 222 infantry battalions, 186 cavalry squadrons and 774 guns. General De Wimpffen, the commander of the French V Corps in reserve, hoped to launch a combined infantry and cavalry attack against the Prussian XI Corps. But by 11:00, Prussian artillery took a toll on the French while more Prussian troops arrived on the battlefield. The French cavalry, commanded by General Marguerite, launched three desperate attacks on the nearby village of Floing where the Prussian XI Corps was concentrated. Marguerite was killed leading the very first charge and the two additional charges led to nothing but heavy losses. By the end of the day, with no hope of breaking out, Napoleon III called off the attacks. The French lost over 17,000 men, killed or wounded, with 21,000 captured. The Prussians reported their losses at 2,320 killed, 5,980 wounded and 700 captured or missing. By the next day, on 2 September, Napoleon III surrendered and was taken prisoner with 104,000 of his soldiers. It was an overwhelming victory for the Prussians, for they not only captured an entire French army, but the leader of France as well. The defeat of the French at Sedan had decided the war in Prussia's favour. One French army was now immobilised and besieged in the city of Metz, and no other forces stood on French ground to prevent a German invasion. Nevertheless, the war would continue.
The war of the Government of National Defence.
Government of National Defence.
When the news arrived at Paris of the surrender at Sedan of Napoleon III and 80,000 men, the Second Empire was overthrown by a popular uprising in Paris, which forced the proclamation of a Provisional Government and a Third Republic by general Trochu, Favre and Gambetta at Paris on 4 September, the new government calling itself the Government of National Defence. After the German victory at Sedan, most of the French standing army was either besieged in Metz or prisoner of the Germans, who hoped for an armistice and an end to the war. Bismarck wanted an early peace but had difficulty in finding a legitimate French authority with which to negotiate. The Government of National Defence had no electoral mandate, the Emperor was a captive and the Empress in exile but there had been no abdication "de jure" and the army was still bound by an oath of allegiance to the defunct imperial régime.
The Germans expected to negotiate an end to the war but immediately ordered an advance on Paris; by 15 September Moltke issued the orders for an investment of Paris and on 20 September the encirclement was complete. Bismarck met Favre on 18 September at the Château de Ferrières and demanded a frontier immune to a French war of revenge, which included Strasbourg, Alsace and most the Moselle department in Lorraine of which Metz was the capital. In return for an armistice for the French to elect a National Assembly, Bismarck demanded the surrender of Strasbourg and the fortress city of Toul. To allow supplies into Paris, one of the perimeter forts had to be handed over. Favre was unaware that the real aim of Bismarck in making such extortionate demands was to establish a durable peace on the new western frontier of Germany, preferably by a peace with a friendly government, on terms acceptable to French public opinion. An impregnable military frontier was an inferior alternative to him, favoured only by the militant nationalists on the German side.
While the republican government was amenable to war reparations or ceding colonial territories in Africa or in South East Asia to Prussia, Favre on behalf of the Government of National Defense, declared on 6 September that France would not "yield an inch of its territory nor a stone of its fortresses." The republic then renewed the declaration of war, called for recruits in all parts of the country and pledged to drive the German troops out of France by a "guerre à outrance". Under these circumstances, the Germans had to continue the war, yet could not pin down any proper military opposition in their vicinity. As the bulk of the remaining French armies were digging-in near Paris, the German leaders decided to put pressure upon the enemy by attacking Paris. By September 15, German troops reached the outskirts of the fortified city. On September 19, the Germans surrounded it and erected a blockade, as already established at Metz.
When the war had begun, European public opinion heavily favored the Germans; many Italians attempted to sign up as volunteers at the Prussian embassy in Florence and a Prussian diplomat visited Giuseppe Garibaldi in Caprera. Bismarck's demand for the return of Alsace caused a dramatic shift in that sentiment in Italy, which was best exemplified by the reaction of Garibaldi soon after the revolution in Paris, who told the "Movimento" of Genoa on 7 September 1870 that "Yesterday I said to you: war to the death to Bonaparte. Today I say to you: rescue the French Republic by every means." Garibaldi went to France and assumed command of the Army of the Vosges.
Siege of Paris.
Prussian forces commenced the Siege of Paris on 19 September 1870. Faced with the blockade, the new French government called for the establishment of several large armies in the French provinces. These new bodies of troops were to march towards Paris and attack the Germans there from various directions at the same time. Armed French civilians were to create a guerilla force—the so-called "Francs-tireurs"—for the purpose of attacking German supply lines.
These developments prompted calls from the German public for a bombardment of the city. Von Blumenthal, who commanded the siege, was opposed to the bombardment on moral grounds. In this he was backed by other senior military figures such as the Crown Prince and Moltke.
Loire campaign.
Dispatched from Paris as the republican government emissary, Léon Gambetta flew over the German lines in a balloon inflated with coal gas from the city's gasworks and organized the recruitment of the Armée de la Loire. Rumors about an alleged German "extermination" plan infuriated the French and strengthened their support of the new regime. Within a few weeks, five new armies totalling more than 500,000 troops were recruited.
The Germans dispatched some of their troops to the French provinces to detect, attack and disperse the new French armies before they could become a menace. The Germans were not prepared for an occupation of the whole of France. 
On 10 October, hostilities began between German and French republican forces near Orléans. At first, the Germans were victorious but the French drew reinforcements and defeated the Germans at the Battle of Coulmiers on 9 November. After the surrender of Metz, more than 100,000 well-trained and experienced German troops joined the German 'Southern Army'. The French were forced to abandon Orléans on 4 December, to be finally defeated at the Battle of Le Mans (10–12 January). A second French army which operated north of Paris, was turned back at the Battle of Amiens (27 November), the Battle of Bapaume (3 January 1871) and the Battle of St. Quentin (13 January).
Northern campaign.
Following the Army of the Loire's defeats, Gambetta turned to general Faidherbe's Army of the North. The army had achieved several small victories at towns such as Ham, La Hallue, and Amiens and was protected by the belt of fortresses in northern France, allowing Faidherbe's men to launch quick attacks against isolated Prussian units, then retreat behind the fortresses. Despite access to the armaments factories of Lille, the Army of the North suffered from severe supply difficulties, which depressed morale. In January 1871, Gambetta forced Faidherbe to march his army beyond the fortresses and engage the Prussians in open battle. The army was severely weakened by low morale, supply problems, the terrible winter weather and low troop quality, whilst general Faidherbe was unable to command due to his poor health, the result of decades of campaigning in West Africa. At the Battle of St. Quentin, the Army of the North suffered a crushing defeat and was scattered, releasing thousands of Prussian soldiers to be relocated to the East.
Eastern campaign.
Following the destruction of the French Army of the Loire, remnants of the Loire army gathered in eastern France to form the Army of the East, commanded by general Charles-Denis Bourbaki. In a final attempt to cut the German supply lines in north-east France, Bourbaki's army marched north to attack the Prussian siege of Belfort and relieve the defenders.
In the battle of the Lisaine, Bourbaki's men failed to break through German lines commanded by General August von Werder. Bringing in the German 'Southern Army', General von Manteuffel then drove Bourbaki's army into the mountains near the Swiss border. Facing annihilation, the last intact French army crossed the border and was disarmed and interned by the neutral Swiss near Pontarlier (1 February).
Armistice.
On 28 January 1871 the Government of National Defense based in Paris negotiated an armistice with the Prussians. With Paris starving, and Gambetta's provincial armies reeling from one disaster after another, French foreign minister Favre went to Versailles on 24 January to discuss peace terms with Bismarck. Bismarck agreed to end the siege and allow food convoys to immediately enter Paris (including trains carrying millions of German army rations), on condition that the Government of National Defence surrender several key fortresses outside Paris to the Prussians. Without the forts, the French Army would no longer be able to defend Paris.
Although public opinion in Paris was strongly against any form of surrender or concession to the Prussians, the Government realised that it could not hold the city for much longer, and that Gambetta's provincial armies would probably never break through to relieve Paris. President Trochu resigned on 25 January and was replaced by Favre, who signed the surrender two days later at Versailles, with the armistice coming into effect at midnight. Several sources claim that in his carriage on the way back to Paris, Favre broke into tears, and collapsed into his daughter's arms as the guns around Paris fell silent at midnight. At Tours, Gambetta received word from Paris on 30 January that the Government had surrendered. Furious, he refused to surrender and launched an immediate attack on German forces at Orleans which, predictably, failed. A delegation of Parisian diplomats arrived in Tours by train on 5 February to negotiate with Gambetta, and the following day Gambetta stepped down and surrendered control of the provincial armies to the Government of National Defence, which promptly ordered a cease-fire across France.
The war at sea.
Blockade.
When the war began, the French government ordered a blockade of the North German coasts, which the small North German navy ("Norddeutsche Bundesmarine") with only five ironclads could do little to oppose. For most of the war, the three largest German ironclads were out of service with engine troubles; only the turret ship was available to conduct operations. By the time engine repairs had been completed, the French fleet had already departed. The blockade proved only partially successful due to crucial oversights by the planners in Paris. Reservists that were supposed to be at the ready in case of war, were working in the Newfoundland fisheries or in Scotland. Only part of the 470-ship French Navy put to sea on 24 July. Before long, the French navy ran short of coal, needing 200 ST per day and having a bunker capacity in the fleet of only 250 ST. A blockade of Wilhelmshaven failed and conflicting orders about operations in the Baltic Sea or a return to France, made the French naval efforts futile. Spotting a blockade-runner became unwelcome because of the "question du charbon"; pursuit of Prussian ships quickly depleted the coal reserves of the French ships.
To relieve pressure from the expected German attack into Alsace-Lorraine, Napoleon III and the French high command planned an seaborne invasion of northern Germany as soon as war began. The French expected the invasion to divert German troops and to encourage Denmark to join in the war, with its 50,000-strong army and the Royal Danish Navy. It was discovered that Prussia had recently built defences around the big North German ports, including coastal artillery batteries with Krupp heavy artillery, which with a range of 4000 yd, had double the range of French naval guns. The French Navy lacked the heavy guns to engage the coastal defences and the topography of the Prussian coast made a seaborne invasion of northern Germany impossible.
The French Marines and naval infantry intended for the invasion of northern Germany were dispatched to reinforce the French Army of Châlons and fell into captivity at Sedan along with Napoleon III. A shortage of officers, following the capture of most of the professional French army at the Siege of Metz and at the Battle of Sedan, led naval officers to be sent from their ships to command hastily assembled reservists of the "Garde Mobile". As the autumn storms of the North Sea forced the return of more of the French ships, the blockade of the north German ports diminished and in September 1870 the French navy abandoned the blockade for the winter. The rest of the navy retired to ports along the English Channel and remained in port for the rest of the war.
Pacific and Caribbean.
Outside Europe, the French corvette "Dupleix" blockaded the German corvette SMS "Hertha" in Nagasaki and the Battle of Havana took place between the Prussian gunboat SMS "Meteor" and the French aviso "Bouvet" off Havana, Cuba, in November 1870.
Aftermath.
Analysis.
The quick German victory over the French stunned neutral observers, many of whom had expected a French victory and most of whom had expected a long war. The strategic advantages possessed by the Germans were not appreciated outside Germany until after hostilities had ceased. Other countries quickly discerned the advantages given to the Germans by their military system, and adopted many of their innovations, particularly the General Staff, universal conscription and highly detailed mobilization systems.
The Prussian General Staff developed by Moltke proved to be extremely effective, in contrast to the traditional French school. This was in large part due to the fact that the Prussian General Staff was created to study previous Prussian operations and learn to avoid mistakes. The structure also greatly strengthened Moltke's ability to control large formations spread out over significant distances. The Chief of the General Staff, effectively the commander in chief of the Prussian army, was independent of the minister of war and answered only to the monarch. The French General Staff—along with those of every other European military—was little better than a collection of assistants for the line commanders. This disorganization hampered the French commanders' ability to exercise control of their forces.
In addition, the Prussian military education system was superior to the French model; Prussian staff officers were trained to exhibit initiative and independent thinking. Indeed, this was Moltke's expectation. The French, meanwhile, suffered from an education and promotion system that stifled intellectual development. According to the military historian Dallas Irvine, the system "was almost completely effective in excluding the army's brain power from the staff and high command. To the resulting lack of intelligence at the top can be ascribed all the inexcusable defects of French military policy."
Albrecht von Roon, the Prussian Minister of War from 1859 to 1873, put into effect a series of reforms of the Prussian military system in the 1860s. Among these were two major reforms that substantially increased the military power of Germany. The first was a reorganization of the army that integrated the regular army and the "Landwehr" reserves. The second was the provision for the conscription of every male Prussian of military age in the event of mobilization. Thus, despite the population of France being greater than the population of all of the German states that participated in the war, the Germans mobilized more soldiers for battle.
At the outset of the Franco–Prussian War, 462,000 German soldiers concentrated on the French frontier while only 270,000 French soldiers could be moved to face them, the French army having lost 100,000 stragglers before a shot was fired through poor planning and administration. This was partly due to the peacetime organisations of the armies. Each Prussian Corps was based within a "Kreis" (literally "circle") around the chief city in an area. Reservists rarely lived more than a day's travel from their regiment's depot. By contrast, French regiments generally served far from their depots, which in turn were not in the areas of France from which their soldiers were drawn. Reservists often faced several days' journey to report to their depots, and then another long journey to join their regiments. Large numbers of reservists choked railway stations, vainly seeking rations and orders.
The effect of these differences was accentuated by the pre-war preparations. The Prussian General Staff had drawn up minutely detailed mobilization plans using the railway system, which in turn had been partly laid out in response to recommendations of a Railway Section within the General Staff. The French railway system, with multiple competing companies, had developed purely from commercial pressures and many journeys to the front in Alsace and Lorraine involved long diversions and frequent changes between trains. Furthermore, no system had been put in place for military control of the railways, and officers simply commandeered trains as they saw fit. Rail sidings and marshalling yards became choked with loaded wagons, with nobody responsible for unloading them or directing them to the destination.
Although Austria-Hungary and Denmark had both wished to avenge their recent military defeats against Prussia, they chose not to intervene in the war due to a lack of confidence in the French. Napoleon III also failed to cultivate alliances with the Russian Empire and the United Kingdom, partially due to the diplomatic efforts of the Prussian chancellor Otto von Bismarck, and thus faced the German states alone.
The French breech-loading rifle, the Chassepot, had a far longer range than the German needle gun; 1500 yd compared to 600 yd. The French also had an early machine-gun type weapon, the mitrailleuse, which could fire its thirty-seven barrels at a range of around 1200 yd. It was developed in such secrecy, that little training with the weapon had occurred, leaving French gunners with no experience; the gun was treated like artillery and in this role it was ineffective. Worse still, once the small number of soldiers who had been trained how to use the new weapon became casualties, there were no replacements who knew how to operate the mitrailleuse.
The French were equipped with bronze, rifled muzzle-loading artillery, while the Prussians used new steel breech-loading guns, which had a far longer range and a faster rate of fire. Prussian gunners strove for a high rate of fire, which was discouraged in the French army in the belief that it wasted ammunition. In addition, the Prussian artillery batteries had 30% more guns than their French counterparts. The Prussian guns typically opened fire at a range of 2 –, beyond the range of French artillery or the Chassepot rifle. The Prussian batteries could thus destroy French artillery with impunity, before being moved forward to directly support infantry attacks.
Effects on military thought.
The events of the Franco-Prussian War had great influence on military thinking over the next forty years. Lessons drawn from the war included the need for a general staff system, the scale and duration of future wars and the tactical use of artillery and cavalry. The bold use of artillery by the Prussians, to silence French guns at long range and then to directly support infantry attacks at close range, proved to be superior to the defensive doctrine employed by French gunners. The Prussian tactics were adopted by European armies by 1914, exemplified in the French 75, an artillery piece optimised to provide direct fire support to advancing infantry. Most European armies ignored the evidence of the Russo-Japanese War of 1904–05 which suggested that infantry armed with new smokeless-powder rifles, could engage gun crews effectively. This forced gunners to fire at longer range using indirect fire, usually from a position of cover.
At the Battle of Mars-la-Tours, the Prussian 12th Cavalry Brigade, commanded by General Adalbert von Bredow, conducted a charge against a French artillery battery. The attack was a costly success and came to be known as "von Bredow's Death Ride", which was held to prove that cavalry charges could still prevail on the battlefield. Use of traditional cavalry on the battlefields of 1914 proved to be disastrous, due to accurate, long-range rifle fire, machine-guns and artillery. Von Bredow's attack had succeeded only because of an unusually effective artillery bombardment just before the charge, along with favorable terrain that masked his approach.
Subsequent events.
Prussian reaction and withdrawal.
The Prussian Army, under the terms of the armistice, held a brief victory parade in Paris on 17 February; the city was silent and draped with black and the Germans quickly withdrew. Bismarck honoured the armistice, by allowing train loads of food into Paris and withdrawing Prussian forces to the east of the city, prior to a full withdrawal once France agreed to pay a 5,000 million franc war indemnity. At the same time, Prussian forces were concentrated in the provinces of Alsace and Lorraine. An exodus occurred from Paris as some 200,000 people, predominantly middle-class, went to the countryside.
The Paris Commune.
During the war, the Paris National Guard, particularly in the working-class neighbourhoods of Paris, had become highly politicised and units elected officers; many refused to wear uniform or obey commands from the national government. National guard units tried to seize power in Paris on 31 October 1870 and 22 January 1871. On 18 March 1871, when the regular army tried to remove cannons from an artillery park on Montmartre, National Guard units resisted and killed two army generals. The national government and regular army forces retreated to Versailles and a revolutionary government was proclaimed in Paris. A Commune was elected, which was dominated by socialists, anarchists and revolutionaries. The red flag replaced the French tricolour and a civil war began between the Commune and the regular army, which attacked and recaptured Paris from 21–28 May in "La Semaine Sanglante" (Bloody week). 
During the fighting, the Communards killed  500 people, including the Archbishop of Paris, and burned down many government buildings, including the Tuileries Palace and the Hotel de Ville. Communards captured with weapons were routinely shot by the army and Government troops killed from 7,000–30,000 Communards in the fighting and in massacres of men women and children during and after the Commune. More recent histories, based on studies of the number buried in Paris cemeteries and in mass graves after the fall of the Commune, put the number killed at between 6,000 and 10,000. Twenty-six courts were established to try more than 40,000 people who had been arrested, which took until 1875 and imposed 95 death sentences, of which 23 were inflicted. Forced labour for life was imposed on 251 people, 1,160 people were transported to "a fortified place" and 3,417 people were transported. About 20,000 Communards were held in prison hulks until released in 1872 and a great many Communards fled abroad to England, Switzerland, Belgium or the United States. The survivors were amnestied by a bill introduced by Gambetta in 1880 and allowed to return.
German unification and power.
The creation of a unified German Empire ended the balance of power that had been created with the Congress of Vienna after the end of the Napoleonic Wars. Germany had established itself as the main power in continental Europe with the most powerful and professional army in the world. Although Great Britain remained the dominant world power, British involvement in European affairs during the late 19th century was very limited, allowing Germany to exercise great influence over the European mainland. Besides, the Crown Prince's marriage with the daughter of Queen Victoria was only the most prominent of several German–British relationships.
Poland.
In the Prussian province of Posen, with a large Polish population, there was strong support for the French and angry demonstrations at news of Prussian-German victories—a clear manifestation of Polish nationalist feeling. Calls were also made for Polish recruits to desert from the Prussian Army—though these went mainly unheeded. An alarming report on the Posen situation, sent to Bismarck on 16 August 1870, led to the quartering of reserve troop contingents in the restive province. The Franco–Prussian War thus turned out to be a significant event also in German–Polish relations, marking the beginning of a prolonged period of repressive measures by the authorities and efforts at Germanisation.
References.
</dl>
Further reading.
</dl>
French and German studies.
</dl>

</doc>
<doc id="44041" url="http://en.wikipedia.org/wiki?curid=44041" title="Solvation">
Solvation

Solvation, also sometimes called dissolution, is the process of attraction and association of molecules of a solvent with molecules or ions of a solute. As ions dissolve in a solvent they spread out and become surrounded by solvent molecules. Solvation is the process of surrounding solute with solvent. It involves evening out a concentration gradient and evenly distributing the solute within the solvent.
Distinction between solvation, dissolution and solubility.
By an IUPAC definition, solvation is an interaction of a solute with the solvent, which leads to stabilization of the solute species in the solution. One may also refer to the solvated state, whereby an ion in a solution is surrounded or complexed by solvent molecules (see solvation shell). The concept of the solvation interaction can also be applied to an insoluble material, for example, solvation of functional groups on a surface of ion-exchange resin.
Solvation is, in concept, distinct from dissolution and solubility. Dissolution is a kinetic process, and is quantified by its rate. Solubility quantifies the dynamic equilibrium state achieved when the rate of dissolution equals the rate of precipitation. 
The consideration of the units makes the distinction clearer. Complexation can be described by coordination number and the complex stability constants. The typical unit for dissolution rate is mol/s. The unit for solubility can be mol/kg.
Liquefaction accompanied by an irreversible chemical change is also distinct from solvation. For example, zinc cannot be solvated by hydrochloric acid, but it can be converted into the soluble salt zinc chloride by a chemical reaction.
Solvents and intermolecular interactions.
Polar solvents are those with a molecular structure that contains dipoles. Such compounds are often found to have a high dielectric constant. The polar molecules of these solvents can solvate ions because they can orient the appropriate partially charged portion of the molecule towards the ion in response to electrostatic attraction. This stabilizes the system and creates a solvation shell (or hydration shell in the case of water). Water is the most common and well-studied polar solvent, but others exist, such as ethanol, methanol, acetone, acetonitrile, dimethyl sulfoxide, and propylene carbonate. These solvents can be used to dissolve inorganic compounds such as salts.
Solvation involves different types of intermolecular interactions: hydrogen bonding, ion-dipole, and dipole-dipole attractions or van der Waals forces. The hydrogen bonding, ion-dipole, and dipole-dipole interactions occur only in polar solvents. Ion-ion interactions occur only in ionic solvents. The solvation process will be thermodynamically favored only if the overall Gibbs energy of the solution is decreased, compared to the Gibbs energy of the separated solvent and solid (or gas or liquid). This means that the change in enthalpy minus the change in entropy (multiplied by the absolute temperature) is a negative value, or that the Gibbs free energy of the system decreases.
The conductivity of a solution depends on the solvation of its ions. Hydration also affects electronic and vibrational properties of biomolecules.
Thermodynamic considerations.
For solvation to occur, energy is required to release individual ions and molecules from the crystal lattices in which they are present. This is necessary to break the attractions the ions have with each other and is equal to the solid's lattice free energy (the energy released at the formation of the lattice as the ions bonded with each other). The energy for this comes from the energy released when ions of the lattice associate with molecules of the solvent. Energy released in this form is called the free energy of solvation.
The enthalpy of solution is the solution enthalpy minus the enthalpy of the separate systems, whereas the entropy is the corresponding difference in entropy. Most gases have a negative enthalpy of solution. A negative enthalpy of solution means that the solute is less soluble at high temperatures.
Although early thinking was that a higher ratio of a cation's ion charge to ionic radius, or the charge density, resulted in more solvation, this does not stand up to scrutiny for ions like iron(III) or lanthanides and actinides, which are readily hydrolyzed to form insoluble (hydrous) oxides. As these are solids, it is apparent that they are not solvated.
Enthalpy of solvation can help explain why solvation occurs with some ionic lattices but not with others. The difference in energy between that which is necessary to release an ion from its lattice and the energy given off when it combines with a solvent molecule is called the enthalpy change of solution. A negative value for the enthalpy change of solution corresponds to an ion that is likely to dissolve, whereas a high positive value means that solvation will not occur. It is possible that an ion will dissolve even if it has a positive enthalpy value. The extra energy required comes from the increase in entropy that results when the ion dissolves. The introduction of entropy makes it harder to determine by calculation alone whether a substance will dissolve or not. A quantitative measure for solvation power of solvents is given by donor numbers.
In general, thermodynamic analysis of solutions is done by modeling them as reactions. For example; if you add sodium chloride(s) to water, the salt will dissociate into the ions sodium(+aq) and chloride(-aq). The equilibrium constant for this dissociation can be predicted by the change in Gibbs free energy of this reaction.
Max Born developed the first quantitative model for solvation of ionic compounds.

</doc>
<doc id="44042" url="http://en.wikipedia.org/wiki?curid=44042" title="Rubi">
Rubi

Rubi may refer to:

</doc>
<doc id="44043" url="http://en.wikipedia.org/wiki?curid=44043" title="Rawa">
Rawa

Rawa may refer to:

</doc>
<doc id="44044" url="http://en.wikipedia.org/wiki?curid=44044" title="Oceanography">
Oceanography

Oceanography (compound of the Greek words ὠκεανός meaning "ocean" and γράφω meaning "write"), also known as oceanology and marine science, is the branch of Earth science that studies the ocean. It covers a wide range of topics, including marine organisms and ecosystem dynamics; ocean currents, waves, and geophysical fluid dynamics; plate tectonics and the geology of the sea floor; and fluxes of various chemical substances and physical properties within the ocean and across its boundaries. These diverse topics reflect multiple disciplines that oceanographers blend to further knowledge of the world ocean and understanding of processes within: astronomy, biology, chemistry, climatology, geography, geology, hydrology, meteorology and physics. Paleoceanography studies the history of the oceans in the geologic past.
History.
Early history.
Humans first acquired knowledge of the waves and currents of the seas and oceans in pre-historic times. Observations on tides were recorded by Aristotle and Strabo. Early exploration of the oceans was primarily for cartography and mainly limited to its surfaces and of the animals that fishermen brought up in nets, though depth soundings by lead line were taken.
Although Juan Ponce de León in 1513 first identified the Gulf Stream, and the current was well-known to mariners, Benjamin Franklin made the first scientific study of it and gave it its name. Franklin measured water temperatures during several Atlantic crossings and correctly explained the Gulf Stream's cause. Franklin and Timothy Folger printed the first map of the Gulf Stream in 1769-1770.
Information on the currents of the Pacific Ocean was gathered by explorers of the late 18th century, including James Cook and Louis Antoine de Bougainville. James Rennell wrote the first scientific textbooks on oceanography, detailing the current flows of the Atlantic and Indian oceans. During a voyage around the Cape of Good Hope in 1777, he mapped "the banks and currents at the Lagullas". He was also the first to understand the nature of the intermittent current near the Isles of Scilly, (now known as Rennell's Current).
Sir James Clark Ross took the first modern sounding in deep sea in 1840, and Charles Darwin published a paper on reefs and the formation of atolls as a result of the Second voyage of HMS Beagle in 1831-6. Robert FitzRoy published a four-volume report of the Beagle's three voyages. In 1841–1842 Edward Forbes undertook dredging in the Aegean Sea that founded marine ecology.
The first superintendent of the United States Naval Observatory (1842–1861), Matthew Fontaine Maury devoted his time to the study of marine meteorology, navigation, and charting prevailing winds and currents. His 1855 textbook "Physical Geography of the Sea" was one of the first comprehensive oceanography studies. Many nations sent oceanographic observations to Maury at the Naval Observatory, where he and his colleagues evaluated the information and distributed the results worldwide.
Modern oceanography.
Despite all this, human knowledge of the oceans remained confined to the topmost few fathoms of the water and a small amount of the bottom, mainly in shallow areas. Almost nothing was known of the ocean depths. The Royal Navy's efforts to chart all of the world's coastlines in the mid-19th century reinforced the vague idea that most of the ocean was very deep, although little more was known. As exploration ignited both popular and scientific interest in the polar regions and Africa, so too did the mysteries of the unexplored oceans.
The seminal event in the founding of the modern science of oceanography was the 1872-76 Challenger expedition. As the first true oceanographic cruise, this expedition laid the groundwork for an entire academic and research discipline. In response to a recommendation from the Royal Society, The British Government announced in 1871 an expedition to explore world's oceans and conduct appropriate scientific investigation. Charles Wyville Thompson and Sir John Murray launched the Challenger expedition. The Challenger, leased from the Royal Navy, was modified for scientific work and equipped with separate laboratories for natural history and chemistry. Under the scientific supervision of Thomson, Challenger travelled nearly 70000 nmi surveying and exploring. On her journey circumnavigating the globe, 492 deep sea soundings, 133 bottom dredges, 151 open water trawls and 263 serial water temperature observations were taken. Around 4,700 new species of marine life were discovered. The result was the "Report Of The Scientific Results of the Exploring Voyage of H.M.S. Challenger during the years 1873-76". Murray, who supervised the publication, described the report as "the greatest advance in the knowledge of our planet since the celebrated discoveries of the fifteenth and sixteenth centuries". He went on to found the academic discipline of oceanography at the University of Edinburgh, which remained the centre for oceanographic research well into the 20th century. Murray was the first to study marine trenches and in particular the Mid-Atlantic Ridge, and map the sedimentary deposits in the oceans. He tried to map out the world's ocean currents based on salinity and temperature observations, and was the first to correctly understand the nature of coral reef development.
In the late 19th century, other Western nations also sent out scientific expeditions (as did private individuals and institutions). The first purpose built oceanographic ship, the Albatros, was built in 1882. In 1893, Fridtjof Nansen allowed his ship, Fram, to be frozen in the Arctic ice. This enabled him to obtain oceanographic, meteorological and astronomical data at a stationary spot over an extended period.
Between 1907 and 1911 Otto Krümmel published the "Handbuch der Ozeanographie", which became influential in awakening public interest in oceanography. The four-month 1910 North Atlantic expedition headed by John Murray and Johan Hjort was the most ambitious research oceanographic and marine zoological project ever mounted until then, and led to the classic 1912 book "The Depths of the Ocean".
The first acoustic measurement of sea depth was made in 1914. Between 1925 and 1927 the "Meteor" expedition gathered 70,000 ocean depth measurements using an echo sounder, surveying the Mid-Atlantic ridge.
Sverdrup, Johnson and Fleming published "The Oceans" in 1942, which was a major landmark. "The Sea" (in three volumes, covering physical oceanography, seawater and geology) edited by M.N. Hill was published in 1962, while Rhodes Fairbridge's "Encyclopedia of Oceanography" was published in 1966.
The Great Global Rift, running along the Mid Atlantic Ridge, was discovered by Maurice Ewing and Bruce Heezen in 1953; in 1954 a mountain range under the Arctic Ocean was found by the Arctic Institute of the USSR. The theory of seafloor spreading was developed in 1960 by Harry Hammond Hess. The Ocean Drilling Project started in 1966. Deep sea vents were discovered in 1977 by John Corlis and Robert Ballard in the submersible DSV "Alvin".
In the 1950s, Auguste Piccard invented the bathyscaphe and used the "Trieste" to investigate the ocean's depths. The United States nuclear submarine Nautilus made the first journey under the ice to the North Pole in 1958. In 1962 the FLIP (Floating Instrument Platform), a 355-foot spar buoy, was first deployed.
From the 1970s, there has been much emphasis on the application of large scale computers to oceanography to allow numerical predictions of ocean conditions and as a part of overall environmental change prediction. An oceanographic buoy array was established in the Pacific to allow prediction of El Niño events.
1990 saw the start of the World Ocean Circulation Experiment (WOCE) which continued until 2002. Geosat seafloor mapping data became available in 1995.
In recent years studies advanced particular knowledge on ocean acidification, ocean heat content, ocean currents, the El Niño phenomenon, mapping of methane hydrate deposits, the carbon cycle, coastal erosion, weathering and climate feedbacks in regards to climate change interactions.
Study of the oceans is linked to understanding global climate changes, potential global warming and related biosphere concerns. The atmosphere and ocean are linked because of evaporation and precipitation as well as thermal flux (and solar insolation). Wind stress is a major driver of ocean currents while the ocean is a sink for atmospheric carbon dioxide. All these factors relate to the ocean's biogeochemical setup.
Ocean acidification.
Ocean acidification describes the decrease in ocean pH that is caused by anthropogenic carbon dioxide (CO2) emissions into the atmosphere. Seawater is slightly alkaline and had a preindustrial pH of about 8.2. More recently, anthropogenic activities have steadily increased the carbon dioxide content of the atmosphere; about 30–40% of the added CO2 is absorbed by the oceans, forming carbonic acid and lowering the pH (now below 8.1) through ocean acidification. The pH is expected to reach 7.7 by the year 2100.
An important element for the skeletons of marine animals is calcium, but calcium carbonate becomes more soluble with pressure, so carbonate shells and skeletons dissolve below the carbonate compensation depth. Calcium carbonate becomes more soluble at lower pH, so ocean acidification is likely to affect marine organisms with calcareous shells, such as oysters, clams, sea urchins and corals, and the carbonate compensation depth will rise closer to the sea surface. Affected planktonic organisms will include pteropods, coccolithophorids and foraminifera, all important in the food chain. In tropical regions, corals are likely to be severely affected as they become less able to build their calcium carbonate skeletons, in turn adversely impacting other reef dwellers.
The current rate of ocean chemistry change seems to be unprecedented in Earth's geological history, making it unclear how well marine ecosystems will adapt to the shifting conditions of the near future. Of particular concern is the manner in which the combination of acidification with the expected additional stressors of higher temperatures and lower oxygen levels will impact the seas.
Ocean currents.
Since the early ocean expeditions in oceanography, a major interest was the study of the ocean currents and temperature measurements. The tides, the Coriolis effect, changes in direction and strength of wind, salinity and temperature are the main factors determining ocean currents. The thermohaline circulation (THC) "thermo-" referring to temperature and "-haline" referring to salt content connects 4 of 5 ocean basins and is primarily dependent on the density of sea water. Ocean currents such as the Gulf Stream are wind-driven surface currents.
Ocean heat content.
Oceanic heat content (OHC) refers to the heat stored in the ocean. The changes in the ocean heat play an important role in sea level rise, because of thermal expansion. Ocean warming accounts for 90% of the energy accumulation from global warming between 1971 and 2010.
Branches.
The study of oceanography is divided into branches:
These branches reflect the fact that many oceanographers are first trained in the exact sciences or mathematics and then focus on applying their interdisciplinary knowledge, skills and abilities to oceanography.
Data derived from the work of Oceanographers is used in marine engineering, in the design and building of oil platforms, ships, harbours, and other structures that allow us to use the ocean safely.
Oceanographic data management is the discipline ensuring that oceanographic data both past and present are available to researchers.
Oceanographic institutions.
The first international organization of oceanography was created in 1902 as the International Council for the Exploration of the Sea. In 1903 the Scripps Institution of Oceanography was founded, followed by Woods Hole Oceanographic Institution in 1930, Virginia Institute of Marine Science in 1938, and later the Lamont-Doherty Earth Observatory at Columbia University, and the School of Oceanography at University of Washington. In Britain, the National Oceanography Centre (part of Natural Environment Research Council) is the successor to the Institute of Oceanography. In Australia, CSIRO Marine and Atmospheric Research (CMAR), is a leading centre. In 1921 the International Hydrographic Bureau (IHB) was formed in Monaco.

</doc>
<doc id="44048" url="http://en.wikipedia.org/wiki?curid=44048" title="Northern Province">
Northern Province

Northern Province or North Province may refer to:
In fiction, "North Province" may refer to:

</doc>
<doc id="44049" url="http://en.wikipedia.org/wiki?curid=44049" title="Chiricahua">
Chiricahua

Chiricahua ( ) are Native Americans who lived in the Southwest United States. Historically, similar to other Apaches, the Chiricahua were a collection of bands which shared a common area, language, customs, and intertwined family relations. At the time they encountered Europeans, they were living in 15 million acres (61,000 km2) that included southwestern New Mexico and southeastern Arizona in the United States, and in northern Sonora and Chihuahua in Mexico.
Today, there are two Chiricahua federally recognized tribes in the United States: the Fort Sill Apache Tribe, located near Apache, Oklahoma; and the Chiricahua tribe located on the Mescalero Apache reservation near Ruidoso, New Mexico.
Name.
The Chiricahua Apache were initially given their name by the Spanish. They are also known as the "Chiricagui", "Apaches de Chiricahui", "Chiricahues", "Chilicague", "Chilecagez", and "Chiricagua". The White Mountain Apache, including the "Cibecue" and "Bylas" groups of the Western Apache, called them "Ha'i’ą́há" (meaning 'Eastern Sunrise"). The San Carlos Apache called them "Hák'ą́yé." The Navajo, call the Chiricahua "Chíshí".
Please see the Bands section below for more names of bands and sub-bands of the Chiricahua (Central Apache).
Culture and Orgaization.
Several loosely affiliated bands of Apache came improperly to be usually known as the Chiricahuas. These included the "Chokonen" (recte: Tsokanende), the "Chihenne" (recte: Tchihende), the "Nednai" ("Nednhi") and "Bedonkohe" (recte, both of them together: Ndendahe). Today, all are commonly referred to as Chiricahua, but they were not historically a single band nor the same Apache division, being more correctly identified, all together, as "Central Apaches".
Many other bands and groups of Apachean language-speakers ranged over eastern Arizona and the American Southwest. The bands that are grouped under the Chiricahua term today had much history together: they intermarried and lived alongside each other, and they also occasionally fought with each other. They formed short-term as well as longer alliances that have caused scholars to classify them as one people.
The Apachean groups and the Navajo peoples were part of the Athabaskan migration into the North American continent from Asia, across the Bering Strait from Siberia. As the people moved south and east into North America, groups splintered off and became differentiated by language and culture over time. Some anthropologists believe that the Apache and the Navajo were pushed south and west into what is now New Mexico and Arizona by pressure from other Great Plains Indians, such as the Comanche and Kiowa. Among the last of such splits were those that resulted in the formation of the different Apachean bands whom the later Europeans encountered: the southwestern Apache groups and the Navajo. Although both speaking forms of Southern Athabaskan, the Navajo and Apache have become culturally distinct.
History.
The Tsokanende (Chiricahua) Apache division was once led, from the beginning of the 18th century, by chiefs such as Pisago Cabezon, Relles, Posito Moraga, Yrigollen, Tapilà, Teboca, Vivora, Miguel Narbona, Esquinaline, and finally Cochise (whose name was derived from the Apache word "Cheis," meaning "having the quality of oak") and, after his death, his sons Tahzay and, later, Naiche, under the guardianship of Cochise's war chief Nahilzay, and the independent chiefs Chihuahua, Skinya and Pionsenay; Tchihende (Mimbreño) people was led, during the same period, by chiefs as Juan Josè Compa, Fuerte a.k.a. Soldado Fiero, Mangas Coloradas, Cuchillo Negro, Delgadito, Ponce, Nana, Victorio, Loco, Mangus; Ndendahe Apache people, in the meanwhile, was led by Mahko, Mano Mocha, Coleto Amarillo, Luis, Laceres, Felipe, Natiza, Juh and "Goyaałé" (known to the Americans as Geronimo). After Victorio's death, Nana, Geronimo, Mangus (youngest Mangas Colaradas' son) and youngest Cochise's son "Naiche" were the last leaders of the Central Apaches, and their mixed Apache group was the last to continue to resist U.S. government control of the American Southwest.
European-Apache relations.
From the beginning of EuropeanAmerican/Apache relations, there was conflict between them, as they competed for land and other resources, and had very different cultures. Their encounters were preceded by more than 100 years of Spanish colonial and Mexican incursions and settlement on the Apache lands. The United States settlers were newcomers to the competition for land and resources in the Southwest, but they inherited its complex history, and brought their own attitudes with them about American Indians and how to use the land. By the Treaty of Guadalupe Hidalgo of 1848, the US took on the responsibility to prevent and punish cross-border incursions by Apache who were raiding in Mexico.
The Apache viewed the United States colonists with ambivalence, and in some cases enlisted them as allies in the early years against the Mexicans. In 1852, the US and some of the Chiricahua signed a treaty, but it had little lasting effect. During the 1850s, American miners and settlers began moving into Chiricahua territory, beginning encroachment that had been renewed in the migration to the Southwest of the previous two decades.
This forced the Apachean people to change their lives as nomads, free on the land. The US Army defeated them and forced them into the confinement of reservation life, on lands ill-suited for subsistence farming, which the US proffered as the model of civilization. Today, the Chiricahua are preserving their culture as much as possible, while forging new relationships with the peoples around them. The Chiricahua are a living and vibrant culture, a part of the greater American whole and yet distinct based on their history and culture.
Hostilities.
Although they had lived peaceably with most Americans in the New Mexico Territory up to about 1860, the Chiricahua became increasingly hostile to American encroachment in the Southwest after a number of provocations had occurred between them.
In 1835, Mexico had placed a bounty on Apache scalps which further inflamed the situation. In 1837 Warm Springs Mimbreños' head chief and famed raider, Soldado Fiero a.k.a. Fuerte was killed by Mexicans soldiers of the garrison at Janos (only two days' travel from Santa Rita del Cobre), and his son Cuchillo Negro succeeded him as head chief. In the same 1837, the American John (a.k.a. James) Johnson invited the Coppermine Mimbreños in the Pinos Altos area to trade with his party (near the mines at Santa Rita del Cobre, New Mexico) and, when they gathered around a blanket on which "pinole" (a ground corn flour) had been placed for them, Johnson and his men opened fire on the Chihenne with rifles and a concealed cannon loaded with scrap iron, glass, and a length of chain). They killed about 20 Apache, including the chief Juan José Compá. Mangas Coloradas is said to have witnessed this attack, which inflamed his and other Apache warriors' desires for vengeance for many years; he led the survivors to safety and subsequently, together with Cuchillo Negro, took Mimbreño revenge. The historian Rex W. Strickland argued that the Apache had come to the meeting with their own intentions of attacking Johnson's party, but were taken by surprise. In 1839 scalp-hunter James Kirker was employed by Robert McKnight to re-open the road to Santa Rita del Cobre.
After the conclusion of the US/Mexican War (1848) and the Gadsden Purchase (1853), Americans began to enter the territory in greater numbers. This increased the opportunities for incidents and misunderstandings. Anyway, the Apaches, including Mangas Coloradas and Cuchillo Negro, were not at first hostile to the Americans, considering them enemies of their own Mexican enemies. 
Cuchillo Negro, with Ponce, Delgadito, Victorio and other Mimbreño chiefs, signed a treaty at Fort Webster in April 1853, but, during the spring of 1857 the U.S. Army set out on a campaign, led by Col. Benjamin L.E. deBonneville, Col. Dixon S. Miles (3° Cavalry from Fort Thorn) and Col. William W. Loring (commanding a Mounted Rifles Regiment from Albuquerque), against Mogollon and Coyotero Apaches: Loring's Pueblo Indian scouts found out and attacked an Apache rancheria in the Canyon de Los Muertos Carneros (May 25, 1857), where Cuchillo Negro and some Mimbreño Apache were resting after a raid against the Navahos. Some Apaches, including Cuchillo Negro himself, were killed.
In December 1860, after several bad incidents provoked by the miners led by James H. Tevis in the Pinos Altos area, "Mangas Coloradas" went to Pinos Altos, New Mexico to try to convince the miners to move away from the area he loved and to go to the Sierra Madre and seek gold there, but they tied him to a tree and whipped him badly. His Mimbreño and Ndendahe followers and related Chiricahua bands were incensed by the treatment of their respected chief. Mangas had been just as great a chief in his prime (during the 1830s and 1840s), along with Cuchillo Negro, as Cochise was then becoming.
In 1861, the US Army seized and killed some of Cochise's relatives near Apache Pass, in what became known as the Bascom Affair. Remembering how Cochise had escaped, the Chiricahua called the incident "cut the tent." In 1863, Gen. James H. Carleton set out leading a new campaign against the Mescalero Apache, and Capt. Edmund Shirland (10° California Cavalry) invited Mangas Coloradas for a "parley" but, after he entered the U.S. camp to negotiate a peace, the great Mimbreño chief was arrested and convicted in Fort McLane, where, probably after Gen. Joseph R. West's orders, Mangas Coloradas was killed by American soldiers (Jan. 18, 1863). His body was mutilated by the soldiers, and his people were enraged by his murder. The Chiricahuas began to consider the Americans as "enemies we go against them." From that time, they waged almost constant war against US settlers and the Army for the next 23 years. Cochise, his brother-in-law Nahilzay (war chief of Cochise's people), Chihuahua, Skinya, Pionsenay, Ulzana and other warring chiefs became a nightmare to settlers and military garrisons and patrols. In the meantime, the great Victorio, Delgadito (soon killed in 1864), Nana, Loco, young Mangus (last son of Mangas Coloradas) and other minor chiefs led on the warpath the Mimbreños, Chiricahuas' cousins and allies, and Juh led the Ndendahe (Nednhi and Bedonkohe together).
In 1872, General Oliver O. Howard, with the help of Thomas Jeffords, succeeded in negotiating a peace with Cochise. The US established a Chiricahua Apache Reservation with Jeffords as US Agent, near Fort Bowie, Arizona Territory. It remained open for about 4 years, during which the chief Cochise died (from natural causes). In 1877, about three years after Cochise's death, the US moved the Chiricahua and some other Apache bands to the San Carlos Apache Indian Reservation, still in Arizona. The mountain people hated the desert environment of San Carlos, and some frequently began to leave the reservation and sometimes raid neighboring settlers. They surrendered to General Nelson Miles in 1886. The best-known warrior leader of the renegades, although he was not considered a 'chief', was the forceful and influential Geronimo. He and "Naiche" (the son of Cochise and hereditary leader after Tahzay's death) together led many of the resisters during those last few years of freedom.
They made a stronghold in the Chiricahua Mountains, part of which is now inside Chiricahua National Monument, and across the intervening Willcox Playa to the northeast, in the Dragoon Mountains (all in southeastern Arizona). In late frontier times, the Chiricahua ranged from San Carlos and the White Mountains of Arizona, to the adjacent mountains of southwestern New Mexico around what is now Silver City, and down into the mountain sanctuaries of the Sierra Madre (of northern Mexico). There they often joined with their "Nednai" Apache kin.
General George Crook, then General Miles' troops, aided by Apache scouts from other groups, pursued the exiles until they gave up. Mexico and the United States had negotiated an agreement allowing their troops in pursuit of the Apache to continue into each other's territories. This prevented the Chiricahua groups from using the border as an escape route, and as they could gain little time to rest and consider their next move, the fatigue, attrition and demoralization of the constant hunt led to their surrender.
The final 34 hold-outs, including Geronimo and Naiche, surrendered to units of General Miles' forces in September 1886. From Bowie Station, Arizona, they were entrained, along with most of the other remaining Chiricahua (as well as the Army's Apache scouts), and exiled to Fort Marion, Florida. At least two Apache warriors, Massai and Gray Lizard, escaped from their prison car and made their way back to Arizona in a 1200 mi journey to their ancestral lands.
After a number of Chiricahua deaths at the Fort Marion prison near St. Augustine, Florida, the survivors were moved, first to Alabama, and later to Fort Sill, Oklahoma. Geronimo's surrender ended the Indian Wars in the United States. However, another group of Chiricahua (a.k.a. the "Nameless Ones" or "Bronco Apache") were not captured by U.S. forces and refused to surrender. They escaped over the border to Mexico, and settled in the remote Sierra Madre mountains. There they built hidden camps, raided homes for cattle and other food supplies, and engaged in periodic firefights with units of the Mexican Army and police. Most were eventually captured or killed by soldiers or by private ranchers armed and deputized by the Mexican government.
Eventually, the surviving Chiricahua prisoners were moved to the Fort Sill military reservation in Oklahoma. In August 1912, by an act of the U.S. Congress, they were released from their prisoner of war status as they were thought to be no further threat. Although promised land at Fort Sill, they met resistance from local non-Apache. They were given the choice to remain at Fort Sill or to relocate to the Mescalero reservation near Ruidoso, New Mexico. Two-thirds of the group, 183 people, elected to go to New Mexico, while 78 remained in Oklahoma. Their descendants still reside in these places. At the time, they were not permitted to return to Arizona because of hostility from the long wars.
Bands.
Since the "band" as a unit was much more important than "tribe" in Chiricahua culture, the Chiricahua had no name for themselves (autonym) as a people. The name Chiricahua is most likely the Spanish rendering of the Opata word "Chiguicagui" ('mountain of the wild turkey'). The Chiricahua tribal territory encompassed today's SE Arizona, SW New Mexico, NE Sonora and NW Chihuahua. The Chiricahua range extended to the east as far as the Rio Grande Valley in New Mexico and to the west as far as the San Pedro River Valley in Arizona, north of Magdalena just below present day Hwy I-40 corridor in New Mexico and with the town Ciudad Madera (276 km northwest of the state capital, Chihuahua, and 536 km southwest of Ciudad Juárez (formerly known as Paso del Norte) on the Mexico–United States border), as their southernmost range.
According to Morris E. Opler (1941), the Chiricahuas consisted of three bands:
Schroeder (1947) lists five bands:
The Chiricahua-Warm Springs Fort Sill Apache tribe in Oklahoma say they have four bands in Fort Sill:
Today they use the word Chidikáágu (derived from the Spanish word "Chiricahua") to refer to the Chiricahua in general, and the word Indé, to refer to the Apache in general.
Other sources list these and additional bands:
The "Chokonen", "Chihenne", "Nednhi" and "Bedonkohe" had probably up to three other local groups, named respectively after their leader or the area they inhabited. By the end of the 19th century, the surviving Apache no longer remembered such groups. They may have been annihilated (like the Pinaleño-Nednhi) or had joined more mighty local groups (the remnant of the Carrizaleños-Nedhni camped together with their northern kin, the Janero-Nednhi).
The Carrizaleňo-Nednhi shared overlapping territory in the surroundings of Casas Grandes and Agua Nuevas with the "Tsebekinéndé", a southern Mescalero band (which was often called "Aguas Nuevas" by the Spanish). The Spanish referred to the Apache band by the same name of Tsebekinéndé. These two different Apache bands were often confused with each other. (Similar confusion arose over distinguishing the Janeros-Nednhi of the Chiricahua ("Dzilthdaklizhéndé") and the "Dzithinahndé" of the Mescalero.

</doc>
<doc id="44050" url="http://en.wikipedia.org/wiki?curid=44050" title="John Abbott (disambiguation)">
John Abbott (disambiguation)

John Abbott (1821–1893) was Prime Minister of Canada, 1891–1892.
John Abbott or Abbot may also refer to:

</doc>
<doc id="44055" url="http://en.wikipedia.org/wiki?curid=44055" title="Measurable function">
Measurable function

In mathematics, particularly in measure theory, measurable functions are structure-preserving functions between measurable spaces; as such, they form a natural context for the theory of integration. Specifically, a function between measurable spaces is said to be measurable if the preimage of each measurable set is measurable, analogous to the situation of continuous functions between topological spaces.
In probability theory, the sigma algebra often represents the set of available information, and a function (in this context a random variable) is measurable if and only if it represents an outcome that is knowable based on the available information. In contrast, functions that are not Lebesgue measurable are generally considered pathological, at least in the field of analysis.
Formal definition.
Let ("X", Σ) and ("Y", Τ) be measurable spaces, meaning that "X" and "Y" are sets equipped with respective sigma algebras Σ and Τ. A function "f": "X" → "Y" is said to be measurable if the preimage of "E" under "f" is in Σ for every "E" ∈ Τ; i.e.
The notion of measurability depends on the sigma algebras Σ and Τ. To emphasize this dependency, if "f": "X" → "Y" is a measurable function, we will write
Caveat.
This definition can be deceptively simple, however, as special care must be taken regarding the σ-algebras involved. In particular, when a function "f": R → R is said to be Lebesgue measurable what is actually meant is that formula_3 is a measurable function—that is, the domain and range represent different σ-algebras on the same underlying set (here formula_4 is the sigma algebra of Lebesgue measurable sets, and formula_5 is the Borel algebra on R). As a result, the composition of Lebesgue-measurable functions need not be Lebesgue-measurable.
By convention a topological space is assumed to be equipped with the Borel algebra generated by its open subsets unless otherwise specified. Most commonly this space will be the real or complex numbers. For instance, a real-valued measurable function is a function for which the preimage of each Borel set is measurable. A complex-valued measurable function is defined analogously. In practice, some authors use measurable functions to refer only to real-valued measurable functions with respect to the Borel algebra. If the values of the function lie in an infinite-dimensional vector space instead of R or C, usually other definitions of measurability are used, such as weak measurability and Bochner measurability.
Non-measurable functions.
Real-valued functions encountered in applications tend to be measurable; however, it is not difficult to find non-measurable functions.

</doc>
<doc id="44056" url="http://en.wikipedia.org/wiki?curid=44056" title="MD Data">
MD Data

MD Data stands for MiniDisc-Data, and is a magneto-optical medium for storing computer data. Sony wanted MD Data to replace floppy disks, but the Zip drive from Iomega ended up filling that market need and, later on, the advent of affordable CD-writers and very cheap blank CD media, coupled with the availability of memory sticks and cards proved the final straw for MD-Data. 
The technology provided 140 MB of data storage, but it was slow and expensive. Also, data drives can only read or write audio MDs when set in "play" mode, which, however, does not provide computer access to the data.
MD Data appeared in products such as an MD still camera and an MD document scanner. MD Data was also used in the late 1990s in 4- and 8-track multitrack recording decks. Meant as a step up from the popular 4-track cassette-based studios, these recorders enjoyed a brief prominence before they were replaced by relatively affordable -- and far more flexible -- direct-to-hard drive recording on Windows and Macintosh based computers. The format has always been hampered by the lack of an affordable MD-Data drive with which to manipulate and back up data using a PC. 
In 1997, Sony introduced the MD-Data2 format, which could hold 650 MB of data (as opposed to MD-Data's 140 MB). However this format was only used by Sony's DCM-M1 camcorder (capable of still images and MPEG-2 video), and a few multitrack "portastudio"-style audio recorders such as Sony's MDM-X4 and Tascam's 564, among others.
The introduction of Hi-MD in 2004 allowed any type of data (files, music, etc.) to be stored on a Hi-MD formatted MiniDisc. This allows for storage capacity of around 340MB on a standard MiniDisc and approx. 1GB on a new, higher-density Hi-MD.

</doc>
<doc id="44057" url="http://en.wikipedia.org/wiki?curid=44057" title="Galactic astronomy">
Galactic astronomy

Galactic astronomy is the study of our own Milky Way galaxy and all its contents. This is in contrast to extragalactic astronomy, which is the study of everything outside our galaxy, including all other galaxies.
Galactic astronomy should not be confused with galaxy formation and evolution, which is the general study of galaxies, their formation, structure, components, dynamics, interactions, and the range of forms they take.
Our own Milky Way galaxy, where our Solar System belongs, is in many ways the best studied galaxy, although important parts of it are obscured from view in visible wavelengths by regions of cosmic dust. The development of radio astronomy, infrared astronomy and submillimetre astronomy in the 20th Century allowed the gas and dust of the Milky Way to be mapped for the first time.
Subcategories.
A standard set of subcategories is used by astronomical journals to split up the subject of Galactic Astronomy:

</doc>
<doc id="44058" url="http://en.wikipedia.org/wiki?curid=44058" title="Big Bang nucleosynthesis">
Big Bang nucleosynthesis

In physical cosmology, Big Bang nucleosynthesis (abbreviated BBN, also known as primordial nucleosynthesis) refers to the production of nuclei other than those of the lightest isotope of hydrogen (hydrogen-1, 1H, having a single proton as a nucleus) during the early phases of the universe. Primordial nucleosynthesis is believed by most cosmologists to have taken place from 10 seconds to 20 minutes after the Big Bang, and is calculated to be responsible for the formation of most of the universe's helium as the isotope helium-4 (4He), along with small amounts of the hydrogen isotope deuterium (2H or D), the helium isotope helium-3 (3He), and a very small amount of the lithium isotope lithium-7 (7Li). In addition to these stable nuclei, two unstable or radioactive isotopes were also produced: the heavy hydrogen isotope tritium (3H or T); and the beryllium isotope beryllium-7 (7Be); but these unstable isotopes later decayed into 3He and 7Li, as above.
Essentially all of the elements that are heavier than lithium and beryllium were created much later, by stellar nucleosynthesis in evolving and exploding stars.
Characteristics.
There are two important characteristics of Big Bang nucleosynthesis (BBN):
where t is time in seconds, T is temperature in MeV and g* is the effective number of particle species. (g* includes contributions of 2 from photons, 7/2 from electron-positron pairs and 7/4 from each neutrino flavor. In the standard model g* is 10.75). This expression also shows how a different number of neutrino flavors will change the rate of cooling of the early universe.
The key parameter which allows one to calculate the effects of BBN is the number of photons per baryon. This parameter corresponds to the temperature and density of the early universe and allows one to determine the conditions under which nuclear fusion occurs. From this we can derive elemental abundances. Although the baryon per photon ratio is important in determining elemental abundances, the precise value makes little difference to the overall picture. Without major changes to the Big Bang theory itself, BBN will result in mass abundances of about 75% of hydrogen-1, about 25% helium-4, about 0.01% of deuterium, trace amounts (on the order of 10−10) of lithium and beryllium, and no other heavy elements. (Traces of boron have been found in some old stars, giving rise to the question whether some boron, not really predicted by the theory, might have been produced in the Big Bang. The question is not presently resolved.) That the observed abundances in the universe are generally consistent with these abundance numbers is considered strong evidence for the Big Bang theory.
In this field it is customary to quote percentages "by mass", so that 25% helium-4 means that helium-4 atoms account for 25% of the mass, but only about 8% of the atoms would be helium-4 atoms.
Important parameters.
The creation of light elements during BBN was dependent on a number of parameters; among those was the neutron-proton ratio (calculable from Standard Model physics) and the baryon-photon ratio.
Neutron-proton ratio.
Neutrons can react with positrons or electron neutrinos to create protons and other products in one of the following reactions:
These reactions continue until expansion of the universe outpaces the reactions, which occurs at about T = 0.7 MeV and is called the freeze out temperature. At freeze out, the neutron-proton ratio is about 1/7. Almost all neutrons that exist after the freeze out ended up combined into helium-4, due to the fact that helium-4 has the highest binding energy per nucleon among light elements. This predicts that about 8% of all atoms should be helium-4, leading to a mass fraction of helium-4 of about 25%, which is in line with observations. Some deuterium and helium-3 remained as there was insufficient time and density for them to react and form helium-4.
Baryon-photon ratio.
The baryon-photon ratio, η, is a strong indicator of the abundance of light elements present in the early universe. Baryons can react with light elements in the following reactions:
It is evident that reactions with baryons during BBN would ultimately result in helium-4, and also that the abundance of primordial deuterium is indirectly related to the baryon density or baryon-photon ratio. That is, the larger the baryon-photon ratio the more reactions there will be and the more deuterium will be eventually transformed into helium-4. This result makes deuterium a very useful tool in measuring the baryon-to-photon ratio.
Sequence.
Big Bang nucleosynthesis began a few seconds after the big bang, when the universe had cooled sufficiently to allow deuterium nuclei to survive disruption by high-energy photons. This time is essentially independent of dark matter content, since the universe was highly radiation dominated until much later, and this dominant component controls the temperature/time relation. 
The relative abundances of protons and neutrons follow from simple thermodynamical arguments, combined with the way that the mean temperature of the universe changes over time. If the reactions needed to reach the thermodynamically favoured equilibrium values are too slow compared to the temperature change brought about by the expansion, abundances would have remained at some specific non-equilibrium value. Combining thermodynamics and the changes brought about by cosmic expansion, one can calculate the fraction of protons and neutrons based on the temperature at this point. The answer is that there are about seven protons for every neutron at the beginning of nucleosynthesis. This fraction is in favour of protons, primarily because their lower mass with respect to the neutron favors their production. Free neutrons decay to protons with a half-life of about 10.2 minutes, but this time-scale is longer than the first three minutes of nucleogenesis, during which time a substantial fraction of them were combined with protons into deuterium and then He-4. The sequence of these reaction chains is shown on the image.
One feature of BBN is that the physical laws and constants that govern the behavior of matter at these energies are very well understood, and hence BBN lacks some of the speculative uncertainties that characterize earlier periods in the life of the universe. Another feature is that the process of nucleosynthesis is determined by conditions at the start of this phase of the life of the universe, and proceeds independently of what happened before.
As the universe expands, it cools. Free neutrons and protons are less stable than helium nuclei, and the protons and neutrons have a strong tendency to form helium-4. However, forming helium-4 requires the intermediate step of forming deuterium. Before nucleosynthesis began, the temperature was high enough for many photons to have energy greater than the binding energy of deuterium; therefore any deuterium that was formed was immediately destroyed (a situation known as the deuterium bottleneck). Hence, the formation of helium-4 is delayed until the universe became cool enough for deuterium to survive (at about T = 0.1 MeV); after which there was a sudden burst of element formation. However, very shortly thereafter, at twenty minutes after the Big Bang, the universe became too cool for any further nuclear fusion and nucleosynthesis to occur. At this point, the elemental abundances were nearly fixed, and only change was the result of the radioactive decay of some products of BBN (such as tritium).
History of theory.
The history of Big Bang nucleosynthesis began with the calculations of Ralph Alpher in the 1940s. Alpher published the seminal Alpher–Bethe–Gamow paper that outlined the theory of light-element production in the early universe.
During the 1970s, there was a major puzzle in that the density of baryons as calculated by Big Bang nucleosynthesis was much less than the observed mass of the universe based on calculations of the expansion rate. This puzzle was resolved in large part by postulating the existence of dark matter.
Heavy elements.
Big Bang nucleosynthesis produced no elements heavier than beryllium, due to a bottleneck: the absence of a stable nucleus with 8 or 5 nucleons. This deficit of larger atoms also limited the amounts of lithium-7 and beryllium-9 produced during BBN. In stars, the bottleneck is passed by triple collisions of helium-4 nuclei, producing carbon (the triple-alpha process). However, this process is very slow, taking tens of thousands of years to convert a significant amount of helium to carbon in stars, and therefore it made a negligible contribution in the minutes following the Big Bang.
Helium-4.
Big Bang nucleo-synthesis predicts a primordial abundance of about 25% helium-4 by mass, irrespective of the initial conditions of the universe. As long as the universe was hot enough for protons and neutrons to transform into each other easily, their ratio, determined solely by their relative masses, was about 1 neutron to 7 protons (allowing for some decay of neutrons into protons). Once it was cool enough, the neutrons quickly bound with an equal number of protons to form first deuterium, then helium-4. Helium-4 is very stable and is nearly the end of this chain if it runs for only a short time, since helium neither decays nor combines easily to form heavier nuclei (since there are no stable nuclei with mass numbers of 5 or 8, helium does not combine easily with either protons, or with itself). Once temperatures are lowered, out of every 16 nucleons (2 neutrons and 14 protons), 4 of these (25% of the total particles and total mass) combine quickly into one helium-4 nucleus. This produces one helium for every 12 hydrogens, resulting in a universe that is a little over 8% helium by number of atoms, and 25% helium by mass.
One analogy is to think of helium-4 as ash, and the amount of ash that one forms when one completely burns a piece of wood is insensitive to how one burns it. The resort to the BBN theory of the helium-4 abundance is necessary as there is far more helium-4 in the universe than can be explained by stellar nucleosynthesis. In addition, it provides an important test for the Big Bang theory. If the observed helium abundance is much different from 25%, then this would pose a serious challenge to the theory. This would particularly be the case if the early helium-4 abundance was much smaller than 25% because it is hard to destroy helium-4. For a few years during the mid-1990s, observations suggested that this might be the case, causing astrophysicists to talk about a Big Bang nucleosynthetic crisis, but further observations were consistent with the Big Bang theory.
Deuterium.
Deuterium is in some ways the opposite of helium-4 in that while helium-4 is very stable and very difficult to destroy, deuterium is only marginally stable and easy to destroy. The temperatures, time, and densities were sufficient to combine a substantial fraction of the deuterium nuclei to form helium-4 but insufficient to carry the process further using helium-4 in the next fusion step. BBN did not convert all of the deuterium in the universe to helium-4 due to the expansion that cooled the universe and reduced the density and so, cut that conversion short before it could proceed any further. One consequence of this is that unlike helium-4, the amount of deuterium is very sensitive to initial conditions. The denser the initial universe was, the more deuterium would be converted to helium-4 before time ran out, and the less deuterium would remain.
There are no known post-Big Bang processes which can produce significant amounts of deuterium. Hence observations about deuterium abundance suggest that the universe is not infinitely old, which is in accordance with the Big Bang theory.
During the 1970s, there were major efforts to find processes that could produce deuterium, but those revealed ways of producing isotopes other than deuterium. The problem was that while the concentration of deuterium in the universe is consistent with the Big Bang model as a whole, it is too high to be consistent with a model that presumes that most of the universe is composed of protons and neutrons. If one assumes that all of the universe consists of protons and neutrons, the density of the universe is such that much of the currently observed deuterium would have been burned into helium-4. The standard explanation now used for the abundance of deuterium is that the universe does not consist mostly of baryons, but that non-baryonic matter (also known as dark matter) makes up most of the mass of the universe. This explanation is also consistent with calculations that show that a universe made mostly of protons and neutrons would be far more "clumpy" than is observed.
It is very hard to come up with another process that would produce deuterium other than by nuclear fusion. Such a process would require that the temperature be hot enough to produce deuterium, but not hot enough to produce helium-4, and that this process should immediately cool to non-nuclear temperatures after no more than a few minutes. It would also be necessary for the deuterium to be swept away before it reoccurs.
Producing deuterium by fission is also difficult. The problem here again is that deuterium is very unlikely due to nuclear processes, and that collisions between atomic nuclei are likely to result either in the fusion of the nuclei, or in the release of free neutrons or alpha particles. During the 1970s, cosmic ray spallation was proposed as a source of deuterium. That theory failed to account for the abundance of deuterium, but led to explanations of the source of other light elements.
Measurements and status of theory.
The theory of BBN gives a detailed mathematical description of the production of the light "elements" deuterium, helium-3, helium-4, and lithium-7. Specifically, the theory yields precise quantitative predictions for the mixture of these elements, that is, the primordial abundances at the end of the big-bang.
In order to test these predictions, it is necessary to reconstruct the primordial abundances as faithfully as possible, for instance by observing astronomical objects in which very little stellar nucleosynthesis has taken place (such as certain dwarf galaxies) or by observing objects that are very far away, and thus can be seen in a very early stage of their evolution (such as distant quasars).
As noted above, in the standard picture of BBN, all of the light element abundances depend on the amount of ordinary matter (baryons) relative to radiation (photons). Since the universe is presumed to be homogeneous, it has one unique value of the baryon-to-photon ratio. For a long time, this meant that to test BBN theory against observations one had to ask: can "all" of the light element observations be explained with a "single value" of the baryon-to-photon ratio? Or more precisely, allowing for the finite precision of both the predictions and the observations, one asks: is there some "range" of baryon-to-photon values which can account for all of the observations?
More recently, the question has changed: Precision observations of the cosmic microwave background radiation with the Wilkinson Microwave Anisotropy Probe (WMAP) give an independent value for the baryon-to-photon ratio. Using this value, are the BBN predictions for the abundances of light elements in agreement with the observations?
The present measurement of helium-4 indicates good agreement, and yet better agreement for helium-3. But for lithium-7, there is a significant discrepancy between BBN and WMAP, and the abundance derived from Population II stars. The discrepancy is a factor of 2.4―4.3 below the theoretically predicted value and is considered a problem for the original models, that have resulted in revised calculations of the standard BBN based on new nuclear data, and to various reevaluation proposals for primordial proton-proton nuclear reactions, especially the abundances of 7Be(n,p)7Li versus 7Be(d,p)8Be.
Non-standard scenarios.
In addition to the standard BBN scenario there are numerous non-standard BBN scenarios. These should not be confused with non-standard cosmology: a non-standard BBN scenario assumes that the Big Bang occurred, but inserts additional physics in order to see how this affects elemental abundances. These pieces of additional physics include relaxing or removing the assumption of homogeneity, or inserting new particles such as massive neutrinos.
There have been, and continue to be, various reasons for researching non-standard BBN. The first, which is largely of historical interest, is to resolve inconsistencies between BBN predictions and observations. This has proved to be of limited usefulness in that the inconsistencies were resolved by better observations, and in most cases trying to change BBN resulted in abundances that were more inconsistent with observations rather than less. The second reason for researching non-standard BBN, and largely the focus of non-standard BBN in the early 21st century, is to use BBN to place limits on unknown or speculative physics. For example, standard BBN assumes that no exotic hypothetical particles were involved in BBN. One can insert a hypothetical particle (such as a massive neutrino) and see what has to happen before BBN predicts abundances which are very different from observations. This has been usefully done to put limits on the mass of a stable tau neutrino.

</doc>
<doc id="44059" url="http://en.wikipedia.org/wiki?curid=44059" title="Harrison Ford">
Harrison Ford

Harrison Ford (born July 13, 1942) is an American actor and film producer. He gained worldwide fame for his starring roles as Han Solo in the original "Star Wars" epic space opera trilogy and the title character of the "Indiana Jones" film series. Ford is also known for his roles as Rick Deckard in the 1982 neo-noir dystopian science fiction film "Blade Runner", John Book in the 1985 thriller "Witness" and Jack Ryan in the 1992 action-suspense film "Patriot Games" and the 1994 spy action thriller film "Clear and Present Danger". 
His career has spanned six decades and includes roles in several Hollywood blockbusters; including the epic war film "Apocalypse Now" (1979), the legal drama "Presumed Innocent" (1990), the action film "The Fugitive" (1993), the political action thriller "Air Force One" (1997) and the psychological thriller "What Lies Beneath" (2000). At one point, four of the top six box-office hits of all time included one of his roles. Seven of his films have been inducted into the National Film Registry: "American Graffiti" (1973), "The Conversation" (1974), "Star Wars" (1977), "Apocalypse Now", "The Empire Strikes Back" (1980), "Raiders of the Lost Ark" (1981) and "Blade Runner".
In 1997, Ford was ranked No. 1 in "Empire"‍‍ '​‍s "The Top 100 Movie Stars of All Time" list. s of 2008[ [update]], the US domestic box office grosses of Ford's films total over US$3.5 billion, with worldwide grosses surpassing $6 billion, making Ford the 4th highest grossing U.S. domestic box-office star. Ford is married to actress Calista Flockhart, who is known for playing the title role in the comedy-drama series "Ally McBeal".
Early life.
Ford was born July 13, 1942, at Chicago, Illinois's Swedish Covenant Hospital. His mother, Dorothy (née Dora Nidelman, 1917-2004), was a homemaker and former radio actress, and his father, Christopher Ford (born John William Ford, 1906-1999), was an advertising executive and a former actor. A younger brother, Terence, was born in 1945. Ford's paternal grandparents, John Fitzgerald Ford and Florence Veronica Niehaus, were of Irish Catholic and German descent, respectively. Ford's maternal grandparents, Harry Nidelman and Anna Lifschutz, were Jewish immigrants from Minsk, Belarus (at that time a part of the Russian Empire). When asked in which religion he and his brother were raised, Ford has jokingly responded, "Democrat," "to be liberals of every stripe". In a television interview shown in August 2000, when asked about what influence his Irish Catholic and Russian Jewish ancestry may have had on his life as a person and as an artist, Ford humorously stated "As a man I've always felt Irish, as an actor I've always felt Jewish."
Ford was active in the Boy Scouts of America, and achieved its second-highest rank, Life Scout. He worked at Napowan Adventure Base Scout camp as a counselor for the Reptile Study merit badge. Because of this, he and Eagle Scout director Steven Spielberg later decided to depict the young Indiana Jones as a Life Scout in the film "Indiana Jones and the Last Crusade". They also jokingly reversed Ford's knowledge of reptiles into Jones' fear of snakes.
In 1960, Ford graduated from Maine East High School in Park Ridge, Illinois. His was the first student voice broadcast on his high school's new radio station, WMTH, and he was its first sportscaster during his senior year (1959–60). He attended Ripon College in Wisconsin, where he was a member of the Sigma Nu fraternity. He took a drama class in the final quarter of his senior year to get over his shyness. Ford, a self-described "late bloomer," became fascinated with acting.
Early career.
In 1964, after a season of summer stock with the Belfry Players in Wisconsin, Ford traveled to Los Angeles, California to apply for a job in radio voice overs. He did not get it, but stayed in California and eventually signed a $150 a week contract with Columbia Pictures' "New Talent" program, playing bit roles in films. His first known part was an uncredited role as a bellhop in "Dead Heat on a Merry-Go-Round" (1966). There is little record of his non-speaking roles (or "extra" work) in film. Ford was at the bottom of the hiring list, having offended producer Jerry Tokovsky after he played a bellboy in the feature. He was told by Tokovsky that when actor Tony Curtis delivered a bag of groceries, he did it like a movie star; Ford felt his job was to act like a bellboy. Ford managed to secure other roles in movies, such as "A Time for Killing (The Long Ride Home)", starring Glenn Ford, George Hamilton and Inger Stevens.
His speaking roles continued next with "Luv" (1967), though he was still uncredited. He was finally credited as "Harrison J. Ford" in the 1967 Western film, "A Time for Killing", but the "J" did not stand for anything, since he has no middle name. It was added to avoid confusion with a silent film actor named Harrison Ford, who appeared in more than 80 films between 1915 and 1932, and died in 1957. Ford later said that he was unaware of the existence of the earlier Harrison Ford until he came upon a star with his own name on the Hollywood Walk of Fame. Ford soon dropped the "J" and worked for Universal Studios, playing minor roles in many television series throughout the late 1960s and early 1970s, including "Gunsmoke", "Ironside", "The Virginian", "The F.B.I.", "Love, American Style", and "Kung Fu". He appeared in the western "Journey to Shiloh" (1968) and had an uncredited, non-speaking role in Michelangelo Antonioni's 1970 film "Zabriskie Point", as an arrested student protester. Not happy with the roles being offered to him, Ford became a self-taught professional carpenter to support his then-wife and two small sons. While working as a carpenter, he became a stagehand for the popular rock band The Doors. He also built a sun deck for actress Sally Kellerman and a recording studio for Brazilian band leader Sérgio Mendes.
He was then hired to build cabinets at the home of director George Lucas, who subsequently cast him in a pivotal supporting role for his film "American Graffiti" (1973). Ford's relationship with Lucas would profoundly affect his career later on. After director Francis Ford Coppola's film "The Godfather" was a success, he hired Ford to expand his office and gave him small roles in his next two films, "The Conversation" (1974) and "Apocalypse Now" (1979); in the latter film he played an army officer named "G. Lucas". During this early period, Ford often came to auditions directly from work still wearing his carpenter's clothes and gear, to subtly remind casting directors that he had other options in life.
Milestone franchises.
"Star Wars".
Harrison Ford's carpentry work for George Lucas eventually landed him his first starring film role, when he was hired by Lucas to read lines for actors auditioning for parts in his then-upcoming film "Star Wars" (1977). Lucas was eventually won over by Ford's performance during these line reads and cast him as Han Solo. "Star Wars" became one of the most successful movies of all time worldwide and established Ford as a superstar. He went on to star in the similarly successful "Star Wars" sequels, "The Empire Strikes Back" (1980) and "Return of the Jedi" (1983), as well as the "Star Wars Holiday Special" (1978). Ford wanted Lucas to kill off Han Solo at the end of "Return of the Jedi", saying, "That would have given the whole film a bottom," but Lucas refused. 
Ford will reprise the role of Solo for the upcoming "" (2015). During filming on June 11, 2014, Ford suffered what is said to be a fractured ankle when a hydraulic door fell on him. He was rushed to the hospital for treatment. Ford's son Ben released details on his father's injury, saying that his ankle will likely need a plate and screws and that filming could be altered slightly with the crew needing to shoot Ford from the waist up for a short time until he recovers. Ford made his return to filming in mid-August after a two-month layoff as he recovered from his injury.
"Indiana Jones".
Ford's status as a leading actor was solidified when he starred as globe-trotting archeologist Indiana Jones in the film "Raiders of the Lost Ark" (1981), a collaboration between George Lucas and Steven Spielberg. Though Spielberg was interested in casting Ford from the start, Lucas was not, due to having already worked with the actor in "American Graffiti" and "Star Wars", but he eventually relented after Tom Selleck was unable to accept. 
Ford went on to reprise the role of Jones for the prequel "Indiana Jones and the Temple of Doom" (1984) and the sequel "Indiana Jones and the Last Crusade" (1989). He returned to the role yet again for a 1993 episode of the television series "The Young Indiana Jones Chronicles", and even later for the fourth film "Indiana Jones and the Kingdom of the Crystal Skull" (2008).
Other film work.
Ford has been in other films, including "Heroes" (1977), "Force 10 from Navarone" (1978), and "Hanover Street" (1979). Ford also co-starred alongside Gene Wilder in the buddy-Western "The Frisco Kid" (1979), playing a bank robber with a heart of gold. He then starred as Rick Deckard in Ridley Scott's cult sci-fi classic "Blade Runner" (1982), and in a number of dramatic-action films: Peter Weir's "Witness" (1985) and "The Mosquito Coast" (1986), and Roman Polanski's "Frantic" (1988).
The 1990s brought Ford the role of Jack Ryan in Tom Clancy's "Patriot Games" (1992) and "Clear and Present Danger" (1994), as well as leading roles in Alan Pakula's "Presumed Innocent" (1990) and "The Devil's Own" (1997), Andrew Davis' "The Fugitive" (1993), Sydney Pollack's remake of "Sabrina" (1995), and Wolfgang Petersen's "Air Force One" (1997). Ford also played straight dramatic roles, including an adulterous husband in both "Presumed Innocent" (1990) and "What Lies Beneath" (2000), and a recovering amnesiac in Mike Nichols' "Regarding Henry" (1991).
Many of Ford's major film roles came to him by default through unusual circumstances: he won the role of Han Solo while reading lines for other actors, was cast as Indiana Jones because Tom Selleck was not available, and took the role of Jack Ryan supposedly due to Alec Baldwin's fee demands, although Baldwin disputes this (Baldwin had previously played the role in "The Hunt for Red October").
Recent roles.
Starting in the late 1990s, Ford appeared in several critically derided and commercially disappointing movies, including "Six Days Seven Nights" (1998), "Random Hearts" (1999), "" (2002), "Hollywood Homicide" (2003), "Firewall" (2006), and "Extraordinary Measures" (2010). One exception was 2000's "What Lies Beneath," which grossed over $155 million in the United States and $291 million worldwide.
In 2004, Ford declined a chance to star in the thriller "Syriana", later commenting that "I didn't feel strongly enough about the truth of the material and I think I made a mistake." The role eventually went to George Clooney, who won an Oscar and a Golden Globe for his work. Prior to that, he had passed on a role in another Stephen Gaghan-written role, Robert Wakefield in "Traffic". That role went to Michael Douglas.
In 2008, Ford enjoyed success with the release of "Indiana Jones and the Kingdom of the Crystal Skull", another Lucas/Spielberg collaboration. The film received generally positive reviews and was the second highest-grossing film worldwide in 2008. He later said he would like to star in another sequel, "...if it didn't take another 20 years to digest."
Other 2008 work included "Crossing Over", directed by Wayne Kramer. In the film, he plays an immigrations officer, working alongside Ashley Judd and Ray Liotta. He also narrated a feature documentary film about the Dalai Lama entitled "Dalai Lama Renaissance".
Ford filmed the medical drama "Extraordinary Measures" in 2009 in Portland, Oregon. Released January 22, 2010, the film also starred Brendan Fraser and Alan Ruck. Also in 2010, he co-starred in the film "Morning Glory," along with Patrick Wilson, Rachel McAdams, and Diane Keaton.
In July 2011, Ford starred alongside Daniel Craig and Olivia Wilde in the science fiction Western film "Cowboys & Aliens". To promote the film, Ford appeared at the San Diego Comic-Con International, and, apparently surprised by the warm welcome, told the audience, "I just wanted to make a living as an actor. I didn't know about this."
In 2011, Ford starred in Japanese commercials advertising the video game ' for the PlayStation 3. In 2013, Ford co-starred in the corporate espionage thriller "Paranoia", with Liam Hemsworth and Gary Oldman, and directed by Robert Luketic. as well as "Ender's Game", "42", and '.
In 2015, Ford was confirmed to be reprising his role as Rick Deckard in the sequel to "Blade Runner".
Personal life.
Marriages and family.
Ford is one of Hollywood's most private actors, guarding his personal life. He has two sons (Benjamin and Willard) with his first wife, Mary Marquardt, as well as two children (Malcolm and Georgia) with his second wife, screenwriter Melissa Mathison.
Ford began dating actress Calista Flockhart after meeting at the 2002 Golden Globes, and together they are parents to her adopted son, Liam. Ford proposed to Flockhart over Valentine's Day weekend in 2009. They married on June 15, 2010, in Santa Fe, New Mexico, where Ford was filming "Cowboys & Aliens".
Ford has three grandchildren: Eliel (born 1993), Giuliana (born 1997), and Ethan (born 2000). Son Benjamin, a chef and restaurateur, owned Ford's Filling Station, a gastropub in Culver City, California, until its closure in 2014. Son Willard is co-owner of Ford & Ching showroom, as well as Ludwig Clothing company.
Back injury.
In June 1983, at age 40, during the filming of "Indiana Jones and the Temple of Doom" in London, he herniated a disc in his back, forcing him to fly back to Los Angeles for an operation. He returned six weeks later.
Ankle injury.
On June 11, 2014, Ford injured his ankle during filming of "". He was airlifted to John Radcliffe Hospital in Oxford, England. Ford's wife soon traveled from the U.S. to be at his hospital bedside as it was feared that injuries sustained on the set could be worse than previously thought. Doctors suspected that his ankle might have been broken and he might have received a pelvic injury. Producers stated that filming would continue as planned.
Aviation.
Ford is a private pilot of both fixed-wing aircraft and helicopters, and owns an 800 acre ranch in Jackson, Wyoming, approximately half of which he has donated as a nature reserve. On several occasions, Ford has personally provided emergency helicopter services at the request of local authorities, in one instance rescuing a hiker overcome by dehydration.
Ford began flight training in the 1960s at Wild Rose Idlewild Airport in Wild Rose, Wisconsin, flying in a Piper PA-22 Tri-Pacer, but at $15 an hour, he could not afford to continue the training. In the mid-1990s, he bought a used Gulfstream II and asked one of his pilots, Terry Bender, to give him flying lessons. They started flying a Cessna 182 out of Jackson, Wyoming, later switching to Teterboro, New Jersey, flying a Cessna 206, the aircraft he soloed in.
On October 23, 1999, Harrison Ford was involved in the crash of a Bell 206L4 LongRanger helicopter (N36R). The NTSB accident report states that Ford was piloting the aircraft over the Lake Piru riverbed near Santa Clarita, California, on a routine training flight. While making his second attempt at an autorotation with powered recovery, Ford allowed the aircraft's altitude to drop to 150–200 feet before beginning power-up. The aircraft was unable to recover power before hitting the ground. The aircraft landed hard and began skidding forward in the loose gravel before one of its skids struck a partially embedded log, flipping the aircraft onto its side. Neither Ford nor the instructor pilot suffered any injuries, though the helicopter was seriously damaged. When asked about the incident by fellow pilot James Lipton in an interview on the TV show "Inside the Actor's Studio" Ford replied, "I broke it."
Ford keeps his aircraft at Santa Monica Airport, though the Bell 407 is often kept and flown in Jackson, Wyoming, and has been used by the actor in two mountain rescues during the actor's assigned duty time assisting the Teton County Search and Rescue. On one of the rescues, Ford recovered a hiker who had become lost and disoriented. She boarded Ford's Bell 407 and promptly vomited into one of the rescuers' caps, unaware of who the pilot was until much later; "I can't believe I barfed in Harrison Ford's helicopter!" she said later.
Ford flies his de Havilland Canada DHC-2 Beaver (N28S) more than any of his other aircraft, and has repeatedly said that he likes this aircraft and the sound of its Pratt & Whitney R-985 radial engine. According to Ford, it had been flown in the CIA's Air America operations, and was riddled with bullet holes that had to be patched up.
In March 2004, Ford officially became chairman of the Young Eagles program of the Experimental Aircraft Association (EAA). Ford was asked to take the position by Greg Anderson, Senior Vice President of the EAA at the time, to replace General Charles "Chuck" Yeager, who was vacating the post that he had held for many years. Ford at first was hesitant, but later accepted the offer and has made appearances with the Young Eagles at the EAA AirVenture Oshkosh gathering at Oshkosh, Wisconsin for two years. In July 2005, at the gathering in Oshkosh, Ford agreed to accept the position for another two years. Ford has flown over 280 children as part of the Young Eagles program, usually in his DHC-2 Beaver, which can seat the actor and five children. He is involved with the EAA chapter in Driggs, Idaho, just over the Teton Range from Jackson, Wyoming.
As of 2009, Ford appears in internet advertisements for General Aviation Serves America, a campaign by the advocacy group Aircraft Owners and Pilots Association (AOPA).
Ford is an honorary board member of the humanitarian aviation organization Wings of Hope.
On March 5, 2015, Ford's plane, believed to be a Ryan PT-22 Recruit, made an emergency landing on the Penmar Golf Course in Venice, California. Ford had radioed in to report that the plane had suffered engine failure. He was taken to Ronald Reagan UCLA Medical Center, where he was reported to be in fair to moderate condition. Ford suffered a broken pelvis and broken ankle during the accident, as well as other injuries.
Activism.
Environmental causes.
Ford is vice-chair of Conservation International an American nonprofit environmental organization headquartered in Arlington, Virginia. The organization's intent is to protect nature. The institution tries to combine the services or benefits of science, field work, and partnership to find global solutions to global problems. Three ways CI goes about solving nature-related problems are: 1) identifying and moving to protect locations that are crucial, such as those affecting water, food, and air; 2) working with large companies that are involved in energy and agriculture, to ensure the environment is being protected; and 3) working with governments to ensure they have the knowledge and the proper tools to construct policies that are environmentally friendly.
From its origins as an NGO dedicated to protecting tropical biodiversity, CI has evolved into an organization that works with governments, scientists, charitable foundations, and business.CI has been criticised for links to companies with a poor environmental record such as BP, Cargill, Chevron, Monsanto and Shell and for allegedly offering greenwashing services. CI has also been chastised for poor judgment in its expenditure of donors' money.
In September 2013, Ford, while filming an environmental documentary in Indonesia, interviewed the Indonesian Forestry Minister Mr. Zulkifli Hasan. After the interview Presidential Advisor Mr Andi Arief accused Ford and his crew of "harassing state institutions" and publicly threatened them with deportation. Questions within the interview concerned the Tesso Nilo National Park, Sumatra. It was alleged the Minister of Forestry was given no prior warning of questions nor the chance to explain the challenges of catching people with illegal logging. Ford was provided an audience with the Indonesian President, Mr. Susilo Bambang Yudhoyono, during which he expressed concerns regarding Indonesia's environmental degradation and the government efforts to address climate change. In response, the President explained Indonesia's commitment to preserving its oceans and forests.
In 1993, the arachnologist Norman Platnick named a new species of spider "Calponia harrisonfordi", and in 2002, the entomologist Edward O. Wilson named a new ant species "Pheidole harrisonfordi" (in recognition of Harrison's work as Vice Chairman of Conservation International).
Since 1992, Ford has lent his voice to a series of public service messages promoting environmental involvement for EarthShare, an American federation of environmental and conservation charities.
Ford has been a spokesperson for Restore Hetch Hetchy, a non-profit organization dedicated to restoring Yosemite National Park's Hetch Hetchy Valley to its original condition.
Ford appears in the documentary series "Years of Living Life Dangerously", which provides reports on those affected by, and seeking solutions to climate change.
Political views.
Like his parents, Ford is a lifelong Democrat, and a close friend of former President Bill Clinton.
On September 7, 1995, Ford testified before the U.S. Senate Foreign Relations Committee in support of the Dalai Lama and an independent Tibet. In 2008, he narrated the documentary "Dalai Lama Renaissance".
In 2003, he publicly condemned the Iraq War and called for "regime change" in the United States. He also criticized Hollywood for making violent movies, and called for more gun control in the United States.
Archaeology.
Following on his success portraying the archaeologist Indiana Jones, Ford also plays a part in supporting the work of professional archaeologists. He serves as a General Trustee on the Governing Board of the Archaeological Institute of America (AIA), North America's oldest and largest organization devoted to the world of archaeology. Ford assists them in their mission of increasing public awareness of archaeology and preventing looting and the illegal antiquities trade.
Community work.
Ford volunteered as a food server. On November 21, 2007, Ford and other celebrities, including Kirk Douglas, Nia Long and Calista Flockhart, helped serve hot meals to the homeless at the annual Thanksgiving feast at the Los Angeles Mission.
Awards.
Ford received a nomination for the Academy Award for Best Actor for "Witness", for which he also received "Best Actor" BAFTA and Golden Globe nominations. He received the Cecil B. DeMille Award at the 2002 Golden Globe Awards and on June 2, 2003, he received a star on the Hollywood Walk of Fame. He has received three additional "Best Actor" Golden Globe nominations for "The Mosquito Coast", "The Fugitive" and "Sabrina".
He received the first ever Hero Award for his many iconic roles, including Han Solo and Indiana Jones, at the 2007 Scream Awards, and in 2008, the Spike TV's Guy's Choice Award for Brass Balls.
Ford has also been honored multiple times for his involvement in general aviation, receiving the Living Legends of Aviation Award and EAA's Freedom of Flight Award in 2009, Wright Brothers Memorial Trophy in 2010, and the Al Ueltschi Humanitarian Award in 2013. In 2013, "Flying magazine" ranked him number 48 on their list of the 51 Heroes of Aviation.
Harrison Ford received the AFI Life Achievement Award in 2000.

</doc>
<doc id="44060" url="http://en.wikipedia.org/wiki?curid=44060" title="Barents Sea">
Barents Sea

The Barents Sea (Russian: Баренцево море, "Barentsevo More" Norwegian: "Barentshavet") is a marginal sea of the Arctic Ocean, located off the northern coasts of Norway and Russia with vast majority of it lying in Russian territorial waters. Known in the Middle Ages as the Murman Sea, the sea takes its current name from the Dutch navigator Willem Barentsz. It is a rather shallow shelf sea, with an average depth of 230 m, and is an important site for both fishing and hydrocarbon exploration. The Barents Sea is bordered by the shelf edge towards the Norwegian Sea to the west, and the archipelagos of Svalbard to the northwest, Franz Josef Land to the north east and Novaya Zemlya to the east. Novaya Zemlya, an extension of the northern part of the Ural Mountains, separates the Barents Sea from the Kara Sea.
Geography.
The southern half of the Barents Sea, including the ports of Murmansk (Russia) and Vardø (Norway) remain ice-free year round due to the warm North Atlantic drift. In September, the entire Barents Sea is more or less completely ice-free. Until the Winter War (1939–40), Finland's territory also reached to the Barents Sea, with the harbor at Petsamo being Finland's only ice-free winter harbor.
There are three main types of water masses in the Barents Sea: Warm, salty Atlantic water (temperature >3 °C, salinity >35) from the North Atlantic drift, cold Arctic water (temperature <0 °C, salinity <35) from the north, and warm, but not very salty coastal water (temperature >3 °C, salinity <34.7). Between the Atlantic and Polar waters, a front called the Polar Front is formed. In the western parts of the sea (close to Bear Island), this front is determined by the bottom topography and is therefore relatively sharp and stable from year to year, while in the east (towards Novaya Zemlya), it can be quite diffuse and its position can vary a lot between years.
The lands of Novaya Zemlya attained most of their early Holocene coastal deglaciation approximately 10,000 years before present.
Extent.
The International Hydrographic Organization defines the limits of the "Barentsz Sea" ["sic"] as follows:
Geology.
The Barents Sea was originally formed from two major continental collisions: the Caledonian orogeny, in which the Baltica and Laurentia collided to form Laurasia, and a subsequent collision between Laurasia and Western Siberia. Most of its geological history is dominated by extensional tectonics, caused by the collapse of the Caledonian and Uralian orogenic belts and the break-up of Pangaea. These events created the major rift basins that dominate the Barents Shelf, along with various platforms and structural highs. The later geological history of the Barents Sea is dominated by Late Cenozoic uplift, particularly that caused by Quaternary glaciation, which has resulted in erosion and deposition of significant sediment.
Ecology.
Due to the North Atlantic drift, the Barents Sea has a high biological production compared to other oceans of similar latitude. The spring bloom of phytoplankton can start quite early close to the ice edge, because the fresh water from the melting ice makes up a stable water layer on top of the sea water. The phytoplankton bloom feeds zooplankton such as "Calanus finmarchicus", "Calanus glacialis", "Calanus hyperboreus", "Oithona" spp., and krill. The zooplankton feeders include young cod, capelin, polar cod, whales, and little auk. The capelin is a key food for top predators such as the north-east Arctic cod, harp seals, and seabirds such as common guillemot and Brunnich's guillemot. The fisheries of the Barents Sea, in particular the cod fisheries, are of great importance for both Norway and Russia.
SIZEX-89 was an international winter experiment where the main objectives were to perform sensor signature studies of different ice types in order to develop SAR algorithms for ice variables such as ice types, ice concentrations and ice kinematics.
Although previous research suggested that predation by whales may be the cause of depleting fish stocks, more recent research suggests that marine mammal consumption has only a trivial influence on fisheries and a model examining the impact of fisheries and climate was far more accurate at describing trends in fish abundance. There is a genetically distinct polar bear population associated with the Barents Sea.
History.
Name.
The Barents Sea was formerly known to Russians as Murmanskoye Morye, or the "Sea of Murmans" (i.e., Norwegians), and it appears with this name in sixteenth-century maps, including Gerard Mercator's "Map of the Arctic" published in his 1595 atlas. Its eastern corner, in the region of the Pechora River's estuary, has been known as Pechorskoye Morye, that is, Pechora Sea.
This sea was given its present name in honor of Willem Barentsz, a Dutch navigator and explorer. Barentsz was the leader of early expeditions to the far north, at the end of the sixteenth century.
Modern Era.
Seabed mapping was completed in 1933 with the first full map produced by Russian marine geologist Maria Klenova.
The Barents Sea was also the site of a notable World War II engagement, a German surface merchant raiding attack on a British convoy that later became known as the Battle of the Barents Sea. Under the command of Oskar Kummetz, the German warships sank minelayer HMS "Bramble" and destroyer , but in turn lost destroyer "Z16 Friedrich Eckoldt" and "Admiral Hipper" was severely damaged by British gunfire. The Germans later retreated and the British convoy arrived safely at Murmansk shortly afterwards.
During the Cold War, the Soviet Red Banner Northern Fleet used the southern reaches of the sea as a ballistic missile submarine bastion, a strategy that Russia continues. Nuclear contamination from dumped Russian naval reactors is an environmental concern in the Barents Sea.
Economy.
Political Status.
For decades there was a boundary dispute between Norway and Russia regarding the position of the boundary between their respective claims to the Barents Sea. The Norwegians favoured a median line, based on the Geneva Convention of 1958, whereas the Russians favoured a meridian based sector line, based on a Soviet decision of 1926. This led to a neutral "grey" zone between the competing claims that had an area of 175,000 sq.km, which is approximately 12% of the total area of the Barents Sea. The two countries started negotiations on the location of the boundary in 1974 and a moratorium on hydrocarbon exploration was declared in 1976.
In 2010, Norway and Russia signed an agreement that placed the boundary equidistant from their competing claims, This was ratified and went into force on 7 July 2011, opening the grey zone for hydrocarbon exploration.
Oil and Gas.
Encouraged by the success of the North Sea in the 1960s, hydrocarbon exploration in the Barents Sea got underway in 1969. The Norwegian authorities acquired seismic reflection surveys through the following years, which were analysed to understand the location of the main sedimentary basins. NorskHydro drilled the first well in 1980, which was a dry hole, and the first discoveries were made the following year - the Alke and Askeladden gas fields. Several more discoveries were made on the Norwegian side of the Barents Sea throughout the 1980s, including the important Snøhvit field. However interest in the area began to wane due to a succession of dry holes, the wells only containing gas (which was cheap at the time) and the prohibitive costs of developing wells in such a remote area. Interest in the area was reignited in the late 2000s, after the Snovhit field was finally bought into production and two new large discoveries were made.
Exploration on the Russian side got underway around the same time as that on Norwegian side, encouraged by the success in the Timan-Pechora Basin. The first wells were drilled in the early 1980s and some very large gas fields were discovered throughout this decade. The Shtokman field was discovered in 1988 and is classed as a giant gas field—currently the 5th largest gas field in the world. However, due to the same reasons that interest declined in the Norwegian side of the Barents Sea, in addition to the political instability of the 1990s, interest in the Russian side of the Barents Sea declined.
Fishing.
The Barents Sea contains the world largest remaining cod population, as well as an important stocks of haddock and capelin. Fishing is managed jointly by Russia and Norway in the form of the Joint Norwegian–Russian Fisheries Commission, established in 1976, in an attempt to keep track of how many fish are leaving the ecosystem due to fishing. The Joint Norwegian-Russian Fisheries Commission sets Total Allowable Catches (TACs) for multiple species throughout their migratory tracks. Through the Commission, Norway and Russia also exchange fishing quotas and catch statistics to ensure the TACs are not being violated. However, there are problems with the system and the effects of fishing on the Barents Sea ecosystem are not completely accurate. A large portion of catches are not reported when the fishing boats land to account for profits that are being lost to high taxes and fees. Since many fishermen do not strictly follow the TACs and rules set forth by the Commission, the amount of fish being extracted annually from the Barents Sea is underestimated.
Barents Sea biodiversity and marine bioprospecting.
The Barents Sea, where temperate waters from the Gulf Stream and cold waters from the Arctic meet, is home to an enormous diversity of organisms, which are well adapted to the extreme conditions of their marine habitats. This makes these arctic species very attractive for marine bioprospecting. Marine bioprospecting may be defined as the search for bioactive molecules and compounds from marine sources having new, unique properties and the potential for commercial applications. Amongst others, applications include medicines, food and feed, textiles, cosmetics and the process industry.
The Norwegian government strategically supports the development of marine bioprospecting as it has the potential to contribute to new and sustainable wealth creation. Tromsø and the northern areas of Norway play a central role in this strategy due to excellent access to unique Arctic marine organisms and the presence of marine industries and R&D competence and infrastructure in this region.Since 2007 science and industry have cooperated closely on bioprospecting and the development and commercialization of new products.
Institutions and industry supporting marine bioprospecting in Barents Sea
MabCent-SFI is one of fourteen Research-Based Innovation Centers initiated by the Research Council of Norway, and is the only one within the field of “bioactive compounds and drug discovery” that is based on bioactives from marine organisms. MabCent-SFI maintains a focus on bioactives from Arctic and sub-Arctic organisms. By the end of 2011, MabCent has tested about 200,000 extracts, finding several hundred "hits". Through further research and development, some of these hits will become valuable "leads", i.e. characterized compounds known to possess biological effects of interest.
The commercial partners in MabCent-SFI are Biotec Pharmacon ASA and its subsidiary ArcticZymes AS, ABC BioScience AS, Lytix Biopharma AS and Pronova BioPharma ASA. ArcticZymes is also a partner in MARZymes, a project financed by the Research Council of Norway to find marine enzymes which are adapted to the extreme conditions in the Arctic. The science partners in MabCent-SFI are Marbank, a national marine biobank located in Tromsø, Marbio, a medium/high-throughput platform for screening and identification of bioactive compounds and Norstruct, a protein structure determination platform. Mabcent-SFI is hosted by the University of Tromsø.
 is an emerging biotechnology cluster of enterprises and R&D organizations, which cooperate closely with regional funding and development actors (triple helix). As bioactive molecules and compounds from Arctic marine resources form the basis of activities for the majority of the cluster members, BioTech North serves as a marine biotech cluster. The majority of BioTech North’s enterprises are active within life science applications and markets. To date the cluster contains around thirty organizations from both the private and public sector. It has received Arena status and is funded through the programme financed by , and The Research Council of Norway. Stakeholders of BioTech North include Barents BioCentre Lab, BioStruct, Marbank, Norut, Nofima, Mabcent-SFI, University of Tromsø, Unilab, Barentzymes AS, Trofi, Scandiderma AS, Prophylix Pharma AS, Olivita, Marealis, ProCelo, Probio, Lytix Biopharma, Integorgen, d'Liver, Genøk, Cognis, Clare AS, Chitinor, Calanus AS, Biotec Betaglucans, Ayanda, ArcticZymes AS, ABC Bioscience, Akvaplanniva.

</doc>
<doc id="44061" url="http://en.wikipedia.org/wiki?curid=44061" title="Convulsion">
Convulsion

A convulsion is a medical condition where body muscles contract and relax rapidly and repeatedly, resulting in an uncontrolled shaking of the body. Because a convulsion is often a symptom of an epileptic seizure, the term "convulsion" is sometimes used as a synonym for "seizure". However, not all epileptic seizures lead to convulsions, and not all convulsions are caused by epileptic seizures. Convulsions are also consistent with an electric shock and improper Enriched Air Scuba Diving. For non-epileptic convulsions, see non-epileptic seizures.
The word "fit" is sometimes used to mean a convulsion or epileptic seizure.
Symptoms.
When a person is having a convulsion, they may experience several different symptoms. These may include: a brief blackout, confusion, drooling, loss of bowel/bladder control, sudden shaking of entire body, uncontrollable muscle spasms, temporary cessation of breathing, and many more. Symptoms usually last from a few seconds to around 15 minutes. If someone has a fit like this, it is advised to make sure they don't fall and injure themselves, cushion their head and loosen any restricting clothing/jewelry, and also call for medical help. 
Causes.
Convulsions are often caused by some sort of electrical activity mishap in the brain. Often times, the cause is not able to be pinpointed. Convulsions can be caused by chemicals in the blood, as well as infections like meningitis or encephalitis. A very common cause of convulsions is fevers. Other possibilities include head trauma, stroke or lack of oxygen to the brain. Sometimes the convulsion can be caused by genetic defects or brain tumors. 
Grand Mal Seizures.
The most common type of seizure is called a Grand Mal Seizure, also known as a generalized convulsion. This is characterized by a loss of consciousness which may lead to the person collapsing. The body stiffens for about a minute and then jerks uncontrollably for the next minute. During this, the patient may fall and injure themselves or bite their tongue and lose control of their bladder. A familial history of this puts a person at a greater risk for developing them.
Lasting effects.
A common question about convulsions is whether or not they cause harm to the brain in the long run. It has been found that they can have a negative impact on cognition. Some research studies have found that subjects who have frequent convulsions have had decreased cognitive abilities as well as reduced verbal and visual-spatial abilities. The degree of complications arising from the convulsions depends on the location in the brain that is affected. The longer a patient suffers from convulsions, the more damage they are likely to have accumulated.

</doc>
<doc id="44062" url="http://en.wikipedia.org/wiki?curid=44062" title="X-ray astronomy">
X-ray astronomy

X-ray astronomy is an observational branch of astronomy which deals with the study of X-ray observation and detection from astronomical objects. X-radiation is absorbed by the Earth's atmosphere, so instruments to detect X-rays must be taken to high altitude by balloons, sounding rockets, and satellites. X-ray astronomy is the space science related to a type of space telescope that can see farther than standard light-absorption telescopes, such as the Mauna Kea Observatories, via x-ray radiation.
X-ray emission is expected from astronomical objects that contain extremely hot gasses at temperatures from about a million kelvin (K) to hundreds of millions of kelvin (MK). Although X-rays have been observed emanating from the Sun since the 1940s, the discovery in 1962 of the first cosmic X-ray source was a surprise. This source is called Scorpius X-1 (Sco X-1), the first X-ray source found in the constellation Scorpius. The X-ray emission of Scorpius X-1 is 10,000 times greater than its visual emission, whereas that of the Sun is about a million times less. In addition, the energy output in X-rays is 100,000 times greater than the total emission of the Sun in all wavelengths. Based on discoveries in this new field of X-ray astronomy, starting with Scorpius X-1, Riccardo Giacconi received the Nobel Prize in Physics in 2002. It is now known that such X-ray sources as Sco X-1 are compact stars, such as neutron stars or black holes. Material falling into a black hole may emit X-rays, but the black hole itself does not. The energy source for the X-ray emission is gravity. Infalling gas and dust is heated by the strong gravitational fields of these and other celestial objects.
Many thousands of X-ray sources are known. In addition, the space between galaxies in galaxy clusters is filled with a very hot, but very dilute gas at a temperature between 10 and 100 megakelvins (MK). The total amount of hot gas is five to ten times the total mass in the visible galaxies.
Sounding rocket flights.
The first sounding rocket flights for X-ray research were accomplished at the White Sands Missile Range in New Mexico with a V-2 rocket on January 28, 1949. A detector was placed in the nose cone section and the rocket was launched in a suborbital flight to an altitude just above the atmosphere.
X-rays from the Sun were detected by the U.S. Naval Research Laboratory Blossom experiment on board. An Aerobee 150 rocket was launched on June 12, 1962 and it detected the first X-rays from other celestial sources (Scorpius X-1).
The largest drawback to rocket flights is their very short duration (just a few minutes above the atmosphere before the rocket falls back to Earth) and their limited field of view. A rocket launched from the United States will not be able to see sources in the southern sky; a rocket launched from Australia will not be able to see sources in the northern sky.
X-ray Quantum Calorimeter (XQC) project.
In astronomy, the interstellar medium (or ISM) is the gas and cosmic dust that pervade interstellar space: the matter that exists between the star systems within a galaxy. It fills interstellar space and blends smoothly into the surrounding intergalactic medium. The interstellar medium consists of an extremely dilute (by terrestrial standards) mixture of ions, atoms, molecules, larger dust grains, cosmic rays, and (galactic) magnetic fields. The energy that occupies the same volume, in the form of electromagnetic radiation, is the interstellar radiation field.
Of interest is the hot ionized medium (HIM) consisting of a coronal cloud ejection from star surfaces at 106-107 K which emits X-rays. The ISM is turbulent and full of structure on all spatial scales. Stars are born deep inside large complexes of molecular clouds, typically a few parsecs in size. During their lives and deaths, stars interact physically with the ISM. Stellar winds from young clusters of stars (often with giant or supergiant HII regions surrounding them) and shock waves created by supernovae inject enormous amounts of energy into their surroundings, which leads to hypersonic turbulence. The resultant structures are stellar wind bubbles and superbubbles of hot gas. The Sun is currently traveling through the Local Interstellar Cloud, a denser region in the low-density Local Bubble.
To measure the spectrum of the diffuse X-ray emission from the interstellar medium over the energy range 0.07 to 1 keV, NASA launched a Black Brant 9 from White Sands Missile Range, New Mexico on May 1, 2008. The Principal Investigator for the mission is Dr. Dan McCammon of the University of Wisconsin.
Balloons.
Balloon flights can carry instruments to altitudes of up to 40 km above sea level, where they are above as much as 99.997% of the Earth's atmosphere. Unlike a rocket where data are collected during a brief few minutes, balloons are able to stay aloft for much longer. However, even at such altitudes, much of the X-ray spectrum is still absorbed. X-rays with energies less than 35 keV (5,600 aJ) cannot reach balloons. On July 21, 1964, the Crab Nebula supernova remnant was discovered to be a hard X-ray (15 – 60 keV) source by a scintillation counter flown on a balloon launched from Palestine, Texas, USA. This was likely the first balloon-based detection of X-rays from a discrete cosmic X-ray source.
High-energy focusing telescope.
The high-energy focusing telescope (HEFT) is a balloon-borne experiment to image astrophysical sources in the hard X-ray (20–100 keV) band. Its maiden flight took place in May 2005 from Fort Sumner, New Mexico, USA. The angular resolution of HEFT is ~1.5'. Rather than using a grazing-angle X-ray telescope, HEFT makes use of a novel tungsten-silicon multilayer coatings to extend the reflectivity of nested grazing-incidence mirrors beyond 10 keV. HEFT has an energy resolution of 1.0 keV full width at half maximum at 60 keV. HEFT was launched for a 25-hour balloon flight in May 2005. The instrument performed within specification and observed Tau X-1, the Crab Nebula.
High-resolution gamma-ray and hard X-ray spectrometer (HIREGS).
A balloon-borne experiments called the High-resolution gamma-ray and hard X-ray spectrometer (HIREGS) made observed in X-ray and gamma-rays It was launched from McMurdo Station, Antarctica in December 1991. Steady winds carried the balloon on a circumpolar flight lasting about two weeks.
Rockoons.
The rockoon (a portmanteau of rocket and balloon) was a solid fuel rocket that, rather than being immediately lit while on the ground, was first carried into the upper atmosphere by a gas-filled balloon. Then, once separated from the balloon at its maximum height, the rocket was automatically ignited. This achieved a higher altitude, since the rocket did not have to move through the lower thicker air layers that would have required much more chemical fuel.
The original concept of "rockoons" was developed by Cmdr. Lee Lewis, Cmdr. G. Halvorson, S. F. Singer, and James A. Van Allen during the Aerobee rocket firing cruise of the USS "Norton Sound" on March 1, 1949.
From July 17 to July 27, 1956, the Naval Research Laboratory (NRL) shipboard launched eight Deacon rockoons for solar ultraviolet and X-ray observations at ~30° N ~121.6° W, southwest of San Clemente Island, apogee: 120 km.
X-ray astronomy satellites.
X-ray astronomy satellites study X-ray emissions from celestial objects. Satellites, which can detect and transmit data about the X-ray emissions are deployed as part of branch of space science known as X-ray astronomy. Satellites are needed because X-radiation is absorbed by the Earth's atmosphere, so instruments to detect X-rays must be taken to high altitude by balloons, sounding rockets, and satellites.
X-ray telescopes and mirrors.
X-ray telescopes (XRTs) have varying directionality or imaging ability based on glancing angle reflection rather than refraction or large deviation reflection.
This limits them to much narrower fields of view than visible or UV telescopes. The mirrors can be made of ceramic or metal foil.
The first X-ray telescope in astronomy was used to observe the Sun. The first X-ray picture (taken with a grazing incidence telescope) of the Sun was taken in 1963, by a rocket-borne telescope. On April 19, 1960 the very first X-ray image of the sun was taken using a pinhole camera on an Aerobee-Hi rocket.
The utilization of X-ray mirrors for extrasolar X-ray astronomy simultaneously requires:
X-ray astronomy detectors.
X-ray astronomy detectors have been designed and configured primarily for energy and occasionally for wavelength detection using a variety of techniques usually limited to the technology of the time.
X-ray detectors collect individual X-rays (photons of X-ray electromagnetic radiation) and count the number of photons collected (intensity), the energy (0.12 to 120 keV) of the photons collected, wavelength (~0.008 to 8 nm), or how fast the photons are detected (counts per hour), to tell us about the object that is emitting them.
Astrophysical sources of X-rays.
Several types of astrophysical objects emit, fluoresce, or reflect X-rays, from galaxy clusters, through black holes in active galactic nuclei (AGN) to galactic objects such as supernova remnants, stars, and binary stars containing a white dwarf (cataclysmic variable stars and super soft X-ray sources), neutron star or black hole (X-ray binaries). Some solar system bodies emit X-rays, the most notable being the Moon, although most of the X-ray brightness of the Moon arises from reflected solar X-rays. A combination of many unresolved X-ray sources is thought to produce the observed X-ray background. The X-ray continuum can arise from bremsstrahlung, black-body radiation, synchrotron radiation, or what is called inverse Compton scattering of lower-energy photons by relativistic electrons, knock-on collisions of fast protons with atomic electrons, and atomic recombination, with or without additional electron transitions.
An intermediate-mass X-ray binary (IMXB) is a binary star system where one of the components is a neutron star or a black hole. The other component is an intermediate mass star.
Hercules X-1 is composed of a neutron star accreting matter from a normal star (HZ Herculis) probably due to Roche lobe overflow. X-1 is the prototype for the massive X-ray binaries although it falls on the borderline, ~2 M☉, between high- and low-mass X-ray binaries.
Celestial X-ray sources.
The celestial sphere has been divided into 88 constellations. The International Astronomical Union (IAU) constellations are areas of the sky. Each of these contains remarkable X-ray sources. Some of them are have been identified from astrophysical modeling to be galaxies or black holes at the centers of galaxies. Some are pulsars. As with sources already successfully modeled by X-ray astrophysics, striving to understand the generation of X-rays by the apparent source helps to understand the Sun, the universe as a whole, and how these affect us on Earth. Constellations are an astronomical device for handling observation and precision independent of current physical theory or interpretation. Astronomy has been around for a long time. Physical theory changes with time. With respect to celestial X-ray sources, X-ray astrophysics tends to focus on the physical reason for X-ray brightness, whereas X-ray astronomy tends to focus on their classification, order of discovery, variability, resolvability, and their relationship with nearby sources in other constellations.
Within the constellations Orion and Eridanus and stretching across them is a soft X-ray "hot spot" known as the Orion-Eridanus Superbubble, the Eridanus Soft X-ray Enhancement, or simply the Eridanus Bubble, a 25° area of interlocking arcs of Hα emitting filaments. Soft X-rays are emitted by hot gas (T ~ 2–3 MK) in the interior of the superbubble. This bright object forms the background for the "shadow" of a filament of gas and dust. The filament is shown by the overlaid contours, which represent 100 micrometre emission from dust at a temperature of about 30 K as measured by IRAS. Here the filament absorbs soft X-rays between 100 and 300 eV, indicating that the hot gas is located behind the filament. This filament may be part of a shell of neutral gas that surrounds the hot bubble. Its interior is energized by ultraviolet (UV) light and stellar winds from hot stars in the Orion OB1 association. These stars energize a superbubble about 1200 lys across which is observed in the visual (Hα) and X-ray portions of the spectrum.
Proposed (future) X-ray observatory satellites.
There are several projects that are proposed for X-ray observatory satellites. See main article link above.
Explorational X-ray astronomy.
Usually observational astronomy is considered to occur on Earth's surface (or beneath it in neutrino astronomy). The idea of limiting observation to Earth includes orbiting the Earth. As soon as the observer leaves the cozy confines of Earth, the observer becomes a deep space explorer. Except for Explorer 1 and Explorer 3 and the earlier satellites in the series, usually if a probe is going to be a deep space explorer it leaves the Earth or an orbit around the Earth.
For a satellite or space probe to qualify as a deep space X-ray astronomer/explorer or "astronobot"/explorer, all it needs to carry aboard is an XRT or X-ray detector and leave Earth orbit.
Ulysses is launched October 6, 1990, and reached Jupiter for its "gravitational slingshot" in February 1992. It passed the south solar pole in June 1994 and crossed the ecliptic equator in February 1995. The solar X-ray and cosmic gamma-ray burst experiment (GRB) had 3 main objectives: study and monitor solar flares, detect and localize cosmic gamma-ray bursts, and in-situ detection of Jovian aurorae. Ulysses was the first satellite carrying a gamma burst detector which went outside the orbit of Mars. The hard X-ray detectors operated in the range 15–150 keV. The detectors consisted of 23-mm thick × 51-mm diameter CsI(Tl) crystals mounted via plastic light tubes to photomultipliers. The hard detector changed its operating mode depending on (1) measured count rate, (2) ground command, or (3) change in spacecraft telemetry mode. The trigger level was generally set for 8-sigma above background and the sensitivity is 10−6 erg/cm2 (1 nJ/m2). When a burst trigger is recorded, the instrument switches to record high resolution data, recording it to a 32-kbit memory for a slow telemetry read out. Burst data consist of either 16 s of 8-ms resolution count rates or 64 s of 32-ms count rates from the sum of the 2 detectors. There were also 16 channel energy spectra from the sum of the 2 detectors (taken either in 1, 2, 4, 16, or 32 second integrations). During 'wait' mode, the data were taken either in 0.25 or 0.5 s integrations and 4 energy channels (with shortest integration time being 8 s). Again, the outputs of the 2 detectors were summed.
The Ulysses soft X-ray detectors consisted of 2.5-mm thick × 0.5 cm2 area Si surface barrier detectors. A 100 mg/cm2 beryllium foil front window rejected the low energy X-rays and defined a conical FOV of 75° (half-angle). These detectors were passively cooled and operate in the temperature range −35 to −55 °C. This detector had 6 energy channels, covering the range 5–20 keV.
Theoretical X-ray astronomy.
Theoretical X-ray astronomy is a branch of theoretical astronomy that deals with the theoretical astrophysics and theoretical astrochemistry of X-ray generation, emission, and detection as applied to astronomical objects.
Like theoretical astrophysics, theoretical X-ray astronomy uses a wide variety of tools which include analytical models to approximate the behavior of a possible X-ray source and computational numerical simulations to approximate the observational data. Once potential observational consequences are available they can be compared with experimental observations. Observers can look for data that refutes a model or helps in choosing between several alternate or conflicting models.
Theorists also try to generate or modify models to take into account new data. In the case of an inconsistency, the general tendency is to try to make minimal modifications to the model to fit the data. In some cases, a large amount of inconsistent data over time may lead to total abandonment of a model.
Most of the topics in astrophysics, astrochemistry, astrometry, and other fields that are branches of astronomy studied by theoreticians involve X-rays and X-ray sources. Many of the beginnings for a theory can be found in an Earth-based laboratory where an X-ray source is built and studied.
Dynamos.
Dynamo theory describes the process through which a rotating, convecting, and electrically conducting fluid acts to maintain a magnetic field. This theory is used to explain the presence of anomalously long-lived magnetic fields in astrophysical bodies. If some of the stellar magnetic fields are really induced by dynamos, then field strength might be associated with rotation rate.
Astronomical models.
From the observed X-ray spectrum, combined with spectral emission results for other wavelength ranges, an astronomical model addressing the likely source of X-ray emission can be constructed. For example, with Scorpius X-1 the X-ray spectrum steeply drops off as X-ray energy increases up to 20 keV, which is likely for a thermal-plasma mechanism. In addition, there is no radio emission, and the visible continuum is roughly what would be expected from a hot plasma fitting the observed X-ray flux. The plasma could be a coronal cloud of a central object or a transient plasma, where the energy source is unknown, but could be related to the idea of a close binary.
In the Crab Nebula X-ray spectrum there are three features that differ greatly from Scorpius X-1: its spectrum is much harder, its source diameter is in light-years (ly)s, not astronomical units (AU), and its radio and optical synchrotron emission are strong. Its overall X-ray luminosity rivals the optical emission and could be that of a nonthermal plasma. However, the Crab Nebula appears as an X-ray source that is a central freely expanding ball of dilute plasma, where the energy content is 100 times the total energy content of the large visible and radio portion, obtained from the unknown source.
The "Dividing Line" as giant stars evolve to become red giants also coincides with the Wind and Coronal Dividing Lines. To explain the drop in X-ray emission across these dividing lines, a number of models have been proposed:
Analytical X-ray astronomy.
Analytical X-ray astronomy is applied to an astronomy puzzle in an attempt to provide an acceptable solution. Consider the following puzzle.
High-mass X-ray binaries (HMXBs) are composed of OB supergiant companion stars and compact objects, usually neutron stars (NS) or black holes (BH). Supergiant X-ray binaries (SGXBs) are HMXBs in which the compact objects orbit massive companions with orbital periods of a few days (3–15 d), and in circular (or slightly eccentric) orbits. SGXBs show typical the hard X-ray spectra of accreting pulsars and most show strong absorption as obscured HMXBs. X-ray luminosity ("L"x) increases up to 1036 erg·s−1 (1029 watts).
The mechanism triggering the different temporal behavior observed between the classical SGXBs and the recently discovered supergiant fast X-ray transients (SFXT)s is still debated.
Aim: use the discovery of long orbits (>15 d) to help discriminate between emission models and perhaps bring constraints on the models.
Method: analyze archival data on various SGXBs such as has been obtained by INTEGRAL for candidates exhibiting long orbits. Build short- and long-term light curves. Perform a timing analysis in order to study the temporal behavior of each candidate on different time scales.
Compare various astronomical models:
Draw some conclusions: for example, the SGXB SAX J1818.6-1703 was discovered by BeppoSAX in 1998, identified as a SGXB of spectral type between O9I−B1I, which also displayed short and bright flares and an unusually very low quiescent level leading to its classification as a SFXT. The analysis indicated an unusually long orbital period: 30.0 ± 0.2 d and an elapsed accretion phase of ~6 d implying an elliptical orbit and possible supergiant spectral type between B0.5-1I with eccentricities e ~ 0.3–0.4. The large variations in the X-ray flux can be explained through accretion of macro-clumps formed within the stellar wind.
Choose which model seems to work best: for SAX J1818.6-1703 the analysis best fits the model that predicts SFXTs behave as SGXBs with different orbital parameters; hence, different temporal behavior.
Stellar X-ray astronomy.
Stellar X-ray astronomy is said to have started on April 5, 1974, with the detection of X-rays from Capella. A rocket flight on that date briefly calibrated its attitude control system when a star sensor pointed the payload axis at Capella (α Aur). During this period, X-rays in the range 0.2–1.6 keV were detected by an X-ray reflector system co-aligned with the star sensor. The X-ray luminosity of "L"x = 1031 erg·s−1 (1024 W) is four orders of magnitude above the Sun's X-ray luminosity.
Eta Carinae.
New X-ray observations by the Chandra X-ray Observatory show three distinct structures: an outer, horseshoe-shaped ring about 2 light years in diameter, a hot inner core about 3 light-months in diameter, and a hot central source less than 1 light-month in diameter which may contain the superstar that drives the whole show. The outer ring provides evidence of another large explosion that occurred over 1,000 years ago. These three structures around Eta Carinae are thought to represent shock waves produced by matter rushing away from the superstar at supersonic speeds. The temperature of the shock-heated gas ranges from 60 MK in the central regions to 3 MK on the horseshoe-shaped outer structure. "The Chandra image contains some puzzles for existing ideas of how a star can produce such hot and intense X-rays," says Prof. Kris Davidson of the University of Minnesota. Davidson is principal investigator for the Eta Carina observations by the Hubble Space telescope. "In the most popular theory, X-rays are made by colliding gas streams from two stars so close together that they'd look like a point source to us. But what happens to gas streams that escape to farther distances? The extended hot stuff in the middle of the new image gives demanding new conditions for any theory to meet."
Stellar coronae.
Coronal stars, or stars within a coronal cloud, are ubiquitous among the stars in the cool half of the Hertzsprung-Russell diagram. Experiments with instruments aboard Skylab and Copernicus have been used to search for soft X-ray emission in the energy range ~0.14–0.284 keV from stellar coronae. The experiments aboard ANS succeeded in finding X-ray signals from Capella and Sirius (α CMa). X-ray emission from an enhanced solar-like corona was proposed for the first time. The high temperature of Capella's corona as obtained from the first coronal X-ray spectrum of Capella using HEAO 1 required magnetic confinement unless it was a free-flowing coronal wind.
In 1977 Proxima Centauri is discovered to be emitting high-energy radiation in the XUV. In 1978, α Cen was identified as a low-activity coronal source. With the operation of the Einstein observatory, X-ray emission was recognized as a characteristic feature common to a wide range of stars covering essentially the whole Hertzsprung-Russell diagram. The Einstein initial survey led to significant insights:
To fit the medium-resolution spectrum of UX Ari, subsolar abundances were required.
Stellar X-ray astronomy is contributing toward a deeper understanding of
Current wisdom has it that the massive coronal main sequence stars are late-A or early F stars, a conjecture that is supported both by observation and by theory.
Unstable winds.
Given the lack of a significant outer convection zone, theory predicts the absence of a magnetic dynamo in earlier A stars. In early stars of spectral type O and B, shocks developing in unstable winds are the likely source of X-rays.
Coolest M dwarfs.
Beyond spectral type M5, the classical αω dynamo can no longer operate as the internal structure of dwarf stars changes significantly: they become fully convective. As a distributed (or α2) dynamo may become relevant, both the magnetic flux on the surface and the topology of the magnetic fields in the corona should systematically change across this transition, perhaps resulting in some discontinuities in the X-ray
characteristics around spectral class dM5. However, observations do not seem to support this picture: long-time lowest-mass X-ray detection, VB 8 (M7e V), has shown steady emission at levels of X-ray luminosity ("L"X) ≈ 1026 erg·s−1 (1019 W) and flares up to an order of magnitude higher. Comparison with other late M dwarfs shows a rather continuous trend.
Strong X-ray emission from Herbig Ae/Be stars.
Herbig Ae/Be stars are pre-main sequence stars. As to their X-ray emission properties, some are
The nature of these strong emissions has remained controversial with models including
K giants.
The FK Com stars are giants of spectral type K with an unusually rapid rotation and signs of extreme activity. Their X-ray coronae are among the most luminous ("L"X ≥ 1032 erg·s−1 or 1025 W) and the hottest known with dominant temperatures up to 40 MK. However, the current popular hypothesis involves a merger of a close binary system in which the orbital angular momentum of the companion is transferred to the primary.
Pollux is the brightest star in the constellation Gemini, despite its Beta designation, and the 17th brightest in the sky. Pollux is a giant orange K star that makes an interesting color contrast with its white "twin", Castor. Evidence has been found for a hot, outer, magnetically supported corona around Pollux, and the star is known to be an X-ray emitter.
Amateur X-ray astronomy.
Collectively, amateur astronomers observe a variety of celestial objects and phenomena sometimes with equipment that they build themselves. The United States Air Force Academy (USAFA) is the home of the US's only undergraduate satellite program, and has and continues to develop the FalconLaunch sounding rockets. In addition to any direct amateur efforts to put X-ray astronomy payloads into space, there are opportunities that allow student-developed experimental payloads to be put on board commercial sounding rockets as a free-of-charge ride.
There are major limitations to amateurs observing and reporting experiments in X-ray astronomy: the cost of building an amateur rocket or balloon to place a detector high enough and the cost of appropriate parts to build a suitable X-ray detector.
History of X-ray astronomy.
In 1927, E.O. Hulburt of the US Naval Research Laboratory and associates Gregory Breit and Merle A. Tuve of the Carnegie Institution of Washington explored the possibility of equipping Robert H. Goddard's rockets to explore the upper atmosphere. "Two years later, he proposed an experimental program in which a rocket might be instrumented to explore the upper atmosphere, including detection of ultraviolet radiation and X-rays at high altitudes".
In the late 1930s, the presence of a very hot, tenuous gas surrounding the Sun was inferred indirectly from optical coronal lines of highly ionized species. The Sun has been known to be surrounded by a hot tenuous corona. In the mid-1940s radio observations revealed a radio corona around the Sun.
The beginning of the search for X-ray sources from above the Earth's atmosphere was on August 5, 1948 12:07 GMT. A US Army (formerly German) V-2 rocket as part of Project Hermes was launched from White Sands Proving Grounds. The first solar X-rays were recorded by T. Burnight.
Through the 1960s, 70s, 80s, and 90s, the sensitivity of detectors increased greatly during the 60 years of X-ray astronomy. In addition, the ability to focus X-rays has developed enormously—allowing the production of high-quality images of many fascinating celestial objects.
Major questions in X-ray astronomy.
As X-ray astronomy uses a major spectral probe to peer into source, it is a valuable tool in efforts to understand many puzzles.
Stellar magnetic fields.
Magnetic fields are ubiquitous among stars, yet we do not understand precisely why, nor have we fully understood the bewildering variety of plasma physical mechanisms that act in stellar environments. Some stars, for example, seem to have magnetic fields, fossil stellar magnetic fields left over from their period of formation, while others seem to generate the field anew frequently.
Extrasolar X-ray source astrometry.
With the initial detection of an extrasolar X-ray source, the first question usually asked is "What is the source?" An extensive search is often made in other wavelengths such as visible or radio for possible coincident objects. Many of the verified X-ray locations still do not have readily discernible sources. X-ray astrometry becomes a serious concern that results in ever greater demands for finer angular resolution and spectral radiance.
There are inherent difficulties in making X-ray/optical, X-ray/radio, and X-ray/X-ray identifications based solely on positional coincidents, especially with handicaps in making identifications, such as the large uncertainties in positional determinants made from balloons and rockets, poor source separation in the crowded region toward the galactic center, source variability, and the multiplicity of source nomenclature.
X‐ray source counterparts to stars can be identified by calculating the angular separation between source centroids and position of the star. The maximum allowable separation is a compromise between a larger value to identify as many real matches as possible and a smaller value to minimize the probability of spurious matches. "An adopted matching criterion of 40" finds nearly all possible X‐ray source matches while keeping the probability of any spurious matches in the sample to 3%."
Solar X-ray astronomy.
All of the detected X-ray sources at, around, or near the Sun are within or associated with the coronal cloud which is its outer atmosphere.
Coronal heating problem.
In the area of solar X-ray astronomy, there is the coronal heating problem. The photosphere of the Sun has an effective temperature of 5,570 K yet its corona has an average temperature of 1–2 × 106 K. However, the hottest regions are 8–20 × 106 K. The high temperature of the corona shows that it is heated by something other than direct heat conduction from the photosphere.
It is thought that the energy necessary to heat the corona is provided by turbulent motion in the convection zone below the photosphere, and two main mechanisms have been proposed to explain coronal heating. The first is wave heating, in which sound, gravitational or magnetohydrodynamic waves are produced by turbulence in the convection zone. These waves travel upward and dissipate in the corona, depositing their energy in the ambient gas in the form of heat. The other is magnetic heating, in which magnetic energy is continuously built up by photospheric motion and released through magnetic reconnection in the form of large solar flares and myriad similar but smaller events—nanoflares.
Currently, it is unclear whether waves are an efficient heating mechanism. All waves except Alfvén waves have been found to dissipate or refract before reaching the corona. In addition, Alfvén waves do not easily dissipate in the corona. Current research focus has therefore shifted towards flare heating mechanisms.
Coronal mass ejection.
A coronal mass ejection (CME) is an ejected plasma consisting primarily of electrons and protons (in addition to small quantities of heavier elements such as helium, oxygen, and iron), plus the entraining coronal closed magnetic field regions. Evolution of these closed magnetic structures in response to various photospheric motions over different time scales (convection, differential rotation, meridional circulation) somehow leads to the CME. Small-scale energetic signatures such as plasma heating (observed as compact soft X-ray brightening) may be indicative of impending CMEs.
The soft X-ray sigmoid (an S-shaped intensity of soft X-rays) is an observational manifestation of the connection between coronal structure and CME production. "Relating the sigmoids at X-ray (and other) wavelengths to magnetic structures and current systems in the solar atmosphere is the key to understanding their relationship to CMEs."
The first detection of a Coronal mass ejection (CME) as such was made on December 1, 1971 by R. Tousey of the US Naval Research Laboratory using OSO 7. Earlier observations of coronal transients or even phenomena observed visually during solar eclipses are now understood as essentially the same thing.
The largest geomagnetic perturbation, resulting presumably from a "prehistoric" CME, coincided with the first-observed solar flare, in 1859. The flare was observed visually by Richard Christopher Carrington and the geomagnetic storm was observed with the recording magnetograph at Kew Gardens. The same instrument recorded a crotchet, an instantaneous perturbation of the Earth's ionosphere by ionizing soft X-rays. This could not easily be understood at the time because it predated the discovery of X-rays (by Roentgen) and the recognition of the ionosphere (by Kennelly and Heaviside).
Exotic X-ray sources.
A microquasar is a smaller cousin of a quasar that is a radio emitting X-ray binary, with an often resolvable pair of radio jets.
LSI+61°303 is a periodic, radio-emitting binary system that is also the gamma-ray source, CG135+01.
Observations are revealing a growing number of recurrent X-ray transients, characterized by short outbursts with very fast rise times (tens of minutes) and typical durations of a few hours that are associated with OB supergiants and hence define a new class of massive X-ray binaries: Supergiant Fast X-ray Transients (SFXTs).
Observations made by Chandra indicate the presence of loops and rings in the hot X-ray emitting gas that surrounds Messier 87. A magnetar is a type of neutron star with an extremely powerful magnetic field, the decay of which powers the emission of copious amounts of high-energy electromagnetic radiation, particularly X-rays and gamma rays.
X-ray dark stars.
During the solar cycle, as shown in the sequence of images at right, at times the Sun is almost X-ray dark, almost an X-ray variable. Betelgeuse, on the other hand, appears to be always X-ray dark. Hardly any X-rays are emitted by red giants. There is a rather abrupt onset of X-ray emission around spectral type A7-F0, with a large range of luminosities developing across spectral class F. Altair is spectral type A7V and Vega is A0V. Altair's total X-ray luminosity is at least an order of magnitude larger than the X-ray luminosity for Vega. The outer convection zone of early F stars is expected to be very shallow and absent in A-type dwarfs, yet the acoustic flux from the interior reaches a maximum for late A and early F stars provoking investigations of magnetic activity in A-type stars along three principal lines. Chemically peculiar stars of spectral type Bp or Ap are appreciable magnetic radio sources, most Bp/Ap stars remain undetected, and of those reported early on as producing X-rays only few of them can be identified as probably single stars. X-ray observations offer the possibility to detect (X-ray dark) planets as they eclipse part of the corona of their parent star while in transit. "Such methods are particularly promising for low-mass stars as a Jupiter-like planet could eclipse a rather significant coronal area."
X-ray dark planet/comet.
X-ray observations offer the possibility to detect (X-ray dark) planets as they eclipse part of the corona of their parent star while in transit. "Such methods are particularly promising for low-mass stars as a Jupiter-like planet could eclipse a rather significant coronal area."
As X-ray detectors have become more sensitive, they have observed that some planets and other normally X-ray non-luminescent celestial objects under certain conditions emit, fluoresce, or reflect X-rays.
Comet Lulin.
NASA's Swift Gamma-ray Explorer satellite was monitoring Comet Lulin as it closed to 63 Gm of Earth. For the first time, astronomers can see simultaneous UV and X-ray images of a comet. "The solar wind—a fast-moving stream of particles from the sun—interacts with the comet's broader cloud of atoms. This causes the solar wind to light up with X-rays, and that's what Swift's XRT sees", said Stefan Immler, of the Goddard Space Flight Center. This interaction, called charge exchange, results in X-rays from most comets when they pass within about three times Earth's distance from the Sun. Because Lulin is so active, its atomic cloud is especially dense. As a result, the X-ray-emitting region extends far sunward of the comet.
Single X-ray stars.
In addition to the Sun there are many unary stars or star systems throughout the galaxy that emit X-rays. β Hydri (G2 IV) is a normal single, post main-sequence subgiant star, "T"eff = 5800 K. It exhibits coronal X-ray fluxes.
The benefit of studying single stars is that it allows measurements free of any effects of a companion or being a part of a multiple star system. Theories or models can be more readily tested. See, e.g., Betelgeuse, Red giants, and Vega and Altair.

</doc>
<doc id="44063" url="http://en.wikipedia.org/wiki?curid=44063" title="Extragalactic astronomy">
Extragalactic astronomy

Extragalactic astronomy is the branch of astronomy concerned with objects outside our own Milky Way galaxy. In other words, it is the study of all astronomical objects which are not covered by galactic astronomy, the next level of galactic astronomy.
As instrumentation has improved, more distant objects can now be examined in detail. It is therefore useful to sub-divide this branch into Near-Extragalactic Astronomy and Far-Extragalactic Astronomy. The former deals with objects such as the galaxies of our Local Group, which are close enough to allow very detailed analyses of their contents (e.g. supernova remnants, stellar associations). The latter describes the study of objects sufficiently far away that only the brightest phenomena are observable.
Some topics include:

</doc>
<doc id="44069" url="http://en.wikipedia.org/wiki?curid=44069" title="Vulcan (hypothetical planet)">
Vulcan (hypothetical planet)

Vulcan is a small hypothetical planet that was proposed to exist in an orbit between Mercury and the Sun. Attempting to explain peculiarities of Mercury's orbit, the 19th-century French mathematician Urbain Le Verrier hypothesized that they were the result of another planet, which he named "Vulcan". 
A number of reputable investigators became involved in the search for Vulcan, but no such planet was ever found, and the peculiarities in Mercury's orbit have now been explained by Albert Einstein's theory of general relativity. Searches of NASA's two STEREO spacecraft data have failed to detect any vulcanoids between Mercury and the Sun that might have accounted for claimed observations of Vulcan. It is doubtful that there are any vulcanoids larger than 5.7 km in diameter. Other than Mercury, asteroid 2007 EB26 with a semi-major axis of 0.55 AU has the smallest known semi-major axis of any known object orbiting the Sun.
Argument for Vulcan's existence.
In 1840, François Arago, the director of the Paris Observatory, suggested to the French mathematician Urbain Le Verrier that he work on the topic of the planet Mercury's orbital motion around the Sun. The goal of this study was to construct a model based on Sir Isaac Newton's laws of motion and gravitation. By 1843, Le Verrier published his provisional theory on the subject, which would be tested during a transit of Mercury across the face of the Sun in 1843. As it turned out, predictions from Le Verrier's theory failed to match the observations.
Le Verrier renewed his work and, in 1859, published a more thorough study of Mercury's motion. This was based on a series of meridian observations of the planet as well as 14 transits. The rigor of this study meant that any differences from observation would be caused by some unknown factor. Indeed, there still remained some discrepancy. During Mercury's orbit, its perihelion advances by a small amount each orbit, technically called perihelion precession. The phenomenon is predicted by classical mechanics, but the observed value differed from the predicted value by the small amount of 43 arcseconds per century.
Le Verrier postulated that the excess precession could be explained by the presence of a small planet inside the orbit of Mercury, and he proposed the name "Vulcan" for this object. In Roman mythology, Vulcan was the god of beneficial and hindering fire, including the fire of volcanoes, making it an apt name for a planet so close to the Sun. Le Verrier's recent success in discovering the planet Neptune using the same techniques lent veracity to his claim, and astronomers around the world attempted to observe a new planet there, but nothing was ever found.
The search for Vulcan.
In December 1859, Le Verrier received a letter from a French physician and amateur astronomer called Edmond Modeste Lescarbault, who claimed to have seen a transit of the hypothetical planet earlier in the year. Le Verrier took the train to the village of Orgères-en-Beauce, some 70 kilometres southwest of Paris, where Lescarbault had built himself a small observatory. Le Verrier arrived unannounced and proceeded to interrogate the man.
Lescarbault described in detail how, on 26 March 1859, he noticed a small black dot on the face of the Sun, which he was studying with his modest 3.75 in refractor. Thinking it to be a sunspot, Lescarbault was not at first surprised, but after some time had passed he realized that it was moving. Having observed the transit of Mercury in 1845, he guessed that what he was observing was another transit, but of a previously undiscovered body. He took some hasty measurements of its position and direction of motion, and using an old clock and a pendulum with which he took his patients’ pulses, he estimated the duration of the transit at 1 hour, 17 minutes and 9 seconds.
Le Verrier was satisfied that Lescarbault had seen the transit of a previously unknown planet. On 2 January 1860 he announced the discovery of Vulcan to a meeting of the Académie des Sciences in Paris. Lescarbault, for his part, was awarded the Légion d'honneur and invited to appear before numerous learned societies.
Not everyone accepted the veracity of Lescarbault's "discovery", however. An eminent French astronomer, Emmanuel Liais, who was working for the Brazilian government in Rio de Janeiro in 1859, claimed to have been studying the surface of the Sun with a telescope twice as powerful as Lescarbault's at the very moment that Lescarbault said he observed his mysterious transit. Liais, therefore, was "in a condition to deny, in the most positive manner, the passage of a planet over the sun at the time indicated".
Based on Lescarbault’s "transit", Le Verrier computed Vulcan’s orbit: it supposedly revolved about the Sun in a nearly circular orbit at a distance of 21 million kilometres, or 0.14 astronomical units. The period of revolution was 19 days and 17 hours, and the orbit was inclined to the ecliptic by 12 degrees and 10 minutes (an incredible degree of precision). As seen from the Earth, Vulcan’s greatest elongation from the Sun was 8 degrees.
Numerous reports—all of them unreliable—began to reach Le Verrier from other amateurs who claimed to have seen unexplained transits. Some of these reports referred to observations made many years earlier, and many could not be properly dated. Nevertheless, Le Verrier continued to tinker with Vulcan’s orbital parameters as each new reported sighting reached him. He frequently announced dates of future Vulcan transits, and when these failed to materialize, he tinkered with the parameters some more.
Among the earlier alleged observers of Vulcan, the following are the most noteworthy:
Shortly after eight o'clock on the morning of 29 January 1860, F A R Russell and three other people saw an alleged transit of an intra-Mercurial planet from London. An American observer, Richard Covington, many years later claimed to have seen a well-defined black spot progress across the Sun’s disk around 1860, when he was stationed in Washington Territory.
No "observations" of Vulcan were made in 1861. Then, on the morning of 22 March 1862, between eight and nine o’clock Greenwich Time, another amateur astronomer, a Mr Lummis of Manchester, England, saw a transit. His colleague whom he alerted also saw the event. Based on these two men's reports, two French astronomers, Benjamin Valz and Rodolphe Radau, independently calculated the object’s supposed orbital period, with Valz deriving a figure of 17 days and 13 hours, and Radau a figure of 19 days and 22 hours.
On 8 May 1865 another French astronomer, Aristide Coumbrary, observed an unexpected transit from Istanbul, Turkey.
Between 1866 and 1878 no reliable observations of the hypothetical planet were made. Then, during the total solar eclipse of 29 July 1878, two experienced astronomers, Professor James Craig Watson, the director of the Ann Arbor Observatory in Michigan, and Lewis Swift, an amateur from Rochester, New York, both claimed to have seen a Vulcan-type planet close to the Sun. Watson, observing from Separation, Wyoming, placed the planet about 2.5 degrees southwest of the Sun, and estimated its magnitude at 4.5. Swift, who was observing the eclipse from a location near Denver, Colorado, saw what he took to be an intra-mercurial planet about 3 degrees southwest of the Sun. He estimated its brightness to be the same as that of Theta Cancri, a fifth-magnitude star which was also visible during totality, about six or seven minutes from the "planet". Theta Cancri and the planet were very nearly in line with the centre of the Sun.
Watson and Swift had reputations as excellent observers. Watson had already discovered more than twenty asteroids, while Swift had several comets named after him. Both described the colour of their hypothetical intra-mercurial planet as "red". Watson reported that it had a definite disk—unlike stars, which appear in telescopes as mere points of light—and that its phase indicated that it was approaching superior conjunction.
These are merely the more "reliable observations" of alleged intra-Mercurial planets. For half a century or more, many other observers tried to find the hypothetical Vulcan. Many false alarms were triggered by round sunspots that closely resembled planets in transit. During solar eclipses, stars close to the Sun were mistaken for planets. At one point, to reconcile different observations, at least two intra-mercurial planets were postulated.
Search conclusion.
In 1877 Le Verrier died, convinced to the end of having discovered another planet. With the loss of its principal proponent, however, the search for Vulcan abated. After many years of searching, astronomers were seriously doubting the planet's existence.
In 1915 Einstein's theory of relativity, an entirely different approach to understanding gravity than classical mechanics, solved the problem. His equations predicted exactly the observed amount of advance of Mercury's perihelion without any recourse to the existence of a hypothetical Vulcan. The new theory modified the predicted orbits of all planets, but the magnitude of the differences from Newtonian theory diminishes rapidly as one gets farther from the Sun. Also, Mercury's fairly eccentric orbit makes it much easier to detect the perihelion shift than is the case for the nearly circular orbits of Venus and Earth.
Vulcanoids.
Observing a planet inside the orbit of Mercury would be extremely difficult, since the telescope must be pointed very close to the Sun, where the sky is never black. Also, an error in pointing the telescope can result in damage for the optics, and injury to the observer. The huge amount of light present even quite far away from the Sun can produce false reflections inside the optics, thus fooling the observer into seeing things that do not exist.
The best observational strategy might be to monitor the Sun's disk for possible transits, but transits would only be seen from Earth provided the object orbits close enough to the ecliptic plane. A small, dark spot might be seen to move across the Sun's disk, as happens with transits of Mercury and Venus.
In 1915, when Einstein successfully explained the apparent anomaly in Mercury's orbit, most astronomers abandoned the search for Vulcan. A few, however, remained convinced that not all the alleged observations of Vulcan were unfounded. Among these was Henry C Courten, of Dowling College, New York. Studying photographic plates of the 1970 eclipse of the Sun, he and his associates detected several objects which appeared to be in orbits close to the Sun. Even accounting for artifacts, Courten felt that at least seven of the objects were real. 
Courten believed that an intra-Mercurial planetoid between 130 and 800 kilometres in diameter was orbiting the Sun at a distance of about 0.1 AU. Other images on his eclipse plates led him to postulate the existence of an asteroid belt between Mercury and the Sun.
None of these claims has ever been substantiated after more than forty years of observation. It has been surmised, however, that some of these objects—and other alleged intra-Mercurial objects—may exist, being nothing more than previously unknown comets or small asteroids. Today, the search continues for these so-called vulcanoid asteroids, which are thought to exist in the region where Vulcan was once sought. None have been found yet and searches have ruled out any such asteroids larger than about 6 km. Neither SOHO nor STEREO has detected a planet inside the orbit of Mercury.

</doc>
<doc id="44070" url="http://en.wikipedia.org/wiki?curid=44070" title="Avro Vulcan">
Avro Vulcan

The Avro Vulcan (officially Hawker Siddeley Vulcan from July 1963) is a jet-powered delta wing strategic bomber, which was operated by the Royal Air Force (RAF) from 1956 until 1984. Aircraft manufacturer A.V. Roe and Company (Avro) designed the Vulcan in response to Specification B.35/46. Of the three V bombers produced, the Vulcan was considered the most technically advanced and hence the riskiest option. Several scale aircraft, designated Avro 707, were produced to test and refine the delta wing design principles.
The Vulcan B.1 was first delivered to the RAF in 1956; deliveries of the improved Vulcan B.2 started in 1960. The B.2 featured more powerful engines, a larger wing, an improved electrical system and electronic countermeasures (ECM); many were modified to accept the Blue Steel missile. As a part of the V-force, the Vulcan was the backbone of the United Kingdom’s airborne nuclear deterrent during much of the Cold War. Although the Vulcan was typically armed with nuclear weapons, it was capable of conventional bombing missions, a capability which was used in Operation Black Buck during the Falklands War between the United Kingdom and Argentina in 1982.
The Vulcan had no defensive weaponry, initially relying upon high-speed high-altitude flight to evade interception. Electronic countermeasures were employed by the B.1 (designated B.1A) and B.2 from circa 1960. A change to low-level tactics was made in the mid-1960s. In the mid-1970s nine Vulcans were adapted for maritime radar reconnaissance operations, redesignated as B.2 (MRR). In the final years of service six Vulcans were converted to the K.2 tanker configuration for aerial refuelling.
Since retirement by the RAF one example, B.2 "XH558", named "The Spirit of Great Britain" has been restored for use in display flights and air shows, whilst two other B.2s, XL426 and "XM655", are kept in taxiable condition for ground runs and demonstrations at London Southend Airport and Wellesbourne Mountford Airfield respectively.
Development.
Origins.
The origin of the Vulcan and the other V bombers is linked with early British atomic weapon programme and nuclear deterrent policies. Britain's atom bomb programme began with Air Staff Operational Requirement OR.1001 issued in August 1946. This anticipated a government decision in January 1947 to authorise research and development work on atomic weapons, the U.S. Atomic Energy Act of 1946 (McMahon Act) having prohibited exporting atomic knowledge, even to countries that had collaborated on the Manhattan Project. OR.1001 envisaged a weapon not to exceed 24 ft in length, 5 ft in diameter and 10000 lb in weight. The weapon had to be suitable for release from 20000 ft to 50000 ft.
In January 1947, the Ministry of Supply distributed Specification B.35/46 to UK aviation companies to satisfy Air Staff Operational Requirement OR.229 for "a medium range bomber landplane capable of carrying one 10000 lb bomb to a target 1500 nmi from a base which may be anywhere in the world." A cruising speed of 500 kn at heights between 35000 ft and 50000 ft was specified. The maximum weight when fully loaded ought not to exceed 100000 lb. In addition to a "Special" (i.e., atomic) bomb, the aircraft was to be capable of alternatively carrying a conventional bomb load of 20000 lb. The similar OR.230 required a "long range bomber" with a 2000 nmi radius of action with a maximum weight of 200000 lb when fully loaded; this requirement was considered too difficult. A total of six companies would submit technical brochures to this specification, including Avro.
Required to tender by the end of April 1947, work began on receipt of Specification B.35/46 at Avro, led by technical director Roy Chadwick and chief designer Stuart Davies; the type designation was "Avro 698". It was obvious to the design team that conventional aircraft could not satisfy the Specification; knowing little about high-speed flight and unable to glean much from the Royal Aircraft Establishment or the US, they investigated German Second World War swept wing research. The team estimated that an otherwise conventional aircraft, with a swept wing of 45°, would have doubled the weight requirement. Realising that swept wings increase longitudinal stability, the team deleted the tail (empennage) and the supporting fuselage, it thus became a swept-back flying wing with only a rudimentary forward fuselage and a fin (vertical stabilizer) at each wingtip. The estimated weight was now only 50% over the requirement; a delta shape resulted from reducing the wingspan and maintaining the wing area by filling in the space between the wingtips, which enabled the specification to be met. Though Dr Alexander Lippisch is generally credited as the pioneer of the delta wing, Chadwick’s team had followed its own logical design process. The initial design submission had four large turbojets stacked in pairs buried in the wing either side of the centreline. Outboard of the engines were two bomb-bays.
In August 1947, Roy Chadwick was killed in the crash of the Avro Tudor 2 prototype and was succeeded by Sir William Farren. Reductions in wing thickness made it impossible to incorporate the split bomb bays and stacked engines, thus the engines were placed side-by-side in pairs either side of a single bomb-bay, the fuselage growing somewhat. The wingtip fins gave way to a single fin on the aircraft's centreline. Rival manufacturer Handley Page received a prototype contract for its crescent-winged HP.80 B.35/46 tender in November 1947. Though considered the best option, contract placement for Avro's design was delayed whilst its technical strength was established. Instructions to proceed with the construction of two Avro 698 prototypes was received in January 1948. As an insurance measure against both radical designs failing, Short Brothers received a contract for the prototype SA.4 to the less-stringent Specification B.14/46; the SA.4, later named Sperrin, was not required. In April 1948, Vickers also received authority to proceed with their Type 660 which, although falling short of the B.35/46 Specification, being of a more conventional design would be available sooner; this plane entered service as the Valiant.
Avro 707 and Avro 710.
As Avro had no flight experience of the delta wing, the company planned two smaller experimental aircraft based on the 698, the one-third scale model 707 for low-speed handling and the one-half scale model 710 for high-speed handling. Two of each were ordered. However, the 710 was cancelled when it was considered too time-consuming to develop; a high-speed variant of the 707 was designed in its place, the 707A. The first 707, "VX784", flew in September 1949 but crashed later that month killing the pilot, Avro test pilot Flt Lt Eric Esler. The second low-speed 707, "VX790", built with the still uncompleted 707A’s nose section (containing an ejection seat) and redesignated 707B, flew in September 1950 piloted by Avro test pilot Wg Cdr Roland "Roly" Falk. The high speed 707A, "WD480", followed in July 1951.
Due to the delay of the 707 programme, the contribution of the 707B and 707A towards the basic design of the 698 was not considered significant, though it did highlight a need to increase the length of the nosewheel to give a ground incidence of 3.5 degrees, the optimum take-off attitude. The 707B and 707A proved the design's validity and gave confidence in the delta planform. A second 707A, "WZ736" and a two-seat 707C, "WZ744" were also constructed but they played no part in the 698's development.
Vulcan B.1 and B.2.
Prototypes and type certification.
More influential than the 707 in the 698's design was wind-tunnel testing performed by the Royal Aircraft Establishment at Farnborough, which indicated the need for a wing redesign to avoid the onset of compressibility drag which would have restricted the maximum speed. Painted gloss white, the 698 prototype "VX770" flew for the first time on 30 August 1952 piloted by Roly Falk flying solo. The prototype 698, then fitted with only the first-pilot's ejection seat and a conventional control wheel, was powered by four Rolls-Royce RA.3 Avon engines of 6500 lbf thrust; there were no wing fuel tanks, temporary tankage was carried in the bomb bay. "VX770" made an appearance at the 1952 Society of British Aircraft Constructors' (SBAC) Farnborough Air Show the next month when Falk demonstrated an "almost vertical bank". After its Farnborough appearance, the future name of the Avro 698 was a subject of speculation; Avro had strongly recommended the name "Ottawa", in honour of the company's connection with Avro Canada. Weekly magazine "Flight" suggested "Albion" after rejecting "Avenger", "Apollo" and "Assegai". The Chief of the Air Staff preferred a V-class of bombers, the Air Council announced the following month that the 698 would be called "Vulcan" after the Roman god of fire and destruction. In January 1953, "VX770" was grounded for the installation of wing fuel tanks, Armstrong Siddeley ASSa.6 Sapphire engines of 7500 lbf thrust and other systems; it flew again in July 1953.
The second prototype, "VX777", flew in September 1953. More representative of production aircraft, it was lengthened to accommodate a longer nose undercarriage leg, featured a visual bomb-aiming blister under the cabin and was fitted with Bristol Olympus 100 engines of 9750 lbf thrust. At Falk’s suggestion, a fighter-style control stick replaced the control wheel. Both prototypes had almost pure delta wings with straight leading edges. During trials in July 1954, "VX777" was substantially damaged in a heavy landing at Farnborough. It was repaired and fitted with Olympus 101 engines of 11000 lbf thrust before resuming trials in October 1955. While exploring the high speed and high altitude flight envelope, mild buffeting and other undesirable flight characteristics were experienced while approaching the speed of sound, including an alarming tendency to enter an uncontrollable dive, unacceptable to the Aeroplane and Armament Experimental Establishment (A&AEE) at Boscombe Down. The solution included the "Phase 2" wing, featuring a kinked and drooped leading edge and vortex generators on the upper surface, first tested on 707A "WD480". An auto-mach trimmer introduced a nose-up attitude when at high speeds, the control column had to be pushed rather than pulled to maintain level flight.
Meanwhile, the first production B.1, "XA889", had flown in February 1955 with the original wing. In September 1955, Falk, flying the second production B.1 "XA890" amazed crowds at the Farnborough Air Show by executing a barrel roll on his second flypast in front of the SBAC president’s tent. After two days flying, he was called in front of service and civil aviation authorities and ordered to refrain from carrying out this "dangerous" manoeuvre. Now fitted with a Phase 2 wing, "XA889" was delivered in March 1956 to the A&AEE for trials for the type’s initial Certificate of Airworthiness which it received the following month.
Further developments.
The first 15 B.1s were powered by the Olympus 101 of 11000 lbf thrust. Many of these early examples in a metallic finish remained the property of the Ministry of Supply being retained for trials and development purposes. Those entering RAF service were delivered to No 230 Operational Conversion Unit (OCU), the first in July 1956. Later aircraft, painted in anti-flash white and powered by the Olympus 102 of 12000 lbf thrust, began to enter squadron service in July 1957. The Olympus 102s were quickly modified to Olympus 104 standard, ultimately rated at 13500 lbf thrust. As far back as 1952, Bristol Aero Engines had begun development of the BOl.6 (Olympus 6) rated at 16000 lbf thrust but if fitted to the B.1, this would have re-introduced the buffet requiring further redesign of the wing.
The decision to proceed with the B.2 versions of the Vulcan was made in May 1956. It was anticipated that the first B.2 would be around the 45th aircraft of the 99 then on order. As well as being able to achieve greater heights over targets, it was believed that operational flexibility could be extended by the provision of in-flight refuelling equipment and tanker aircraft. The increasing sophistication of Soviet air defences required the fitting of electronic countermeasure (ECM) equipment and vulnerability could be reduced by the introduction of the Avro "Blue Steel" stand-off missile, then in development. In order to develop these proposals, the second Vulcan prototype "VX777" was rebuilt with the larger and thinner Phase 2C wing, improved flying control surfaces and Olympus 102 engines, first flying in this configuration in August 1957. Plans were in hand to equip all Vulcans from the 16th aircraft onwards with in-flight refuelling receiving equipment. A B.1, "XA903", was allocated for Blue Steel development work. Other B.1s were used for the development of the BOl.6 (later Olympus 200), "XA891"; a new AC electrical system, "XA893"; and ECM including jammers within a bulged tail-cone and a tail warning radar, "XA895".
The 46th production aircraft and first B.2, "XH533", first flew in September 1958 fitted with Olympus 200 engines of 16000 lbf thrust, six months before the last B.1 "XH532" was delivered in March 1959. Rebuilding B.1s as B.2s was considered but rejected over cost. Nevertheless, to extend the B.1's service life, 28 were upgraded by Armstrong Whitworth between 1959 and 1963 to the B.1A standard, including features of the B.2 such as ECM equipment, in-flight refuelling receiving equipment, and UHF radio. The second B.2, "XH534", flew in January 1959. Powered by production Olympus 201 of 17000 lbf thrust, it was more representative of a production aircraft, being fitted with an in-flight refuelling probe and a bulged ECM tail cone. Some subsequent B.2s were initially lacking probes and ECM tail cones, but these were fitted retrospectively. The first 10 B.2s outwardly showed their B.1 ancestry, retaining narrow engine air intakes. Anticipating even more powerful engines, the air intakes were deepened on the 11th ("XH557") and subsequent aircraft. Many of the early aircraft were retained for trials and it was the 12th B.2, "XH558", that was the first to be delivered to the RAF in July 1960.
The 26th B.2, "XL317", the first of a production batch ordered in February 1956, was the first Vulcan, apart from development aircraft, capable of carrying the Blue Steel missile; 33 aircraft were delivered to the RAF with these modifications. When the Mk.2 version of Blue Steel was cancelled in favour of the Douglas GAM-87 Skybolt air-launched ballistic missile in December 1959, fittings were changed in anticipation of the new missile, one under each wing. Though Skybolt was cancelled in November 1962, many aircraft were delivered or retrofitted with "Skybolt" blisters. Later aircraft ("XL391" and "XM574" onwards) were delivered with Olympus 301 engines of 20000 lbf thrust. Two earlier aircraft were re-engined ("XH557" and "XJ784") for trials and development work; another seven aircraft ("XL384"-"XL390") were converted circa 1963.
The last B.2 was delivered in 1965 and the type served till 1984. Whilst in service the B.2 was continuously updated with modifications including rapid engine starting, bomb-bay fuel tanks, wing strengthening to give the fatigue life to enable the aircraft to fly at low level (a tactic introduced in the mid-60s), upgraded navigation equipment, Terrain Following Radar (TFR), standardisation on a common nuclear weapon (WE.117) and improved ECM equipment. The B.1As were not strengthened, thus all were withdrawn by 1968. Nine B.2s were modified for the Maritime Radar Reconnaissance (MRR) role and six for the airborne tanker role.
Proposed developments and cancelled projects.
The Avro 718 was a 1951 proposal for a delta-winged military transport based on the Type 698 to carry 80 troops or 110 passengers. It would have been powered by four Bristol Olympus BOl.3 engines.
The Avro Type 722 Atlantic was a 1952 proposal (announced in June 1953) for a 120-passenger delta-winged airliner based on the Type 698.
The Avro 732 was a 1956 proposal for a supersonic development of the Vulcan and would have been powered by 8 de Havilland Gyron Junior engines. Unlike the proposed Avro 721 low-level bomber of 1952 or the Avro 730 supersonic stainless steel canard bomber dating from 1954 (cancelled in 1957 before completion of the prototype), the Type 732 showed its Vulcan heritage.
In 1960, the Air Staff approached Avro with a request into a study for a Patrol Missile Carrier armed with up to six Skybolt missiles capable of a mission length of 12 hours. Avro's submission in May 1960 was the Phase 6 Vulcan, which if built would have been the Vulcan B.3. The aircraft was fitted with an enlarged wing of 121 ft span with increased fuel capacity; additional fuel tanks in a dorsal spine; a new main undercarriage to carry an all-up-weight of 339000 lb; and reheated Olympus 301s of 30000 lbf thrust. An amended proposal of October 1960 inserted a 10 ft plug into the forward fuselage with capacity for six crew members including a relief pilot, all facing forwards on ejection seats, and aft-fan versions of the Olympus 301.
Export proposals.
Other countries expressed interest in purchasing Vulcans but, as with the other V-bombers, no foreign sales materialised.
As early as 1954, Australia recognised that the English Electric Canberra was becoming outdated and evaluated aircraft such as the Avro Vulcan and Handley-Page Victor as potential replacements. Political pressure for a Canberra replacement only rose to a head in 1962; at which point more modern types such as the BAC TSR-2, General Dynamics F-111C, and North American A-5 Vigilante had become available. The RAF would have transferred several V-bombers, including Vulcans, for interim use by the RAAF if they had purchased the TSR-2, but the RAAF selected the F-111C.
In the early 1980s, Argentina approached the UK with a proposal to buy a number of Vulcans. An application, made in September 1981, requested the 'early availability' of a 'suitable aircraft'. With some reluctance, ministers approved the export of a single aircraft but emphasised that clearance had not been given for the sale of a larger number. A letter from the British Foreign and Commonwealth Office to the Ministry of Defence in January 1982 stated that little prospect was seen of this happening without ascertaining the Argentine interest and whether such interest was genuine: 'On the face of it, a strike aircraft would be entirely suitable for an attack on the Falklands.' Argentina invaded the Falkland Islands less than three months later.
Design.
Overview.
In spite of its radical and unusual shape, the airframe was built along traditional lines. Except for the most highly stressed parts, the whole structure was manufactured from standard grades of light alloy. The airframe was broken down into a number of major assemblies: the centre section, a rectangular box containing the bomb-bay and engine bays bounded by the front and rear spars and the wing transport joints; the intakes and centre fuselage; the front fuselage, incorporating the pressure cabin; the nose; the outer wings; the leading edges; the wing trailing edge and tail end of the fuselage; the wings were not sealed and used directly as fuel tankage, but carried bladders for fuel in the void spaces of the wings; and there was a single swept tail fin with a single rudder on the trailing edge.
A five-man crew, the first pilot, co-pilot, navigator radar, navigator plotter and air electronics officer (AEO) was accommodated within the pressure cabin on two levels; the pilots sitting on Martin-Baker 3K (3KS on the B.2) ejection seats whilst on the lower level, the other crew sat facing rearwards and would abandon the aircraft via the entrance door. The original B35/46 specification sought a jettisonable crew compartment, this requirement was removed in a subsequent amendment, the rear crew's escape system was often an issue of controversy, such as when a practical refit scheme was rejected. A rudimentary sixth seat forward of the navigator radar was provided for an additional crew member; the B.2 had an additional seventh seat opposite the sixth seat and forward of the AEO. These seats were no more than cushions, a full harness and an oxygen and intercom facility. The visual bomb-aimer’s compartment could be fitted with a T4 (Blue Devil) bombsight, in many B.2s this space housed a vertically mounted Vinten F95 Mk.10 camera for assessing simulated low-level bombing runs.
Fuel was carried in 14 bag tanks, four in the centre fuselage above and to the rear of the nosewheel bay and five in each outer wing. The tanks were split into four groups of almost equal capacity, each normally feeding its respective engine though cross-feeding was possible. The centre of gravity was automatically maintained by electric timers which sequenced the booster pumps on the tanks. B.2 aircraft could be fitted with one or two additional fuel tanks in the bomb-bay.
Despite being designed before a low radar cross-section (RCS) and other stealth factors were ever a consideration, a Royal Aircraft Establishment technical note of 1957 stated that of all the aircraft so far studied, the Vulcan due to its shape appeared by far the simplest radar echoing object: only one or two components contributing significantly to the echo at any aspect, compared with three or more on most other types.
Colour schemes.
The two prototype Vulcans were finished in gloss white. Early Vulcan B.1s left the factory in a natural metal finish; the front half of the nose radome was painted black, the rear half painted silver. Front-line Vulcan B.1s had a finish of anti-flash white and RAF "type D" roundels. Front-line Vulcan B.1As and B.2s were similar but with 'type D pale' roundels.
With the adoption of low-level attack profiles in the mid-1960s, B.1As and B.2s were given a glossy Sea Grey Medium and Dark Green disruptive pattern camouflage on the upper surfaces, white undersurfaces and "type D" roundels. (The last 13 Vulcan B.2s, XM645 onwards, were delivered thus from the factory). In the mid-1970s: Vulcan B.2s received a similar scheme with matte camouflage, Light Aircraft Grey undersides, and "low-visibility" roundels; B.2(MRR)s received a similar scheme in gloss; and the front half of the radomes were no longer painted black. Beginning in 1979, 10 Vulcans received a wrap-around camouflage of Dark Sea Grey and Dark Green because, during Red Flag exercises in the USA, defending SAM forces had found that the grey-painted undersides of the Vulcan became much more visible against the ground at high angles of bank.
Avionics.
The original Vulcan B.1 radio fit was: two 10-channel VHF transmitter/receivers ("TR-1985/TR-1986") and a 24-channel HF transmitter/receiver ("STR-18"). The Vulcan B.1A also featured an UHF transmitter/receiver ("ARC-52"). The initial B.2 radio fit was similar to the B.1A though it was ultimately fitted with the "ARC-52", a V/UHF transmitter/receiver ("PTR-175"), and a SSB HF transmitter/receiver (Collins "618T").
The Navigation and Bombing System (NBS) comprised an "H2S" Mk9 radar and a Navigation Bombing Computer (NBC) Mk1. Other B.1 navigation aids included a Marconi radio compass (ADF), "GEE" Mk3, "Green Satin" Doppler radar to determine the groundspeed and drift angle, radio and radar altimeters, and ILS. "TACAN" replaced "GEE" in the B.1A and B.2 and Decca "Doppler 72" replaced "Green Satin" in the B.2.<ref name='PB&E102/3'>Price, Blackman and Edmonson 2010, pp. 102, 103.</ref> A continuous display of the aircraft's position was maintained by a Ground Position Indicator (GPI).
Vulcan B.2s were eventually fitted with the twin-gyro free-running gyroscopic Heading Reference System (HRS) Mk.2, based upon the inertial platform of the "Blue Steel" missile, which had been integrated into the system when the missile had been carried. With the HRS a Navigator's Heading Unit (NHU)was provided which enabled the Navigator Plotter to adjust the aircraft heading, through the autopilot, by as little as 0.1 degrees. The B.2 (MRR) was additionally fitted with the "LORAN C" navigation system.
The original ECM fit as fitted to the B.1A and B.2 was: one "Green Palm" voice communications' jammer; two "Blue Diver" metric jammers; three "Red Shrimp" S-band jammers; ; a "Blue Saga" Passive Warning Receiver with 4 aerials (PWR); one "Red Steer" tail warning radar; and window (chaff) dispensers. The bulk of the equipment was carried in a large extended tail cone, and a flat ECM aerial counterpoise plate mounted between the starboard tailpipes. Later equipment on the B.2 included: an L band jammer (replacing a "Red Shrimp"); the "ARI 18146" X-band jammer; replacing the Green Palm; the improved "Red Steer" Mk.2; infra-red decoys (flares); and the "ARI 18228" PWR with its aerials that gave a squared top to the fin.
Controls.
The aircraft was controlled by a fighter-type control stick and rudder bar which operated the powered flying controls (PFCs). Each PFC had a single electro-hydraulic powered flying control unit (PFCU) except the rudder which had two, one running as a back-up. Artificial feel and autostabilisation in the form of pitch and yaw dampers were provided, as well as an auto mach trimmer.
The flight instruments in the B.1 were traditional and included "G4B" compasses; Mk.4 artificial horizons; and zero reader flight display instruments. The B.1 had a Smiths Mk10 autopilot. In the B.2, these features were incorporated into the Smiths Military Flight System (MFS), the pilots' components being: two beam compasses; two director-horizons; and a Mk.10A or Mk.10B autopilot. From 1966, B.2s were fitted with the "ARI 5959" Terrain-following radar (TFR), built by General Dynamics, its commands being fed into the director-horizons.
The B.1 had four elevators (inboard) and four ailerons (outboard). In the B.2, these were replaced by eight elevons. The Vulcan was also fitted with six electrically-operated three-position (in, medium drag, high drag) airbrakes, four in the upper centre section and two in the lower. There were originally four lower airbrakes but the outboard two were deleted before the aircraft entered service. A brake parachute was installed inside the tail cone.
Electrical and hydraulic systems.
The main electrical system on the B.1/B.1A was 112V DC supplied by four 22.5kW engine-driven generators. Backup power was provided by four 24V 40Ah batteries connected in series providing 96V. Secondary electrical systems were 28V DC, single-phase 115V AC at 1600Hz, and three-phase 115V AC at 400 Hz, driven by transformers and inverters from the main system. The 28V DC system was backed up by a single 24V battery.
For greater efficiency and higher reliability, the main system on the B.2 was changed to three-phase 200V AC at 400 Hz supplied by four 40kVA engine-driven constant speed alternators. Standby supplies in the event of a main AC failure were provided by a Ram Air Turbine (RAT) driving a 17kVA alternator that could operate at high altitude down to 20000 ft, and an Airborne Auxiliary Power Plant (AAPP), a Rover gas turbine driving a 40kVA alternator, which could be started once the aircraft was below an altitude of 30000 ft. Secondary electrical supplies were similar to the B.1.
The change to an AC system was a significant improvement. The Vulcan's powered flying controls were hydraulically actuated but each Powered Flying Control Unit (PFCU) had a hydraulic pump which was driven by an electrical motor. Because there was no manual reversion, a total electrical failure would result in a loss of control. The standby batteries on the B.1 were designed to give enough power for 20 minutes of flying time but this proved to be optimistic and two aircraft, "XA891" and "XA908", crashed as a result.
The main hydraulic system provided pressure for: undercarriage raising and lowering and bogie trim; nosewheel centring and steering; wheelbrakes (fitted with Maxarets); bomb doors opening and closing; and (B.2 only) AAPP air scoop lowering. Hydraulic pressure was provided by three hydraulic pumps fitted to Nos. 1, 2 and 3 engines. An electrically operated hydraulic power pack (EHPP) could be used to operate the bomb doors and recharge the brake accumulators. A compressed air (later nitrogen) system was provided for emergency undercarriage lowering.
Engine.
The Rolls-Royce Olympus, originally known as the "Bristol BE.10 Olympus", is a two-spool axial-flow turbojet that powered the Vulcan. Each Vulcan had four engines buried in the wings, positioned in pairs close to the centre of the fuselage. The engine's design began in 1947, intended to power the Bristol Aeroplane Company's own rival design to the Vulcan. A serendipitous arrangement in air intakes can cause the Vulcan to emit a distinctive "howl" when the engines are at approximately 90% power, which can be heard as the aircraft performs a flypast, such as at public airshows.
As the prototype Vulcan VX770 was ready for flight prior to the Olympus being available, it first flew using Rolls-Royce Avon RA.3 engines of 6500 lbf thrust. These were quickly replaced by Armstrong Siddeley Sapphire ASSa.6 engines of 7500 lbf thrust. VX770 later became a flying test bed for the Rolls-Royce Conway. The second prototype VX777 first flew with Olympus 100s of 10000 lbf thrust. It was subsequently re-engined with Olympus 101 engines of 11000 lbf thrust. When VX777 flew with a Phase 2C (B.2) wing in 1957, it was fitted with Olympus 102 engines of 12000 lbf thrust.
Early B.1s were engined with the Olympus 101. Later aircraft were delivered with Olympus 102s. All Olympus 102s became the Olympus 104 of 13000 lbf thrust on overhaul and ultimately 13500 lbf thrust on uprating. The first B.2 flew with the second-generation Olympus 200 of 16000 lbf thrust, design of which began in 1952. Subsequent B.2s were engined with either the uprated Olympus 201 of 17000 lbf thrust or the Olympus 301 of 20000 lbf thrust. The Olympus 201 was designated 202 on being fitted with a rapid air starter. The engine would later be developed into a reheated (afterburning) powerplant for the cancelled supersonic BAC TSR-2 strike bomber and the supersonic passenger transport Concorde.
Operational history.
Introduction.
In September 1956, the RAF received its first Vulcan B.1, "XA897", which immediately embarked upon a round-the-world tour. The tour was to be an important demonstration of the range and capabilities of the aircraft, it also had other benefits in the form of conducting goodwill visits in various countries; in later life Vulcans routinely visited various nations and distant parts of the former British Empire as a show of support and military protection. This first tour, however, was struck by misfortune; on 1 October 1956, while landing in bad weather at London Heathrow Airport at the completion of the world tour, "XA897" was destroyed in a fatal accident.
The first two aircraft were delivered to 230 OCU in January 1957 and the training of crews started on 21 February 1957, in the following months more aircraft were delivered to the OCU. The first OCU course to qualify was No. 1 Course, on 21 May 1957, and they went on to form the first flight of No.83 Squadron. No. 83 Squadron was the first operational squadron to use the bomber, at first using borrowed Vulcans from the OCU and on 11 July 1956 it received the first aircraft of its own. By September 1957, several Vulcans had been handed over to No.83 Squadron The second OCU course also formed a Flight of 83 Squadron, but subsequent trained crews were also used to form the second bomber squadron, 101 Squadron. The last aircraft from the first batch of 25 aircraft had been delivered by the end of 1957 to 101 Squadron.
In order to increase the mission range and flight time for Vulcan operations, in-flight refuelling capabilities were added in 1959 onwards; several Valiant bombers were refurbished as tankers to refuel the Vulcans. Continuous airborne patrols proved untenable, however, and the refuelling mechanisms across the Vulcan fleet fell into disuse in the 1960s. Both Vulcans and the other V-force aircraft routinely visited the Far East, in particular Singapore, where a fully equipped nuclear weapons storage facility had been constructed in 1959. During the Indonesia–Malaysia confrontation Britain planned to deploy three squadrons of V-bomber aircraft and 48 Red Beard tactical nuclear weapons to the region, although this was ultimately decided against, Vulcans trained in the region for both conventional and nuclear missions. Britain regularly deployed Vulcans to the Far East as a part of their contribution to SEATO operations, often to test the defenses of friendly nations in joint exercises. In the early 1970s, the RAF decided to permanently deploy two squadrons of Vulcans overseas in the Near East Air Force Bomber Wing, based at RAF Akrotiri in Cyprus; the Vulcans were withdrawn as Cypriot intercommunal violence intensified in the mid-1970s.
Vulcans did some very long range missions. In June 1961, one of them took off from RAF Scampton to Sydney, with an 18,507 km long journey, flown in only a bit more than 20 hours and three air refuellings. Vulcans frequently visited the United States during the 1960s and 1970s to participate in air shows and static displays, as well as to participate in the Strategic Air Command's Annual Bombing and Navigation Competition at such locations as Barksdale AFB, Louisiana and the former McCoy AFB, Florida, with the RAF crews representing Bomber Command and later Strike Command. Vulcans also took part in the 1960, 1961, and 1962 Operation Skyshield exercises, in which NORAD defences were tested against possible Soviet air attack, the Vulcans simulating Soviet fighter/bomber attacks against New York, Chicago and Washington. The results of the tests were classified until 1997. The Vulcan proved quite successful during the 1974 'Giant Voice' exercise, in which it had managed to avoid USAF interceptors 
Nuclear deterrent.
As part of Britain's independent nuclear deterrent, the Vulcan initially carried Britain's first nuclear weapon, the "Blue Danube" gravity bomb. "Blue Danube" was a low-kiloton yield fission bomb designed before the United States detonated the first hydrogen bomb. These were supplemented by U.S.-owned "Mk 5" bombs (made available under the Project E programme) and later by the British "Red Beard" tactical nuclear weapon. The UK had previously embarked on its own hydrogen bomb programme, and to bridge the gap until these were ready the V-bombers were equipped with an Interim Megaton Weapon based on the "Blue Danube" casing containing "Green Grass", a large pure-fission warhead of 400 ktonTNT yield. This bomb was known as "Violet Club". Only five were deployed before the "Green Grass" warhead was incorporated into a developed weapon as "Yellow Sun Mk.1."
The later "Yellow Sun Mk 2", was fitted with "Red Snow", a British-built variant of the U.S. W28 warhead. "Yellow Sun Mk 2" was the first British thermonuclear weapon to be deployed, and was carried on both the Vulcan and Handley Page Victor. The Valiant retained U.S. nuclear weapons assigned to SACEUR under the dual-key arrangements. "Red Beard" was pre-positioned in Singapore for use by Vulcan and Victor bombers. From 1962, three squadrons of Vulcan B.2s and two squadrons of Victor B.2s were armed with the "Blue Steel" missile, a rocket-powered stand-off bomb, which was also armed with the 1.1 MtonTNT yield "Red Snow" warhead.
Operationally, RAF Bomber Command and the U.S. Strategic Air Command cooperated in the Single Integrated Operational Plan (SIOP) to ensure coverage of all major Soviet targets from 1958, 108 aircraft of the RAF's V-Bombers were assigned targets under SIOP by the end of 1959. From 1962 onwards, two jets in every major RAF base were armed with nuclear weapons and on standby permanently under the principle of Quick Reaction Alert (QRA). Vulcans on QRA standby were to be airborne within four minutes of receiving an alert, as this was identified as the amount of time between warning of a USSR nuclear strike being launched and it arriving in Britain. The closest the Vulcan came to take part in potential nuclear conflict was during the Cuban missile crisis in October 1962, where Bomber Command was moved to Alert Condition 3, an increased state of preparedness from normal operations, however stood down in early November.
The Vulcans were intended to be equipped with the American "Skybolt" Air Launched Ballistic Missile to replace the "Blue Steel", with Vulcan B.2s carrying two "Skybolts" under the wings; the last 28 B.2s were modified on the production line to fit pylons to carry the "Skybolt". A B.3 variant with increased wingspan to carry up to six "Skybolts" was proposed in 1960. When the "Skybolt" missile system was cancelled by U.S. President John F. Kennedy on the recommendation of his Secretary of Defense, Robert McNamara in 1962, "Blue Steel" was retained. To supplement it until the Royal Navy took on the deterrent role with Polaris ICBM-equipped submarines, the Vulcan bombers adopted a new mission profile of flying high during clear transit, dropping down low to avoid enemy defences on approach, and deploying a parachute-retarded bomb, the "WE.177B". However, since the aircraft had been designed for high-altitude flight, at low altitudes it could not exceed 350 knots. RAF Air Vice Marshal Ron Dick, a former Vulcan pilot, said "it is [thus] questionable whether it could have been effective flying at low level in a war against ... the Soviet Union.”
After the British Polaris submarines became operational and Blue Steel was taken out of service in 1970, the Vulcan continued to carry "WE.177B" in a tactical nuclear strike role as part of the British contribution to Europe's standing NATO forces, although they no longer held aircraft at 15 minutes readiness in peacetime. Two squadrons were also stationed in Cyprus as part of the Near East Air Force and assigned to Central Treaty Organization in a strategic strike role. With the eventual demise of the "WE.177B" and the Vulcan bombers, the Blackburn Buccaneer, SEPECAT Jaguar, and Panavia Tornado, continued with the "WE.177C" until its retirement in 1998. While not a like-for-like replacement, the multi-role Tornado interdictor/strike bomber is the successor for the roles previously filled by the Vulcan.
Conventional role.
Although in operational usage, the Vulcan typically carried various nuclear armaments, the type also had a conventional secondary role. While performing conventional combat missions, the Vulcan can carry up to 21 1000 lb bombs inside its bomb bay. Since the 1960s, the various Vulcan squadrons were routinely conducting conventional training missions; the aircrews were expected to be able to perform conventional bombing missions in addition to the critical nuclear strike mission the Vulcan normally performed.
The only combat missions in which the Vulcan would conduct actually took place towards the end of the type's service in 1982. During the Falklands War, the Vulcan was deployed against Argentinian forces which had occupied the Falkland Islands. This conflict was the only occasion in which any of the V-bombers would participate in conventional warfare. The missions performed by the Vulcan became known as the "Black Buck" raids, each aircraft had to fly 3889 mi from Ascension Island to reach Stanley on the Falklands. Victor tankers conducted the necessary air-to-air refuelling for the Vulcan to cover the distance involved; approximately 1.1 million gal (5 million L) of fuel were used in each mission.
A total of five Vulcans had been selected to participate in the operation. In order to do so, each aircraft had to receive various last-minute adaptions; including modifications to the bomb bay, the reinstatement of the long out-of-use in-flight refuelling system, the installation of a new navigational system derived from the Vickers VC-10, the updating of several onboard electronics. Underneath the wings, new pylons were fitted to carry an ECM pod and Shrike anti-radar missiles at wing hardpoint locations; these hardpoints had originally been installed for the purpose of carrying the cancelled Skybolt nuclear missile. Engineering work to retrofit these Vulcans had begun on 9 April.
On 1 May, the first mission was conducted by a single Vulcan that flew over Port Stanley and dropped its bombs on the airfield concentrating on the single runway, with one direct hit, making it unsuitable for fighter aircraft. The Vulcan's mission was quickly followed up by strikes against anti-air installations, flown by British Aerospace Sea Harriers from nearby Royal Navy carriers. A total of three Vulcan missions were flown against the airfield, a further two missions in which missiles were launched against radar installations; an additional two missions were cancelled. At the time, these missions held the record for the world's longest-distance raids. The ECM systems on board the Vulcans proved to be effective at jamming Argentine radars; while a Vulcan was within the theatre, other British aircraft in the vicinity had a greatly reduced chance of coming under effective fire.
On 3 June 1982, Vulcan B.2 "XM597" of No. 50 Squadron took part in the "Black Buck 6" mission against Argentinian radar sites at Stanley airfield on the Falkland Islands. While attempting to refuel for its return journey to Ascension Island, the probe broke, leaving the Vulcan with insufficient fuel, forcing a diversion to Galeão Air Force Base, Rio de Janeiro in neutral Brazil. En route, secret papers were dumped along with the two remaining AGM-45 Shrike missiles, although one failed to launch. After a mayday call, the Vulcan, escorted by Brazilian Air Force Northrop F-5 fighters, was permitted an emergency landing at Rio with very little fuel left on board. The Vulcan and her crew were detained until the end of hostilities nine days later.
Maritime radar reconnaissance.
In November 1973, No. 27 Squadron reformed at RAF Scampton in the maritime radar reconnaissance role. Though initially equipped with a number of B.2 aircraft, the Squadron eventually operated nine B.2 (MRR) aircraft. The main external visual difference was the presence of a gloss paint finish, with a light grey undersurface, to protect against sea spray. TFR was not fitted and the aircraft were equipped with "LORAN" C navigational equipment. Five aircraft were further modified for the Squadron's secondary role of air sampling. These aircraft were distinguishable by the additional hardpoints outside of the underwing Skybolt points upon which could be hung pylons and the air-sampling pods, which had been constructed from de Havilland Sea Vixen drop tanks.
Aerial refuelling role.
After the end of the Falklands War in 1982, the Vulcan B.2 was due to be withdrawn from RAF service that year. However, the Falklands campaign had consumed much of the airframe fatigue life of the RAF's Victor tankers. While Vickers VC10 tanker conversions had been ordered in 1979 and Lockheed TriStar tankers would be ordered subsequent to the conflict, as a stopgap measure six Vulcans were converted into single point tankers. The Vulcan tanker conversion was accomplished by removing the jammers from the ECM bay in the tail of the aircraft, and replacing them with a single Hose Drum Unit (HDU). An additional cylindrical bomb-bay tank was fitted, making a total of three, giving a fuel capacity of almost 100000 lb.
The go-ahead for converting the six aircraft was given on 4 May 1982. Just 50 days after being ordered, the first Vulcan tanker, "XH561", was delivered to RAF Waddington. The Vulcan K.2s were operated by No. 50 Squadron, along with three Vulcan B.2s, in support of UK air defence activities until it was disbanded in March 1984.
Production.
A total of 134 production Vulcans were assembled at Woodford Aerodrome, 45 to the B.1 design and 89 were B.2 models, the last being delivered to the RAF in January 1965.
Operators.
V-Bomber dispersal airfields.
In the event of transition to war, the V Bomber squadrons were to deploy four aircraft at short notice to each of 26 pre-prepared dispersal airfields around the United Kingdom. In the early 1960s the RAF ordered 20 Beagle Basset communication aircraft to move the crews to dispersal airfields; the importance of these aircraft was only brief, diminishing when the primary nuclear deterrent switched to the Royal Navy's Polaris Missile.
Aircraft on display.
 Media related to at Wikimedia Commons
XH558.
The last airworthy Vulcan (XH558) has been restored to flying condition by the "Vulcan to the Sky Trust" after years of effort and fundraising. The first post-restoration flight, which lasted 34 minutes, took place on 18 October 2007.
The aircraft's airworthiness status was in peril as maintenance funding was in need before the end of February 2010. At the last moment an anonymous benefactor presented £458,000 to the foundation, ensuring its airworthiness for both its 50th birthday and the prospect of a flight performance for the 2012 Summer Olympic Games opening ceremony in London. It is currently based at Doncaster/Sheffield Robin Hood Airport, formerly RAF Finningley.
Specifications.
Vulcan B.1.
"Data from" Polmar, LamingGeneral characteristics* Crew: 5 (pilot, co-pilot, AEO, Navigator Radar, Navigator Plotter)
Performance
</ul>Armament
 *21 × 1,000 pounds (454 kg) of conventional bombs
References.
Bibliography.
</dl>
Further reading.
</dl>

</doc>
<doc id="44071" url="http://en.wikipedia.org/wiki?curid=44071" title="M61 Vulcan">
M61 Vulcan

The M61 Vulcan is a hydraulically or pneumatically driven, six-barrel, air-cooled, electrically fired Gatling-style rotary cannon which fires 20 mm rounds at an extremely high rate (typically 6,000 rounds per minute). The M61 and its derivatives have been the principal cannon armament of United States military fixed-wing aircraft for fifty years.
The M61 was originally produced by General Electric, and after several mergers and acquisitions is currently produced by General Dynamics.
Development.
At the end of World War II, the United States Army began to consider new directions for future military aircraft guns. The higher speeds of jet-powered fighter aircraft meant that achieving an effective number of hits would be extremely difficult without a much higher volume of fire. While captured German designs (principally the Mauser MG 213C) showed the potential of the single-barrel revolver cannon, the practical rate of fire of such a design was still limited by ammunition feed and barrel wear concerns. The Army wanted something better, combining extremely high rate of fire with exceptional reliability. In 1947, the Air Force became a separate branch of the military. The new Air Force made a request for a new aircraft gun. A lesson of World War II air combat was that German, Italian and Japanese fighters could attack American aircraft from long range with their cannon main armament. American fighters with .50 cal main armament, such as the P-51 and P-47, had to be close with the enemy in order to hit and damage enemy aircraft. The 20mm Hispano carried by the P-38 while formidable against propeller driven planes was deemed a relatively low velocity weapon in the age of jets, while other cannon were notoriously unreliable.
In response to this requirement, the Armament Division of General Electric resurrected an old idea: the multi-barrel Gatling gun. The original Gatling gun had fallen out of favor because of the need for an external power source to rotate the barrel assembly, but the new generation of turbojet-powered fighters offered sufficient electric power to operate the gun, and electric operation was more reliable than gas-operated reloading. With multiple barrels, the rate of fire per barrel could be lower than a single-barrel revolver cannon while providing greater overall rate of fire. The idea of powering a Gatling gun from an external electric power source was not a novel idea at the end of the World War II, as Richard Jordan Gatling himself had done just that in 1893, with a patent he filed.
In 1946 the Army issued General Electric a contract for "Project Vulcan", a six-barrel weapon capable of firing 7,200 rounds per minute (rpm). Although European designers were moving towards heavier 30 mm weapons for better hitting power, the U.S. initially concentrated on a powerful 0.60 in cartridge designed for a pre-war anti-tank rifle, expecting that the cartridge's high muzzle velocity would be beneficial for improving hit ratios on high speed targets.
The first GE prototypes of the 0.60 in caliber T45 were ground-fired in 1949; it achieved 2,500 rpm, which was increased to 4,000 rpm by 1950. By the early 1950s, the USAF decided that high velocity alone might not be sufficient to ensure target destruction and tested 20 mm and 27 mm alternatives based on the 0.60 in caliber cartridge. These variants of the T45 were known as the T171 and T150 respectively, and were first tested in 1952. Eventually, the 20×102 mm cartridge was determined to have the desired balance of projectile and explosive weight and muzzle velocity.
The development of the Lockheed F-104 Starfighter revealed that the T171 Vulcan (later redesignated "M61") suffered problems with its linked ammunition, being prone to misfeed and presenting a foreign object damage (FOD) hazard with discarded links. A linkless ammunition feed system was developed for the upgraded "M61A1", which subsequently became the standard cannon armament of U.S. fighters.
In 1993, General Electric sold its aerospace division, including GE Armament Systems along with the design and production tooling for the M61 and GE's other rotary cannon, to Martin Marietta. After Martin's merger with Lockheed, the rotary cannon became the responsibility of Lockheed Martin Armament Systems. Lockheed Martin Armament Systems was later acquired by General Dynamics, who currently produce the M61 and its variants.
Description.
Each of the cannon's six barrels fires once in turn during each revolution of the barrel cluster. The multiple barrels provide both a very high rate of fire—around 100 rounds per second—and contribute to prolonged weapon life by minimizing barrel erosion and heat generation. Mean time between jams or failures is in excess of 10,000 rounds, making it an extremely reliable weapon. The success of the Vulcan Project and its subsequent progeny, the very-high-speed Gatling gun, has led to guns of the same configuration being referred to as "Vulcan cannon", which can sometimes confuse nomenclature on the subject.
Most aircraft versions of the M61 are hydraulically driven and electrically primed. The gun rotor, barrel assembly and ammunition feed system are rotated by a hydraulic drive motor through a system of flexible drive shafts. The round is fired by an electric priming system where an electrical current from a firing lead passes through the firing pin to the primer as each round is rotated into the firing position.
The self-powered version, the GAU-4 (called M130 in Army service), is gas-operated, tapping gun gas from three of the six barrels to operate the gun gas driven mechanism. The self-powered Vulcan weighs about 10 lb more than its electric counterpart, but requires no external power source to operate, except for an electric, inertia starter to initiate gun rotation, allowing the first rounds to be chambered and fired.
The initial "M61" used linked, belted ammunition, but the ejection of spent links created considerable (and ultimately insuperable) problems. The original weapon was soon replaced by the "M61A1", with a linkless feed system. Depending on the application, the feed system can be either single-ended (ejecting spent cases and unfired rounds) or double-ended (returning casings back to the magazine). A disadvantage of the M61 is that the bulk of the weapon, its feed system, and ammunition drum makes it difficult to fit it into a densely packed airframe.
The feed system must be custom-designed for each application, adding 300 - to the complete weapon. Most aircraft installations are double-ended, because the ejection of empty cartridges can cause a foreign-object damage (FOD) hazard for jet engines and because the retention of spent cases assists in maintaining the center of gravity of the aircraft. The first aircraft to carry the M61A1 was the C model of the F-104, starting in 1959.
A lighter version of the Vulcan developed for use on the F-22 Raptor, the "M61A2", is mechanically the same as the M61A1, but with thinner barrels to reduce overall weight to 202 lb. The rotor and housing have also been modified to remove any piece of metal not absolutely needed for operation and replaces some metal components with lighter weight materials. The F/A-18E/F Super Hornet also uses this version.
The Vulcan's rate of fire is typically 6,000 rounds per minute, although some versions (such as that of the AMX and the F-106 Delta Dart) are limited to a lower rate, and others (A-7 Corsair) have a selectable rate of fire of either 4,000 or 6,000 rounds per minute. The M61A2's lighter barrels allow a somewhat higher rate of fire up to 6,600 rounds per minute.
Ammunition.
Practically no powered rotary cannon is supplied with sufficient ammunition for a full minute of firing, due to its weight. In order to avoid using the few hundred rounds carried in a matter of a single trigger pull, a burst controller is generally used to limit the number of rounds fired at each trigger pull. Bursts of from two or three up to 40 or 50 can be selected. The size of the airframe and available internal space limits the size of the ammunition drum and thus limits the ammunition capacity.
Until the late 1980s the M61 primarily used the M50 series of ammunition in various types, typically firing a 3.5 oz projectile at a muzzle velocity of about 3380 ft/s. A variety of Armor-Piercing Incendiary (API), High Explosive Incendiary (HEI), and training rounds are available. Around 1988 a new round was introduced, the PGU-28/B, which is now standard for US Navy and Air Force aircraft. The PGU-28/B is a "low-drag" round designed to reduce in-flight drag and deceleration, and has a slightly increased muzzle velocity of 3450 ft/s.
The PGU-28/B is a semi-armor piercing high explosive incendiary (SAPHEI) round, providing substantial improvements in range, accuracy, and power over the preceding M56A3 HEI round. The PGU-28/B has not been without problems, however. A 2000 USAF safety report noted 24 premature detonation mishaps (causing serious damage in many cases) in 12 years with the SAPHEI round, compared to only two such mishaps in the entire recorded history of the M56 round. The report estimated that the current PGU-28/B had a potential failure rate 80 times higher than USAF standards permit.
The main types of combat rounds and their main characteristics are listed in the table below.
Applications and first combat use.
The Vulcan first entered aerial combat on 4 April 1965 when four North Vietnamese Air Force MiG-17s (J-5s) attacked a force of 10 escorting North American F-100 Super Sabres (2 of which were assigned weather reconnaissance duties) and 48 Vulcan armed, but "bomb-laden F-105s", shooting down two Thunderchiefs. The MiG Leader, Capt. Tran Hanh, and the only survivor from the 4 MiGs reported that U.S. jets had pursued them and that F-105s had shot down 3 of his aircraft, killing Lieutenants Pham Giay, Le Minh Huan, and Tran Nguyen Nam. Capt. Donald Kilgus piloting an F-100 did receive an official probable kill with his four M39 20mm cannons during the engagement; however no other US pilot reported destroying any MiGs during the battle, leaving open the plausibility that at least 2 of the MiG-17s may have been downed by their own AA fire.
The first confirmed Vulcan gun kill occurred on 29 June 1966 when Major Fred Tracy, flying his F-105 Thunderchief with the 421st TFS, fired 200 rounds of 20mm into a MiG-17 that had just fired a 23mm shell through one side of his cockpit and which exited out the other side. When the NVAF MiG flew in front of him after making his pass, Maj. Tracy opened fire on him.
The gun was installed in the Air Force's A-7D version of the LTV A-7 Corsair II where it replaced the earlier United States Navy A-7's Colt Mk 12 cannon and was adopted by the Navy on the A-7C and A-7E. It was integrated into the newer F-4E Phantom II variants. The F-4 was originally designed without a cannon as it was believed that missiles had made guns obsolete. Combat experience in Vietnam showed that a gun could be more effective than guided missiles in many combat situations, and that an externally carried gun pod was less effective than an internal gun; the first generation of gun pods such as the SUU-16 were not oriented with the sights of the fighter. The improved pods were self-powered and properly synchronized to the fighters sights. The next generation of fighters built post-Vietnam incorporated the M61 gun internally.
The Vulcan was later fitted into the weapons bay of some Convair F-106 Delta Dart and General Dynamics F-111 Aardvark models. It was also adopted as standard in the "teen"-series air superiority fighters, the Grumman F-14 Tomcat, the McDonnell Douglas F-15 Eagle, General Dynamics F-16 Fighting Falcon and McDonnell Douglas F/A-18 Hornet. Other aircraft include the Italian/Brazilian AMX International AMX (on Italian aircraft only), and the F-22 Raptor. It was fitted in a side-firing installation on the Fairchild AC-119, some marks of the Lockheed AC-130 gunships, and was used in the tail turrets of both the Convair B-58 Hustler and Boeing B-52H Stratofortress bombers. Japan's Mitsubishi F-1 carried one internally mounted JM61A1 Vulcan with 750 rounds.
Two gun pod versions, the SUU-16/A (also designated M12 by the US Army) and improved SUU-23/A (US Army M25), were developed in the 1960s, often used on gunless versions of the F-4. The SUU-16/A uses the electric M61A1 with a ram-air turbine to power the motor. This proved to cause serious aerodynamic drag at higher speeds, while speeds under 400 mph did not provide enough airflow for maximum rate of fire.
The subsequent SUU-23/A uses the "GAU-4/A" self-powered Vulcan, with an electric inertia starter to bring it up to speed. Both pods ejected empty cases and unfired rounds rather than retaining them. Both pods contained 1,200 rounds of ammunition, with a loaded weight of 1615 and respectively. During service in the Vietnam War the pods proved to be relatively inaccurate: the pylon mounting was not rigid enough to prevent deflection when firing, and repeated use would misalign the pod on its pylon, making matters worse.
A variant with much shorter barrels, designated the M195, was also developed for use on the M35 Armament Subsystem for use on the AH-1G Cobra helicopter. This variant fed from ammunition boxes fitted to the landing skid and was developed to provide the AH-1 helicopter with a longer-range suppressive fire system before the adoption of the M97 Universal Turret mounting the M197 cannon.
The M61 is also the basis of the US Navy Mk 15 Phalanx Close-in weapon system system and the M163 VADS Vulcan Air Defense System, using the M168 variant.

</doc>
<doc id="44077" url="http://en.wikipedia.org/wiki?curid=44077" title="Captains Courageous">
Captains Courageous

Captains Courageous is an 1897 novel, by Rudyard Kipling, that follows the adventures of fifteen-year-old Harvey Cheyne Jr., the spoiled son of a railroad tycoon, after he is saved from drowning by a Portuguese fisherman in the north Atlantic. The novel originally appeared as a serialisation in "McClure's", beginning with the November 1896 edition.
The book's title comes from the ballad "Mary Ambree", which starts, "When captains courageous, whom death could not daunt". Kipling had previously used the same title for an article on businessmen as the new adventurers, published in "The Times" of 23 November 1892.
Plot.
Protagonist Harvey Cheyne, Jr., is the son of a wealthy railroad magnate and his wife, in San Diego, California. Washed overboard from a transatlantic steamship and rescued by fishermen off the Grand Banks of Newfoundland, Harvey can neither persuade them to take him quickly to port, nor convince them of his wealth. Disko Troop, captain of the schooner "We're Here", offers him temporary membership in the crew until they return to port, and Harvey later accepts.
Through a series of trials and adventures, Harvey, with the help of the captain's son Dan Troop, becomes acclimated to the fishing lifestyle, and even skillful. Eventually, the schooner returns to port and Harvey wires his parents, who immediately hasten to Boston, Massachusetts, and thence to the fishing town of Gloucester to recover him. There, Harvey's mother rewards the seaman Manuel, who initially rescued her son; Harvey's father hires Dan to work on his prestigious tea clipper fleet; and Harvey begins his career in his father's shipping lines.
Notes.
The book was written during Kipling's time living in Brattleboro, Vermont. Kipling recalled in his autobiography:
Now our Dr. [James] Conland had served in [the Gloucester] fleet when he was young. One thing leading to another, as happens in this world, I embarked on a little book which was called "Captains Courageous". My part was the writing; his the details. This book took us (he rejoicing to escape from the dread respectability of our little town) to the shore-front, and the old T-wharf of Boston Harbour, and to queer meals in sailors’ eating-houses, where he renewed his youth among ex-shipmates or their kin. We assisted hospitable tug-masters to help haul three- and four-stick schooners of Pocahontas coal all round the harbour; we boarded every craft that looked as if she might be useful, and we delighted ourselves to the limit of delight. ... Old tales, too, he dug up, and the lists of dead and gone schooners whom he had loved, and I revelled in profligate abundance of detail—not necessarily for publication but for the joy of it. ...I wanted to see if I could catch and hold something of a rather beautiful localised American atmosphere that was already beginning to fade. Thanks to Conland I came near this.
Kipling also recalled:
When, at the end of my tale, I desired that some of my characters should pass from San Francisco [sic] to New York in record time, and wrote to a railway magnate of my acquaintance asking what he himself would do, that most excellent man sent a fully worked-out time-table, with watering halts, changes of engine, mileage, track conditions and climates, so that a corpse could not have gone wrong in the schedule.
The resulting account, in Chapter 9, of the Cheynes' journey from San Diego to Boston, is a classic of railway literature. The couple travel in the Cheynes' private rail car, the "Constance", and are taken from San Diego to Chicago as a special train, hauled by sixteen locomotives in succession. It takes precedence over 177 other trains. "Two and one-half minutes would be allowed for changing engines; three for watering and two for coaling". The "Constance" is attached to the scheduled express "New York Limited" to Buffalo, New York and then transferred to the New York Central for the trip across the state to Albany. Switched to the Boston and Albany Railroad, the Cheynes complete the trip to Boston in their private car, with the entire cross-country run taking 87 hours 35 minutes. 
Kipling also recalled:
My characters arrived triumphantly; and, then, a real live railway magnate was so moved after reading the book that he called out his engines and called out his men, hitched up his own private car, and set himself to beat "my" time on paper over the identical route, and succeeded. 
Disko Troop claims to receive his given name for his birth on board his father's ship near Disko Island on the west coast of Greenland. His crewman 'Long Jack' once calls him "Discobolus".
Film, TV, theatrical, or other adaptations.
"Captains Courageous" has been adapted for film three times:
Musical theatre:
Other adaptations:

</doc>
<doc id="44085" url="http://en.wikipedia.org/wiki?curid=44085" title="John Byron">
John Byron

Vice Admiral The Hon. John Byron, RN (8 November 1723 – 10 April 1786) was a Royal Navy officer. He was known as Foul-weather Jack because of his frequent encounters with bad weather at sea.
Early career.
Byron was the son of William Byron, 4th Baron Byron and Frances Berkeley. He joined the navy in 1731, accompanying George Anson on his circumnavigation of the globe as a midshipman. On 14 May 1741, Byron's ship, HMS "Wager", was shipwrecked on the coast of Chile. The survivors decided to split in two teams, one to make its way by boat to Rio de Janeiro on the Atlantic coast; the other, John Byron's, to sail North and meet Spaniards.
He later wrote about his adventures and the Wager Mutiny in "The Narrative of the Honourable John Byron" (1768). His book sold well enough to be printed in several editions. His experiences form the basis of the novel "The Unknown Shore" by Patrick O'Brian, which closely follows Byron's account.
Byron was appointed captain of HMS "Siren" in December 1746.
Seven Years War.
In 1760 during the Seven Years' War, Byron commanded a squadron sent to destroy the fortifications at Louisbourg, Quebec, which had been captured by the British two years before. They wanted to ensure it could not be used by the French in Canada. In July of that year he defeated the French flotilla sent to relieve New France at the Battle of Restigouche.
Between June 1764 and May 1766, Byron completed his own circumnavigation of the globe as captain of HMS "Dolphin". This was the first such circumnavigation that was accomplished in less than 2 years. During this voyage, in 1765 he took possession of the Falkland Islands on behalf of Britain on the grounds of prior discovery. His action nearly caused a war between Great Britain and Spain, as both countries had armed fleets ready to contest the sovereignty of the barren islands. Later Byron encountered islands and extant residents of the Tuamotus and Tokelau Islands, and Nikunau in the southern Gilbert Islands; he also visited Tinian in the Northern Marianas Islands.
In 1769 he was appointed governor of Newfoundland off the mainland of Canada, an office he held for the next three years. He was promoted to rear admiral on 31 March 1775, and vice admiral on 29 January 1778.
In 1778 and 1779, he served as Commander-in-chief of the British fleet in the West Indies during the American War of Independence. He unsuccessfully attacked a French fleet under the Comte d'Estaing at the Battle of Grenada in July 1779. Byron was briefly Commander-in-Chief, North American Station from 1 October 1779.
Family.
On 8 September 1748 he married Sophia Trevanion, daughter of John Trevanion of Caerhays in Cornwall, by whom he had two sons and seven daughters, three of whom died in infancy. Their eldest son, John "Mad Jack" Byron, in turn fathered the poet George Gordon Byron, the future 6th Baron Byron. John Byron was also the grandfather of George Anson Byron, another admiral and explorer and later the 7th Baron Byron. He was the brother of Hon. George Byron, married to Frances Levett, daughter of Elton Levett of Nottingham, a descendant of Ambrose Elton, Esq., High Sheriff of Herefordshire in 1618 and a surgeon in Nottingham.
Death.
John Byron died on 10 April 1786. His remains were buried in the Berkeley family vault situated beneath the chancel of the Church of St Mary the Virgin, Twickenham.

</doc>
<doc id="44086" url="http://en.wikipedia.org/wiki?curid=44086" title="Lew Wallace">
Lew Wallace

Lewis "Lew" Wallace (April 10, 1827 – February 15, 1905) was an American lawyer, Union general in the American Civil War, governor of the New Mexico Territory, politician, diplomat, and author from Indiana. Among his novels and biographies, Wallace is best known for his historical adventure story, "" (1880), a bestselling novel that has been called "the most influential Christian book of the nineteenth century."
Wallace's military career included service in the Mexican-American War and the American Civil War. He was appointed Indiana's adjutant general and commanded the 11th Indiana Infantry Regiment. Wallace, who attained the rank of major general, participated in the battle of Fort Donelson, the battle of Shiloh, and the battle of Monocacy. He also served on the military commission for the trials of the Lincoln assassination conspirators, and presided over the military investigation of Henry Wirz, a Confederate commandant of the Andersonville prison camp.
Wallace resigned from the U.S. Army in November 1865 and briefly served as a major general in the Mexican army, before returning to the United States. Wallace was appointed governor of the New Mexico Territory (1878–81) and served as U.S. minister to the Ottoman Empire (1881–85). Wallace retired to his home in Crawfordsville, Indiana, where he continued to write until his death in 1905.
Early life and education.
Lewis "Lew" Wallace was born on April 10, 1827, in Brookville, Indiana. He was the second of four sons born to Esther French Wallace (née Test) and David Wallace. Lew's father, a graduate of the U.S. Military Academy in West Point, New York, left the military in 1822 and moved to Brookville, where he established a law practice and entered Indiana politics. David served in the Indiana General Assembly and later as the state's lieutenant governor, and governor, and as a member of Congress. Lew Wallace's maternal grandfather was circuit court judge and Congressman John Test.
In 1832 the family moved to Covington, Indiana, where Lew's mother died from tuberculosis on July 14, 1834. In December 1836, David married nineteen-year-old Zerelda Gray Sanders Wallace, who later became a prominent suffragist and temperance advocate. In 1837, after David's election as governor of Indiana, the family moved to Indianapolis.
Lew began his formal education at the age of six at a public school in Covington, but he much preferred the outdoors. Wallace had a talent for drawing and loved to read, but he was a discipline problem at school. In 1836, at the age of nine, Lew joined his older brother in Crawfordsville, Indiana, where he briefly attended Wabash Preparatory School, but soon transferred to another school more suitable for his age. In 1840, when Wallace was thirteen, his father sent him to a private academy at Centerville, Indiana, where his teacher encouraged Lew's natural affinity for writing. Wallace returned to Indianapolis the following year.
Sixteen-year-old Lew went out to earn his own wages in 1842, after his father refused to pay for more schooling. Wallace found a job copying records at the Marion County clerk's office and lived in an Indianapolis boardinghouse. He also joined the Marion Rifles, a local militia unit, and began writing his first novel, "The Fair God", but it was not published until 1873. Wallace acknowledged in his autobiography that he had never been a member of any organized religion, but he did believe "in the Christian conception of God."
By 1846, at the start of the Mexican–American War, the nineteen-year-old Wallace was studying law at his father's law office, but left that pursuit to established a recruiting office for the Marion Volunteers in Indianapolis. He was appointed a second lieutenant, and on June 19, 1846, mustered into military service with the Marion Volunteers (also known as Company H, 1st Indiana Volunteer Infantry). Wallace rose to the position of regimental adjutant and the rank of first lieutenant while serving in the army of Zachary Taylor, but Wallace personally did not participate in combat. Wallace was mustered out of the volunteer service on June 15, 1847, and returned to Indiana, where he intended to practice law. After the war, Wallace and William B. Greer operated a Free Soil newspaper, "The Free Soil Banner," in Indianapolis.
Marriage and family.
In 1848 Wallace met Susan Arnold Elston at the Crawfordsville home of Henry Smith Lane, Wallace's former commander during the Mexican War. Susan was the daughter of Major Isaac Compton Elston, a wealthy Crawfordsville merchant, and Maria Akin Elson, whose family were Quakers from upstate New York. Susan accepted Wallace's marriage proposal in 1849, and they were married in Crawfordsville on May 6, 1852. The Wallaces had one son, Henry Lane Wallace, who was born on February 17, 1853.
Early law and military career.
Wallace was admitted to the bar in February 1849, and moved from Indianapolis to Covington, Indiana, where he established a law practice. In 1851 Wallace was elected prosecuting attorney of Indiana's 1st congressional district, but he resigned in 1853 and moved his family to Crawfordsville, in Montgomery County, Indiana. Wallace continued to practice law and was elected as a Democrat to a two-year term in the Indiana Senate in 1856.
While living in Crawfordsville, Wallace organized the Crawfordsville Guards Independent Militia, later called the Montgomery Guards. During the winter of 1859–60, after reading about elite units of the French Army in Algeria, Wallace adopted the Zouave uniform and their system of training for the group. The Montgomery Guards would later form the core of his first military command, the 11th Indiana Volunteer Infantry, during the American Civil War.
Civil War service.
Wallace, a staunch, pro-Union supporter who became a member of the Republican party, began his full-time military career after the Confederate attack on Fort Sumter, South Carolina, on April 12, 1861. Indiana's Republican governor, Oliver P. Morton, asked Wallace to help recruit Indiana volunteers for the Union army. Wallace, who also sought a military command, agreed to become the state's adjutant general on the condition that he would be given command of a regiment of his choice. Indiana's quota of six regimental units was filled within a week, and Wallace took command of the 11th Indiana Volunteer Infantry Regiment, which was mustered into the Union army on April 25, 1861. Wallace received his formal commission as a colonel in the Union army the following day.
On June 5, 1861, Wallace went with the 11th Indiana to Cumberland, Maryland, and on June 12, the regiment won a minor battle at Romney, Virginia, (in present-day West Virginia). The rout boosted morale for Union troops and led to the Confederate evacuation of Harpers Ferry on June 18. On September 3, 1861, Wallace was promoted to brigadier general of U.S. Army volunteers and given command of a brigade.
Forts Henry and Donelson.
On February 4 and 5, 1862, prior to the advance against Fort Henry, Union troops under the command of Brig. Gen. Ulysses S. Grant and a flotilla of Union ironclads and timberclad gunboats under the command of Flag Officer Andrew Hull Foote made their way toward the Confederate fort along the Tennessee River in western Tennessee. Wallace's brigade, which was attached to Brig. Gen. Charles F. Smith's division, was ordered to occupy Fort Heiman, an uncompleted Confederate fort across the river from Fort Henry. Wallace's troops secured the deserted fort and watched the Union attack on Fort Henry from their hilltop position. On February 6, after more than an hour of bombardment from the Union gunboats, Confederate Brig. Gen. Lloyd Tilghman, surrendered Fort Henry to Grant.
Grant's superior, Maj. Gen. Henry W. Halleck, was concerned that Confederate reinforcements would try to retake the two forts when the Union troops moved overland toward Fort Donelson, so Wallace was left in command at Fort Henry to keep the forts secure. Displeased to have been left behind, Wallace prepared his troops to move out at a moment's notice. The order came at midnight on February 13. Wallace arrived along the Cumberland River the following day and was placed in charge of the 3rd Division. Many of the men in the division were untested reinforcements. Wallace's three brigades took up position in the center of the Union line, facing Fort Donelson.
During the fierce Confederate assault on February 15, Wallace acted on his own initiative to send a brigade to reinforce the beleaguered division of Brig. Gen. John A. McClernand despite orders from Grant to avoid a general engagement. Wallace's decision stopped the forward movement of the Confederates and was key in stabilizing a defensive line for the Union troops. After the Confederate assault had been checked, Wallace led a counterattack that regained lost ground on the Union right. On March 21, 1862, Wallace, McClernand, and C. F. Smith were promoted to major general for their efforts. Wallace, who was age thirty-four at the time of his promotion, became the youngest major general in the Union army.
Shiloh.
Wallace's most controversial command came at the battle of Shiloh, where he continued as the 3rd Division commander under Maj. Gen. Grant. The long-standing controversy developed around the contents of Wallace's written orders on April 6, the 3rd Division's movements on the first day of battle, and their late arrival on the field. On the second day of battle, Wallace's division joined reinforcements from Maj. Gen. Don Carlos Buell's army to play an important role in the Union victory.
Prior to the battle, Wallace's division had been left in reserve and was encamped near Crump's Landing. Their orders were to guard the Union’s right flank and cover the road to Bethel Station, Tennessee, where railroad lines led to Corinth, Mississippi, 20 mi to the south. To protect the road from Crump's Landing and Bethel Station, Wallace sent Col. John M. Thayer's 2nd Brigade to Stoney Lonesome, 3 mi west of at Crump's Landing, and the 3rd Brigade, commanded by Col. Charles Whittlesey to Adamsville, 5.5 mi west of Crump's Landing. Col. Morgan L. Smith's 1st Brigade remained with Wallace at Crump's Landing, 5 mi north of Pittsburg Landing, Tennessee.
Between 5 and 6 a.m. on April 6, 1862, Grant's army at Pittsburg Landing was surprised and nearly routed by a sudden attack from the Confederate army under Gen. Albert Sidney Johnston. Grant, who heard the early morning artillery fire, took a steamboat from his headquarters at Savannah, Tennessee, to Crump's Landing, where he gave Wallace orders to wait in reserve and be ready to move. Grant proceeded to Pittsburg Landing, where he arrived around 8:30 a.m. Grant's new orders to Wallace, which arrived between 11 and 11:30 a.m., were given verbally to an aide, who transcribed them before they were delivered. The written orders were lost during the battle, so their exact wording cannot be confirmed; however, eyewitness accounts agree that Grant ordered Wallace to join the right side of the Union army, presumably in support of Brig. Gen. William Tecumseh Sherman's 5th Division, who were encamped near Shiloh Church on the morning of April 6.
Knowledge of the area's roads played a critical role in Wallace's journey to the battlefield on April 6. In late March, after heavy rains made transportation difficult between Crump's Landing and Pittsburg Landing, Wallace's men had opened a route to Pittsburg Landing along Shunpike road, which connected to a road near Sherman's camp. Brig. Gen. W. H. L. Wallace's men at Pittsburg Landing opened the River Road (also known as the Hamburg-Savannah Road), a route farther east.
Of the two main routes that Wallace could use to move his men to the front, he chose the Shunpike road, the more direct route to reach Sherman's division near Shiloh Church. The day before the battle, Wallace wrote a letter to a fellow officer. W. H. L. Wallace, stating his intention to do so. Lew Wallace and his staff maintained after the battle that Grant's order did not specify Pittsburg Landing as their destination or indicated a specific route. However, Grant claimed in his memoirs that he had ordered Wallace to take the route nearest to the river to reach Pittsburg Landing. Historians are divided, with some stating that Wallace's explanation is the most logical.
After a second messenger from Grant arrived around noon with word to move out, Wallace's division of approximately 5,800 men began their march toward the battlefield. Between 2 and 2:30 p.m., a third messenger from Grant found Wallace along the Shunpike road, where he informed Wallace that Sherman had been forced back from Shiloh Church and was fighting closer to the river, near Pittsburg Landing. The Union army had been pushed back so far that Wallace was to the rear of the advancing Southern troops.
Wallace considered attacking the Confederates, but abandoned the idea. Instead he made a controversial decision to countermarch his troops along the Shunpike road, follow a crossroads to the River Road, and then move south to Pittsburg Landing. Rather than realigning his troops, so that the rear guard would be in the front, Wallace countermarched his column to maintain their original order, keeping his artillery in the lead position to support the Union infantry on the field After the time-consuming maneuver was completed, Wallace's troops returned to the midpoint on the Shunpike road, crossed east over a path to the River Road, and followed it south to join Grant's army on the field. Progress was slow due to the road conditions and countermarch. Wallace's division arrived at Pittsburg Landing about 6:30 p.m., after having marched about 14 mi in nearly seven hours over roads that had been left in terrible conditions by recent rainstorms and previous Union marches. They gathered at the battlefield at dusk, about 7 p.m., with the fighting nearly over for the day, and took up a position on the right of the Union line.
The next day, April 7, Wallace's division held the extreme right of the Union line. Two of Wallace's batteries with the aid of a battery from the 1st Illinois Light Artillery were the first to attack at about 5:30 a.m. Sherman's and Wallace's troops helped force the Confederates to fall back, and by 3 p.m. the Confederates were retreating southwest, toward Corinth.
Shiloh controversy.
At first, the battle was viewed by the North as a victory; however, on April 23, after civilians began hearing news of the high number of casualties, the Lincoln administration asked the Union army for further explanation. Grant, who was accused of poor leadership at Shiloh, and his superior, Halleck, placed the blame on Wallace by asserting that his failure to follow orders and the delay in moving up the reserves on April 6 had nearly cost them the battle. On April 30, 1862, Halleck reorganized his army and removed Wallace and John McClernand from active duty, placing both of them in reserve.
Wallace's reputation and career as a military leader suffered a significant setback from controversy over Shiloh. He spent the remainder of his life trying to resolve the accusations and change public opinion about his role in the battle. On March 14, 1863, Wallace wrote a letter to Halleck that provided an official explanation of his actions. He also wrote Grant several letters and met with him in person more than once in an attempt to vindicate himself. On August 16, 1863, Wallace wrote Sherman for advice on the issue. Sherman urged Wallace to be patient and not to request a formal inquiry. Although Sherman brought Wallace's concerns to Grant's attention, Wallace was not given another active duty command until March 1864.
For many years Grant stood by his original version of the orders to Wallace. As late as 1884, when Grant wrote an article on Shiloh for "The Century Magazine" that appeared in its February 1885 issue, he maintained that Wallace had taken the wrong road on the first day of battle. After W. H. L. Wallace's widow gave Grant a letter that Lew Wallace had written to her husband the day before the battle (the one indicating his plans to use the Shunpike road to pass between Shiloh and his position west of Crump's Landing), Grant changed his mind. Grant wrote a letter to the editors at "Century", which was published in its September 1885 issue, and added a note to his memoirs to explain that Wallace's letter "modifies very materially what I have said, and what has been said by others, about the conduct of General Lew Wallace at the battle of Shiloh." While reaffirming that he had ordered Wallace to take the River Road, Grant stated that he could not be sure the exact content of Wallace's written orders, since his verbal orders were given to one of his aides and transcribed.
Grant's article in the February 1885 issue of "Century" became the basis of his chapter on Shiloh in his memoirs, which were published in 1886, and influenced many later accounts of Wallace's actions on the first day of battle. Grant acknowledged in his memoirs: "If the position of our front had not changed, the road which Wallace took would have been somewhat shorter to our right than the River road." Wallace's account of the events appeared in his autobiography, which was published posthumously in 1906. Despite his later fame and fortune as the novelist of "Ben-Hur", Wallace continued to lament, "Shiloh and its slanders! Will the world ever acquit me of them? If I were guilty I would not feel them as keenly."
Other military assignments.
On August 17, 1862, Wallace accepted a regiment command in the Department of the Ohio to help with the successful defense of Cincinnati during Braxton Bragg's incursion into Kentucky. Next, Wallace took command of Camp Chase, a prisoner-of-war camp at Columbus, Ohio, where he remained until October 30, 1862. A month later Wallace was placed in charge of a five-member commission to investigate Maj. Gen. Don Carlos Buell's conduct in response to the Confederate invasion of Kentucky. The commission criticized Buell for his retreat, but it did not find him disloyal to the Union. When the commission's work was completed on May 6, 1863, Wallace returned to Indiana to wait for a new command. In mid-July 1863, while Wallace was home, he helped protect the railroad junction at North Vernon, Indiana, from Confederate general John Hunt Morgan's raid into southern Indiana.
Monocacy.
Wallace's most notable service came on Saturday, July 9, 1864 at the battle of Monocacy part of the Valley Campaigns of 1864. Although Confederate General Jubal A. Early and an estimated 15,000 troops defeated Wallace's troops at Monocacy Junction, Maryland, forcing them to retreat to Baltimore, the effort cost Early a chance to capture Washington, D.C. Wallace's men were able to delay the Confederate advance toward Washington for an entire day, giving the city time to organize its defenses. Early arrived in Washington at around noon on July 11, two days after defeating Wallace at Monocacy, the northernmost Confederate victory of the war, but Union reinforcements had already arrived at Fort Stevens to repel the Confederates and force their retreat to Virginia.
Wallace, who had returned to active duty on March 12, 1864, assumed command of VIII Corps, which was headquartered in Baltimore. On July 9, a combined Union force of approximately 5,800 men under Wallace's command (mostly hundred-days' men from VIII Corps) and a division under James B. Ricketts from VI Corps encountered Confederate troops at Monocacy Junction between 9 and 10 a.m. Although Wallace was uncertain whether Baltimore or Washington, D.C. was the Confederate objective, he knew his troops would have to delay the advance until Union reinforcements arrived. Wallace's men repelled the Confederate attacks for more than six hours before retreating to Baltimore.
After the battle Wallace informed Halleck that his forces fought until 5 p.m., but the Confederate troops, which he estimated at 20,000 men, had overwhelmed them. When Grant learned of the defeat, he named Maj. Gen. E. O. C. Ord as Wallace's replacement in command of VIII Corps. On July 28, after officials learned how Wallace's efforts at Monocacy helped save Washington D.C. from capture, he was reinstated as commander of VIII Corps. In Grant's memoirs, he praised Wallace's delaying tactics at Monocacy:
If Early had been but one day earlier, he might have entered the capital before the arrival of the reinforcements I had sent. ... General Wallace contributed on this occasion by the defeat of the troops under him, a greater benefit to the cause than often falls to the lot of a commander of an equal force to render by means of a victory.
Later military service.
On January 22, 1865 Grant ordered Wallace to the Rio Grande in southern Texas to investigate Confederate military operations in the area. Although Wallace was not officially authorized to offer terms, he did discuss proposals for the surrender of the Confederate troops in the Trans-Mississippi Department. Wallace provided Grant with copies of his proposals and reported on the negotiations, but no agreement was made. Before returning to Baltimore, Wallace also met with Mexican military leaders to discuss the U.S. government's unofficial efforts to aid in expelling Maximilian's French occupation forces from Mexico.
Following President Lincoln's death on April 15, 1865, Wallace was appointed to the military commission that investigated the Lincoln assassination conspirators. The commission, which began in May, was dissolved on June 30, 1865, after all eight conspirators were found guilty. In mid-August 1865, Wallace was appointed head of an eight-member military commission that investigated the conduct of Henry Wirz, the Confederate commandant in charge of the South's Andersonville prison camp. The court-martial which took nearly two months, opened on August 21, 1865. At its conclusion Wirz was found guilty and sentenced to death.
On April 30, 1865, Wallace had accepted an offer to become a major general in the Mexican army, but the agreement, which was contingent upon his resignation from the U.S. Army, was delayed by Wallace's service on the two military commissions. Wallace tendered his resignation from the U.S. Army on November 4, 1865, effective November 30, and returned to Mexico to assist the Mexican army. Although the Juárez government promised Wallace $100,000 for his services, he returned to the United States in 1867
in deep financial debt.
Political and diplomatic career.
Wallace returned to Indiana in 1867 to practice law, but the profession did not appeal to him, and he turned to politics. Wallace made two unsuccessful bids for a seat in Congress (in 1868 and 1870), and supported Republican presidential candidate Rutherford B. Hayes in the 1878 election. As a reward for his political support, Hayes appointed Wallace as governor of the New Mexico Territory, where he served from August 1878 to March 1881. His next assignment came in March 1881, when Republican president James A. Garfield appointed Wallace to an overseas diplomatic post in Constantinople, Turkey, as U.S. Minister to the Ottoman Empire. Wallace remained in this post until 1885.
Territorial governor of New Mexico.
Wallace arrived in Santa Fe, on September 29, 1878, to begin his service as governor of the New Mexico Territory during a time of lawless violence and political corruption. Wallace was involved in efforts to resolve New Mexico's Lincoln County War, a contentious and violent disagreement among the county’s residents, and tried to end a series of Apache raids on territorial settlers. In 1880, while living at the Palace of the Governors in Santa Fe, Wallace also completed the manuscript for "Ben Hur".
On March 1, 1879, after previous efforts to restore order in Lincoln County had failed, Wallace ordered the arrest of those responsible for local killings. One of the outlaws was William Henry McCarty, Jr. (alias William H. Bonney), better known as Billy the Kid. On March 17, 1879, Wallace secretly met with the Kid, who had witnessed the murder of a Lincoln County lawyer named Chapman. Wallace wanted the Kid to testify in the trial of Chapman's accused murderers, but the Kid had killed others and wanted Wallace's protection from the outlaw gang and amnesty for his crimes. During their meeting, the pair arranged for the Kid to become an informant in exchange for a full pardon of his previous crimes. Wallace supposedly assured the Kid that he would be "scot free with a pardon in your pocket for all your misdeeds." On March 20, the Kid agreed to testify against others involved in Chapman's murder. Wallace arranged for the Kid's arrest and detention in a local jail to assure his safety. After the Kid testified in court on April 14, the local district attorney revoked Wallace's bargain and refused to set the outlaw free. The Kid escaped from jail and returned to his criminal ways, which included killing additional men. The Kid was shot and killed on July 14, 1881 by Pat Garrett who had been appointed by local ranching interests who had tired of his rustling their herds. In the meantime, Wallace had resigned from his duties as territorial governor on March 9, 1881, and was waiting for a new political appointment.
On December 31, 2010, on his last day in office, then-Governor Bill Richardson of New Mexico declined a pardon request from supporters of the Kid, citing a "lack of conclusiveness and the historical ambiguity" over Wallace's promise of amnesty. Descendants of Wallace and Pat Garrett, the sheriff who killed Billy the Kid, were among those who opposed the pardon.
U.S. diplomat in Turkey.
On May 19, 1881, Wallace was appointed U.S. Minister to the Ottoman Empire in Constantinople (present-day Istanbul), Turkey. Wallace remained at the diplomatic post until 1885, and became a trusted friend of Sultan Abu Hamid II. When a crisis developed between the Turkish and British governments over control of Egypt, Wallace served as an intermediary between the sultan and Lord Dufferin, the British ambassador. Although Wallace's efforts were unsuccessful, he earned respect for his efforts and a promotion in the U.S. diplomatic service.
In 1883, an editorial aimed at Wallace appeared in Havatzelet (xiii. No. 6) titled "An American and yet a Despot". The editorial caused the Havatzelet to be suspended and its editor imprisoned for forty-five days by order from Constantinople directed to the pasha of Jerusalem. The incident that lead to the editorial was the dismissal, made at Wallace's request, of Joseph Kriger, the Jewish secretary and interpreter to the pasha of Jerusalem. Wallace complained that Kriger had failed to receive him with the honor due to his rank, and refused to issue any apology for the alleged shortcoming. Havatzelet claimed that the proceeding was instigated by missionaries, whom Wallace strongly supported.
In addition to Wallace's diplomatic duties, which included protection of U.S. citizens and U.S. trade rights in the area, Wallace found time to travel and do historical research. Wallace visited Jerusalem and the surrounding area, the site for his novel, "Ben-Hur", and did research in Constantinople, the locale for "The Prince of India; or, Why Constantinople Fell", which he began writing in 1887.
The election of Grover Cleveland, the Democratic candidate for president, ended Wallace's political appointment. He resigned from the U.S. diplomatic service on March 4, 1885. The sultan wanted Wallace to continue to work in Turkey, and even made a proposal to have him represent Turkish interests in England or France, but Wallace declined and returned home to Crawfordsville.
Writing career.
Wallace confessed in his autobiography that he took up writing as a diversion from studying law. Although he wrote several books, Wallace is best known for his historical adventure story, "" (1880), which established his fame as an author.
In 1843 Wallace began writing his first novel, "The Fair God", but it was not published until 1873. The popular historical novel, with Cortez's conquest of Mexico as its central theme, was based on William H. Prescott's "History of the Conquest of Mexico". Wallace's book sold seven thousand copies in its first year. Its sales continued to rise after Wallace's reputation as an author was established with the publication of subsequent novels.
Wallace wrote the manuscript for "Ben-Hur", his second and best-known novel, during his spare time at Crawfordsville, and completed it in Santa Fe, while serving as the territorial governor of New Mexico. "Ben-Hur", an adventure story of revenge and redemption, is told from the perspective of a Jewish nobleman named Judah Ben-Hur. Because Wallace had not been to the Holy Land before writing the book, he began research to familiarize himself with the area's geography and its history at the Library of Congress in Washington, D.C. in 1873. Harper and Brothers published the book on November 12, 1880.
"Ben-Hur" made Wallace a wealthy man and established his reputation as a famous author. Sales were slow at first, only 2,800 copies were sold in the first seven months after its release, but the book became popular among readers around the world By 1886 it was earning Wallace about $11,000 in annual royalties, a substantial amount at the time, and provided Wallace’s family with financial security. By 1889 Harper and Brothers had sold 400,000 copies and the book had been translated into several languages.
In 1900 "Ben-Hur" became the best-selling American novel of the 19th century, surpassing Harriet Beecher Stowe's "Uncle Tom's Cabin". Amy Lifson, an editor for "Humanities", identified it as the most influential Christian book of the 19th century. Others named it one of the best-selling novels of all time. At the time of "Ben-Hur"'s one hundredth anniversary in 1980, it had "never been out of print" and had been adapted for the stage and several motion pictures. One historian, Victor Davis Hanson, has argued that "Ben-Hur" drew from Wallace's life, particularly his experiences at Shiloh, and the damage it did to his reputation. The book's main character, Judah Ben-Hur, accidentally causes injury to a high-ranking Roman commander, for which he and his family suffer tribulations and calumny.
Wallace wrote subsequent novels and biographies, but "Ben-Hur" remained his most important work. Wallace considered "The Prince of India; or, Why Constantinople Fell" (1893) as his best novel. He also wrote a biography of President Benjamin Harrison, a fellow Hoosier and Civil War general, and "The Wooing of Malkatoon" (1898), a narrative poem. Wallace was writing his autobiography when he died in 1905. His wife Susan completed it with the assistance of Mary Hannah Krout, another author from Crawfordsville. It was published posthumously in 1906.
Later years.
Wallace continued to write after his return from Turkey. He also patented several of his own inventions, built a seven-story apartment building in Indianapolis, and drew up plans for a private study at his home in Crawfordsville. Wallace remained active in veterans groups, including writing a speech for the dedication of the battlefield at the Chickamauga.
Wallace's elaborate writing study, which he described as "a pleasure-house for my soul", served as his private retreat. Now called the General Lew Wallace Study and Museum, it was built between 1895 and 1898, adjacent to his residence in Crawfordsville, and set in an enclosed park. The study along with three and one-half acres of its grounds were designated a National Historic Landmark in 1976. The property is operated as a museum, open to the public.
On April 5, 1898, at the outbreak of the Spanish–American War, Wallace, at age seventy-one, offered to raise and lead a force of soldiers, but the war office refused. Undeterred, he went to a local recruiting office and attempted to enlist as a private, but was rejected again, presumably because of his age.
Wallace's service at the battle of Shiloh continued to haunt him in later life. The debate persisted in book publications, magazine articles, pamphlets, speeches, and in private correspondence. Wallace attended a reunion at Shiloh in 1894, his first return since 1862, and retraced his journey to the battlefield with veterans from the 3rd Division. He returned to Shiloh for a final time in 1901 to walk the battlefield with David W. Reed, the Shiloh Battlefield Commission's historian, and others. Wallace died before the manuscript of his memoirs was fully completed, so it is unknown whether he would have revised his final account of the battle.
Death.
Wallace died at home in Crawfordsville, on February 15, 1905, of atrophic gastritis. He was seventy-seven years old. Wallace is buried in Crawfordsvill'e Oak Hill Cemetery.
Legacy and honors.
Wallace was a man of many interests and a lifelong adventure seeker, who remained a persistent, self-confident man of action. He was also impatient and highly sensitive to personal criticisms, especially those related to his command decisions at Shiloh. Despite Wallace's career in law and politics, combined with years of military and diplomatic service, he achieved his greatest fame as a novelist, most notably for a best-selling biblical tale, "Ben-Hur".
Following Wallace's death, the State of Indiana commissioned sculptor Andrew O'Connor to create a marble statue of Wallace dressed in a military uniform for the National Statuary Hall Collection in the U.S. Capitol. The statue was unveiled during a ceremony held on January 11, 1910. Wallace is the only novelist honored in the hall. A bronze copy of the statue is installed on the grounds of Wallace's study in Crawfordsville.
Lew Wallace High School opened in 1926 at 415 West 45th Avenue in Gary, Indiana. On June 3, 2014, the Gary School Board voted 4 to 2 to close Lew Wallace, along with five other schools.

</doc>
<doc id="44088" url="http://en.wikipedia.org/wiki?curid=44088" title="Vittorio Gassman">
Vittorio Gassman

Vittorio Gassman, Knight Grand Cross, OMRI (]; born Vittorio Gassmann; 1 September 1922 – 29 June 2000), popularly known as Il Mattatore, was an Italian theatre and film actor, as well as director.
He is considered one of the greatest Italian actors and is commonly recalled as an extremely professional, versatile, magnetic interpreter, whose long career includes both important productions as well as dozens of "divertissements" (which made him greatly popular).
Biography.
Early life.
He was born in Genoa to a German father, Heinrich Gassmann, and a Pisan Jewish mother, Luisa Ambron. While still very young he moved to Rome, where he studied at the Accademia Nazionale d'Arte Drammatica.
Career.
Gassman's debut was in Milan, in 1942, with Alda Borelli in Niccodemi's "Nemica" (theatre). He then moved to Rome and acted at the "Teatro Eliseo" joining Tino Carraro and Ernesto Calindri in a team that remained famous for some time; with them he acted in a range of plays from bourgeois comedy to sophisticated intellectual theatre. In 1946, he made his film debut in "Preludio d'amore", while only one year later he appeared in five films. In 1948 he played in "Riso amaro".
It was with Luchino Visconti's company that Gassman achieved his mature successes, together with Paolo Stoppa, Rina Morelli and Paola Borboni. He played Stanley Kowalski in Tennessee Williams' "Un tram che si chiama desiderio" ("A Streetcar Named Desire"), as well as in "As You Like It" (by Shakespeare) and "Oreste" (by Vittorio Alfieri). He joined the "Teatro Nazionale" with Tommaso Salvini, Massimo Girotti, Arnoldo Foà to create a successful "Peer Gynt" (by Henrik Ibsen). With Luigi Squarzina in 1952 he co-founded and co-directed the "Teatro d'Arte Italiano", producing the first complete version of "Hamlet" in Italy, followed by rare works such as Seneca's "Thyestes" and Aeschylus's "The Persians".
In 1956 Gassman played the title role in a production of "Othello". He was so well received by his acting in the television series entitled "Il Mattatore" ("Spotlight Chaser") that "Il Mattatore" became the nickname that accompanied him for the rest of his life. Gassman's debut in the commedia all'italiana genre was rather accidental, in Mario Monicelli's "I soliti ignoti" ("Big Deal on Madonna Street", 1958). Famous movies featuring Gassman include: "Il sorpasso" (1962), "La Grande Guerra" (1962), "I mostri" (1963), "L'Armata Brancaleone" (1966), "Profumo di donna" (1974) and "C'eravamo tanto amati" (1974).
He directed "Adelchi", a lesser-known work by Alessandro Manzoni. Gassman brought this production to half a million spectators, crossing Italy with his "Teatro Popolare Itinerante" (a newer edition of the famous "Carro di Tespi"). His productions have included many of the famous authors and playwrights of the 20th century, with repeated returns to the classics of Shakespeare, Dostoyevsky and the Greek tragicians. He also founded a theatre school in Florence (Bottega Teatrale di Firenze), which educated many of the more talented actors of the current generation of Italian thespians.
In cinema, he worked frequently both in Italy and abroad. He met and fell in love with American actress Shelley Winters while she was touring Europe with fiancé Farley Granger. When Winters was forced to return to Hollywood to fulfill contractual obligations, he followed her there and married her. With his natural charisma and his fluency in English he scored a number of roles in Hollywood, including "Rhapsody" with Elizabeth Taylor and "The Glass Wall" before returning to Italy and the theatre. While rehearsing "Hamlet", he began an affair with Anna Maria Ferrero, his 16-year-old Ophelia, which ended his marriage to Winters. He and Winters were forced to work together on "Mambo" just as their marriage was unraveling, providing fodder for tabloids all over the world. He later voiced Mufasa in the Italian version of "The Lion King".
Personal life.
Gassman married three actresses: Nora Ricci (with whom he had Paola, an actress and wife of Ugo Pagliai); Shelley Winters (mother of his daughter Vittoria); and Diletta D'Andrea, by whom he had a son, Jacopo. In addition, he had an affair with actress Juliette Mayniel (mother of his son Alessandro, also an actor). In the 1990s he took part in the popular TV show "Tunnel" in which he very formally and "seriously"' recited documents such as utility bills, yellow pages and similar trivial texts, such as washing instructions for a wool sweater or cookies ingredients. He rendered them with the same professional skill that made him famous while reciting Dante's "Divine Comedy".
On 29 June 2000, Gassman died of a heart attack at his home in Rome, aged 77.
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="44093" url="http://en.wikipedia.org/wiki?curid=44093" title="Electroconvulsive therapy">
Electroconvulsive therapy

Electroconvulsive therapy (ECT), formerly known as electroshock therapy and often referred to as shock treatment, is a standard psychiatric treatment in which seizures are electrically induced in patients to provide relief from psychiatric illnesses.
ECT is often used with informed consent as a last line of intervention for major depressive disorder, mania and catatonia.
A round of ECT is effective for about 50% of people with treatment-resistant major depressive disorder, whether it is unipolar or bipolar. Followup treatment is still poorly studied, but about half of people who respond relapse within twelve months.
Aside from effects in the brain, the general physical risks of ECT are similar to those of brief general anesthesia.:259 Immediately following treatment, the most common adverse effects are confusion and memory loss. ECT is considered one of the least harmful treatment options available for severely depressed pregnant women.
A usual course of ECT involves multiple administrations, typically given two or three times per week until the patient is no longer suffering symptoms. ECT is administered under anesthetic with a muscle relaxant. Electroconvulsive therapy can differ in its application in three ways: electrode placement, frequency of treatments, and the electrical waveform of the stimulus. These three forms of application have significant differences in both adverse side effects and symptom remission. After treatment, drug therapy is usually continued, and some patients receive maintenance ECT. Administration (in the UK) is most commonly bilateral, in which the electrical current is passed across the whole brain. This seems to have greater efficacy, but also carries greater risk of memory loss. Less commonly (in the UK), ECT is administered unilaterally, which is less successful in producing the desired result, but carries a lower risk of memory loss. 
ECT appears to work in the short term via an anticonvulsant effect mostly in the frontal lobes, and longer term via neurotrophic effects primarily in the medial temporal lobe.
Medical use.
ECT is used with informed consent in treatment-resistant major depressive disorder, treatment-resistant catatonia, or prolonged or severe mania, and in conditions where "there is a need for rapid, definitive response because of the severity of a psychiatric or medical condition (e.g., when illness is characterized by stupor, marked psychomotor retardation, depressive delusions or hallucinations, or life–threatening physical exhaustion associated with mania)."
Major depressive disorder.
For major depressive disorder, ECT is generally used only when other treatments have failed, or in emergencies, such as imminent suicide. ECT has also been used in selected cases of depression occurring in the setting of multiple sclerosis, Parkinson's disease, Huntington's chorea, developmental delay, brain arteriovenous malformations and hydrocephalus.
Efficacy.
A meta-analysis done on the effectiveness of ECT in unipolar and bipolar depression was conducted in 2012. Findings showed that, although patients with unipolar depression and bipolar depression responded to other medical treatments very differently, both groups responded equally well to ECT. Overall remission rate for patients with unipolar depression to a round of ECT treatment was 51.5% and 50.9% in those with bipolar depression. The severity of each patient’s depression was assessed at the same baseline in each group.
There is little agreement on the most appropriate followup to ECT for people with major depressive disorder. When ECT is followed by treatment with antidepressants, about 50% of people relapsed by 12 months following successful initial treatment with ECT, with the about 37% relapsing within the first 6 months. About twice as many relapsed with no antidepressants. Most of the evidence for continuation therapy is with tricyclics; evidence for relapse prevention with newer antidepressants is lacking.
In 2008, a meta-analytic review paper found in terms of efficacy, "a significant superiority of ECT in all comparisons: ECT versus simulated ECT, ECT versus placebo, ECT versus antidepressants in general, ECT versus TCAs and ECT versus MAOIs."
In 2003, The UK ECT Review group published a systematic review and meta-analysis comparing ECT to placebo and antidepressant drugs. This meta-analysis demonstrated a large effect size (high efficacy relative to the mean in terms of the standard deviation) for ECT versus placebo, and versus antidepressant drugs.
Compared with transcranial magnetic stimulation for people with treatment-resistant major depressive disorder, ECT relieves depression about twice as well, reducing the score on the Hamilton Rating Scale for Depression by about 15 points, while TMS reduced it by 9 points.
Catatonia.
ECT is generally a second-line treatment for people with catatonia who don't respond to other treatments, but is a first-line treatment for severe or life-threatening catatonia. There is a lack of clinical evidence for its efficacy but "the excellent efficacy of ECT in catatonia is generally acknowledged". For people with autism spectrum disorders who have catatonia, there is little published evidence about the efficacy of ECT; as of 2014 there were twelve case reports, and while ECT had "life saving" efficacy in some, results were mixed and temporary, and maintenance ECT was necessary to sustain any benefit.
Mania.
ECT is used to treat people who have severe or prolonged mania; NICE recommends it only in life-threatening situations or when other treatments have failed. and as a second-line treatment for biopolar mania.
Schizophrenia.
ECT is rarely used in treatment-resistant schizophrenia, but is sometimes recommended for schizophrenia when short term global improvement is desired, or the subject shows little response to antipsychotics alone.
Administration.
ECT requires the informed consent of the patient.:1880
Whether psychiatric medications are terminated prior to treatment or maintained, varies.:1885 However, drugs that are known to cause toxicity in combination with ECT, such as lithium, are discontinued, and benzodiazepines, which increase seizure thresholds, are either discontinued, a benzodiazepine antagonist is administered at each ECT session, or the ECT treatment is adjusted accordingly.:1879:1875
The placement of electrodes, as well as the dose and duration of the stimulation is determined on a per-patient basis.:1881
In unilateral ECT, both electrodes are placed on the same side of the patient's head. Unilateral ECT may be used first to minimize side effects (memory loss). When electrodes are placed on both sides of the head, this is known as bilateral ECT. In bifrontal ECT, an uncommon variation, the electrode position is somewhere between bilateral and unilateral. Unilateral is thought to cause fewer cognitive effects than bilateral but is considered less effective if the dose administered is close to the seizure threshold.:1881 In the USA most patients receive bilateral ECT. In the UK almost all patients receive bilateral ECT.
The electrodes deliver an electrical stimulus. The stimulus levels recommended for ECT are in excess of an individual's seizure threshold: about one and a half times seizure threshold for bilateral ECT and up to 12 times for unilateral ECT.:1881 Below these levels treatment may not be effective in spite of a seizure, while doses massively above threshold level, especially with bilateral ECT, expose patients to the risk of more severe cognitive impairment without additional therapeutic gains. Seizure threshold is determined by trial and error ("dose titration"). Some psychiatrists use dose titration, some still use "fixed dose" (that is, all patients are given the same dose) and others compromise by roughly estimating a patient's threshold according to age and sex. Older men tend to have higher thresholds than younger women, but it is not a hard and fast rule, and other factors, for example drugs, affect seizure threshold.
Immediately prior to treatment, a patient is given a short-acting anesthetic such as methohexital, etomidate, or thiopental, a muscle relaxant such as suxamethonium (succinylcholine), and occasionally atropine to inhibit salivation.:1882 In a minority of countries such as Japan, India, and Nigeria, ECT may be used without anesthesia. The Union Health Ministry of India recommended a ban on ECT without anesthesia in India's Mental Health Care Bill of 2010 and the Mental Health Care Bill of 2013. Some psychiatrists in India argued against the ban on unmodified ECT due to a lack of trained anesthesiologists available to administer ECT with anesthesia. The practice was abolished in Turkey's largest psychiatric hospital in 2008.
The patient's EEG, ECG, and blood oxygen levels are monitored during treatment.:1882
ECT is usually administered three times a week, on alternate days, over a course of two to four weeks.:1882–1883
ECT devices.
Most modern ECT devices deliver a brief-pulse current, which is thought to cause fewer cognitive effects than the sine-wave currents which were originally used in ECT. A small minority of psychiatrists in the USA still use sine-wave stimuli. Sine-wave is no longer used in the UK or Ireland.
Typically, the electrical stimulus used in ECT is about 800 milliamps and has up to several hundred watts, and the current flows for between one and 6 seconds.
In the USA, ECT devices are manufactured by two companies, Somatics, which is owned by psychiatrists Richard Abrams and Conrad Swartz, and Mecta. In the UK, the market for ECT devices was long monopolized by Ectron Ltd, which was set up by psychiatrist Robert Russell.
Adverse effects.
Aside from effects in the brain, the general physical risks of ECT are similar to those of brief general anesthesia; the U.S. Surgeon General's report says that there are "no absolute health contraindications" to its use.:259 Immediately following treatment, the most common adverse effects are confusion and memory loss. It must be used very cautiously in people with epilepsy or other neurological disorders because by its nature it provokes small tonic-clonic seizures, and so would likely not be given to a person whose epilepsy is not well controlled. Some patients experience muscle soreness after ECT. This is due to the muscle relaxants given during the procedure and rarely due to muscle activity. ECT, especially if combined with deep sleep therapy, may lead to brain damage if administered in such a way as to lead to hypoxia or anoxia in the patient.
The death rate due to ECT is around 4 per 100,000 procedures. There is evidence and rationale to support giving low doses of benzodiazepines or else low doses of general anesthetics which induce sedation but not anesthesia to patients to reduce adverse effects of ECT.
While there are no absolute contraindications for ECT, there is increased risk for patients who have unstable or severe cardiovascular conditions or aneurysms; who have recently had a stroke; who have increased intracranial pressure (for instance, due to a solid brain tumor), or who have severe pulmonary conditions, or who are generally at high risk for receiving anesthesia.:30
Effects on memory.
Retrograde amnesia occurs to some extent in almost all ECT recipients. The American Psychiatric Association report (2001) acknowledges: “In some patients the recovery from retrograde amnesia will be incomplete, and evidence has shown that ECT can result in persistent or permanent memory loss”. It is the purported effects of ECT on long-term memory that give rise to much of the concern surrounding its use.
However, the methods used to measure memory loss are generally poor, and their application to people with depression, who have cognitive deficits including problems with memory, have been problematic.
The acute effects of ECT can include amnesia, both retrograde (for events occurring before the treatment) and anterograde (for events occurring after the treatment). Memory loss and confusion are more pronounced with bilateral electrode placement rather than unilateral, and with outdated sine-wave rather than brief-pulse currents. The use of either constant or pulsing electrical impulses also varied the memory loss results in patients. Patients who received pulsing electrical impulses as opposed to a steady flow seemed to incur less memory loss. The vast majority of modern treatment uses brief pulse currents.
Retrograde amnesia is most marked for events occurring in the weeks or months before treatment, with one study showing that although some people lose memories from years prior to treatment, recovery of such memories was "virtually complete" by seven months post-treatment, with the only enduring loss being memories in the weeks and months prior to the treatment. Anterograde memory loss is usually limited to the time of treatment itself or shortly afterwards. In the weeks and months following ECT these memory problems gradually improve, but some people have persistent losses, especially with bilateral ECT. One published review summarizing the results of questionnaires about subjective memory loss found that between 29% and 55% of respondents believed they experienced long-lasting or permanent memory changes. In 2000, American psychiatrist Sarah Lisanby and colleagues found that bilateral ECT left patients with more persistently impaired memory of public events as compared to RUL ECT.
Effects on brain structure.
Considerable controversy exists over the effects of ECT on brain tissue, although a number of mental health associations — including the American Psychiatric Association — have concluded that there is no evidence that ECT causes structural brain damage. A 1999 report by the U.S. Surgeon General states, "The fears that ECT causes gross structural brain pathology have not been supported by decades of methodologically sound research in both humans and animals".
Many expert proponents of ECT maintain that the procedure is safe and does not cause brain damage. Dr. Charles Kellner, a prominent ECT researcher and former chief editor of the "Journal of ECT", stated in a 2007 interview that, "There are a number of well-designed studies that show ECT does not cause brain damage and numerous reports of patients who have received a large number of treatments over their lifetime and have suffered no significant problems due to ECT." Dr. Kellner cites a study purporting to show an absence of cognitive impairment in eight subjects after more than 100 lifetime ECT treatments. Dr. Kellner stated "Rather than cause brain damage, there is evidence that ECT may reverse some of the damaging effects of serious psychiatric illness."
Effects in pregnancy.
If steps are taken to decrease potential risks, ECT is generally accepted to be relatively safe during all trimesters of pregnancy, particularly when compared to pharmacological treatments. Suggested preparation for ECT during pregnancy includes a pelvic examination, discontinuation of nonessential anticholinergic medication, uterine tocodynamometry, intravenous hydration, and administration of a nonparticulate antacid. During ECT, elevation of the pregnant woman's right hip, external fetal cardiac monitoring, intubation, and avoidance of excessive hyperventilation are recommended.
Mechanism of action.
Despite decades of research, the exact mechanism of action of ECT remains elusive. Neuroimaging studies in people who have had ECT, investigating differences between responders and nonresponders, and people who relapse, find that responders have anticonvulsant effects mostly in the frontal lobes, which corresponds to immediate responses, and neurotrophic effects primarily in the medial temporal lobe. The anticonvulsant effects are decreased blood flow and decreased metabolism, while the neurotrophic effects are opposite - increased perfusion and metabolism, as well as increased volume of the hippocampus.
Usage.
As of 2001, it was estimated that about one million people received ECT annually.
There is wide variation in ECT use between different countries, different hospitals, and different psychiatrists. International practice varies considerably from widespread use of the therapy in many western countries to a small minority of countries that do not use ECT at all, such as Slovenia.
About 70 percent of ECT patients are women. This may be due to the fact that women are more likely to be diagnosed with depression. Older and more affluent patients are also more likely to receive ECT. The use of ECT is not as common in ethnic minorities.
Sarah Hall reports, "ECT has been dogged by conflict between psychiatrists who swear by it, and some patients and families of patients who say that their lives have been ruined by it. It is controversial in some European countries such as the Netherlands and Italy, where its use is severely restricted".
United States.
ECT became popular in the United States in the 1940s. At this time psychiatric hospitals were overrun with patients whom doctors were desperate to treat and cure. The practices of ECT and lobotomies became popular because they held some promise of addressing the overpopulation problem. Whereas lobotomies would reduce a patient to a more manageable submissive state ECT helped to improve mood in those with severe depression. In the United States, a survey of psychiatric practice in the late 1980s found that an estimated 100,000 people received ECT annually, with wide variation between metropolitan statistical areas.
Accurate statistics about the frequency, context and circumstances of ECT in the United States are difficult to obtain because only a few states have reporting laws that require the treating facility to supply state authorities with this information. In 13 of the 50 states, the practice of ECT is regulated by law.
One state which does report such data is Texas, where in the mid-1990s ECT was used in about one third of psychiatric facilities and given to about 1,650 people annually.
Usage of ECT has since declined slightly; in 2000–01 ECT was given to about 1500 people aged from 16 to 97 (in Texas it is illegal to give ECT to anyone under sixteen). ECT is more commonly used in private psychiatric hospitals than in public hospitals, and minority patients are underrepresented in the ECT statistics.
In the United States, ECT is usually given three times a week; in the UK, it is usually given twice a week. Occasionally it is given on a daily basis. A course usually consists of 6–12 treatments, but may be more or fewer. Following a course of ECT some patients may be given continuation or maintenance ECT with further treatments at weekly, fortnightly or monthly intervals. A few psychiatrists in the USA use multiple-monitored ECT (MMECT) where patients receive more than one treatment per anesthetic. Electroconvulsive therapy is not a required subject in US medical schools and not a required skill in psychiatric residency training. Privileging for ECT practice at institutions is a local option: no national certification standards are established, and no ECT-specific continuing training experiences are required of ECT practitioners.
United Kingdom.
In the United Kingdom in 1980, an estimated 50,000 people received ECT annually, with use declining steadily since then to about 12,000 per annum in 2002. It is still used in nearly all psychiatric hospitals, with a survey of ECT use from 2002 finding that 71 percent of patients were women and 46 percent were over 65 years of age. Eighty-one percent had a diagnosis of mood disorder; schizophrenia was the next most common diagnosis. Sixteen percent were treated without their consent. In 2003, the National Institute for Health and Care Excellence, a government body which was set up to standardize treatment throughout the National Health Service in England and Wales, issued guidance on the use of ECT. Its use was recommended "only to achieve rapid and short-term improvement of severe symptoms after an adequate trial of treatment options has proven ineffective and/or when the condition is considered to be potentially life-threatening in individuals with severe depressive illness, catatonia or a prolonged manic episode".
The guidance received a mixed reception. It was welcomed by an editorial in the British Medical Journal but the Royal College of Psychiatrists launched an unsuccessful appeal. The NICE guidance, as the British Medical Journal editorial points out, is only a policy statement and psychiatrists may deviate from it if they see fit. Adherence to standards has not been universal in the past. A survey of ECT use in 1980 found that more than half of ECT clinics failed to meet minimum standards set by the Royal College of Psychiatrists, with a later survey in 1998 finding that minimum standards were largely adhered to, but that two-thirds of clinics still fell short of current guidelines, particularly in the training and supervision of junior doctors involved in the procedure. A voluntary accreditation scheme, ECTAS, was set up in 2004 by the Royal College, but as of 2006 only a minority of ECT clinics in England, Wales, Northern Ireland and the Republic of Ireland have signed up.
China.
ECT was introduced in China in the early 1950s and while it was originally practiced without anesthesia, as of 2012 almost all procedures were conducted with it. As of 2012, there are approximately 400 ECT machines in China, and 150,000 ECT treatments are performed each year. Chinese national practice guidelines recommend ECT for the treatment of schizophrenia, depressive disorders, and bipolar disorder and in the Chinese literature, ECT is an effective treatment for schizophrenia and mood disorders.
History.
As early as the 16th century, agents to induce seizures were used to treat psychiatric conditions. In 1785, the therapeutic use of seizure induction was documented in the "London Medical Journal". As to its earliest antecedents one doctor claims 1744 as the dawn of electricity's therapeutic use, as documented in the first issue of "Electricity and Medicine". Treatment and cure of hysterical blindness was documented eleven years later. Benjamin Franklin wrote an electro static machine cured "a woman of hysterical fits." G.B.C. Duchenne, the mid-19th century "Father of Electrotherapy," said its use was integral to a neurological practice.
In the second half of the nineteenth century, such efforts were frequent enough in British asylums as to make it notable.
Convulsive therapy was introduced in 1934 by Hungarian neuropsychiatrist Ladislas J. Meduna who, believing mistakenly that schizophrenia and epilepsy were antagonistic disorders, induced seizures first with camphor and then metrazol (cardiazol). Ladislas Meduna is thought to be the father of convulsive therapy. In 1937, the first international meeting on convulsive therapy was held in Switzerland by the Swiss psychiatrist Muller. The proceedings were published in the "American Journal of Psychiatry" and, within three years, cardiazol convulsive therapy was being used worldwide. Italian Professor of neuropsychiatry Ugo Cerletti, who had been using electric shocks to produce seizures in animal experiments, and his colleague Lucio Bini developed the idea of using electricity as a substitute for metrazol in convulsive therapy and, in 1937, experimented for the first time on a person. It was known early on that inducing convulsions aided in helping those with severe schizophrenia. Cerletti had noted a shock to the head produced convulsions in dogs. The idea to use electroshock on humans came to Cerletti when he saw how pigs were given an electric shock before being butchered to put them in an anesthetized state. Cerletti and Bini practiced until they felt they had the right parameters needed to have a successful human trial. Once they started trials on patients they found that after 10-20 treatments the results were significant. Patients had much improved. A positive side effect to the treatment was retrograde amnesia. It was because of this side effect that patients could not remember the treatments and had no ill feelings toward it.
ECT soon replaced metrazol therapy all over the world because it was cheaper, less frightening and more convenient. Cerletti and Bini were nominated for a Nobel Prize but did not receive one. By 1940, the procedure was introduced to both England and the US. In Germany and Austria it was promoted by Friedrich Meggendorfer. Through the 1940s and 1950s, the use of ECT became widespread.
In the early 1940s, in an attempt to reduce the memory disturbance and confusion associated with treatment, two modifications were introduced: the use of unilateral electrode placement and the replacement of sinusoidal current with brief pulse. It took many years for brief-pulse equipment to be widely adopted. In the 1940s and early 1950s ECT was usually given in "unmodified" form, without muscle relaxants, and the seizure resulted in a full-scale convulsion. A rare but serious complication of unmodified ECT was fracture or dislocation of the long bones. In the 1940s psychiatrists began to experiment with curare, the muscle-paralysing South American poison, in order to modify the convulsions. The introduction of suxamethonium (succinylcholine), a safer synthetic alternative to curare, in 1951 led to the more widespread use of "modified" ECT. A short-acting anesthetic was usually given in addition to the muscle relaxant in order to spare patients the terrifying feeling of suffocation that can be experienced with muscle relaxants.
The steady growth of antidepressant use along with negative depictions of ECT in the mass media led to a marked decline in the use of ECT during the 1950s to the 1970s. The Surgeon General stated there were problems with electroshock therapy in the initial years before anesthesia was routinely given, and that "these now-antiquated practices contributed to the negative portrayal of ECT in the popular media." The New York Times described the public's negative perception of ECT as being caused mainly by one movie. "For Big Nurse in "One Flew Over the Cuckoo's Nest," it was a tool of terror, and, in the public mind, "shock therapy" has retained the tarnished image given it by Ken Kesey's novel: dangerous, inhumane and overused".
In 1976, Dr. Blatchley demonstrated the effectiveness of his constant current, brief pulse device ECT. This device eventually largely replaced earlier devices because of the reduction in cognitive side effects, although as of 2012 some ECT clinics still were using sine-wave devices. The 1970s saw the publication of the first American Psychiatric Association (APA) task force report on electroconvulsive therapy (to be followed by further reports in 1990 and 2001). The report endorsed the use of ECT in the treatment of depression. The decade also saw criticism of ECT. Specifically critics pointed to shortcomings such as noted side effects, the procedure being used as a form of abuse, and uneven application of ECT. The use of ECT declined until the 1980s, "when use began to increase amid growing awareness of its benefits and cost-effectiveness for treating severe depression". In 1985 the National Institute of Mental Health and National Institutes of Health convened a consensus development conference on ECT and concluded that, while ECT was the most controversial treatment in psychiatry and had significant side-effects, it had been shown to be effective for a narrow range of severe psychiatric disorders.
Because of the backlash noted previously, national institutions reviewed past practices and set new standards. In 1978, The American Psychiatric Association released its first task force report in which new standards for consent were introduced and the use of unilateral electrode placement was recommended. The 1985 NIMH Consensus Conference confirmed the therapeutic role of ECT in certain circumstances. The American Psychiatric Association released its second task force report in 1990 where specific details on the delivery, education, and training of ECT were documented. Finally in 2001 the American Psychiatric Association released its latest task force report. This report emphasizes the importance of informed consent, and the expanded role that the procedure has in modern medicine.
Society and culture.
Surveys of public opinion, the testimony of former patients, legal restrictions on its use and disputes as to the efficacy, ethics and adverse effects of ECT within the psychiatric and wider medical community indicate that the use of ECT remains controversial. This is reflected in the recent vote by the United States Food and Drug Administration's (FDA's) Neurological Devices Advisory Panel to recommend that FDA maintain ECT devices in the Class III device category for high risk devices except for patients suffering from catatonia. This may result in the manufacturers of such devices having to do controlled trials on their safety and efficacy for the first time. In justifying their position, panelists referred to the memory loss associated with ECT and the lack of long-term data.
Legal status.
Informed consent.
The World Health Organization (2005) advises that ECT should be used only with the informed consent of the patient (or their guardian if their incapacity to consent has been established).
In the US, this doctrine places a legal obligation on a doctor to make a patient aware of: the reason for treatment, the risks and benefits of a proposed treatment, the risks and benefits of alternative treatment, and the risks and benefits of receiving no treatment. The patient is then given the opportunity to accept or reject the treatment. The form states how many treatments are recommended and also makes the patient aware that consent may be revoked and treatment discontinued at anytime during a course of ECT. The Surgeon General's Report on Mental Health states that patients should be warned that the benefits of ECT are short-lived without active continuation treatment in the form of drugs or further ECT, and that there may be some risk of permanent, severe memory loss after ECT. The report advises psychiatrists to involve patients in discussion, possibly with the aid of leaflets or videos, both before and during a course of ECT.
To demonstrate what he believes should be required to fully satisfy the legal obligation for informed consent, one psychiatrist, working for an anti-psychiatry organisation, has formulated his own consent form using the consent form developed and enacted by the Texas Legislature as a model.
According to the Surgeon General, involuntary treatment is uncommon in the United States and is typically used only in cases of great extremity, and only when all other treatment options have been exhausted. The use of ECT is believed to be a potentially life-saving treatment.
In one of the few jurisdictions where recent statistics on ECT usage are available, a national audit of ECT by the Scottish ECT Accreditation Network indicated that 77% of patients who received the treatment in 2008 were capable of giving informed consent.
In the UK, in order for consent to be valid it requires an explanation in "broad terms" of the nature of the procedure and its likely effects. One review from 2005 found that only about half of patients felt they were given sufficient information about ECT and its adverse effects and another survey found that about fifty percent of psychiatrists and nurses agreed with them.
A 2005 study published in the "British Journal of Psychiatry" described patients' perspectives on the adequacy of informed consent before ECT. The study found that, "About half (45–55%) of patients reported they were given an adequate explanation of ECT, implying a similar percentage felt they were not." The authors also stated:
"Approximately a third did not feel they had freely consented to ECT even when they had signed a consent form. The proportion who feel they did not freely choose the treatment has actually increased over time. The same themes arise whether the patient had received treatment a year ago or 30 years ago. Neither current nor proposed safeguards for patients are sufficient to ensure informed consent with respect to ECT, at least in England and Wales."
Involuntary ECT.
Procedures for involuntary ECT vary from country to country depending on local mental health laws.
United States.
In the United States, ECT devices came into existence prior to medical devices being regulated by the Food and Drug Administration; when the law came into effect the FDA was obligated to retrospectively review already existing devices and classify them, and determine whether clinical trials were needed to prove efficacy and safety. While the FDA has classified the devices used to administer ECT as , as of 2011 the FDA had not yet determined whether the devices should be withdrawn from the market until clinical trials prove their safety and efficacy.:5 The FDA considers ECT machinery to be experimental devices.
In most states in the USA, a judicial order following a formal hearing is needed before a patient can be forced to undergo involuntary ECT. However, ECT can also be involuntarily administered in situations with less immediate danger. Suicidal intent is a common justification for its involuntary use, especially when other treatments are ineffective.
United Kingdom.
Until 2009 in England and Wales, the Mental Health Act 1983 allowed the use of ECT on detained patients whether or not they had capacity to consent to it. However, following amendments which took effect in 2009, ECT may not generally be given to a patient who has capacity and refuses it, irrespective of his or her detention under the Act. In fact, even if a patient is deemed to lack capacity, if they made a valid advance decision refusing ECT then they should not be given it; and even if they do not have an advance decision, the psychiatrist must obtain an independent second opinion (which is also the case if the patient is under age of consent). However, there is an exception regardless of consent and capacity; under Section 62 of the Act, if the treating psychiatrist says the need for treatment is urgent they may start a course of ECT without authorization. About 2,000 people a year in England and Wales are treated without their consent under the Mental Health Act. Concerns have been raised by the official regulator that psychiatrists are too readily assuming that patients have the capacity to consent to their treatments, and that there is a worrying lack of independent advocacy. In Scotland the Mental Health (Care and Treatment) (Scotland) Act 2003 also gives patients with capacity the right to refuse ECT.
Public perception and mass media.
A questionnaire survey of 379 members of the general public in Australia indicated that more than 60% of respondents had some knowledge about the main aspects of ECT. Participants were generally opposed to the use of ECT on depressed individuals with psychosocial issues, on children, and on involuntary patients. Public perceptions of ECT were found to be mainly negative.
Famous cases.
Ernest Hemingway, American author, committed suicide shortly after ECT at the Mayo Clinic in 1961. He is reported to have said to his biographer, "Well, what is the sense of ruining my head and erasing my memory, which is my capital, and putting me out of business? It was a brilliant cure but we lost the patient..." American surgeon and award-winning author Sherwin B. Nuland is another notable person who has undergone ECT. In his 40s, this successful surgeon's depression became so severe that he had to be institutionalized. After exhausting all treatment options, a young resident assigned to his case suggested ECT, which ended up being successful.
Fictional examples.
Electroconvulsive therapy has been depicted in fiction and works based on true experiences. These include Sylvia Plath's autobiographical novel, "The Bell Jar".

</doc>
<doc id="44095" url="http://en.wikipedia.org/wiki?curid=44095" title="Sui generis">
Sui generis

Sui generis (; ]) is a Latin phrase, meaning "of its (his, her, or their) own kind; in a class by itself; unique".
The term is widely used to refer to more esoteric entities in a number of disciplines, including
Philosophy.
The expression is often used in analytic philosophy to indicate an idea, an entity, or a reality which cannot be reduced to a lower concept or included in a wider concept.
Biology.
In the taxonomical structure "genus → species", a species is described as "sui generis" if its genus was created to classify it (i.e., its uniqueness at the time of classification merited the creation of a new genus, the sole member of which was initially the "sui generis" species). A species that is the sole "extant" member of its genus (e.g. the "Homo" genus) is not necessarily "sui generis": extinction may have eliminated other species of that genus.
Law.
In law, it is a term of art used to identify a legal classification that exists independently of other categorizations because of its singularity or due to the specific creation of an entitlement or obligation. For example, a court's contempt powers arise" sui generis" and not from statute or rule. The New York Court of Appeals has used the term in describing cooperative apartment corporations, mostly because this form of housing is considered real property for some purposes and personal property for other purposes.
When citing cases and other authorities, lawyers and judges may refer to "a "sui generis" case", or "a "sui generis" authority", meaning it is a special one confined to "its own" facts, and therefore may not be of broader application.
Statutory.
In statutory interpretation, it refers to the problem of giving meaning to groups of words where one of the words is ambiguous or inherently unclear. For example, in criminal law, a statute might require a "mens rea" element of "unlawful and malicious" intent. Whereas the word "malicious" is well-understood, the word "unlawful" in this context is less clear. Hence, it must be given a meaning of the "same kind" as the word of established meaning.
Two or more words are connected by "and", as opposed to "or" are particularly susceptible to vagueness. The interpretation of the two or more words might therefore have a "sui generis" difference depending on the circumstances. Courts sometimes have to attribute a conjunctive (X 'and' Y) intention to the legislature even though the list is disjunctive (X 'or' Y) because, otherwise, no overall interpretation of the law in question would make sense.
Town planning.
In British town planning law, such as the Town and Country Planning (Use Classes) Order 1987, many common types of land use are classified in "use classes". Change of use of land "within" a use class does not require planning permission; however, changing "between" use classes might require planning permission, but not if the use class is "sui generis".
Examples of "sui generis" transference include embassies, theatres, amusement arcades, laundrettes, taxi or vehicle hire businesses, petrol filling stations, scrapyards, nightclubs, motor car showrooms, retail warehouses, clubs and hostels.
The grant of private hire vehicle (taxicab) operators licences by local authorities frequently has a condition attached that the appropriate "sui generis" change of use planning permission is granted to those premises to ensure those businesses cannot trade lawfully without the appropriate planning consents. 
Even qualified and experienced town planners misconceive that changing use from an existing use class to one which is "sui generis" always requires planning permission; it does not because the property is not transferred "between" two existing use classes. Permission is only required if the "sui generis" use is materially different from the existing one, such as from a petrol station where petrol tanks might have leaked. As in other applications of the phrase "sui generis", the decisions will be a unique matter of fact, degree, and professional opinion.
Aboriginal law and education.
The motto "Sui Generis" has been adopted by the Akitsiraq Law School because it is a "sui generis" (aboriginal) title in all of Canadian aboriginal law institutes by dint of its title being Inuktitut, the Aboriginal language of the Inuit in the far north of Canada. More importantly, in aboriginal professional legal education, the work of Aboriginal people to define and create contemporary aboriginal education is a "thing of its own kind" having "sui generis" admissions and "sui generis" curriculum.
Intellectual property law.
Generally speaking, protection for intellectual property extends to intellectual creations in order to incentivize innovation, and depends upon the nature of the work and its "characteristics". The main types of intellectual property law are: copyright, which protects creative works; patent, which protects invention; trade secret, which protects information not generally known or readily ascertainable that is valuable to the secret holder; and trademark, which protects branding and other exclusive properties of products and services. Any matter that meets such criteria is protected.
However, "sui generis" statutes exist in many countries that extend intellectual property protection to matter that does not meet characteristic definitions: integrated circuit layouts, ship hull designs, fashion designs in France, databases, or plant varieties require "sui generis" statutes because of their unique characteristics. The United States, Japan, and many EU countries protect the topography of semiconductor chips and integrated circuits under "sui generis" laws, which borrow some aspects from patent or copyright law. In the U.S. this "sui generis" law is known as the Semiconductor Chip Protection Act of 1984.
Politics and society.
In political philosophy, the unparalleled development of the European Union as compared to other international organizations has led to its designation as a "sui generis" geopolitical entity. The legal nature of the EU is widely debated because its mixture of intergovernmental and supranational elements cause it to share characteristics with confederal and federal entities. Compared to other international organizations, the EU is often considered "sui generis" because its legal system comprehensively rejects any use of retaliatory sanctions by one member state against another.
A similar case which has led to the use of the label "sui generis" is the unique relationship between France and New Caledonia because the legal status of New Caledonia can aptly be said to lie "somewhere between an overseas collectivity and a sovereign nation"; although other examples of such a status for other disputed or dependent territories may exist, this arrangement is certainly unique within the French Republic.
In local government, a "sui generis" entity is one which does not fit with the general scheme of local governance of a country. For example in England, the City of London and the Isles of Scilly are the two "sui generis" localities, as their forms of local government are both (for historical or geographical reasons) very different from those of elsewhere in the country. Therefore "The City of London and the Isles of Scilly" are said to be "sui generis authorities, pre-dating recent reforms of local government." The Joint Council of Municipalities of Croatia is a "sui generis" council of municipalities to Croatians because it formed after international agreement and therefore has no similar example in the rest of the country. The legal status of the Holy See has been described as a "sui generis" entity possessing an international personality.
Creative arts.
A book, movie, television series, or other artistic creation is said to be "sui generis" when it does not fit into standard genre boundaries. Movie critic Richard Schickel identifies Joe Versus the Volcano as a "sui generis" movie. Much of Joss Whedon's work is also "sui generis", to the point where his name has become adjectival in describing his work. The video game "Killer7" is often referred to as a game without genre, due to its combination of on-rails movement, first-person aiming and puzzle solving.

</doc>
<doc id="44096" url="http://en.wikipedia.org/wiki?curid=44096" title="Hans von Bodeck">
Hans von Bodeck

Hans von Bodeck (1582–1658) was a German diplomat and chancellor of the Hohenzollern Prince-electors of Brandenburg-Prussia.
Bodeck came from a prominent patrician family of Elbing (Elbląg) in the Polish province of Royal Prussia. His grandfather was the burgomaster, while his father was a city councilman. His ancestor Johann III von Bodeck (1542–1595) received imperial status from Emperor Rudolf II and was allowed to improve the family's coat of arms. The family also held offices in Danzig (Gdańsk).
In order to find a trading partner Bodeck was sent on a diplomatic mission from Elbing throughout Europe. During that time he wrote "liber amicorum", which is now studied by musicologists. During his diplomatic tour Bodeck visited the Netherlands, France, Switzerland, and England, where he attended the Universities of Oxford and Cambridge. He attended the funeral ceremony for Queen Elizabeth I of England and the coronation of the new English king, James I. The council of Elbing had sent two delegates with dual missions: firstly, to pay its respects to the new king and secondly to oppose the transfer of English trade from Elbing to nearby Danzig.
In 1604 Bodeck left for London and met John Dowland, Philip Rosseter, and Thomas Campion. All three composers of lute songs lived in the same district of London. Bodeck befriended them, and Campion wrote a song dedicated to Bodeck. Many people from England and Scotland came to live in Elbling.
Later that year Bodeck left for Paris and met Count Christopher von Dohna, a nobleman of Prussia, who lived 15 km from Elbing. Bodeck then became the chancellor to Elector Joachim Frederick of Brandenburg. He died in 1658.
A collection of pieces for lute was purchased by Dohna and kept at the Elbing library. In 1929 Hans Bauer wrote a full description of the register of Bodeck. During the capture of Elbing by the Soviet Red Army in 1945 during World War II and the subsequent expulsion of the city's German populace, the library was destroyed. Many Prussian documents and original manuscripts have since been discovered in Kraków, leading music researchers to hope that some of Bodeck's works might resurface.

</doc>
<doc id="44099" url="http://en.wikipedia.org/wiki?curid=44099" title="HMS Antelope">
HMS Antelope

Twelve ships of the Royal Navy have been named HMS "Antelope", after the Antelope:

</doc>
<doc id="44101" url="http://en.wikipedia.org/wiki?curid=44101" title="Satellite state">
Satellite state

The political term satellite state designates a country that is formally independent in the world, but under heavy political, economic and military influence or control from another country. The term was coined by analogy to planetary objects orbiting a larger object, such as smaller moons revolving around larger planets, and is used mainly to refer to Central and Eastern European countries of the Warsaw Pact during the Cold War or to Mongolia between 1924 and 1990, for example. As used for Central and Eastern European countries it implies that the countries in question were "satellites" under the hegemony of the Soviet Union. In some contexts it also refers to other countries in the Soviet sphere of influence during the Cold War—such as North Korea (especially in the years surrounding the Korean War of 1950–1953) and Cuba (particularly after it joined the Comecon in 1972). In Western usage, the term has seldom been applied to states other than those in the Soviet orbit. In Soviet usage, the term applied to the states in the orbit of Nazi Germany and Fascist Italy.
The Oxford English Dictionary traces the use of the phrase "satellite state" in English back at least as far as 1916.
In times of war or political tension, satellite states sometimes serve as buffers between an enemy country and the nation exerting control over the satellites. "Satellite state" is one of several contentious terms used to describe the (alleged) subordination of one state to another. Other such terms include puppet state and neo-colony. In general, the term "satellite state" implies deep ideological and military allegiance to the hegemonic power, whereas "puppet state" implies political and military dependence, and "neo-colony" implies (often abject) economic dependence. Depending on which aspect of dependence is being emphasised, a state may fall into more than one category.
Soviet satellite states.
Post World War I.
When the Mongolian Revolution of 1921 broke out, Mongolian revolutionaries expelled Russian White Guards (during the Russian Civil War of 1917–1923 following the Communist October Revolution of 1917) from Mongolia, which became independent when the Qing Empire of China collapsed in 1911, with the assistance of the Soviet Red Army. The revolution also officially ended Manchurian sovereignty over Mongolia, which had existed since 1691. Although the theocratic Bogd Khaanate of Mongolia still nominally continued, with successive series of violent struggles, Soviet influence got ever stronger, and after the death of the Bogd Khaan ("Great Khan", or "Emperor"), the Mongolian People's Republic was proclaimed on November 26, 1924. Being a sovereign and nominally independent country, it has been described for the years 1924–1990 as a satellite state of the Soviet Union.
During the Russian Civil War, the Soviet Red Army troops took Tuva in January 1920, which was also part of the Qing Empire of China and a protectorate of Imperial Russia. The Tuvan People's Republic, was proclaimed independent in 1921 and was as a satellite state of Soviet Union until its annexation in 1944 by the Soviet Union.
Another early Soviet satellite state in Asia was the short-lived Far East Republic in Siberia.
Post World War II.
At the end of World War II, most eastern and central European countries were occupied by the Soviet Union, and along with the USSR made up what is sometimes called the Soviet Empire. The Soviets remained in these countries after the war's end. Through a series of coalition governments including Communist parties, and then a forced liquidation of coalition members unliked by the Soviets, Stalinist systems were established in each country. Stalinists gained control of existing governments, police, press and radio outlets in these countries. Soviet satellite states in Europe included:
The Federal People's Republic of Yugoslavia is sometimes referred to as a Soviet satellite, though it broke from the Soviet Union in the 1948 Tito-Stalin split, with the Cominform offices being moved from Belgrade to Bucharest, and subsequently initiated formation of the Non-Aligned Movement. The People's Republic of Albania, under the leadership of Stalinist Enver Hoxha, broke ties with the Soviet Union in 1960 following the Soviet de-Stalinization process. These countries were, at least between 1945 and 1948, all members of the Eastern Bloc.
The Democratic Republic of Afghanistan can also be considered a Soviet satellite; from 1978 until 1991 the central government in Kabul was aligned with the Communist bloc, and was directly supported by Soviet military power between 1979 and 1989. The short-lived East Turkestan Republic (1944–1950) was a Soviet satellite until it was absorbed into the People's Republic of China along with the rest of Xinjiang.
Post-Cold War use of the term.
Commentators have sometimes expressed concern that United States military and diplomatic interventions in the Middle East and elsewhere might lead, or perhaps has already led, to the existence of American satellite states. William Pfaff has warned that a permanent American presence in Iraq would "turn Iraq into an American satellite state." The term has also been used to describe the relationship between Lebanon and Syria, as Syria has been accused of intervening in Lebanese political affairs.

</doc>
<doc id="44103" url="http://en.wikipedia.org/wiki?curid=44103" title="Tribute in Light">
Tribute in Light

The Tribute in Light is an art installation of 88 searchlights placed next to the site of the World Trade Center to create two vertical columns of light in remembrance of the September 11 attacks. It is produced annually by the Municipal Art Society of New York. 
The two beams cost approximately $1,858.56 (assuming $0.11 kWh) to run for 24 hours. There are 88 spotlights (44 for each tower) which each consume 8,000 watts.
Overview.
The Tribute in Light initially ran as a temporary installation from March 11 to April 14, 2002, and was launched again in 2003 to mark the second anniversary of the attack. s of 2013[ [update]], it has been repeated every year on September 11. It had been announced that 2008 would be its final year, but the tribute was continued in 2009. 
On December 17, 2009, it was confirmed that the tribute would continue through to the tenth anniversary of the attacks in 2011, but continued again in 2012. As of July 23, 2012, plans are underway for the National September 11 Memorial & Museum to assume the lease for the MTA property used during this tribute, and to begin transitioning operation of the tribute from the Municipal Art Society to the memorial foundation.
Development.
Those working on the project came up with the concept in the week following the attack. On September 13, 2001, the idea was presented to executives of Consolidated Edison, the electric utility company serving New York City, by John Englehart, then president of the brand innovation firm Arnell Group Architects John Bennett and Gustavo Bonevardi of PROUN Space Studio distributed their "Project for the Immediate Reconstruction of Manhattan's Skyline". Artists Julian LaVerdiere and Paul Myoda, who before September 11 were working on the 91st floor of the World Trade Center north tower on a proposed light sculpture on the giant radio antenna with Creative Time, conceived of a project called "Phantom Towers". They were commissioned by "The New York Times Magazine" to create an image of the project for its September 23 cover.
Richard Nash Gould, a New York architect, presented the concept to the Municipal Art Society. On September 19, Municipal Art Society chairman Philip K. Howard wrote to Mayor Rudy Giuliani, asking him "to consider placing two large searchlights near the disaster site, projecting their light straight up into the sky."
After some consideration it was decided to contact lighting experts in the field of high intensity light displays. An Italian searchlight company named Space Cannon was chosen to help design the installation and to supply the 88 special fixtures that would be needed.
The project was originally going to be named "Towers of Light", but the victims' families felt that the name emphasized the buildings destroyed instead of the people killed.
On clear nights, the lights could be seen from over 60 miles away, visible in all of New York City and most of suburban Northern New Jersey and Long Island, Fairfield County, Connecticut, Westchester County, Orange County, New York and Rockland County, New York. The beams were clearly visible from the terrace at Century Country Club in Purchase, New York, from at least as far west as western Morris County, in Flanders, New Jersey, at least as far as the barrier beach of Fire Island in Suffolk County, New York on Long Island, and as far south near Trenton, New Jersey in nearby Hamilton.
A permanent fixture of the Tribute in Light was to be installed on the roof of One World Trade Center, but never was due to the Durst Organization who redesigned the spire, which did not include the permanent fixture of Tribute in Light.
Since 2008, the generators that power Tribute in Light have been fueled with biodiesel made from used cooking oil collected from local restaurants provided by Tri-State Biodiesel. 
The lights have caused confusion for thousands of migrating birds, trapping them in the beams, and requiring that the lights be switched off for 20 minute periods to allow the birds to escape. To ensure the lights do not affect migrating birds, the Municipal Art Society works with the New York City Audubon on the illumination.
Media appearances.
The "Tribute in Light" has been featured in Boyz II Men's music video for "Color of Love". It also made a notable appearance during the opening credits of Spike Lee's 2002 film "25th Hour". In the Spider-Man 2 Video Game for Xbox, PlayStation 2 and GameCube, it appeared as a virtual memorial.

</doc>
<doc id="44104" url="http://en.wikipedia.org/wiki?curid=44104" title="Sigismund von Herberstein">
Sigismund von Herberstein

Siegmund (Sigismund) Freiherr von Herberstein, (or Baron Sigismund von Herberstein), (23 August 1486 – 28 March 1566) was a Carniolan diplomat, writer, historian and member of the Holy Roman Empire Imperial Council. He was most noted for his extensive writing on the geography, history and customs of Russia and contributed greatly to early Western European knowledge of that area.
Early life.
Herberstein was born in 1486 in Vipava (German "Wippach") in the Duchy of Carniola, now in Slovenia, then part of the Habsburg Monarchy's state of Inner Austria. His parents were Leonhard von Herberstein and Barbara von Lueg, members of the prominent German-speaking family which had already resided in Herberstein Castle for nearly 200 years. Little is known of his early life apart from the fact that he became familiar with the Slovene language spoken in the region. This knowledge became significant later in his life.
In 1499 he entered the University of Vienna to study philosophy and law. In 1506 he entered the army as an officer and served in a number of campaigns. In 1508 he was knighted by the Maximilian I, Holy Roman Emperor personally. In 1515 he entered the Imperial council, or Parliament, and began a long and illustrious diplomatic career.
Diplomatic career.
Between 1515 and 1553, Herberstein carried out approximately 69 missions abroad, travelling throughout much of Europe, including Turkey. He was feted by the ruling Habsburgs and rewarded with titles and estates. He was twice sent to Russia as ambassador of the Holy Roman Emperor, in 1517 to attempt to arrange a truce between Russia and Lithuania, and in 1526 to renew a treaty between the two signed in 1522. These extended visits (nine months in his 1517 visit) provided him with the opportunity to study a hitherto largely unknown Russian society.
Writing on Russia.
Herberstein's knowledge of Slovene, acquired in his youth, allowed him to communicate freely with Russians, as Slovene and Russian both belong to the Slavic languages. He used this ability to question a variety of people in Russia on a wide range of topics. This gave him an insight into Russia and Russians unavailable to the few previous visitors to Russia. 
He probably wrote his first account of life in Russia between 1517 and 1527, but no copy of this survives. In 1526 he was asked to produce a formal report on his experiences in Russia, but this remained relatively unnoticed in the archives until he was able to find time to revise and expand it, which he possibly started in the 1530s.
The evidence suggests that Herberstein was an energetic and capable ethnographer. He investigated in depth both by questioning locals and by critically examining the scarce existing literature on Russia. The result was his major work, a book written in Latin titled "Rerum Moscoviticarum Commentarii" (literally "Notes on Muscovite Affairs"), published in 1549. This became the main early source of knowledge in Western Europe on Russia.
He was the first to record the spelling of "tsar" as "czar" (both spellings are meant to express the same pronunciation). Later, English and French began to move from the 'cz' spelling to the 'ts' spelling in the 19th century.

</doc>
<doc id="44106" url="http://en.wikipedia.org/wiki?curid=44106" title="Marwan al-Shehhi">
Marwan al-Shehhi

Marwan Yousef Mohamed Rashid Lekrab al-Shehhi (Arabic: مروان يوسف محمد رشيد لكراب الشحي‎, "Marwān Yūsuf Muḥammad Rashīd Lekrāb ash-Sheḥḥī", also transliterated as Alshehhi; 9 May 1978 – 11 September 2001) was the hijacker-pilot of United Airlines Flight 175, crashing the plane into the South Tower of the World Trade Center as part of the September 11 attacks.
al-Shehhi was a student from the United Arab Emirates who moved to Germany in 1996 and soon became close friends with Mohamed Atta, Ziad Jarrah and Ramzi bin al-Shibh, forming the Hamburg cell. Together, after pledging their lives to martyrdom, they became the leaders of the September 11 attacks. In late 1999, al-Shehhi, Atta, Jarrah, and bin al-Shibh traveled to terrorist training camps in Afghanistan and met with Osama bin Laden who recruited the four Hamburg cell members for the attacks in the United States. He arrived in the United States in May 2000, one month before Atta. They both trained in Florida at Huffman Aviation, receiving their commercial pilot licenses in December 2000 from the FAA. 
Al-Shehhi spent 2001 making preparations for the attack itself, such as meeting with crucial September 11 planners abroad, assisting with the arrival of hijackers aboard the other flights, and travelling on surveillance flights determining details on how the hijacking would take place. On 9 September 2001, he flew from Florida to Boston, where he stayed at the Milner Hotel up until 11 September. Upon boarding United 175, al-Shehhi and 4 other hijackers waited 30 minutes into the flight to make their attack, which then allowed al-Shehhi to take over control as pilot, and at 9:03 a.m., 17 minutes after Mohamed Atta crashed American 11 into the North Tower, 23-year-old al-Shehhi crashed the Boeing 767 into the South Tower of the World Trade Center. He was the youngest hijacker-pilot in the attacks. The impact of the Boeing 767 operating as United 175 into the South Tower was seen live on television around the world as it happened.
Early life.
Al-Shehhi was born in Ras al-Khaimah, on 9 May 1978, in the United Arab Emirates, to a Muslim cleric who died in 1997. He was described as a quiet and devout Muslim. After graduating from high school in 1995, al-Shehhi enlisted in the Emirati military and received a half a year of basic training before he was admitted into a military scholarship program that allowed him to continue his education in Germany. Upon arriving in Germany in April 1996, al-Shehhi moved into an apartment, which he shared three other scholarship students for two months before boarding with a local German family. Several months later, he moved into his own apartment. Those who knew him described al-Shehhi as a very religious and friendly individual who wore western clothes and sometimes rented cars for trips to Berlin, France, and the Netherlands. 
While in Germany, al-Shehhi enrolled in the University of Bonn after completing a German course. He left Germany in June 1997 to attend to problems at home although the university forbade him. In early 1998, al-Shehhi transferred to the Technical University of Hamburg. A poor student, Marwan was directed by the Scholarship program administrators to repeat a semester of his studies back in Bonn beginning in August 1998. Al-Shehhi did not enroll back at Bonn until January 1999 and continued to struggle with his studies. By July 1999, Marwan returned to Hamburg to study shipbuilding.
Radicalization.
After moving to Hamburg in 1998, al-Shehhi helped form the Hamburg cell with Mohamed Atta and Ramzi bin al-Shibh. There, his views became more and more radical. They met three or four times a week to discuss anti-American feelings and plot possible attacks. When someone asked why he and Atta never laughed, al-Shehhi retorted, "How can you laugh when people are dying in Palestine?"
In October 1999 Marwan al-Shehhi was filmed at Said Bahaji's wedding in Germany with other 9/11 hijackers including Ziad Jarrah.
In late 1999, al-Shehhi, Atta, Ziad Jarrah, Said Bahaji, and Ramzi bin al-Shibh decided to travel to Chechnya to fight against the Russians, but were convinced by Khalid al-Masri and Mohamedou Ould Slahi at the last minute to change their plans. They instead traveled to Afghanistan to meet with Osama bin Laden and train for terrorist attacks. Immediately afterwards, Atta, al-Shehhi, and Jarrah reported their passports stolen, possibly to erase travel visas to Afghanistan. After their training, the hijackers began to attempt to hide their radicalism. al-Shehhi shaved his beard and seemed to his old friends like he had become less religious. After the attacks, a librarian in Hamburg reported that al-Shehhi boasted to her "There will be thousands of dead. You will think of me ... You will see, in America something is going to happen. There will be many people killed."
Al-Shehhi returned to Germany in March 2000, and began to learn to fly airplanes. Ammar al-Baluchi, one of the most important 9/11 financial organizers, bought a Boeing 747 flight simulator program using al-Shehhi's credit card. Eventually they decided that German flight schools would not work for them, and they decided to train in the United States. 
In the United States.
Flight education and preparation.
Al-Shehhi was the first of the Hamburg group to leave for the United States. He arrived in Newark, New Jersey on 29 May 2000. Atta joined him the next month, and the two began to search for flight schools. Al-Shehhi posed as a body guard of Atta, who was also posing as a "Saudi Arabian royal family member" while the two of them took flying lessons in Venice, Florida. They logged hundreds of hours on a Boeing 727 flight simulator. They received their licenses by December 2000. Their expenses were paid for by Ali Abdul Aziz Ali. On either 26 or 27 December, Atta and Marwan abandoned a Piper Cherokee that had stalled on the runway of Miami International Airport. On 29 December, Atta and Marwan went to the Opa-Locka Airport and practiced on a Boeing 727 simulator. Al-Shehhi began to take "surveillance flights" in the summer of 2001, watching the operations of flight crews and making final preparations. 
Travels in early 2001.
Ziad Jarrah, Atta, and al-Shehhi, having progressed in their training, all took foreign trips during the holiday period of 2000-2001. When Atta returned to Florida, al-Shehhi left for Morocco, traveling to Casablanca in mid-January 2001. al-Shehhi's family, concerned about not having heard from him, reported him missing to the UAE government. The UAE embassy in turn contacted the Hamburg police and a UAE representative tried to find him in Germany, visiting mosques and al-Shehhi's last address in Hamburg. After learning that his family was looking for him, al-Shehhi telephoned them on 20 January and said he was living and studying in Hamburg. The UAE government then told the Hamburg police they could call off the search.
Atta and al-Shehhi both encountered some difficulty reentering the United States, on 10 January and 19 January, respectively. As neither had presented a student visa, both of them had to persuade INS inspectors that they should be admitted so that they could continue their flight training. Neither operative had any problem clearing customs. After returning to Florida from their trips, Atta and al-Shehhi visited Georgia, staying briefly in Norcross and Decatur, and renting a single-engine plane to fly with an instructor in Lawrenceville. By 19 February, Atta and al-Shehhi were in Virginia. They had rented a mailbox in Virginia Beach, cashed a check, and then promptly returned to Georgia, staying in Stone Mountain. In mid-March, Ziad Jarrah was in Georgia as well, staying in Decatur. At the end of the month, Jarrah left the United States again and visited Sengün in Germany for two weeks. In early April, Atta and al-Shehhi returned to Virginia Beach and closed the mailbox they had opened in February.
Atta and al-Shehhi returned to Virginia Beach from their travels in Georgia, making their way to a large Dar Al-Hijrah mosque, sometime in early April. They were joined there by 9/11 hijackers Nawaf al-Hazmi and Hani Hanjour who had moved out of San Diego and Arizona after living in or visiting Abdussattar Shaikh's house, where Khalid al-Mihdhar also stayed. This mosque had recently in January 2001 hired the same imam Anwar al-Awlaki with whom Hazmi had spent time at the Rabat mosque in San Diego. He remembered Hazmi from San Diego but denied having contact with Hazmi or Hanjour in Virginia. Atta and al-Shehhi returned to Florida and moved into an apartment in Coral Springs. Atta stayed in Florida, awaiting the arrival of the first muscle hijackers. Al-Shehhi, on the other hand, bought a ticket to Cairo and flew there from Miami on 18 April. Al-Shehhi met with Atta's father, who stated in a post-9/11 interview that al-Shehhi wanted to pick up Atta's international driver's license and some money.
Al-Shehhi returned to Miami on 2 May. That day, Atta and Jarrah were together, about 30 miles to the north, visiting a Department of Motor Vehicles office in Lauderdale Lakes, Florida, to get Florida driver's licenses. In mid-July 2001, some of the hijackers and members of the Hamburg cell gathered near Salou, Spain, for a period of a few days up to a couple of weeks. Since hotel records are sparse during some of that time, it is thought that they may have spent considerable time in and around safe houses related to the al-Qaeda leader in Spain, Imad Yarkas. After 9/11, Spanish investigators followed the trails backwards, and the events they uncovered were chronicled in the Spanish nationwide newspaper "El País". Witnesses told Spanish investigators they saw a man who resembled al-Shehhi on 17 July 2001 at the Universal Studios PortAventura theme park next to Salou, Spain. The visitor, who was accompanied by two men, inquired about rides at the customer service counter. Witnesses indicated these companions resembled Ziad Jarrah, the later pilot on United Airlines Flight 93, and Said Bahaji, a then 26-year-old German-Moroccan member of the al-Qaeda cell in Hamburg. Back in Germany, it had been Bahaji's 1999 wedding during which al-Shehhi was filmed. Other witnesses elsewhere had pointed out Bahaji from photos, as one of the men they saw in Spain. But Bahaji bore a resemblance in appearance to Atta, who was traced to the same areas in Spain via hotel and travel records.
August 2001.
On 23 August, Israeli Mossad reportedly gave al-Shehhi's name to the CIA as part of a list of 19 names they said were planning an attack in the near future. Only four of the names are known for certain,` including al-Shehhi; Nawaf al-Hazmi, Mohamed Atta, and Khalid al-Mihdhar.
On 26 August, Marwan signed into the Panther Motel in Deerfield Beach, Florida, paying US$500, saying he wanted to stay until 2 September, and listing a Mailboxes Etc. as his permanent address. His register entry indicated that he was driving a blue Chevrolet Malibu, assumed to be the one rented by Atta two weeks prior, and manager Richard Surma said that he bent rules to allow Marwan to have another man as an overnight guest. On 28 August, Marwan went to the Miami International Airport, accompanied by an unknown man, where he purchased his ticket for Flight 175. On 9 September, the motel manager, cleaning the room that al-Shehhi had vacated, found a bag containing a German/English dictionary, a protractor, flight manuals and local airport listings. Another employee later reported finding a box cutter. 
According to librarian Kathleen Hensmen, Wail al-Shehri and Waleed al-Shehri used Internet access at Delray Beach Public Library in August 2001, where they may have been looking at information on crop dusting. They reportedly left the library with a third middle-eastern man, thought to be Marwan al-Shehhi, whom Hensmen claimed asked her for the name of a local restaurant. Staff at Shuckum's Oyster Pub and Seafood Grill in Hollywood, Florida claimed they recognized both Atta and Marwan as two of the people who had been at the restaurant on either 7 or 8 September. While there are varying stories about Atta's activities, all sources indicate that al-Shehhi drank rum and coke while talking to the others. On 9 September, they flew to Boston. The next day, al-Shehhi and three of the other hijackers, Fayez Banihammad, Mohand al-Shehri, and Satam al-Suqami, shared a room at the Milner Hotel in Boston. 
Attacks.
According to the 9/11 Commission Report, al-Shehhi made a 3-minute call to Mohamed Atta (6:52-6:55 a.m.) on 11 September from within Logan International Airport as both American 11 and United 175 were to fly from Boston Logan to LAX. 
al-Shehhi boarded United 175 at 7:27 a.m. Around 30 minutes into the flight, between 8:42 and 8:46 a.m., the plane was hijacked. During the flight, the plane narrowly avoided a mid-air collision with another aircraft, Delta Airlines Flight 2315. Several calls were made from the plane to relatives, the passengers learning of the fate of American 11.
The plane was flown into the South Tower of the World Trade Center at 9:03:02 a.m. The plane crashed with a speed of approximately 590 mph (950 km/h). The plane was carrying about 10,000 gallons (37,850 litres) of jet fuel. It was seen live on television around the world as it crashed into the South Tower, being filmed from multiple vantage points. al-Shehhi flew the plane faster and lower into the tower than Atta did, into the eastern half of the South Tower's southern facade close to the southeast corner, leading to the South Tower collapsing before the North Tower; which was the first to be hit.

</doc>
<doc id="44108" url="http://en.wikipedia.org/wiki?curid=44108" title="Minuet">
Minuet

A minuet (; also spelled "menuet"), is a social dance of French origin for two people, usually in 3/4 time. The word was adapted from Italian "minuetto" and French "menuet", possibly from the French "menu" meaning slender, small, referring to the very small steps, or from the early 17th-century popular group dances called "branle à mener" or "amener".
The term also describes the musical style that accompanies the dance, which subsequently developed more fully, often with a longer musical form called the minuet and trio, and was much used as a movement in the early classical symphony.
Dance.
The name may refer to the short steps, "pas menus", taken in the dance, or else be derived from the "branle à mener" or "amener", popular group dances in early 17th-century France . The minuet was traditionally said to have descended from the "bransle de Poitou", though there is no evidence making a clear connection between these two dances. The earliest treatise to mention the possible connection of the name to the expression "pas menus" is Gottfried Taubert's "Rechtschaffener Tantzmeister", published in Leipzig in 1717, but this source does not describe the steps as being particularly small or dainty . At the period when it was most fashionable it was controlled, ceremonious and graceful.
Music.
Rhythm and form.
The name is also given to a musical composition written in the same time and rhythm, though when not accompanying an actual dance the pace was quicker. Stylistically refined minuets, apart from the social dance context, were introduced—to opera at first—by Jean-Baptiste Lully, who included no fewer than 92 of them in his theatrical works and in the late 17th century the minuet was adopted into the suite, such as some of the suites of Johann Sebastian Bach and George Frideric Händel. Among Italian composers the minuet was often considerably quicker and livelier and was sometimes written in 3/8 or 6/8 time. Initially, before its adoption in contexts other than social dance, the minuet was usually in binary form, with two repeated sections of usually eight bars each. But the second section eventually expanded, resulting in a kind of ternary form. The second (or middle) minuet provided form of contrast by means of different key and orchestration. On a larger scale, two such minuets might be further combined, so that the first minuet was followed by a second one and then by a repetition of the first. The whole form might in any case be repeated as long as the dance lasted.
Minuet and trio.
Around Lully's time it became a common practice to score this middle section for a trio (such as two oboes and a bassoon, as is common in Lully). As a result, this middle section came to be called trio, even when no trace of such an orchestration remains. The overall structure is called rounded binary or minuet form :
 A :||: B A or A'
 I(->V) :||: V(or other closely related) I
The minuet and trio eventually became the standard third movement in the four-movement classical symphony, Johann Stamitz being the first to employ it thus with regularity. A livelier form of the minuet later developed into the scherzo (which was generally also coupled with a trio). This term came into existence approximately from Beethoven onwards, but the form itself can be traced back to Haydn. An example of the true form of the minuet is to be found in "Don Giovanni".
A famous example of a more recent instrumental work in minuet form is Ignacy Jan Paderewski's Minuet in G.

</doc>
<doc id="44112" url="http://en.wikipedia.org/wiki?curid=44112" title="John Dalton">
John Dalton

John Dalton FRS (6 September 1766 – 27 July 1844) was an English chemist, physicist, and meteorologist. He is best known for his pioneering work in the development of modern atomic theory; and his research into colour blindness, sometimes referred to as Daltonism, in his honour.
Early life.
John Dalton was born into a Quaker family at the settlement of Eaglesfield, near the town of Cockermouth, in the county of Cumberland, England in 1766. His father was a weaver. He received his early education from his father and from Quaker John Fletcher, who ran a private school at Pardshaw Hall. With his family too poor to support him for long, he began to earn his living at the age of ten in the service of a wealthy local Quaker, Elihu Robinson. It is said he began teaching at a local school at age 12, and was prolific in Latin at age 14.
Early careers.
He joined his older brother Jonathan at age 15 in running a Quaker school at Stramongate in Kendal, about forty five miles from his home. Around age 23 Dalton may have considered studying law or medicine, but his relatives did not encourage him, perhaps because being a Dissenter (a Christian opposed to a state religion and mandatory membership in the Church of England), he was barred from attending English universities. He acquired much scientific knowledge from informal instruction by John Gough, a blind philosopher who was gifted in the sciences and arts. At age 27 he was appointed teacher of mathematics and natural philosophy at the "New College" in Manchester, a dissenting academy. He remained there until age 34, when the college's worsening financial situation led him to resign his post and begin a new career as a private tutor for mathematics and natural philosophy.
Scientific contributions.
Meteorology.
Dalton's early life was highly influenced by a prominent Eaglesfield Quaker named Elihu Robinson, a competent meteorologist and instrument maker, who got him interested in problems of mathematics and meteorology. During his years in Kendal, Dalton contributed solutions of problems and questions on various subjects to "The Ladies' Diary" and the "Gentleman's Diary". In 1787 at age 21 he began to keep a meteorological diary in which, during the succeeding 57 years, he entered more than 200,000 observations. He also rediscovered George Hadley's theory of atmospheric circulation (now known as the Hadley cell) around this time. Dalton's first publication was "Meteorological Observations and Essays" at age 27 in 1793, which contained the seeds of several of his later discoveries. However, in spite of the originality of his treatment, little attention was paid to them by other scholars. A second work by Dalton, "Elements of English Grammar", was published at age 35 in 1801.
Colour blindness.
In 1794 at age 28, shortly after his arrival in Manchester, Dalton was elected a member of the Manchester Literary and Philosophical Society, the "Lit & Phil", and a few weeks later he communicated his first paper on "Extraordinary facts relating to the vision of colours", in which he postulated that shortage in colour perception was caused by discoloration of the liquid medium of the eyeball. In fact, a shortage of colour perception in some people had not even been formally described or officially noticed until Dalton wrote about his own. Since both he and his brother were colour blind, he recognized that this condition must be hereditary.
Although Dalton's theory lost credence in his own lifetime, the thorough and methodical nature of his research into his own visual problem was so broadly recognized that Daltonism became a common term for colour blindness. Examination of his preserved eyeball in 1995 demonstrated that Dalton actually had a less common kind of colour blindness, deuteroanopia, in which medium wavelength sensitive cones are missing (rather than functioning with a mutated form of their pigment, as in the most common type of colour blindness, deuteroanomaly). Besides the blue and purple of the optical spectrum he was able to recognize only one colour, yellow, or, as he says in his paper,
"that part of the image which others call red appears to me little more than a shade or defect of light. After that the orange, yellow and green seem one colour which descends pretty uniformly from an intense to a rare yellow, making what I should call different shades of yellow"
Measuring mountains in the Lake District.
Dalton regularly holidayed in the Lake District where his study of meteorology involved a lot of mountain climbing: until the advent of aeroplanes and weather balloons, the only way to make measurements of temperature and humidity at altitude was to climb a mountain. The altitude achieved was estimated using a barometer. This meant that, until the Ordnance Survey started publishing their maps for the Lake District in the 1860s, Dalton was one of the few sources of such information. Dalton was often accompanied by Jonathan Otley, who was one of the few other authorities on the heights of the Lake District mountains. He became both an assistant and a friend.
Gas laws.
In 1800, at age 34 Dalton became a secretary of the Manchester Literary and Philosophical Society, and in the following year he orally presented an important series of papers, entitled "Experimental Essays" on the constitution of mixed gases; on the pressure of steam and other vapours at different temperatures, both in a vacuum and in air; on evaporation; and on the thermal expansion of gases. These four essays were published in the "Memoirs" of the Lit & Phil in 1802.
The second of these essays opens with the striking remark,
"There can scarcely be a doubt entertained respecting the reducibility of all elastic fluids of whatever kind, into liquids; and we ought not to despair of effecting it in low temperatures and by strong pressures exerted upon the unmixed gases further".
After describing experiments to ascertain the pressure of steam at various points between 0 and 100 °C (32 and 212 °F), Dalton concluded from observations on the vapour pressure of six different liquids, that the variation of vapour pressure for all liquids is equivalent, for the same variation of temperature, reckoning from vapour of any given pressure.
In the fourth essay he remarks,
"I see no sufficient reason why we may not conclude that all elastic fluids under the same pressure expand equally by heat and that for any given expansion of mercury, the corresponding expansion of air is proportionally something less, the higher the temperature. It seems, therefore, that general laws respecting the absolute quantity and the nature of heat are more likely to be derived from elastic fluids than from other substances."
He thus enunciated Gay-Lussac's law or J.A.C. Charles's law, published in 1802 at age 36 by Joseph Louis Gay-Lussac. In the two or three years following the reading of these essays, Dalton published several papers on similar topics, that on the absorption of gases by water and other liquids (1803), containing his law of partial pressures now known as Dalton's law.
Atomic theory.
The most important of all Dalton's investigations are those concerned with the atomic theory in chemistry, with which his name is inseparably associated. It has been proposed that this theory was suggested to him either by researches on ethylene ("olefiant gas") and methane ("carburetted hydrogen") or by analysis of nitrous oxide ("protoxide of azote") and nitrogen dioxide ("deutoxide of azote"), both views resting on the authority of Thomas Thomson. However, a study of Dalton's own laboratory notebooks, discovered in the rooms of the Lit & Phil, concluded that so far from Dalton being led by his search for an explanation of the law of multiple proportions to the idea that chemical combination consists in the interaction of atoms of definite and characteristic weight, the idea of atoms arose in his mind as a purely physical concept, forced upon him by study of the physical properties of the atmosphere and other gases. The first published indications of this idea are to be found at the end of his paper on the absorption of gases already mentioned, which was read on 21 October 1803, though not published until 1805. Here he says:
"Why does not water admit its bulk of every kind of gas alike? This question I have duly considered, and though I am not able to satisfy myself completely I am nearly persuaded that the circumstance depends on the weight and number of the ultimate particles of the several gases".
The main points of Dalton's atomic theory were:
Dalton proposed an additional "rule of greatest simplicity" that created controversy, since it could not be independently confirmed.
This was merely an assumption, derived from faith in the simplicity of nature. No evidence was then available to scientists to deduce how many atoms of each element combine to form compound molecules. But this or some other such rule was absolutely necessary to any incipient theory, since one needed an assumed molecular formula in order to calculate relative atomic weights. In any case, Dalton's "rule of greatest simplicity" caused him to assume that the formula for water was OH and ammonia was NH, quite different from our modern understanding (H2O, NH3).
Despite the uncertainty at the heart of Dalton's atomic theory, the principles of the theory survived. To be sure, the conviction that atoms cannot be subdivided, created, or destroyed into smaller particles when they are combined, separated, or rearranged in chemical reactions is inconsistent with the existence of nuclear fusion and nuclear fission, but such processes are nuclear reactions and not chemical reactions. In addition, the idea that all atoms of a given element are identical in their physical and chemical properties is not precisely true, as we now know that different isotopes of an element have slightly varying weights. However, Dalton had created a theory of immense power and importance. Indeed, Dalton's innovation was fully as important for the future of the science as Antoine Laurent Lavoisier's oxygen-based chemistry had been.
Atomic weights.
Dalton proceeded to print his first published table of relative atomic weights. Six elements appear in this table, namely hydrogen, oxygen, nitrogen, carbon, sulfur, and phosphorus, with the atom of hydrogen conventionally assumed to weigh 1. Dalton provided no indication in this first paper how he had arrived at these numbers. However, in his laboratory notebook under the date 6 September 1803 there appears a list in which he sets out the relative weights of the atoms of a number of elements, derived from analysis of water, ammonia, carbon dioxide, etc. by chemists of the time.
It appears, then, that confronted with the problem of calculating the relative diameter of the atoms of which, he was convinced, all gases were made, he used the results of chemical analysis. Assisted by the assumption that combination always takes place in the simplest possible way, he thus arrived at the idea that chemical combination takes place between particles of different weights, and it was this which differentiated his theory from the historic speculations of the Greeks, such as Democritus and Lucretius.
The extension of this idea to substances in general necessarily led him to the law of multiple proportions, and the comparison with experiment brilliantly confirmed his deduction. It may be noted that in a paper on the proportion of the gases or elastic fluids constituting the atmosphere, read by him in November 1802, the law of multiple proportions appears to be anticipated in the words: "The elements of oxygen may combine with a certain portion of nitrous gas or with twice that portion, but with no intermediate quantity", but there is reason to suspect that this sentence may have been added some time after the reading of the paper, which was not published until 1805.
Compounds were listed as binary, ternary, quaternary, etc. (molecules composed of two, three, four, etc. atoms) in the "New System of Chemical Philosophy" depending on the number of atoms a compound had in its simplest, empirical form.
He hypothesized the structure of compounds can be represented in whole number ratios. So, one atom of element X combining with one atom of element Y is a binary compound. Furthermore, one atom of element X combining with two elements of Y or vice versa, is a ternary compound. Many of the first compounds listed in the "New System of Chemical Philosophy" correspond to modern views, although many others do not.
Dalton used his own symbols to visually represent the atomic structure of compounds. These were depicted in the"New System of Chemical Philosophy", where Dalton listed twenty elements and seventeen simple molecules.
Other investigations.
Dalton published papers on such diverse topics as rain and dew and the origin of springs (hydrosphere); on heat, the color of the sky, steam, and the reflection and refraction of light; and on the grammatical subjects of the auxiliary verbs and participles of the English language.
Experimental approach.
As an investigator, Dalton was often content with rough and inaccurate instruments, though better ones were obtainable. Sir Humphry Davy described him as "a very coarse experimenter", who almost always found the results he required, trusting to his head rather than his hands. On the other hand, historians who have replicated some of his crucial experiments have confirmed Dalton's skill and precision.
In the preface to the second part of Volume I of his "New System", he says he had so often been misled by taking for granted the results of others that he determined to write "as little as possible but what I can attest by my own experience", but this independence he carried so far that it sometimes resembled lack of receptivity. Thus he distrusted, and probably never fully accepted, Gay-Lussac's conclusions as to the combining volumes of gases. He held unconventional views on chlorine. Even after its elementary character had been settled by Davy, he persisted in using the atomic weights he himself had adopted, even when they had been superseded by the more accurate determinations of other chemists. He always objected to the chemical notation devised by Jöns Jakob Berzelius, although most thought that it was much simpler and more convenient than his own cumbersome system of circular symbols.
Other publications.
For "Rees's Cyclopædia" Dalton contributed articles on Chemistry and Meteorology, but the topics are not known.
He contributed 117 "Memoirs of the Literary and Philosophical Society of Manchester", from 1817 until his death in 1840, while president of that organization. Of these the earlier are the most important. In one of them, read in 1814, he explains the principles of volumetric analysis, in which he was one of the earliest workers. In 1840 a paper on the phosphates and arsenates, often regarded as a weaker work, was refused by the Royal Society, and he was so incensed that he published it himself. He took the same course soon afterwards with four other papers, two of which ("On the quantity of acids, bases and salts in different varieties of salts" and "On a new and easy method of analysing sugar") contain his discovery, regarded by him as second in importance only to the atomic theory, that certain anhydrates, when dissolved in water, cause no increase in its volume, his inference being that the salt enters into the pores of the water.
Public life.
Before he had propounded the atomic theory, he had already attained a considerable scientific reputation. In 1803, he was chosen to give a course of lectures on natural philosophy at the Royal Institution in London, and he delivered another course of lectures there in 1809–1810. However, some witnesses reported that he was deficient in the qualities that make an attractive lecturer, being harsh and indistinct in voice, ineffective in the treatment of his subject, and singularly wanting in the language and power of illustration.
In 1810, Sir Humphry Davy asked him to offer himself as a candidate for the fellowship of the Royal Society, but Dalton declined, possibly for financial reasons. However, in 1822 he was proposed without his knowledge, and on election paid the usual fee. Six years previously he had been made a corresponding member of the French Académie des Sciences, and in 1830 he was elected as one of its eight foreign associates in place of Davy. In 1833, at age 67 Earl Grey's government conferred on him a pension of £150, raised in 1836 to £300. He was elected a Foreign Honorary Member of the American Academy of Arts and Sciences in 1834 at age 68.
A young James Prescott Joule, who later studied and published (1843) on the nature of heat and its relationship to mechanical work, was a famous pupil of Dalton in his last years.
Personal life.
Dalton never married and had only a few close friends. All in all as a Quaker he lived a modest and unassuming life.
For twenty-six years until Dalton's death, he lived in a room in the home of the Rev. W. Johns, a published botanist, and his wife in George Street, Manchester. Dalton and Johns died in the same year - 1844. Dalton's daily round of laboratory work and tutoring in Manchester was broken only by annual excursions to the Lake District and occasional visits to London. In 1822 he paid a short visit to Paris, where he met many distinguished resident men of science. He attended several of the earlier meetings of the British Association at York, Oxford, Dublin and Bristol.
Disability and death.
Dalton suffered a minor stroke in 1837, and a second one in 1838 left him with a speech impairment, though he remained able to perform experiments. In May 1844 he had yet another stroke; on 26 July 1844 he recorded with trembling hand his last meteorological observation. On 27 July 1844, in Manchester, Dalton fell from his bed and was found lifeless by his attendant.
Dalton was accorded a civic funeral with full honours. His body was laid in state in Manchester Town Hall for four days and more than 40,000 people filed past his coffin. The funeral procession included representatives of the city’s major civic, commercial, and scientific bodies. He was buried in Manchester in Ardwick cemetery. The cemetery is now a playing field, but pictures of the original grave may be found in published materials.
Legacy.
The standard author abbreviation Jn.Dalton is used to indicate this individual as the author when citing a botanical name.

</doc>
<doc id="44114" url="http://en.wikipedia.org/wiki?curid=44114" title="Symphony">
Symphony

A symphony is an extended musical composition in Western classical music, most often written for orchestra. Although the term has had many meanings from its origins in the ancient Greek era, by the late 18th century the word had taken on the meaning common today: a work usually consisting of multiple distinct sections or movements, often four, with the first movement in sonata form.
The word is also used as a reference to orchestras that are primarily associated with the performance of symphonies (for example, Miami's New World Symphony).
Origins.
The word "symphony" is derived from Greek συμφωνία ("symphonia"), meaning "agreement or concord of sound", "concert of vocal or instrumental music", from σύμφωνος ("symphōnos"), "harmonious" ("Oxford English Dictionary"). The word referred to an astonishing variety of different things, before ultimately settling on its current meaning designating a musical form.
In late Greek and medieval theory, the word was used for consonance, as opposed to διαφωνία ("diaphōnia"), which was the word for dissonance (Brown 2001). In the Middle Ages and later, the Latin form "symphonia" was used to describe various instruments, especially those capable of producing more than one sound simultaneously (Brown 2001). Isidore of Seville was the first to use the word symphonia as the name of a two-headed drum, and from c. 1155 to 1377 the French form "symphonie" was the name of the "organistrum" or hurdy-gurdy. In late medieval England, "symphony" was used in both of these senses, whereas by the 16th century it was equated with the dulcimer. In German, "Symphonie" was a generic term for spinets and virginals from the late 16th century to the 18th century (Marcuse 1975, 501). 
In the sense of "sounding together," the word begins to appear in the titles of some works by 16th- and 17th-century composers including Giovanni Gabrieli's "Sacrae symphoniae", and "Symphoniae sacrae, liber secundus", published in 1597 and 1615, respectively; Adriano Banchieri's "Eclesiastiche sinfonie, dette canzoni in aria francese, per sonare, et cantare", op. 16, published in 1607; Lodovico Grossi da Viadana's "Sinfonie musicali", op. 18, published in 1610; and Heinrich Schütz's "Symphoniae sacrae", op. 6, and "Symphoniarum sacrarum secunda pars", op. 10, published in 1629 and 1647, respectively. Except for Viadana's collection, which contained purely instrumental and secular music, these were all collections of sacred vocal works, some with instrumental accompaniment (Bowman 1971, 7; Larue, Bonds, Walsh, and Wilson 2001).
In the 17th century, for most of the Baroque period, the terms "symphony" and "sinfonia" were used for a range of different compositions, including instrumental pieces used in operas, sonatas and concertos—usually part of a larger work. The "opera sinfonia", or "Italian overture" had, by the 18th century, a standard structure of three contrasting movements: fast, slow, fast and dance-like. It is this form that is often considered as the direct forerunner of the orchestral symphony. The terms "overture", "symphony" and "sinfonia" were widely regarded as interchangeable for much of the 18th century (Larue, Bonds, Walsh, and Wilson 2001).
18th century.
During the 18th century, "the symphony was cultivated with extraordinary intensity" (Larue, Bonds, Walsh, and Wilson 2001, §I.2, citing two scholarly catalogs listing over 13,000 distinct works: Larue 1959 and 1988). It played a role in many areas of public life, including church services (Larue, Bonds, Walsh, and Wilson 2001, §I.2), but a particularly strong area of support for symphonic performances was the aristocracy. In Vienna, perhaps the most important location in Europe for the composition of symphonies, "literally hundreds of noble families supported musical establishments, generally dividing their time between Vienna and their ancestral estate [elsewhere in the Empire]" (Larue, Bonds, Walsh, and Wilson 2001, §I.10). Since the normal size of the orchestra at the time was quite small, many of these courtly establishments were capable of performing symphonies. The young Joseph Haydn, taking up his first job as a music director in 1757 for the Morzin family, found that when the Morzin household was in Vienna, his own orchestra was only part of a lively and competitive musical scene, with multiple aristocrats sponsoring concerts with their own ensembles (Carpani 1823, 66, cited in Gotwals 1968).
Larue, Bonds, Walsh, and Wilson (2001, §I.4) trace the gradual expansion of the symphonic orchestra through the 18th century. At first, symphonies were string symphonies, written in just four parts: first violin, second violin, viola, and bass (the bass line was taken by cello(s), double bass(es) playing the part an octave below, and perhaps also a bassoon). Occasionally the early symphonists even dispensed with the viola part, thus creating three-part symphonies. A basso continuo part including a bassoon together with a harpsichord or other chording instrument was also possible (Larue, Bonds, Walsh, and Wilson 2001, §I.4).
The first additions to this simple ensemble were a pair of horns, occasionally a pair of oboes, and then both horns and oboes together. Over the century, other instruments were added to the classical orchestra: flutes (sometimes replacing the oboes), separate parts for bassoons, clarinets, and trumpets and timpani. Works varied in their scoring concerning which of these additional instruments were to appear. The full-scale classical orchestra, deployed at the end of the century for the largest-scale symphonies, has the standard string ensemble mentioned above, pairs of winds (flutes, oboes, clarinets, bassoons), a pair of horns, and timpani. A keyboard continuo instrument (harpsichord or piano) remained an option.
The "Italian" style of symphony, often used as overture and entr'acte in opera houses, became a standard three-movement form: a fast movement, a slow movement, and another fast movement. Haydn and Mozart, whose early symphonies were in this form, eventually replaced it with a four-movement form through the addition of a second middle movement (Prout 1895, 249). The four-movement symphony became dominant in the latter part of the 18th century and most of the 19th century. This symphonic form was influenced by Germanic practice, and would come to be associated with the classical style of Haydn and Mozart.
The normal four-movement form became the following (Jackson 1999, 26; Stein 1979, 106):
Variations on this layout, like changing the order of the middle movements or adding a slow introduction to the first movement, were common. Haydn, Mozart and their contemporaries restricted their use of the four-movement form to orchestral or multi-instrument chamber music such as quartets, though since Beethoven solo sonatas are as often written in four as in three movements (Prout 1895, 249).
The composition of early symphonies was centred on Milan, Vienna, and Mannheim. The Milanese school centred around Giovanni Battista Sammartini and included Antonio Brioschi, Ferdinando Galimberti and Giovanni Battista Lampugnani. Early exponents of the form in Vienna included Georg Christoph Wagenseil, Wenzel Raimund Birck and Georg Matthias Monn, while later significant Viennese composers of symphonies included Johann Baptist Wanhal, Karl Ditters von Dittersdorf and Leopold Hoffmann. The Mannheim school included Johann Stamitz (Anon. n.d.).
The most important symphonists of the latter part of the 18th century are Haydn, who wrote at least 107 symphonies over the course of 36 years (Webster and Feder 2001), and Mozart, with at least 47 symphonies in 24 years (Eisen and Sadie 2001).
19th century.
At the beginning of the 19th century, Beethoven elevated the symphony from an everyday genre produced in large quantities to a supreme form in which composers strove to reach the highest potential of music in just a few works (Dahlhaus 1989, 265). Beethoven began with two works directly emulating his models Mozart and Haydn, then seven more symphonies, starting with the Third Symphony ("Eroica") that expanded the scope and ambition of the genre. His Symphony No. 5 is perhaps the most famous symphony ever written; its transition from the emotionally stormy C minor opening movement to a triumphant major-key finale provided a model adopted by later symphonists such as Brahms (Libbey 1999, 40) and Mahler. His Symphony No. 6 is a programmatic work, featuring instrumental imitations of bird calls and a storm, and a convention-defying fifth movement. His Symphony No. 9 takes the step unprecedented since the early baroque era of including parts for vocal soloists and choir in the last movement, making it a choral symphony.
Of the symphonies of Franz Schubert, two are core repertory items and are frequently performed. Of the Eighth Symphony (1822), Schubert completed only the first two movements; this highly Romantic work is usually called by its nickname "The Unfinished." His last completed symphony, the Ninth (1826) is a massive work in the Classical idiom (Rosen 1997:521).
Of the early Romantics, Felix Mendelssohn (five symphonies) and Robert Schumann (four) continued to write symphonies in the classical mold, though using their own musical language. In contrast, Hector Berlioz favored programmatic works, including his "dramatic symphony" "Roméo et Juliette" and the highly original "Symphonie fantastique". The latter is also a programme work and has both a march and a waltz and five movements instead of the customary four. His fourth and last symphony, the "Grande symphonie funèbre et triomphale" (originally titled "Symphonie militaire") was composed in 1840 for a 200-piece marching military band, to be performed out of doors, and is an early example of a band symphony. Berlioz later added optional string parts and a choral finale (Macdonald 2001b, §3: 1831–42). In 1851, Richard Wagner declared that all of these post-Beethoven symphonies were no more than an epilogue, offering nothing substantially new. Indeed, after Schumann's last symphony, the "Rhenish" composed in 1850, for two decades the Lisztian symphonic poem appeared to have displaced the symphony as the leading form of large-scale instrumental music. If the symphony had been eclipsed, it was not long before it re-emerged in a "second age" in the 1870s and 1880s, with the symphonies of Anton Bruckner, Johannes Brahms, Pyotr Ilyich Tchaikovsky, Alexander Borodin, Antonín Dvořák, and César Franck—works which continued to dominate the concert repertory for at least a century (Dahlhaus 1989, 265).
Over the course of the 19th century, composers continued to add to the size of the symphonic orchestra. Around the beginning of the century, a full-scale orchestra would consist of the string section plus pairs of flutes, oboes, clarinets, bassoons, horns, trumpets, and lastly a set of timpani (Larue, Bonds, Walsh, and Wilson 2001, II.1) This is, for instance, the scoring used in Beethoven's symphonies numbered 1, 2, 4, 7, and 8 (instrumentation of Beethoven symphonies taken from the chapter headings for each symphony in Hopkins 1981,). Trombones, which had previously been confined to church and theater music, came to be added to the symphonic orchestra, notably in Beethoven's 5th, 6th, and 9th symphonies. The combination of bass drum, triangle, and cymbals (sometimes also: piccolo), which 18th century composers employed as a coloristic effect in so-called "Turkish music", came to be increasingly used during the second half of the 19th century without any such connotations of genre (Larue, Bonds, Walsh, and Wilson 2001, II.1). By the time of Mahler (see below), it was possible for a composer to write a symphony scored for "a veritable compendium of orchestral instruments" (Larue, Bonds, Walsh, and Wilson 2001, II.1). In addition to increasing in variety of instruments, 19th century symphonies were gradually augmented with more string players and more wind parts, so that that the orchestra grew substantially in sheer numbers, as concert halls likewise grew (Larue, Bonds, Walsh, and Wilson 2001, II.1).
20th century.
At the beginning of the 20th century, Gustav Mahler wrote long, large-scale symphonies. His Eighth Symphony, for example, was composed in 1906 and is nicknamed the "Symphony of a Thousand" because of the forces required to perform it. The 20th century also saw further diversification in the style and content of works that composers labeled "symphonies" (Anon. 2008). Some composers, including Dmitri Shostakovich, Sergei Rachmaninoff, and Carl Nielsen, continued to write in the traditional four-movement form, while other composers took different approaches: Jean Sibelius' "Symphony No. 7", his last, is in one movement, whereas Alan Hovhaness's Symphony No. 9, "Saint Vartan"—originally op. 80, changed to op. 180—composed in 1949–50, is in twenty-four (Tawa 2001, 352).
A concern with unification of the traditional four-movement symphony into a single, subsuming formal conception had emerged in the late 19th century. This has been called a "two-dimensional symphonic form", and finds its key turning point in Arnold Schoenberg's Chamber Symphony No. 1, Op. 9 (1909), which was followed in the 1920s by other notable single-movement German symphonies, including Kurt Weill’s First Symphony (1921), Max Butting’s Chamber Symphony, Op. 25 (1923), and Paul Dessau's 1926 Symphony (Vande Moortele 2013, 269, 284n9).
There remained, however, certain tendencies. Designating a work a "symphony" still implied a degree of sophistication and seriousness of purpose. The word "sinfonietta" came into use to designate a work that is shorter, of more modest aims, or "lighter" than a symphony, such as Sergei Prokofiev's Sinfonietta for orchestra (Kennedy 2006a; Temperley 2001).
In the first half of the century, Edward Elgar, Gustav Mahler, Jean Sibelius, Carl Nielsen, Igor Stravinsky, Bohuslav Martinů, Roger Sessions, and Dmitri Shostakovich composed symphonies "extraordinary in scope, richness, originality, and urgency of expression" (Steinberg 1995, 404). One measure of the significance of a symphony is the degree to which it reflects conceptions of temporal form particular to the age in which it was created. Five composers from across the span of the 20th century who fulfill this measure are Sibelius, Stravinsky, Luciano Berio (in his Sinfonia, 1968–69), Elliott Carter (in his "Symphony of Three Orchestras", 1976), and Pelle Gudmundsen-Holmgreen (in "Symphony/Antiphony", 1980) (Grimley 2013, 287).
Beginning in the 20th century, more symphonies have been written for concert band than in past centuries. Although examples exist from as early as 1932, the first such symphony of importance since Hector Berlioz wrote the "Grande symphonie funèbre et triomphale" in 1840 is Nikolai Miaskovsky’s Symphony No. 19, Op. 46, composed in 1939 (Battisti 2002, 42). Some further examples are Paul Hindemith's Symphony in B-flat for Band, composed in 1951; Morton Gould's Symphony No. 4 "West Point", composed in 1952; Vincent Persichetti's Symphony No. 6, Op. 69, composed in 1956; Vittorio Giannini's Symphony No.3, composed in 1959; Alan Hovhaness's Symphonies No. 4, op. 165, No. 7, "Nanga Parvat", op. 175, No. 14, "Ararat", op. 194, and No. 23, "Ani", op. 249, composed in 1958, 1959, 1961, and 1972 respectively; Alfred Reed's 2nd, 3rd, 4th, and 5th symphonies, composed in 1979, 1988, 1992, and 1994 respectively; and Johan de Meij's Symphony No. 1 "The Lord of the Rings", composed in 1988, and his Symphony No. 2 "The Big Apple", composed in 1993.
Sources.
</dl>

</doc>
<doc id="44116" url="http://en.wikipedia.org/wiki?curid=44116" title="Concerto">
Concerto

A concerto (from the Italian: "concerto", plural "concerti" or, often, the anglicised form "concertos") is a musical composition usually composed in three parts or movements, in which (usually) one solo instrument (for instance, a piano, violin, cello or flute) is accompanied by an orchestra or concert band.
The etymology is uncertain, but the word seems to have originated from the conjunction of the two Latin words "conserere" (meaning to tie, to join, to weave) and "certamen" (competition, fight): the idea is that the two parts in a concerto, the soloist and the orchestra or concert band, alternate episodes of opposition, cooperation, and independence in the creation of the music flow.
The concerto, as understood in this modern way, arose in the Baroque period side by side with the concerto grosso, which contrasted a small group of instruments with the rest of the orchestra. The popularity of the concerto grosso form declined after the Baroque period, and the genre was not revived until the 20th century. The solo concerto, however, has remained a vital musical force from its inception to this day.
Early Baroque concerto.
The term "concerto" was initially used to denote works involving voices and instruments in which the instruments had independent parts—as opposed to the Renaissance common practice in which the instruments that accompanied voices only doubled the voice parts. Examples of this earlier form of concerto include Giovanni Gabrieli's "In Ecclesiis" or Heinrich Schütz's "Saul, Saul, was verfolgst du mich."
Late Baroque concerto.
The concerto began to take its modern shape in the late Baroque period.
Starting from a form called Concerto grosso popularized by Arcangelo Corelli, it evolved into the form we understand today as performance of a soloist with/"against" an orchestra.
The main composers of concerti of the baroque were Tommaso Albinoni, Antonio Vivaldi, Georg Philipp Telemann, Johann Sebastian Bach, George Frideric Handel, Pietro Locatelli, Giuseppe Tartini, Francesco Geminiani and Johann Joachim Quantz.
The concerto was intended as a composition typical of the Italian style of the time, and all the composers were studying how to compose in the Italian fashion (all'italiana).
The baroque concerto was mainly for a string instrument (violin, viola, cello, seldom viola d'amore or harp) or a wind instrument (oboe, trumpet, flute, or horn).
During the baroque period, before the invention of the piano, keyboard concertos were comparatively rare, with the exception of the organ and some harpsichord concertos by Johann Sebastian Bach. As the harpsichord evolved into the fortepiano, and in the end to the modern piano, the increased volume and the richer sound of the new instrument allowed the keyboard instrument to better compete with a full orchestra.
Cello concertos have been written since the Baroque era if not earlier. Among the works from that period, those by Antonio Vivaldi and Giuseppe Tartini are still part of the standard repertoire today.
Classical concerto.
The concerti of the sons of Johann Sebastian Bach are perhaps the best links between those of the Baroque period and those of the Classical era.
It is conventional to state that the first movements of concerti from the Classical period onwards follow the structure of sonata form. Final movements are often in rondo form, as in J.S. Bach's E Major Violin Concerto.
Violin concertos.
Mozart wrote five violin concertos, in quick succession. They show a number of influences, notably Italian and Austrian. Several passages have leanings towards folk music, as manifested in Austrian serenades.
Haydn wrote four violin concerti.
Beethoven wrote only one violin concerto.
Cello concertos.
Haydn wrote at least two cello concertos which are the most important works in that genre of the classical era. However, C.P.E. Bach’s three cello concertos are also noteworthy.
Keyboard concertos.
C.P.E. Bach’s keyboard concertos contain some brilliant soloistic writing. Some of them have movements that run into one another without a break, and there are frequent cross-movement thematic references.
Mozart, as a boy, made arrangements for harpsichord and orchestra of three sonata movements by Johann Christian Bach. By the time he was twenty, Mozart was able to write concerto ritornelli that gave the orchestra admirable opportunity for asserting its character in an exposition with some five or six sharply contrasted themes, before the soloist enters to elaborate on the material. Some of his twenty-seven piano are considered central in the instrument's repertoire.
Haydn wrote a dozen keyboard concertos, although a couple of them are considered spurious.
Concertos for other instruments.
C.P.E. Bach wrote four flute concertos and two oboe concertos.
Mozart wrote one concerto each for flute, oboe (later rearranged for flute and known as Flute Concerto No. 2), clarinet, and bassoon, four for horn, a Concerto for Flute, Harp, and Orchestra, a Sinfonia Concertante for Violin, Viola and Orchestra, and "Exsultate, jubilate", a "de facto" concerto for soprano voice. They all exploit and explore the characteristics of the solo instrument.
Haydn wrote an important trumpet concerto and a "Sinfonia Concertante" for violin, cello, oboe and bassoon as well as two horn concertos.
Romantic concerto.
Violin concertos.
In the 19th century the concerto as a vehicle for virtuosic display flourished as never before. It was the age in which the artist was seen as hero, to be worshipped and adulated with rapture. Early Romantic traits can be found in the violin concertos of Viotti, but it is Spohr’s twelve violin concertos, written between 1802 and 1827, that truly embrace the Romantic spirit with their melodic as well as their dramatic qualities.
Beethoven’s Violin Concerto is unique in its scale and melodic qualities. Recitative elements are often incorporated, showing the influence of Italian opera on purely instrumental forms.
Mendelssohn opens his violin concerto (1844) with the singing qualities of the violin solo. Even later passage work is dramatic and recitative-like, rather than merely virtuosic. The wind instruments state the lyrical second subject over a low pedal G on the violin – certainly an innovation. The cadenza, placed at the end of the development and acting as a link to the recapitulation, is fully written out and integrated into the structure.
The great violin virtuoso Niccolò Paganini was a legendary figure who, as a composer, exploited the technical potential of his instrument to its very limits. Each one exploits rhapsodic ideas but is unique in its own form. The Belgian violinist Henri Vieuxtemps, himself a major virtuoso, contributed several works to this form.
Édouard Lalo's "Symphonie Espagnole" (1875) displays virtuoso writing with a Spanish flavor.
Max Bruch wrote three violin concertos, but it is the first, in G minor, that has remained a firm favorite in the repertoire. The opening movement relates so closely to the two remaining movements that it functions like an operatic prelude.
Tchaikovsky’s violin concerto (1878) is a powerful work which succeeds in being lyrical as well as superbly virtuosic.
In the same year Brahms wrote his violin concerto for the virtuoso Joseph Joachim. This work makes new demands on the player, so much so that when it was first written it was referred to as a "concerto against the violin". The first movement brings the concerto into the realm of symphonic development. The second movement is traditionally lyrical, and the finale is based on a lively Hungarian theme.
Cello concertos.
Since the Romantic era, the cello has received as much attention as the piano and violin as a concerto instrument, and many great Romantic and even more 20th-century composers left examples.
Antonín Dvořák’s cello concerto ranks among the supreme examples from the Romantic era while Robert Schumann's focuses on the lyrical qualities of the instrument. The instrument was also popular with composers of the Franco-Belgian tradition: Saint-Saëns and Vieuxtemps wrote two cello concertos each and Lalo and Jongen one. Elgar's popular concerto, while written in the early 20th century, belongs to the late romantic period stylistically.
Beethoven contributed to the repertoire with a "Triple Concerto" for piano, violin, cello and orchestra while later in the century, Brahms wrote a "Double Concerto" for violin, cello and orchestra.
Tchaikovsky’s contribution to the genre is a series of Variations on a Rococo Theme. He also left very fragmentary sketches of a projected Cello Concerto. Cellist Yuriy Leonovich and Tchaikovsky researcher Brett Langston published their completion of the piece in 2006.
Carl Reinecke, David Popper and Julius Klengel also wrote cello concertos that were popular in their time and are still played occasionally nowadays.
Today's 'core' repertoire which is performed the most of any cello concertos are by Elgar, Dvořák, Saint-Saëns, Haydn, Shostakovich and Schumann, but there are many more concertos which are performed nearly as often (see below: cello concertos in the 20th century).
Piano concertos.
Beethoven’s five piano concertos increase the technical demands made on the soloist. The last two are particularly remarkable, integrating the concerto into a large symphonic structure with movements that frequently run into one another. His Piano Concerto No. 4 starts, against tradition, with a statement by the piano, after which the orchestra enters in a foreign key, to present what would normally have been the opening tutti. The work has an essentially lyrical character. The slow movement is a dramatic dialogue between the soloist and the orchestra. His Piano Concerto No. 5 has the basic rhythm of a Viennese military march. There is no lyrical second subject, but in its place a continuous development of the opening material. He also wrote a "Triple Concerto" for piano, violin, cello, and orchestra.
The piano concertos of Cramer, Field, Düssek, Woelfl, and Hummel provide a link from the Classical concerto to the Romantic concerto.
Chopin wrote two piano concertos in which the orchestra is very much relegated to an accompanying role. Schumann, despite being a pianist-composer, wrote a piano concerto in which virtuosity is never allowed to eclipse the essential lyrical quality of the work. The gentle, expressive melody heard at the beginning on woodwind and horns (after the piano’s heralding introductory chords) bears the material for most of the argument in the first movement. In fact, argument in the traditional developmental sense is replaced by a kind of variation technique in which soloist and orchestra interweave their ideas.
Liszt's mastery of piano technique matched that of Paganini for the violin. His concertos No. 1 and No. 2 left a deep impression on the style of piano concerto writing, influencing Rubinstein, and especially Tchaikovsky, whose first piano concerto's rich chordal opening is justly famous. Grieg’s concerto likewise begins in a striking manner after which it continues in a lyrical vein.
Brahms's "First Piano Concerto" in D minor (pub 1861) was the result of an immense amount of work on a mass of material originally intended for a symphony. His "Second Piano Concerto" in B♭ major (1881) has four movements and is written on a larger scale than any earlier concerto. Like his violin concerto, it is symphonic in proportions.
Fewer piano concertos were written in the late Romantic Period. But Grieg-inspired Sergei Rachmaninoff wrote 4 piano concertos between 1891 and 1926. His 2nd and 3rd, being the most popular of the 4, went on to become among the most famous in piano repertoire.
Other romantic piano concertos, like Kalkbrenner's, Henri Herz's Moscheles' and Thalberg's concertos were also very popular in the Romantic era, but not today.
Small-scale works.
Besides the usual three-movement works with the title "concerto", many 19th-century composers wrote shorter pieces for solo instrument and orchestra, often bearing descriptive titles. From around 1800 such pieces were often called "Konzertstück" or "Phantasie" by German composers.
Liszt wrote the "Totentanz" for piano and orchestra, a paraphrase of the "Dies Irae". Max Bruch wrote a popular "Scottish Fantasy" for violin and orchestra, César Franck wrote "Les Djinns" and "Variations symphoniques", and Gabriel Fauré wrote a "Ballade" for piano and orchestra. Rachmaninoff's Rhapsody on a Theme of Paganini is widely considered to be structured similarly to a piano concerto.
Tchaikovsky's Variations on a Rococo Theme for cello and orchestra have an important place in the instrument's repertoire.
20th century.
Many of the concertos written in the early 20th century belong more to the late Romantic school than to any modernistic movement. Masterpieces were written by Edward Elgar (a violin concerto and a cello concerto), Sergei Rachmaninoff and Nikolai Medtner (four and three piano concertos, respectively), Jean Sibelius (a violin concerto), Frederick Delius (a violin concerto, a cello concerto, a piano concerto and a double concerto for violin and cello), Karol Szymanowski (two violin concertos and a "Symphonie Concertante" for piano), and Richard Strauss (two horn concertos, a violin concerto, "Don Quixote" —a tone poem which features the cello as a soloist— and among later works, an oboe concerto).
However, in the first decades of the 20th century, several composers such as Debussy, Schoenberg, Berg, Hindemith, Stravinsky, Prokofiev and Bartók started experimenting with ideas that were to have far-reaching consequences for the way music is written and, in some cases, performed. Some of these innovations include a more frequent use of modality, the exploration of non-western scales, the development of atonality, the wider acceptance of dissonances, the invention of the twelve-tone technique of composition and the use of polyrhythms and complex time signatures.
These changes also affected the concerto as a musical form. Beside more or less radical effects on musical language, they led to a redefinition of the concept of virtuosity in order to include new and extended instrumental techniques as well as a focus on aspects of sound that had been neglected or even ignored before such as pitch, timbre and dynamics. In some cases, they also brought about a new approach to the role of the soloist and its relation to the orchestra.
Violin concertos.
Two great innovators of early 20th-century music, Schoenberg and Stravinsky, both wrote violin concertos. The material in Schoenberg’s concerto, like that in Berg’s, is linked by the twelve-tone serial method. Bartók, another major 20th-century composer, wrote two important concertos for violin. Russian composers Prokofiev and Shostakovich both wrote two concertos while Khachaturian wrote a concerto and a Concerto-Rhapsody for the instrument. Hindemith’s concertos hark back to the forms of the 19th century, even if the harmonic language which he used was different.
Three violin concertos from David Diamond show the form in neoclassical style.
More recently, Dutilleux's "L'Arbre des Songes" has proved an important addition to the repertoire and a fine example of the composer's atonal yet melodic style.
Other composers of major violin concertos include Jean Sibelius, Ralph Vaughan Williams, Samuel Barber, Walton, Benjamin Britten, Frank Martin, Carl Nielsen, Paul Hindemith, Alfred Schnittke, György Ligeti, Philip Glass, Dmitri Shostakovich, Sergei Prokofiev, Aram Khachaturian, Bela Bartok and John Adams.
Cello concertos.
In the 20th century, particularly after the Second World War, the cello enjoyed an unprecedented popularity. As a result, its concertante repertoire caught up with those of the piano and the violin both in terms of quantity and quality.
An important factor in this phenomenon was the rise of virtuoso cellist Mstislav Rostropovich. His outstanding technique and passionate playing prompted dozens of composers to write pieces for him, first in his native Soviet Union and then abroad. His creations include such masterpieces as Sergei Prokofiev's Symphony-Concerto, Dmitri Shostakovich's two cello concertos, Benjamin Britten's Cello-Symphony (which emphasizes, as its title suggests, the equal importance of soloist and orchestra), Henri Dutilleux' "Tout un monde lointain...", Witold Lutosławski's cello concerto, Dmitri Kabalevsky's two cello concertos, Aram Khachaturian's "Concerto-Rhapsody", Arvo Pärt's "Pro et Contra", Alfred Schnittke, André Jolivet and Krzysztof Penderecki second cello concertos, Sofia Gubaidulina's "Canticles of the Sun", Luciano Berio's "Ritorno degli Snovidenia", Leonard Bernstein's "Three Meditations", James MacMillan's cello concerto and Olivier Messiaen's "Concert à quatre" (a quadruple concerto for cello, piano, oboe, flute and orchestra).
In addition, several important composers who were not directly influenced by Rostropovich wrote cello concertos: György Ligeti, Alexander Glazunov, Paul Hindemith, Toru Takemitsu, Darius Milhaud, Arthur Honegger, Nikolai Myaskovsky, Samuel Barber, Joaquín Rodrigo, Elliot Carter, Erich Wolfgang Korngold, William Walton, Heitor Villa-Lobos, Hans Werner Henze, Bernd Alois Zimmermann and Einojuhani Rautavaara for instance.
Piano concertos.
Igor Stravinsky wrote three works for solo piano and orchestra: Concerto for Piano and Wind Instruments, Capriccio for Piano and Orchestra, and Movements for Piano and Orchestra. Sergei Prokofiev, another Russian composer, wrote no less than five piano concertos which he himself performed. Dmitri Shostakovich composed two. Fellow soviet composer Aram Khachaturian contributed to the repertoire with a piano concerto and a Concerto-Rhapsody.
Arnold Schoenberg’s "Piano Concerto" is a well-known example of a dodecaphonic piano concerto.
Béla Bartók also wrote three piano concertos. Like their violin counterparts, they show the various stages in his musical development. Bartok's also rearranged his chamber piece, Sonata for Two Pianos and Percussion, into a "Concerto for Two Pianos and Percussion", adding orchestral accompaniment.
Ralph Vaughan Williams wrote a concerto for piano (in fact a reworking of a concerto for two pianos - both versions have been recorded) while Benjamin Britten's concerto for piano (1938) is a prominent work from his early period.
György Ligeti's concerto (1988) has a synthetic quality: it mixes complex rhythms, the composer's Hungarian roots and his experiments with micropolyphony from the 1960s and 70's. Witold Lutoslawski's piano concerto, completed in the same year, alternates between playfulness and mystery. It also displays a partial return to melody after the composer's aleatoric period.
Russian composer Rodion Shchedrin has written six piano concertos. Finnish composer Einojuhani Rautavaara wrote three piano concertos, the third one dedicated to Vladimir Ashkenazy, who played and conducted the world première.
Concertos for other instruments.
The 20th century also witnessed a growth of the concertante repertoire of instruments, some of which had seldom or never been used in this capacity. As a result, almost all classical instruments now have a concertante repertoire. Examples include:
Among the works of the prolific composer Alan Hovhaness may be noted "Prayer of St. Gregory" for trumpet and strings.
Today the concerto tradition has been continued by composers such as Maxwell Davies, whose series of Strathclyde Concertos exploit some of the instruments less familiar as soloists.
Concertos for orchestra or concert band.
In the 20th and 21st centuries, several composers wrote concertos for orchestra or concert band. In these works, different sections and/or instruments of the orchestra or concert band are treated at one point or another as soloists with emphasis on solo sections and/or instruments changing during the piece. Some examples include those written by:
Orchestra:
Dutilleux has also described his "Métaboles" as a concerto for orchestra, while Britten's well-known pedagogical work "The Young Person's Guide to the Orchestra" is essentially a concerto for orchestra in all but name.
Concert band:
Concertos for two or more instruments.
Many composers also wrote concertos for two or more soloists.
In the Baroque era:
In the Classical era:
In the Romantic era:
In the 20th century:
In the 21st century:

</doc>
<doc id="44118" url="http://en.wikipedia.org/wiki?curid=44118" title="Sonata">
Sonata

Sonata (; Italian: ], pl. "sonate"; from Latin and Italian: "sonare", "to sound"), in music, literally means a piece "played" as opposed to a cantata (Latin and Italian "cantare", "to sing"), a piece "sung". The term, being vague, evolved through the history of music, designating a variety of forms until the Classical era, when it took on increasing importance, and by the early 19th century came to represent a principle of composing large scale works. It was applied to most instrumental genres and regarded—alongside the fugue—as one of two fundamental methods of organizing, interpreting and analyzing concert music. Though the musical style of sonatas has changed since the Classical era, most 20th- and 21st-century sonatas still maintain the same structure.
Usage of "sonata".
The term sonatina, literally "small sonata", is often used for a short or technically easy sonata.
Instrumentation.
In the Baroque period, a sonata was for one or more instruments almost always with continuo. After the Baroque period most works designated as sonatas specifically are performed by a solo instrument, most often a keyboard instrument, or by a solo instrument accompanied by a keyboard instrument. In the late Baroque and early Classical periods, a work with instrument and written-out keyboard part was referred to as having an obbligato keyboard part, in order to distinguish this from use of an instrument or instruments as continuo, though this fell out of usage by the early 19th century.
Sonatas for a solo instrument other than keyboard have been composed, as have sonatas for other combinations of instruments.
Brief history of the usage of sonata.
The Baroque sonata.
In the works of Arcangelo Corelli and his contemporaries, two broad classes of sonata were established, and were first described by Sébastien de Brossard in his "Dictionaire de musique" (third edition, Amsterdam, ca. 1710): the sonata da chiesa (that is, suitable for use in church), which was the type "rightly known as "Sonatas"", and the sonata da camera (proper for use at court), which consists of a prelude followed by a succession of dances, all in the same key. Although the four, five, or six movements of the sonata da chiesa are also most often in one key, one or two of the internal movements are sometimes in a contrasting tonality .
The sonata da chiesa, generally for one or more violins and bass, consisted normally of a slow introduction, a loosely fugued allegro, a slow movement, and a lively finale in some binary form suggesting affinity with the dance-tunes of the suite. This scheme, however, was not very clearly defined, until the works of Arcangelo Corelli when it became the essential sonata and persisted as a tradition of Italian violin music—even into the early 19th century, in the works of Boccherini.
The sonata da camera consisted almost entirely of idealized dance-tunes. On the other hand, the features of "sonata da chiesa" and "sonata da camera" then tended to be freely intermixed. Although nearly half of Bach's 1,100 surviving compositions, arrangements, and transcriptions are instrumental works, only about 4% are sonatas .
The term "sonata" is also applied to the series of over 500 works for harpsichord solo, or sometimes for other keyboard instruments, by Domenico Scarlatti, originally published under the name "Essercizi per il gravicembalo" (Exercises for the Harpsichord). Most of these pieces are in one binary-form movement only, with two parts that are in the same tempo and use the same thematic material, though occasionally there will be changes in tempo within the sections. They are frequently virtuosic, and use more distant harmonic transitions and modulations than were common for other works of the time. They were admired for their great variety and invention.
Both the solo and trio sonatas of Vivaldi show parallels with the concerti he was writing at the same time. He composed over 70 sonatas, the great majority of which are of the solo type; most of the rest are trio sonatas, and a very small number are of the multivoice type .
The sonatas of Domenico Paradies are mild and elongated works with a graceful and melodious little second movement included.
The sonata in the Classical period.
The practice of the Classical period would become decisive for the sonata; the term moved from being one of many terms indicating genres or forms, to designating the fundamental form of organization for large-scale works. This evolution stretched over fifty years. The term came to apply both to the structure of individual movements (see Sonata form and History of sonata form) and to the layout of the movements in a multi-movement work. In the transition to the Classical period there were several names given to multimovement works, including divertimento, serenade, and partita, many of which are now regarded effectively as sonatas. The usage of "sonata" as the standard term for such works began somewhere in the 1770s. Haydn labels his first piano sonata as such in 1771, after which the term "divertimento" is used sparingly in his output. The term "sonata" was increasingly applied to either a work for keyboard alone (see piano sonata), or for keyboard and one other instrument, often the violin or cello. It was less and less frequently applied to works with more than two instrumentalists; for example piano trios were not often labelled "sonata for piano, violin, and cello."
Initially the most common layout of movements was:
However, two-movement layouts also occur, a practice Haydn uses as late as the 1790s. There was also in the early Classical period the possibility of using four movements, with a dance movement inserted before the slow movement, as in Haydn's Piano sonatas No. 6 and No. 8. Mozart's sonatas were also primarily in three movements. Of the works that Haydn labelled "piano sonata", "divertimento", or "partita" in Hob XIV, seven are in two movements, thirty-five are in three, and three are in four; and there are several in three or four movements whose authenticity is listed as "doubtful." Composers such as Boccherini would publish sonatas for piano and obbligato instrument with an optional third movement—–in Boccherini's case, 28 cello sonatas.
But increasingly instrumental works were laid out in four, not three movements, a practice seen first in string quartets and symphonies, and reaching the sonata proper in the early sonatas of Beethoven. However, two- and three-movement sonatas continued to be written throughout the Classical period: Beethoven's opus 102 pair has a two-movement C major sonata and a three-movement D major sonata. Nevertheless, works with fewer or more than four movements were increasingly felt to be exceptions; they were labelled as having movements "omitted," or had "extra" movements. 
Thus, the four-movement layout was by this point standard for the string quartet, and overwhelmingly the most common for the symphony. The usual order of the four movements was:
When movements appeared out of this order they would be described as "reversed", such as the scherzo coming before the slow movement in Beethoven's 9th Symphony. This usage would be noted by critics in the early 19th century, and it was codified into teaching soon thereafter.
It is difficult to overstate the importance of Beethoven's output of sonatas: 32 piano sonatas, plus sonatas for cello and piano or violin and piano, forming a large body of music that would over time increasingly be thought essential for any serious instrumentalist to master.
The sonata in the Romantic period.
In the early 19th century the current usage of the term "sonata" was established, both as regards form "per se", and in the sense that a fully elaborated sonata serves as a norm for concert music in general, which other forms are seen in relation to. From this point forward, the word "sonata" in music theory labels as much the abstract musical form as particular works. Hence there are references to a symphony as a "sonata for orchestra". This is referred to by William Newman as the "sonata idea".
Among works expressly labeled "sonata" for the piano, there are the three of Frédéric Chopin, those of Felix Mendelssohn, the three of Robert Schumann, Franz Liszt's Sonata in B Minor, and later the sonatas of Johannes Brahms and Sergei Rachmaninoff.
In the early 19th century the sonata form was defined, from a combination of previous practice and the works of important Classical composers, particularly Haydn, Mozart, Beethoven, but composers such as Clementi also. All works not explicitly labeled "sonata" were an expression of the same governing structural practice. It is during this period that the differences between the three- and the four-movement layouts became a subject of commentary, with emphasis on the concerto being laid out in three movements, and the symphony in four. The four-movement form was deemed the superior layout. The concerto form was Italianate, while the four-movement form's predominance was ascribed to Haydn, and was considered German.
Ernest Newman wrote in the essay "Brahms and the Serpent":
In this view the sonata called for no explicit analysis in Haydn, Mozart, and Beethoven's era, in the same sense that Bach "knew" what a fugue was and how to compose one, whereas later composers were bound by an "academic" sense of form that was not well suited to the Romantic era's more frequent and more rapid modulations.
The sonata after the Romantic period.
The role of the sonata as an extremely important form of extended musical argument would inspire composers such as Hindemith, Prokofiev, Shostakovich to compose in sonata form, and works with traditional sonata structures continue to be composed and performed.
The sonata in scholarship and musicology.
The sonata idea or principle.
Research into the practice and meaning of sonata form, style, and structure has been the motivation for important theoretical works by Heinrich Schenker, Arnold Schoenberg, and Charles Rosen among others; and the pedagogy of music continued to rest on an understanding and application of the rules of sonata form as almost two centuries of development in practice and theory had codified it.
The development of the classical style and its norms of composition formed the basis for much of the music theory of the 19th and 20th centuries. As an overarching formal principle, sonata was accorded the same central status as Baroque fugue; generations of composers, instrumentalists, and audiences were guided by this understanding of sonata as an enduring and dominant principle in Western music. The sonata idea begins before the term had taken on its present importance, along with the evolution of the Classical period's changing norms. The reasons for these changes, and how they relate to the evolving sense of a new formal order in music, is a matter to which research is devoted. Some common factors which were pointed to include: the shift of focus from vocal music to instrumental music; changes in performance practice, including the loss of the continuo .
Crucial to most interpretations of the sonata form is the idea of a tonal center; and, as the "Grove Concise Dictionary of Music" puts it: "The main form of the group embodying the 'sonata principle', the most important principle of musical structure from the Classical period to the 20th century: that material first stated in a complementary key be restated in the home key" (, ).
The sonata idea has been thoroughly explored by William Newman in his monumental three-volume work "Sonata in the Classic Era (A History of the Sonata Idea)", begun in the 1950s and published in what has become the standard edition of all three volumes in 1972.
20th-century theory.
Heinrich Schenker argued that there was an "Urlinie" or basic tonal melody, and a basic bass figuration. He held that when these two were present, there was basic structure, and that the sonata represented this basic structure in a whole work with a process known as "interruption" .
As a practical matter, Schenker applied his ideas to the editing of the piano sonatas of Beethoven, using original manuscripts and his own theories to "correct" the available sources. The basic procedure was the use of tonal theory to infer meaning from available sources as part of the critical process, even to the extent of completing works left unfinished by their composers. While many of these changes were and are controversial, that procedure has a central role today in music theory, and is an essential part of the theory of sonata structure as taught in most music schools.

</doc>
<doc id="44120" url="http://en.wikipedia.org/wiki?curid=44120" title="Jelly Roll Morton">
Jelly Roll Morton

Ferdinand Joseph LaMothe (October 20, 1890 – July 10, 1941), known professionally as Jelly Roll Morton, was an American ragtime and early jazz pianist, bandleader and composer who started his career in New Orleans, Louisiana.
Widely recognized as a pivotal figure in early jazz, Morton is perhaps most notable as jazz's first arranger, proving that a genre rooted in improvisation could retain its essential spirit and characteristics when notated. His composition "Jelly Roll Blues" was the first published jazz composition, in 1915. Morton is also notable for naming and popularizing the "Spanish Tinge" (habanera rhythm and tresillo), and for writing such standards as "King Porter Stomp", "Wolverine Blues", "Black Bottom Stomp", and "I Thought I Heard Buddy Bolden Say", the last a tribute to New Orleans musicians from the turn of the 19th century to 20th century.
Reputed for his arrogance and self-promotion as often as recognized in his day for his musical talents, Morton claimed to have invented jazz outright in 1902—much to the derision of later musicians and critics. The jazz historian, musician, and composer Gunther Schuller says of Morton's "hyperbolic assertions" that there is "no proof to the contrary" and that Morton's "considerable accomplishments in themselves provide reasonable substantiation". However, the scholar Katy Martin has argued that Morton's bragging was exaggerated by Alan Lomax in the book "Mister Jelly Roll", and this portrayal has influenced public opinion and scholarship on Morton since.
Biography.
Early life and education.
Morton was born into a creole of color family in the Faubourg Marigny neighborhood of downtown New Orleans, Louisiana. Sources differ as to his birth date: a baptismal certificate issued in 1894 lists his date of birth as October 20, 1890; Morton and his half-sisters claimed he was born on September 20, 1885. His World War I draft registration card showed September 13, 1884, but his California death certificate listed his birth as September 20, 1889. He was born to F. P. Lamothe and Louise Monette (written as Lemott and Monett on his baptismal certificate). Eulaley Haco (Eulalie Hécaud) was the godparent. Hécaud helped choose his christening name of Ferdinand. His parents lived in a common-law marriage and were not legally married. No birth certificate has been found to date.
Ferdinand started playing music as a child, showing early talent. After his parents separated, his mother married a man named Mouton. Ferdinand took his stepfather's name and anglicized it as "Morton"
Musical career.
At the age of fourteen, Morton began working as a piano player in a brothel (or, as it was referred to then, a sporting house). While working there, he was living with his religious, church-going great-grandmother; he had her convinced that he worked as a night watchman in a barrel factory.
In that atmosphere, he often sang smutty lyrics; he took the nickname "Jelly Roll", which was black slang for female genitalia.
After Morton's grandmother found out that he was playing jazz in a local brothel, she kicked him out of her house. 
He said:
When my grandmother found out that I was playing jazz in one of the sporting houses in the District, she told me that I had disgraced the family and forbade me to live at the house... She told me that devil music would surely bring about my downfall, but I just couldn't put it behind me.
Tony Jackson, also a pianist at brothels and an accomplished guitar player, was a major influence on Morton's music. Jelly Roll said that Jackson was the only pianist better than he was.
Touring.
Around 1904, Morton also started touring in the American South, working with minstrel shows, gambling and composing. His works "Jelly Roll Blues", "New Orleans Blues", "Frog-I-More Rag", "Animule Dance", and "King Porter Stomp" were composed during this period. He got to Chicago in 1910 and New York City in 1911, where future stride greats James P. Johnson and Willie "The Lion" Smith caught his act, years before the blues were widely played in the North.
In 1912–1914, Morton toured with his girlfriend Rosa Brown as a vaudeville act before settling in Chicago for three years. By 1914, he had started writing down his compositions. In 1915 his "Jelly Roll Blues" was arguably the first jazz composition ever published, recording as sheet music the New Orleans traditions that had been jealously guarded by the musicians. In 1917, he followed bandleader William Manuel Johnson and Johnson's sister Anita Gonzalez to California, where Morton's tango, "The Crave", made a sensation in Hollywood.
Vancouver.
Morton was invited to play a new Vancouver, British Columbia nightclub called The Patricia, on East Hastings Street. The jazz historian Mark Miller described his arrival as "an extended period of itinerancy as a pianist, vaudeville performer, gambler, hustler, and, as legend would have it, pimp".
Chicago.
Morton returned to Chicago in 1923 to claim authorship of his recently published rag, "The Wolverines", which had become a hit as "Wolverine Blues" in the Windy City. He released the first of his commercial recordings, first as piano rolls, then on record, both as a piano soloist and with various jazz bands.
In 1926, Morton succeeded in getting a contract to record for the largest and most prestigious company in the United States, Victor. This gave him a chance to bring a well-rehearsed band to play his arrangements in Victor's Chicago recording studios. These recordings by "Jelly Roll Morton & His Red Hot Peppers" are regarded as classics of 1920s jazz. The Red Hot Peppers featured such other New Orleans jazz luminaries as Kid Ory, Omer Simeon, George Mitchell, Johnny St. Cyr, Barney Bigard, Johnny Dodds, Baby Dodds, and Andrew Hilaire. Jelly Roll Morton & His Red Hot Peppers were one of the first acts booked on tours by MCA.
Marriage and family.
In November 1928, Morton married the showgirl Mabel Bertrand in Gary, Indiana.
New York City.
They moved that year to New York City, where Morton continued to record for Victor. His piano solos and trio recordings are well regarded, but his band recordings suffer in comparison with the Chicago sides, where Morton could draw on many great New Orleans musicians for sidemen. Although he recorded with the noted musicians clarinetists Omer Simeon, George Baquet, Albert Nicholas, Wilton Crawley, Barney Bigard, Lorenzo Tio and Artie Shaw, trumpeters Bubber Miley, Johnny Dunn and Henry "Red" Allen, saxophonists Sidney Bechet, Paul Barnes and Bud Freeman, bassist Pops Foster, and drummers Paul Barbarin, Cozy Cole and Zutty Singleton, Morton generally had trouble finding musicians who wanted to play his style of jazz. His New York sessions failed to produce a hit.
With the Great Depression and the near collapse of the record industry, Victor did not renew Morton's recording contract for 1931. Morton continued playing in New York, but struggled financially. He briefly had a radio show in 1934, then took on touring in the band of a traveling burlesque act for some steady income. His compositions were recorded by the musicians Fletcher Henderson, Benny Goodman and others, but he received no royalties from these recordings.
Washington, D.C..
In 1935, Morton moved to Washington, D.C., to become the manager/piano player of a bar called, at various times, the "Music Box", "Blue Moon Inn", and "Jungle Inn" in the African-American neighborhood of Shaw. (The building that hosted the nightclub stands at 1211 U Street NW.) Morton was also the master of ceremonies, bouncer, and bartender of the club. He lived in Washington for a few years; the club owner allowed all her friends free admission and drinks, which prevented Morton from making the business a success.
In 1938 Morton was stabbed by a friend of the owner and suffered wounds to the head and chest. After this incident his wife Mabel demanded that they leave Washington.
During Morton's brief residency at the Music Box, the folklorist Alan Lomax heard the pianist playing in the bar. In May 1938, Lomax invited Morton to record music and interviews for the Library of Congress. The sessions, originally intended as a short interview with musical examples for use by music researchers in the Library of Congress, soon expanded to record more than eight hours of Morton talking and playing piano. Lomax also conducted longer interviews during which he took notes but did not record. Despite the low fidelity of these non-commercial recordings, their musical and historical importance have attracted numerous jazz fans, and they have helped to ensure Morton's place in jazz history.
Lomax was very interested in Morton's Storyville days in New Orleans and the ribald songs of the time. Although reluctant to recount and record these, Morton eventually obliged Lomax. Because of the suggestive nature of the songs, some of the Library of Congress recordings were not released until 2005.
In his interviews, Morton claimed to have been born in 1885. He was aware that if he had been born in 1890, he would have been slightly too young to make a good case as the inventor of jazz. He said in the interview that Buddy Bolden played ragtime but not jazz; this is not accepted by the consensus of Bolden's other New Orleans contemporaries. The contradictions may stem from different definitions for the terms "ragtime" and "jazz". These interviews, released in different forms over the years, were released on an eight-CD boxed set in 2005, "The Complete Library of Congress Recordings". This collection won two Grammy Awards. The same year, Morton was honored with the Grammy Lifetime Achievement Award.
Later years.
When Morton was stabbed and wounded, a nearby whites-only hospital refused to treat him, as the city had racially segregated facilities. He was transported to a black hospital farther away. When he was in the hospital, the doctors left ice on his wounds for several hours before attending to his eventually fatal injury. His recovery from his wounds was incomplete, and thereafter he was often ill and easily became short of breath. Morton made a new series of commercial recordings in New York, several recounting tunes from his early years that he discussed in his Library of Congress interviews. 
Worsening asthma sent him to a New York hospital for three months at one point. He continued to suffer from respiratory problems when visiting Los Angeles with a series of manuscripts of new tunes and arrangements, planning to form a new band and restart his career. Morton died on July 10, 1941 after an eleven-day stay in Los Angeles County General Hospital.
According to the jazz historian David Gelly in 2000, Morton's arrogance and "bumptious" persona alienated so many musicians over the years that no colleagues or admirers attended his funeral. But, a contemporary news account of the funeral in the August 1, 1941 issue of "Downbeat" says that fellow musicians Kid Ory, Mutt Carey, Fred Washington and Ed Garland were among his pall bearers. The story notes the absence of Duke Ellington and Jimmie Lunceford, both of whom were appearing in Los Angeles at the time. (The article is reproduced in Alan Lomax's biography of Morton, "Mister Jelly Roll", University of California Press, 1950.)
Piano style.
Morton's piano style was formed from early secondary ragtime and "shout", which also evolved separately into the New York school of stride piano. Morton's playing was also close to barrelhouse, which produced boogie woogie.
Morton often played the melody of a tune with his right thumb, while sounding a harmony above these notes with other fingers of the right hand. This added a rustic or "out-of-tune" sound (due to the playing of a diminished 5th above the melody). This may still be recognized as belonging to New Orleans. Morton also walked in major and minor sixths in the bass, instead of tenths or octaves. He played basic swing rhythms in both the left and right hand.
Compositions.
Some of Morton's songs (listed alphabetically):
Several of Morton's compositions were musical tributes to himself, including "Winin' Boy", "The Jelly Roll Blues", subtitled "The Original Jelly-Roll"; and "Mr. Jelly Lord". In the Big Band era, his "King Porter Stomp", which Morton had written decades earlier, was a big hit for Fletcher Henderson and Benny Goodman; it became a standard covered by most other swing bands of that time. Morton claimed to have written some tunes that were copyrighted by others, including "Alabama Bound" and "Tiger Rag". "Sweet Peter," which Morton recorded in 1926, appears to be the source for the melody of the hit song "All Of Me," ostensibly written by Gerald Marks and Seymour Simons in 1931.
His musical influence continues in the work of Dick Hyman and Reginald Robinson.

</doc>
<doc id="44122" url="http://en.wikipedia.org/wiki?curid=44122" title="American Beauty (1999 film)">
American Beauty (1999 film)

 
American Beauty is a 1999 American romantic dark comedy drama film, directed by Sam Mendes and written by Alan Ball. Kevin Spacey stars as Lester Burnham, an office worker who has a midlife crisis when he becomes infatuated with his teenage daughter's best friend, Angela (Mena Suvari). Annette Bening co-stars as Lester's materialistic wife, Carolyn, and Thora Birch plays their insecure daughter, Jane. Wes Bentley, Chris Cooper and Allison Janney also feature. The film has been described by academics as a satire of American middle class notions of beauty and personal satisfaction; analysis has focused on the film's explorations of romantic and paternal love, sexuality, beauty, materialism, self-liberation, and redemption.
Ball began writing "American Beauty" as a play in the early 1990s, partly inspired by the media circus around the Amy Fisher trial in 1992. He shelved the play after realizing the story would not work on stage. After several years as a television screenwriter, Ball revived the idea in 1997 when attempting to break into the film industry. The modified script had a cynical outlook that was influenced by Ball's frustrating tenures writing for several sitcoms. Producers Dan Jinks and Bruce Cohen took "American Beauty" to DreamWorks; the then-fledgling film studio bought Ball's script for $250,000, outbidding several other production bodies. DreamWorks financed the $15 million production and served as its North American distributor. "American Beauty" marked acclaimed theater director Mendes' film debut; courted after his successful productions of the musicals "Oliver!" and "Cabaret", Mendes was nevertheless only given the job after twenty others were considered and several "A-list" directors turned down the opportunity.
Spacey was Mendes' first choice for the role of Lester, even though DreamWorks had urged the director to consider better-known actors; similarly, the studio suggested several actors for the role of Carolyn until Mendes offered the part to Bening without DreamWorks' knowledge. Principal photography took place between December 1998 and February 1999 on soundstages at the Warner Bros. backlot in Burbank, California and on location in Los Angeles. Mendes' dominant style was deliberate and composed; he made extensive use of static shots and slow pans and zooms to generate tension. Cinematographer Conrad Hall complemented Mendes' style with peaceful shot compositions to contrast with the turbulent on-screen events. During editing, Mendes made several changes that gave the film a less cynical tone than the script.
Released in North America on September 15, 1999, "American Beauty" was positively received by critics and audiences; it was the best-reviewed American film of the year and grossed over $350 million worldwide. Reviewers praised most aspects of the production, with particular emphasis on Mendes, Spacey and Ball; criticism tended to focus on the familiarity of the characters and setting. DreamWorks launched a major campaign to increase "American Beauty"‍ '​s chances of Academy Award success; at the 72nd Academy Awards the following year, the film won Best Picture, Best Director, Best Actor (for Spacey), Best Original Screenplay and Best Cinematography. The film was nominated for and won many other awards and honors, mainly for the direction, writing and acting.
Plot.
Forty-two-year-old advertising executive and magazine journalist Lester Burnham despises his job writing for a magazine published by the advertising agency by which he is employed. His wife, Carolyn, is an ambitious and materialistic real estate broker while their sixteen-year-old daughter, Jane, abhors her parents and has low self-esteem. The Burnhams' new neighbors are retired United States Marine Corps Colonel Frank Fitts and his introverted wife, Barbara. Their teenage son, Ricky, obsessively films his surroundings with a camcorder, keeping a collection of recordings on video tapes in his bedroom. He also secretly deals marijuana, using a job as a part-time bar caterer as a front. Having been previously forced into a military academy and a psychiatric hospital, Ricky is subjected by Frank to a strict disciplinarian lifestyle. Jim Olmeyer and Jim Berkley, a gay couple who live nearby, welcome the family to the neighborhood; Frank later reveals his homophobia when angrily discussing the incident with Ricky.
Lester becomes infatuated with Jane's vain cheerleader friend, Angela Hayes, after seeing her perform a half-time dance routine at a high school basketball game. He starts having sexual fantasies about Angela, during which red rose petals are a recurring motif. Carolyn begins an affair with a business rival, Buddy Kane, unknown to Lester. Lester is told he is to be laid off, but in an "epiphany" decides to blackmail Brad, the agency's editor-in-chief, for $60,000 and quits his job, taking employment serving fast food. He trades his Toyota Camry for a Pontiac Firebird sports car and starts working out after he overhears Angela tell Jane that she would find him sexually attractive if he improved his physique. He begins smoking marijuana supplied by Ricky, and flirts with Angela whenever she visits Jane. The girls' friendship cools when Jane becomes involved with Ricky, who seemed interested in Jane from the start. Jane and Ricky bond over what the latter considers the most beautiful imagery he has filmed: a plastic bag being blown in the wind.
Lester discovers Carolyn's infidelity when the two arrive at the fast food drive-thru, but reacts indifferently. Buddy ends the affair, fearing an expensive divorce. Frank becomes suspicious of Lester and Ricky's friendship and later finds his son's footage of Lester lifting weights while nude, which Ricky captured by chance, leading him to think that the two are homosexual. After spying on Ricky and Lester's drug affair through the Burhams' garage window, wherein he thinks Ricky is performing oral sex on Lester, Frank mistakenly concludes the pair are sexually involved. He later beats Ricky and accuses him of being gay. Ricky, in retaliation, goads his father into kicking him out of their home by falsely admitting the charge as he sees this accusation as a chance to finally be free from him. He goes to Jane, finding her arguing with Angela about her friend's flirtation with Lester. Ricky convinces Jane to flee with him to New York City and accuses Angela of being boring and ordinary, as well as using and berating Jane to make herself feel better and to boost her public image.
Carolyn loads a gun and begins to drive home, upset that the affair is over. Frank, having come to terms with his own homosexuality, confronts Lester and attempts to kiss him; Lester rebuffs Frank, who flees. Lester finds a distraught Angela sitting alone in the dark, and she asks him to tell her she is beautiful. He does, and she begins to seduce him. When the two nearly have sex, Angela reveals her virginity, though she has told Jane multiple times about her "sex life". Upon learning this, Lester quits the act and comforts her, and the pair instead bond over their shared frustrations. Angela goes to the bathroom and Lester smiles at a family photograph in his kitchen as an unseen person presses a gun to the back of his head. A gunshot sounds and blood sprays on the wall. Ricky and Jane find Lester's body, while Carolyn is seen crying in the closet. A bloodied Frank returns home to reveal that a gun is missing from his collection. Lester's closing narration describes meaningful experiences during his life; he says that, despite his death, he is happy because there is so much beauty in the world.
Themes and analysis.
Multiple interpretations.
Scholars and academics have offered many possible readings of "American Beauty"; film critics are similarly divided, not so much about the quality of the film as their interpretations of it. Described by many as about "the meaning of life" or "the hollow existence of the American suburbs", the film has defied categorization by even the filmmakers. Mendes is indecisive, saying the script seemed to be about something different each time he read it: "a mystery story, a kaleidoscopic journey through American suburbia, a series of love stories; [...] it was about imprisonment, [...] loneliness [and] beauty. It was funny; it was angry, sad." The literary critic and author Wayne C. Booth concludes that the film resists any one interpretation: "["American Beauty"] cannot be adequately summarized as 'here is a satire on what's wrong with American life'; that plays down the celebration of beauty. It is more tempting to summarize it as 'a portrait of the beauty underlying American miseries and misdeeds'; but that plays down the scenes of cruelty and horror, and Ball's disgust with our mores. It cannot be summarized with either Lester's or Ricky's philosophical statements about what life is or how one should live." He argues that the problem of interpreting the film is tied with that of finding its center—a controlling voice who "[unites] all of the choices". He contends that in "American Beauty"‍ '​s case it is neither Mendes nor Ball. Mendes considers the voice to be Ball's, but even while the writer was "strongly influential" on set, he often had to accept deviations from his vision, particularly ones that transformed the cynical tone of his script into something more optimistic. With "innumerable voices intruding on the original author's," Booth says, those who interpret "American Beauty" "have forgotten to probe for the elusive center". According to Booth, the film's true controller is the creative energy "that hundreds of people put into its production, agreeing and disagreeing, inserting and cutting".
Imprisonment and redemption.
Mendes called "American Beauty" a rites of passage film about imprisonment and escape from imprisonment. The monotony of Lester's existence is established through his gray, nondescript workplace and characterless clothing. In these scenes, he is often framed as if trapped, "reiterating rituals that hardly please him". He masturbates in the confines of his shower; the shower stall evokes a jail cell and the shot is the first of many where Lester is confined behind bars or within frames, such as when he is reflected behind columns of numbers on a computer monitor, "confined [and] nearly crossed out". The academic and author Jody W. Pennington argues that Lester's journey is the story's center. His sexual reawakening through meeting Angela is the first of several turning points as he begins to "[throw] off the responsibilities of the comfortable life he has come to despise". After Lester shares a joint with Ricky, his spirit is released and he begins to rebel against Carolyn. Changed by Ricky's "attractive, profound confidence", Lester is convinced that Angela is attainable and sees that he must question his "banal, numbingly materialist suburban existence"; he takes a job at a fast-food outlet, which allows him to regress to a point when he could "see his whole life ahead of him".
When Lester is caught masturbating by Carolyn, his angry retort about their lack of intimacy is the first time he says aloud what he thinks about her. By confronting the issue and Carolyn's "superficial investments in others", Lester is trying to "regain a voice in a home that [only respects] the voices of mother and daughter". His final turning point comes when he and Angela almost have sex; after she confesses her virginity, he no longer thinks of her as a sex object, but as a daughter. He holds her close and "wraps her up". Mendes called it "the most satisfying end to [Lester's] journey there could possibly have been". With these final scenes, Mendes intended to show Lester at the conclusion of a "mythical quest". After Lester gets a beer from the refrigerator, the camera pushes toward him, then stops facing a hallway down which he walks "to meet his fate". Having begun to act his age again, Lester achieves closure. As he smiles at a family photo, the camera pans slowly from Lester to the kitchen wall, onto which blood spatters as a gunshot rings out; the slow pan reflects the peace of Lester's death. His body is discovered by Jane and Ricky. Mendes said that Ricky's staring into Lester's dead eyes is "the culmination of the theme" of the film: that beauty is found where it is least expected.
Conformity and beauty.
Like other American films of 1999—such as "Fight Club", "Bringing Out the Dead" and "Magnolia"—"American Beauty" instructs its audience to "[lead] more meaningful lives". The film argues the case against conformity, but does not deny that people need and want it; even the gay characters just want to fit in. Jim and Jim, the Burnhams' other neighbors, are a satire of "gay bourgeois coupledom", who "[invest] in the numbing sameness" that the film criticizes in heterosexual couples. The feminist academic and author Sally R. Munt argues that "American Beauty" uses its "art house" trappings to direct its message of non-conformity primarily to the middle classes, and that this approach is a "cliché of bourgeois preoccupation; [...] the underlying premise being that the luxury of finding an individual 'self' through denial and renunciation is always open to those wealthy enough to choose, and sly enough to present themselves sympathetically as a rebel."
Professor Roy M. Anker argues that the film's thematic center is its direction to the audience to "look closer". The opening combines an unfamiliar viewpoint of the Burnhams' neighborhood with Lester's narrated admission that he will soon die, forcing audiences to consider their own mortality and the beauty around them. It also sets a series of mysteries; Anker asks, "from what place exactly, and from what state of being, is he telling this story? If he's already dead, why bother with whatever it is he wishes to tell about his last year of being alive? There is also the question of how Lester has died—or will die." Anker believes the preceding scene—Jane's discussion with Ricky about the possibility of his killing her father—adds further mystery. Professor Ann C. Hall disagrees; she says by presenting an early resolution to the mystery, the film allows the audience to put it aside "to view the film and its philosophical issues". Through this examination of Lester's life, rebirth and death, "American Beauty" satirizes American middle class notions of meaning, beauty and satisfaction. Even Lester's transformation only comes about because of the possibility of sex with Angela; he therefore remains a "willing devotee of the popular media's exultation of pubescent male sexuality as a route to personal wholeness". Carolyn is similarly driven by conventional views of happiness; from her belief in "house beautiful" domestic bliss to her car and gardening outfit, Carolyn's domain is a "fetching American millennial vision of Pleasantville, or Eden". The Burnhams are unaware that they are "materialists philosophically, and devout consumers ethically" who expect the "rudiments of American beauty" to give them happiness. Anker argues that "they are helpless in the face of the prettified economic and sexual stereotypes [...] that they and their culture have designated for their salvation."
The film presents Ricky as its "visionary, [...] spiritual and mystical center". He sees beauty in the minutiae of everyday life, videoing as much as he can for fear of missing it. He shows Jane what he considers the most beautiful thing he has filmed: a plastic bag, tossing in the wind in front of a wall. He says capturing the moment was when he realized that there was "an entire life behind things"; he feels that "sometimes there's so much beauty in the world I feel like I can't take it... and my heart is going to cave in." Anker argues that Ricky, in looking past the "cultural dross", has "[grasped] the radiant splendor of the created world" to see God. As the film progresses, the Burnhams move closer to Ricky's view of the world. Lester only forswears personal satisfaction at the film's end. On the cusp of having sex with Angela, he returns to himself after she admits her virginity. Suddenly confronted with a child, he begins to treat her as a daughter; in doing so Lester sees himself, Angela and his family "for the poor and fragile but wondrous creatures they are". He looks at a picture of his family in happier times, and dies having had an epiphany that infuses him with "wonder, joy, and soul-shaking gratitude"—he has finally seen the world as it is.
According to Patti Bellantoni, colors are used symbolically throughout the film, none more so than red, which is an important thematic signature that drives the story and "[defines] Lester's arc". First seen in drab colors that reflect his passivity, Lester surrounds himself with red as he regains his individuality. The American Beauty rose is repeatedly used as symbol; when Lester fantasizes about Angela, she is usually naked and surrounded by rose petals. In these scenes, the rose symbolizes Lester's desire for her. When associated with Carolyn, the rose represents a "façade for suburban success". Roses are included in almost every shot inside the Burnhams' home, where they signify "a mask covering a bleak, unbeautiful reality". Carolyn feels that "as long as there can be roses, all is well". She cuts the roses and puts them in vases, where they adorn her "meretricious vision of what makes for beauty" and begin to die. The roses in the vase in the Angela–Lester seduction scene symbolize Lester's previous life and Carolyn; the camera pushes in as Lester and Angela get closer, finally taking the roses—and thus Carolyn—out of the shot. Lester's epiphany at the end of the film is expressed via rain and the use of red, building to a crescendo that is a deliberate contrast to the release Lester feels. The constant use of red "lulls [the audience] subliminally" into becoming used to it; consequently, it leaves the audience unprepared when Lester is shot and his blood spatters on the wall.
Sexuality and repression.
Pennington argues that "American Beauty" defines its characters through their sexuality. Lester's attempts to relive his youth are a direct result of his lust for Angela, and the state of his relationship with Carolyn is in part shown through their lack of sexual contact. Also sexually frustrated, Carolyn has an affair that takes her from "cold perfectionist" to a more carefree soul who "[sings] happily along with" the music in her car. Jane and Angela constantly reference sex, through Angela's descriptions of her supposed sexual encounters and the way the girls address each other. Their nude scenes are used to communicate their vulnerability. By the end of the film, Angela's hold on Jane has weakened until the only power she has over her friend is Lester's attraction to her. Col. Fitts reacts with disgust to meeting Jim and Jim; he asks, "How come these faggots always have to rub it in your face? How can they be so shameless?" To which Ricky replies, "That's the thing, Dad—they don't feel like it's anything to be ashamed of." Pennington argues that Col. Fitts' reaction is not homophobic, but an "anguished self-interrogation".
With other turn-of-the-millennium films such as "Fight Club", "In the Company of Men" (1997), "American Psycho" (2000) and "Boys Don't Cry" (1999), "American Beauty" "raises the broader, widely explored issue of masculinity in crisis". Professor Vincent Hausmann charges that in their reinforcement of masculinity "against threats posed by war, by consumerism, and by feminist and queer challenges", these films present a need to "focus on, and even to privilege" aspects of maleness "deemed 'deviant'". Lester's transformation conveys "that he, and not the woman, has borne the brunt of [lack of being]" and he will not stand for being emasculated. Lester's attempts to "strengthen traditional masculinity" conflict with his responsibilities as a father. Although the film portrays the way Lester returns to that role positively, he does not become "the hypermasculine figure implicitly celebrated in films like "Fight Club"". Hausmann concludes that Lester's behavior toward Angela is "a misguided but nearly necessary step toward his becoming a father again".
Hausmann says the film "explicitly affirms the importance of upholding the prohibition against incest"; a recurring theme of Ball's work is his comparison of the taboos against incest and homosexuality. Instead of making an overt distinction, "American Beauty" looks at how their repression can lead to violence. Col. Fitts is so ashamed of his homosexuality that it drives him to murder Lester. Ball said, "The movie is in part about how homophobia is based in fear and repression and about what [they] can do." The film implies two unfulfilled incestuous desires: Lester's pursuit of Angela is a manifestation of his lust for his own daughter, while Col. Fitts' repression is exhibited through the almost sexualized discipline with which he controls Ricky. Consequently, Ricky realizes that he can only hurt his father by falsely telling him he is homosexual, while Angela's vulnerability and submission to Lester reminds him of his responsibilities and the limits of his fantasy. Col. Fitts represents Ball's father, whose repressed homosexual desires led to his own unhappiness. Ball rewrote Col. Fitts to delay revealing him as homosexual, which Munt reads as a possible "deferment of Ball's own patriarchal-incest fantasies".
Temporality and music.
"American Beauty" follows a traditional narrative structure, only deviating with the displaced opening scene of Jane and Ricky from the middle of the story. Although the plot spans one year, the film is narrated by Lester at the moment of his death. Jacqueline Furby says that the plot "occupies [...] no time [or] all time", citing Lester's claim that life did not flash before his eyes, but that it "stretches on forever like an ocean of time". Furby argues that a "rhythm of repetition" forms the core of the film's structure. For example, two scenes see the Burnhams sitting down to an evening meal, shot from the same angle. Each image is broadly similar, with minor differences in object placement and body language that reflect the changed dynamic brought on by Lester's new-found assertiveness. Another example is the pair of scenes in which Jane and Ricky film each other. Ricky films Jane from his bedroom window as she removes her bra, and the image is reversed later for a similarly "voyeuristic and exhibitionist" scene in which Jane films Ricky at a vulnerable moment.
Lester's fantasies are emphasized by slow motion and repetitive motion shots; Mendes uses double-and-triple cut backs in several sequences, and the score alters to make the audience aware that it is entering a fantasy. One example is the gymnasium scene—Lester's first encounter with Angela. While the cheerleaders perform their half-time routine to "On Broadway", Lester becomes increasingly fixated on Angela. Time slows to represent his "voyeuristic hypnosis" and Lester begins to fantasize that Angela's performance is for him alone. "On Broadway"—which provides a conventional underscore to the onscreen action—is replaced by discordant, percussive music that lacks melody or progression. This nondiegetic score is important to creating the narrative stasis in the sequence; it conveys a moment for Lester that is stretched to an indeterminate length. The effect is one that Stan Link likens to "vertical time", described by the composer and music theorist Jonathan Kramer as music that imparts "a single present stretched out into an enormous duration, a potentially infinite 'now' that nonetheless feels like an instant". The music is used like a visual cue, so that Lester and the score are staring at Angela. The sequence ends with the sudden reintroduction of "On Broadway" and teleological time.
According to Drew Miller of "Stylus", the soundtrack "[gives] unconscious voice" to the characters' psyches and complements the subtext. The most obvious use of pop music "accompanies and gives context to" Lester's attempts to recapture his youth; reminiscent of how the counterculture of the 1960s combated American repression through music and drugs, Lester begins to smoke cannabis and listen to rock music. Mendes' song choices "progress through the history of American popular music". Miller argues that although some may be over familiar, there is a parodic element at work, "making good on [the film's] encouragement that viewers look closer". Toward the end of the film, Thomas Newman's score features more prominently, creating "a disturbing tempo" that matches the tension of the visuals. The exception is "Don't Let It Bring You Down", which plays during Angela's seduction of Lester. At first appropriate, its tone clashes as the seduction stops. The lyrics, which speak of "castles burning", can be seen as a metaphor for Lester's view of Angela—"the rosy, fantasy-driven exterior of the 'American Beauty'"—as it burns away to reveal "the timid, small-breasted girl who, like his wife, has willfully developed a false public self".
Production.
Development.
In 1997, Alan Ball resolved to move into the film industry after several frustrating years writing for the television sitcoms "Grace Under Fire" and "Cybill". He joined the United Talent Agency (UTA), where his representative, Andrew Cannava, suggested he write a spec script to "reintroduce [himself] to the town as a screenwriter". Ball pitched three ideas to Cannava: two conventional romantic comedies and "American Beauty", which he had originally conceived as a play in the early 1990s. Despite the story's lack of an easily marketable concept, Cannava selected "American Beauty" because he felt it was the one Ball had the most passion for. While developing the script, Ball created another television sitcom, "Oh, Grow Up". He channeled his anger and frustration at having to accede to network demands on that show—and during his tenures on "Grace Under Fire" and "Cybill"—into writing "American Beauty".
Ball did not expect to sell the script, believing it would act as more of a calling card, but "American Beauty" drew interest from several production bodies. Cannava passed the script to several producers, including Dan Jinks and Bruce Cohen, who took it to DreamWorks. With the help of executives Glenn Williamson and Bob Cooper, and Steven Spielberg in his capacity as studio partner, Ball was convinced to develop the project at DreamWorks; he received assurances from the studio—known at the time for its more conventional fare—that it would not "iron the [edges] out". In an unusual move, DreamWorks decided not to option the script; instead, in April 1998, the studio bought it outright for $250,000, outbidding Fox Searchlight Pictures, October Films, Metro-Goldwyn-Mayer and Lakeshore Entertainment. DreamWorks planned to make the film for $6–8 million.
Jinks and Cohen involved Ball throughout the film's development, including casting and director selection. The producers met with about twenty interested directors, several of whom were considered "A-list" at the time. Ball was not keen on the more well-known directors because he believed their involvement would increase the budget and lead DreamWorks to become "nervous about the content". Nevertheless, the studio offered the film to Mike Nichols and Robert Zemeckis; neither accepted. In the same year, Mendes (then a theater director) revived the musical "Cabaret" in New York with fellow director Rob Marshall. Beth Swofford of the Creative Artists Agency arranged meetings for Mendes with studio figures in Los Angeles to see if film direction was a possibility. Mendes came across "American Beauty" in a pile of eight scripts at Swofford's house, and knew immediately that it was the one he wanted to make; early in his career, he had been inspired by how the film "Paris, Texas" (1984) presented contemporary America as a mythic landscape and he saw the same theme in "American Beauty", as well as parallels with his own childhood. Mendes later met with Spielberg; impressed by Mendes' productions of "Oliver!" and "Cabaret", Spielberg encouraged him to consider "American Beauty".
Mendes found that he still had to convince DreamWorks' production executives to let him direct. He had already discussed the film with Jinks and Cohen, and felt they supported him. Ball was also keen; having seen "Cabaret", he was impressed with Mendes' "keen visual sense" and thought he did not make obvious choices. Ball felt that Mendes liked to look under the story's surface, a talent he felt would be a good fit with the themes of "American Beauty". Mendes' background also reassured him, because of the prominent role the playwright usually has in a theater production. Over two meetings—the first with Cooper, Walter Parkes and Laurie MacDonald, the second with Cooper alone—Mendes pitched himself to the studio. The studio soon approached Mendes with a deal to direct for the minimum salary allowed under Directors Guild of America rules—$150,000. Mendes accepted, and later recalled that after taxes and his agent's commission, he only earned $38,000. In June 1998, DreamWorks confirmed that it had contracted Mendes to direct the film.
Writing.
"I think I was writing about [...] how it's becoming harder and harder to live an authentic life when we live in a world that seems to focus on appearance. [...] For all the differences between now and the [1950s], in a lot of ways this is just as oppressively conformist a time. [...] You see so many people who strive to live the unauthentic life and then they get there and they wonder why they're not happy. [...] I didn't realize it when I sat down to write ["American Beauty"], but these ideas are important to me."
—Alan Ball, 2000
Ball was partly inspired by two encounters he had in the early 1990s. In about 1991–92, Ball saw a plastic bag blowing in the wind outside the World Trade Center. He watched the bag for ten minutes, saying later that it provoked an "unexpected emotional response". In 1992, Ball became preoccupied with the media circus around the Amy Fisher trial. Discovering a comic book telling of the scandal, he was struck by how quickly it had become commercialized. He said he "felt like there was a real story underneath [that was] more fascinating and way more tragic" than the story presented to the public, and attempted to turn the idea into a play. Ball produced around 40 pages, but stopped when he realized it would work better as a film. He felt that because of the visual themes, and because each character's story was.. "intensely personal", it could not be done on a stage. All the main characters appeared in this version, but Carolyn did not feature strongly; Jim and Jim instead had much larger roles.
Ball based Lester's story on aspects of his own life. Lester's re-examination of his life parallels feelings Ball had in his mid-30s; like Lester, Ball put aside his passions to work in jobs he hated for people he did not respect. Scenes in Ricky's household reflect Ball's own childhood experiences. Ball suspected his father was homosexual and used the idea to create Col. Fitts, a man who "gave up his chance to be himself". Ball said the script's mix of comedy and drama was not intentional, but that it came unconsciously from his own outlook on life. He said the juxtaposition produced a starker contrast, giving each trait more impact than if they appeared alone.
In the script that was sent to prospective actors and directors, Lester and Angela had sex; by the time of shooting, Ball had rewritten the scene to the final version. Ball initially rebuffed counsel from others that he change the script, feeling they were being puritanical; the final impetus to alter the scene came from DreamWorks' then-president Walter Parkes. He convinced Ball by indicating that in Greek mythology, the hero "has a moment of epiphany before [...] tragedy occurs". Ball later said his anger when writing the first draft had blinded him to the idea that Lester needed to refuse sex with Angela to complete his emotional journey—to achieve redemption. Jinks and Cohen asked Ball not to alter the scene straight away, as they felt it would be inappropriate to make changes to the script before a director had been hired. Early drafts also included a flashback to Col. Fitts service in the Marines, a sequence that unequivocally established his homosexual leanings. In love with another Marine, Col. Fitts sees the man die and comes to believe that he is being punished for the "sin" of being gay. Ball removed the sequence because it did not fit the structure of the rest of the film—Col. Fitts was the only character to have a flashback—and because it removed the element of surprise from Col. Fitts' later pass at Lester. Ball said he had to write it for his own benefit to know what happened to Col. Fitts, even though all that remained in later drafts was subtext.
Ball remained involved throughout production; he had signed a television show development deal, so had to get permission from his producers to take a year off to be close to "American Beauty". Ball was on-set for rewrites and to help interpret his script for all but two days of filming. His original bookend scenes—in which Ricky and Jane are prosecuted for Lester's murder after being framed by Col. Fitts—were excised in post-production; the writer later felt the scenes were unnecessary, saying they were a reflection of his "anger and cynicism" at the time of writing (see "Editing"). Ball and Mendes revised the script twice before it was sent to the actors, and twice more before the first read-through.
The shooting script features a scene in Angela's car in which Ricky and Jane talk about death and beauty; the scene differed from earlier versions, which set it as a "big scene on a freeway" in which the three witness a car crash and see a dead body. The change was a practical decision, as the production was behind schedule and they needed to cut costs. The schedule called for two days to be spent filming the crash, but only half a day was available. Ball agreed, but only if the scene could retain a line of Ricky's where he reflects on having once seen a dead homeless woman: "When you see something like that, it's like God is looking right at you, just for a second. And if you're careful, you can look right back." Jane asks: "And what do you see?" Ricky: "Beauty." Ball said, "They wanted to cut that scene. They said it's not important. I said, 'You're out of your fucking mind. It's one of the most important scenes in the movie!' [...] If any one line is the heart and soul of this movie, that is the line." Another scene was rewritten to accommodate the loss of the freeway sequence; set in a schoolyard, it presents a "turning point" for Jane in that she chooses to walk home with Ricky instead of going with Angela. By the end of filming, the script had been through ten drafts.
Casting.
Mendes had Spacey and Bening in mind for the leads from the beginning, but DreamWorks executives were unenthusiastic. The studio suggested several alternatives, including Bruce Willis, Kevin Costner or John Travolta to play Lester, and Helen Hunt or Holly Hunter to play Carolyn. Mendes did not want a big star "weighing the film down"; he felt Spacey was the right choice based on his performances in the 1995 films "The Usual Suspects" and "Seven", and 1992's "Glengarry Glen Ross". Spacey was surprised; he said, "I usually play characters who are very quick, very manipulative and smart. [...] I usually wade in dark, sort of treacherous waters. This is a man living one step at a time, playing by his instincts. This is actually much closer to me, to what I am, than those other parts." Mendes offered Bening the role of Carolyn without the studio's consent; although executives were upset at Mendes, by September 1998, DreamWorks had entered negotiations with Spacey and Bening.
Spacey loosely based Lester's early "schlubby" deportment on Walter Matthau. During the film, Lester's physique improves from flabby to toned; Spacey worked out during filming to improve his body, but because Mendes shot the scenes out of chronological order, Spacey varied postures to portray the stages. Before filming, Mendes and Spacey analyzed Jack Lemmon's performance in "The Apartment" (1960), because Mendes wanted Spacey to emulate "the way [Lemmon] moved, the way he looked, the way he was in that office and the way he was an ordinary man and yet a special man". Spacey's voiceover is a throwback to "Sunset Boulevard" (1950), which is also narrated in retrospect by a dead character. Mendes felt it evoked Lester's—and the film's—loneliness. Bening recalled women from her youth to inform her performance: "I used to babysit constantly. You'd go to church and see how people present themselves on the outside, and then be inside their house and see the difference." Bening and a hair stylist collaborated to create a "PTA president coif" hairstyle, and Mendes and production designer Naomi Shohan researched mail order catalogs to better establish Carolyn's environment of a "spotless suburban manor". To help Bening get into Carolyn's mindset, Mendes gave her music that he believed Carolyn would like. He lent Bening the Bobby Darin version of the song "Don't Rain on My Parade", which she enjoyed and persuaded the director to include it for a scene in which Carolyn sings in her car.
For the roles of Jane, Ricky and Angela, DreamWorks gave Mendes "carte blanche". By November 1998, Thora Birch, Wes Bentley, and Mena Suvari had been cast in the parts—in Birch's case, despite the fact she was underage for her nude scene. As Birch was 16 at the time she made the film, and thus classified as a minor in the United States, her parents had to approve her brief topless scene in the movie. They and child labor representatives were on the set for the shooting of the scene. Bentley overcame competition from top actors under the age of 25 to be cast. The 2009 documentary "My Big Break" followed Bentley, and several other young actors, before and after he landed the part. To prepare, Mendes provided Bentley with a video camera, telling the actor to film what Ricky would. Peter Gallagher and Alison Janney were cast (as Buddy Kane and Barbara Fitts) after filming began in December 1998. Mendes gave Janney a book of paintings by Edvard Munch. He told her, "Your character is in there somewhere." Mendes cut much of Barbara's dialogue, including conversations between her and Colonel Fitts, as he felt that what needed to be said about the pair—their humanity and vulnerability—was conveyed successfully through their shared moments of silence. Chris Cooper plays Colonel Fitts, Scott Bakula plays Jim Olmeyer, and Sam Robards plays Jim Berkley. Jim and Jim were deliberately depicted as the most normal, happy—and boring—couple in the film. Ball's inspiration for the characters came from a thought he had after seeing a "bland, boring, heterosexual couple" who wore matching clothes: "I can't wait for the time when a gay couple can be just as boring." Ball also included aspects of a gay couple he knew who had the same forename.
Mendes insisted on two weeks of cast rehearsals, although the sessions were not as formal as he was used to in the theater, and the actors could not be present at every one. Several improvisations and suggestions by the actors were incorporated into the script. An early scene showing the Burnhams leaving home for work was inserted later on to show the low point that Carolyn and Lester's relationship had reached. Spacey and Bening worked to create a sense of the love that Lester and Carolyn once had for one another; for example, the scene in which Lester almost seduces Carolyn after the pair argue over Lester's buying a car was originally "strictly contentious".
Filming.
Principal photography lasted about 50 days from December 14, 1998, to February 1999. "American Beauty" was filmed on soundstages at the Warner Bros. backlot in Burbank, California, and at Hancock Park and Brentwood in Los Angeles. The aerial shots at the beginning and end of the film were captured in Sacramento, California, and many of the school scenes were shot at South High School in Torrance, California; several extras in the gym crowd were South High students. The film is set in an upper middle class neighborhood in an unidentified American town. Production designer Naomi Shohan likened the locale to Evanston, Illinois, but said, "it's not about a place, it's about an archetype. [...] The milieu was pretty much Anywhere, USA—upwardly mobile suburbia." The intent was for the setting to reflect the characters, who are also archetypes. Shohan said, "All of them are very strained, and their lives are constructs." The Burnhams' household was designed as the reverse of the Fitts'—the former a pristine ideal, but graceless and lacking in "inner balance", leading to Carolyn's desire to at least give it the appearance of a "perfect all-American household"; the Fitts' home is depicted in "exaggerated darkness [and] symmetry".
The production selected two adjacent properties on the Warner backlot's "Blondie Street" for the Burnhams' and Fitts' homes. The crew rebuilt the houses to incorporate false rooms that established lines of sight—between Ricky and Jane's bedroom windows, and between Ricky's bedroom and Lester's garage. The garage windows were designed specifically to obtain the crucial shot toward the end of the film in which Col. Fitts—watching from Ricky's bedroom—mistakenly assumes that Lester is paying Ricky for sex. Mendes made sure to establish the line of sight early on in the film to make the audience feel a sense of familiarity with the shot. The house interiors were filmed on the backlot, on location, and on soundstages when overhead shots were needed. The inside of the Burnhams' home was shot at a house close to Interstate 405 and Sunset Boulevard in Los Angeles; the inside of the Fitts' home was shot in the city's Hancock Park neighborhood. Ricky's bedroom was designed to be cell-like to suggest his "monkish" personality, while at the same time blending with the high-tech equipment to reflect his voyeuristic side. The production deliberately minimized the use of red, as it was an important thematic signature elsewhere. The Burnhams' home uses cool blues, while the Fitts' is kept in a "depressed military palette".
Mendes' dominating visual style was deliberate and composed, with a minimalist design that provided "a sparse, almost surreal feeling—a bright, crisp, hard edged, near Magritte-like take on American suburbia"; Mendes constantly directed his set dressers to empty the frame. He made Lester's fantasy scenes "more fluid and graceful", and Mendes made minimal use of steadicams, feeling that stable shots generated more tension. For example, when Mendes used a slow push in to the Burnhams' dinner table, he held the shot because his training as a theater director taught him the importance of putting distance between the characters. He wanted to keep the tension in the scene, so he only cut away when Jane left the table. Mendes did use a hand-held camera for the scene in which Col. Fitts beats Ricky. Mendes said the camera provided the scene with a "kinetic [...] off balance energy". He also went hand-held for the excerpts of Ricky's camcorder footage. It took Mendes a long time to get the quality of Ricky's footage to the level he wanted. For the plastic bag footage, Mendes used wind machines to move the bag in the air. The scene took four takes; two by the second unit did not satisfy Mendes, so he shot the scene himself. He felt his first take lacked grace, but for the last attempt he changed the location to the front of a brick wall and added leaves on the ground. Mendes was satisfied by the way the wall gave definition to the outline of the bag.
Mendes avoided using close-ups, as he believed the technique was overused; he also cited Spielberg's advice that he should imagine an audience silhouetted at the bottom of the camera monitor, to keep in mind that he was shooting for display on a 40 ft screen. Spielberg—who visited the set a few times—also advised Mendes not to worry about costs if he had a "great idea" toward the end of a long working day. Mendes said, "That happened three or four times, and they are all in the movie." Despite Spielberg's support, DreamWorks and Mendes fought constantly over the schedule and budget—although the studio interfered little with the film's content. Spacey, Bening and Hall worked for significantly less than their usual rates. "American Beauty" cost DreamWorks $15 million to produce, slightly above their projected sum. Mendes was so dissatisfied with his first three days' filming that he obtained permission from DreamWorks to reshoot the scenes. He said, "I started with a wrong scene, actually, a comedy scene. And the actors played it way too big: [...] it was badly shot, my fault, badly composed, my fault, bad costumes, my fault [...]; and everybody was doing what I was asking. It was all my fault." Aware that he was a novice, Mendes drew on the experience of Hall: "I made a very conscious decision early on, if I didn't understand something technically, to say, without embarrassment, 'I don't understand what you're talking about, please explain it.'"
Mendes encouraged some improvisation; for example, when Lester masturbates in bed beside Carolyn, the director asked Spacey to improvise several euphemisms for the act in each take. Mendes said, "I wanted that not just because it was funny [...] but because I didn't want it to seem rehearsed. I wanted it to seem like he was blurting it out of his mouth without thinking. [Spacey] is so in control—I wanted him to break through." Spacey obliged, eventually coming up with 35 phrases, but Bening could not always keep a straight face, which meant the scene had to be shot ten times. The production used small amounts of computer-generated imagery. Most of the rose petals in Lester's fantasies were added in post-production, although some were real and had the wires holding them digitally removed. When Lester fantasizes about Angela in a rose petal bath, the steam was real, save for in the overhead shot. To position the camera, a hole had to be cut in the ceiling, through which the steam escaped; it was instead added digitally.
Editing.
"American Beauty" was edited by Christopher Greenbury and Tariq Anwar; Greenbury began in the position, but had to leave halfway through post-production because of a scheduling conflict with "Me, Myself and Irene" (2000) (which Chris Cooper also starred in). Mendes and an assistant edited the film for ten days between the appointments. Mendes realized during editing that the film was different from the one he had envisioned. He believed he had been making a "much more whimsical, [...] kaleidoscopic" film than what came together in the edit suite. Instead, Mendes was drawn to the emotion and darkness; he began to use the score and shots he had intended to discard to craft the film along these lines. In total, he cut about 30 minutes from his original edit. The opening included a dream in which Lester imagines himself flying above the town. Mendes spent two days filming Spacey against bluescreen, but removed the sequence as he believed it to be too whimsical—"like a Coen brothers movie"—and therefore inappropriate for the tone he was trying to set. The opening in the final cut reused a scene from the middle of the film where Jane tells Ricky to kill her father. This scene was to be the revelation to the audience that the pair were not responsible for Lester's death, as the way it was scored and acted made it clear that Jane's request was not serious. However, in the portion he used in the opening—and when the full scene plays out later—Mendes used the score and a reaction shot of Ricky to leave a lingering ambiguity as to his guilt. The subsequent shot—an aerial view of the neighborhood—was originally intended as the plate shot for the bluescreen effects in the dream sequence.
Mendes spent more time re-cutting the first ten minutes than the rest of the film taken together. He trialled several versions of the opening; the first edit included bookend scenes in which Jane and Ricky are convicted of Lester's murder, but Mendes excised these in the last week of editing because he felt they made the film lose its mystery, and because they did not fit with the theme of redemption that had emerged during production. Mendes believed the trial drew focus away from the characters and turned the film "into an episode of "NYPD Blue"". Instead, he wanted the ending to be "a poetic mixture of dream and memory and narrative resolution". When Ball first saw a completed edit, it was a version with truncated versions of these scenes. He felt that they were so short that they "didn't really register". He and Mendes argued, but Ball was more accepting after Mendes cut the sequences completely; Ball felt that without the scenes the film was more optimistic and had evolved into something that "for all its darkness had a really romantic heart".
Cinematography.
Conrad Hall was not the first choice for director of photography; Mendes believed he was "too old and too experienced" to want the job, and he had been told that Hall was difficult to work with. Instead, Mendes asked Fred Elmes, who turned the job down because he did not like the script. Hall was recommended to Mendes by Tom Cruise, because of Hall's work on "Without Limits" (1998), which Cruise had executive produced. Mendes was directing Cruise's then-wife Nicole Kidman in the play "The Blue Room" during pre-production on "American Beauty", and had already storyboarded the whole film. Hall was involved for one month during pre-production; his ideas for lighting the film began with his first reading of the script, and further passes allowed him to refine his approach before meeting Mendes. Hall was initially concerned that audiences would not like the characters; he only felt able to identify with them during cast rehearsals, which gave him fresh ideas on his approach to the visuals.
Hall's approach was to create peaceful compositions that evoked classicism, to contrast with the turbulent on-screen events and allow audiences to take in the action. Hall and Mendes would first discuss the intended mood of a scene, but he was allowed to light the shot in any way he felt necessary. In most cases, Hall first lit the scene's subject by "painting in" the blacks and whites, before adding fill light, which he reflected from beadboard or white card on the ceiling. This approach gave Hall more control over the shadows while keeping the fill light unobtrusive and the dark areas free of spill. Hall shot "American Beauty" in a 2.39:1 aspect ratio in the Super 35 format, using Kodak Vision 500T 5279 35 mm film stock. He used Super 35 partly because its larger scope allowed him to capture elements such as the corners of the petal-filled pool in its overhead shot, creating a frame around Angela within. He shot the whole film at the same T-stop (T1.9); given his preference for shooting that wide, Hall favored high-speed stocks to allow for more subtle lighting effects. He used Panavision Platinum cameras with the company's Primo series of prime and zoom lenses. Hall employed Kodak Vision 200T 5274 and EXR 5248 stock for scenes with daylight effects. He had difficulty adjusting to Kodak's newly introduced Vision release print stock, which, combined with his contrast-heavy lighting style, created a look with too much contrast. Hall contacted Kodak, who sent him a batch of 5279 that was 5% lower in contrast. Hall used a 1/8 inch Tiffen Black ProMist filter for almost every scene, which he said in retrospect may not have been the best choice, as the optical steps required to blow Super 35 up for its anamorphic release print led to a slight amount of degradation; therefore, the diffusion from the filter was not required. When he saw the film in a theater, Hall felt that the image was slightly unclear and that had he not used the filter, the diffusion from the Super 35–anamorphic conversion would have generated an image closer to what he originally intended.
A shot where Lester and Ricky share a cannabis joint behind a building came from a misunderstanding between Hall and Mendes. Mendes asked Hall to prepare the shot in his absence; Hall assumed the characters would look for privacy, so he placed them in a narrow passage between a truck and the building, intending to light from the top of the truck. When Mendes returned, he explained that the characters did not care if they were seen. He removed the truck and Hall had to rethink the lighting; he lit it from the left, with a large light crossing the actors, and with a soft light behind the camera. Hall felt the consequent wide shot "worked perfectly for the tone of the scene". Hall made sure to keep rain, or the suggestion of it, in every shot near the end of the film. In one shot during Lester's encounter with Angela at the Burnhams' home, Hall created rain effects on the foreground cross lights; in another, he partly lit the pair through French windows to which he had added material to make the rain run slower, intensifying the light (although the strength of the outside light was unrealistic for a night scene, Hall felt it justified because of the strong contrasts it produced). For the close-ups when Lester and Angela move to the couch, Hall tried to keep rain in the frame, lighting through the window onto the ceiling behind Lester. He also used rain boxes to produce rain patterns where he wanted without lighting the entire room.
Music.
Thomas Newman's score was recorded in Santa Monica, California. He mainly used percussion instruments to create the mood and rhythm, the inspiration for which was provided by Mendes. Newman "favored pulse, rhythm and color over melody", making for a more minimalist score than he had previously created. He built each cue around "small, endlessly repeating phrases"—often, the only variety through a "thinning of the texture for eight bars". The percussion instruments included tablas, bongos, cymbals, piano, xylophones and marimbas; also featured were guitars, flute, and world music instruments. Newman also used electronic music and on "quirkier" tracks employed more unorthodox methods, such as tapping metal mixing bowls with a finger and using a detuned mandolin. Newman believed the score helped move the film along without disturbing the "moral ambiguity" of the script: "It was a real delicate balancing act in terms of what music worked to preserve [that]."
The soundtrack features songs by Newman, Bobby Darin, The Who, Free, Eels, The Guess Who, Bill Withers, Betty Carter, Peggy Lee, The Folk Implosion, Gomez, and Bob Dylan, as well as two cover versions—The Beatles' "Because" performed by Elliott Smith, and Neil Young's "Don't Let It Bring You Down" performed by Annie Lennox. Produced by the film's music supervisor Chris Douridas, an abridged soundtrack album was released on October 5, 1999 and went on to be nominated for a Grammy Award for Best Soundtrack Album. An album featuring 19 tracks from Newman's score was released on January 11, 2000, and won the Grammy Award for Best Score Soundtrack Album. "Filmmaker" considered the score to be one of Newman's best, saying it "[enabled] the film's transcendentalist aspirations". In 2006, the magazine chose the score as one of twenty essential soundtracks it believed spoke to the "complex and innovative relationships between music and screen storytelling".
Release.
Publicity.
DreamWorks contracted Amazon.com to create the official website, marking the first time that Amazon had created a special section devoted to a feature film. The website included an overview, a photo gallery, cast and crew filmographies, and exclusive interviews with Spacey and Bening. The film's tagline—"look closer"—originally came from a cutting pasted on Lester's workplace cubicle by the set dresser. DreamWorks ran parallel marketing campaigns and trailers—one aimed at adults, the other at teenagers. Both trailers ended with the poster image of a girl holding a rose. Reviewing the posters of several 1999 films, David Hochman of "Entertainment Weekly" rated "American Beauty"‍ '​s highly, saying it evoked the tagline; he said, "You return to the poster again and again, thinking, this time you're gonna find something." DreamWorks did not want to test screen the film; according to Mendes, the studio was pleased with it, but he insisted on one where he could question the audience afterward. The studio reluctantly agreed and showed the film to a young audience in San Jose, California. Mendes claimed the screening went very well.
Theatrical run.
The film had its world premiere on September 8, 1999, at Grauman's Egyptian Theatre in Los Angeles. Three days later, the film appeared at the Toronto International Film Festival. With the filmmakers and cast in attendance, it screened at several American universities, including the University of California at Berkeley, New York University, the University of California at Los Angeles, the University of Texas at Austin, and Northwestern University.
On September 15, 1999, "American Beauty" opened to the public in limited release at three theaters in Los Angeles and three in New York. More theaters were added during the limited run, and on October 1, the film officially entered wide release by screening in 706 theaters across North America. The film grossed $8,188,587 over the weekend, ranking third at the box office. Audiences polled by the market research firm CinemaScore gave "American Beauty" a "B+" grade on average. The theater count hit a high of 1,528 at the end of the month, before a gradual decline. Following "American Beauty"‍ '​s wins at the 57th Golden Globe Awards, DreamWorks re-expanded the theater presence from a low of 7 in mid-February, to a high of 1,990 in March. The film ended its North American theatrical run on June 4, 2000, having grossed $130.1 million.
"American Beauty" had its European premiere at the London Film Festival on November 18, 1999; in January 2000, it began to screen in various territories outside North America. It debuted in Israel to "potent" returns, and limited releases in Germany, Italy, Austria, Switzerland, the Netherlands and Finland followed on January 21. After January 28 opening weekends in Australia, the United Kingdom, Spain and Norway, "American Beauty" had earned $7 million in 12 countries for a total of $12.1 million outside North America. On February 4, "American Beauty" debuted in France and Belgium. Expanding to 303 theaters in the United Kingdom, the film ranked first at the box office with $1.7 million. On the weekend of February 18—following "American Beauty"‍ '​s eight nominations for the 72nd Academy Awards—the film grossed $11.7 million from 21 territories, for a total of $65.4 million outside North America. The film had "dazzling" debuts in Hungary, Denmark, the Czech Republic, Slovakia and New Zealand.
As of February 18, the most successful territories were the United Kingdom ($15.2 million), Italy ($10.8 million), Germany ($10.5 million), Australia ($6 million) and France ($5.3 million). The Academy Award nominations meant strong performances continued across the board; the following weekend, "American Beauty" grossed $10.9 million in 27 countries, with strong debuts in Brazil, Mexico and South Korea. Other high spots included robust returns in Argentina, Greece and Turkey. On the weekend of March 3, 2000, "American Beauty" debuted strongly in Hong Kong, Taiwan and in Singapore, markets traditionally "not receptive to this kind of upscale fare". The impressive South Korean performance continued, with a return of $1.2 million after nine days. In total, "American Beauty" grossed $130.1 million in North America and $226.2 million internationally, for $356.3 million worldwide.
Home media.
"American Beauty" was released on VHS on May 9, 2000 and on DVD with the DTS format on October 24, 2000. Before the North American rental release on May 9, Blockbuster Video wanted to purchase hundreds of thousands of extra copies for its "guaranteed title" range, whereby anyone who wanted to rent the film would be guaranteed a copy. Blockbuster and DreamWorks could not agree on a profit sharing deal, so Blockbuster ordered two thirds the number of copies it originally intended. DreamWorks made around one million copies available for rental; Blockbuster's share would usually have been about 400,000 of these. Some Blockbuster stores only displayed 60 copies, and others did not display the film at all, forcing customers to ask for it. The strategy required staff to read a statement to customers explaining the situation; Blockbuster claimed it was only "[monitoring] customer demand" due to the reduced availability. Blockbuster's strategy leaked before May 9, leading to a 30% order increase from other retailers. In its first week of rental release, "American Beauty" made $6.8 million. This return was lower than would have been expected had DreamWorks and Blockbuster reached an agreement. The same year The Sixth Sense" made $22 million, while "Fight Club" made $8.1 million, even though the latter's North American theatrical performance was just 29% that of "American Beauty". Blockbuster's strategy also affected rental fees; "American Beauty" averaged $3.12, compared with $3.40 for films that Blockbuster fully promoted. Only 53% of the film's rentals were from large outlets in the first week, compared with the usual 65%.
The DVD release included a behind-the-scenes featurette, film audio commentary from Mendes and Ball and a storyboard presentation with discussion from Mendes and Hall. In the film commentary, Mendes refers to deleted scenes he intended to include in the release. However, these scenes are not on the DVD as he changed his mind after recording the commentary; Mendes felt that to show scenes he previously chose not to use would detract from the film's integrity.
On September 21, 2010, Paramount Home Entertainment released "American Beauty" on Blu-ray, as part of Paramount's "Sapphire Series". All the extras from the DVD release were present, with the theatrical trailers upgraded to HD.
Critical reception.
"American Beauty" was widely considered the best film of 1999 by the American press. It received overwhelming praise, chiefly for Spacey, Mendes and Ball. "Variety" reported that "no other 1999 movie has benefited from such universal raves." It was the best-received title at the Toronto International Film Festival (TIFF), where it won the People's Choice Award after a ballot of the festival's audiences. TIFF's director, Piers Handling, said, ""American Beauty" was the buzz of the festival, the film most talked about."
Writing in "Variety", Todd McCarthy said the cast ensemble "could not be better"; he praised Spacey's "handling of innuendo, subtle sarcasm and blunt talk" and the way he imbued Lester with "genuine feeling". Janet Maslin in "The New York Times" said Spacey was at his "wittiest and most agile" to date, and Roger Ebert of the "Chicago Sun-Times" singled Spacey out for successfully portraying a man who "does reckless and foolish things [but who] doesn't deceive himself". Kevin Jackson of "Sight & Sound" said Spacey impressed in ways distinct from his previous performances, the most satisfying aspect being his portrayal of "both sap and hero". Writing in "Film Quarterly", Gary Hentzi praised the actors, but said that characters such as Carolyn and Col. Fitts were stereotypes. Hentzi accused Mendes and Ball of identifying too readily with Jane and Ricky, saying the latter was their "fantasy figure"—a teenaged boy who's an absurdly wealthy artist able to "finance [his] own projects". Hentzi said Angela was the most believable teenager, in particular with her "painfully familiar" attempts to "live up to an unworthy image of herself". Maslin agreed that some characters were unoriginal, but said their detailed characterizations made them memorable. Kenneth Turan of the "Los Angeles Times" said the actors coped "faultlessly" with what were difficult roles; he called Spacey's performance "the energy that drives the film", saying the actor commanded audience involvement despite Lester's not always being sympathetic. "Against considerable odds, we do like [these characters]," Turan concluded.
Maslin felt that Mendes directed with "terrific visual flair", saying his minimalist style balanced "the mordant and bright" and that he evoked the "delicate, eroticized power-playing vignettes" of his theater work. Jackson said Mendes' theatrical roots rarely showed, and that the "most remarkable" aspect was that Spacey's performance did not overshadow the film. He said that Mendes worked the script's intricacies smoothly, to the ensemble's strengths, and staged the tonal shifts skillfully. McCarthy believed "American Beauty" a "stunning card of introduction" for film débutantes Mendes and Ball. He said Mendes' "sure hand" was "as precise and controlled" as his theater work. McCarthy cited Hall's involvement as fortunate for Mendes, as the cinematographer was "unsurpassed" at conveying the themes of a work. Turan agreed that Mendes' choice of collaborators was "shrewd", naming Hall and Newman in particular. Turan suggested that "American Beauty" may have benefited from Mendes' inexperience, as his "anything's possible daring" made him attempt beats that more seasoned directors might have avoided. Turan felt that Mendes' accomplishment was to "capture and enhance [the] duality" of Ball's script—the simultaneously "caricatured [...] and painfully real" characters. Hentzi, while critical of many of Mendes and Ball's choices, admitted the film showed off their "considerable talents".
Turan cited Ball's lack of constraint when writing the film as the reason for its uniqueness, in particular the script's subtle changes in tone. McCarthy said the script was "as fresh and distinctive" as any of its American film contemporaries, and praised how it analyzed the characters while not compromising narrative pace. He called Ball's dialogue "tart" and said the characters—Carolyn excepted—were "deeply drawn". One other flaw, McCarthy said, was the revelation of Col. Fitts' homosexuality, which he said evoked "hoary Freudianism". Jackson said the film transcended its clichéd setup to become a "wonderfully resourceful and sombre comedy". He said that even when the film played for sitcom laughs, it did so with "unexpected nuance". Hentzi criticized how the film made a mystery of Lester's murder, believing it manipulative and simply a way of generating suspense. McCarthy cited the production and costume design as plusses, and said the soundtrack was good at creating "ironic counterpoint[s]" to the story. Hentzi concluded that "American Beauty" was "vital but uneven"; he felt the film's examination of "the ways which teenagers and adults imagine each other's lives" was its best point, and that although Lester and Angela's dynamic was familiar, its romantic irony stood beside "the most enduring literary treatments" of the theme, such as "Lolita". Nevertheless, Hentzi believed that the film's themes of materialism and conformity in American suburbia were "hackneyed". McCarthy conceded that the setting was familiar, but said it merely provided the film with a "starting point" from which to tell its "subtle and acutely judged tale". Maslin agreed; she said that while it "takes aim at targets that are none too fresh", and that the theme of nonconformity did not surprise, the film had its own "corrosive novelty". Ebert awarded "American Beauty" four stars out of four, and Turan said it was layered, subversive, complex and surprising, concluding it was "a hell of a picture".
A few months after the film's release, reports of a backlash appeared in the American press, and the years since have seen its critical regard wane. In 2005, "Premiere" named "American Beauty" as one of 20 "most overrated movies of all time"; Mendes accepted the inevitability of the critical reappraisal, saying, "I thought some of it was entirely justified—it was a little overpraised at the time."
Currently, the film holds an 88% score on Rotten Tomatoes based on 181 reviews, with an average rating of 8.2/10; the critical consensus reads, "Flawlessly cast and brimming with dark, acid wit, "American Beauty" is a smart, provocative high point of late '90s mainstream Hollywood film." Metacritic gives the film a score of 86, based on 33 reviews, indicating "universal acclaim."
Accolades.
"American Beauty" was not considered an immediate favorite to dominate the American awards season. Several other contenders opened at the end of 1999, and US critics spread their honors among them when compiling their end-of-year lists. The Chicago Film Critics Association and the Broadcast Film Critics Association named the film the best of 1999, but while the New York Film Critics Circle, the National Society of Film Critics and the Los Angeles Film Critics Association recognized "American Beauty", they gave their top awards to other films. By the end of the year, reports of a critical backlash suggested "American Beauty" was the underdog in the race for Best Picture; however, at the Golden Globe Awards in January 2000, "American Beauty" won Best Film, Best Director and Best Screenplay.
As the nominations for the 72nd Academy Awards approached, a frontrunner had not emerged. DreamWorks had launched a major campaign for "American Beauty" five weeks before ballots were due to be sent to the 5,600 Academy Award voters. Its campaign combined traditional advertising and publicity with more focused strategies. Although direct mail campaigning was prohibited, DreamWorks reached voters by promoting the film in "casual, comfortable settings" in voters' communities. The studio's candidate for Best Picture the previous year, "Saving Private Ryan", lost to "Shakespeare in Love", so the studio took a new approach by hiring outsiders to provide input for the campaign. It hired three veteran consultants, who told the studio to "think small". Nancy Willen encouraged DreamWorks to produce a special about the making of "American Beauty", to set up displays of the film in the communities' bookstores, and to arrange a question-and-answer session with Mendes for the British Academy of Film and Television Arts. Dale Olson advised the studio to advertise in free publications that circulated in Beverly Hills—home to many voters—in addition to major newspapers. Olson arranged to screen "American Beauty" to about 1,000 members of the Actors Fund of America, as many participating actors were also voters. Bruce Feldman took Ball to the Santa Barbara International Film Festival, where Ball attended a private dinner in honor of Anthony Hopkins, meeting several voters who were in attendance.
In February 2000, "American Beauty" was nominated for eight Academy Awards; its closest rivals, "The Cider House Rules" and "The Insider", received seven nominations each. In March 2000, the major industry labor organizations all awarded their top honors to "American Beauty"; perceptions had shifted—the film was now the favorite to dominate the Academy Awards. "American Beauty"‍ '​s closest rival for Best Picture was still "The Cider House Rules", from Miramax. Both studios mounted aggressive campaigns; DreamWorks bought 38% more advertising space in "Variety" than Miramax. On March 26, 2000, "American Beauty" won five Academy Awards: Best Picture, Best Director, Best Actor (Spacey), Best Original Screenplay and Best Cinematography. At the 53rd British Academy Film Awards, "American Beauty" won six of the fourteen awards for which it was nominated: Best Film, Best Actor, Best Actress (Bening), Best Cinematography, Best Film Music and Best Editing. In 2000, the Publicists Guild of America recognized DreamWorks for the best film publicity campaign. In September 2008, "Empire" named "American Beauty" the 96th "Greatest Movie of All Time" after a poll of 10,000 readers, 150 filmmakers and 50 film critics, the 4th highest ranked movie from 1999 (behind "Fight Club", "The Matrix", and "Magnolia"). In 2013, the Writers Guild of America ranked the screenplay #38 on its list of 101 Greatest Screenplays.
The film was nominated for AFI's 100 Years...100 Movies (10th Anniversary Edition) in 2007.
References.
</dl>

</doc>
<doc id="44125" url="http://en.wikipedia.org/wiki?curid=44125" title="Gyroscope">
Gyroscope

A gyroscope (from Greek γῦρος "gûros", "circle" and σκοπέω "skopéō", "to look") is a spinning wheel or disc in which the axis of rotation is free to assume any orientation. When rotating, the orientation of this axis is unaffected by tilting or rotation of the mounting, according to the conservation of angular momentum. Because of this, gyroscopes are useful for measuring or maintaining orientation.
Gyroscopes based on other operating principles also exist, such as the electronic, microchip-packaged MEMS gyroscopes found in consumer electronics devices, solid-state ring lasers, fibre optic gyroscopes, and the extremely sensitive quantum gyroscope.
Applications of gyroscopes include inertial navigation systems where magnetic compasses would not work (as in the Hubble telescope) or would not be precise enough (as in intercontinental ballistic missiles), or for the stabilization of flying vehicles like radio-controlled helicopters or unmanned aerial vehicles, and recreational boats and commercial ships. Due to their precision, gyroscopes are also used in gyrotheodolites to maintain direction in tunnel mining. Gyroscopes can be used to construct gyrocompasses, which complement or replace magnetic compasses (in ships, aircraft and spacecraft, vehicles in general), to assist in stability (Hubble Space Telescope, bicycles, motorcycles, and ships) or be used as part of an inertial guidance system.
Description and diagram.
Within mechanical systems or devices, a conventional "gyroscope" is a mechanism comprising a rotor journaled to spin about one axis, the journals of the rotor being mounted in an inner gimbal or ring; the inner gimbal is journaled for oscillation in an outer gimbal for a total of two gimbals.
The outer gimbal or ring, which is the gyroscope frame, is mounted so as to pivot about an axis in its own plane determined by the support. This outer gimbal possesses one degree of rotational freedom and its axis possesses none. The next inner gimbal is mounted in the gyroscope frame (outer gimbal) so as to pivot about an axis in its own plane that is always perpendicular to the pivotal axis of the gyroscope frame (outer gimbal). This inner gimbal has two degrees of rotational freedom.
The axle of the spinning wheel defines the spin axis. The rotor is journaled to spin about an axis, which is always perpendicular to the axis of the inner gimbal. So the rotor possesses three degrees of rotational freedom and its axis possesses two.
The wheel responds to a force applied about the input axis by a reaction force about the output axis.
The behaviour of a gyroscope can be most easily appreciated by consideration of the front wheel of a bicycle. If the wheel is leaned away from the vertical so that the top of the wheel moves to the left, the forward rim of the wheel also turns to the left. In other words, rotation on one axis of the turning wheel produces rotation of the third axis.
A gyroscope flywheel will roll or resist about the output axis depending upon whether the output gimbals are of a free- or fixed- configuration. Examples of some free-output-gimbal devices would be the attitude reference gyroscopes used to sense or measure the pitch, roll and yaw attitude angles in a spacecraft or aircraft.
The centre of gravity of the rotor can be in a fixed position. The rotor simultaneously spins about one axis and is capable of oscillating about the two other axes, and, thus, except for its inherent resistance due to rotor spin, it is free to turn in any direction about the fixed point. Some gyroscopes have mechanical equivalents substituted for one or more of the elements. For example, the spinning rotor may be suspended in a fluid, instead of being pivotally mounted in gimbals. A control moment gyroscope (CMG) is an example of a fixed-output-gimbal device that is used on spacecraft to hold or maintain a desired attitude angle or pointing direction using the gyroscopic resistance force.
In some special cases, the outer gimbal (or its equivalent) may be omitted so that the rotor has only two degrees of freedom. In other cases, the centre of gravity of the rotor may be offset from the axis of oscillation, and, thus, the centre of gravity of the rotor and the centre of suspension of the rotor may not coincide.
History.
Essentially, a gyroscope is a top combined with a pair of gimbals. Tops were invented in many different civilizations, including classical Greece, Rome, Indus and China, and the Māori culture a thousand years later. Most of these were not utilized as instruments.
The first known apparatus similar to a gyroscope (the "Whirling Speculum" or "Serson's Speculum") was invented by John Serson in 1743. It was used as a level, to locate the horizon in foggy or misty conditions.
The first instrument used more like an actual gyroscope was made by German Johann Bohnenberger, who first wrote about it in 1817. At first he called it the "Machine". Bohnenberger's machine was based on a rotating massive sphere. In 1832, American Walter R. Johnson developed a similar device that was based on a rotating disc. The French mathematician Pierre-Simon Laplace, working at the École Polytechnique in Paris, recommended the machine for use as a teaching aid, and thus it came to the attention of Léon Foucault. In 1852, Foucault used it in an experiment involving the rotation of the Earth. It was Foucault who gave the device its modern name, in an experiment to see (Greek "skopeein", to see) the Earth's rotation (Greek "gyros", circle or rotation), which was visible in the 8 to 10 minutes before friction slowed the spinning rotor.
In the 1860s, the advent of electric motors made it possible for a gyroscope to spin indefinitely; this led to the first prototype heading indicators, and a rather more complicated device, the gyrocompass. The first functional gyrocompass was patented in 1904 by German inventor Hermann Anschütz-Kaempfe. The American Elmer Sperry followed with his own design later that year, and other nations soon realized the military importance of the invention—in an age in which naval prowess was the most significant measure of military power—and created their own gyroscope industries. The Sperry Gyroscope Company quickly expanded to provide aircraft and naval stabilizers as well, and other gyroscope developers followed suit.
In 1917, the Chandler Company of Indianapolis, created the "Chandler gyroscope", a toy gyroscope with a pull string and pedestal. Chandler continued to produce the toy until the company was purchased by TEDCO inc. in 1982. The chandler toy is still produced by TEDCO today.
In the first several decades of the 20th century, other inventors attempted (unsuccessfully) to use gyroscopes as the basis for early black box navigational systems by creating a stable platform from which accurate acceleration measurements could be performed (in order to bypass the need for star sightings to calculate position). Similar principles were later employed in the development of inertial navigation systems for ballistic missiles.
During World War II, the gyroscope became the prime component for aircraft and anti-aircraft gun sights. After the war, the race to miniaturize gyroscopes for guided missiles and weapons navigation systems resulted in the development and manufacturing of so-called midget gyroscopes that weighed less than 3 oz and had a diameter of approximately 1 in. Some of these miniaturize gyroscopes could reach a speed of 24,000 revolutions per minute in less than 10 seconds.
Three-axis MEMS-based gyroscopes are also being used in portable electronic devices such as Apple's current generation of iPad, iPhone and iPod touch. This adds to the 3-axis acceleration sensing ability available on previous generations of devices. Together these sensors provide 6 component motion sensing; acceleration for X,Y, and Z movement, and gyroscopes for measuring the extent and rate of rotation in space (roll, pitch and yaw).
Variations.
Gyrostat.
A gyrostat is a variant of the gyroscope. It consists of a massive flywheel concealed in a solid casing. Its behaviour on a table, or with various modes of suspension or support, serves to illustrate the curious reversal of the ordinary laws of static equilibrium due to the gyrostatic behaviour of the interior invisible flywheel when rotated rapidly. The first gyrostat was designed by Lord Kelvin to illustrate the more complicated state of motion of a spinning body when free to wander about on a horizontal plane, like a top spun on the pavement, or a hoop or bicycle on the road. Kelvin also made use of gyrostats to develop mechanical theories of the elasticity of matter and of the ether; these theories are of purely historical interest today.
In modern times, the gyrostat concept is used in the design of attitude control systems for orbiting spacecraft and satellites. For instance, the Mir space station had three pairs of internally mounted flywheels known as "gyrodynes" or "control moment gyros".
In physics, there are several systems whose dynamical equations resemble the equations of motion of a gyrostat. Examples include a solid body with a cavity filled with an inviscid, incompressible, homogeneous liquid, the static equilibrium configuration of a stressed elastic rod in elastica theory, the polarization dynamics of a light pulse propagating through a nonlinear medium, the Lorenz system in chaos theory, and the motion of an ion in a Penning trap mass spectrometer.
MEMS.
A MEMS gyroscope takes the idea of the Foucault pendulum and uses a vibrating element, known as a MEMS (microelectromechanical system). The MEMS-based gyro was initially made practical and producible by Systron Donner Inertial (SDI). Today, SDI is a large manufacturer of MEMS gyroscopes.
FOG.
A fiber optic gyroscope (FOG) is a gyroscope that uses the interference of light to detect mechanical rotation. The sensor is a coil of as much as 5 km of optical fiber. The development of low-loss single-mode optical fiber in the early 1970s for the telecommunications industry enabled the development of Sagnac effect fiber optic gyroscopes.
HRG.
The hemispherical resonator gyroscope (HRG), also called wine-glass gyroscope or mushroom gyro, makes using a thin solid-state hemispherical shell, anchored by a thick stem. This shell is driven to a flexural resonance by electrostatic forces generated by electrodes which are deposited directly onto separate fused-quartz structures that surround the shell. Gyroscopic effect is obtained from the inertial property of the flexural standing waves.
VSG or CVG.
A vibrating structure gyroscope (VSG), also called a Coriolis vibratory gyroscope (CVG), uses a resonator made of different metallic alloys. It takes a position between the low-accuracy, low-cost MEMS gyroscope and the higher-accuracy and higher-cost FOG. Accuracy parameters are increased by using low-intrinsic damping materials, resonator vacuumization, and digital electronics to reduce temperature dependent drift and instability of control signals.
High quality wine-glass resonators are used for precise sensors like HRG or CRG.
DTG.
A dynamically tuned gyroscope (DTG) is a rotor suspended by a universal joint with flexure pivots. The flexure spring stiffness is independent of spin rate. However, the dynamic inertia (from the gyroscopic reaction effect) from the gimbal provides negative spring stiffness proportional to the square of the spin speed (Howe and Savet, 1964; Lawrence, 1998). Therefore, at a particular speed, called the tuning speed, the two moments cancel each other, freeing the rotor from torque, a necessary condition for an ideal gyroscope.
RLG.
A ring laser gyroscope relies on the movement of a light source and a co-located color detector, since the constant velocity of light waves from a moving platform will cause the beam to change phase in a manner similar to the color change known as redshift
London moment.
A London moment gyroscope relies on the quantum-mechanical phenomenon, whereby a spinning superconductor generates a magnetic field whose axis lines up exactly with the spin axis of the gyroscopic rotor. A magnetometer determines the orientation of the generated field, which is interpolated to determine the axis of rotation. Gyroscopes of this type can be extremely accurate and stable. For example, those used in the Gravity Probe B experiment measured changes in gyroscope spin axis orientation to better than 0.5 milliarcseconds (1.4×10−7 degrees) over a one-year period. This is equivalent to an angular separation the width of a human hair viewed from 32 km away.
The GP-B gyro consists of a nearly-perfect spherical rotating mass made of fused quartz, which provides a dielectric support for a thin layer of niobium superconducting material. To eliminate friction found in conventional bearings, the rotor assembly is centered by the electric field from six electrodes. After the initial spin-up by a jet of helium which brings the rotor to 4,000 RPM, the polished gyroscope housing is evacuated to an ultra-high vacuum to further reduce drag on the rotor. Provided the suspension electronics remain powered, the extreme rotational symmetry, lack of friction, and low drag will allow the angular momentum of the rotor to keep it spinning for about 15,000 years.
A sensitive DC SQUID is able to discriminate changes as small as one quantum, or about 2 ×10−15 Wb, is used to monitor the gyroscope. A precession, or tilt, in the orientation of the rotor causes the London moment magnetic field to shift relative to the housing. The moving field passes through a superconducting pickup loop fixed to the housing, inducing a small electric current. The current produces a voltage across a shunt resistance, which is resolved to spherical coordinates by a microprocessor. The system is designed to minimize Lorentz torque on the rotor.
Modern uses.
In addition to being used in compasses, aircraft, computer pointing devices, etc., gyroscopes have been introduced into consumer electronics. Since the gyroscope allows the calculation of orientation and rotation, designers have incorporated them into modern technology. The integration of the gyroscope has allowed for more accurate recognition of movement within a 3D space than the previous lone accelerometer within a number of smartphones. Gyroscopes in consumer electronics are frequently combined with accelerometers (acceleration sensors) for more robust direction- and motion-sensing. Examples of such applications include smartphones such as the Samsung Galaxy Note 4, HTC Titan, Nexus 5, iPhone 5s, Nokia 808 PureView and Sony Xperia, game console peripherals such as the PlayStation 3 controller and the Wii Remote, and virtual reality sets such as the Oculus Rift.
Nintendo has integrated a gyroscope into the Wii console's Wii Remote controller by an additional piece of hardware called "Wii MotionPlus". It is also included in the 3DS and the Wii U GamePad, which detects movement when turning.
Cruise ships use gyroscopes to level motion-sensitive devices such as self-leveling pool tables.
Described as a "motorbike-car" hybrid, the Lit Motors C-1 two wheeler uses a set of futuristic electronic gyroscopes, or control momentum gyroscopes, to ensure it remains upright and balanced, similar to the positioning technology used in the International Space Station and the Hubble Space Telescope. A similarity the Lit C-1 shares with the Segway is an IMU. A similar device has been used in the RYNO and Honda UX3 monocycles.
An electric powered flywheel gyroscope inserted in a bicycle wheel is being sold as a training wheel replacement.

</doc>
<doc id="44126" url="http://en.wikipedia.org/wiki?curid=44126" title="Hitch">
Hitch

Hitch may refer to:

</doc>
<doc id="44127" url="http://en.wikipedia.org/wiki?curid=44127" title="Military incompetence">
Military incompetence

Military incompetence refers to incompetencies and failures of military organisations, whether through incompetent individuals or through a flawed institutional culture.
The effects of isolated cases of "personal" incompetence can be disproportionately significant in military organisations. Strict hierarchies of command provide the opportunity for a single decision to direct the work of thousands, whilst an institutional culture devoted to following orders without debate can help ensure that a bad or miscommunicated decision is implemented without being challenged or corrected.
However, the most common cases of "military incompetence" can be attributable to a flawed organisational culture. Perhaps the most marked of these is a conservative and traditionalist attitude, where innovative ideas or new technology are discarded or left untested. A tendency to believe that a problem can be solved by applying an earlier (failed) solution "better", be that with more men, more firepower, or simply more "élan", is common. A strict hierarchical system often discourages the devolution of power to junior commanders, and can encourage micromanagement by senior officers.
The nature of warfare provides several factors which exacerbate these effects; the fog of war means that information about the enemy forces is often limited or inaccurate, making it easy for the intelligence process to interpret the information to agree with existing assumptions, or to fit it to their own preconceptions and expectations. Communications tend to deteriorate in battlefield situations, with the flow of information between commanders and combat units being disrupted, making it difficult to react to changes in the situation as they develop.
After operations have ceased, military organisations often fail to learn effectively from experience. In victory, whatever methods have been used—no matter how inefficient—appear to have been vindicated, whilst in defeat there is a tendency to select scapegoats and to avoid looking in detail at the broader reasons for failure.

</doc>
<doc id="44128" url="http://en.wikipedia.org/wiki?curid=44128" title="Lashing (ropework)">
Lashing (ropework)

A lashing is an arrangement of rope wire or webbing with linking device used to secure and fasten two or more items together in a somewhat rigid manner. Lashings are most commonly applied to timber poles, and are commonly associated with the cargo, containerisation, the Scouting movement, and with sailors.
This word usage derives from using whipcord to tie things together.
It has been imagined that the first lashing made by humans was wrapping a few strips of bark around a stone to hold it to a tree branch to make an ax to hunt and build with. In modern times, the same methods are used, but strips of bark and vines have been replaced with natural and synthetic fiber ropes. Scouts and campers use lashings to build camp gadgets and improve their campsites for comfort and convenience. Lashings are also used in pioneering, the art of creating structures such as bridges and towers, using ropes and wooden spars.
There are still areas in the world where lashing spars (or poles) is the basic means of building.
Types.
Square lashing.
Square lashing is a type of lashing used to bind spars together. There are different types, but all consist of a series of wraps around the spars, and fraps around the wraps between the spars.
Diagonal lashing.
Diagonal lashing is a type of lashing used to bind spars or poles together, to prevent racking. It gets its name from the fact that the wrapping turns cross the poles diagonally and is used to spring poles together where they do not touch as in the X-brace of a trestle.
Shear lashing.
Shear lashing (two-spar shear lashing) also spelled "sheer lashing" is used for lashing together two parallel spars which will be opened out of the parallel to form sheer legs as in the formation of an A-frame. The clove hitch is tied around one leg only and frapping turns are taken between the poles.
Round lashing.
The round lashing is most frequently used to join two poles together to extend their length. In the simple version, a clove hitch is tied around both poles and there are no frapping turns.
Tripod lashing.
The tripod lashing (also known as gyn lashing, figure of eight lashing, and three-spar shear lashing) is used to join three spars together to form a tripod.

</doc>
<doc id="44130" url="http://en.wikipedia.org/wiki?curid=44130" title="Plait">
Plait

A plait may refer to:
Plait may also refer to:

</doc>
<doc id="44132" url="http://en.wikipedia.org/wiki?curid=44132" title="Ramallah">
Ramallah

Ramallah (Arabic: رام الله‎, pronounced "Rāmallāh"   ) is a Palestinian city in the central West Bank located 10 km north of Jerusalem at an elevation of 875 meters above sea level, adjacent to al-Bireh. It currently serves as the de facto administrative capital of the State of Palestine. With a population of nearly 27,092 in 2007, Ramallah was historically a Christian town, but today Muslims form the majority of the population, with Christians still making up a significant minority.
Etymology.
"Ramallah" is composed of "Ram", an Aramaic word that means "high place or mountain", and "Allah", the Arabic word for God, or "the Hill of God".
History.
Rock-cut tombs have been found by Ramallah, as have potsherds from the Crusader/Ayyubid and early Ottoman period.
Ramallah has been identified with the Crusader place called "Ramalie". Remains of a building with an arched doorway from the Crusader era, called "al-Burj", has been identified, but the original use of the building is undetermined.
Ottoman era.
Ramallah was incorporated into the Ottoman Empire in 1517 with all of Palestine, and in 1596 it appeared in the tax registers as being in the "nahiya" of Quds of the "Liwa of Quds". It had a population of 71 Christian households and 9 Muslim households. It paid taxes on wheat, barley, olives, vines or fruit trees, and goats or beehives.
Modern Ramallah was founded in the mid-1500s by the Haddadins, a clan of brothers descended from Ghassanid Christians. The Haddadins, and their leader Rashid El-Haddadin, arrived from east of the Jordan River from the areas of Karak and Shoubak. The Haddadin migration is attributed to fighting and unrest among clans in that area.
Rashid and his brothers were Blacksmiths. The Haddadin name comes from the old (Aramaicܚܕܕ or ܚܕܐܕ ) word Haddad, which translates to Blacksmith.
Haddadin was attracted to the mountainous site of Ramallah because it was similar to the other mountainous areas he came from, as well as being a heavily forested area which could supply him with plenty of fuel for his forges.
In 1838 Edward Robinson visited, and found the inhabitants to be Christian "of the Greek rite". There were 200 taxable men, which gives an estimated total population of 800-900 people. The village "belonged" to the Haram al-Sharif, Jerusalem, to which it paid an annual tax of 350 Mids of grain.
In 1883, the Palestine Exploration Fund's "Survey of Western Palestine" described Ramallah as "A large Christian village, of well-built
stone houses, standing on a high ridge, with a view on the west extending to the sea. It stands amongst gardens and olive-yards, and has three springs to the south and one on the west; on the north there are three more, within a mile from the village. On the east there is a well. There are rock-cut tombs to the north-east with well-cut entrances, but completely blocked with rubbish. In the village is a Greek church, and on the east a Latin convent and a Protestant schoolhouse, all modern buildings. The village lands are Wakuf, or ecclesiastical property, belonging to the Haram of Jerusalem. About a quarter of the inhabitants are Roman Catholics, the rest Orthodox Greeks."
Today, a large community of people with direct descent from the Haddadins who founded Ramallah live in the United States. The town is now predominately Muslim, but still contains a Christian minority. This is due mostly to new immigration of Muslims to the area, and emigration of Christians from the area.
Christian presence.
Ramallah grew dramatically throughout the 17th and 18th centuries as an agricultural village; thus, attracting more (predominantly Christian) inhabitants from all around the region. In 1700, Yacoub Elias was the first Ramallah native to be ordained by the Eastern Greek Melkite Orthodox Church of Jerusalem, the Christian denomination that prevailed in the Holy Land at the time. In the early 19th century, the first Greek Melkite Jerusalemite Orthodox Christian church was built. Later in the 1850s, "The Church of Transfiguration", was built to replace it and is the sole Orthodox Church in Ramallah today. During that same decade, the Latin (Roman Catholic) Church established its presence in Ramallah, constituting the second largest Christian denomination in the city. The Roman Catholic Church established the St. Joseph's Girl's School runs by St. Joseph sisters, as well as the co-educational Al-Ahliyyah College high school runs by Rosary sisters. With the influx of Muslim and Christian refugees and internal migration, new mosques and churches were built.
In the 19th century, the Religious Society of Friends (Quakers) established a presence in Ramallah and built the Ramallah Friends Schools, one for girls and later a boys school, to alleviate the dearth of education for women and girls. Eli and Sybil Jones opened “The Girls Training Home of Ramallah” in 1869. A medical clinic was established in 1883, with Dr. George Hassenauer serving as the first doctor in Ramallah. In 1889, the girls academy became the Friends Girls School (FGS). As the FGS was also a boarding school, it attracted a number of girls from surrounding communities, including Jerusalem, Lydda, Jaffa, and Beirut. The Friends Boys School (FBS) was founded in 1901 and opened in 1918. The Quakers opened a Friends Meeting House for worship in the city center in 1910. According to the schools' official website, most high school students choose to take the International Baccalaureate exams instead of the traditional "Tawjihi" university exams.
The activity of foreign churches in Palestine in the late 19th century increased awareness of prosperity in the West. In Ramallah and Bethlehem, a few miles south, local residents began to seek their fortunes overseas. In 1901, merchants from Ramallah emigrated to the United States and established import-export businesses, selling handmade rugs and other exotic wares across the Atlantic. Increased trade dramatically improved living standards for Ramallah's inhabitants. American cars, mechanized farming equipment,radios, and later televisions became attainable luxuries for upper-class families. As residents of Jaffa and Lydda moved to Ramallah, the balance of Muslims and Christians began to change.
Ramallah was declared a modern city in 1908 and had an elected municipality as well as partnership projects with the adjacent town of al-Bireh. The Friends Boys School became a temporary hospital during the war.
British Mandate.
The British Army occupied Ramallah in December 1917, and it remained under British rule until 1948. The economy improved in the 1920s, and the landed aristocracy and merchants of the Palestinian upper class built stately multi-storied villas, many of which still stand. The Jerusalem Electric Company brought electricity to Ramallah in 1936, and most homes were wired shortly thereafter. The same year the British inaugurated the "Palestine Broadcasting Service" in Ramallah. The British Broadcasting Corporation trained the local staff to deliver daily broadcasts in Arabic, Hebrew, and English. The station was later renamed "Kol Yerushalayim" (The Voice of Jerusalem).
During the 1936–39 Arab revolt in Palestine, Ramallah was a center of activities against the British. Nancy Parker McDowell describes vividly how the British attacked Ramallah using the Air Force. Many were killed and wounded. The wounded had to be transferred to Jerusalem since no significant medical facilities existed in Ramallah.
Jordanian occupation.
Following the creation of the State of Israel and the ensuing war, Jordan seized the part of Palestine they named the West Bank. This included Ramallah. The West Bank was relatively peaceful during the years of Jordanian rule between 1948 and 1967, with its residents enjoying freedom of movement between the West Bank, Jordan, Lebanon, and Syria. Jordan annexed the West Bank, applying its national law to the conquered territory. However, many Palestinians were jailed for being members of "illegal political parties", which included the Palestine Communist Party and other socialist and pro-independence groups. Jordanian law also allegedly restricted the creativity and freedom desired by many Palestinians. By 1953, Ramallah's population had doubled, but the economy and infrastructure could not accommodate the influx of poor villagers. Natives of Ramallah began to emigrate, primarily to the United States. By 1956, about one fourth of Ramallah's 6,000 natives had left, with Arabs from the surrounding towns and villages( particularly Hebron) buying the homes and land the émigrés left behind. 
Israeli occupation.
During the Six-Day War in 1967, Israel captured Ramallah from Jordan, imposing a military closure and conducting a census a few weeks later. Every person registered in the census was given an Israeli identity card which allowed the bearer to continue to reside there. Those who were abroad during the census lost their residency rights. For residents of Ramallah, the situation had now reversed itself; for the first time in 19 years residents could freely visit Israel and the Gaza Strip and engage in commerce there.
Unlike the Jordanians, Israel did not offer citizenship to the residents. Ramallah residents were issued permits to work in Israel, but did not gain the rights associated with Israeli citizenship. The city remained under Israeli military rule for over four decades. The Israeli Civil Administration (CA), established in 1981, was in charge of civilian and day-to-day services such as issuing permission to travel, build, export or import, and host relatives from abroad. The CA reprinted Jordanian textbooks for distribution in schools but did not update them. The CA was in charge of tax collection and land expropriation, which sometimes included Israeli theft of olive groves that Arab villagers had tended for generations. According to the Israeli Human Rights activists, Jewish settlements in the Ramallah area, such as Beit El and Psagot, prevented the expansion of the city and cut it off from the surrounding villages. As resistance increased, Ramallah residents were jailed or deported to neighboring countries for membership in the Palestine Liberation Organization. In December 1987, the popular uprising known as the Intifada erupted.
First Intifada.
Ramallah residents were among the early joiners of the First Intifada. The Intifada Unified Leadership, an umbrella organization of various Palestinian factions, distributed weekly bulletins on the streets of Ramallah with a schedule of the daily protests, strikes and action against Israeli patrols in the city. At the demonstrations, tyres were burned in the street and the crowds threw stones and Molotov cocktails. The IDF responded with tear gas and rubber bullets. Schools in Ramallah were forcibly shut down, and opened gradually for a few hours a day. House arrests were carried out and curfews were imposed that restricted travel and exports in what Palestinians regarded as collective punishment. In response to the closure of schools, residents organized home schooling sessions to help students make up missed material; this became one of the few symbols of civil disobedience. The Intifada leadership organized "tree plantings" and resorted to the tactics used in pre-1948 Palestine, such as ordering general strikes in which no commercial businesses were allowed to open and no cars were allowed on the streets.
In 1991, the Palestinian delegation to the Madrid International Peace Conference included many notables from Ramallah. As the Intifada wound down and the peace process moved forward, normal life in Ramallah resumed. On September 13, 1993 the famous White House handshake between Israeli Prime Minister Yitzhak Rabin and Palestinian Leader Yasser Arafat took place, and schoolchildren in Ramallah handed out olive branches to Israeli soldiers patrolling the streets. In December 1995, in keeping with the Oslo Accords, the Israeli army abandoned the Mukataa and withdrew to the city outskirts. The newly established Palestinian Authority assumed civilian and security responsibility for the city, which was designated "Area A" under the accords.
Palestinian Authority rule.
1990s.
The years between 1993 and 2000 (known locally as the "Oslo Years") brought relative prosperity to Ramallah. Many expatriates returned to establish businesses there and the atmosphere was one of optimism. In 2000, unemployment began to rise and the economy of Ramallah declined. The Israel Defense Forces remained in control of the territories and the freedom of movement enjoyed by Ramallah residents prior to the first Intifada was not restored. Travel to Jerusalem required special permits, and expansion of Israeli settlements around Ramallah increased dramatically. A network of bypass roads for use of Israeli citizens only was built around Ramallah, and land was confiscated for settlements. Many official documents previously handled by the Israeli Civil Administration were now handled by the Palestinian Authority but still required Israeli approval. A Palestinian passport issued to Ramallah residents was not valid unless the serial number was registered with the Israeli authorities, who controlled border crossings. The failure of the Camp David summit in July 2000 led to the outbreak of the Second Intifada (al-Aqsa Intifada) in September 2000.
Second Intifada.
Young Ramallah residents demonstrated daily against the Israeli army, with marches to the Israeli checkpoints at the outskirts of the city. Over time, the marches were replaced by sporadic use of live ammunition against Israeli soldiers; and various attacks targeting Jewish settlers, particularly on the Israeli-only bypass roads. Army checkpoints were established to restrict movement in and out of Ramallah.
On October 12, 2000, two Israeli army reservists, Vadim Norzhich and Yosef Avrahami were lynched in Ramallah. They had taken a wrong turn, and were set upon by a mob, enraged in particular by the Muhammad al-Durrah incident in Gaza. A frenzied crowd killed the two IDF reservists, mutilated their bodies, and dragged them through the streets. Later that afternoon, Israeli army carried out an air strike on Ramallah, demolishing the police station, Israel later succeeded in capturing and prosecuting some of those involved in the deaths.
In 2002, Ramallah was reoccupied by Israel in an IDF operation codenamed Operation Defensive Shield, which resulted in curfews, electricity cuts, school closures and disruptions of commercial life. Many Ramallah institutions, including government ministries, were vandalized, and equipment was destroyed or stolen. The IDF took over local Ramallah television stations, and social and economic conditions deteriorated. Many expatriates left, as did many other Palestinians who complained that the living conditions had become intolerable. The Israeli West Bank barrier has furthered Ramallah's isolation.
Yasser Arafat established his West Bank headquarters, the Mukataa, in Ramallah. Although considered an interim solution, Ramallah became the de facto capital of the Palestinian Authority, now officially known as the State of Palestine, hosting almost all governmental headquarters. In December 2001, Arafat held meetings at the Mukataa, but lived with his wife and daughter in Gaza City. After suicide bombings in Haifa, Arafat was confined to the Ramallah compound. In 2002, the compound was partly demolished by the Israeli Defense Forces and Arafat's building was cut off from the rest of the compound.
On November 11, 2004 Arafat died at the Percy training hospital of the Armies near Paris. He was buried in the courtyard of the Mukataa on November 12, 2004. The site still serves as the Ramallah headquarters of the Palestinian Authority, as well the official West Bank office of Mahmoud Abbas. Throughout 2005, while the Disengagement Plan was underway, it was suggested by some US government officials to the Palestinian leadership to move the provisional capital back to Gaza, as was the case when the Palestinian Aurhority was just established in 1994. President Abbas, however, refrained from doing so arguing that at this point it was important to keep the administrative center in the West Bank in order to remind the international community that the West Bank was still awaiting territorial solution.
Economic rehabilitation.
In December 2005, local elections were held in Ramallah in which candidates from three different factions competed for the 15-seat municipal council for a four-year term. The council elected Janet Mikhail as mayor, the first woman to hold the post.
Munir Hamdan, a member of Fatah and a Ramallah businessman, told a journalist that the fact that “The president and prime minister have their offices here. So do the parliament and all the government ministries,” represents a "collusion" between the Palestinian Authority and Israel to turn Ramallah into the political as well as the financial capital of the Palestinians. He is particularly worried by the construction of a large new governmental complex by the PA. Hatem Abdel Kader, a Jerusalem resident, Fatah legislator and former Minister for Jerusalem Affairs, complained that “If they are building a new government compound here, that means they have no plans to be based in Jerusalem... Unfortunately, the Palestinian government of Salam Fayyad has abandoned Jerusalem in favor of Ramallah.”
Many foreign nations have located their diplomatic missions to the Palestinian Authority in Ramallah, including, as of 2010, Argentina, Australia, Austria, Korea, South Africa, Norway, Sri Lanka, Switzerland, China, Poland, Portugal, The Netherlands, Russia, Jordan, Brazil, Finland, Denmark, Ireland, Germany, India, Japan, the Czech Republic, Canada and Mexico.
In November 2011, king Abdullah II of Jordan visited Ramallah for the first time since 2000.
Geography and climate.
This area enjoys a Mediterranean climate of a dry summer and mild, rainy winter with occasional snowfall. The recorded average of Ramallah's rainfall is about 694 mm and minimum rainfall is 307 mm and maximum rainfall is 1591 mm.
The Köppen climate classification places Ramallah in the Csa category. Climates of this class generally occur on the western sides of continents between the latitudes of 30° and 45°. These climates are in the polar front region in winter, and thus have moderate temperatures and changeable, rainy weather. Summers are hot and dry, due to the domination of the subtropical high pressure systems, except in the immediate coastal areas, where summers are milder due to the nearby presence of cold ocean currents that may bring fog but prevent rain.
Economy.
Ramallah has been described as the seat of power of the Palestinian Authority and serves as the headquarters for most international NGOs and embassies. Hundreds of millions of dollars in aid flowing into the city have boosted Ramallah’s economy greatly since the end of the Second Intifada.
The Ramallah construction boom is one of the most obvious signs of West Bank economic growth estimated at an annual rate of 8 percent. This has been attributed to relative stability and Western donor support to the Palestinian Authority. Ramallah's buoyant economy continues to draw Palestinians from other West Bank towns where jobs are fewer. The built-up area has grown fivefold since 2002.
By 2010, Ramallah had become the leading center of economic and political activity in the territories under the control of the Palestinian Authority. A building boom in the early years of the 21st century saw apartment buildings and "five-star" hotels erected, particularly in the Al-Masyoun neighborhood. In 2010, "more than one hundred" Palestinian businesses were reported to have moved to Ramallah from East Jerusalem, because “Here they pay less taxes and have more customers." One local boasted to a journalist that “Ramallah is becoming the de facto capital of Palestine.” This boast was seconded by the New York Times which, in 2010, called Ramallah the "de facto capital of the West Bank. According to Sani Meo, the publisher of This Week in Palestine, "Capital or no capital, Ramallah has done well and Palestine is proud of its achievements.” Some Palestinians allege that Ramallah's prosperity is part of an Israeli "conspiracy" to make Ramallah the capital of a Palestinian state, instead of Jerusalem.
ASAL technologies, an information technology company in Ramallah, has 120 employees and is looking forward to "exponential growth." 
Demographics.
In the 1922 census of Palestine conducted by the British Mandate authorities, Ramallah had a population of 3,104; 2,972 Christians, 125 Muslims, and 10 Jews. 
This had increased at the time of the 1931 census to 4,286, 3766 Christians, 519 Muslim and 1 Jew, in a total of 1014 houses.
In Sami Hadawi's 1945 survey, the population stood at 5,080, with Christians forming the majority of the population. However, the demographic makeup of the town changed drastically between 1948 and 1967 with only slightly more than half of the city's 12,134 inhabitants being Christian, the other half Muslim.
Ramallah's population drastically decreased in the late 20th century from 24,722 inhabitants in 1987 to 17,851 in 1997. In the Palestinian Central Bureau of Statistics (PCBS) census in 1997, Palestinian refugees accounted for 60.3% of the population which was 17,851. There were 8,622 males and 9,229 females. People younger than 20 years of age made up 45.9% of the population, while those aged between 20 and 64 were 45.4%, and residents aged over 64 constituted 4.7%.
Only in 2005 did the population reach over 24,000. In a PCBS projection in 2006, Ramallah had a population of 25,467 inhabitants. In the 2007 PCBS census, there were 27,460 people living in the city. Sources vary about the current Christian population in the city, ranging around 25%.
Infrastructure.
Health.
In the aftermath of the 1936–39 Arab revolt, the Ramallah Hospital Foundation was established and registered as a tax exempt organization in New York in 1944. It bought large pieces of land in the south-eastern fringes of the city dedicated for the future hospital. In 1963 a hospital was opened. The present Ramallah Government Hospital and the Palestine Medical Centered are located on the land purchased by the Foundation. In January 1987 the first open-heart surgery was performed at the Hospital under the direction of Dr. Shehadeh (Shawki) Harb, a Palestinian surgeon trained in the United States.
Religious institutions.
The Jamal Abdel Nasser Mosque is one of the city's largest. The Orthodox Church of Ramallah, an Orthodox Christian convent, Melkite Catholic Church, Evangelical Lutheran Church, Arab Episcopal (Anglican) Church, Ramallah Local Church (Evangelical\Born Again) and Ramallah Baptist Church all operate schools in the city. A large new church has been built on top of one of the highest hills of Ramallah, belonging to the Coptic Orthodox Church. A small group of Jehovah Witnesses are present in the area as well and others.
During the annual "Saturday of Light" religious festival (which occurs on the Saturday between Good Friday and Easter Sunday to commemorate the light that tradition holds shone from the tomb of Jesus), the scouts hold a parade through the city streets to receive the flame from Jerusalem. (The flame is ignited in Jerusalem's Church of the Holy Sepulchre and is passed on through candles and lanterns to regional churches.) A variety of mosques and churches of different denominations dot the landscape.
Culture.
Ramallah is generally considered the most affluent and cultural, as well as the most liberal, of all Palestinian cities, and is home to a number of popular Palestinian activists, poets, artists, and musicians. It boasts a lively nightlife, with many restaurants including the Stars and Bucks Cafe, a branch of the Tche Tche Cafe and the Orjuwan Lounge, described in 2010 as two among the "dozens of fancy restaurants, bars and discotheques that have cropped up in Ramallah in the last three years."
One hallmark of Ramallah is Rukab's Ice Cream, which is based on the resin of chewing gum and thus has a distinctive taste. Another is the First Ramallah Group, a boy- and girl-scout club that also holds a number of traditional dance (Dabka) performances and is also home to men's and women's basketball teams that compete regionally. International music and dance troupes occasionally make a stop in Ramallah, and renowned Israeli pianist Daniel Barenboim performs there often. The Khalil Sakakini Cultural Center, founded in 1996, is a popular venue for such events. The Al-Kasaba Theatre is a venue for plays and movies. In 2004, the state-of-the art Ramallah Cultural Palace opened in the city. The only cultural center of its kind in the Palestinian-governed areas, it houses a 736-seat auditorium, as well as conference rooms, exhibit halls, and movie-screening rooms. It was a joint venture of the Palestinian Authority, the United Nations Development Programme (UNDP), and the Japanese government. Ramallah hosted its first annual international film festival in 2004.
Ramallah folklore.
Ramallah, like most Palestinian areas, has a rich folklore of song and dance. Songs accompanied people in every occasion whether it was the harvest season, roofing a house, traveling, coming back from travel, engagement, wedding, or even death. Most of the songs were sung by the women with the exception of Zaffeh and Mal'ab which are sung by the men at wedding celebrations. Palestinian educator Bahia Khalil's book "Ramallah Folklore Songs and Traditions" documents to a great extend this oral tradition inherited from one generation to another. The second edition of the book was published in 2002 by the American Federation of Ramallah, Palestine, an organization for Palestinian-Americans from the Ramallah region living in the United States.
Foreign travelers to Palestine in late 19th and early 20th centuries often commented on the rich variety of costumes among the Palestinian people, and particularly among the fellaheen or village women. Until the 1940s, a woman's economic status, whether married or single, and the town or area they were from could be deciphered by most Palestinian women by the type of cloth, colors, cut, and embroidery motifs, or lack thereof, used for the robe-like dress or "thoub" in Arabic
Palestinian costume.
Though experts in the field trace the origins of Palestinian costumes to ancient times, there are no surviving clothing artifacts from this early period against which the modern items might be definitively compared. Influences from the various empires to have ruled Palestine, such as Ancient Egypt, Ancient Rome, Byzantine empire, and Ayyubids, among others, have been documented by scholars largely based on the depictions in art and descriptions in literature of costumes produced during these times.
Hanan Munayyer, collector and researcher of Palestinian clothing, sees examples of proto-Palestinian attire in artifacts from the Canaanite period (1500 BCE) such as Egyptian paintings depicting Canaanites in A-shaped garments. Munayyer says that from 1200 BC to 1940 AD, all Palestinian dresses were cut from natural fabrics in a similar A-line shape with triangular sleeves. This shape is known to archaeologists as the "Syrian tunic" and appears in artifacts such as an ivory engraving from Megiddo dating to 1200 BC.
Until the 1940s, traditional Palestinian costumes reflected a woman's economic and marital status and her town or district of origin, with knowledgeable observers discerning this information from the fabric, colours, cut, and embroidery motifs (or lack thereof) used in the apparel.
Due to the difficulty of travel in the 19th century, villages in Palestine remained isolated. As a result, clothing and accessories became a statement of region. In Ramallah, the back panels of dresses often incorporated a palm tree motif embroidered in cross-stitch. Ramallah women were famous for their distinctive dress of white linen fabric embroidered with red silk thread. The headdress or "smadeh" worn in Ramallah was common throughout northern Palestine: a small roundish cap, padded and stiffened, with gold and silver coins set in a fringe with a long veil pinned to the back, sometimes of silk and sometimes embroidered.
Twin towns – sister cities.
Ramallah is twinned with:
Bibliography.
</dl>

</doc>
<doc id="44133" url="http://en.wikipedia.org/wiki?curid=44133" title="Overture">
Overture

Overture (French "ouverture", lit. "opening"; German "Ouvertüre", "Vorspiel", i.e., "prelude", lit. "play before") in music is the term originally applied to the instrumental introduction to an opera. During the early Romantic era, composers such as Beethoven and Mendelssohn began to use the term to refer to independent, self-existing instrumental, programmatic works that presaged genres such as the symphonic poem. These were "at first undoubtedly intended to be played at the head of a programme".
History.
17th century.
The idea of an instrumental opening to opera existed during the 17th century. Peri's "Euridice" opens with a brief instrumental ritornello, and Monteverdi's "L'Orfeo" (1607) opens with a toccata, in this case a fanfare for muted trumpets. More important, however, was the prologue, which comprised sung dialogue between allegorical characters which introduced the overarching themes of the stories depicted.
French overture.
As a musical form, however, the French overture first appears in the court ballet and operatic overtures of Jean-Baptiste Lully, which he elaborated from a similar, two-section form called "Ouverture", found in the French ballets de cour as early as 1640. This French overture consists of a slow introduction in a marked "dotted rhythm" (i.e., exaggerated iambic, if the first chord is disregarded), followed by a lively movement in fugato style. The overture was frequently followed by a series of dance tunes before the curtain rose, and would often return following the Prologue to introduce the action proper. This ouverture style was also used in English opera, most notably in Henry Purcell's "Dido and Æneas". Its distinctive rhythmic profile and function thus led to the French overture style as found in the works of late Baroque composers such as Johann Sebastian Bach. The style is most often used in preludes to suites, and can be found in non-staged vocal works such as cantatas, for example in the opening chorus of Bach's cantata "Nun komm, der Heiden Heiland, BWV 61". Handel also uses the French overture form in some of his Italian operas such as Giulio Cesare.
Italian overture.
In Italy, a distinct form called "overture" arose in the 1680s, and became established particularly through the operas of Alessandro Scarlatti, and spread throughout Europe, supplanting the French form as the standard operatic overture by the mid-18th century. Its usual form is in three generally homophonic movements: fast–slow–fast. The opening movement was normally in duple metre and in a major key; the slow movement in earlier examples was usually quite short, and could be in a contrasting key; the concluding movement was dance-like, most often with rhythms of the gigue or minuet, and returned to the key of the opening section. As the form evolved, the first movement often incorporated fanfare-like elements and took on the pattern of so-called "sonatina form" (sonata form without a development section), and the slow section became more extended and lyrical. Italian overtures were often detached from their operas and played as independent concert pieces. In this context, they became important in the early history of the symphony.
19th-century opera.
In 19th-century opera the overture, "Vorspiel", "Einleitung," Introduction, or whatever else it may be called, is generally nothing more definite than that portion of the music which takes place before the curtain rises. Richard Wagner's "Vorspiel" to "Lohengrin" is a short self-contained movement founded on the music of the Grail.
In Italian opera after about 1800, the "overture" became known as the "sinfonia". Fisher also notes the term "Sinfonia avanti l'opera" (literally, the "symphony before the opera") was "an early term for a sinfonia used to begin an opera, that is, as an overture as opposed to one serving to begin a later section of the work".
Concert overture.
Early 19th century.
Although by the end of the eighteenth century opera overtures were already beginning to be performed as separate items in the concert hall, the "concert overture", intended specifically as an individual concert piece without reference to stage performance and generally based on some literary theme, began to appear early in the Romantic era. Carl Maria von Weber wrote two concert overtures, "Der Beherrscher der Geister" ('The Ruler of the Spirits', 1811, a revision of the overture to his unfinished opera "Rübezahl" of 1805), and "Jubel-Ouvertüre" ('Jubilee Overture', 1818, incorporating "God Save the King" at its climax). However, the overture "A Midsummer Night's Dream" (1826) by Felix Mendelssohn is generally regarded as the first concert overture (Temperley 2001). Mendelssohn's other contributions to this genre include his "Calm Sea and Prosperous Voyage" overture (1828), his overture "The Hebrides" (1830; also known as "Fingal's Cave") and the overtures "Die schöne Melusine" ("The Fair Melusine", 1834) and "Ruy Blas" (1839). Other notable early concert overtures were written by Hector Berlioz (e.g., "Les Francs juges" (1826), and "Le corsaire" (1828)).
Later 19th century.
In the 1850s the concert overture began to be supplanted by the symphonic poem, a form devised by Franz Liszt in several works that began as dramatic overtures. The distinction between the two genres was the freedom to mould the musical form according to external programmatic requirements (Temperley 2001). The symphonic poem became the preferred form for the more "progressive" composers, such as César Franck, Richard Strauss, Alexander Scriabin, and Arnold Schoenberg, while more conservative composers like Anton Rubinstein, Tchaikovsky, Johannes Brahms, Robert Schumann and Arthur Sullivan remained faithful to the overture.
In the age when the symphonic poem had already become popular, Brahms wrote his "Academic Festival Overture", Op. 80, as well as his "Tragic Overture", Op. 81. An example clearly influenced by the symphonic poem is Tchaikovsky's "1812 Overture". His equally well-known "Romeo and Juliet" is also labelled a 'fantasy-overture'.
20th century.
In European music after 1900, an example of an overture displaying a connection with the traditional form is Dmitri Shostakovich's "Festive Overture", Op. 96 (1954), which is in two linked sections, "Allegretto" and "Presto" (Temperely 2001). Malcolm Arnold's "A Grand, Grand Overture", Op. 57 (1956), is a 20th-century parody of the late 19th century concert overture, scored for an enormous orchestra with organ, additional brass instruments, and obbligato parts for four rifles, three Hoover vacuum cleaners (two uprights in B♭, one horizontal with detachable sucker in C), and an electric floor polisher in E♭; it is dedicated "to President Hoover".
Film.
In motion pictures, an overture is a piece of music setting the mood for the film before the opening credits start. For a comprehensive list, see the list of films with overtures.
List of some common overtures.
Some well-known or commonly played Overtures:

</doc>
<doc id="44136" url="http://en.wikipedia.org/wiki?curid=44136" title="John Huston">
John Huston

John Marcellus Huston (August 5, 1906 – August 28, 1987) was an American film director, screenwriter and actor. He wrote the screenplays for most of the 37 feature films he directed, many of which are today considered classics: "The Maltese Falcon" (1941), "The Treasure of the Sierra Madre" (1948), "Key Largo" (1948), "The Asphalt Jungle" (1950), "The African Queen" (1951), "Moulin Rouge" (1952), "The Misfits" (1961), and "The Man Who Would Be King" (1975). During his 46-year career, Huston received 15 Oscar nominations, won twice, and directed both his father, Walter Huston, and daughter, Anjelica Huston, to Oscar wins in different films.
Huston was known to direct with the vision of an artist, having studied and worked as a fine art painter in Paris in his early years. He continued to explore the visual aspects of his films throughout his career: sketching each scene on paper beforehand, then carefully framing his characters during the shooting. While most directors rely on post-production editing to shape their final work, Huston instead created his films while they were being shot, making them both more economical and cerebral, with little editing needed.
Most of Huston's films were adaptations of important novels, often depicting a "heroic quest," as in "Moby Dick", or "The Red Badge of Courage". In many films, different groups of people, while struggling toward a common goal, would become doomed, forming "destructive alliances," giving the films a dramatic and visual tension. Many of his films involved themes such as religion, meaning, truth, freedom, psychology, colonialism and war.
Before becoming a Hollywood filmmaker, he had been an amateur boxer, reporter, short-story writer, portrait artist in Paris, a cavalry rider in Mexico, and a documentary filmmaker during World War II. Huston has been referred to as "a titan", "a rebel", and a "renaissance man" in the Hollywood film industry. Author Ian Freer describes him as "cinema's Ernest Hemingway"—a filmmaker who was "never afraid to tackle tough issues head on."
Early life.
John Huston was born on August 5, 1906, in Nevada, Missouri. He was the only child of Rhea (née Gore) and Canadian-born Walter Huston, originally Walter Houghston. His father was an actor, initially in vaudeville, and later in films. His mother initially worked as a sports editor for various publications but gave it up after Huston was born. Similarly, his father gave up his stage acting career for steady employment as a civil engineer, although he returned to stage acting within a few years. He would later become highly successful on both Broadway and then in motion pictures. He had Scottish, Scots-Irish, English and Welsh ancestry.
Huston's parents divorced in 1913, when he was 6, and as a result much of his childhood was spent living in boarding schools. During summer vacations, he traveled with each of his parents separately — with his father on vaudeville tours, and with his mother to racetracks or other sports events. The young Huston benefited greatly from seeing his father act on stage, as he was later drawn to the world of acting. Some critics, such as Lawrence Grobel, surmise that his relationship with his mother may have been the cause of his five marriages, and why few of his relationships lasted. Grobel wrote, "When I interviewed some of the women who had loved him, they inevitably referred to his mother as the key to unlocking Huston's psyche." According to actress Olivia de Havilland, "she [his mother] was the central character. I always felt that John was ridden by witches. He seemed pursued by something destructive. If it wasn't his mother, it was his idea of his mother."
As a child he was often ill and was treated for an enlarged heart and kidney ailments. He recovered after an extended bedridden stay in Arizona, and moved with his mother to Los Angeles, where he went to Lincoln Heights High School. He dropped out of high school after two years to become a professional boxer, and by age 15 was already a top-ranking amateur lightweight boxer in California. He ended his brief boxing career after suffering a broken nose. He also "plunged" himself into a multitude of interests, including abstract painting, ballet, English and French literature, opera, and horseback riding. Living in Los Angeles he became "infatuated" with the new film industry and motion pictures, but as a spectator only. To Huston, "Charlie Chaplin was a god."
He moved back to New York to live with his father, who was then acting in off-Broadway productions, and John had a few small roles. He remembers, while watching his father rehearse, being fascinated with the mechanics of acting:
After a short period acting on stage, and having undergone surgery, he traveled on his own to Mexico. During his two years there, among his other adventures, he got a position riding as an honorary member of the Mexican cavalry. He returned to Los Angeles and married a girlfriend from high school, Dorothy Harvey. Their marriage only lasted a year.
Early career as writer.
During his stay in Mexico, he wrote a play called "Frankie and Johnny", based on the ballad of the same title. After selling it easily, he decided that writing would be a viable career, and he focused on it. His self-esteem was enhanced when H. L. Mencken, editor of the popular magazine, "American Mercury," bought two of his stories, "Fool" and "Figures of Fighting Men." During subsequent years his stories and feature articles were published in "Esquire," "Theatre Arts," and the "New York Times." He also worked for a period on the "New York Graphic." In 1931, when he was 25, he moved back to Los Angeles with his hopes aimed at writing for the blossoming film industry, where the silent film industry had given way to "talkies", and writers were in demand. In addition, his father had earlier moved there where he was already successful in a number of films.
He received a script editing contract with Samuel Goldwyn Productions, but after six months of receiving no assignments, quit to work for Universal Studios, where his father was by then a star. At Universal, he got a job in the script department, and began by writing dialogue for a number of films in 1932, including "Murders in the Rue Morgue", "A House Divided", and "Law and Order". The last two also starred his father, Walter Huston. In addition, "House Divided" was directed by William Wyler, who gave Huston his first real "inside view" of the filmmaking process during all stages of production. Wyler and Huston would also later become close friends and collaborators on a number of leading films.
Huston gained a reputation as a "lusty, hard-drinking libertine" during his first years as a writer in Hollywood. Huston describes those years as a "series of misadventures and disappointments", however. His brief career as a Hollywood writer ended suddenly after a car he was driving struck and killed a young female pedestrian. He was absolved of blame by a coroner's jury, but the incident left him "traumatized" nonetheless, and he moved to London and Paris, living as a "drifter."
By 1937, after five years, the 31-year-old Huston returned to Hollywood intent on being a "serious writer." He also married Lesley Black. His first job was as scriptwriter with Warner Brothers Studio, with his personal longterm goal of directing his own scripts. For the next four years, he co-wrote scripts for major films such as "Jezebel, " "The Amazing Dr. Clitterhouse", "Juarez", "Dr. Ehrlich's Magic Bullet" and "Sergeant York" (1941). He was nominated for an Academy Award for his writing both "Ehrlich" and "Sergeant York." Huston writes that "Sergeant York", which was directed by Howard Hawks, has "gone down as one of Howard's best pictures, and Gary Cooper had a triumph playing the young mountaineer.":77
Huston was becoming a recognized and respected screenwriter. He was able to persuade Warners to give him a chance to direct, under the condition that his next script also became a hit. Huston writes:
They indulged me rather. They liked my work as a writer and they wanted to keep me on. If I wanted to direct, why, they'd give me a shot at it, and if it didn't come off all that well, they wouldn't be too disappointed as it was to be a very small picture.
The next script he was given to work on was "High Sierra" (1941), to be directed by Raoul Walsh. The film became the hit Huston wanted. It also made Humphrey Bogart a star with his first major role, as a gunman on the run. Warners kept their end of the bargain, and gave Huston his choice of subject.
Screenwriter and director.
"The Maltese Falcon" (1941).
For his first directing assignment, Huston chose Dashiell Hammett's detective thriller, "The Maltese Falcon", a film which had already failed at the box office in two earlier versions by Warners. However, studio head Jack Warner approved of Huston's treatment of Hammett's 1930 novel, as he stood by his word to let Huston choose his first subject.
Huston kept the screenplay close to the novel, keeping much of Hammett's dialogue, and directing it in an uncluttered style, much like the book's narrative. He also did the unusual preparation for this, his first directing job, by sketching out each shot beforehand, including camera positions, lighting, and compositional scale, for such things as closeups.
He especially benefited by selecting a superior cast, giving Humphrey Bogart the lead role. Bogart was happy to take the role, as he liked working with Huston. In addition, the supporting cast included other noted actors: Mary Astor, Peter Lorre, Sydney Greenstreet (his first film role), and his own father, Walter Huston. The film, however, was given only a small B-movie budget, and received minimal publicity by Warners, as they had low expectations. The entire film was made in eight weeks for only $300,000.
Upon receiving immediate enthusiastic response by the public and critics, Warners was surprised. Critics hailed the film as a "classic", and up until the present day it is claimed by many to be the "best detective melodrama ever made." "Herald Tribune" critic Howard Barnes called it a "triumph." Huston again received an Academy Award nomination for the screenplay. After this film, Huston would from then on direct all of his screenplays, except for one, "Three Strangers" (1946).
In 1942, he directed two more hits, "In This Our Life" (1942), starring Bette Davis, and "Across the Pacific", another thriller starring Humphrey Bogart.
Army years during World War II.
In 1942 he was activated by the U.S. Army to make films for the Army Signal Corps. While in uniform with the rank of captain, he directed and produced three films that some critics rank as "among the finest made about World War II: "Report from the Aleutians" (1943), about soldiers preparing for combat; "The Battle of San Pietro" (1945), the story (censored by the Army) of a failure by America's intelligence agencies which resulted in many deaths, and "Let There Be Light" (1946), about psychologically damaged veterans, also censored for 35 years, until 1981. He rose to the rank of major and received the Legion of Merit award for "courageous work under battle conditions." Nonetheless, all of his films made for the Army were "controversial", and either not released, censored, or banned outright, as they were considered "demoralizing" to soldiers and the public. Years later, after moving to Ireland, his daughter, actress Anjelica Huston, recalled that the "main movies we watched were the war documentaries.":10
Huston did an uncredited rewrite of Anthony Veiller's screenplay for "The Stranger" (1946), a film he was to have directed. When Huston became unavailable Orson Welles was offered the opportunity to direct. He had been cast the in role of a high-ranking Nazi fugitive who manages to settle in New England under an assumed name.
"The Treasure of the Sierra Madre" (1948).
His next picture, which he wrote, directed, and briefly appeared in as an American, asked to "help out a fellow American, down on his luck", was "The Treasure of the Sierra Madre" (1948). It would become one of the films which established his reputation as a leading filmmaker. The film, also starring Humphrey Bogart, was the story of three drifters who band together to prospect for gold. Huston also gave a supporting role to his father, Walter Huston.
Warners studio was initially uncertain what to make of the film. They had allowed Huston to film on location in Mexico, which was a "radical move" for a studio at the time. They also knew that Huston was gaining a reputation as "one of the wild men of Hollywood." In any case, studio boss Jack Warner initially "detested it." But whatever doubts Warners had were soon removed, as the film achieved widespread public and critical acclaim. Hollywood writer James Agee called it "one of the most beautiful and visually alive movies I have ever seen." "Time" magazine described it as "one of the best things Hollywood has done since it learned to talk." Huston won Oscars for Best Director and Best Adapted Screenplay; his father won for Best Supporting Actor. It also won other awards in the U.S. and overseas. "Film Comment" magazine devoted four pages to the film in its May–June 1980 edition, with author Richard T. Jameson offering his impressions:
This film has impressed itself on the heart and mind and soul of anyone who has seen it, to the extent that filmmakers of great originality and distinctiveness like Robert Altman and Sam Peckinpah can be said to have remade it again and again ... without compromising its uniqueness.
Also in 1948 he directed his next film, "Key Largo", again with Humphrey Bogart starring. It was the story about a disillusioned returning veteran clashing with gangsters on a remote Florida key. It co-starred Lauren Bacall, Claire Trevor, and Edward G. Robinson. The film was an adaptation of the stage play by Maxwell Anderson, and the film itself seemed overly stage-bound for many viewers. However, the "outstanding performances" by all the actors saved the film, and Claire Trevor won an Oscar for best supporting actress. Huston was annoyed that the studio cut several scenes from the final release without his agreement. That, along with some earlier disputes, angered Huston enough that he left the studio when his contract expired.
"The Asphalt Jungle" (1950).
In 1950 he wrote and directed "The Asphalt Jungle", a film which broke new ground by depicting criminals as somewhat sympathetic characters, simply doing their professional work, "an occupation like any other", or what Huston calls "a left-handed form of human endeavor.":177 Huston achieved that effect by giving "deep attention" to the plot, involving a large jewelry theft, by examining the minute, step by step details and difficulties each of the characters had of carrying it out. In doing so, some critics felt that Huston had achieved an almost "documentary" style.
Film critic Andrew Sarris considered it to be "Huston's best film", and the film that made Marilyn Monroe a recognized actress. Sarris also notes the similar themes in many of Huston's films, as exemplified by this one: "His protagonists almost invariably fail at what they set out to do." This theme was also similar to the story in "Treasure of the Sierra Madre", where greed became the cause of the group's undoing.
It starred Sterling Hayden and Huston's personal friend, Sam Jaffe. It also became the first serious role for Marilyn Monroe, according to Huston: "it was, of course, where Marilyn Monroe got her start.":177 The film succeeded at the box office and Huston was again nominated for an Oscar for best screenplay and best director, along with winning the Screen Directors Guild Award. It would subsequently become a model for many similar movies by other filmmakers.
"The Red Badge of Courage" (1951).
After completing "The Asphalt Jungle", Huston's next film, "The Red Badge of Courage" (1951), was of a completely different subject: war and its effect on soldiers. While in the army during World War II, he became interested in Stephen Crane's classic American Civil War novel of the same title. For the starring role, Huston chose World War II hero Audie Murphy to play the young Union soldier who deserts his company out of fear, but later returns to fight alongside them. MGM, however, saw the message of the movie as too antiwar. Without Huston's input, they cut down the running time of the film from eighty-eight minutes to sixty-nine, added narration, and deleted what Huston felt was a crucial scene.
The movie did poorly at the box office. Huston suggests that it was possibly because it "brought war very close to home." Huston recalls that at the preview showing, before the film was halfway through, "damn near a third of the audience got up and walked out of the theater." Despite the "butchering" and weak public response, film historian Michael Barson describes the movie as "a minor masterpiece."
"The African Queen" (1951).
Before the "Asphalt Jungle" opened in theaters, Huston was already in Africa shooting "The African Queen" (1951), a story based on C. S. Forester's popular novel. It starred Humphrey Bogart and Katharine Hepburn in a combination of romance, comedy and adventure. Barson calls it "one of the most popular Hollywood movies of all time." The film's producer, Sam Spiegel, urged Huston to change the ending to allow the protagonists to survive, instead of dying. Huston agreed, and the ending was rewritten. It became Huston's most successful film financially, and "it remains one of his finest works." Huston was nominated for two Academy Awards—best director and best screenplay. Bogart, however, won an Oscar for best actor, his first time winning.
HUAC period.
In 1952 Huston moved to Ireland as a result of his "disgust" at the "witch-hunt" and the "moral rot" he felt was created by House Committee on Un-American Activities (HUAC), which had affected many of his friends in the movie industry. Huston had, with friends including director William Wyler and screenwriter Philip Dunne, established the "Committee for the First Amendment", as a response to the ongoing government investigations into communists within the film industry. The HUAC was calling numerous filmmakers, screenwriters, and actors to testify about any past affiliations.
"Moby Dick" (1956).
Huston took producing, writing, and directing credits for his next two films: "Moulin Rouge" (1953); and "Beat the Devil" (1953). "Moby Dick" (1956), however, was written by Ray Bradbury, although Huston had his name added to the screenplay credit after the completion of the project. Although Huston had personally hired Bradbury to adapt Herman Melville's novel into a screenplay, Bradbury and Huston did not get along during pre-production, and Bradbury later dramatized their relationship in the short story "Banshee"; Peter O'Toole would later play the role based on John Huston when "Banshee" was adapted into an episode of Ray Bradbury Theater.
Huston had been planning to film Herman Melville's "Moby Dick" for the previous ten years, and originally saw it as an excellent part for his father, Walter Huston. However, his father died in 1950, and he chose Gregory Peck to play the starring role of Captain Ahab. The movie was filmed over a three-year period on location in Ireland, where Huston was then living. The fishing village of New Bedford, Massachusetts was recreated along the waterfront; the sailing ship in the film was fully constructed to be seaworthy; and three 100-foot whales were built out of steel, wood, and plastic. However, the film failed at the box office, with some critics, like David Robinson, suggesting that the movie lacked the "mysticism of the book" and thereby "loses its significance."
"The Misfits" (1961).
Of his next five films, only "The Misfits" (1961), found critical approval. However, critics have noted the "retrospective atmosphere of doom" which now hangs over the film. Clark Gable, the star, died of a heart attack a few days after the filming was completed; Marilyn Monroe never did another film and died a year later; and costars Montgomery Clift and Thelma Ritter also died over the next few years. During the filming itself, Monroe was often on drugs of various kinds, which led to her arriving late on the set and often forgetting her lines. Monroe's problems also led to the breakup of her marriage to the film's scriptwriter, Arthur Miller, "virtually on set." Huston later commented about this period in her career: "Marilyn was on her way out. Not only of the picture, but of life."
"Freud: the Secret Passion" (1962).
He followed "The Misfits" with "", a film quite different from most of his others. Besides directing, he also narrates portions of the story. Film historian Stuart M. Kaminsky notes that Huston presents Sigmund Freud, played by Montgomery Clift, "as a kind of savior and messiah", with an "almost Biblical detachment." As the film begins, Huston describes Freud as a "kind of hero or God on a quest for mankind":
This is the story of Freud's descent into a region as black as hell, man's unconscious, and how he let in the light.
Huston explains how he became interested in psychotherapy, the subject of the film:
I first got into that through an experience in a hospital during the war, where I made a documentary about patients suffering from battle neuroses. I was in the army and made the picture "Let There Be Light". That experience started my interest in psychotherapy, and to this day Freud looms as the single huge figure in that field.
"The Night of the Iguana" (1964).
For his next film, Huston once again traveled down to Puerto Vallarta, Mexico. He adapted the stage play by Tennessee Williams. The film stars Richard Burton and Ava Gardner, and was nominated for several Academy Awards. Production attracted much attention, due to Burton bringing his wife Elizabeth Taylor to Puerto Vallarta, Mexico. Huston liked the town where filming took place so much that he bought a house there.
"The Bible: In the Beginning" (1966).
Producer Dino De Laurentis traveled to Ireland to ask Huston to direct "". Although De Laurentis had ambitions for a broader story, he realized that the subject could not be adequately covered and limited the story to the first half of the Book of Genesis. Huston enjoyed directing the film, as it gave him a chance to indulge his love of animals. Besides directing he also played the role of Noah and the voice of God. The film did poorly at the box office, however, and at a cost of 18 million dollars, it was the most expensive movie in his career. Huston likes describing details about the filming:
Every morning before beginning work, I visited the animals. One of the elephants, Candy, loved to be scratched on the belly behind her foreleg. I'd scratch her and she would lean farther and farther toward me until there was some danger of her toppling over on me. One time I started to walk away from her, and she reached out and took my wrist with her trunk and pulled me back to her side. It was a command: "Don't stop!" I used it in the picture. Noah scratches the elephant's belly and walks away, and the elephant pulls him back to her time after time.:317
"Fat City" (1972).
After several films that were not well received, Huston returned to critical acclaim with "Fat City". Based on Leonard Gardner's 1969 novel of the same name. About an aging, washed-up alcoholic boxer in Stockton, California trying to get his name back on the map, while having a new relationship with a world weary alcoholic, and an amateur boxer trying to find success in boxing. The film was nominated for several awards upon its release. It starred Stacy Keach, a young Jeff Bridges, and Susan Tyrrell, in which she was nominated for an Academy Award for Best Supporting Actress. Roger Ebert stated "Fat City" as one of Huston's best films, giving it four out of four stars, his highest rating.
"The Man Who Would Be King" (1975).
Perhaps Huston's most highly regarded film of the 1970s, "The Man Who Would Be King" was both a critical and commercial success. Huston had been planning to make this film since the 50's, originally with his friends Humphrey Bogart and Clark Gable. Eventually the lead roles went to Sean Connery and Michael Caine. The movie was filmed on location in North Africa. The film was praised for its use of old fashioned escapism and entertainment. Steven Spielberg has cited the film as one of his inspirations for his film "Raiders of the Lost Ark".
"Wise Blood" (1979).
After filming "The Man Who Would Be King", Huston took his longest break between directing films. He returned with "the offbeat and somewhat controversial film" based on the novel "Wise Blood". Here, Huston showed his skills as a storyteller and boldness when it came to difficult subjects such as religion.
"Under the Volcano" (1984).
Huston's last film set in Mexico stars Albert Finney as an alcoholic ambassador during the beginnings of World War II. The film gained a strong critical reception, most notably for Finney's portrayal of a desperate and depressed alcoholic. The film was also a success on the independent circuit.
"The Dead" (1987).
John Huston's final film is an adaptation of the classic short story by James Joyce. This may have been one of Huston's most personal films, due to his citizenship in Ireland and his passion for classic literature. Huston directed most of the film from a wheelchair, as he needed an oxygen tank to breathe during the last few months of his life. The film was nominated for two Academy Awards, and was praised by critics. Roger Ebert eventually placed it in his Great Movies list; a section of movies he claims to be some of the best ever made. Huston died nearly four months before the film's release date.
As an actor.
Toward the end of his career he also began to act in various films. In 1963, director Otto Preminger asked if he would portray a Boston prelate in "The Cardinal", and, writes author Philip Kemp, he "virtually stole the picture." He was nominated for an Academy Award for Best Supporting Actor for his role. He had a little participation (as did many others) in 1967's "Casino Royale" as actor and director. He acted in Roman Polanski's "Chinatown" (1974) as the film's central corrupt businessman, and as Teddy Roosevelt's secretary of state John Hay in "The Wind and the Lion". Huston enjoyed acting and denied that he took it all that seriously. "It's a cinch," he once said, "and they pay you damn near as much as you make directing."
Huston said he did not regard himself very highly as an actor, saying he was only proud of his performance in "Chinatown", although he had also greatly enjoyed acting in "Winter Kills". He also played the Lawgiver in "Battle for the Planet of the Apes".
Huston is also famous to a generation of fans of J. R. R. Tolkien's Middle-earth stories as the voice of the wizard Gandalf in the Rankin/Bass animated adaptations of "The Hobbit" (1977) and "The Return of the King" (1980).
Movie themes.
Huston's films were insightful about human nature and human predicaments. They also sometimes included scenes or brief dialogue passages that were remarkably prescient concerning environmental issues that came to public awareness in the future, in the period starting about 1970; examples include "The Misfits" and "The Night of the Iguana" (1964). Huston spent long evenings carousing in the Nevada casinos after filming, surrounded by reporters and beautiful women, gambling, drinking, and smoking cigars.
According to Kaminsky, Huston's stories were often about "failed quests" by a group of different people. The group would persist in the face of poor odds, doomed at the outset by the circumstances created by an impossible situation. However, some members of the doomed group usually survive, those who are "cool" and "intelligent", or someone who "will sacrifice everything for self-understanding and independence". Those types of characters are exemplified by Bogart in "The Maltese Falcon", and Montgomery Clift in "Freud."
Another type of quest often seen in Huston's films involve a pair of potential lovers trying to face a hostile world. Flint adds, however, that he "bucked Hollywood's penchant for happy endings", and many of his stories ended with "love unsatisfied".
Film historian James Goodwin adds that in virtually all of his films, there is some type of "heroic quest — even if it involves questionable motives or destructive alliances". In addition, the quest "is preferable to the spiritless, amoral routines of life". As a result, his best films, according to Flint, "have lean, fast-paced scripts and vibrant plots and characterizations, and many of them deal ironically with vanity, avarice and unfulfilled quests".
However, in the opinion of critics Tony Tracy and Roddy Flynn, "... what fundamentally fascinated Huston was not movies "per se" — that is, form — but the human condition ... and literature offered a road map for exploring that condition." In many of his films, therefore, he tried to express his interest by developing themes involving some of the "grand narratives" of the twentieth century, such as "faith, meaning, truth, freedom, psychology, colonialism, war and capitalism".:3
To Jameson, all of Huston's films are adaptations, and he believes that through his films there was a "cohesive world-view, not only thematically but also stylistically; there is the Huston look". The "Huston look" was also noted by screenwriter James Agee, who adds that this "look proceeds from Huston's sense of what is natural to the eye and his delicate, simple feeling for space relationships." In any case, notes Flint, Huston took "uncommon care to preserve the writer's styles and values ... and sought repeatedly to transpose the interior essence of literature to film with dramatic and visual tension", as he did in "Red Badge of Courage," "Moby Dick," and "Under the Volcano."
Religion is also a theme that runs through many of Huston's films. In "The Night of the Iguana," Kaminsky notes how Richard Burton, while preaching a sermon to his congregation, seems "lost, confused, his speech is gibberish", and leads his congregation to turn away from him. In other films, adds Kaminsky, religion is seen as "part of the fantasy world", that the actors must overcome to survive physically or emotionally. "These religious zealots counsel a move away from the pleasure of the world and human love, a world that Huston believes in," concludes Kaminsky. Such religious themes were also seen in "The Bible," and "Wise Blood," for example.
To Barson, however, Huston was among the "least consistent" filmmakers, although he concludes that he was one of the "most interesting directors of the past sixty years". Throughout his long career, many of his films did poorly and were criticized as a result. To a writer in 1972 he commented, "Criticism isn't a new experience for me. Pictures that are now thought of as, forgive the term, classics, weren't all that well thought of at the time they came out." After an interview a few years before he died, the reporter writes that "Huston said he missed the major studio era when people savored making movies, not just money."
According to Roger Ebert, on his review of "Fat City", "His fascination with underdogs and losers. The characters in Huston movies hardly ever set out to achieve what they're aiming for. Sam Spade, in "The Maltese Falcon", Huston's first film, ends up minus one partner and one woman he thought he could trust. Everyone is a loser in "The Treasure of the Sierra Madre", and the gold blows back into the dust and is lost in it. Ahab, in "Moby Dick". Marlon Brando's career Army officer in "Reflections in a Golden Eye", even Bogart and Hepburn in "The African Queen" – they all fall short of their plans. "The African Queen" does have a happy ending, but it feels tacked-on and ridiculous, and the Queen destroys itself in destroying the German steamer. So this "[Fat City]" is a theme we find in Huston's work, but rarely does he fit it to characters and a time and place so well as in "Fat City". Maybe that's because Huston knows the territory: he was a professional boxer himself for a while, and not a very good one."
Directing techniques.
George Stevens, Jr. notes that while many directors rely on post-production editing to shape their final work, Huston instead created his films while they were being shot: "I don't even know the editor of my films most of the time," Huston said. Actor Michael Caine also observed the same technique: "Most directors don't know what they want so they shoot everything they can think of — they use the camera like a machine gun. John uses it like a sniper."
Film writer Peter Flint also agrees and points out other benefits to that style: "He shot economically, eschewing the many protective shots favored by timid directors, and edited cerebrally so that financial backers would have trouble trying to cut scenes." Huston shot most of his films on location, working "intensely" six days a week, and "on Sundays, played equally intense poker with the cast and crew."
When asked how he envisions his films while directing and what his goals are, Huston replied:
To me the ideal film — which I've never succeeded in making — would be as though the reel were behind one's eyes and you were projecting it yourself, seeing what you wish to see. This has a great deal in common with thought processes ... That's why I think the camera is an eye as well as a mind. Everything we do with the camera has physiological and mental significance.
According to Kaminsky, much of Huston's vision probably came from his early experience as a painter on the streets of Paris. While there, he studied art and worked at it for a year and a half. Huston continued painting as a hobby for most of his life. Kaminsky also notes that most of Huston's films "reflected this prime interest in the image, the moving portrait and the use of color." Huston explored the use of "stylistic framing", especially well-planned close-ups, in much of his directing. In his first film, "The Maltese Falcon", for instance, Huston sketched out all of his scenes beforehand, "like canvases of paintings". His daughter, Anjelica Huston adds that even for his subsequent films, he sketched storyboards "constantly". She agrees that for her father, "it was a form of study, and my father was a painter, a very good one." She also notes that "there was an extremely developed sensory quality about my father, he didn't miss a trick.":20
Awards and honors.
Huston received 15 Oscar nominations in the course of his career, and is the oldest person ever to be nominated for the Best Director Oscar when, at 79 years old, he was nominated for "Prizzi's Honor" (1985). He won two Oscars, for directing and writing the screenplay for "The Treasure of the Sierra Madre". Huston also won a Golden Globe for that film and received multiple lifetime achievement awards (including one from American Film Institute in 1982).
He also has the unique distinction of directing both his father Walter and his daughter Anjelica in Oscar-winning performances (in "The Treasure of the Sierra Madre" and "Prizzi's Honor", respectively), making the Hustons the first family to have three generations of Academy Award winners.
In addition, he also directed 13 other actors in Oscar-nominated performances: Sydney Greenstreet, Claire Trevor, Sam Jaffe, Humphrey Bogart, Katharine Hepburn, José Ferrer, Colette Marchand, Deborah Kerr, Grayson Hall, Susan Tyrrell, Albert Finney, Jack Nicholson and William Hickey.
In 1965, Huston received the Laurel Award for Screenwriting Achievement from the Writers Guild of America.
In 1981 his film "Escape to Victory" was nominated for the Golden Prize at the 12th Moscow International Film Festival.
A statue of Huston, sitting in his director's chair, stands in Plaza John Huston in Puerto Vallarta, Mexico.
Personal life.
To producer George Stevens, Jr., Huston symbolized "intellect, charm and physical grace" within the film industry. He adds, "He was the most charismatic of the directors I knew, speaking with a soothing, melodic voice that was often mimicked, but was unique to him."
Huston loved the outdoors, especially sports such as hunting while living in Ireland. He claimed that he had no orthodox religion.:234 Among his life's adventures before becoming a Hollywood filmmaker, he had been an amateur boxer, reporter, short-story writer, portrait artist in Paris, a cavalry rider in Mexico, and a documentary filmmaker during World War II. Besides sports and adventure, he enjoyed hard liquor and relationships with women of all types — one of the reasons he was married five times. Stevens describes him as someone who "lived life to its fullest". Barson even suggests that Huston's "flamboyant life" as a rebel would possibly make for "an even more engaging tale than most of his movies".
His daughter, Anjelica Huston notes that he did not like Hollywood, and "especially despised Beverly Hills ... he thought it was just fake from the ground up. He didn't like any of that; he was not intrigued or attracted by it." She notes that in contrast, "he liked to be in the wild places; he liked animals as much as he liked people.":20
He was married five times:
Four of his marriages ended in divorce. His fourth wife, Enrica Soma, died in a car accident in 1969, while they were married. In addition to his children with Soma, he fathered a son, actor Danny Huston, with author Zoe Sallis.
Among his friends were Orson Welles and Ernest Hemingway. Humphrey Bogart was one of his best friends and Huston delivered the eulogy at his funeral.
Huston visited Ireland in 1951 and stayed at Luggala, County Wicklow, the home of Garech Browne, a member of the Guinness family. He visited Ireland several times afterwards and on one of these visits he purchased and restored a Georgian home, St Clerans, of Craughwell, County Galway. Between 1960 and 1971 he served as Master of Fox Hounds (MFH) of the County Galway Hunt – the famous "Galway Blazers" – whose kennels are at Craughwell. He renounced his U.S. citizenship and became an Irish citizen in 1964. His daughter Anjelica attended school in Ireland at Kylemore Abbey for a number of years. A film school is now dedicated to him on the NUIG campus.
Huston was an accomplished painter who wrote in his autobiography, "Nothing has played a more important role in my life". As a young man he studied at the Smith School of Art in Los Angeles but dropped out within a few months. He later studied at the Art Students League of New York. He painted throughout his life and had studios in each of his homes. He had owned a wide collection of art, including a notable collection of Pre-Columbian art.
A heavy smoker, he was diagnosed with emphysema in 1978. By the last year of his life he could not breathe for more than twenty minutes without needing oxygen. He died on August 28, 1987 in Middletown, Rhode Island from pneumonia as a complication of lung disease in his rented home. Huston is interred in the Hollywood Forever Cemetery in Hollywood near his mother.

</doc>
<doc id="44138" url="http://en.wikipedia.org/wiki?curid=44138" title="Cantata">
Cantata

A cantata (literally "sung", past participle of the Italian verb "cantare") is a vocal composition with an instrumental accompaniment, typically in several movements, often involving a choir.
The meaning of the term changed over time, from the simple single voice madrigal of the early 17th century, to the multi-voice "cantata da camera" and the "cantata da chiesa" of the later part of that century, from the more substantial dramatic forms of the 18th century to the usually sacred-texted 19th-century cantata, which was effectively a type of short oratorio. Cantatas for use in the liturgy of church services are called church cantata or sometimes sacred cantata, others sometimes secular cantata. Johann Sebastian Bach composed around 200 cantatas. Several cantatas were, and still are, written for special occasions, such as Christmas cantatas.
Historical context.
The term originated in the early 17th century simultaneously with opera and oratorio. Prior to that all "cultured" music was vocal. With the rise of instrumental music the term appeared, while the instrumental art became sufficiently developed to be embodied in sonatas. From the beginning of the 17th century until late in the 18th, the cantata for one or two solo voices with accompaniment of basso continuo (and perhaps a few solo instruments) was a principal form of Italian vocal chamber music.
A cantata consisted first of a declamatory narrative or scene in recitative, held together by a primitive aria repeated at intervals. Fine examples may be found in the church music of Giacomo Carissimi; and the English vocal solos of Henry Purcell (such as "Mad Tom" and "Mad Bess") show the utmost that can be made of this archaic form. With the rise of the da capo aria, the cantata became a group of two or three arias joined by recitative. George Frideric Handel's numerous Italian duets and trios are examples on a rather large scale. His Latin motet "Silete Venti", for soprano solo, shows the use of this form in church music.
Differences from other musical forms.
The Italian solo cantata tended, when on a large scale, to become indistinguishable from a scene in an opera, in the same way the church cantata, solo or choral, is indistinguishable from a small oratorio or portion of an oratorio. This is equally evident whether we examine the unparalleled church cantatas of Bach, of which nearly 200 are extant (see List of Bach cantatas), or the "Chandos Anthems" of Handel. In Johann Sebastian Bach's case many of the larger cantatas are actually called oratorios; and the "Christmas Oratorio" is a collection of six church cantatas actually intended for performance on six different days, though together forming as complete an artistic whole as any classical oratorio.
Baroque.
Cantatas were in great demand for the services of the Lutheran church. Sacred cantatas for the liturgy or other occasions were not only composed by Bach but also by Dieterich Buxtehude, Christoph Graupner, Gottfried Heinrich Stölzel and Georg Philipp Telemann, to name a few. Many secular cantatas were composed for events in the nobility. They were so similar in form to the sacred ones that many of them were parodied (in parts or completely) to sacred cantatas, for example in Bach's "Christmas Oratorio".
Classical and romantic period.
The term cantata came to be applied almost exclusively to choral works, as distinguished from solo vocal music. In early 19th-century cantatas the chorus is the vehicle for music more lyric and songlike than in oratorio, not excluding the possibility of a brilliant climax in a fugue as in Ludwig van Beethoven's "Glorreiche Augenblick", Carl Maria von Weber's "Jubel-Kantate", and Felix Mendelssohn's "Die erste Walpurgisnacht". Anton Bruckner composed several Name-day cantatas, a Festive Cantata and two secular cantatas (Germanenzug and Helgoland). Mendelssohn's Symphony Cantata, the "Lobgesang", is a hybrid work, partly in the oratorio style. It is preceded by three symphonic movements, a device avowedly suggested by Beethoven's ninth symphony; but the analogy is not accurate, as Beethoven's work is a symphony of which the fourth movement is a choral finale of essentially single design, whereas Mendelssohn's "Symphony Cantata" is a cantata with three symphonic preludes. Robert Schumann wrote the cantata "Paradise and the Peri". The full lyric possibilities of a string of choral songs were realized by Johannes Brahms in his "Rinaldo", that—like the "Walpurgisnacht"—was set to a text by Goethe. Other cantatas, Beethoven's "Meeresstille", works of Brahms and many notable small English choral works, such as cantatas of John Henry Maunder and John Stanley, find various ways to set poetry to choral music. The competition for the French Prix de Rome prescribed that each candidate submit a cantata. Hector Berlioz failed in three attempts before finally winning in 1830 with "Sardanapale". While almost all of the Prix de Rome cantatas have long since been forgotten (along with their composers, for the most part), Debussy's prize-winning "L'enfant prodigue" (1884, following his unsuccessful "Le gladiateur" of 1883) is still performed occasionally today. Late in the century, Gustav Mahler wrote his early "Das klagende Lied" on his own words, between 1878 and 1880, and Samuel Coleridge-Taylor created a successful trilogy of cantatas "The Song of Hiawatha" between 1898 and 1900.
Twentieth century and beyond.
Cantatas, both of the chamber variety and on a grand scale, were composed after 1900 as well. In the early part of the century, secular cantatas once again became prominent, while the 19th-century tradition of sacred cantatas also continued. Ralph Vaughan Williams composed both kinds: "festival" cantatas such as "Toward the Unknown Region" (1907), "Five Mystical Songs" (1911), and "Five Tudor Portraits" (1936), and sacred cantatas including "Sancta civitas" (1926), "Benedicite" (1930), "Dona nobis pacem" (1936), and "Hodie" (1954). Joseph Ryelandt also composed secular and sacred cantatas, such as "Le chant de la pauvreté" op. 92 in 1928 and "Veni creator" op. 123 in 1938. Béla Bartók composed the secular "Cantata Profana", subtitled "The Nine Splendid Stags" and based on a Romanian folk tale, in 1930. Although it began as a song cycle (as reflected also by its title), Arnold Schoenberg's "Gurre-Lieder" (1900–1903/1910–11) evolved into one of the century's largest secular cantatas. Paul Hindemith composed three works he designated as cantatas: "Die Serenaden", op. 35, for soprano, oboe, viola, and cello (1924), "Mahnung an die Jugend, sich der Musik zu befleissigen" (from the "Plöner Musiktage", 1932), and "Ite angeli veloces" for alto and tenor, mixed chorus, and orchestra, with audience participation (1953–55). Of Anton Webern's last three compositions, two are secular cantatas: Cantata No. 1, op. 29 (1938–39), and Cantata No. 2, op. 31 (1941–43), both setting texts by Hildegard Jone. Webern had begun sketching a Third Cantata by the time he was killed in 1945. Ernst Krenek also composed two examples: a "scenic cantata", "Die Zwingburg", op. 14 (1922), and a "Cantata for Wartime", op. 95, for women's voices and orchestra (1943). Sergei Prokofiev composed "Semero ikh" (1917–18; rev. 1933), and in 1939 premiered a cantata drawn from the film music for "Alexander Nevsky". Among the most famous of all cantatas is Carl Orff's "Carmina Burana", written 1935–36; the introductory and concluding movement, "O Fortuna", has been used in countless films, and has become some of the most recognizable music ever written.
Patriotic cantatas celebrating anniversaries of events in the Revolution or extolling state leaders were frequently commissioned in the Soviet Union between 1930 and the middle of the century, though these occasional works were seldom among their composers' best. Examples include Dmitri Shostakovich's "Poem of the Motherland", op. 47 (1947) and "The Sun Shines over Our Motherland", op. 90 (1952), and thee works by Prokofiev, "Zdravitsa!" [Hail to Stalin] (1939), along with two festival cantatas, the "Cantata for the Twentieth Anniversary of the October Revolution", op. 74, and "Flourish, Mighty Homeland", op. 114, for the thirtieth anniversary of the same event. Dmitry Kabalevsky also composed four such cantatas, "The Great Homeland", op. 35 (1941–42), "The Song of Morning, Spring and Peace", op. 57 (1957–58), "Leninists", op. 63 (1959), and "About Our Native Land", op. 82 (1965).
In 1940, the Brazilian composer Heitor Villa-Lobos created a secular cantata titled "Mandu çarará", based on an Indian legend collected by Barbosa Rodrigues. Igor Stravinsky composed a work titled simply "Cantata" in 1951–52, which used stanzas from the 15th-century "Lyke-wake Dirge" as a narrative frame for other anonymous English lyrics, and later designated "A Sermon, a Narrative and a Prayer" (1961) as "a cantata for alto and tenor soli, speaker, chorus, and orchestra". Luigi Nono wrote "Il canto sospeso" in 1955–56. Hans Werner Henze composed a "Cantata della fiaba estrema" and "Novae de infinito laudes" (both in 1963), as well as a number of other works that might be regarded as cantatas, such as "Kammermusik" (1958, rev. 1963), "Muzen Siziliens" (1966), and "El Cimarrón" (1969–70). "Momente" (1962–64/1969), one of the most important works of Karlheinz Stockhausen, is often described as a cantata. Benjamin Britten composed at least six works he designated as cantatas: "The Company of Heaven" (1937), "Rejoice in the Lamb", op. 30 (1943), "Saint Nicolas", op. 42 (1949), the "Cantata academica", op. 62 (1959), the "Cantata Misericordium", op. 69 (1963), and "Phaedra", op. 93 (1975). Alberto Ginastera also composed three works in this form: the "Cantata para América Mágica", op. 27 (1960), "Bomarzo", op. 32 (1964), and "Milena", op. 37 (1971), and Gottfried von Einem composed in 1973 "An die Nachgeborenen" based on diverse texts, the title taken from a poem of Bertolt Brecht. Mikis Theodorakis composed the cantatas "According to the Sadducees" and "Canto Olympico". Herbert Blendinger's "Media in vita" was premiered in 1980, his "Mich ruft zuweilen eine Stille" (Sometimes a silence calls me) in (1992), and "Allein den Betern kann es noch gelingen" (It can only be achieved by those who pray) in 1995. Iván Erőd wrote in 1988/89) "Vox Lucis" (Voice of the Light), op. 56. Ivan Moody wrote in 1995 "Revelation". Cantatas were also composed by Mark Alburger, Erik Bergman, Carlos Chávez, Osvald Chlubna, Peter Maxwell Davies, Norman Dello Joio, Lukas Foss, Roy Harris, Arthur Honegger, Alan Hovhaness, Dmitry Kabalevsky, Libby Larsen, Peter Mennin, Dimitri Nicolau, Krzysztof Penderecki, Daniel Pinkham, Earl Robinson, Ned Rorem, William Schuman ("A Free Song"), Roger Sessions, Siegfried Strohbach, Michael Tippett, and Kurt Weill.

</doc>
<doc id="44142" url="http://en.wikipedia.org/wiki?curid=44142" title="Metric system">
Metric system

The metric system is an internationally agreed decimal system of measurement that was originally based on the "mètre des Archives" and the "kilogramme des Archives" introduced by the French First Republic in 1799. Over the years, the definitions of the metre and the kilogram have been refined, and the metric system has been extended to incorporate many more units. Although a number of variants of the metric system emerged in the late nineteenth and early twentieth centuries, the term is now often used as a synonym for "SI" or the "International System of Units"—the official system of measurement in almost every country in the world.
The metric system has been officially sanctioned for use in the United States since 1866, but it remains the only industrialised country that has not adopted the metric system as its official system of measurement. Many sources also cite Liberia and Burma as the only other countries not to have done so. Although the United Kingdom uses the metric system for most official purposes, the use of the imperial system of measure, particularly among the public, is widespread and is permitted by the law.
Although the originators intended to devise a system that was equally accessible to all, it proved necessary to use prototype units in the custody of national or local authorities as standards. Control of the prototype units of measure was maintained by the French government until 1875, when it passed to an inter-governmental organisation—the General Conference on Weights and Measures (CGPM). It is now hoped that the last of these prototypes can be retired by 2014.
From its beginning, the main features of the metric system were the standard set of inter-related base units and a standard set of prefixes in powers of ten. These base units are used to derive larger and smaller units that could replace a huge number of other units of measure in existence. Although the system was first developed for commercial use, the development of coherent units of measure made it particularly suitable for science and engineering.
The uncoordinated use of the metric system by different scientific and engineering disciplines, particularly in the late 19th century, resulted in different choices of fundamental units, even though all were based on the same definitions of the metre and the kilogram. During the 20th century, efforts were made to rationalise these units, and in 1960 the CGPM published the International System of Units which, since then, has been the internationally recognised standard metric system.
Features.
Although the metric system has changed and developed since its inception, its basic concepts have hardly changed. Designed for transnational use, it consisted of a basic set of units of measurement, now known as base units. Derived units were built up from the base units using logical rather than empirical relationships while multiples and submultiples of both base and derived units were decimal-based and identified by a standard set of prefixes.
Universality.
At the outbreak of the French Revolution in 1789, most countries and even some cities had their own system of measurement. Although different countries might have used units of measure with the same name, such as the foot, or local language equivalents such as "pied", "fuß" and "voet", there was no consistency in the magnitude of those units, nor in the relationships with their multiples and sub-multiples, much like the modern-day differences between the US and the UK pints and gallons.
The metric system was designed to be universal—in the words of the French philosopher Marquis de Condorcet it was to be "for all people for all time".:1 It was designed for ordinary people, for engineers who worked in human-related measurements and for astronomers and physicists who worked with numbers both small and large, hence the huge range of prefixes that have now been defined in SI.
When the French Government first investigated the idea of overhauling their system of measurement, the concept of universality was put into practice when, in 1789, Maurice de Talleyrand, acting on Condorcet's advice, invited John Riggs Miller, a British Parliamentarian and Thomas Jefferson, the American Secretary of State to George Washington, to work with the French in producing an international standard by promoting legislation in their respective legislative bodies. However, these overtures failed and the custody of the metric system remained in the hands of the French government until 1875.:250–253
Unit names are ordinary nouns and although they use the character set and follow the grammatical rules of the language concerned for example "kilometre", ""kilómetro"", each unit has a symbol that is independent of language, for example "km" for "kilometre", "V" for "volts" etc.
Decimal multiples.
In the metric system, multiples and sub-multiples of units follow a decimal pattern, a concept identified as a possibility in 1586 by Simon Stevin, the Flemish mathematician who had introduced decimal fractions into Europe. This is done at the cost of losing the simplicity associated with many traditional systems of units where division by 3 or 4 does not result in awkward fractions; for example one third of a foot is four inches, a simplicity that in 1790 was debated, but rejected by the originators of the metric system. In 1854, in the introduction to the proceedings of the [British] Decimal Association, the mathematician Augustus de Morgan, summarised the advantages of a decimal based system were over a non-decimal system thus: "In the "simple" rules of arithmetic, we practice a pure decimal system, nowhere interrupted by the entrance of any other system: "from column to column we never carry anything but tens"".
A common set of decimal-based prefixes that have the effect multiplication or division by an integer power of ten can be applied to units which are too large or too small for practical use. The concept of using consistent classical (Latin or Greek) names for the prefixes was first proposed in a report by the [French Revolutionary] Commission on Weights and Measures in May 1793.:89–96 The prefix "kilo", for example, is used to multiply the unit by 1000, and the prefix "milli" is to indicate a one-thousandth part of the unit. Thus the "kilogram" and "kilometre" are a thousand grams and metres respectively, and a "milligram" and "millimetre" are one thousandth of a gram and metre respectively. These relations can be written symbolically as:
In the early days, multipliers that were positive powers of ten were given Greek-derived prefixes such as "kilo-" and "mega-", and those that were negative powers of ten were given Latin-derived prefixes such as "centi-" and "milli-". However, 1935 extensions to the prefix system did not follow this convention; the prefixes "nano-" and "micro-", for example used prefixes with Greek roots. During the 19th century the prefix "myria-", derived from the Greek word μύριοι ("mýrioi"), was used as a multiplier for (104).
When applying prefixes to derived units of area and volume that are expressed in terms of units of length squared or cubed, the square and cube operators are applied to the unit of length including the prefix, as illustrated here:
Prefixes are not usually used to indicate multiples of a second greater than 1; the non-SI units of minute, hour and day are used instead. On the other hand, prefixes are used for multiples of the non-SI unit of volume, the litre (l, L) such as millilitres (ml).
Realisability and replicable prototypes.
The base units used in the metric system must be realisable, ideally with reference to natural phenomena rather than unique artefacts. Each of the base units in SI is accompanied by a "mise en pratique" [practical realisation] published by the BIPM that describes in detail at least one way in which the base unit can be measured. Where possible, definitions of the base units were developed so that any laboratory equipped with proper instruments would be able to realise a standard without reliance on an artefact held by another country. In practice, such realisation is done under the auspices of a mutual acceptance arrangement (MAA).
Metre and kilogram.
In the original version of the metric system the base units could be derived from a specified length (the metre) and the weight [mass] of a specified volume (1⁄1000 of a cubic metre) of pure water. Initially the "de facto" French Government of the day, the "Assemblée nationale constituante", considered defining the metre as the length of a pendulum that has a period of one second at 45°N and an altitude equal to sea level. The altitude and latitude were specified to accommodate variations in gravity; the specified latitude was a compromise between the latitude of London (51° 30'N), Paris (48° 50'N) and the median parallel of the United States (38°N) to accommodate variations.:94 However the mathematician Borda persuaded the assembly that a survey having its ends at sea level and based on a meridian that spanned at least 10% of the earth's quadrant would be more appropriate for such a basis.:96
The available technology of the 1790s made it impracticable to use these definitions as the basis of the kilogram and the metre, so prototypes that represented these quantities insofar as was practicable were manufactured. On 22 June 1799 these prototypes were adopted as the definitive reference pieces, deposited in the "Archives nationales" and became known as the "mètre des Archives" and the "kilogramme des Archives". Copies were made and distributed around France.:266–269 These artefacts were replaced in 1889 by the new prototypes manufactured under international supervision. Insofar as was possible, the new prototypes were exact copies of the original prototypes, but used a later technology to ensure better stability. One of each of the kilogram and metre prototypes were chosen by lot to serve as the definitive international reference piece with the remainder being distributed to signatories of the Metre Convention.
In 1889 there was no generally accepted theory regarding the nature of light but by 1960 the wavelength of specific light spectra could give a more accurate and reproducible value than a prototype metre. In that year the prototype metre was replaced by a formal definition which defines the metre in terms of the wavelength of specified light spectra. By 1983 it was accepted that the speed of light in vacuum was constant and that this constant provided a more reproducible procedure for measuring length. Therefore the metre was redefined in terms of the speed of light. These definitions give a much better reproducibility and also allow anyone, anywhere with a suitably equipped laboratory, to make a standard metre.
Other base units.
None of the other base units rely on a prototype – all are based on phenomena that are directly observable and had been in use for many years before formally becoming part of the metric system.
The second first became a "de facto" base unit within the metric system when, in 1832, Carl Friedrich Gauss used it, the centimetre and the gram to express the units associated with values of absolute measurements of the Earth's magnetic field. The second, if based on the Earth's rotation, is not a constant as the Earth's rotation is slowing down—in 2008 the solar day was 0.002 s longer than in 1820. This had been known for many years; consequently in 1952 the International Astronomical Union (IAU) defined the second in terms of the Earth's rotation in the year 1900. Measurements of time were made using extrapolation from readings based on astronomy. With the launch of SI in 1960, the 11th CGPM adopted the IAU definition. In the years that followed, atomic clocks became significantly more reliable and precise and in 1968 the 13th CGPM redefined the second in terms of the frequency of a specific frequency from the emission spectrum of the caesium 133 atom, a component of atomic clocks. This provided the means to measure the time associated with astronomical phenomena rather than using astronomical phenomena as the basis from which time measurements were made.
The CGS absolute unit of electric current, the abampere, had been defined in terms of the force between two parallel current-carrying wires in 1881. In the 1940s, the International Electrotechnical Commission adopted an MKS variant of this definition for the ampere which was adopted in 1948 by the CGPM.
Temperature has always been based on observable phenomena—in 1744 the degree Centigrade was based on the freezing and boiling points of water. In 1948 the CGPM adopted the Centigrade scale, renamed it the "Celsius" temperature scale name and defined it in terms of the triple point of water.
When the mole and the candela were accepted by the CGPM in 1971 and 1975 respectively, both had been defined by third parties by reference to phenomena rather than artefacts.
Coherence.
Each variant of the metric system has a degree of coherence—the various derived units being directly related to the base units without the need of intermediate conversion factors. For example, in a coherent system the units of force, energy and power are chosen so that the equations
hold without the introduction of constant factors. Once a set of coherent units have been defined, other relationships in physics that use those units will automatically be true. Therefore Einstein's mass-energy equation, "E" = "mc"2, does not require extraneous constants when expressed in coherent units.
The CGS system had two units of energy, the erg that was related to mechanics and the calorie that was related to thermal energy so only one of them (the erg) could bear a coherent relationship to the base units. Coherence was a design aim of SI resulting in only one unit of energy being defined – the joule.
In SI, which is a coherent system, the unit of power is the "watt" which is defined as "one joule per second". In the US customary system of measurement, which is non-coherent, the unit of power is the "horsepower" which is defined as "550 foot-pounds per second" (the pound in this context being the pound-force). Similarly, neither the US gallon nor the imperial gallon is one cubic foot or one cubic yard— the US gallon is 231 cubic inches and the imperial gallon is 277.42 cubic inches.
The concept of coherence was only introduced into the metric system in the third quarter of the nineteenth century; in its original form the metric system was non-coherent—in particular the litre was 0.001 m3 and the are (from which the hectare derives) was 100 m2. A precursor to the concept of coherence was however present in that the units of mass and length were related to each other through the physical properties of water, the gram having been designed as being the mass of one cubic centimetre of water at its freezing point.
History.
In 1586 the Flemish mathematician Simon Stevin published a small pamphlet called "De Thiende" ("the tenth"). Decimal fractions had been employed for the extraction of square roots some five centuries before his time, but nobody used decimal numbers in daily life. Stevin declared that using decimals was so important that the universal introduction of decimal weights, measures and coinage was only a matter of time.
One of the earliest proposals for a decimal system in which length, area, volume and mass were linked to each other was made by John Wilkins, first secretary of the Royal Society of London in his 1668 essay "An Essay towards a Real Character and a Philosophical Language". His proposal used a pendulum that had a beat of one second as the basis of the unit of length. Two years later, in 1670, Gabriel Mouton, a French abbot and scientist, proposed a decimal system of length based on the circumference of the Earth. His suggestion was that a unit, the milliare, be defined as a minute of arc along a meridian. He then suggested a system of sub-units, dividing successively by factors of ten into the centuria, decuria, virga, virgula, decima, centesima, and millesima. His ideas attracted interest at the time, and were supported by both Jean Picard and Christiaan Huygens in 1673, and also studied at the Royal Society in London. In the same year, Gottfried Leibniz independently made proposals similar to those of Mouton.
In pre-revolutionary Europe, each state had its own system of units of measure. Some countries, such as Spain and Russia, saw the advantages of harmonising their units of measure with those of their trading partners. However, vested interests who profited from variations in units of measure opposed this. This was particularly prevalent in France where the huge inconsistency in the size of units of measure was one of the causes that, in 1789, led to the outbreak of the French Revolution.:2 During the early years of the revolution, savants including the Marquis de Condorcet, Pierre-Simon Laplace, Adrien-Marie Legendre, Antoine Lavoisier and Jean-Charles de Borda set up a Commission of Weights and Measures. The commission was of the opinion that the country should adopt a completely new system of measure based on the principles of logic and natural phenomena. Logic dictated that such a system should be based on the radix used for counting. Their report of March 1791 to the "Assemblée nationale constituante" considered but rejected the view of Lapace that a duodecimal system of counting should replace the existing decimal system; the view such a system was bound to fail prevailed. The commission's final recommendation was that the assembly should promote a decimal based system of measurement. The leaders of the assembly accepted the views of the commission.:99–100
Initially France attempted to work with other countries towards the adoption of a common set of units of measure.:99–100 Among the supporters of such an international system of units was Thomas Jefferson who, in 1790, presented a document "Plan for Establishing Uniformity in the Coinage, Weights, and Measures of the United States" to congress in which he advocated a decimal system that used traditional names for units (such as ten inches per foot). The report was considered but not adopted by Congress.:249–250
Original metric system.
The French law of 18 Germinal, Year III (7 April 1795) defined five units of measure:
This system continued the tradition of having separate base units for geometrically related dimensions, e.g., "mètre" for lengths, "are" (100 m2) for areas, "stère" (1 m3) for dry capacities, and "litre" (1 dm3) for liquid capacities. The "hectare", equal to a hundred "ares", the area of a square 100 metres on a side (about 2.47 acres), is still in use. The early metric system included only a few prefixes from "milli" (one thousandth) to "myria" (ten thousand).
Originally the "kilogramme", defined as being one "pinte" (later renamed the "litre") of water at the melting point of ice, was called the "grave"; the "gramme" being an alternative name for a thousandth of a "grave". However, the word "grave", being a synonym for the title "count", had aristocratic connotations and was renamed the "kilogramme". The name "mètre" was suggested by Auguste-Savinien Leblond in May 1790.:92
France officially adopted the metric system on 10 December 1799. Although it was decreed that its use was to be mandatory in Paris that year and across the provinces the following year, the decree was not universally observed across France.
International adoption.
Areas annexed by France during the Napoleonic era were the first to inherit the metric system. In 1812 Napoleon introduced a system known as "mesures usuelles" which used the names of pre-metric units of measure, but defined them in terms of metric units – for example, the "livre metrique" (metric pound) was 500 g and the "toise metrique" (metric fathom) was 2 metres. After the Congress of Vienna in 1815, France lost the territories that she had annexed; some, such as the Papal States reverted to their pre-revolutionary units of measure, others such as Baden adopted a modified version of the "mesures usuelles", but France kept her system of measurement intact.
In 1817 the Netherlands reintroduced the metric system, but used pre-revolutionary names—for example 1 centimetre became the "duim" (thumb), the "ons" (ounce) became 100 g and so on. Certain German states adopted similar systems and in 1852 the German Zollverein (customs union) adopted the zollpfund (customs pound) of 500 g for intrastate commerce. In 1872 the newly formed German Empire adopted the metric system as its official system of weights and measures and the newly formed Kingdom of Italy likewise, following the lead given by Piedmont, adopted the metric system in 1861.
The "Exposition Universelle (1867)" (Paris Exhibition) devoted a stand to the metric system and by 1875 two thirds of the European population and close on half the world's population had adopted the metric system. By 1872 the only principal European countries not to have adopted the metric system were Russia and the United Kingdom.
By 1920 countries comprising 22% of the world's population, mainly English-speaking, used the imperial system; 25% used mainly the metric system and the remaining 53% used neither.
In 1927 several million people in the United States sent over 100,000 petitions backed by the Metric Association and The General Federation of Women's Clubs urging Congress to adopt the metric system. The petition was opposed by the manufacturing industry, citing the cost of the conversion.
International standards.
In 1861 a committee of the British Association for Advancement of Science (BAAS) including William Thomson (later Lord Kelvin), James Clerk Maxwell and James Prescott Joule introduced the concept of a coherent system of units based on the metre, gram and second which, in 1873, was extended to include electrical units.
On 20 May 1875 an international treaty known as the "Convention du Mètre" (Metre Convention) was signed by 17 states. This treaty established the following organisations to conduct international activities relating to a uniform system for measurements:
In 1881 first International Electrical Congress adopted the BAAS recommendations on electrical units, followed by a series of congresses in which further units of measure were defined and the International Electrotechnical Commission (IEC) was set up with the specific task of overseeing electrical units of measure. This was followed by the International Congress of Radiology (ISR) who, at their inaugural meeting in 1926, initiated the definition of radiological-related units of measure.
In 1921 the Metre Convention was extended to cover all units of measure, not just length and mass and in 1933 the 8th CGPM resolved to work with other international bodies to agree standards for electrical units that could be related back to the international prototypes. Since 1954 the CIPM committee that oversees the definition of units of measurement, the Consultative Committee for Units, has representatives from many international organisations including the ISR, IEC and ISO under the chairmanship of the CIPM.
Variants.
A number of variants of the metric system evolved, all using the "Mètre des Archives" and "Kilogramme des Archives" (or their descendants) as their base units, but differing in the definitions of the various derived units.
Centimetre-gram-second systems.
The centimetre gram second system of units (CGS) was the first coherent metric system, having been developed in the 1860s and promoted by Maxwell and Thomson. In 1874, this system was formally promoted by the British Association for the Advancement of Science (BAAS). The system's characteristics are that density is expressed in g/cm3, force expressed in dynes and mechanical energy in ergs. Thermal energy was defined in calories, one calorie being the energy required to raise the temperature of one gram of water from 15.5 °C to 16.5 °C. The meeting also proposed two sets of units for electrical and magnetic properties – the electrostatic set of units and the electromagnetic set of units.
Metre-kilogram-second systems.
The CGS units of electricity were cumbersome to work with. This was remedied at the 1893 International Electrical Congress held in Chicago by defining the "international" ampere and ohm using definitions based on the metre, kilogram and second. In 1901, Giovanni Giorgi showed that by adding an electrical unit as a fourth base unit, the various anomalies in electromagnetic systems could be resolved. The metre-kilogram-second-coulomb (MKSC) and metre-kilogram-second-ampere (MKSA) systems are examples of such systems.
The International System of Units ("Système international d’unités" or SI) is the current international standard metric system and is also the system most widely used around the world. It is an extension of Giorgi's MKSA system—its base units are the metre, kilogram, second, ampere, kelvin, candela and mole.
Metre-tonne-second systems.
The metre-tonne-second system of units (MTS) was based on the metre, tonne and second – the unit of force was the sthène and the unit of pressure was the pièze. It was invented in France for industrial use and from 1933 to 1955 was used both in France and in the Soviet Union.
Gravitational systems.
Gravitational metric systems use the kilogram-force (kilopond) as a base unit of force, with mass measured in a unit known as the hyl, "Technische Mass Einheit" (TME), mug or metric slug. Although the CGPM passed a resolution in 1901 defining the standard value of acceleration due to gravity to be 980.665 cm/s2, gravitational units are not part of the International System of Units (SI).
International System of Units.
The 9th CGPM met in 1948, three years after the end of the Second World War and fifteen years after the 8th CGPM. In response to formal requests made by the International Union of Pure and Applied Physics and by the French Government to establish a practical system of units of measure, the CGPM requested the CIPM to prepare recommendations for such a system, suitable for adoption by all countries adhering to the Metre Convention. The recommendation also catalogued symbols for the most important MKS and CGS units of measure and for the first time the CGPM made recommendations concerning derived units. At the same time the CGPM formally adopted a recommendation for the writing and printing of unit symbols and of numbers.
The CIPM's draft proposal, which was an extensive revision and simplification of the metric unit definitions, symbols and terminology based on the MKS system of units, was put to the 10th CGPM in 1954. In accordance with Giorgi's proposals of 1901, the CIPM also recommended that the ampere be the base unit from which electromechanical units would be derived. The definitions for the ohm and volt that had previously been in use were discarded and these units became derived units based on the metre, ampere, second and kilogram. After negotiations with the International Commission on Illumination (CIE) and IUPAP, two further base units, the degree kelvin and the candela were also proposed as base units. The full system and name "Système International d'Unités" were adopted at the 11th CGPM in October 1960. During the years that followed the definitions of the base units and particularly the methods of applying these definitions have been refined.
The formal definition of International System of Units (SI) along with the associated resolutions passed by the CGPM and the CIPM are published by the BIPM in brochure form at regular intervals. The eighth edition of the brochure "Le Système International d'Unités—The International System of Units" was published in 2006 and is available on the internet.
In October 2011, at the 24th CGPM proposals were made to change the definitions of four of the base units. These changes should not affect the average person.
Relating SI to the real world.
Although SI, as published by the CGPM, should, in theory, meet all the requirements of commerce, science and technology, certain units of measure have acquired such a position within the world community that it is likely they will be used for many years to come. In order that such units are used consistently around the world, the CGPM catalogued such units in Tables 6 to 9 of the SI brochure. These categories are:
Usage around the world.
The usage of the metric system varies around the world. According to the US Central Intelligence Agency's "Factbook" (2007), the International System of Units has been adopted as the official system of weights and measures by all nations in the world except for Burma, Liberia and the United States, while the NIST has identified the United States as the only industrialised country where the metric system is not the predominant system of units. However, reports published since 2007 hold this is no longer true of Liberia or Burma. An Agence France-Presse report from 2010 stated that Sierra Leone had passed a law to replace the imperial system with the metric system thereby aligning its system of measurement with that used by its Mano River Union (MRU) neighbours Guinea and Liberia. Reports from Burma suggest that that country is also planning to adopt the metric system.
In the United States metric units, authorised by Congress in 1866, are widely used in science, military, and partially in industry, but customary units predominate in household use. At retail stores the litre is a commonly used unit for volume, especially on bottles of beverages, and milligrams are used to denominate the amounts of medications, rather than grains. On the other hand non-metric units are used in certain regulated environments such as nautical miles and knots in international aviation.
In the countries of the Commonwealth of Nations the metric system has replaced the imperial system by varying degrees: Australia, New Zealand and Commonwealth countries in Africa are almost totally metric, India is mostly metric, Canada is partly metric while in the United Kingdom the metric system, the use of which was first permitted for trade in 1864, is used in much government business, in most industries including building, health and engineering and for pricing by measure or weight in most trading situations, both wholesale and retail. However the imperial system is widely used by the British public, such as feet and inches as a measurement of height, and is legally mandated in various cases, such as road-sign distances "must" be in yards and miles. In 2007, the European Commission announced that it was to abandon the requirement for metric-only labelling on packaged goods in the UK, and to allow dual metric–imperial marking to continue indefinitely. 
Some other jurisdictions, such as Hong Kong, have laws mandating or permitting other systems of measurement in parallel with the metric system in some or all contexts.
Variations in spelling.
The SI symbols for the metric units are intended to be identical, regardless of the language used but unit names are ordinary nouns and use the character set and follow the grammatical rules of the language concerned. For example, the SI unit symbol for kilometre is "km" everywhere in the world, even though the local language word for the unit name may vary. Language variants for the kilometre unit name include: "chilometro" (Italian), "Kilometer" (German), "kilometer" (Dutch), "kilomètre" (French), "χιλιόμετρο" (Greek), "quilómetro/quilômetro" (Portuguese), "kilómetro" (Spanish) and "километр" (Russian).
Variations are also found with the spelling of unit names in countries using the same language, including differences in American English and British spelling. For example "meter" and "liter" are used in the United States whereas "metre" and "litre" are used in other English-speaking countries. In addition, the official US spelling for the rarely used SI prefix for ten is "deka". In American English the term "metric ton" is the normal usage whereas in other varieties of English "tonne" is common. "Gram" is also sometimes spelled "gramme" in English-speaking countries other than the United States, though this older usage is declining.
Conversion and calculation incidents.
The dual usage of or confusion between metric and non-metric units has resulted in a number of serious incidents. These include:
Conversion between SI and legacy units.
During its evolution, the metric system has adopted many units of measure. The introduction of SI rationalised both the way in which units of measure were defined and also the list of units in use. These are now catalogued in the official SI Brochure. The table below lists the units of measure in this catalogue and shows the conversion factors connecting them with the equivalent units that were in use on the eve of the adoption of SI.
The SI Brochure also catalogues certain non-SI units that are widely used with the SI in matters of everyday life or units that are exactly defined values in terms of SI units and are used in particular circumstances to satisfy the needs of commercial, legal, or specialised scientific interests. These units include:
Future developments.
After the metre was redefined in 1960, the kilogram was the only SI base unit that relied on a specific artefact. After the 1996–1998 recalibrations a clear divergence between the international and various national prototype kilograms was observed.
At the 23rd CGPM (2007), the CIPM was mandated to investigate the use of natural constants as the basis for all units of measure rather than the artefacts that were then in use. At a meeting of the CCU held in Reading, United Kingdom in September 2010, a resolution and draft changes to the SI brochure that were to be presented to the next meeting of the CIPM in October 2010 were agreed to in principle. The CCU proposed to
The CIPM meeting of October 2010 found that "the conditions set by the General Conference at its 23rd meeting have not yet been fully met. For this reason the CIPM does not propose a revision of the SI at the present time". The CIPM did however sponsor a resolution at the 24th CGPM in which the changes were agreed in principle and which were expected to be finalised at the 25th CGPM in 2014.

</doc>
<doc id="44145" url="http://en.wikipedia.org/wiki?curid=44145" title="Interquartile mean">
Interquartile mean

The interquartile mean (IQM) (or midmean) is a statistical measure of central tendency, much like the mean (in more popular terms called the average), the median, and the mode.
The IQM is a "truncated mean" and so is very similar to the scoring method used in sports that are evaluated by a panel of judges: "discard the lowest and the highest scores; calculate the mean value of the remaining scores".
Calculation.
In calculation of the IQM, only the data in the second and third quartiles is used (as in the interquartile range), and the lowest 25% and the highest 25% of the scores are discarded. These points are called the first and third quartiles, hence the name of the IQM. (Note that the "second" quartile is also called the median).
assuming the values have been ordered.
Examples.
Dataset divisible by four.
The method is best explained with an example. Consider the following dataset:
First sort the list from lowest-to-highest:
There are 12 observations (datapoints) in the dataset, thus we have 4 quartiles of 3 numbers. Discard the lowest and the highest 3 values:
We now have 6 of the 12 observations remaining; next, we calculate the arithmetic mean of these numbers:
For comparison, the arithmetic mean of the original dataset is
due to the strong influence of the outlier, 38.
Dataset not divisible by four.
The above example consisted of 12 observations in the dataset, which made the determination of the quartiles very easy. Of course, not all datasets have a number of observations that is divisible by 4. We can adjust the method of calculating the IQM to accommodate this. So ideally we want to have the IQM equal to the mean for symmetric distributions, e.g.:
has a mean value "x"mean = 3, and since it is a symmetric distribution, "x"IQM = 3 would be desired.
We can solve this by using a weighted average of the quartiles and the interquartile dataset:
Consider the following dataset of 9 observations:
There are 9/4 = 2.25 observations in each quartile, and 4.5 observations in the interquartile range. Truncate the fractional quartile size, and remove this number from the 1st and 4th quartiles (2.25 observations in each quartile, thus the lowest 2 and the highest 2 are removed).
Thus, there are 3 "full" observations in the interquartile range, and 2 fractional observations. Since we have a total of 4.5 observations in the interquartile range, the two fractional observations each count for 0.75 (and thus 3×1 + 2×0.75 = 4.5 observations).
The IQM is now calculated as follows:
In the above example, the mean has a value xmean = 9. The same as the IQM, as was expected. The method of calculating the IQM for any number of observations is analogous; the fractional contributions to the IQM can be either 0, 0.25, 0.50, or 0.75.
Comparison with mean and median.
The Interquartile Mean shares some properties from both the mean as well as the median:

</doc>
<doc id="44146" url="http://en.wikipedia.org/wiki?curid=44146" title="1100s BC (decade)">
1100s BC (decade)


</doc>
<doc id="44147" url="http://en.wikipedia.org/wiki?curid=44147" title="1110s BC">
1110s BC


</doc>
<doc id="44149" url="http://en.wikipedia.org/wiki?curid=44149" title="1560s BC">
1560s BC


</doc>
<doc id="44150" url="http://en.wikipedia.org/wiki?curid=44150" title="1710s BC">
1710s BC


</doc>
<doc id="44151" url="http://en.wikipedia.org/wiki?curid=44151" title="1550s BC">
1550s BC


</doc>
<doc id="44152" url="http://en.wikipedia.org/wiki?curid=44152" title="1720s BC">
1720s BC


</doc>
<doc id="44153" url="http://en.wikipedia.org/wiki?curid=44153" title="Kill Doctor Lucky">
Kill Doctor Lucky

Kill Doctor Lucky is a humorous board game designed by James Ernest and released in 1996 by Cheapass Games. In 1998, "Kill Doctor Lucky" won the Origins Award for "Best Abstract Board Game of 1997".
"Kill Doctor Lucky" is, in concept, a sort of inversion and perhaps a parody of "Cluedo" ("Clue" in North America). Both games are set in a sprawling mansion full of colorfully named rooms, feature a variety of dangerous weapons, and deal with the murder of the mansion's owner. "Cluedo" begins after the murder has been committed, and players compete to solve it; "Kill Doctor Lucky" ends with the murder, and players compete to commit it.
Gameplay.
The gameboard is a floor plan of Doctor Lucky's mansion, and it is accompanied by a deck of cards representing the objects and opportunities that can be found there. Players take turns moving through the rooms of the mansion and accumulating cards, while Doctor Lucky moves through the mansion following a predetermined path. A player may attempt to kill Doctor Lucky by playing a weapon card (such as a runcible spoon, a monkey hand, a letter opener, a trowel, a chainsaw or pinking shears) while the player's token is in the same room as Doctor Lucky and out of sight of all other players. Each weapon card has a certain point value, and certain weapons are worth more points when used in certain rooms (for example, the trowel is worth extra points when used in the wine cellar, an allusion to Poe's "The Cask of Amontillado").
At this point, the player making the murder attempt succeeds, and thereby wins the game, unless the opponents play Failure cards of combined value equal to the value of the weapon used. The situation is complicated by the requirement that players play Failure cards in clockwise order, with each player having only one opportunity to play cards. Since it is to any player's advantage to eliminate failure cards from his opponents' hands, a large part of the strategy of the game consists in bluffing: when one player attacks Doctor Lucky, it is in your interest to persuade your other opponents that you have no failure cards in your hand, to attempt to force them to save the game by spending the required cards.
When played, failure cards are set aside and not returned to the deck. Thus, as the game goes on, fewer and fewer failure cards are in play. This not only builds tension but also forces the game to end in a reasonable amount of time, because once all the failure cards are gone, the next murder attempt cannot fail.
The new Titanic Games version of "Kill Doctor Lucky" makes two changes to the original rules. First, a minor change was made to game play that now allows everyone to take at least one turn before the Doctor Lucky pawn determines turn order. In the original, it was possible for players to position themselves in such a way as to keep some players from ever getting a turn. This is no longer possible.
The second change was the addition of a new game piece called the "spite token" (a variant in the prior edition). Spite tokens are awarded when a murder attempt fails and adds a bonus point to all future murder attempts. A player also has the option to spend a spite token as a failure point to aid in thwarting an opponent's murder attempt. When spite tokens are spent in this manner they are given to the player they're spent against. This speeds the game up and adds a great deal of strategy to the late game when all of the failure cards have been removed from the deck.

</doc>
<doc id="44154" url="http://en.wikipedia.org/wiki?curid=44154" title="Catherine de' Medici">
Catherine de' Medici

Catherine de' Medici (Italian: "Caterina de' Medici" ]; French "Catherine de Médicis" ], 13 April 1519 – 5 January 1589), daughter of Lorenzo II de' Medici and of Madeleine de La Tour d'Auvergne, was an Italian noblewoman who was Queen of France from 1547 until 1559, as the wife of King Henry II. As the mother of three sons who became kings of France during her lifetime she had extensive, if at times varying, influence in the political life of France. For a time she ruled France as its regent.
In 1533, at the age of fourteen, Caterina married Henry, second son of King Francis I and Queen Claude of France. Under the gallicised version of her name, Catherine de Médicis, she was Queen consort of France as the wife of King Henry II of France from 1547 to 1559. Throughout his reign, Henry excluded Catherine from participating in state affairs and instead showered favours on his chief mistress, Diane de Poitiers, who wielded much influence over him. Henry's death thrust Catherine into the political arena as mother of the frail fifteen-year-old King Francis II. When he died in 1560, she became regent on behalf of her ten-year-old son King Charles IX and was granted sweeping powers. After Charles died in 1574, Catherine played a key role in the reign of her third son, Henry III. He dispensed with her advice only in the last months of her life.
Catherine's three sons reigned in an age of almost constant civil and religious war in France. The problems facing the monarchy were complex and daunting but Catherine was able to keep the monarchy and the state institutions functioning even at a minimum level
. At first, Catherine compromised and made concessions to the rebelling Protestants, or Huguenots, as they became known. She failed, however, to grasp the theological issues that drove their movement. Later, she resorted in frustration and anger to hard-line policies against them. In return, she came to be blamed for the excessive persecutions carried out under her sons' rule, in particular for the St. Bartholomew's Day massacre of 1572, in which thousands of Huguenots were killed in Paris and throughout France.
Some historians have excused Catherine from blame for the worst decisions of the crown, though evidence for her ruthlessness can be found in her letters. In practice, her authority was always limited by the effects of the civil wars. Her policies, therefore, may be seen as desperate measures to keep the Valois monarchy on the throne at all costs, and her patronage of the arts as an attempt to glorify a monarchy whose prestige was in steep decline. Without Catherine, it is unlikely that her sons would have remained in power. The years in which they reigned have been called "the age of Catherine de' Medici". According to one of her biographers Mark Strage, Catherine was the most powerful woman in sixteenth-century Europe.
Birth and upbringing.
Catherine was born in Florence, Republic of Florence, as Caterina Maria Romula di Lorenzo de' Medici.
The Medici family were at the time the "de facto" rulers of Florence: originally bankers, they came to great wealth and power by bankrolling the monarchies of Europe. Catherine's father, Lorenzo II de' Medici, was made Duke of Urbino by his uncle Pope Leo X, and the title reverted to Francesco Maria I della Rovere after Lorenzo's death. Thus, even though her father was a duke, Catherine was of relatively low birth. However her mother, Madeleine de la Tour d'Auvergne, the Countess of Boulogne, was from one of the most prominent and ancient French noble families; this prestigious maternal heritage was of benefit to her future marriage to a "fils de France".
According to a contemporary chronicler, when Catherine de' Medici was born, her parents, were "as pleased as if it had been a boy". Madeleine died on 28 April of puerperal fever or plague, and Lorenzo died on 4 May. The young couple were married the year before at Amboise as part of the alliance between King Francis I of France and Pope Leo against the Holy Roman Emperor Maximilian I. King Francis wanted Catherine to be raised at the French court, but Pope Leo had other plans for her. He intended to marry her to his brother's illegitimate son, Ippolito de' Medici, and set them up to rule Florence.
Catherine was first cared for by her paternal grandmother, Alfonsina Orsini (wife of Piero de' Medici). After Alfonsina's death in 1520, Catherine joined her cousins and was raised by her aunt, Clarice Strozzi. The death of Pope Leo in 1521 interrupted Medici power briefly, until Cardinal Giulio de' Medici was elected Pope Clement VII in 1523. Clement housed Catherine in the Palazzo Medici Riccardi in Florence, where she lived in state. The Florentine people called her "duchessina" ("the little duchess"), in deference to her unrecognised claim to the Duchy of Urbino.
In 1527, the Medici were overthrown in Florence by a faction opposed to the regime of Clement's representative, Cardinal Silvio Passerini, and Catherine was taken hostage and placed in a series of convents. The final one, the "Santissima Annuziata delle Murate" was her home for three years. Mark Strage described these years as "the happiest of her entire life". Clement had no choice but to crown Charles Holy Roman Emperor in return for his help in retaking the city. In October 1529, Charles's troops laid siege to Florence. As the siege dragged on, voices called for Catherine to be killed and exposed naked and chained to the city walls. Some even suggested that she be handed over to the troops to be used for their sexual gratification. The city finally surrendered on 12 August 1530. Clement summoned Catherine from her beloved convent to join him in Rome where he greeted her with open arms and tears in his eyes. Then he set about the business of finding her a husband.
Marriage.
On her visit to Rome, the Venetian envoy described Catherine as "small of stature, and thin, and without delicate features, but having the protruding eyes peculiar to the Medici family". Suitors, however, lined up for her hand, including James V of Scotland who sent the Duke of Albany to Clement to conclude a marriage in April and November 1530. When Francis I of France proposed his second son, Henry, Duke of Orléans, in early 1533, Clement jumped at the offer. Henry was a prize catch for Catherine, who despite her wealth was from commoner origins.
The wedding, a grand affair marked by extravagant display and gift-giving, took place in the Église Saint-Ferréol les Augustins in Marseille on 28 October 1533. Prince Henry danced and jousted for Catherine. The fourteen-year-old couple left their wedding ball at midnight to perform their nuptial duties. Henry arrived in the bedroom with King Francis, who is said to have stayed until the marriage was consummated. He noted that "each had shown valour in the joust". Clement visited the newlyweds in bed the next morning and added his blessings to the night's proceedings.
Catherine saw little of her husband in their first year of marriage, but the ladies of the court treated her well, impressed with her intelligence and keenness to please. The death of Pope Clement VII on 25 September 1534, however, undermined Catherine's standing in the French court. The next pope, Paul III, broke the alliance with France and refused to pay her huge dowry. King Francis lamented, "The girl has come to me stark naked."
Prince Henry showed no interest in Catherine as a wife; instead, he openly took mistresses. For the first ten years of the marriage, Catherine failed to produce any children. In 1537, on the other hand, Philippa Duci, one of Henry's mistresses, gave birth to a daughter, whom he publicly acknowledged. This proved that Henry was fertile and added to the pressure on Catherine to produce a child.
Dauphine.
In 1536, Henry's older brother, Francis, caught a chill after a game of tennis, contracted a fever, and died, leaving Henry the heir. As Dauphine, Catherine was now expected to provide a future heir to the throne. According to the court chronicler Brantôme, "many people advised the king and the Dauphin to repudiate her, since it was necessary to continue the line of France". Divorce was discussed. In desperation, Catherine tried every known trick for getting pregnant, such as placing cow dung and ground stags' antlers on her "source of life", and drinking mule's urine. On 19 January 1544, she at last gave birth to a son, named after King Francis.
After becoming pregnant once, Catherine had no trouble doing so again. She may have owed her change of luck to the physician Jean Fernel, who had noticed slight abnormalities in the couple's sexual organs and advised them how to solve the problem. Catherine quickly conceived again and on 2 April 1545 she bore a daughter, Elisabeth. She went on to bear Henry a further eight children, six of whom survived infancy, including the future Charles IX (born 27 June 1550); the future Henry III (born 19 September 1551); and Francis, Duke of Anjou (born 18 March 1555). The long-term future of the Valois dynasty, which had ruled France since the 14th century, seemed assured.
Catherine's new-found ability to bear children, however, failed to improve her marriage. In 1538, at the age of nineteen, Henry had taken as his mistress the thirty-eight-year-old Diane de Poitiers, whom he adored for the rest of his life. Even so, he respected Catherine's status as his consort. When King Francis I died in 1547 Catherine became queen consort of France. She was crowned in the basilica of Saint-Denis on 10 June 1549.
Queen of France.
Henry allowed Catherine almost no political influence as queen. Although she sometimes acted as regent during his absences from France, her powers were strictly nominal. Henry gave the Château of Chenonceau, which Catherine had wanted for herself, to Diane de Poitiers, who took her place at the centre of power, dispensing patronage and accepting favours.
The imperial ambassador reported that in the presence of guests, Henry would sit on Diane's lap and play the guitar, chat about politics, or fondle her breasts. Diane never regarded Catherine as a threat. She even encouraged the king to sleep with her and father more children. In 1556, Catherine nearly died giving birth to twin daughters. Surgeons saved her life by breaking the legs of one of the two babies, who died in her womb. The surviving daughter died seven weeks later. Catherine had no more children.
Henry's reign also saw the rise of the Guise brothers, Charles, who became a cardinal, and Henry's boyhood friend Francis, who became Duke of Guise. Their sister Mary of Guise had married James V of Scotland in 1538 and was the mother of Mary, Queen of Scots. At the age of five and a half, Mary was brought to the French court, where she was promised to the Dauphin, Francis. Catherine brought her up with her own children at the French court, while Mary of Guise governed Scotland as her daughter's regent.
On 3–4 April 1559, Henry signed the Peace of Cateau-Cambrésis with the Holy Roman Empire and England, ending a long period of Italian wars. The treaty was sealed by the betrothal of Catherine's thirteen-year-old daughter Elisabeth to Philip II of Spain. Their proxy wedding in Paris on 22 June 1559 was celebrated with festivities, balls, masques, and five days of jousting.
King Henry took part in the jousting, sporting Diane's black-and-white colours. He defeated the dukes of Guise and Nemours, but the young Gabriel, comte de Montgomery, knocked him half out of the saddle. Henry insisted on riding against Montgomery again, and this time, Montgomery's lance shattered into the king's face. Henry reeled out of the clash, his face pouring blood, with splinters "of a good bigness" sticking out of his eye and head. Catherine, Diane, and Prince Francis all fainted. Henry was carried to the Château de Tournelles, where five splinters of wood were extracted from his head, one of which had pierced his eye and brain. Catherine stayed by his bedside, but Diane kept away, "for fear", in the words of a chronicler, "of being expelled by the Queen". For the next ten days, Henry's state fluctuated. At times he even felt well enough to dictate letters and listen to music. Slowly, however, he lost his sight, speech, and reason, and on 10 July 1559 he died. From that day, Catherine took a broken lance as her emblem, inscribed with the words "lacrymae hinc, hinc dolor" ("from this come my tears and my pain"), and wore black mourning in memory of Henry.
Queen mother.
Reign of Francis II.
Francis II became king at the age of fifteen. In what has been called a "coup d'état", the Cardinal of Lorraine and the Duke of Guise—whose niece, Mary, Queen of Scots, had married Francis the year before—seized power the day after Henry II's death and quickly moved themselves into the Louvre Palace with the young couple. The English ambassador reported a few days later that "the house of Guise ruleth and doth all about the French king". For the moment, Catherine worked with the Guises out of necessity. She was not strictly entitled to a role in Francis's government, because he was deemed old enough to rule for himself. Nevertheless, all his official acts began with the words: "This being the good pleasure of the Queen, my lady-mother, and I also approving of every opinion that she holdeth, am content and command that ..." Catherine did not hesitate to exploit her new authority. One of her first acts was to force Diane de Poitiers to hand over the crown jewels and return the Château de Chenonceau to the crown. She later did her best to efface or outdo Diane's building work there.
The Guise brothers set about persecuting the Protestants with zeal. Catherine adopted a moderate stance and spoke up against the Guise persecutions, though she had no particular sympathy for the Huguenots, whose beliefs she never shared. The Protestants looked for leadership first to Antoine de Bourbon, King of Navarre, the First Prince of the Blood, and then, with more success, to his brother, Louis de Bourbon, Prince of Condé, who backed a plot to overthrow the Guises by force. When the Guises heard of the plot, they moved the court to the fortified Château of Amboise. The Duke of Guise launched an attack into the woods around the château. His troops surprised the rebels and killed many of them on the spot, including the commander, La Renaudie. Others they drowned in the river or strung up around the battlements while Catherine and the court watched.
In June 1560, Michel de l'Hôpital was appointed Chancellor of France. He sought the support of France's constitutional bodies and worked closely with Catherine to defend the law in the face of the growing anarchy. Neither saw the need to punish Protestants who worshipped in private and did not take up arms. On 20 August 1560, Catherine and the chancellor advocated this policy to an assembly of notables at Fontainebleau. Historians regard the occasion as an early example of Catherine's statesmanship. Meanwhile, Condé raised an army and in autumn 1560 began attacking towns in the south. Catherine ordered him to court and had him imprisoned as soon as he arrived. He was tried in November, found guilty of offences against the crown, and sentenced to death. His life was saved by the illness and death of the king, as a result of an infection or an abscess in his ear.
When Catherine had realized Francis was going to die, she made a pact with Antoine de Bourbon by which he would renounce his right to the regency of the future king, Charles IX, in return for the release of his brother Condé. As a result, when Francis died on 5 December 1560, the Privy Council appointed Catherine as governor of France ("gouvernante de France"), with sweeping powers. She wrote to her daughter Elisabeth: "My principal aim is to have the honour of God before my eyes in all things and to preserve my authority, not for myself, but for the conservation of this kingdom and for the good of all your brothers".
Reign of Charles IX.
At first Catherine kept the nine-year-old king, who cried at his coronation, close to her, and slept in his chamber. She presided over his council, decided policy, and controlled state business and patronage. However, she was never in a position to control the country as a whole, which was on the brink of civil war. In many parts of France the rule of nobles held sway rather than that of the crown. The challenges Catherine faced were complex and in some ways difficult for her to comprehend as a foreigner.
She summoned church leaders from both sides to attempt to solve their doctrinal differences. Despite her optimism, the resulting Colloquy of Poissy ended in failure on 13 October 1561, dissolving itself without her permission. Catherine failed because she saw the religious divide only in political terms. In the words of historian R. J. Knecht, "she underestimated the strength of religious conviction, imagining that all would be well if only she could get the party leaders to agree". In January 1562, Catherine issued the tolerant Edict of Saint-Germain in a further attempt to build bridges with the Protestants. On 1 March 1562, however, in an incident known as the Massacre of Vassy, the Duke of Guise and his men attacked worshipping Huguenots in a barn at Vassy (Wassy), killing 74 and wounding 104. Guise, who called the massacre "a regrettable accident", was cheered as a hero in the streets of Paris while the Huguenots called for revenge. The massacre lit the fuse that sparked the French Wars of Religion. For the next thirty years, France found itself in a state of either civil war or armed truce.
Within a month Louis de Bourbon, Prince of Condé, and Admiral Gaspard de Coligny had raised an army of 1,800. They formed an alliance with England and seized town after town in France. Catherine met Coligny, but he refused to back down. She therefore told him: "Since you rely on your forces, we will show you ours". The royal army struck back quickly and laid siege to Huguenot-held Rouen. Catherine visited the deathbed of Antoine de Bourbon, King of Navarre, after he was fatally wounded by an arquebus shot. Catherine insisted on visiting the field herself and when warned of the dangers laughed, "My courage is as great as yours". The Catholics took Rouen, but their triumph was short lived. On 18 February 1563, a spy called Poltrot de Méré fired an arquebus into the back of the Duke of Guise, at the siege of Orléans. The murder triggered an aristocratic blood feud that complicated the French civil wars for years to come. Catherine, however, was delighted with the death of her ally. "If Monsieur de Guise had perished sooner", she told the Venetian ambassador, "peace would have been achieved more quickly". On 19 March 1563, the Edict of Amboise, also known as the Edict of Pacification, ended the war. Catherine now rallied both Huguenot and Catholic forces to retake Le Havre from the English.
Huguenots.
On 17 August 1563, Charles IX was declared of age at the Parlement of Rouen, but he was never able to rule on his own and showed little interest in government. Catherine decided to launch a drive to enforce the Edict of Amboise and revive loyalty to the crown. To this end, she set out with Charles and the court on a progress around France that lasted from January 1564 until May 1565. Catherine held talks with the Protestant Queen Jeanne III of Navarre at Mâcon and Nérac. She also met her daughter Elisabeth at Bayonne near the Spanish border, amidst lavish court festivities. Philip II excused himself from the occasion. He sent the Duke of Alba to tell Catherine to scrap the Edict of Amboise and to find punitive solutions to the problem of heresy.
In 1566, through the ambassador to the Ottoman Empire, Guillaume de Grandchamp de Grantrie, and because of a long-standing Franco-Ottoman alliance, Charles IX of France and Catherine de Medicis proposed to the Ottoman Court a plan to resettle French Huguenots and French and German Lutherans in Ottoman-controlled Moldavia, in order to create a military colony and a buffer against the Habsburg. This plan also had the added advantage of removing the Huguenots from France, but it failed to interest the Ottomans.
On 27 September 1567, in a swoop known as the Surprise of Meaux, Huguenot forces attempted to ambush the king, triggering renewed civil war. Taken unawares, the court fled to Paris in disarray. The war was ended by the Peace of Longjumeau of 22–23 March 1568, but civil unrest and bloodshed continued. The Surprise of Meaux marked a turning point in Catherine's policy towards the Huguenots. From that moment, she abandoned compromise for a policy of repression. She told the Venetian ambassador in June 1568 that all one could expect from Huguenots was deceit, and she praised the Duke of Alba's reign of terror in the Netherlands, where Calvinists and rebels were put to death in the thousands.
The Huguenots retreated to the fortified stronghold of La Rochelle on the west coast, where Jeanne d'Albret and her fifteen-year-old son, Henry of Bourbon, joined them. "We have come to the determination to die, all of us", Jeanne wrote to Catherine, "rather than abandon our God, and our religion". Catherine called Jeanne, whose decision to rebel posed a dynastic threat to the Valois, "the most shameless woman in the world". Nevertheless, the Peace of Saint-Germain-en-Laye, signed on 8 August 1570 because the royal army ran out of cash, conceded wider toleration to the Huguenots than ever before.
Catherine looked to further Valois interests by grand dynastic marriages. In 1570, Charles IX married Elisabeth of Austria, daughter of Maximilian II, Holy Roman Emperor. Catherine was also eager for a match between one of her two youngest sons and Elizabeth I of England. After Catherine's daughter Elisabeth died in childbirth in 1568, she had touted her youngest daughter Margaret as a bride for Philip II of Spain. Now she sought a marriage between Margaret and Henry III of Navarre, with the aim of uniting Valois and Bourbon interests. Margaret, however, was secretly involved with Henry of Guise, the son of the late Duke of Guise. When Catherine found this out, she had her daughter brought from her bed. Catherine and the king then beat her, ripping her nightclothes and pulling out handfuls of her hair.
Catherine pressed Jeanne d'Albret to attend court. Writing that she wanted to see Jeanne's children, she promised not to harm them. Jeanne replied: "Pardon me if, reading that, I want to laugh, because you want to relieve me of a fear that I've never had. I've never thought that, as they say, you eat little children". When Jeanne did come to court, Catherine pressured her hard, playing on Jeanne's hopes for her beloved son. Jeanne finally agreed to the marriage between her son and Margaret, so long as Henry could remain a Huguenot. When Jeanne arrived in Paris to buy clothes for the wedding, she was taken ill and died, aged forty-four. Huguenot writers later accused Catherine of murdering her with poisoned gloves. The wedding took place on 18 August 1572 at Notre-Dame, Paris.
St. Bartholomew's Day massacre.
Three days later, Admiral Coligny was walking back to his rooms from the Louvre when a shot rang out from a house and wounded him in the hand and arm. A smoking arquebus was discovered in a window, but the culprit had made his escape from the rear of the building on a waiting horse. Coligny was carried to his lodgings at the Hôtel de Béthisy, where the surgeon Ambroise Paré removed a bullet from his elbow and amputated a damaged finger with a pair of scissors. Catherine, who was said to have received the news without emotion, made a tearful visit to Coligny and promised to punish his attacker. Many historians have blamed Catherine for the attack on Coligny. Others point to the Guise family or a Spanish-papal plot to end Coligny's influence on the king. Whatever the truth, the bloodbath that followed was soon beyond the control of Catherine or any other leader.
The St. Bartholomew's Day massacre, which began two days later, has stained Catherine's reputation ever since. There is no reason to believe she was not party to the decision when on 23 August Charles IX ordered, "Then kill them all! Kill them all!" The thinking was clear. Catherine and her advisers expected a Huguenot uprising to avenge the attack on Coligny. They chose therefore to strike first and wipe out the Huguenot leaders while they were still in Paris after the wedding.
The slaughter in Paris lasted for almost a week. It spread to many parts of France, where it persisted into the autumn. In the words of historian Jules Michelet, "St Bartholomew was not a day, but a season". On 29 September, when Navarre knelt before the altar as a Roman Catholic, having converted to avoid being killed, Catherine turned to the ambassadors and laughed. From this time dates the legend of the wicked Italian queen. Huguenot writers branded Catherine a scheming Italian, who had acted on Machiavelli's principles to kill all enemies in one blow.
Reign of Henry III.
Two years later, Catherine faced a new crisis with the death of Charles IX at the age of twenty-three. His dying words were "oh, my mother ...". The day before he died, he named Catherine regent, since his brother and heir, Henry the Duke of Anjou, was in the Polish-Lithuanian Commonwealth, where he had been elected king the year before. However, three months after his coronation at Wawel Cathedral, Henry abandoned that throne and returned to France in order to become king of France. Catherine wrote to Henry of Charles IX's death: "I am grief-stricken to have witnessed such a scene and the love which he showed me at the end ... My only consolation is to see you here soon, as your kingdom requires, and in good health, for if I were to lose you, I would have myself buried alive with you."
Henry was Catherine's favourite son. Unlike his brothers, he came to the throne as a grown man. He was also healthier, though he suffered from weak lungs and constant fatigue. His interest in the tasks of government, however, proved fitful. He depended on Catherine and her team of secretaries until the last few weeks of her life. He often hid from state affairs, immersing himself in acts of piety, such as pilgrimages and flagellation. He was, however, also famous for his circle of favorites, called Les Mignons (from mignon, French for "the darlings" or "the dainty ones"). It was a term used by polemicists in the toxic atmosphere of the French Wars of Religion and taken up by the people of Paris, to designate the favourites of Henry III of France, from his return from Poland to reign in France in 1574, to his assassination in 1589, a disastrous end to which the perception of effeminate weakness contributed.[1] The mignons were frivolous and fashionable young men, to whom public malignity attributed heterodox sexuality, rumors that some historians have found to be a factor in the disintegration of the late Valois monarchy.
According to the contemporary chronicler Pierre de l'Estoile,[2] they made themselves "exceedingly odious, as much by their foolish and haughty demeanour, as by their effeminate and immodest dress, but above all by the immense gifts the king made to them." The Joyeuse wedding in 1581 occasioned one of the most extravagant display of the reign.
Henry married Louise de Lorraine-Vaudémont in February 1575, two days after his coronation. His choice thwarted Catherine's plans for a political marriage to a foreign princess. Rumours of Henry's inability to produce children were by that time in wide circulation. The papal nuncio Salviati observed, "it is only with difficulty that we can imagine there will be offspring ... physicians and those who know him well say that he has an extremely weak constitution and will not live long." As time passed and the likelihood of children from the marriage receded, Catherine's youngest son, Francis, Duke of Alençon, known as "Monsieur", played upon his role as heir to the throne, repeatedly exploiting the anarchy of the civil wars, which were by now as much about noble power struggles as religion. Catherine did all in her power to bring Francis back into the fold. On one occasion, in March 1578, she lectured him for six hours about his dangerously subversive behaviour.
In 1576, in a move that endangered Henry's throne, Francis allied with the Protestant princes against the crown. On 6 May 1576, Catherine gave in to almost all Huguenot demands in the Edict of Beaulieu. The treaty became known as the "Peace of Monsieur" because it was thought that Francis had forced it on the crown. Francis died of consumption in June 1584, after a disastrous intervention in the Low Countries during which his army had been massacred. Catherine wrote, the next day: "I am so wretched to live long enough to see so many people die before me, although I realize that God's will must be obeyed, that He owns everything, and that he lends us only for as long as He likes the children whom He gives us." The death of her youngest son was a calamity for Catherine's dynastic dreams. Under Salic law, by which only males could ascend the throne, the Huguenot Henry of Navarre now became heir presumptive to the French crown.
Catherine had at least taken the precaution of marrying Margaret, her youngest daughter, to Navarre. Margaret, however, became almost as much of a thorn in Catherine's side as Francis, and in 1582, she returned to the French court without her husband. Catherine was heard yelling at her for taking lovers. Catherine sent Pomponne de Bellièvre to Navarre to arrange Margaret's return. In 1585, Margaret fled Navarre again. She retreated to her property at Agen and begged her mother for money. Catherine sent her only enough "to put food on her table". Moving on to the fortress of Carlat, Margaret took a lover called d'Aubiac. Catherine asked Henry to act before Margaret brought shame on them again. In October 1586, therefore, he had Margaret locked up in the Château d'Usson. D'Aubiac was executed, though not, despite Catherine's wish, in front of Margaret. Catherine cut Margaret out of her will and never saw her again.
Catherine was unable to control Henry in the way she had Francis and Charles. Her role in his government became that of chief executive and roving diplomat. She travelled widely across the kingdom, enforcing his authority and trying to head off war. In 1578, she took on the task of pacifying the south. At the age of fifty-nine, she embarked on an eighteen-month journey around the south of France to meet Huguenot leaders face to face. Her efforts won Catherine new respect from the French people. On her return to Paris in 1579, she was greeted outside the city by the Parlement and crowds. The Venetian ambassador, Gerolamo Lipomanno, wrote: "She is an indefatigable princess, born to tame and govern a people as unruly as the French: they now recognize her merits, her concern for unity and are sorry not to have appreciated her sooner." She was under no illusions, however. On 25 November 1579, she wrote to the king, "You are on the eve of a general revolt. Anyone who tells you differently is a liar."
Catholic League.
Many leading Roman Catholics were appalled by Catherine's attempts to appease the Huguenots. After the Edict of Beaulieu, they had started forming local leagues to protect their religion. The death of the heir to the throne in 1584 prompted the Duke of Guise to assume the leadership of the Catholic League. He planned to block Henry of Navarre's succession and place Henry's Catholic uncle Cardinal Charles de Bourbon on the throne instead. In this cause, he recruited the great Catholic princes, nobles and prelates, signed the treaty of Joinville with Spain, and prepared to make war on the "heretics". By 1585, Henry III had no choice but to go to war against the League. As Catherine put it, "peace is carried on a stick" ("bâton porte paix"). "Take care", she wrote to the king, "especially about your person. There is so much treachery about that I die of fear."
Henry was unable to fight the Catholics and the Protestants at once, both of whom had stronger armies than his own. In the Treaty of Nemours, signed on 7 July 1585, he was forced to give in to all the League's demands, even that he pay its troops. He went into hiding to fast and pray, surrounded by a bodyguard known as "the Forty-five", and left Catherine to sort out the mess. The monarchy had lost control of the country, and was in no position to assist England in the face of the coming Spanish attack. The Spanish ambassador told Philip II that the abscess was about to burst.
By 1587, the Catholic backlash against the Protestants had become a campaign across Europe. Elizabeth I of England's execution of Mary, Queen of Scots, on 8 February 1587 outraged the Catholic world. Philip II of Spain prepared for an invasion of England. The League took control of much of northern France to secure French ports for his armada.
Last months and death.
Henry hired Swiss troops to help him defend himself in Paris. The Parisians, however, claimed the right to defend the city themselves. On 12 May 1588, they set up barricades in the streets and refused to take orders from anyone except the Duke of Guise. When Catherine tried to go to mass, she found her way barred, though she was allowed through the barricades. The chronicler L'Estoile reported that she cried all through her lunch that day. She wrote to Bellièvre, "Never have I seen myself in such trouble or with so little light by which to escape." As usual, Catherine advised the king, who had fled the city in the nick of time, to compromise and live to fight another day. On 15 June 1588, Henry duly signed the Act of Union, which gave in to all the League's latest demands.
On 8 September 1588 at Blois, where the court had assembled for a meeting of the Estates, Henry dismissed all his ministers without warning. Catherine, in bed with a lung infection, had been kept in the dark. The king's actions effectively ended her days of power.
At the meeting of the Estates, Henry thanked Catherine for all she had done. He called her not only the mother of the king but the mother of the state. Henry did not tell Catherine of his plan for a solution to his problems. On 23 December 1588, he asked the Duke of Guise to call on him at the Château of Blois. As Guise entered the king's chamber, the Forty-five plunged their blades into his body, and he died at the foot of the king's bed. At the same moment, eight members of the Guise family were rounded up, including the Duke of Guise's brother, Louis II, Cardinal of Guise, whom Henry's men hacked to death the next day in the palace dungeons. Immediately after the murder of Guise, Henry entered Catherine's bedroom on the floor below and announced, "Please forgive me. Monsieur de Guise is dead. He will not be spoken of again. I have had him killed. I have done to him what he was going to do to me." Catherine's immediate reaction is not known; but on Christmas Day, she told a friar, "Oh, wretched man! What has he done? ... Pray for him ... I see him rushing towards his ruin." She visited her old friend Cardinal de Bourbon on 1 January 1589 to tell him she was sure he would soon be freed. He shouted at her, "Your words, Madam, have led us all to this butchery." She left in tears.
On 5 January 1589, Catherine died at the age of sixty-nine, probably from pleurisy. L'Estoile wrote: "those close to her believed that her life had been shortened by displeasure over her son's deed." He added that she had no sooner died than she was treated with as much consideration as a dead goat. Because Paris was held by enemies of the crown, Catherine had to be buried at Blois. Diane, daughter of Henry II and Philippa Duci, later had her body moved to Saint-Denis basilica. In 1793, a revolutionary mob tossed her bones into a mass grave with those of the other kings and queens. Eight months after Catherine's burial, Jacques Clément stabbed Henry III to death. At the time, Henry was besieging Paris with the King of Navarre, who would succeed him as Henry IV of France. Henry III's assassination ended nearly three centuries of Valois rule and brought the Bourbon dynasty into power.
Henry IV was later reported to have said of Catherine:
I ask you, what could a woman do, left by the death of her husband with five little children on her arms, and two families of France who were thinking of grasping the crown—our own [the Bourbons] and the Guises? Was she not compelled to play strange parts to deceive first one and then the other, in order to guard, as she did, her sons, who successively reigned through the wise conduct of that shrewd woman? I am surprised that she never did worse.
Patron of the arts.
Catherine believed in the humanist ideal of the learned Renaissance prince whose authority depended on letters as well as arms. She was inspired by the example of her father-in-law, King Francis I of France, who had hosted the leading artists of Europe at his court, and by her Medici ancestors. In an age of civil war and declining respect for the monarchy, she sought to bolster royal prestige through lavish cultural display. Once in control of the royal purse, she launched a programme of artistic patronage that lasted for three decades. During this time, she presided over a distinctive late French Renaissance culture in all branches of the arts.
An inventory drawn up at the Hôtel de la Reine after Catherine's death shows her to have been a keen collector. Listed works of art included tapestries, hand-drawn maps, sculptures, rich fabrics, ebony furniture inlaid with ivory, sets of china, and Limoges pottery. There were also hundreds of portraits, for which a vogue had developed during Catherine's lifetime. Many portraits in her collection were by Jean Clouet (1480–1541) and his son François Clouet (c. 1510 – 1572). François Clouet drew and painted portraits of all Catherine's family and of many members of the court. After Catherine's death, a decline in the quality of French portraiture set in. By 1610, the school patronised by the late Valois court and brought to its pinnacle by François Clouet had all but died out.
Beyond portraiture, little is known about the painting at Catherine de' Medici's court. In the last two decades of her life, only two painters stand out as recognisable personalities: Jean Cousin the Younger (c. 1522 – c. 1594), few of whose works survive, and Antoine Caron (c. 1521 – 1599), who became Catherine's official painter after working at Fontainebleau under Primaticcio. Caron's vivid Mannerism, with its love of ceremonial and its preoccupation with massacres, reflects the neurotic atmosphere of the French court during the Wars of Religion.
Many of Caron's paintings, such as those of the "Triumphs of the Seasons", are of allegorical subjects that echo the festivities for which Catherine's court was famous. His designs for the Valois Tapestries celebrate the "fêtes", picnics, and mock battles of the "magnificent" entertainments hosted by Catherine. They depict events held at Fontainebleau in 1564; at Bayonne in 1565 for the summit meeting with the Spanish court; and at the Tuileries in 1573 for the visit of the Polish ambassadors who presented the Polish crown to Catherine's son Henry of Anjou. Biographer Leonie Frieda suggests that "Catherine, more than anyone, inaugurated the fantastic entertainments for which later French monarchs also became renowned".
The musical shows in particular allowed Catherine to express her creative gifts. They were usually dedicated to the ideal of peace in the realm and based on mythological themes. To create the necessary dramas, music, and scenic effects for these events, Catherine employed the leading artists and architects of the day. Historian Frances Yates has called her "a great creative artist in festivals". Catherine gradually introduced changes to the traditional entertainments: for example, she increased the prominence of dance in the shows that climaxed each series of entertainments. A distinctive new art form, the "ballet de cour", emerged from these creative advances. Owing to its synthesis of dance, music, verse, and setting, the production of the "Ballet Comique de la Reine" in 1581 is regarded by scholars as the first authentic ballet.
Catherine de' Medici's great love among the arts was architecture. "As the daughter of the Medici", suggests French art historian Jean-Pierre Babelon, "she was driven by a passion to build and a desire to leave great achievements behind her when she died." After Henry II's death, Catherine set out to immortalise her husband's memory and to enhance the grandeur of the Valois monarchy through a series of costly building projects. These included work on châteaux at Montceaux-en-Brie, Saint-Maur-des-Fossés, and Chenonceau. Catherine built two new palaces in Paris: the Tuileries and the Hôtel de la Reine. She was closely involved in the planning and supervising of all her architectural schemes.
Catherine had emblems of her love and grief carved into the stonework of her buildings. Poets lauded her as the new Artemisia, after Artemisia II of Caria, who built the Mausoleum at Halicarnassus as a tomb for her dead husband. As the centrepiece of an ambitious new chapel, she commissioned a magnificent tomb for Henry at the basilica of Saint Denis. It was designed by Francesco Primaticcio (1504–1570), with sculpture by Germain Pilon (1528–1590). Art historian Henri Zerner has called this monument "the last and most brilliant of the royal tombs of the Renaissance". Catherine also commissioned Germain Pilon to carve the marble sculpture that contains Henry II's heart. A poem by Ronsard, engraved on its base, tells the reader not to wonder that so small a vase can hold so large a heart, since Henry's real heart resides in Catherine's breast.
Although Catherine spent ruinous sums on the arts, most of her patronage left no permanent legacy. The end of the Valois dynasty so soon after her death brought a change in priorities.
Issue.
Catherine de' Medici married Henry, Duke of Orléans, the future Henry II of France, in Marseille on 28 October 1533. She gave birth to ten children, seven of whom survived to adulthood. Her three oldest sons became king of France; two of her daughters married kings; and one married a duke. Catherine outlived all her children except Henry III, who died seven months after her, and Margaret, who inherited her robust health.

</doc>
<doc id="44156" url="http://en.wikipedia.org/wiki?curid=44156" title="Clue">
Clue

Clue may refer to:
In arts and entertainment:
In science and technology:
Other uses:

</doc>
<doc id="44158" url="http://en.wikipedia.org/wiki?curid=44158" title="Conservative force">
Conservative force

A conservative force is a force with the property that the work done in moving a particle between two points is independent of the taken path. Equivalently, if a particle travels in a closed loop, the net work done (the sum of the force acting along the path multiplied by the distance travelled) by a conservative force is zero.
A conservative force is dependent only on the position of the object. If a force is conservative, it is possible to assign a numerical value for the potential at any point. When an object moves from one location to another, the force changes the potential energy of the object by an amount that does not depend on the path taken. If the force is not conservative, then defining a scalar potential is not possible, because taking different paths would lead to conflicting potential differences between the start and end points.
Gravity is an example of a conservative force, while friction is an example of a non-conservative force.
Informal definition.
Informally, a conservative force can be thought of as a force that "conserves" mechanical energy. Suppose a particle starts at point A, and there is a force "F" acting on it. Then the particle is moved around by other forces, and eventually ends up at A again. Though the particle may still be moving, at that instant when it passes point A again, it has traveled a closed path. If the net work done by "F" at this point is 0, then "F" passes the closed path test. Any force that passes the closed path test for all possible closed paths is classified as a conservative force.
The gravitational force, spring force, magnetic force (according to some definitions, see below) and electric force (at least in a time-independent magnetic field, see Faraday's law of induction for details) are examples of conservative forces, while friction and air drag are classical examples of non-conservative forces.
For non-conservative forces, the mechanical energy that is lost (not conserved) has to go somewhere else, by conservation of energy. Usually the energy is turned into heat, for example the heat generated by friction. In addition to heat, friction also often produces some sound energy. The water drag on a moving boat converts the boat's mechanical energy into not only heat and sound energy, but also wave energy at the edges of its wake. These and other energy losses are irreversible because of the second law of thermodynamics.
Path independence.
A direct consequence of the closed path test is that the work done by a conservative force on a particle moving between any two points does not depend on the path taken by the particle. 
This is illustrated in the figure to the right: The work done by the gravitational force on an object depends only on its change in height because the gravitational force is conservative. The work done by a conservative force is equal to the negative of change in potential energy during that process. For a proof, imagine two paths 1 and 2, both going from point A to point B. The variation of energy for the particle, taking path 1 from A to B and then path 2 backwards from B to A, is 0; thus, the work is the same in path 1 and 2, i.e., the work is independent of the path followed, as long as it goes from A to B.
For example, if a child slides down a frictionless slide, the work done by the gravitational force on the child from the top of the slide to the bottom will be the same no matter what the shape of the slide; it can be straight or it can be a spiral. The amount of work done only depends on the vertical displacement of the child.
Mathematical description.
A force field "F", defined everywhere in space (or within a simply-connected volume of space), is called a "conservative force" or "conservative vector field" if it meets any of these three "equivalent" conditions:
The term "conservative force" comes from the fact that when a conservative force exists, it conserves mechanical energy. The most familiar conservative forces are gravity, the electric force (in a time-independent magnetic field, see Faraday's law), and spring force.
Many forces (particularly those that depend on velocity) are not force "fields". In these cases, the above three conditions are not mathematically equivalent. For example, the magnetic force satisfies condition 2 (since the work done by a magnetic field on a charged particle is always zero), but does not satisfy condition 3, and condition 1 is not even defined (the force is not a vector field, so one cannot evaluate its curl). Accordingly, some authors classify the magnetic force as conservative, while others do not. The magnetic force is an unusual case; most velocity-dependent forces, such as friction, do not satisfy any of the three conditions, and therefore are unambiguously nonconservative.
Nonconservative forces.
Nonconservative forces can arise in classical physics due to neglected degrees of freedom or from time-dependent potentials. For instance, friction may be treated without resorting to the use of nonconservative forces by considering the motion of individual molecules; however that means every molecule's motion must be considered rather than handling it through statistical methods. For macroscopic systems the nonconservative approximation is far easier to deal with than millions of degrees of freedom. Examples of nonconservative forces are friction and non-elastic material stress.
However, general relativity is non-conservative, as seen in the anomalous precession of Mercury's orbit. However, general relativity can be shown to conserve a stress-energy-momentum pseudotensor.

</doc>
<doc id="44159" url="http://en.wikipedia.org/wiki?curid=44159" title="Coda">
Coda

Coda can denote any concluding event, summation, or section.
Coda may also refer to:

</doc>
<doc id="44160" url="http://en.wikipedia.org/wiki?curid=44160" title="Sauk people">
Sauk people

The Sac or Sauk are a group of Native Americans of the Eastern Woodlands culture group. Their autonym is oθaakiiwaki, and their exonym is Ozaagii(-wag) in Ojibwe. The latter name was transliterated into French and English by colonists of those cultures.
Today they have three federally recognized tribes, together with the Meskwaki (Fox), located in Iowa, Oklahoma and Kansas.
History.
The Sauk, an Algonquian languages people, are believed to have developed as a people along the St. Lawrence River. They were driven by pressure from other tribes, especially the powerful Iroquois League or "Haudenosaunee", to migrate to Michigan, where they settled around Saginaw Bay. Due to the yellow-clay soils found around Saginaw Bay, they called themselves the autonym of "Oθaakiiwaki" (often interpreted to mean "yellow-earth".)
The neighboring Ojibwe and Ottawa peoples referred to them by the exonym "Ozaagii", meaning "those at the outlet". French colonists transliterated that as "Sac" and the English as "Sauk". Anishinaabe expansion and the Huron attempt to gain regional stability drove the Sac out of their territory. The Huron were armed with guns supplied by their French trading partners. The Sac moved south to territory in parts of what are now northern Illinois and Wisconsin.
A closely allied tribe, the "Meskwaki" (Fox), were noted for resisting French encroachment, having fought two wars against them in the early 18th century. After a devastating battle of September 9, 1730, in Illinois, in which hundreds of warriors were killed and many women and children taken captive by French allies, Fox refugees took shelter with the Sac, making them subject to French attack. The Sac continued moving west to Iowa and Kansas. Two important leaders arose among the Sac: Keokuk and Black Hawk. At first Keokuk accepted the loss of land as inevitable in the face of the vast numbers of white soldiers and settlers coming west. He tried to preserve tribal land and his people, and to keep the peace.
Having failed to receive expected supplies from the Americans on credit, Black Hawk wanted to fight, saying his people were "forced into war by being deceived." Led by Black Hawk in 1832, the mainly Sac band resisted the continued loss of lands (in western Illinois, this time.) Their warfare with United States forces resulted in defeat at the hands of General Edmund P. Gaines in the Black Hawk War.
About this time, one group of Sac moved into Missouri, and later to Kansas and Nebraska. In 1869 the larger group of Sac moved into reservations in Oklahoma, where they merged with the Meskwaki as the federally recognized Sac and Fox Nation. (The United States had been making treaties with them together since their residency in the Midwest.) A smaller number returned to the Midwest from Oklahoma (or resisted leaving.) They joined the Mesquakie at the Mesqwaki Settlement, Iowa.
Clan system.
Originally, the Sauk had a patrilineal clan system, in which descent and inheritance was traced through the father. Clans which continue are: Fish, Ocean/Sea, Thunder, Bear, Fox, Potato, Deer, Beaver, Snow, and Wolf. The tribe was governed by a council of sacred clan chiefs, a war chief, the head of families, and the warriors. Chiefs were recognized in three categories: civil, war, and ceremonial. Only the civil chiefs were hereditary. The other two chiefs were recognized by bands after they demonstrated their ability or spiritual power.
This traditional manner of selecting historic clan chiefs and governance was replaced in the 19th century by the United States appointing leaders through their agents at the Sac and Fox Agency, or reservation in Indian Territory (now Oklahoma). In the 20th century, the tribe adopted a constitutional government patterned after the United States form. They elect their chiefs.
Federally recognized tribes.
Today the federally recognized Sac and Fox tribes are:
Language.
Sauk (or Sac) is one of the many Algonquian languages. It is very closely related to the varieties spoken by the Meskwaki and the Kickapoo tribes; linguist often describe these three as dialects of the same language. Each of the dialects contains archaisms and innovations that distinguish them from each other. Sauk and Meskwaki appear to be the most closely related of the three, reflecting the peoples' long relationship. (Goddard 1978). auk is considered to be mutually intelligible, to a point, with Fox.
In their own language, the Sauk at one time called themselves as "asakiwaki" [a-‘sak-i-wa-ki], “people of the outlet. (Bonvillain 1995)
The Sauk people have a syllabic orthography for their language. They published a Primer Book in 1977, based on a “traditional” syllabary which existed in 1906. It is intended to help modern-day Sauk to learn to write as well as speak their ancestral tongue. A newer orthography was proposed around 1994 to aid in language revival. The former syllabary was aimed at remaining native speakers of Sauk; the more recent orthography was developed for native English speakers, as many Sauk grow up with English as their first language (Müller 1994).
Sauk has so few speakers that it is considered an endangered language, as are numerous others native to North America.
In 2012, Shawnee High School in Shawnee, Oklahoma began to offer a Sauk language course.
Phonology.
Sauk does not have a lot of phonemes, compared to many other languages: four vowels, two semi-vowels, and eight consonants.
Consonants.
The voiceless glottal fricative /h/ was omitted in the 1977 syllabary. It has been added back into later editions, because it is an important distinctive sound in the Sauk language (Müller 1994). All the Sauk consonants are voiceless, with the exception of both nasals.
All three stops are recognized to have at least two allophones each, as follows (adapted from Müller 1994):
Semi-Vowels (Glides).
Müller (1994) uses the American Phonetic transcription of the palatal glide, /y/, in her article. The International Phonetic Alphabet is used for the above chart, transcribing the phoneme as /j/.
Vowels.
Vowel length is important in the Sauk language. Müller presents four vowels, each with two allophones (1994):
Pitch and tone.
Pitch and tone are also important when speaking Sauk (Müller 1994).
Syllables & Morphology.
Sauk is a polysynthetic language – affixes are used to modify words. Because of this, in the Sauk orthography, words are written by identifying each syllable, to aid in language learning.
Both the Sac and Fox languages are known for “swallowing” syllables that are in word-final position, which can make identification of individual sounds more difficult for the language learner. (Müller 1994)
Orthography.
Two samples of written Sauk language, as they appear in Müller (1994):
"Ho! Ne nu ta ma"!
'Hi! I speak Sauk!'
"Ni swi me cli ke a ki a la se te ke wa ki a la te ki ki"
"e ka ta wi ke mi yak i e we li ke mi ya ki ne ko ti"
"me cle ke a e cla gwe ne mo tti wi ne li wi tti cle we na"
"li ta ske wa ne li se ke"
"Two turtles were sunning on a bank when a thunderstorm approached. When it began to rain, one turtle said to the other, 'I don’t want to get wet,' and jumped into the lake."
Geographical names.
Lake Osakis in west-central Minnesota, the Sauk River, which flows from Lake Osakis, and the towns of Osakis, Sauk Centre, and Sauk Rapids all were named for association historically with a small party of Sac who made camp on the shores of Lake Osakis. They had been banished from their tribe for murder. According to Anishinaabe oral tradition, these five Sac were killed by local Dakota in the late 18th century.
Place names with "Sauk" references include: 

</doc>
<doc id="44165" url="http://en.wikipedia.org/wiki?curid=44165" title="Cluedo">
Cluedo

Cluedo , or Clue in North America, is a murder mystery game for three to six players, devised by Anthony E. Pratt from Birmingham, England and currently published by the United States game and toy company Hasbro. The object of the game is to determine who murdered the game's victim ("Dr. Black" in the UK version and "Mr. Boddy" in North American versions), where the crime took place, and which weapon was used. Each player assumes the role of one of the six suspects, and attempts to deduce the correct answer by strategically moving around a game board representing the rooms of a mansion and collecting clues about the circumstances of the murder from the other players. 
Numerous games, books, and a film have been released as part of the "Cluedo" franchise. Several spinoffs have been released featuring various extra characters, weapons and rooms, or different game play. The original game is marketed as the "Classic Detective Game", while the various spinoffs are all distinguished by different slogans.
In 2008, "" was created (with changes to board, gameplay and characters) as a modern spinoff.
History.
In 1944, Anthony E. Pratt, an English musician, applied for a patent of his invention of a murder/mystery-themed game, originally named "Murder!" The game was originally invented as a new game to play in bomb shelters. Shortly thereafter, Pratt and his wife presented the game to Waddingtons' executive, Norman Watson, who immediately purchased the game and provided its trademark name of "Cluedo" (a play on "clue" and "Ludo"; "ludo" is Latin for "I play"). Though the patent was granted in 1947, due to post-war shortages, the game was not officially launched until 1949, when the game was simultaneously licensed to Parker Brothers in the United States for publication, where it was renamed "Clue" along with other minor changes.
There were several differences between the original game concept and that initially published in 1949, In particular, Pratt's original design calls for ten characters, one of whom was to be designated the victim by random drawing prior to the start of the game. These ten included the eliminated Mr. Brown, Mr. Gold, Miss Grey, and Mrs. Silver, while renaming Nurse White to Mrs. White and Colonel Yellow to Colonel Mustard. The game allowed for play of up to eight remaining characters, providing for nine suspects in total. Originally there were eleven rooms, including the eliminated "gun room" and cellar. In addition there were nine weapons including the unused bomb, syringe, shillelagh (walking stick/cudgel), fireplace poker and the later used axe and poison. Some of these unused weapons and characters appeared later in spin-off versions of the game.
Some gameplay aspects were different as well. Notably, the remaining playing cards were distributed into the rooms to be retrieved, rather than dealt directly to the players. Players also had to land on another player in order to make suggestions about that player's character through the use of special counter-tokens, and once exhausted, a player could no longer make suggestions. There were other minor differences, all of which were later updated by the game's initial release and remain essentially unchanged in the standard Classic Detective Game editions of the game.
Equipment.
The game consists of a board which shows the rooms, corridors and secret passages of an English country house called "Tudor Mansion", although previously named variously as "Tudor Close" or "Tudor Hall", and in some editions "Boddy Manor" or "Boddy Mansion". More recent editions have restored the name Tudor Mansion to the mansion, and say the mansion is in Hampshire, England in the year 1926. The game box also includes several coloured playing pieces to represent characters, miniature murder weapon props, one or two six-sided dice, three sets of cards, each set describing the aforementioned rooms, characters and weapons, "Solution Cards" envelope to contain one card from each set of cards, and a "Detective's Notes" pad on which are printed lists of rooms, weapons and characters, so players can keep detailed notes during the game.
Suspects.
Depending on edition, the playing pieces are typically made of coloured plastic, shaped like chess pawns, or character figurines. Occasionally they are made from wood or pewter. The standard edition of Cluedo comes with six basic tokens representing these original characters:
Weapons.
The playing tokens are typically made out of unfinished pewter, with the exception of the rope, which may also come in plastic, or string depending on edition. Special editions have included gold plated, brass finished and sterling silver versions, which have appeared in a variety of designs.
Rooms.
There are nine rooms in the mansion where the murder can take place, laid out in circular fashion on the game board, separated by pathways overlaid by playing spaces. Each of the four corner rooms contains a secret passage that leads to the room on the opposite diagonal corner of the map. The centre room (often referred to as the Cellar, or Stairs) is inaccessible to the players, but contains the solution envelope, and is not otherwise used during game play. Coloured "start" spaces encircle the outer perimeter which correspond to each player's suspect token. Each character starts at the corresponding coloured space.
† ‡ "denotes secret passages to opposite corner"
Rules.
At the beginning of play, three cards — one suspect, one weapon, and one room card — are chosen at random and put into a special envelope, so that no one can see them. These cards represent the facts of the case. The remainder of the cards are distributed among the players.
Players are instructed to assume the token/suspect nearest them. In older versions, play begins with Miss Scarlett and proceeds clockwise. In modern versions, all players roll the dice and the highest total starts the game and then proceeds clockwise. Players roll the dice (some versions contain one and others two) and move along the board's corridor spaces, or into the rooms accordingly.
The aim is to deduce the details of the murder; that is, the cards in the envelope. There are six characters, six murder weapons and nine rooms, leaving the players with 324 possibilities. While determining the details of the murder, players announce suggestions to the other players, for example:
The player's token must be in the room they suggest (in the preceding examples, it must be in either the Hall, the Conservatory, the Billiard Room, the Dining Room, the Library, or the Kitchen); suggestions may not be made in the corridors. The token and weapon suggested are moved into the room, if not already present.
The player to the left of the suggesting player must then disprove the suggestion, if they can, by showing the suggesting player one (and only one) of the cards containing one of the suggestion components (either the suspect, the weapon, or the room), as this proves that the card cannot be in the envelope. This is done in secret so that the other players cannot see which card is being used to disprove the suggestion. If a player has more than one such card, they may select which one to show. If the first player to the left of the suggesting player does not have any of the three cards needed to disprove the suggestion, the next player clockwise must disprove the suggestion, if possible, and so on clockwise until either the suggesting player is shown a card that disproves their suggestion, or each player advises that they can not disprove the suggestion. The suggesting player's turn then ends. The suggesting player does not advise the other players whether they hold any of the three cards.
Once a player has sufficiently narrowed the solution, that player can make an accusation. According to the rules, "When you think you have worked out which three cards are in the envelope, you may, on your turn, make an Accusation and name any three elements you want." Players may name any room (unlike a Suggestion, where a player's character pawn must be in the room that the player suggests).
The accusing player checks the validity of the accusation by checking the cards, keeping them concealed from other players. If they've made an incorrect accusation, they play no further part in the game except to reveal cards secretly to one of the remaining players when required to do so to disprove suggestions. Also, according to the rules, "If, after making a false Accusation, your character pawn is blocking a door, [you must] move it into that room so that other players may enter." If the player made a correct accusation, the solution cards are shown to the other players and the game ends.
A player can use the piece representing the murderer. This does not affect the game play; the object of the game is still to be the first to make the correct accusation. All editions of the current version of the game are advertised as a three to six player game only. Traditionally, the UK version was advertised for two to six players.
Strategy.
Though gameplay is relatively straightforward as described above, various strategies allow players to maximize their opportunities to make suggestions and therefore gain the advantage of accumulating information faster. As alluded to above, blocking the entrance to a room is one way to prevent an opponent from entering a desired room and making a suggestion.
Choice of suspect.
The first opportunity is in choosing the initial playing piece. Mrs. Peacock has an immediate advantage of being one space closer to the first room than any of the other players. Miss Scarlet has the advantage of moving first. Professor Plum can move to the study, and then take the secret passage to the Kitchen, the hardest room to get to.
Navigating the board.
The next opportunity is choice of initial rooms to enter. Again Mrs. Peacock has an advantage in that she is closest to the Conservatory, a corner room with a secret passage, enabling a player on their turn to move immediately to another room and make a suggestion without rolling the dice. Miss Scarlet has a similar advantage with the Lounge. Making as many suggestions as possible gives a player an advantage to gain information. Therefore, moving into a new room as frequently as possible is one way to meet this goal. Players should make good use of the secret passages. Following the shortest path between rooms then is a good choice, even if a player already holds the card representing that room in their hand. As mentioned earlier, blocking passage of another player prevents them from attaining rooms from which to make suggestions. Various single space tracks on the board can therefore become traps, which are best avoided by a player when planning a path from room to room.
Making suggestions.
Each player begins the game with three to six cards in his hand. Keeping track of which cards are shown to each player is important in deducing the solution. Detective Notes are supplied with the game to help make this task easier. The pads can keep not only a history of which cards are in a player's hand, but also which cards have been shown by another player. It can also be useful in deducing which cards the other players have shown one another. A player makes a suggestion to learn which cards may be eliminated from suspicion. However, in some cases it may be advantageous for a player to include one of their own cards in a suggestion. This technique can be used for both forcing a player to reveal a different card as well as misleading other players into believing a specific card is suspect. Therefore, moving into a room already held in the player's hand may work to their advantage. Suggestions may also be used to thwart a player's opponent. Since every suggestion results in a suspect token being re-located to the suggested room, a suggestion may be used to prevent another player from achieving their intended destination, preventing them from suggesting a particular room, especially if that player appears to be getting close to a solution.
Notetaking.
One reason the game is enjoyed by many ages and skill levels is that the complexity of notetaking can increase as a player becomes more skilful. An amateur may simply mark off the cards he has been shown; more advanced players will keep track of who has and who does "not" have a particular card, possibly with the aid of an additional grid. Expert players may keep track of each suggestion made, knowing that the player who answers it must have at least one of the cards named; which one can be deduced by later events. One can also keep track of which cards a given player has seen, in order to minimize information revealed to that player and/or to read into that player's suggestions.
Editions.
Parker Brothers and Waddingtons each produced their own unique editions between 1949 and 1992. Hasbro purchased both companies in the early 1990s and continued to produce unique editions for each market until 2002/2003 when the current edition of Clue/Cluedo was first released. At this time, Hasbro produced a unified product across markets. The game was then localized with regional differences in spelling and naming conventions.
During Cluedo's long history, eight unique Clue editions were published in North America (1949, '56/60, '60/63, '72, '86, '92, '96, and 2002), including miniaturized "travel" editions. However, only three distinct editions of Cluedo were released in the UK – the longest of which lasted 47 years from its introduction in 1949 until its first successor in 1996. The eighth North America and fourth UK editions constitute the current shared game design. International versions occasionally developed their own unique designs for specific editions. However, most drew on the designs and art from either the US or UK editions, and in some cases mixing elements from both, while localizing others – specifically suspect portraits.
While the suspects' appearance and interior design of Dr. Black's/Mr. Boddy's mansion changed with each edition, the weapons underwent relatively minor changes, with the only major redesign occurring in the fourth 1972 US edition, which was adopted by the second 1996 UK edition and remains the standard configuration across all Classic Detective Game versions since. The artwork for the previous US editions tended to reflect the current popular style at the time they were released. The earlier UK editions were more artistically stylized themes. From 1972 on, the US editions presented lush box cover art depicting the six suspects in various candid poses within a room of the mansion. The UK would finally adopt this style only in its third release in 2000, prior to which Cluedo boxes depicted basic representations of the contents. Such lavish box art illustrations have become a hallmark of the game, since copied for the numerous licensed variants which pay homage to Clue.
Marketing.
Cluedo was originally marketed as "The Great New Detective Game" upon its launch in 1949 in North America, and quickly made a deal to license "The Great New Sherlock Holmes Game" from the Sir Arthur Conan Doyle estate. Advertising at the time suggested players would take on the guise of "Sherlock Holmes following the path of the criminal", however no depictions of Holmes appears in the advertising or on the box. By 1950 the game was simply marketed as "The Great Detective Game" until the 1960s, at which time it became: "Parker Brothers Detective Game". But the association with Sherlock Holmes was far from over. With the launch of the US 1972 edition, a television commercial showed Holmes and Watson engaged in a particularly competitive game. Adjusting with the times, in 1979 US TV commercials a detective resembling a bumbling Inspector Clouseau from the popular Pink Panther film franchise, looks for clues. In 1986, the marketing slogan added "Classic Detective Game" which persists through the last 2002/2003 edition.
In the UK, Cluedo did not start using "The Great Detective Game" marketing slogan until the mid-1950s, which it continued using it until the 2000 edition when it adopted the "Classic Detective Game" slogan. However, in the mid-1950s Waddingtons also adopted a Sherlock Holmes-type detective to adorn their box covers for a brief time, though unlike the US editions, there was no acknowledgement that the character was actually the famous detective. In the 1980s, as in the US, Sherlock Holmes also appeared in TV advertising of the time, along with other classic detectives such as Sam Spade.
Spinoffs.
Waddingtons, Parker Brothers and Hasbro have created many spin-off versions of the game. Spin-off games consist of alternative rule variations of the original Classic Detective Game, which are not to be confused with themed "variants" which uses the same rules and game configuration. In 1985, the brand expanded to include a feature film, television series, a musical, as well as numerous books.
Games.
In addition to revising the rules of gameplay, many of the games also introduced new characters, rooms and locations, weapons and/or alternative objectives.
Computer and video games.
Various versions of the game were developed for Commodore 64, Atari ST, PC, Game Boy Advance, ZX Spectrum, Nintendo DS, Super Nintendo Entertainment System, CD-i, Sega Mega Drive/Genesis, PC, Mac, Xbox 360 and Apple iPhone / iPod Touch.
"Clue: Murder at Boddy Mansion", was released in 1998 for Microsoft Windows.
In 1999 "" was released, which was not based directly upon the board game, but instead uses the familiar characters in a new mystery.
An arcade version of the game was released on an itbox terminal which involves answering questions with a chance to win money. It is available in many pubs throughout the UK.
In 1994–1996, there were six mysteries: "The Hooded Madonna," "Happy Ever After", "Deadly Patent", "Blackmail", "The Road to Damascus", and "Not in my Backyard", with actors.
"Clue Classic" was released on June 3, 2008 developed by Games Cafe for Hasbro. It is a single player interactive game based on the latest 2002/2003 Classic Detective Game artwork featuring the original six characters, weapons and nine original rooms.
In May 2009 Electronic Arts released a version of Clue for the Apple iPhone and iPod Touch on the Apple iTunes Music Store, entitled . This version is an entirely new game, based on the most recent spin-off game of . Additionally, EA's games site Pogo has a hidden-object game called "" (or "Clue" depending on market), where each game is a 60-minute "episode" (the object being to complete the game overall within this time limit). "Episodes" are usually grouped into "series" of two or more.
On the iWin website, there is a Hidden Object Game called "Clue: Accusations & Alibis."
Film.
A comedic film "Clue," based on the American version of the game, was released in 1985. In this version, the person murdered was Mr. Boddy. The film, which featured different endings released to different theatres, failed at the box office, but has subsequently attracted a cult following. All three endings released to theatres are available on the VHS and DVD versions of the film, to watch one after the other (VHS), or to select playing one or all three endings (DVD/Blu-ray).
In 2008, Universal Pictures reported that Hasbro, the makers of "Cluedo", had licensed several of its board games to the film company for feature film adaptations; among these was "Clue". Gore Verbinski was announced as director.
Television.
Game shows.
There have been several television game shows based upon this game. To date, there have been four seasons of the British version of "Cluedo" (and a Christmas version that in fact shows some similarity to the North American movie), and there have been other versions in Germany, France, Italy, Australia, Portugal and Scandinavia. The format for each puts two teams (each usually containing one celebrity and one person with law enforcement/research experience) against six in-character actors as the famed colour-coded suspects. There is a new murder victim every episode, who usually has it coming to them for one reason or another. Each episode uses different weapons. In the Christmas episode in the UK the six original weapons were used.
TV series.
On August 6, 2010, The Hub announced that an original five-part miniseries based on "Clue" was in pre-production. The miniseries premiered on November 14, 2011 and featured a youthful, ensemble cast loosely based on the characters of the board game, working together to unravel a mystery. The short mini-series draws similarities to the original board game and mostly to the 2012 spin-off Clue: The Classic Mystery Game which both featured the characters belonging or having ties to secret societies/houses and fitting closely with the character descriptions.
Documentary.
The "Clue" title and theme were used in the 1986 US documentary "Clue: Movies, Murders and Mystery" which took a look at mystery-related pieces of media including "Murder on the Orient Express;" "Murder, She Wrote;" "Sherlock Holmes" and other television series and movies, as well as a look at the board game itself. The one-hour special was hosted by Martin Mull, who had starred in the feature film adaptation the previous year; clips from the movie are seen intertwined with the footage.
Musical.
A comedic musical of "Clue," based on the American version of the game, ran Off Broadway in 1997, closing in 1999. At the start of each performance, three audience members each select one card from over-sized versions of the traditional game decks and place them in an envelope. The chosen cards determine the ending of the show, with 216 possible conclusions.
Play.
Penned by Robert Duncan with the cooperation of Waddingtons, the first official theatrical adaptation of Cluedo was presented by the amateur theatre group: The Thame Players in Oxfordshire in July 1985. The play was subsequently picked up by Hiss & Boo productions and began a successful tour of the UK. A second tour was undertaken in 1990. Like the musical, the play involved the audience's random selection of three solution cards which were revealed towards the end of the play, whereupon the actors would then conclude the play by performing one of the 216 endings possible. Presently the play is not available for performance due to a restriction by Hasbro, since Hasbro has been planning to make a new movie. It is unclear whether the restriction applies to the musical as well.
Books.
A series of 18 humorous children's books/teen books were published in the United States by Scholastic Press between 1992 and 1997 based on the "Clue" concept and created by A.E. Parker. The books featured the US "Clue" characters in short, comedic vignettes and asked the reader to follow along and solve a crime at the end of each. The answers are printed "Upside down" with an explanation on the following page following each chapter to see if the reader was able to guess correctly or not. The crime would usually be the murder of another guest besides Mr. Boddy, a robbery of some sort, or a simple contest, in which case they must figure out who won. The tenth and final vignette would always be the murder of Mr. Boddy. Somehow, Mr. Boddy would always manage to cheat death, such as fainting before the shot was fired or being shot with trick bullets. However, at the end of the 18th book, Mrs. Peacock kills Mr. Boddy out of starvation and Mr. Boddy "stays" dead. The books feature mysterious-sounding titles such as "Midnight phone calls" "Footprints in the fog" or "The secret, secret passage".
These books are now out of print but can still be bought from various online retailers in both new and used conditions.
In 2003, Canadian mystery writer Vicki Cameron wrote a new set of mini-mysteries, called the Clue Mysteries books. The series is geared toward a more adult audience while still retaining some comic absurdity as did the 1990s series. Only two were published. Both books feature more complex storylines and vocabulary, as well as fifteen mysteries apiece. The first book contains the more modern looking clue game cover by Drew Struzan.
Another book called "CLUE Code-Breaking Puzzles" was released in December 2008 written by Helene Hovanec. The book contains a whopping 60 mysteries.
A similar series of books featuring the "Clue Jr." characters was also published. The first book, unlike the others, features thirteen mysteries, not ten, and is titled simply enough "Who Killed Mr Boddy?". The name of the book is usually the name of the tenth mystery in which Boddy is killed.
The books notably depart from the film. Mr Boddy is a trillionaire, and the guests are his friends. But since Boddy has his will made out to his friends, they each try to kill him at one point with the intent on cashing in on his will. The guests are all given some sort of defining characteristic for comic effect, as well as to help the reader discern the culprit. Colonel Mustard constantly challenges other guests to duels, Professor Plum often forgets things, even what he is doing or his own name, and Mr. Green is notoriously greedy. Mrs. Peacock is highly proper and will not stand for any lack of manners, the maid Mrs. White hates her employer and all the guests, and Miss Scarlet is beautiful and seductive. The traits all help the reader identify the guests. For example, if a mystery thief suddenly forgets what he is doing, and another guest scolds him for his bad manners, the reader can safely assume the two guests are Plum and Peacock. Mr. Boddy himself is ludicrously naive, to the point where he accepts any attempt to kill him as an accident or a misunderstanding (such as a dropped wrench flying all the way across the Mansion and hitting him in the head), and invites the guests back to the mansion. This explains why he never seeks any legal action against his "friends," and invited them back despite repeated attempts to kill him. However, after a few books, he wises up enough to be suspicious of them, but continues to invite them over against better judgement.
The "Clue Jr." series originally had all six characters, but suddenly, some of the characters were taken out, leaving only four. The mysteries usually only included cases similar to the theft of a toy, but sometimes the cases were more serious. They are usually solved when the culprit traps himself in his own lies.
Jigsaw puzzles.
A series of jigsaw puzzles (500 piece Clue/750 piece Cluedo/200 Jr. ed.), based on the game was introduced in 1991. The jigsaw puzzles presented detailed stories with a biography for each of the standard suspects. The object was to assemble the jigsaw puzzles and then deduce the solutions presented in the mystery stories from the clues provided within the completed pictures.
Variants.
The following games are licensed thematic variations of the game, which follow the basic rules and configuration of the original Classic Detective Game or its spinoffs.
Cluedo: Discover the Secrets.
On August 8, 2008, Hasbro redesigned and updated the board, characters, weapons, and rooms. Changes to the rules of game play were made, some to accommodate the new features.
The suspects have new given names and backgrounds, as well as differing abilities that may be used during the game. The revolver is now a pistol, the lead pipe has been removed, and a bat, axe, and trophy have been added. The nine rooms have changed to (in clockwise order): Hall, Guest House, Dining Room, Kitchen, Patio, Spa, Theatre, Living Room, and Observatory.
There is also a second deck of cards—the Intrigue cards. In this deck, there are two types of cards, Keepers and Clocks. Keepers are special abilities; for example, "You can see the card". There are eight clocks—the first seven drawn do nothing—whoever draws the eighth is killed by the murderer and out of the game.
The player must move to the indoor swimming pool in the centre of the board to make an accusation. This adds some challenge versus the ability to make accusations from anywhere in the original game.
The most significant change to game play is that once the suspect cards have been taken, the remaining cards are dealt so that all players have an even number of cards (rather than dealt out so that "one player may have a slight advantage"). This means that depending on the number of players a number of cards are left over. These cards are placed face down in the middle and are not seen unless a player takes a turn in the pool room to look at them.
The changes to the game have been criticized in the media for unnecessarily altering classic cultural icons. The game has also been criticized by lovers of the original game.
As of December 2012, Hasbro no longer sells the game via its website.
Worldwide differences.
Besides some rule differences listed above, some versions label differently the names of characters, weapons, rooms and in some instances the actual game itself.
In Canada and the U.S., the game is known as "Clue." It was retitled because the traditional British board game Ludo, on which the name is based, was less well known there than its American variant "Parcheesi."
The North American versions of "Clue" also replace the character "Reverend Green" from the original "Cluedo" with "Mr. Green." This is the only region to continue to make such a change. Minor changes include "Miss Scarlett" with her name being spelt with one 't', the spanner being called a wrench, and the dagger renamed a knife. And until 2003, the lead piping was known as the lead pipe only in the North American edition.
In some international versions of the game (mostly the Spanish-language ones) the colours of some pieces are different, so as to correspond with the changes to each suspect's unique foreign name variations. In some cases, rooms and weapons are changed in addition to other regional variances.
In South America it is licensed and sold under several different names. In particular, it is notably marketed as "Detetive" in Brazil.
In Norway it was first released as "Scotland Yard" by Damm. It was later re-released as "Cluedo", but the rules are the same.
Merchandising.
The Clue and Cluedo brands are well-merchandised through umbrellas, books, toys, clothing and other miscellaneous items.

</doc>
<doc id="44168" url="http://en.wikipedia.org/wiki?curid=44168" title="Brahms (disambiguation)">
Brahms (disambiguation)

Johannes Brahms (1833–1897) was a German composer and pianist.
Brahms may also refer to:

</doc>
<doc id="44169" url="http://en.wikipedia.org/wiki?curid=44169" title="Purcell (disambiguation)">
Purcell (disambiguation)

Henry Purcell (1659-1695) was an English composer.
Purcell may also refer to:

</doc>
<doc id="44173" url="http://en.wikipedia.org/wiki?curid=44173" title="Wagner (disambiguation)">
Wagner (disambiguation)

Wagner may refer to:

</doc>
<doc id="44175" url="http://en.wikipedia.org/wiki?curid=44175" title="House of Medici">
House of Medici

The House of Medici ( ; ]) was a banking family, political dynasty and later royal house that first began to gather prominence under Cosimo de' Medici in the Republic of Florence during the late 14th century. The family originated in the Mugello region of the Tuscan countryside, gradually rising until they were able to fund the Medici Bank. The bank was the largest in Europe during the 15th century, seeing the Medici gain political power in Florence — though officially they remained citizens rather than monarchs.
The Medici produced four Popes of the Catholic Church—Pope Leo X (1513–1521), Pope Clement VII (1523–1534), Pope Pius IV (1559–1565), and Pope Leo XI (1605); two regent queens of France—Catherine de' Medici (1547–1559) and Marie de' Medici (1600–1610); and, in 1531, the family became hereditary Dukes of Florence. In 1569, the duchy was elevated to a grand duchy after territorial expansion. They ruled the Grand Duchy of Tuscany from its inception until 1737, with the death of Gian Gastone de' Medici. The grand duchy witnessed degrees of economic growth under the earlier grand dukes, but by the time of Cosimo III de' Medici, Tuscany was fiscally bankrupt.
Their wealth and influence initially derived from the textile trade guided by the guild of the "Arte della Lana". Like other signore families, they dominated their city's government, they were able to bring Florence under their family's power, and they created an environment where art and humanism could flourish. They along with other families of Italy, such as the Visconti and Sforza of Milan, the Este of Ferrara, and the Gonzaga of Mantua, fostered and inspired the birth of the Italian Renaissance.
The Medici Bank was one of the most prosperous and most respected institutions in Europe. There are some estimates that the Medici family were the wealthiest family in Europe for a time. From this base, they acquired political power initially in Florence and later in wider Italy and Europe. A notable contribution to the profession of accounting was the improvement of the general ledger system through the development of the double-entry bookkeeping system for tracking credits and debits. The Medici family were among the earliest businesses to use the system.
History.
Origins.
The Medici family came from the agricultural Mugello region, north of Florence, being mentioned for the first time in a document of 1230.The origin of the name is uncertain. "Medici" is the plural of "medico", also written "del medico" or "delmedigo", meaning, "medical doctor". It has been suggested that the name derived from one "Medico di Potrone", a castellan of Potrone in the late 11th century, who presumably was the family's ancestor.
The dynasty began with the founding of the Medici Bank.
Rise to power.
Until the late 14th century, prior to the Medici, Florence's leading family were the House of Albizzi. In 1293 the Ordinances of Justice were enacted which effectively became the constitution of the republic of Florence throughout the Italian Renaissance. The city's numerous luxurious palazzi were becoming surrounded by townhouses, built by the ever prospering merchant class. In 1298, one of the leading banking families of Europe, the Bonsignoris, were bankrupted and so the city of Siena lost her status as the banking center of Europe to Florence.
The main challengers of the Albizzi family were the Medicis, first under Giovanni de' Medici, later under his son Cosimo di Giovanni de' Medici and grandson, Lorenzo de' Medici. The Medici controlled the Medici bank—then Europe's largest bank—and an array of other enterprises in Florence and elsewhere. In 1433, the Albizzi managed to have Cosimo exiled. The next year, however, saw a pro-Medici Signoria elected and Cosimo returned. The Medici became the town's leading family, a position they would hold for the next three centuries. Florence remained a republic until 1537, traditionally marking the end of the High Renaissance in Florence, but the instruments of republican government were firmly under the control of the Medici and their allies, save during the intervals after 1494 and 1527. Cosimo and Lorenzo rarely held official posts, but were the unquestioned leaders.
The Medici family was connected to most other elite families of the time through marriages of convenience, partnerships, or employment, as a result of which the Medici family had a central position in the social network: several families had systematic access to the rest of the elite families only through the Medici, perhaps similar to banking relationships. Some examples of these families include the Bardi, Salviati, Cavalcanti, and the Tornabuoni. This has been suggested as a reason for the rise of the Medici family.
Members of the family rose to some prominence in the early 14th century in the wool trade, especially with France and Spain. Despite the presence of some Medici in the city's government institutions, they were still far less notable than other outstanding families such as the Albizzi or the Strozzi. One Salvestro de' Medici was speaker of the woolmakers' guild during the Ciompi revolt, and one Antonio was exiled from Florence in 1396. The involvement in another plot in 1400 caused all branches of the family to be banned from Florentine politics for twenty years, with the exception of two: from one of the latter, that of Averardo de' Medici (1320-1363), originated the Medici dynasty.
15th century.
Averardo's son, Giovanni di Bicci de' Medici (c. 1360–1429), increased the wealth of the family through his creation of the Medici Bank, and became one of the richest men in the city of Florence. Although he never held any political charge, he gained strong popular support for the family through his support for the introduction of a proportional taxing system. Giovanni's son Cosimo the Elder, "Pater Patriae", took over in 1434 as gran maestro, and the Medici became unofficial heads of state of the Florentine republic.
Cosimo, Piero, and Lorenzo, three successive generations of the Medici, ruled over Florence through the greater part of the 15th century, without altogether abolishing representative government, yet while clearly dominating it. These three members of the Medici family had great skills in the management of so "restive and independent a city" as Florence, but when Lorenzo died in 1492, his son Piero proved quite incapable, and within two years he and his supporters were forced into exile [with] a republican government replac[ing] him.
Piero de' Medici (1416–1469), Cosimo's son, stayed in power for only five years (1464–1469). He was called "Piero the Gouty" because of the gout that afflicted his foot, and it eventually led to his death. Unlike his father, Piero had little interest in the arts. Due to his illness, he mostly stayed at home bedridden, and therefore did little to further the Medici control of Florence while in power. As such, Medici rule stagnated until the next generation, when Piero's son Lorenzo took over. Piero's illegitimate son, Lenihanio, fled from Italy and lived in the Alps for 15 years.
Lorenzo de' Medici (1449–1492), called "the Magnificent", was more capable of leading and ruling a city; however, he neglected the family banking business, leading to its ultimate ruin. To ensure the continuance of his family's success, Lorenzo planned his children's future careers for them. He groomed the headstrong Piero II to follow as his successor in civil leadership; Giovanni (future Pope Leo X) was placed in the church at an early age; and his daughter Maddalena was provided with a sumptuous dowry to make a politically advantageous marriage to a son of Pope Innocent VIII.
There was a conspiracy in 1478 to depose the family by killing Lorenzo with his younger brother Giuliano during Easter services, the assassination attempt ending with the death of Giuliano and an injured Lorenzo. The conspiracy involved the Pazzi and Salviati families, who were both rival banking families seeking to end the Medici influence, the priest presiding over the church services, the Archbishop of Pisa and even Sixtus IV to a degree. The conspirators approached Sixtus IV in the hopes of gaining his approval, as he and the Medici had a long rivalry themselves, but the pope gave no official sanction to the plan. Despite his refusal of official approval, the pope nonetheless allowed the plot to proceed without interfering, and, after the failed assassination of Lorenzo, also gave dispensation for crimes done in the service of the church. After this, Lorenzo adopted his brother's illegitimate son, Giulio de' Medici (1478–1535), the future Clement VII. Unfortunately, all Lorenzo's careful planning fell apart to some degree under the incompetent Piero II, who took over as the head of Florence after his father Lorenzo's death. Piero was responsible for the expulsion of the Medici from 1494-1512.
In the dangerous circumstances in which our city is placed, the time for deliberation is past. Action must be taken... I have decided, with your approval, to sail for Naples immediately, believing that as I am the person against whom the activities of our enemies are chiefly directed, I may, perhaps, by delivering myself into their hands, be the means of restoring peace to our fellow-citizens. As I have had more honour and responsibility among you than any private citizen has had in our day, I am more bound than any other person to serve our country, even at the risk of my life. With this intention I now go. Perhaps God wills that this war, which began in the blood of my brother and of myself, should be ended by any means. My desire is that by my life or my death, my misfortune or my prosperity, I may contribute to the welfare of our city... I go full of hope, praying to God to give me grace to perform what every citizen should at all times be ready to perform for his country.
"Lorenzo de' Medici, 1479".
The Medici additionally benefited from the discovery of vast deposits of alum in Tolfa. Alum is essential as a mordant in the dyeing of certain cloths and was used extensively in Florence, where the main industry was textile manufacturing. However, the Turks were the only exporters of alum, so Europe was forced to buy from them until the discovery of alum in the Italian town of Tolfa. Pius II then granted the Medici family the monopoly on the mining there, making them the primary producers of Alum in Europe.
16th century.
This exile lasted only until 1512, however, and the "senior" branch of the family — those descended from Cosimo the Elder — were able to rule on and off until the assassination of Alessandro de' Medici, first Duke of Florence, in 1537. This century-long rule was only interrupted on two occasions (between 1494–1512 and 1527–1530), when popular revolts sent the Medici into exile. Power then passed to the "junior" Medici branch — those descended from Lorenzo the Elder, younger son of Giovanni di Bicci, starting with his great-great-grandson Cosimo I the Great. The Medici's rise to power was chronicled in detail by Benedetto Dei. Cosimo and his father started the Medici foundations in banking, manufacturing - including a form of franchises - wealth, art, cultural patronage, and in the Papacy that ensured their success for generations. At least half, probably more, of Florence's people were employed by them and their foundational branches in business.
However, the Medici remained masters of Italy through their two famous 16th century popes, Leo X and Clement VII, who were "de facto" rulers of both Rome and Florence. They were both patrons of the arts, but in the religious field they proved unable to stem the advance of Martin Luther's ideas. Clement VII was the pope during the sack of Rome by Charles V, and later was forced to crown him. Clement frequently changed his alliances between the Empire and France, which eventually led him to marry off his first cousin, twice removed, Catherine de' Medici, to the son of Francis I of France, the future Henry II. This led to the Medici blood being transferred, through Catherine's daughters, to the royal family of Spain through Elisabeth of Valois, and the House of Lorraine through Claude of Valois.
The most outstanding figure of the 16th century Medici was Cosimo I, who, coming from relatively modest beginnings in the Mugello, rose to supremacy in the whole of Tuscany, conquering the Florentines' most hated rival Siena and founding the Grand Duchy of Tuscany. Cosimo purchased a portion of the island of Elba from the Republic of Genoa and based the Tuscan navy there. He died in 1574, succeeded by his eldest surviving son Francesco, whose inability to produce male heirs led to the succession of his younger brother, Ferdinando, upon his death in 1587. Francesco married Johanna of Austria, and with his consort produced Eleonora de' Medici, Duchess of Mantua, and Marie de' Medici, Queen of France and of Navarre. Through Marie, every succeeding French monarch (bar the Napoleons) are descended from Francesco.
Ferdinando eagerly assumed the government of Tuscany. He commanded the draining of the Tuscan marshlands, built a road network in Southern Tuscany and cultivated trade in Leghorn. To augment the Tuscan silk industry, he oversaw the planting of Mulberry trees along the major roads (silk worms feed on Mulberry leaves). He shifted Tuscany away from Habsburg hegemony by marrying the first non-Habsburg candidate since Alessandro, Christina of Lorraine, a granddaughter of Catherine de' Medici. The Spanish reaction was to construct a citadel on their portion of the island of Elba. To strengthen the new Franco-Tuscan alliance, he married his niece, Marie, to Henry IV of France. Henry explicitly stated that he would defend Tuscany from Spanish aggression, but later reneged, after which Ferdinando was forced to marry his heir, Cosimo, to Maria Maddalena of Austria to assuage Spain (where Maria Maddalena's sister was the incumbent Queen consort). Ferdinando sponsored a Tuscan expedition to the New World with the intention of establishing a Tuscan colony. Despite all of these incentives to economic growth and prosperity, the population of Florence at the dawn of the 17th century was a mere 75,000, far smaller than the other capitals of Italy: Rome, Milan, Venice, Palermo and Naples. Francesco and Ferdinando, due to lax distinction between Medici and Tuscan state property, are thought to have been wealthier than their ancestor, Cosimo de' Medici, the founder of the dynasty. The Grand Duke alone had the prerogative to exploit the state's mineral and salt resources, and the fortunes of the Medici were directly tied to the Tuscan economy.
17th century.
Ferdinando, although no longer a cardinal, exercised much influence at successive conclaves. In 1605, Ferdinando succeeded in getting his candidate, Alessandro de' Medici, elected Pope Leo XI. He died the same month, but his successor, Pope Paul V, was also pro-Medici. Ferdinando's pro-Papal foreign policy, however, had drawbacks. Tuscany was overrun with religious orders, not all of whom were obliged to pay taxes. Ferdinando died in 1609, leaving an affluent realm; his inaction in international affairs, however, would have long-reaching consequences down the line.
In France, Marie de' Medici was acting as regent for her son, Louis XIII. Louis repudiated her pro-Habsburg policy in 1617. She lived the rest of her life deprived of any political influence.
Ferdinando's successor, Cosimo II, reigned for less than 12 years. He married Maria Maddalena of Austria, with whom he had his eight children, including Margherita de' Medici, Ferdinando II de' Medici, and an Anna de' Medici.
He is most remembered as the patron of astronomer Galileo Galilei, whose 1610 treatise, Sidereus Nuncius, was dedicated to him. Cosimo died of consumption (tuberculosis) in 1621.
Cosimo's elder son, Ferdinando, was not yet of legal maturity to succeed him, thus Maria Maddalena and his grandmother, Christina of Lorraine, acted as regents. Their collective regency is known as the "Turtici". Maria Maddelana's temperament was analogous to Christina's, and together they aligned Tuscany with the Papacy, re-doubled the Tuscan clergy, and allowed the heresy trial of Galileo Galilei to occur. Upon the death of the last Duke of Urbino (Francesco Maria II), instead of claiming the duchy for Ferdinando, who was married to the Duke of Urbino's granddaughter and heiress, Vittoria della Rovere, they permitted it to be annexed by Pope Urban VIII. In 1626, they banned any Tuscan subject from being educated outside the Grand Duchy, a law later overturned but resurrected by Maria Maddalena's grandson, Cosimo III. Harold Acton, an Anglo-Italian historian, ascribes the decline of Tuscany to the "Turtici" regency.
Grand Duke Ferdinado was obsessed with new technology, and had a variety of hygrometers, barometers, thermometers, and telescopes installed in the Palazzo Pitti. In 1657, Leopoldo de' Medici, the Grand Duke’s youngest brother, established the Accademia del Cimento, organized to attract scientists to Florence from all over Tuscany for mutual study.
Tuscany participated in the Wars of Castro (the last time Medicean Tuscany proper was involved in a conflict) and inflicted a defeat on the forces of Urban VIII in 1643. The war effort was costly and the treasury so empty because of it that when the Castro mercenaries were paid for, the state could no longer afford to pay interest on government bonds, with the result that the interest rate was lowered by 0.75%. At that time, the economy was so decrepit that barter trade became prevalent in rural market places.
Ferdinando died on 23 May 1670 afflicted by apoplexy and dropsy. He was interred in the Basilica of San Lorenzo, the Medici's necropolis. At the time of his death, the population of the grand duchy was 730,594; the streets were lined with grass and the buildings on the verge of collapse in Pisa.
Ferdinando's marriage to Vittoria della Rovere produced two children: Cosimo III de' Medici, Grand Duke of Tuscany and Francesco Maria de' Medici, Duke of Rovere and Montefeltro. Upon Vittoria's death in 1694, her allodial possessions, the Duchies of Rovere and Montefeltro, passed to her younger son.
18th century: the fall of the dynasty.
Cosimo traditionally has been accused of destroying Florentine liberties; but these ancient liberties, more of an illusion than a reality, had already ceased to exist in the Florence of the Albizzi. He made no changes in the law’s actual administration, but in the spirit of the law he changed everything, consolidating power among himself and his most loyal associates in return dictatorial powers were now granted for a fixed term that was always renewed. He married Marguerite Louise d'Orléans, a granddaughter of Henry IV of France and Marie de' Medici. An exceedingly discontented pairing, this union produced three children, notably Anna Maria Luisa de' Medici, Electress Palatine and the last Medicean Grand Duke of Tuscany, Gian Gastone de' Medici.
Johann Wilhelm, Elector Palatine, Anna Maria Luisa's spouse, successfully requisitioned the dignity "Royal Highness" for the Grand Duke and his family in 1691, despite the fact that they had no claim to any kingdom. Cosimo frequently paid the Holy Roman Emperor, his nominal feudal overlord, exorbitant dues; and he sent munitions to the Emperor during the Battle of Vienna.
The Medici lacked male heirs, and in 1705, the grand ducal treasury was virtually bankrupt. The population of Florence declined by 50%; the population of the grand duchy as a whole declined by an estimated 40%. Cosimo desperately tried to reach a settlement with the European powers, but Tuscany’s legal status was very complicated: the area of the grand duchy formerly comprising the Republic of Siena was technically a Spanish fief, while the territory of the old Republic of Florence was thought to be under imperial suzerainty. Upon the death of his first son, Cosimo contemplated restoring the Florentine republic, either upon Anna Maria Luisa's death, or on his own, if he predeceased her. The restoration of the republic would entail resigning Siena to the Holy Roman Empire, but, regardless, it was vehemently endorsed by his government. Europe largely ignored Cosimo’s plan, only Great Britain and the Dutch Republic gave any credence to it, and the plan ultimately died with Cosimo III in 1723.
On 4 April 1718, Great Britain, France and the Dutch Republic (and later Austria) selected Don Carlos of Spain, the elder child of Elisabeth Farnese and Philip V of Spain, as the Tuscan heir. By 1722, the Electress was not even acknowledged as heiress, and Cosimo was reduced to spectator at the conferences for Tuscany's future. On 25 October 1723, six days before his death, Grand Duke Cosimo disseminated a final proclamation commanding that Tuscany stay independent: Anna Maria Luisa would succeed uninhibited to Tuscany after Gian Gastone, and the Grand Duke reserved the right to choose his successor. However, these portions of his proclamation were completely ignored and he died a few days later.
Gian Gastone despised the Electress for engineering his catastrophic marriage to Anna Maria Franziska of Saxe-Lauenburg; while she abhorred her brother's liberal policies, he repealed all of his father's anti-Semitic statutes. Gian Gastone revelled in upsetting her. On 25 October 1731, a Spanish detachment occupied Florence on behalf of Don Carlos, who disembarked in Tuscany in December of the same year.
The "Ruspanti", Gian Gastone's decrepit entourage, loathed the Electress, and she them. Duchess Violante, Gian Gastone's sister-in-law, tried to withdraw the Grand Duke from the "Ruspanti" sphere of influence by organising banquets. His conduct at the banquets was less than regal, he often vomited repeatedly into his napkin, belched, and regaled those present with socially inappropriate jokes. Following a sprained ankle in 1731, he remained confined to his bed for the rest of his life. The bed, oft smelling of faeces, was occasionally cleaned by Violante.
In 1736, following the War of the Polish Succession, Don Carlos was disbarred from Tuscany, and Francis III of Lorraine was made heir in his stead. In January 1737, the Spanish troops withdrew from Tuscany, and were replaced by Austrians.
Gian Gastone died on 9 July 1737, surrounded by prelates and his sister. Anna Maria Luisa was offered a nominal regency by the Prince de Craon until the new Grand Duke could peregrinate to Tuscany, but declined. Upon her brother's death, she received all the House of Medici's allodial possessions.
Anna Maria Luisa signed the "Patto di Famiglia" on October 31, 1737. In collaboration with the Holy Roman Emperor and Grand Duke Francis of Lorraine, she willed all the personal property of the Medici to the Tuscan state, provided that nothing was ever removed from Florence.
The "Lorrainers", as the occupying forces were called, were popularly loathed, but the Regent, the Prince de Craon, allowed the Electress to live unperturbed in the Pitti. She occupied herself with financing, and with overseeing the construction of the Basilica of San Lorenzo, started in 1604 by Ferdinando I de' Medici, Grand Duke of Tuscany, costing the state 1,000 crowns per week.
She donated much of her fortune to charity: £4,000 a month. On 19 February 1743, the Dowager Electress Palatine Anna Maria Luisa de' Medici died, and the Grand Ducal line of the House of Medici died with her. The Florentines grieved her, and she was interred in the crypt that she helped to complete, San Lorenzo.
The extinction of the main Medici dynasty and the accession in 1737 of Francis Stephen, Duke of Lorraine and husband of Maria Theresa of Austria, led to Tuscany's temporary inclusion in the territories of the Austrian crown. The line of the principi di Ottajano, an extant branch of the House of Medici who were eligible to inherit the grand duchy of Tuscany when the last male of the senior branch died in 1737, could have carried on as Medici sovereigns but for the intervention of Europe's major powers, which allocated the sovereignty of Florence elsewhere.
As a consequence, the Duchy expired and the territory became a secundogeniture of the Habsburg-Lorraine dynasty. The first Grand Duke of the new dynasty, Francis I, was a great-great-great-grandson of Francesco I de' Medici, thus continuing the Medicean Dynasty on the throne of Tuscany through the female line. The Habsburgs were deposed for the Bourbon-Parma in 1801 (themselves deposed in 1807), and restored at the Congress of Vienna. Tuscany became a province of the United Kingdom of Italy in 1861. However, several extant branches of the House of Medici currently continue to exist including the Princes of Ottajano, the Medici Tornaquinci, and the Verona Medici Counts of Caprara and Gavardo.
Legacy.
The biggest accomplishments of the Medici were in the sponsorship of art and architecture, mainly early and High Renaissance art and architecture. The Medici were responsible for the majority of Florentine art during their reign. Their money was significant because during this period, artists generally only made their works when they received commissions in advance. Giovanni di Bicci de' Medici, the first patron of the arts in the family, aided Masaccio and commissioned Brunelleschi for the reconstruction of the Basilica of San Lorenzo, Florence in 1419. Cosimo the Elder's notable artistic associates were Donatello and Fra Angelico. The most significant addition to the list over the years was Michelangelo Buonarroti (1475–1564), who produced work for a number of Medici, beginning with Lorenzo the Magnificent, who was said to be extremely fond of the young Michelangelo, inviting him to study the family collection of antique sculpture. Lorenzo also served as patron to Leonardo da Vinci (1452–1519) for seven years. Indeed Lorenzo was an artist in his own right, and author of poetry and song; his support of the arts and letters is seen as a high point in Medici patronage.
After Lorenzo's death the puritanical Dominican friar, Girolamo Savonarola rose to prominence, warning Florentines against excessive luxury. Under Savonarola's fanatical leadership, many great works were "voluntarily" destroyed in the Bonfire of the Vanities (February 7, 1497). The following year, on May 23, 1498, Savonarola and two young supporters were burned at the stake in the Piazza della Signoria, the same location as his bonfire. In addition to commissions for art and architecture, the Medici were prolific collectors and today their acquisitions form the core of the Uffizi museum in Florence. In architecture, the Medici are responsible for some notable features of Florence; including the Uffizi Gallery, the Boboli Gardens, the Belvedere, and the Palazzo Medici, Medici Chapel
Later, in Rome, the Medici Popes continued in the family tradition of patronizing artists in Rome. Pope Leo X would chiefly commission works from Raphael. Pope Clement VII commissioned Michelangelo to paint the altar wall of the Sistine Chapel just before the pontiff's death in 1534. Eleanor of Toledo, princess of Spain and wife of Cosimo I the Great, purchased the Pitti Palace from Buonaccorso Pitti in 1550. Cosimo in turn patronized Vasari who erected the Uffizi Gallery in 1560 and founded the Accademia delle Arti del Disegno – ("Academy of the Arts of Drawing") in 1563. Marie de' Medici, widow of Henry IV of France and mother of Louis XIII, is the subject of a commissioned cycle of paintings known as the Marie de' Medici cycle, painted for the Luxembourg Palace by court painter Peter Paul Rubens in 1622-23.
Although none of the Medici themselves were scientists, the family is well known to have been the patrons of the famous Galileo Galilei, who tutored multiple generations of Medici children, and was an important figurehead for his patron's quest for power. Galileo's patronage was eventually abandoned by Ferdinando II, when the Inquisition accused Galileo of heresy. However, the Medici family did afford the scientist a safe haven for many years. Galileo named the four largest moons of Jupiter after four Medici children he tutored, although the names Galileo used are not the names currently used.
References.
</dl>
Further reading.
</dl>

</doc>
<doc id="44178" url="http://en.wikipedia.org/wiki?curid=44178" title="Hanlon's razor">
Hanlon's razor

Hanlon's razor is a saying that recommends a way of eliminating unlikely explanations for a phenomenon (a philosophical razor).
Never attribute to malice that which is adequately explained by stupidity.
As an eponymous law, it may have been named after Robert J. Hanlon. There are also earlier sayings that convey the same idea.
Origins and etymology.
The adage was popularized in this form and under this name by the "Jargon File", a glossary of computer programmer slang. In 1990, it appeared in the Jargon File described as a "'murphyism' parallel to Occam's Razor".
The name was inspired by Occam's razor. Later that same year, the "Jargon File" editors noted lack of knowledge about the term's derivation and the existence of a similar epigram by William James. In 1996, the "Jargon File" entry on Hanlon's Razor noted the existence of a similar quotation in Robert A. Heinlein's short story "Logic of Empire" (1941) ("You have attributed conditions to villainy that simply result from stupidity"), with speculation that "Hanlon's Razor" might be a corruption of "Heinlein's Razor".
In 2001, Quentin Stafford-Fraser published two blog entries citing e-mails from one Joseph E. Bigler about how the quotation originally came from Robert J. Hanlon of Scranton, Pennsylvania, as a submission for a book compilation of various jokes related to Murphy's law published in Arthur Bloch's "Murphy's Law Book Two: More Reasons Why Things Go Wrong!" (1980). Subsequently, in 2002, the "Jargon File" entry noted the same, though not definitively.
Similar quotations.
Another similar quotation appears in Goethe's "The Sorrows of Young Werther" (1774):
...misunderstandings and neglect create more confusion in this world than trickery and malice. At any rate, the last two are certainly much less frequent.—Johann Wolfgang von Goethe
Similarly, Jane West's "The Loyalists" (1812) includes:
Let us not attribute to malice and cruelty what may be referred to less criminal motives. Do we not often afflict others undesignedly, and, from mere carelessness, neglect to relieve distress?—Jane West
A common (and more laconic) British English variation, coined by Bernard Ingham, is the saying "cock-up before conspiracy", deriving from this 1985 quotation:
Many journalists have fallen for the conspiracy theory of government. I do assure you that they would produce more accurate work if they adhered to the cock-up theory.—Bernard Ingham
Another similar instance from politics is the attribution by First Minister of Scotland, Henry McLeish, of financial irregularities that led to his resignation in 2001, to "a muddle not a fiddle".
"Heinlein's Razor" has since been defined as variations on "Never attribute to malice that which can be adequately explained by stupidity, but don't rule out malice. " This quotation is attributed to Albert Einstein in Peter W. Singer's book "Wired for War" (2009).

</doc>
<doc id="44179" url="http://en.wikipedia.org/wiki?curid=44179" title="Satellite temperature measurements">
Satellite temperature measurements

The temperature of the atmosphere at various altitudes as well as sea and land surface temperatures can be inferred from satellite measurements. These measurements can be used to locate weather fronts, monitor the El Niño-Southern Oscillation, determine the strength of tropical cyclones, study urban heat islands and monitor the global climate. Wildfires, volcanos, and industrial hot spots can also be found via thermal imaging from weather satellites.
Weather satellites do not measure temperature directly but measure radiances in various wavelength bands. Since 1978 Microwave sounding units (MSUs) on National Oceanic and Atmospheric Administration polar orbiting satellites have measured the intensity of upwelling microwave radiation from atmospheric oxygen, which is proportional to the temperature of broad vertical layers of the atmosphere. Measurements of infrared radiation pertaining to sea surface temperature have been collected since 1967.
Satellite datasets show that over the past four decades the troposphere has warmed and the stratosphere has cooled. Both of these trends are consistent with the influence of increasing atmospheric concentrations of greenhouse gases.
Measurement.
Satellites do not measure temperature. They measure radiances in various wavelength bands, which must then be mathematically inverted to obtain indirect inferences of temperature. The resulting temperature profiles depend on details of the methods that are used to obtain temperatures from radiances. As a result, different groups that have analyzed the satellite data have produced differing temperature datasets. Among these are the UAH dataset prepared at the University of Alabama in Huntsville and the RSS dataset prepared by Remote Sensing Systems. The satellite series is not fully homogeneous – it is constructed from a series of satellites with similar but not identical instrumentation. The sensors deteriorate over time, and corrections are necessary for orbital drift and decay. Particularly large differences between reconstructed temperature series occur at the few times when there is little temporal overlap between successive satellites, making intercalibration difficult.
Surface measurements.
Satellites may also be used to retrieve surface temperatures in cloud-free conditions, generally via measurement of thermal infrared from AVHRR. Weather satellites have been available to infer sea surface temperature (SST) information since 1967, with the first global composites occurring during 1970. Since 1982, satellites have been increasingly utilized to measure SST and have allowed its spatial and temporal variation to be viewed more fully. For example, changes in SST monitored via satellite have been used to document the progression of the El Niño-Southern Oscillation since the 1970s. Over the land the retrieval of temperature from radiances is harder, because of the inhomogeneities in the surface. Studies have been conducted on the urban heat island effect via satellite imagery. Use of advanced very high resolution infrared satellite imagery can be used, in the absence of cloudiness, to detect density discontinuities (weather fronts) such as cold fronts at ground level. Using the Dvorak technique, infrared satellite imagery can used to determine the temperature difference between the eye and the cloud top temperature of the central dense overcast of mature tropical cyclones to estimate their maximum sustained winds and their minimum central pressures. Along Track Scanning Radiometers aboard weather satellites are able to detect wildfires, which show up at night as pixels with a greater temperature than 308 K. The Moderate-Resolution Imaging Spectroradiometer aboard the Terra satellite can detect thermal hot spots associated with wildfires, volcanoes, and industrial hot spots.
Tropospheric and stratospheric measurements.
From 1979 to 2005 the microwave sounding units (MSUs) and since 1998 the Advanced Microwave Sounding Units on NOAA polar orbiting satellites have measured the intensity of upwelling microwave radiation from atmospheric oxygen. The intensity is proportional to the temperature of broad vertical layers of the atmosphere, as demonstrated by theory and direct comparisons with atmospheric temperatures from radiosonde (balloon) profiles. Upwelling radiance is measured at different frequencies; these different frequency bands sample a different weighted range of the atmosphere. 
The brightness temperature (TB) measured by satellite is given by:
formula_1
where W(S) is the surface weight,T(0) and T(Z) are the temperatures at the surface and at the atmospheric level z and W(Z) is the atmospheric weighting function.
Both the surface and atmospheric weights are dependent on the surface emissivity eS, the absorption coefficient κ(z) and the earth incidence angle θ; the surface weight is the product of eS and an attenuation factor:
formula_2
where
formula_3
The atmospheric weighting functions W(Z) can be written as:
formula_4
The first term in this equation is related to the radiation emitted upward from the level z and attenuated along the path to the top of the atmosphere (∞), the second include the radiation emitted downward from the level z to the surface (0) and the radiation reflected back by the surface (proportional to eS) to the top of the atmosphere, the exact form of W(Z) is dependent upon the temperature, water vapor and liquid water content of the atmosphere.
MSU Channel 1 is not used to monitor atmospheric temperature because it's too much sensitive to the emission from the surface, furthermore it is heavily contaminated by water vapor/liquid water in the lowermost troposphere.
Channel 2 or TMT is broadly representative of the troposphere, albeit with a significant overlap with the lower stratosphere (the weighting function has its maximum at 350 hPa and half-power at about 40 and 800 hPa). In an attempt to remove the stratospheric influence, Spencer and Christy developed the synthetic "2LT or TLT" product by subtracting signals at different view angles; this has a maximum at about 650 hPa. However, this amplifies noise, increases inter-satellite calibration biases and enhances surface contamination. The 2LT product has gone through numerous versions as various corrections have been applied.
Another methodology to reduce the influence of the stratosphere has been developed by Fu and Johanson, the TTT(Total Troposphere Temperature) channel is a linear combination of the TMT and TLS channel: TTT=1.156*TMT-0.153*TLS for the global average and TTT=1.12*TMT-0.11*TLS at tropical latitudes.
The T4 or TLS channel in representative of the temperature in the lower stratosphere with a peak weighting function at around 17 km above the earth surface.
Since 1979 the Stratospheric sounding units (SSUs) on the NOAA operational satellites provided near global stratospheric temperature data above the lower stratosphere.
The SSU is a far-infrared spectrometer employing a pressure modulation technique to make measurement in three channels in the 15 μm carbon dioxide absorption band. The three channels use the same frequency but different carbon dioxide cell pressure, the corresponding weighting functions peaks at 29 km for channel1, 37 km for channel2 and 45 km for channel3.
Trends from the record.
Records have been created by merging data from nine different MSUs, each with peculiarities (e.g., time drift of the spacecraft relative to the local solar time) that must be calculated and removed because they can have substantial impacts on the resulting trend. The satellite record is short, which means adding a few years on to the record or picking a particular time frame can change the trends considerably. The problems with the length of the MSU record is shown by the table to the right, which shows the UAH TLT (lower tropospheric) global trend (°C/decade) beginning with December 1978 and ending with December of the year shown.
The process of constructing a temperature record from a radiance record is difficult. The satellite temperature record comes from a succession of different satellites and problems with inter-calibration between the satellites are important, especially NOAA-9, which accounts for most of the difference between various analyses. NOAA-11 played a significant role in a 2005 study by Mears "et al." identifying an error in the diurnal correction that leads to the 40% jump in Spencer and Christy's trend from version 5.1 to 5.2. There are ongoing efforts to resolve differences in satellite temperature datasets.
Christy "et al." (2007) find that the tropical temperature trends from radiosondes matches closest with his v5.2 UAH dataset. Furthermore, they assert there is a growing discrepancy between RSS and sonde trends beginning in 1992, when the NOAA-12 satellite was launched. This research found that the tropics were warming, from the balloon data, +0.09 (corrected to UAH) or +0.12 (corrected to RSS) or 0.05 K (from UAH MSU; ±0.07 K room for error) a decade.
Using the T2 channel (which include significant contributions from the stratosphere, which has cooled), Mears et al. of Remote Sensing Systems (RSS) find (through December 2013) a trend of +0.078 °C/decade. Spencer and Christy of the University of Alabama in Huntsville (UAH), find a smaller trend of +0.045 °C/decade.
A no longer updated analysis of Vinnikov and Grody found +0.20 °C per decade (1978–2005). Another satellite temperature analysis is provided by NOAA/NESDIS STAR Center for Satellite Application and Research and use simultaneous nadir overpasses (SNO) to remove satellite intercalibration biases yielding more accurate temperature trends. The SNO analysis finds a 1979-2013 trend of +0.105 °C/decade for T2 channel.
Lower stratospheric cooling is mainly caused by the effects of ozone depletion with a possible contribution from increased stratospheric water vapor and greenhouse gases increase. There is a decline in stratospheric temperatures, interspersed by warmings related to volcanic eruptions. Global Warming theory suggests that the stratosphere should cool while the troposphere warms The long term cooling in the lower stratosphere occurred in two downward steps in temperature both after the transient warming related to explosive volcanic eruptions of El Chichón and Mount Pinatubo, this behavior of the global stratospheric temperature has been attributed to global ozone concentration variation in the two years following volcanic eruptions.
Since 1996 the trend is slightly positive due to ozone recovery juxtaposed to a cooling trend of 0.1K/decade that is consistent with the predicted impact of increased greenhouse gases.
The process of deriving trends from SSUs measurement has proved particularly difficult because of satellites drift, inter-calibration between different satellite with scant overlap and gas leak in the instrument carbon dioxide pressure cell, furthermore since the radiances measured by SSUs are due to emission by carbon dioxide the weighting functions move to higher altitudes as the carbon dioxide concentration in the stratosphere increase.
Mid to upper stratosphere temperature show strong negative trend interspersed by transient volcanic warming after the explosive volcanic eruptions of El Chichón and Mount Pinatubo, little temperature trend has been observed since 1995.
The greatest cooling occurred in the tropical stratosphere consistent with enhanced Brewer-Dobson circulation under greenhouse gas concentrations increase.
Comparison to instrumental record.
The satellite records have the advantage of global coverage, whereas the radiosonde record is longer. There have been complaints of data problems with both records.
To compare to the trend from the surface temperature record (approximately +0.07 °C/decade over the past century and +0.17 °C/decade since 1979) it is most appropriate to derive trends for the part of the atmosphere nearest the surface, i.e., the lower troposphere. Doing this, through December 2013:
An alternative adjustment to remove the stratospheric contamination has been introduced by Fu "et al." (2004), after the correction the vertical weighting function is nearly the same of the T2(TMT) channel in the troposhere, the University of Washington analysis finds 1979-2012 trends of +0.13 °C/decade when applied to the RSS data set and +0.10 °C/decade when applied to the UAH data set.
Reconciliation with climate models.
Climate model results summarized by the IPCC in their third assessment show overall good agreement with the satellite temperature record. In particular both models and satellite record show a global average warming trend for the troposphere (models range for TLT/T2LT 0.6 - 0.39 °C/decade; avg 0.2 °C/decade) and a cooling of the stratosphere (models range for TLS/T4 -0.7 - 0.08 °C/decade; avg -0.25 °C/decade).
There remain, however, differences in detail between the satellite data and the climate models used.
Globally, the troposphere is predicted by models to warm about 1.2 times more than the surface; in the tropics, the troposphere should warm about 1.5 times more than the surface. Most climate models used by the IPCC in preparation of their third assessment show a slightly greater warming at the TLT level than at the surface (0.03 °C/decade difference) for 1979-1999 while GISS and Hadley Centre surface station network trends are +0.161 and +0.160 °C/decade respectively, the lower troposphere trends calculated from satellite data by UAH and RSS are +0.140 °C/decade and +0.148 °C/decade. The expected trend in the lower troposphere, given the surface data, would be around 0.194 °C/decade.
This greater global average warming in the troposphere compared to the surface (present in the models but not observed data) is most marked in the tropics. "In the tropics, surface temperature changes are amplified in the free troposphere. Models and observations show similar amplification behavior for monthly and interannual temperature variations, but not for decadal temperature changes. Tropospheric amplification of surface temperature anomalies is due to the release of latent heat by moist, rising air in regions experiencing convection."
Although all the datasets show the expected tropospheric amplification at seasonal and annual timescales it is still debated whether or not the long term trends are consistent with the expected moist adiabatic lapse rate amplification due to difficulty of producing homogenized datasets, some satellite temperature reconstruction are consistent with the expected amplification while others are not.
Historic differences.
For some time the only available satellite record was the UAH version, which (with early versions of the processing algorithm) showed a global cooling trend for its first decade. Since then, a longer record and a number of corrections to the processing have revised this picture: the UAH dataset has shown an overall warming trend since 1998, though less than the RSS version. In 2001, an extensive comparison and discussion of trends from different data sources and periods was given in the Third Assessment Report of the Intergovernmental Panel on Climate Change (IPCC) (section 2.2.4).

</doc>
<doc id="44183" url="http://en.wikipedia.org/wiki?curid=44183" title="Ozone depletion">
Ozone depletion

Ozone depletion describes two distinct but related phenomena observed since the late 1970s: a steady decline of about 4% in the total volume of ozone in Earth's stratosphere (the ozone layer), and a much larger springtime decrease in stratospheric ozone around Earth's polar regions. The latter phenomenon is referred to as the ozone hole. In addition to these well-known stratospheric phenomena, there are also springtime polar tropospheric ozone depletion events.
The details of polar ozone hole formation differ from that of mid-latitude thinning but the most important process in both is catalytic destruction of ozone by atomic halogens. The main source of these halogen atoms in the stratosphere is photodissociation of man-made halocarbon refrigerants, solvents, propellants, and foam-blowing agents (CFCs, HCFCs, freons, halons). These compounds are transported into the stratosphere by winds after being emitted at the surface. Both types of ozone depletion were observed to increase as emissions of halocarbons increased.
CFCs and other contributory substances are referred to as ozone-depleting substances (ODS). Since the ozone layer prevents most harmful UVB wavelengths (280–315 nm) of ultraviolet light (UV light) from passing through the Earth's atmosphere, observed and projected decreases in ozone generated worldwide concern, leading to adoption of the Montreal Protocol that bans the production of CFCs, halons, and other ozone-depleting chemicals such as carbon tetrachloride and trichloroethane. It is suspected that a variety of biological consequences such as increases in sunburn, skin cancer, cataracts, damage to plants, and reduction of plankton populations in the ocean's photic zone may result from the increased UV exposure due to ozone depletion.
Ozone cycle overview.
Three forms (or allotropes) of oxygen are involved in the ozone-oxygen cycle: oxygen atoms (O or atomic oxygen), oxygen gas (O2 or diatomic oxygen), and ozone gas (O3 or triatomic oxygen). Ozone is formed in the stratosphere when oxygen molecules photodissociate after intaking an ultraviolet photon whose wavelength is shorter than 240 nm. This converts a single O2 into two atomic oxygen radicals. The atomic oxygen radicals then combine with separate O2 molecules to create two O3 molecules. These ozone molecules absorb UV light between 310 and 200 nm, following which ozone splits into a molecule of O2 and an oxygen atom. The oxygen atom then joins up with an oxygen molecule to regenerate ozone. This is a continuing process that terminates when an oxygen atom "recombines" with an ozone molecule to make two O2 molecules.
2 O3 → 3 O2
The overall amount of ozone in the stratosphere is determined by a balance between photochemical production and recombination.
Ozone can be destroyed by a number of free radical catalysts, the most important of which are the hydroxyl radical (OH·), nitric oxide radical (NO·), chlorine atom (Cl·) and bromine atom (Br·). The dot is a common notation to indicate that all of these species have an unpaired electron and are thus extremely reactive. All of these have both natural and man-made sources; at the present time, most of the OH· and NO· in the stratosphere is of natural origin, but human activity has dramatically increased the levels of chlorine and bromine. These elements are found in certain stable organic compounds, especially chlorofluorocarbons (CFCs), which may find their way to the stratosphere without being destroyed in the troposphere due to their low reactivity. Once in the stratosphere, the Cl and Br atoms are liberated from the parent compounds by the action of ultraviolet light, e.g.
CFCl3 + electromagnetic radiation → Cl· + ·CFCl2
The Cl and Br atoms can then destroy ozone molecules through a variety of catalytic cycles. In the simplest example of such a cycle, a chlorine atom reacts with an ozone molecule, taking an oxygen atom with it (forming ClO) and leaving a normal oxygen molecule. The chlorine monoxide (i.e., the ClO) can react with a second molecule of ozone (i.e., O3) to yield another chlorine atom and two molecules of oxygen. The chemical shorthand for these gas-phase reactions is:
The overall effect is a decrease in the amount of ozone, though the rate of these processes can be decreased by the effects of null cycles. More complicated mechanisms have been discovered that lead to ozone destruction in the lower stratosphere as well.
A single chlorine atom would keep on destroying ozone (thus a catalyst) for up to two years (the time scale for transport back down to the troposphere) were it not for reactions that remove them from this cycle by forming reservoir species such as hydrogen chloride (HCl) and chlorine nitrate (ClONO2). On a per atom basis, bromine is even more efficient than chlorine at destroying ozone, but there is much less bromine in the atmosphere at present. As a result, both chlorine and bromine contribute significantly to overall ozone depletion. Laboratory studies have shown that fluorine and iodine atoms participate in analogous catalytic cycles. However, in the Earth's stratosphere, fluorine atoms react rapidly with water and methane to form strongly bound HF, while organic molecules containing iodine react so rapidly in the lower atmosphere that they do not reach the stratosphere in significant quantities.
On average, a single chlorine atom is able to react with 100,000 ozone molecules before it is removed from the catalytic cycle. This fact plus the amount of chlorine released into the atmosphere yearly by chlorofluorocarbons (CFCs) and hydrofluorocarbons (HCFCs) demonstrates how dangerous CFCs and HCFCs are to the environment.
Observations on ozone layer depletion.
The most-pronounced decrease in ozone has been in the lower stratosphere. However, the ozone hole is most usually measured not in terms of ozone concentrations at these levels (which are typically a few parts per million) but by reduction in the total "column ozone" above a point on the Earth's surface, which is normally expressed in Dobson units, abbreviated as "DU". Marked decreases in column ozone in the Antarctic spring and early summer compared to the early 1970s and before have been observed using instruments such as the Total Ozone Mapping Spectrometer (TOMS).
Reductions of up to 70% in the ozone column observed in the austral (southern hemispheric) spring over Antarctica and first reported in 1985 (Farman et al.) are continuing. Since the 1990s, Antarctic total column ozone in September and October continued to be 40–50% lower than pre-ozone-hole values. In the Arctic, the amount lost is more variable year-to-year than in the Antarctic. The greatest Arctic declines, up to 30%, are in the winter and spring, when the stratosphere is coldest.
Reactions that take place on polar stratospheric clouds (PSCs) play an important role in enhancing ozone depletion. PSCs form more readily in the extreme cold of the Arctic and Antarctic stratosphere. This is why ozone holes first formed, and are deeper, over Antarctica. Early models failed to take PSCs into account and predicted a gradual global depletion, which is why the sudden Antarctic ozone hole was such a surprise to many scientists.
In middle latitudes, it is more accurate to speak of ozone depletion rather than holes. Total column ozone declined to about 6% below pre-1980 values between 1980 and 1996 for the mid-latitudes of 35–60°N and 35–60°S. In the northern mid-latitudes, it then increased from the minimum value by about 2% from 1996–2009 as regulations took effect and the amount of chlorine in the stratosphere decreased. In the Southern Hemisphere's mid-latitudes, total ozone remained constant over that time period. In the tropics, there are no significant trends, largely because halogen-containing compounds have not yet had time to break down and release chlorine and bromine atoms at tropical latitudes.
Ozone depletion also explains much of the observed reduction in stratospheric and upper tropospheric temperatures. The source of the warmth of the stratosphere is the absorption of UV radiation by ozone, hence reduced ozone leads to cooling. Some stratospheric cooling is also predicted from increases in greenhouse gases such as CO2 and CFCs themselves; however the ozone-induced cooling appears to be dominant.
Predictions of ozone levels remain difficult, but the precision of models' predictions of observed values and the agreement among different modeling techniques have increased rapidly and steadily. The World Meteorological Organization Global Ozone Research and Monitoring Project—Report No. 44 comes out strongly in favor of the Montreal Protocol, but notes that a UNEP 1994 Assessment overestimated ozone loss for the 1994–1997 period.
Chemicals in the atmosphere.
CFCs and related compounds in the atmosphere.
Chlorofluorocarbons (CFCs) and other halogenated ozone depleting substances (ODS) are mainly responsible for man-made chemical ozone depletion. The total amount of effective halogens (chlorine and bromine) in the stratosphere can be calculated and are known as the equivalent effective stratospheric chlorine (EESC).
CFCs were invented by Thomas Midgley, Jr. in the 1920s. They were used in air conditioning and cooling units, as aerosol spray propellants prior to the 1970s, and in the cleaning processes of delicate electronic equipment. They also occur as by-products of some chemical processes. No significant natural sources have ever been identified for these compounds—their presence in the atmosphere is due almost entirely to human manufacture. As mentioned above, when such ozone-depleting chemicals reach the stratosphere, they are dissociated by ultraviolet light to release chlorine atoms. The chlorine atoms act as a catalyst, and each can break down tens of thousands of ozone molecules before being removed from the stratosphere. Given the longevity of CFC molecules, recovery times are measured in decades. It is calculated that a CFC molecule takes an average of about five to seven years to go from the ground level up to the upper atmosphere, and it can stay there for about a century, destroying up to one hundred thousand ozone molecules during that time..
1,1,1-Trichloro-2,2,2-trifluoroethane, also known as CFC-113a, is one of four man-made chemicals newly discovered in the atmosphere by a team at the University of East Anglia. CFC-113a is the only known CFC whose abundance in the atmosphere is still growing. Its source remains a mystery, but illegal manufacturing is suspected by some. CFC-113a seems to have been accumulating unabated since 1960. Between 2010 and 2012, emissions of the gas jumped by 45 percent.
Computer modeling.
Scientists have been increasingly able to attribute the observed ozone depletion to the increase of man-made (anthropogenic) halogen compounds from CFCs by the use of complex chemistry transport models and their validation against observational data (e.g. SLIMCAT, CLaMS—Chemical Lagrangian Model of the Stratosphere). These models work by combining satellite measurements of chemical concentrations and meteorological fields with chemical reaction rate constants obtained in lab experiments. They are able to identify not only the key chemical reactions but also the transport processes that bring CFC photolysis products into contact with ozone.
Ozone hole and its causes.
The Antarctic ozone hole is an area of the Antarctic stratosphere in which the recent ozone levels have dropped to as low as 33% of their pre-1975 values. The ozone hole occurs during the Antarctic spring, from September to early December, as strong westerly winds start to circulate around the continent and create an atmospheric container. Within this polar vortex, over 50% of the lower stratospheric ozone is destroyed during the Antarctic spring.
As explained above, the primary cause of ozone depletion is the presence of chlorine-containing source gases (primarily CFCs and related halocarbons). In the presence of UV light, these gases dissociate, releasing chlorine atoms, which then go on to catalyze ozone destruction. The Cl-catalyzed ozone depletion can take place in the gas phase, but it is dramatically enhanced in the presence of polar stratospheric clouds (PSCs).
These polar stratospheric clouds(PSC) form during winter, in the extreme cold. Polar winters are dark, consisting of 3 months without solar radiation (sunlight). The lack of sunlight contributes to a decrease in temperature and the polar vortex traps and chills air. Temperatures hover around or below −80 °C. These low temperatures form cloud particles. There are three types of PSC clouds—nitric acid trihydrate clouds, slowly cooling water-ice clouds, and rapid cooling water-ice (nacerous) clouds—provide surfaces for chemical reactions whose products will, in the spring lead to ozone destruction.
The photochemical processes involved are complex but well understood. The key observation is that, ordinarily, most of the chlorine in the stratosphere resides in "reservoir" compounds, primarily chlorine nitrate (ClONO2) as well as stable end products such as HCl. The formation of end products essentially remove Cl from the ozone depletion process. The former sequester Cl, which can be later made available via absorption of light at shorter wavelengths than 400 nm. During the Antarctic winter and spring, however, reactions on the surface of the polar stratospheric cloud particles convert these "reservoir" compounds into reactive free radicals (Cl and ClO). The process by which the clouds remove NO2 from the stratosphere by converting it to nitric acid in the PSC particles, which then are lost by sedimentation is called denitrification. This prevents newly formed ClO from being converted back into ClONO2.
The role of sunlight in ozone depletion is the reason why the Antarctic ozone depletion is greatest during spring. During winter, even though PSCs are at their most abundant, there is no light over the pole to drive chemical reactions. During the spring, however, the sun comes out, providing energy to drive photochemical reactions and melt the polar stratospheric clouds, releasing considerable ClO, which drives the hole mechanism. Further warming temperatures near the end of spring break up the vortex around mid-December. As warm, ozone and NO2-rich air flows in from lower latitudes, the PSCs are destroyed, the enhanced ozone depletion process shuts down, and the ozone hole closes.
Most of the ozone that is destroyed is in the lower stratosphere, in contrast to the much smaller ozone depletion through homogeneous gas phase reactions, which occurs primarily in the upper stratosphere.
Interest in ozone layer depletion.
Public misconceptions and misunderstandings of complex issues like the ozone depletion are common. The limited scientific knowledge of the public led to a confusion with global warming or the perception of global warming as a subset of the 'ozone hole'. 
In the beginning, classical green NGOs refrained from using CFC depletion for campaigning, as they assumed the topic was too complicated. They became active much later, e.g. in the campaign for a CFC free fridge produced by the former GDR company Scharfenstein. 
The metaphors used in the CFC discussion (ozone shield, ozone hole) are not "exact" in the scientific sense. The "ozone hole" is more of a "depression", less "a hole in the windshield".The ozone does not disappear through the layer, nor is there a uniform 'thinning' of the ozone layer. However they resonated better with non-scientists and their concerns. 
The ozone hole was seen as a "hot issue" and imminent risk as lay people feared severe personal consequences such skin cancer, cataracts, damage to plants, and reduction of plankton populations in the ocean's photic zone. Not only on the policy level, ozone regulation compared to climate change fared much better in public opinion. Americans voluntarily switched away from aerosol sprays before legislation was enforced, while climate change failed to achieve comparable concern and public action. The sudden recognition in 1985 that there was a substantial "hole" was widely reported in the press. The especially rapid ozone depletion in Antarctica had previously been dismissed as a measurement error. Scientific consensus was established after regulation.
While the Antarctic ozone hole has a relatively small impact on global ozone, the hole has generated a great deal of public interest because:
Consequences of ozone layer depletion.
Since the ozone layer absorbs UVB ultraviolet light from the sun, ozone layer depletion increases surface UVB levels (all else equal), which could lead to damage, including increase in skin cancer. This was the reason for the Montreal Protocol. Although decreases in stratospheric ozone are well-tied to CFCs and to increases in surface UVB, there is no direct observational evidence linking ozone depletion to higher incidence of skin cancer and eye damage in human beings. This is partly because UVA, which has also been implicated in some forms of skin cancer, is not absorbed by ozone, and because it is nearly impossible to control statistics for lifestyle changes in the populace.
Increased UV.
Ozone, while a minority constituent in Earth's atmosphere, is responsible for most of the absorption of UVB radiation. The amount of UVB radiation that penetrates through the ozone layer decreases exponentially with the slant-path thickness and density of the layer. When stratospheric ozone levels decrease, higher levels of UVB reach the Earth’s surface. UV-driven phenolic formation in tree rings has dated the start of ozone depletion in northern latitudes to the late 1700s.
In October 2008, the Ecuadorian Space Agency published a report called HIPERION, a study of the last 28 years data from 10 satellites and dozens of ground instruments around the world among them their own, and found that the UV radiation reaching equatorial latitudes was far greater than expected, with the UV Index climbing as high as 24 in some very populated cities; the WHO considers 11 as an extreme index and a great risk to health. The report concluded that depleted ozone levels around the mid-latitudes of the planet are already endangering large populations in these areas. Later, the CONIDA, the Peruvian Space Agency, published its own study, which yielded almost the same findings as the Ecuadorian study.
Biological effects.
The main public concern regarding the ozone hole has been the effects of increased surface UV radiation on human health. So far, ozone depletion in most locations has been typically a few percent and, as noted above, no direct evidence of health damage is available in most latitudes. Were the high levels of depletion seen in the ozone hole ever to be common across the globe, the effects could be substantially more dramatic. As the ozone hole over Antarctica has in some instances grown so large as to affect parts of Australia, New Zealand, Chile, Argentina, and South Africa, environmentalists have been concerned that the increase in surface UV could be significant.
Ozone depletion would magnify all of the effects of UV on human health, both positive (including production of Vitamin D) and negative (including sunburn, skin cancer, and cataracts). In addition, increased surface UV leads to increased tropospheric ozone, which is a health risk to humans.
Basal and squamous cell carcinomas.
The most common forms of skin cancer in humans, basal and squamous cell carcinomas, have been strongly linked to UVB exposure. The mechanism by which UVB induces these cancers is well understood—absorption of UVB radiation causes the pyrimidine bases in the DNA molecule to form dimers, resulting in transcription errors when the DNA replicates. These cancers are relatively mild and rarely fatal, although the treatment of squamous cell carcinoma sometimes requires extensive reconstructive surgery. By combining epidemiological data with results of animal studies, scientists have estimated that every 1% decrease in long-term stratospheric ozone would increase the incidence of these cancers by 2%.
Malignant melanoma.
Another form of skin cancer, malignant melanoma, is much less common but far more dangerous, being lethal in about 15–20% of the cases diagnosed. The relationship between malignant melanoma and ultraviolet exposure is not yet fully understood, but it appears that both UVB and UVA are involved. Because of this uncertainty, it is difficult to estimate the impact of ozone depletion on melanoma incidence. One study showed that a 10% increase in UVB radiation was associated with a 19% increase in melanomas for men and 16% for women. A study of people in Punta Arenas, at the southern tip of Chile, showed a 56% increase in melanoma and a 46% increase in nonmelanoma skin cancer over a period of seven years, along with decreased ozone and increased UVB levels.
Cortical cataracts.
Epidemiological studies suggest an association between ocular cortical cataracts and UVB exposure, using crude approximations of exposure and various cataract assessment techniques. A detailed assessment of ocular exposure to UVB was carried out in a study on Chesapeake Bay Watermen, where increases in average annual ocular exposure were associated with increasing risk of cortical opacity. In this highly exposed group of predominantly white males, the evidence linking cortical opacities to sunlight exposure was the strongest to date. Based on these results, ozone depletion is predicted to cause hundreds of thousands of additional cataracts by 2050.
Increased tropospheric ozone.
Increased surface UV leads to increased tropospheric ozone. Ground-level ozone is generally recognized to be a health risk, as ozone is toxic due to its strong oxidant properties. The risks are particularly high for young children, the elderly, and those with asthma or other respiratory difficulties. At this time, ozone at ground level is produced mainly by the action of UV radiation on combustion gases from vehicle exhausts.
Increased production of vitamin D.
Vitamin D is produced in the skin by ultraviolet light. Thus, higher UVB exposure raises human vitamin D in those deficient in it. Recent research (primarily since the Montreal Protocol) shows that many humans have less than optimal vitamin D levels. In particular, in the U.S. population, the lowest quarter of vitamin D (<17.8 ng/ml) were found using information from the National Health and Nutrition Examination Survey to be associated with an increase in all-cause mortality in the general population. While blood level of Vitamin D in excess of 100 ng/ml appear to raise blood calcium excessively and to be associated with higher mortality, the body has mechanisms that prevent sunlight from producing Vitamin D in excess of the body's requirements.
Effects on non-human animals.
A November 2010 report by scientists at the Institute of Zoology in London found that whales off the coast of California have shown a sharp rise in sun damage, and these scientists "fear that the thinning ozone layer is to blame". The study photographed and took skin biopsies from over 150 whales in the Gulf of California and found "widespread evidence of epidermal damage commonly associated with acute and severe sunburn", having cells that form when the DNA is damaged by UV radiation. The findings suggest "rising UV levels as a result of ozone depletion are to blame for the observed skin damage, in the same way that human skin cancer rates have been on the increase in recent decades."
Effects on crops.
An increase of UV radiation would be expected to affect crops. A number of economically important species of plants, such as rice, depend on cyanobacteria residing on their roots for the retention of nitrogen. Cyanobacteria are sensitive to UV radiation and would be affected by its increase.
"Despite mechanisms to reduce or repair the effects of increased ultraviolet radiation, plants have a limited ability to adapt to increased levels of UVB, therefore plant growth can be directly affected by UVB radiation."
Public policy.
The full extent of the damage that CFCs have caused to the ozone layer is not known and will not be known for decades; however, marked decreases in column ozone have already been observed. The Montreal and Vienna conventions were installed long before a scientific consensus was established or important uncertainties in the science field were being resolved. The ozone case was understood comparably well by lay persons as e.g. "Ozone shield" or "ozone hole" were useful "easy-to-understand bridging metaphors". Americans voluntarily switched away from aerosol sprays, resulting in a 50% sales loss even before legislation was enforced.
After a 1976 report by the United States National Academy of Sciences concluded that credible scientific evidence supported the ozone depletion hypothesis a few countries, including the United States, Canada, Sweden, Denmark, and Norway, moved to eliminate the use of CFCs in aerosol spray cans. At the time this was widely regarded as a first step towards a more comprehensive regulation policy, but progress in this direction slowed in subsequent years, due to a combination of political factors (continued resistance from the halocarbon industry and a general change in attitude towards environmental regulation during the first two years of the Reagan administration) and scientific developments (subsequent National Academy assessments that indicated that the first estimates of the magnitude of ozone depletion had been overly large).
A critical DuPont manufacturing patent for Freon was set to expire in 1979. The United States banned the use of CFCs in aerosol cans in 1978. The European Community rejected proposals to ban CFCs in aerosol sprays, and in the U.S., CFCs continued to be used as refrigerants and for cleaning circuit boards. Worldwide CFC production fell sharply after the U.S. aerosol ban, but by 1986 had returned nearly to its 1976 level. In 1993, DuPont shut down its CFC facility.
The U.S. Government's attitude began to change again in 1983, when William Ruckelshaus replaced Anne M. Burford as Administrator of the United States Environmental Protection Agency. Under Ruckelshaus and his successor, Lee Thomas, the EPA pushed for an international approach to halocarbon regulations. In 1985 20 nations, including most of the major CFC producers, signed the Vienna Convention for the Protection of the Ozone Layer, which established a framework for negotiating international regulations on ozone-depleting substances. That same year, the discovery of the Antarctic ozone hole was announced, causing a revival in public attention to the issue. In 1987, representatives from 43 nations signed the Montreal Protocol. Meanwhile, the halocarbon industry shifted its position and started supporting a protocol to limit CFC production. However, this shift was uneven with DuPont acting more quickly than their European counterparts. DuPont may have feared court action related to increased skin cancer especially as the EPA had published a study in 1986 claiming that an additional 40 million cases and 800,000 cancer deaths were to be expected in the U.S. in the next 88 years. The EU shifted its position as well after Germany gave up its defence of the CFC industry and started supporting moves towards regulation. Government and industry in France and the UK tried to defend their CFC producing industries even after the Montreal Protocol had been signed.
At Montreal, the participants agreed to freeze production of CFCs at 1986 levels and to reduce production by 50% by 1999. After a series of scientific expeditions to the Antarctic produced convincing evidence that the ozone hole was indeed caused by chlorine and bromine from manmade organohalogens, the Montreal Protocol was strengthened at a 1990 meeting in London. The participants agreed to phase out CFCs and halons entirely (aside from a very small amount marked for certain "essential" uses, such as asthma inhalers) by 2000 in non-Article 5 countries and by 2010 in Article 5 (less developed) signatories. At a 1992 meeting in Copenhagen, the phase-out date was moved up to 1996. At the same meeting, methyl bromide (MeBr), a fumigant used primarily in agricultural production, was added to the list of controlled substances. For all substances controlled under the protocol, phaseout schedules were delayed for less developed ('Article 5(1)') countries, and phaseout in these countries was supported by transfers of expertise, technology, and money from non-Article 5(1) Parties to the Protocol. Additionally, exemptions from the agreed schedules could be applied for under the Essential Use Exemption (EUE) process for substances other than methyl bromide and under the Critical Use Exemption (CUE) process for methyl bromide.
A hydrocarbon refrigerant was developed in 1992 at the request of the non-governmental organization (NGO) Greenpeace, and was being used by some 40% of the refrigerator market in 2013. In the U.S., however, change has been much slower. To some extent, CFCs were being replaced by the less damaging hydrochlorofluorocarbons (HCFCs), although concerns remain regarding HCFCs also. In some applications, hydrofluorocarbons (HFCs) were being used to replace CFCs. HFCs, which contain no chlorine or bromine, do not contribute at all to ozone depletion although they are potent greenhouse gases. The best known of these compounds is probably HFC-134a (R-134a), which in the United States has largely replaced CFC-12 (R-12) in automobile air conditioners. In laboratory analytics (a former "essential" use) the ozone depleting substances can be replaced with various other solvents. The development and promotion of an ozone-safe hydrocarbon refrigerant was a breakthrough which occurred after the initiative of a non-governmental organization (NGO). Civil society including especially NGOs, in fact, played critical roles at all stages of policy development leading up to the Vienna Conference, the Montreal Protocol, and in assessing compliance afterwards. The major companies claimed that no alternatives to HFC existed. By 1992, Greenpeace had requested a scientific team to research an ozone-safe refrigerant, an effort which resulted in a successful hydrocarbon formula, for which Greenpeace left the patent as open source. The NGO maintained the technology as an open patent as it worked successfully first with a small company to market an appliance beginning in Europe, then Asia and later Latin America, receiving a 1997 UNEP award. By 1995, Germany had already made CFC refrigerators illegal. Production spread to companies like Electrolux, Bosch, and LG, with sales reaching some 300 million refrigerators by 2008. However, the giant corporations all continued to refuse to apply the technology in Latin America. In 2003, a domestic Argentinian company began Greenfreeze production, while the giant Bosch in Brazil began a year later. Chemical companies like Du Pont, whose representatives even disparaged the green technology as "that German technology," maneuvered the EPA to block the technology in the U.S. until 2011. Ben & Jerry's of Unilever and General Electric, spurred by Greenpeace, had expressed formal interest in 2008 which figured in the EPA's final approval. Currently, more than 600 million refrigerators have been sold, making up more than 40% of the market. Since 2004, corporations like Coca-Cola, Carlsberg, and IKEA have been forming a coalition to promote the ozone-safe Greenfreeze units.
More recently, policy experts have advocated for efforts to link ozone protection efforts to climate protection efforts. Many ODS are also greenhouse gases, some thousands of times more powerful agents of radiative forcing than carbon dioxide over the short and medium term. Thus policies protecting the ozone layer have had benefits in mitigating climate change. In fact, the reduction of the radiative forcing due to ODS probably masked the true level of climate change effects of other GHGs, and was responsible for the "slow down" of global warming from the mid-90s. Policy decisions in one arena affect the costs and effectiveness of environmental improvements in the other.
ODS requirements in the marine industry.
The IMO has amended MARPOL Annex VI Regulation 12 regarding ozone depleting substances. As from July 1, 2010, all vessels where MARPOL Annex VI is applicable should have a list of equipment using ozone depleting substances. The list should include name of ODS, type and location of equipment, quantity in kg and date. All changes since that date should be recorded in an ODS Record book on board recording all intended or unintended releases to the atmosphere. Furthermore, new ODS supply or landing to shore facilities should be recorded as well.
Prospects of ozone depletion.
Since the adoption and strengthening of the Montreal Protocol has led to reductions in the emissions of CFCs, atmospheric concentrations of the most-significant compounds have been declining. These substances are being gradually removed from the atmosphere; since peaking in 1994, the Effective Equivalent Chlorine (EECl) level in the atmosphere had dropped about 10% by 2008. The decrease in ozone-depleting chemicals has also been significantly affected by a decrease in bromine-containing chemicals. The data suggest that substantial natural sources exist for atmospheric methyl bromide (CH3Br). The phase-out of CFCs means that nitrous oxide (N2O), which is not covered by the Montreal Protocol, has become the most highly emitted ozone-depleting substance and is expected to remain so throughout the 21st century.
A 2005 IPCC review of ozone observations and model calculations concluded that the global amount of ozone has now approximately stabilized. Although considerable variability is expected from year to year, including in polar regions where depletion is largest, the ozone layer is expected to begin to recover in coming decades due to declining ozone-depleting substance concentrations, assuming full compliance with the Montreal Protocol.
The Antarctic ozone hole is expected to continue for decades. Ozone concentrations in the lower stratosphere over Antarctica will increase by 5–10% by 2020 and return to pre-1980 levels by about 2060–2075. This is 10–25 years later than predicted in earlier assessments, because of revised estimates of atmospheric concentrations of ozone-depleting substances, including a larger predicted future usage in developing countries. Another factor that may prolong ozone depletion is the drawdown of nitrogen oxides from above the stratosphere due to changing wind patterns.
Research history.
The basic physical and chemical processes that lead to the formation of an ozone layer in the Earth's stratosphere were discovered by Sydney Chapman in 1930. Short-wavelength UV radiation splits an oxygen (O2) molecule into two oxygen (O) atoms, which then combine with other oxygen molecules to form ozone. Ozone is removed when an oxygen atom and an ozone molecule "recombine" to form two oxygen molecules, i.e. O + O3 → 2O2. In the 1950s, David Bates and Marcel Nicolet presented evidence that various free radicals, in particular hydroxyl (OH) and nitric oxide (NO), could catalyze this recombination reaction, reducing the overall amount of ozone. These free radicals were known to be present in the stratosphere, and so were regarded as part of the natural balance—it was estimated that in their absence, the ozone layer would be about twice as thick as it currently is.
In 1970 Paul Crutzen pointed out that emissions of nitrous oxide (N2O), a stable, long-lived gas produced by soil bacteria, from the Earth's surface could affect the amount of nitric oxide (NO) in the stratosphere. Crutzen showed that nitrous oxide lives long enough to reach the stratosphere, where it is converted into NO. Crutzen then noted that increasing use of fertilizers might have led to an increase in nitrous oxide emissions over the natural background, which would in turn result in an increase in the amount of NO in the stratosphere. Thus human activity could have an impact on the stratospheric ozone layer. In the following year, Crutzen and (independently) Harold Johnston suggested that NO emissions from supersonic passenger aircraft, which would fly in the lower stratosphere, could also deplete the ozone layer. However, more recent analysis in 1995 by David W. Fahey, an atmospheric scientist at the National Oceanic and Atmospheric Administration, found that the drop in ozone would be from 1 to 2% if a fleet of 500 supersonic passenger aircraft were operated. This, Fahey expressed, would not be a showstopper for advanced supersonic passenger aircraft development.
Rowland–Molina hypothesis.
In 1974 Frank Sherwood Rowland, Chemistry Professor at the University of California at Irvine, and his postdoctoral associate Mario J. Molina suggested that long-lived organic halogen compounds, such as CFCs, might behave in a similar fashion as Crutzen had proposed for nitrous oxide. James Lovelock had recently discovered, during a cruise in the South Atlantic in 1971, that almost all of the CFC compounds manufactured since their invention in 1930 were still present in the atmosphere. Molina and Rowland concluded that, like N2O, the CFCs would reach the stratosphere where they would be dissociated by UV light, releasing chlorine atoms. A year earlier, Richard Stolarski and Ralph Cicerone at the University of Michigan had shown that Cl is even more efficient than NO at catalyzing the destruction of ozone. Similar conclusions were reached by Michael McElroy and Steven Wofsy at Harvard University. Neither group, however, had realized that CFCs were a potentially large source of stratospheric chlorine—instead, they had been investigating the possible effects of HCl emissions from the Space Shuttle, which are very much smaller.
The Rowland–Molina hypothesis was strongly disputed by representatives of the aerosol and halocarbon industries. The Chair of the Board of DuPont was quoted as saying that ozone depletion theory is "a science fiction tale ... a load of rubbish ... utter nonsense". Robert Abplanalp, the President of Precision Valve Corporation (and inventor of the first practical aerosol spray can valve), wrote to the Chancellor of UC Irvine to complain about Rowland's public statements. Nevertheless, within three years most of the basic assumptions made by Rowland and Molina were confirmed by laboratory measurements and by direct observation in the stratosphere. The concentrations of the source gases (CFCs and related compounds) and the chlorine reservoir species (HCl and ClONO2) were measured throughout the stratosphere, and demonstrated that CFCs were indeed the major source of stratospheric chlorine, and that nearly all of the CFCs emitted would eventually reach the stratosphere. Even more convincing was the measurement, by James G. Anderson and collaborators, of chlorine monoxide (ClO) in the stratosphere. ClO is produced by the reaction of Cl with ozone—its observation thus demonstrated that Cl radicals not only were present in the stratosphere but also were actually involved in destroying ozone. McElroy and Wofsy extended the work of Rowland and Molina by showing that bromine atoms were even more effective catalysts for ozone loss than chlorine atoms and argued that the brominated organic compounds known as halons, widely used in fire extinguishers, were a potentially large source of stratospheric bromine. In 1976 the United States National Academy of Sciences released a report concluding that the ozone depletion hypothesis was strongly supported by the scientific evidence. Scientists calculated that if CFC production continued to increase at the going rate of 10% per year until 1990 and then remain steady, CFCs would cause a global ozone loss of 5 to 7% by 1995, and a 30 to 50% loss by 2050. In response the United States, Canada and Norway banned the use of CFCs in aerosol spray cans in 1978. However, subsequent research, summarized by the National Academy in reports issued between 1979 and 1984, appeared to show that the earlier estimates of global ozone loss had been too large.
Crutzen, Molina, and Rowland were awarded the 1995 Nobel Prize in Chemistry for their work on stratospheric ozone.
Antarctic ozone hole.
The discovery of the Antarctic "ozone hole" by British Antarctic Survey scientists Farman, Gardiner and Shanklin (first reported in a paper in "Nature" in May 1985) came as a shock to the scientific community, because the observed decline in polar ozone was far larger than anyone had anticipated. Satellite measurements showing massive depletion of ozone around the south pole were becoming available at the same time. However, these were initially rejected as unreasonable by data quality control algorithms (they were filtered out as errors since the values were unexpectedly low); the ozone hole was detected only in satellite data when the raw data was reprocessed following evidence of ozone depletion in "in situ" observations. When the software was rerun without the flags, the ozone hole was seen as far back as 1976.
Susan Solomon, an atmospheric chemist at the National Oceanic and Atmospheric Administration (NOAA), proposed that chemical reactions on polar stratospheric clouds (PSCs) in the cold Antarctic stratosphere caused a massive, though localized and seasonal, increase in the amount of chlorine present in active, ozone-destroying forms. The polar stratospheric clouds in Antarctica are only formed when there are very low temperatures, as low as −80 °C, and early spring conditions. In such conditions the ice crystals of the cloud provide a suitable surface for conversion of unreactive chlorine compounds into reactive chlorine compounds, which can deplete ozone easily.
Moreover the polar vortex formed over Antarctica is very tight and the reaction occurring on the surface of the cloud crystals is far different from when it occurs in atmosphere. These conditions have led to ozone hole formation in Antarctica. This hypothesis was decisively confirmed, first by laboratory measurements and subsequently by direct measurements, from the ground and from high-altitude airplanes, of very high concentrations of chlorine monoxide (ClO) in the Antarctic stratosphere.
Alternative hypotheses, which had attributed the ozone hole to variations in solar UV radiation or to changes in atmospheric circulation patterns, were also tested and shown to be untenable.
Meanwhile, analysis of ozone measurements from the worldwide network of ground-based Dobson spectrophotometers led an international panel to conclude that the ozone layer was in fact being depleted, at all latitudes outside of the tropics. These trends were confirmed by satellite measurements. As a consequence, the major halocarbon-producing nations agreed to phase out production of CFCs, halons, and related compounds, a process that was completed in 1996.
Since 1981 the United Nations Environment Programme, under the auspices of the World Meteorological Organization, has sponsored a series of technical reports on the Scientific Assessment of Ozone Depletion, based on satellite measurements. The 2007 report showed that the hole in the ozone layer was recovering and the smallest it had been for about a decade.
The 2010 report found, "Over the past decade, global ozone and ozone in the Arctic and Antarctic regions is no longer decreasing but is not yet increasing. The ozone layer outside the Polar regions is projected to recover to its pre-1980 levels some time before the middle of this century. In contrast, the springtime ozone hole over the Antarctic is expected to recover much later."
In 2012, NOAA and NASA reported "Warmer air temperatures high above the Antarctic led to the second smallest season ozone hole in 20 years averaging 17.9 million square kilometres. The hole reached its maximum size for the season on Sept 22, stretching to 921.2 million square kilometres."
The hole in the Earth's ozone layer over the South Pole has affected atmospheric circulation in the Southern Hemisphere all the way to the equator. The ozone hole has influenced atmospheric circulation all the way to the tropics and increased rainfall at low, subtropical latitudes in the Southern Hemisphere.
Arctic ozone hole.
On March 15, 2011, a record ozone layer loss was observed, with about half of the ozone present over the Arctic having been destroyed. The change was attributed to increasingly cold winters in the Arctic stratosphere at an altitude of approximately 20 km, a change associated with global warming in a relationship that is still under investigation. By March 25, the ozone loss had become the largest compared to that observed in all previous winters with the possibility that it would become an ozone hole. This would require that the quantities of ozone to fall below 200 Dobson units, from the 250 recorded over central Siberia. It is predicted that the thinning layer would affect parts of Scandinavia and Eastern Europe on March 30–31.
On October 2, 2011, a study was published in the journal "Nature", which said that between December 2010 and March 2011 up to 80% of the ozone in the atmosphere at about 20 km above the surface was destroyed. The level of ozone depletion was severe enough that scientists said it could be compared to the ozone hole that forms over Antarctica every winter. According to the study, "for the first time, sufficient loss occurred to reasonably be described as an Arctic ozone hole." The study analyzed data from the Aura and CALIPSO satellites, and determined that the larger-than-normal ozone loss was due to an unusually long period of cold weather in the Arctic, some 30 days more than typical, which allowed for more ozone-destroying chlorine compounds to be created. According to Lamont Poole, a co-author of the study, cloud and aerosol particles on which the chlorine compounds are found "were abundant in the Arctic until mid March 2011—much later than usual—with average amounts at some altitudes similar to those observed in the Antarctic, and dramatically larger than the near-zero values seen in March in most Arctic winters".
Tibet ozone hole.
As winters that are colder are more affected, at times there is an ozone hole over Tibet. In 2006, a 2.5 million square kilometer ozone hole was detected over Tibet. Also again in 2011 an ozone hole appeared over mountainous regions of Tibet, Xinjiang, Qinghai and the Hindu Kush, along with an unprecedented hole over the Arctic, though the Tibet one is far less intense than the ones over the Arctic or Antarctic.
Potential depletion by storm clouds.
Research in 2012 showed that the same process that produces the ozone hole over Antarctica occurs over summer storm clouds in the United States, and thus may be destroying ozone there as well.
Ozone depletion and global warming.
Among others, Robert Watson had a role in the science assessment and in the regulation efforts of ozone depletion and global warming. Prior to the 1980s, the EU, NASA, NAS, UNEP, WMO and the British government had dissenting scientific reports and Watson played a crucial role in the process of unified assessments. Based on the experience with the ozone case, the IPCC started to work on a unified reporting and science assessment to reach a consensus to provide the IPCC Summary for Policymakers.
There are various areas of linkage between ozone depletion and global warming science:
Misconceptions.
CFC weight.
Since CFC molecules are heavier than air (nitrogen or oxygen), it is commonly believed that the CFC molecules cannot reach the stratosphere in significant amount. But atmospheric gases are not sorted by weight; the forces of wind can fully mix the gases in the atmosphere. The CFCs are evenly distributed throughout the turbosphere and reach the upper atmosphere.
Percentage of man-made chlorine.
Another misconception is that "it is generally accepted that natural sources of tropospheric chlorine are four to five times larger than man-made ones." While strictly true, "tropospheric" chlorine is irrelevant; it is "stratospheric" chlorine that affects ozone depletion. Chlorine from ocean spray is soluble and thus is washed by rainfall before it reaches the stratosphere. CFCs, in contrast, are insoluble and long-lived, allowing them to reach the stratosphere. In the lower atmosphere, there is much more chlorine from CFCs and related haloalkanes than there is in HCl from salt spray, and in the stratosphere halocarbons are dominant. Only methyl chloride, which is one of these halocarbons, has a mainly natural source, and it is responsible for about 20 percent of the chlorine in the stratosphere; the remaining 80% comes from man made sources.
Very violent volcanic eruptions can inject HCl into the stratosphere, but researchers have shown that the contribution is not significant compared to that from CFCs.
A similar erroneous assertion is that soluble halogen compounds from the volcanic plume of Mount Erebus on Ross Island, Antarctica are a major contributor to the Antarctic ozone hole.
First observation.
G.M.B. Dobson (Exploring the Atmosphere, 2nd Edition, Oxford, 1968) mentioned that when springtime ozone levels in the Antarctic over Halley Bay were first measured in 1956, he was surprised to find that they were ~320 DU, or about 150 DU below spring Arctic levels of ~450 DU. These were at that time the only known Antarctic ozone values available. What Dobson describes is essentially the "baseline" from which the ozone hole is measured: actual ozone hole values are in the 150–100 DU range.
The discrepancy between the Arctic and Antarctic noted by Dobson was primarily a matter of timing: during the Arctic spring ozone levels rose smoothly, peaking in April, whereas in the Antarctic they stayed approximately constant during early spring, rising abruptly in November when the polar vortex broke down.
The behavior seen in the Antarctic ozone hole is completely different. Instead of staying constant, early springtime ozone levels suddenly drop from their already low winter values, by as much as 50%, and normal values are not reached again until December.
Location of hole.
Some people thought that the ozone hole should be above the sources of CFCs.
However, CFCs are well mixed globally in the troposphere and stratosphere. The reason for occurrence of the ozone hole above Antarctica is not because there are more CFCs concentrated but because the low temperatures help form polar stratospheric clouds. In fact, there are findings of significant and localized "ozone holes" above other parts of the earth.
World Ozone Day.
In 1994, the United Nations General Assembly voted to designate September 16 as "World Ozone Day", to commemorate the signing of the Montreal Protocol on that date in 1987.

</doc>
<doc id="44187" url="http://en.wikipedia.org/wiki?curid=44187" title="Sound effect">
Sound effect

Sound effects (or audio effects) are artificially created or enhanced sounds, or sound processes used to emphasize artistic or other content of films, television shows, live performance, animation, video games, music, or other media. In motion picture and television production, a sound effect is a sound recorded and presented to make a specific storytelling or creative point "without" the use of dialogue or music. The term often refers to a process applied to a recording, without necessarily referring to the recording itself. In professional motion picture and television production, dialogue, music, and sound effects recordings are treated as separate elements. Dialogue and music recordings are never referred to as sound effects, even though the processes applied to such as reverberation or flanging effects, often are called "sound effects".
History.
The term "sound effect" ranges back to the early days of radio. In its Year Book 1931 the BBC published a major article about "The Use of Sound Effects". It considers sounds effect deeply linked with broadcasting and states: "It would be a great mistake to think of them as anologous to punctuation marks and accents in print. They should never be "inserted" into a programme already existing. The author of a broadcast play or broadcast construction ought to have used Sound Effects as bricks with which to build, treating them as of equal value with speech and music." It lists six "totally different primary genres of Sound Effect":
According to the author, "It is axiomatic that every Sound Effect, to whatever category it belongs, "must" register in the listener's mind instantaneously. If it fails to do so its presence could not be justified."
Film.
In the context of motion pictures and television, "sound effects" refers to an entire hierarchy of sound elements, whose production encompasses many different disciplines, including:
Each of these sound effect categories is specialized, with sound editors known as specialists in an area of sound effects (e.g. a "Car cutter" or "Guns cutter").
Foley is another method of adding sound effects. Foley is more of a technique for creating sound effects than a type of sound effect, but it is often used for creating the incidental real world sounds that are very specific to what is going on onscreen, such as footsteps. With this technique the action onscreen is essentially recreated to try to match it as closely as possible. If done correctly it is very hard for audiences to tell what sounds were added and what sounds were originally recorded (location sound).
In the early days of film and radio, foley artists would add sounds in realtime or pre-recorded sound effects would be played back from analogue discs in realtime (while watching the picture). Today, with effects held in digital format, it is easy to create any required sequence to be played in any desired timeline.
In the days of silent film, sound effects were added by the operator of a theater organ or photoplayer, both of which also supplied the soundtrack of the film. Theater organ sound effects are usually electric or electro-pneumatic, and activated by a button pressed with the hand or foot.
Photoplayer operators activate sound effects either by flipping switches on the machine or pulling "cow-tail" pull-strings, which hang above. Sounds like bells and drums are made mechanically, sirens and horns electronically. Due to its smaller size, a photoplayer usually has less special effects than a theater organ, or less complex ones.
Video games.
The principles involved with modern video game sound effects (since the introduction of sample playback) are essentially the same as those of motion pictures. Typically a game project requires two jobs to be completed: sounds must be recorded or selected from a library and a sound engine must be programmed so that those sounds can be incorporated into the game's interactive environment.
In earlier computers and video game systems, sound effects were typically produced using sound synthesis. In modern systems, the increases in storage capacity and playback quality has allowed sampled sound to be used. The modern systems also frequently utilize positional audio, often with hardware acceleration, and real-time audio post-processing, which can also be tied to the 3D graphics development. Based on the internal state of the game, multiple different calculations can be made. This will allow for, for example, realistic sound dampening, echoes and doppler effect.
Historically the simplicity of game environments reduced the required number of sounds needed, and thus only one or two people were directly responsible for the sound recording and design. As the video game business has grown and computer sound reproduction quality has increased, however, the team of sound designers dedicated to game projects has likewise grown and the demands placed on them may now approach those of mid-budget motion pictures.
Music.
Some pieces of music use sound effects that are made by a musical instrument or by other means. An early example is the 18th century Toy Symphony. Richard Wagner in the opera Das Rheingold (1869) lets a choir of anvils introduce the scene of the dwarfs who have to work in the mines, similar to the introduction of the dwarfs in the 1937 Disney movie Snow White. Klaus Doldingers soundtrack for the 1981 movie Das Boot includes a title score with a sonar sound to reflect the U-boat setting. John Barry integrated into the title song of Moonraker (1979) a sound representing the beep of a Sputnik like satellite.
Recording.
The most realistic sound effects may originate from original sources; the closest sound to machine-gun fire that we can replay should be an original recording of actual machine guns.
However, real life and actual practice do not always coincide with theory. Often recordings of real life do not sound realistic on playback. That is why we have Foley and f/x. The realistic sound of bacon frying is the crumpling of cellophane. Rain may be recorded as salt falling on a piece of tinfoil.
Less realistic sound effects are digitally synthesized or sampled and sequenced (the same recording played repeatedly using a sequencer). When the producer or content creator demands high-fidelity sound effects, the sound editor usually must augment his available library with new sound effects recorded in the field.
When the required sound effect is of a small subject, such as scissors cutting, cloth ripping, or footsteps, the sound effect is best recorded in a studio, under controlled conditions. Such small sounds are often delegated to a foley artist and foley editor. Many sound effects cannot be recorded in a studio, such as explosions, gunfire, and automobile or aircraft maneuvers. These effects must be recorded by a sound effects editor or a professional sound effects recordist.
When such "big" sounds are required, the recordist will begin contacting professionals or technicians in the same way a producer may arrange a crew; if the recordist needs an explosion, he may contact a demolition company to see if any buildings are scheduled to be destroyed with explosives in the near future. If the recordist requires a volley of cannon fire, he may contact historical re-enactors or gun enthusiasts.
Depending on the effect, recordists may use several DAT, hard disk, or Nagra recorders and a large number of microphones. During a cannon- and musket-fire recording session for the 2003 film "The Alamo", conducted by Jon Johnson and Charles Maynes, two to three DAT machines were used. One machine was stationed near the cannon itself, so it could record the actual firing. Another was stationed several hundred yards away, below the trajectory of the ball, to record the sound of the cannonball passing by. When the crew recorded musket-fire, a set of microphones were arrayed close to the target (in this case a swine carcass) to record the musket-ball impacts.
A counter-example is the common technique for recording an automobile. For recording "Onboard" car sounds (which include the car interiors), a three-microphone technique is common. Two microphones record the engine directly: one is taped to the underside of the hood, near the engine block. The second microphone is covered in a wind screen and tightly attached to the rear bumper, within an inch or so of the tail pipe. The third microphone, which is often a stereo microphone, is stationed inside the car to get the car interior.
Having all of these tracks at once gives a sound designer or audio engineer a great deal of control over how he wants the car to sound. In order to make the car more ominous or low, he can mix in more of the tailpipe recording; if he wants the car to sound like it is running full throttle, he can mix in more of the engine recording and reduce the interior perspective. In cartoons, a pencil being dragged down a washboard may be used to simulate the sound of a sputtering engine.
What we would consider today to be the first recorded sound effect was of Big Ben striking 10:30, 10:45, and 11:00. It was recorded on a brown wax cylinder by technicians at Edison House in London. It was recorded July 16, 1890. This recording is currently in the public domain.
Processing effects.
As the car example demonstrates, the ability to make multiple simultaneous recordings of the same subject—through the use of several DAT or multitrack recorders—has made sound recording into a sophisticated craft. The sound effect can be shaped by the sound editor or sound designer, not just for realism, but for emotional effect.
Once the sound effects are recorded or captured, they are usually loaded into a computer integrated with an audio non-linear editing system. This allows a sound editor or sound designer to heavily manipulate a sound to meet his or her needs.
The most common sound design tool is the use of layering to create a new, interesting sound out of two or three old, average sounds. For example, the sound of a bullet impact into a pig carcass may be mixed with the sound of a melon being gouged to add to the "stickiness" or "gore" of the effect. If the effect is featured in a close-up, the designer may also add an "impact sweetener" from his or her library. The sweetener may simply be the sound of a hammer pounding hardwood, equalized so that only the low-end can be heard. The low end gives the three sounds together added weight, so that the audience actually "feels" the weight of the bullet hit the victim.
If the victim is the villain, and his death is climactic, the sound designer may add reverb to the impact, in order to enhance the dramatic beat. And then, as the victim falls over in slow motion, the sound editor may add the sound of a broom whooshing by a microphone, pitch-shifted down and time-expanded to further emphasize the death. If the film is science-fiction, the designer may phaser the "whoosh" to give it a more sci-fi feel. (For a list of many sound effects processes available to a sound designer, see the bottom of this article.)
Aesthetics.
When creating sound effects for films, sound recordists and editors do not generally concern themselves with the verisimilitude or accuracy of the sounds they present. The sound of a bullet entering a person from a close distance may sound nothing like the sound designed in the above example, but since very few people are aware of how such a thing actually sounds, the job of designing the effect is mainly an issue of creating a conjectural sound which feeds the audience's expectations while still suspending disbelief.
In the previous example, the phased 'whoosh' of the victim's fall has no analogue in real life experience, but it is emotionally immediate. If a sound editor uses such sounds in the context of emotional climax or a character's subjective experience, they can add to the drama of a situation in a way visuals simply cannot. If a visual effects artist were to do something similar to the 'whooshing fall' example, it would probably look ridiculous or at least excessively melodramatic.
The "Conjectural Sound" principle applies even to happenstance sounds, such as tires squealing, doorknobs turning or people walking. If the sound editor wants to communicate that a driver is in a hurry to leave, he will cut the sound of tires squealing when the car accelerates from a stop; even if the car is on a dirt road, the effect will work if the audience is dramatically engaged. If a character is afraid of someone on the other side of a door, the turning of the doorknob can take a second or more, and the mechanism of the knob can possess dozens of clicking parts. A skillful Foley artist can make someone walking calmly across the screen seem terrified simply by giving the actor a different gait.
Techniques.
In music and film/television production, typical effects used in recording and amplified performances are:

</doc>
<doc id="44189" url="http://en.wikipedia.org/wiki?curid=44189" title="Reciprocal altruism">
Reciprocal altruism

In evolutionary biology, reciprocal altruism is a behaviour whereby an organism acts in a manner that temporarily reduces its fitness while increasing another organism's fitness, with the expectation that the other organism will act in a similar manner at a later time. The concept was initially developed by Robert Trivers to explain the evolution of cooperation as instances of mutually altruistic acts. The concept is close to the strategy of "tit for tat" used in game theory.
Theory.
The concept of "reciprocal altruism", as introduced by Trivers, suggests that altruism, defined as an act of helping someone else although incurring some cost for this act, could have evolved since it might be beneficial to incur this cost if there is a chance of being in a reverse situation where the person whom I helped before may perform an altruistic act towards me.
Putting this into the form of a strategy in a repeated prisoner’s dilemma would mean to cooperate unconditionally in the first period and behave cooperatively (altruistically) as long as the other agent does as well. If chances of meeting another reciprocal altruist are high enough or the game is repeated for a long enough amount of time, this form of altruism can evolve within a population.
This is close to the notion of "tit for tat" introduced by Anatol Rapoport, although there still seems a slight distinction in that "tit for tat" cooperates in the first period and from thereon always replicates an opponent’s previous action, whereas “reciprocal altruists” stop cooperation in the first instance of non-cooperation by an opponent and stay non-cooperative from thereon. This distinction leads to the fact that in contrast to reciprocal altruism, tit for tat may be able to restore cooperation under certain conditions despite cooperation having broken down.
Stephens shows a set of necessary and jointly sufficient conditions “… for an instance of reciprocal altruism:
There are two additional conditions necessary "…for reciprocal altruism to evolve:"
The first two conditions are necessary for altruism as such, while the third is distinguishing reciprocal altruism from simple mutualism and the fourth makes the interaction reciprocal.
Condition number five is required as otherwise non-altruists may always exploit altruistic behaviour without any consequences and therefore evolution of reciprocal altruism would not be possible. However, it is pointed out that this “conditioning device” does not need to be conscious. Condition number six is required to avoid cooperation breakdown through backwards induction—a possibility suggested by game theoretical models.
Examples.
The following examples could be understood as altruism. 
However, showing reciprocal altruism in an unambiguous way requires more evidence as will be shown later.
Cleaning symbiosis.
An example of reciprocal altruism is cleaning symbiosis, such as between cleaner fish and their hosts, though cleaners include shrimps and birds, and clients include fish, turtles, octopuses and mammals. Aside from the apparent symbiosis of the cleaner and the host during actual cleaning, which cannot be interpreted as altruism, the host displays additional behaviour that meets the criteria for altruism:
The host fish allows the cleaner fish free entrance and exit and does not eat the cleaner, even after the cleaning is done.
The host signals the cleaner it is about to depart the cleaner's locality, even when the cleaner is not in its body. The host sometimes chases off possible dangers to the cleaner.
The following evidence supports the hypothesis:
The cleaning by cleaners is essential for the host. In the absence of cleaners the hosts leave the locality or suffer from injuries inflicted by ecto-parasites. There is difficulty and danger in finding a cleaner. Hosts leave their element to get cleaned. Others wait no longer than 30 seconds before searching for cleaners elsewhere.
A key requirement for the establishment of reciprocal altruism is that the same two individuals must interact repeatedly, as otherwise the best strategy for the host would be to eat the cleaner as soon as cleaning was complete. This constraint imposes both a spatial and a temporal condition on the cleaner and on its host. Both individuals must remain in the same physical location, and both must have a long enough lifespan, to enable multiple interactions. Surprisingly, there is reliable evidence that individual cleaners and hosts do indeed interact repeatedly.
This example meets the criteria described in Trivers’s model. However, some elements essential for reciprocity have not yet been demonstrated. The criterion that an individual doesn’t expect an immediate payment. In the cleaner-host system the benefit for a cleaner is always immediate. The criterion that one individual's failure to act altruistically causes the other to avoid future altruistic acts. This is hard to show since such failure means the death of the cleaner. However, if Randall’s claim that hosts sometimes chase off possible dangers to the cleaner is correct, an experiment might be constructed in which reciprocity could be demonstrated.
Warning calls in birds.
Warning calls, although exposing a bird and putting it in danger, are frequently given by birds. An explanation in terms of altruistic behaviour is given by Trivers:
It has been shown that predators learn specific localities and specialize individually on prey types and hunting techniques.
It is therefore disadvantageous for a bird to have a predator eat a conspecific, because the experienced predator may then be more likely to eat him. Alarming another bird by giving a warning call tends to prevent predators from specializing on the caller’s species and locality. In this way, birds in areas in which warning calls are given will be at a selective advantage relative to birds in areas free from warning calls.
Nevertheless, this presentation lacks important elements of reciprocity. It is very hard to detect cheaters. Also, there is no evidence that a bird refrains from giving calls when another bird is not reciprocating. And there is no evidence that individuals interact repeatedly.
Another explanation for warning calls is that these are not warning calls at all:
A bird, once it has detected a bird of prey, calls to signal to the bird of prey that it was detected, and that there is no use trying to attack the calling bird. Two facts support this hypothesis:
Nest Protecting.
Red-winged blackbird males help defend neighbor's nests. There are many theories as to why males behave this way. One is that males only defend other nests which contain their extra-pair offspring. Extra-pair offspring is juveniles which may contain some of the male bird's DNA. Another is the tit-for-tat strategy of reciprocal altruism. A third theory is, males help only other closely related males. A study done by The Department of Fisheries and Wildlife provided evidence that males used a tit-for-tat strategy. The Department of Fisheries and Wildlife tested many different nests by placing stuffed crows by nests, and then observing behavior of neighboring males. The behaviors they looked for included the number of calls, dives, and strikes. After analyzing the results, there was not significance evidence for kin selection; the presence of extra-pair offspring did not affect the probability of help in nest defense. However, males reduced the amount of defense given to neighbors when neighbor males reduced defense for their nests. This demonstrates a tit-for-tat strategy, where animals help those who previously helped them. This strategy is one type of reciprocal altruism.
Vampire bats.
Vampire bats also display reciprocal altruism, as described by Wilkinson.
The bats feed each other by regurgitating blood. Since bats only feed on blood and will die after just 70 hours of not eating, this food sharing is a great benefit to the receiver and a great cost to the giver.
To qualify for reciprocal altruism, the benefit to the receiver would have to be larger than the cost to the donor. This seems to hold as these bats usually die if they do not find a blood meal two nights in a row. Also, the requirement that individuals who have behaved altruistically in the past are helped by others in the future is confirmed by the data. However, the consistency of the reciprocal behaviour, namely that a previously non-altruistic bat is refused help when it requires it, has not been demonstrated. Therefore, the bats do not seem to qualify yet as an example for reciprocal altruism.
However, a closer look at the data shows that – except for a single interaction – all instances of feeding happened between individuals of the same group, who are on average cousins. Thus, it seems much more probable that this example is a case of kin selection than reciprocal altruism.
Primates.
Grooming in primates meets the conditions for reciprocal altruism. Studies in crab-eating macaques show that individuals which are groomed are much more likely to groom or support their groomers than monkeys that had not groomed. Vervet monkeys also display grooming behavior among group members. Presumably, this is a display of alliance among family and group members. So long as there is mutual grooming, both parties benefit at a relatively low cost.
Regulation by emotional disposition.
The human altruistic system is a sensitive and unstable one. Therefore, the tendency to give, to cheat, and the response to other’s acts of giving and cheating must be regulated by a complex psychology in each individual. Individuals differ in the degree of these tendencies and responses.
According to Trivers, the following emotional dispositions and their evolution can be understood in terms of regulation of altruism.
It is not known how individuals pick partners as there has been little research on choice. Modeling indicates that altruism about partner choices is unlikely to evolve, as costs and benefits between multiple individuals are variable. Therefore, the time or frequency of reciprocal actions contributes more to an individual's choice of partner than the reciprocal act itself.

</doc>
<doc id="44190" url="http://en.wikipedia.org/wiki?curid=44190" title="The Selfish Gene">
The Selfish Gene

The Selfish Gene is a book on sociobiology by Richard Dawkins, first published in 1976, with the purpose "to examine the biology of selfishness and altruism". Dawkins used the term "selfish gene" as a way of popularizing a gene-centered view of evolution. From the genocentric view, the more two individuals are genetically related, the more sense (at the level of the genes) it makes for them to behave selflessly with each other. An organism is expected to evolve to maximize its inclusive fitness—an individual's own reproductive success plus the success of its relatives (weighted by their relatedness).
In Dawkins' words, natural selection favors genes that build survival machines (later called "vehicles" to include both individuals and some "whole classes of organisms"), and more flamboyantly, "gigantic lumbering robots". In other words, according to genocentrism, organisms evolve as elaborate contraptions constructed and controlled by genes, and evolution selects among these competing phenotypes or "interactors", not by altering the activity or form of the underlying genes ("replicators"), but simply by proliferating those mutated instructions that engender construction of the most successful survival machines. The result of this phenotypic evolution, it is claimed, is to select gene populations with an evolutionarily stable strategy or "ESS". (Dawkins compares a number of strategies with "ESS", particularly Tit for Tat.)
With specific models for genetics, inheritance, and behavior, models which must be empirically vindicated, analysis of statements of this kind falls within the domain of evolutionary game theory, a branch of mathematics concerned with the assumption that the strategy adopted by one individual depends on the strategies exhibited by others. According to Dawkins, his evolutionarily stable, cooperative strategy that suppresses deviancy is the extent of the selfish gene's support of altruism. For greater support, Dawkins says, humans have to depend upon their "conscious foresight" to augment what the selfish gene can provide:
According to Mary Midgley, this idea of Dawkins as to the role for intelligence in producing altruism "is a metaphysical claim to a very strong form of free-will – a mental ability to resist physical causes"
The above-stated role for intelligence occurs at the end of Dawkins' discussion of the "meme", a term coined to describe a unit of human cultural evolution analogous to the gene, Dawkins suggests that "selfish" replication of the meme might model the spreading of human culture. Dawkins elaborated later upon his concept of 'meme' as follows:
Memetics has become a field of study, although it has not become well established in scientific circles. Proponents describe memetics as an approach to evolutionary models of cultural information transfer..
In the foreword to the book's 30th-anniversary edition, Dawkins said he "can readily see that [the book's title] might give an inadequate impression of its contents" and in retrospect thinks he should have taken Tom Maschler's advice and called the book "The Immortal Gene".
Background.
"The Selfish Gene" builds upon George C. Williams's book "Adaptation and Natural Selection", which argued that altruism is not based upon group benefit "per se", but is a result of selection that occurs "at the level of the gene mediated by the phenotype" and any selection at the group level occurred only under rare circumstances. This approach was developed further during the 1960s by W. D. Hamilton and others who opposed group selection and selection aimed directly at benefit to the individual organism:
An extended discussion of Dawkins' views and his book "The Selfish Gene" is provided by Wilkins and Hull.
"Selfish" genes.
In describing genes as being "selfish", the author does not intend (as he states unequivocally) to imply that they are driven by any motives or will, but merely that their effects can be metaphorically and pedagogically described "as if" they were. The contention is that the genes that are passed on are the ones whose evolutionary consequences serve their own implicit interest (to continue the anthropomorphism) in being replicated, not necessarily those of the organism. In later work, Dawkins brings evolutionary "selfishness" down to creation of a widely proliferated "extended phenotype".
For some, the metaphor of "selfishness" is entirely clear, while to others it is confusing, misleading, or simply silly to ascribe mental attributes to something that is mindless. For example, Andrew Brown has written:
Donald Symons also finds it inappropriate to use anthropomorphism in conveying scientific meaning in general and particularly for the present instance:
Genes and selection.
Dawkins proposes the idea of the "replicator":
The original replicator (Dawkins' "Replicator") was the initial molecule which first managed to reproduce itself and thus gained an advantage over other molecules within the primordial soup. As replicating molecules became more complex, Dawkins postulates, the replicators became the genes within organisms, with each organism's body serving the purpose of a 'survival machine' for its genes.
Dawkins writes that gene combinations which help an organism to survive and reproduce tend to also improve the gene's own chances of being replicated and, as a result, frequently "successful" genes can reflect a benefit to the organism. An example of this might be a gene that protects the organism against a disease, which helps the gene spread and also helps the organism.
Genes can reproduce at the expense of the organism.
There are other times when the implicit interests of the vehicle and replicator are in conflict, such as the genes behind certain male spiders' instinctive mating behaviour, which increase the organism's inclusive fitness by allowing it to reproduce, but shorten its life by exposing it to the risk of being eaten by the cannibalistic female. Another good example is the existence of segregation distorter genes that are detrimental to their host but nonetheless propagate themselves at its expense. Likewise, the persistence of junk DNA that provides no benefit to its host can be explained on the basis that it is not subject to selection. These unselected for but transmitted DNA variations connect the individual genetically to its parents, but confer no survival benefit.
Power struggles are rare.
These examples might suggest that there is a power-struggle between genes and their interactor. In fact, the claim is that there isn't much of a struggle because the genes usually win without a fight. However, the claim is made, if the organism becomes intelligent enough to understand its own interests, as distinct from those of its genes, there can be true conflict.
An example of such a conflict might be a person using birth control to prevent fertilization, thereby inhibiting the replication of his or her genes. But this action might not be a conflict of the 'self-interest' of the organism with his or her genes, since a person using birth control might also be enhancing the survival chances of their genes by limiting family size to conform with available resources, thus avoiding extinction as predicted under the Malthusian model of population growth.
Altruism explained.
Dawkins says that in writing "The Selfish Gene" "My purpose is to examine the biology of selfishness and altruism." He does this by supporting the claim that "gene selfishness will usually give rise to selfishness in individual behaviour. However, as we shall see, there are special circumstances in which a gene can achieve its own selfish goals best by fostering a limited form of altruism at the level of individual animals." Gene selection provides one explanation for kin selection and eusociality, where organisms act altruistically, against their individual interests (in the sense of health, safety or personal reproduction), namely the argument that by helping related organisms reproduce, a gene succeeds in "helping" copies of themselves (or sequences with the same phenotypic effect) in other bodies to replicate. The claim is made that these "selfish" actions of genes lead to unselfish actions by organisms. An interesting requirement upon this claim, supported by Dawkins in by examples from nature, is the need to explain how genes achieve kin recognition, or manage to orchestrate mutualism and coevolution. Although Dawkins (and biologists in general) recognize these phenomena result in more copies of a gene, evidence is inconclusive whether this success is selected for at a group or individual level. In fact, Dawkins has proposed that it is at the level of the "extended phenotype": 
Although Dawkins agrees that groups can assist survival, they rank as a 'vehicle' for survival only if the group activity is replicated in decedents, recorded in the gene, the gene being only true replicator. An improvement in the survival lottery for the group must improve that for the gene for sufficient replication to occur. Dawkins argues qualitatively that the lottery for the gene is based upon a very long and broad record of events, and group advantages are usually too specific, too brief, and too fortuitous to change the gene lottery.
Prior to the 1960s, it was common for altruism to be explained in terms of group selection, where the benefits to the organism or even population were supposed to account for the popularity of the genes responsible for the tendency towards that behaviour. Modern versions of "multilevel selection" claim to have overcome the original objections, namely, that at that time no known form of group selection led to an evolutionarily stable strategy. The claim still is made by some that it would take only a single individual with a tendency towards more selfish behaviour to undermine a population otherwise filled only with the gene for altruism towards non-kin.
Reception.
The book was extremely popular when first published, caused "a silent and almost immediate revolution in biology", and continues to be widely read. It has sold over a million copies, and has been translated into more than 25 languages. Proponents argue that the central point, that replicating the gene is the object of selection, usefully completes and extends the explanation of evolution given by Charles Darwin before the basic mechanisms of genetics were understood.
According to Alan Grafen, acceptance of adaptionist theories is hampered by a lack of a mathematical unifying theory and a belief that anything in words alone must be suspect. According to Grafen, these difficulties along with an initial conflict with population genetics models at the time of its introduction "explains why within biology the considerable scientific contributions it ["The Selfish Gene"] makes are seriously underestimated, and why it is viewed mainly as a work of exposition." According to comparative psychologist , "Dawkins presented a version of sociobiology that rested heavily on metaphors drawn from animal behavior, and extrapolated these...One of the weaknesses of the sociological approach is that it tends only to seek confirmatory examples from among the huge diversity of animal behavior. Dawkins did not deviate from this tradition." More generally, critics argue that "The Selfish Gene" oversimplifies the relationship between genes and the organism. (As an example, see Thompson.)
In 1976, Arthur Cain, one of Dawkins's tutors at Oxford in the 1960s, called it a "young man's book" (which Dawkins points out was a deliberate quote of a commentator on A.J. Ayer's "Language, Truth, and Logic"); Dawkins later noted he had been "flattered by the comparison, [but] knew that Ayer had recanted much of his first book and [he] could hardly miss Cain's pointed implication that [he] should, in the fullness of time, do the same." This point also was made by Mary Midgley: "The same thing happened to AJ Ayer, she says, but he spent the rest of his career taking back what he'd written in "Language, Truth and Logic". “This hasn't occurred to Dawkins”, she says. “He goes on saying the same thing.”" However, according to Wilkins and Hull, Dawkins' thinking has developed, although perhaps not defusing this criticism:
Units of selection.
As to the unit of selection: "One internally consistent logical picture is that the unit of replication is the gene...and the organism is one kind of ...entity on which selection acts directly." Dawkins proposed the matter without a distinction between 'unit of replication' and 'unit of selection' that he made elsewhere: "the fundamental unit of selection, and therefore of self-interest, is not the species, nor the group, nor even strictly the individual. It is the gene, the unit of heredity." However, he continues :
Dawkins' later formulation is in his book "The Extended Phenotype", where the process of selection is taken to involve every possible phenotypical effect of a gene.
Stephen Jay Gould finds Dawkins' position tries to have it both ways: 
The view of "The Selfish Gene" (often called "kin selection theory" or "inclusive fitness theory") is that selection based upon groups and populations is rare compared to selection on individuals. Although supported by Dawkins and by many others, this claim continues to be disputed. While naïve versions of group selectionism have been disproved, more sophisticated formulations make accurate predictions in some cases while positing selection at higher levels. Both sides agree that very favourable genes are likely to prosper and replicate if they arise and both sides agree that living in groups can be an advantage to the group members. The conflict arises in part over defining concepts: 
In "The Social Conquest of Earth," E. O. Wilson contends that although the selfish-gene approach was accepted "until 2010 [when] Martin Nowak, Corina Tarnita, and I demonstrated that inclusive fitness theory, often called kin selection theory, is both mathematically and biologically incorrect." Chapter 18 of "The Social Conquest of Earth" describes the deficiencies of kin selection and outlines group selection, which Wilson argues is a more realistic model of social evolution. He criticizes earlier approaches to social evolution, saying: "...unwarranted faith in the central role of kinship in social evolution has led to the reversal of the usual order in which biological research is conducted. The proven best way in evolutionary biology, as in most of science, is to define a problem arising during empirical research, then select or devise the theory that is needed to solve it. Almost all research in inclusive-fitness theory has been the opposite: hypothesize the key roles of kinship and kin selection, then look for evidence to test that hypothesis." According to Wilson: "People must have a tribe...Experiments conducted over many years by social psychologists have revealed how swiftly and decisively people divide into groups, and then discriminate in favor of the one to which they belong." (pp. 57, 59) According to Wilson: "Different parts of the brain have evolved by group selection to create groupishness." (p. 61)
Some authors consider facets of this debate between Dawkins and his critics about the level of selection to be blather:
Other authors say Dawkins has failed to make some critical distinctions, in particular, the difference between group selection for group advantage and group selection conveying individual advantage. Dawkins has raised the issue of the group as a type of 'vehicle', however, at least as a conceptual matter, although he has discounted its importance as discussed in the subsection Altruism explained.
Choice of words.
A good deal of objection to "The Selfish Gene" stemmed from its failure to be always clear about "selection" and "replication". As pointed out in the subsection "Units of selection", Dawkins says the gene is the fundamental unit of selection, and then points out that selection doesn't act directly upon the gene, but upon 'vehicles' or 'extended phenotypes'. Stephen Jay Gould took exception to calling the gene a 'unit of selection' because selection acted only upon phenotypes. Summarizing the Dawkins-Gould difference of view, Sterelny says: 
The word 'cause' here is a bit tricky: does a change in lottery rules (for example, inheriting a defective gene "responsible" for a disorder) 'cause' differences in outcome that might or might not occur? It certainly alters the likelihood of events, but a concatenation of contingencies decides what actually occurs. Dawkins thinks the use of 'cause' as a statistical weighting is acceptable, and it is a common though perhaps not a technical usage. Like Gould, Gabriel Dover in criticizing "The Selfish Gene" says: 
However, from a comparison with Dawkins' discussion of this very same point, it would seem both Gould's and Dover's comments are more a critique of his sloppy usage than a difference of views. Hull suggested a resolution based upon a distinction between replicators and interactors: The term 'replicator' includes genes as the most fundamental replicators but possibly other agents, and "interactor" includes organisms but maybe other agents, much as do Dawkins' 'vehicles'. The distinction is as follows: 
Hull suggests that, despite some similarities, Dawkins takes too narrow a view of these terms, and this engenders some of the objections to his views. According to Godfrey-Smith, this more careful vocabulary has cleared up "misunderstandings in the “units of selection” debates."
Enactive arguments.
Behavior genetics entertains the view:
This view from 1970 still is espoused today, and conflicts with Dawkins' view of "the gene as a form of "information [that] passes through bodies and affects them, but is not affected by them on its way through". The philosophical/biological field of enactivism stresses the interaction of the living agent with its environment and the relation of probing the environment to cognition and adaptation. Gene activation depends upon the cellular milieu. An extended discussion of the contrasts between enactivism and Dawkins' views, and with their support by Dennett, is provided by Thompson.
In "Mind in Life" philosopher Evan Thompson has assembled a multisourced objection to the "selfish gene" idea. Thompson takes issue with Dawkin's reduction of "life" to "genes" and "information":
Thompson objects that the gene cannot operate by itself, it requires an environment, a cell for instance, and life is "the creative outcome of highly structured contingencies". Thompson quotes Sarkar:
Thompson follows with a detailed examination of the concept of DNA as a look-up-table and the role of the cell in orchestrating the DNA-to-RNA transcription, indicating that by anyone's account the DNA is hardly the whole story. Thompson goes on to suggest that the cell-environment interrelationship has much to do with reproduction and inheritance, and a focus on the gene as a form of "information [that] passes through bodies and affects them, but is not affected by them on its way through" is tantamount to adoption of a form of material-informational dualism that has no explanatory value and no scientific basis. (Thomson, p. 187) The enactivist view, however, is that information results from the probing and experimentation of the agent with the agent's environment subject to the limitations of the agent's abilities to probe and process the result of probing, and DNA is simply one mechanism the agent brings to bear upon its activity.
Moral arguments.
Another criticism of the book is its treatment of morality, and more particularly altruism, as existing only as a form of selfishness:
Philosopher Mary Midgley has suggested this position is a variant of Hobbes' explanation of altruism as enlightened self-interest, and that Dawkins goes a step further to suggest that our genetic programming can be overcome by what amounts to an extreme version of free will. Part of Mary Midgley's concern is that Richard Dawkins' account of "The Selfish Gene" serves as a moral and ideological justification for selfishness to be adopted by modern human societies as simply following "nature", providing an excuse for behavior with bad consequences for future human society.
Dawkins' major concluding theme, that humanity is finally gaining power over the "selfish replicators" by virtue of their intelligence, is criticized also by primatologist Frans de Waal, who refers to it as an example of a "veneer theory" (the idea that morality is not fundamental, but is laid over a brutal foundation). Dawkins claims he merely describes how things are under evolution, and makes no moral arguments. On BBC-2 TV, Dawkins pointed to evidence for a "Tit-for-Tat" strategy (shown to be successful in game theory) as the most common, simple, and profitable choice.
More generally, the objection has been made that "The Selfish Gene" discusses philosophical and moral questions that go beyond biological arguments, relying upon anthropomorphisms and careless analogies.
Editions.
"The Selfish Gene" was first published in 1976 in eleven chapters with a preface by the author and a foreword by Robert Trivers. A second edition was published in 1989. This edition added two extra chapters, and substantial endnotes to the preceding chapters, reflecting new findings and thoughts. It also added a second preface by the author, but the original foreword by Trivers was dropped.
30th anniversary.
In 2006, a 30th anniversary edition was published which reinstated the Trivers foreword and contained a new introduction by the author (alongside the previous two prefaces), with some selected extracts from reviews at the back. It was accompanied by a "festschrift" entitled "". In March 2006, a special event entitled was held at the London School of Economics. The event was organised by Helena Cronin, and chaired by Melvyn Bragg. In March 2011, Audible Inc published an audiobook edition narrated by Richard Dawkins and Lalla Ward.
On-line access.
There are many versions of "The Selfish Gene" on line. Some have page numbers, some don't, and some are formatted and some are not. The various versions have different access to pages.

</doc>
<doc id="44191" url="http://en.wikipedia.org/wiki?curid=44191" title="D. H. Lawrence">
D. H. Lawrence

David Herbert Richards Lawrence (11 September 1885 – 2 March 1930) was an English novelist, poet, playwright, essayist, literary critic and painter who published as D. H. Lawrence. His collected works, among other things, represent an extended reflection upon the dehumanising effects of modernity and industrialisation. In them, some of the issues Lawrence explores are emotional health, vitality, spontaneity and instinct.
Lawrence's opinions earned him many enemies and he endured official persecution, censorship, and misrepresentation of his creative work throughout the second half of his life, much of which he spent in a voluntary exile which he called his "savage pilgrimage". At the time of his death, his public reputation was that of a pornographer who had wasted his considerable talents. E. M. Forster, in an obituary notice, challenged this widely held view, describing him as, "The greatest imaginative novelist of our generation." Later, the influential Cambridge critic F. R. Leavis championed both his artistic integrity and his moral seriousness, placing much of Lawrence's fiction within the canonical "great tradition" of the English novel.
Life and career.
Early life.
The fourth child of Arthur John Lawrence, a barely literate miner at Brinsley Colliery, and Lydia (née Beardsall), a former pupil teacher who, owing to her family's financial difficulties, had to do manual work in a lace factory, Lawrence spent his formative years in the coal mining town of Eastwood, Nottinghamshire. The house in which he was born, in Eastwood, 8a Victoria Street, is now the D. H. Lawrence Birthplace Museum. His working-class background and the tensions between his parents provided the raw material for a number of his early works. Lawrence would return to this locality and often wrote about nearby Underwood, calling it; "the country of my heart," as a setting for much of his fiction. Despite common misconception he is not related to T.E. Lawrence.
The young Lawrence attended Beauvale Board School (now renamed Greasley Beauvale D. H. Lawrence Primary School in his honour) from 1891 until 1898, becoming the first local pupil to win a County Council scholarship to Nottingham High School in nearby Nottingham. He left in 1901, working for three months as a junior clerk at Haywood's surgical appliances factory, but a severe bout of pneumonia ended this career. During his convalescence he often visited Hagg's Farm, the home of the Chambers family, and began a friendship with Jessie Chambers. An important aspect of this relationship with Chambers and other adolescent acquaintances was a shared love of books, an interest that lasted throughout Lawrence's life. In the years 1902 to 1906 Lawrence served as a pupil teacher at the British School, Eastwood. He went on to become a full-time student and received a teaching certificate from University College, Nottingham, in 1908. During these early years he was working on his first poems, some short stories, and a draft of a novel, "Laetitia", which was eventually to become "The White Peacock." At the end of 1907 he won a short story competition in the "Nottingham Guardian", the first time that he had gained any wider recognition for his literary talents.
Early career.
In the autumn of 1908 the newly qualified Lawrence left his childhood home for London. While teaching in Davidson Road School, Croydon, he continued writing. Some of the early poetry, submitted by Jessie Chambers, came to the attention of Ford Madox Ford, then known as Ford Hermann Hueffer and editor of the influential "The English Review". Hueffer then commissioned the story "Odour of Chrysanthemums" which, when published in that magazine, encouraged Heinemann, a London publisher, to ask Lawrence for more work. His career as a professional author now began in earnest, although he taught for another year. Shortly after the final proofs of his first published novel, "The White Peacock", appeared in 1910, Lawrence's mother died of cancer. The young man was devastated, and he was to describe the next few months as his "sick year." It is clear that Lawrence had an extremely close relationship with his mother, and his grief became a major turning point in his life, just as the death of Mrs. Morel is a major turning point in his autobiographical novel "Sons and Lovers", a work that draws upon much of the writer's provincial upbringing.
In 1911 Lawrence was introduced to Edward Garnett, a publisher's reader, who acted as a mentor, provided further encouragement, and became a valued friend, as did his son David. Throughout these months the young author revised "Paul Morel", the first draft of what became "Sons and Lovers". In addition, a teaching colleague, Helen Corke, gave him access to her intimate diaries about an unhappy love affair, which formed the basis of "The Trespasser", his second novel. In November 1911, he came down with a pneumonia again; once he recovered, Lawrence decided to abandon teaching in order to become a full-time author. He also broke off an engagement to Louie Burrows, an old friend from his days in Nottingham and Eastwood.
In March 1912 Lawrence met Frieda Weekley ("née" von Richthofen), with whom he was to share the rest of his life. Six years older than her new lover, she was married to Ernest Weekley, his former modern languages professor at University College, Nottingham, and had three young children. She eloped with Lawrence to her parents' home in Metz, a garrison town then in Germany near the disputed border with France. Their stay there included Lawrence's first encounter with tensions between Germany and France, when he was arrested and accused of being a British spy, before being released following an intervention from Frieda's father. After this incident, Lawrence left for a small hamlet to the south of Munich, where he was joined by Frieda for their "honeymoon", later memorialised in the series of love poems titled "Look! We Have Come Through" (1917). 1912 also saw the first of Lawrence's so-called "mining plays", "The Daughter-in-Law", written in Nottingham dialect. The play was never to be performed, or even published, in Lawrence's lifetime.
From Germany they walked southwards across the Alps to Italy, a journey that was recorded in the first of his travel books, a collection of linked essays titled "Twilight in Italy" and the unfinished novel, "Mr Noon". During his stay in Italy, Lawrence completed the final version of "Sons and Lovers" that, when published in 1913, was acknowledged to be a vivid portrait of the realities of working class provincial life. Lawrence, though, had become so tired of the work that he allowed Edward Garnett to cut about a hundred pages from the text.
Lawrence and Frieda returned to Britain in 1913 for a short visit, during which they encountered and befriended critic John Middleton Murry and New Zealand-born short story writer Katherine Mansfield. Lawrence was able to meet Welsh tramp poet W. H. Davies, whose work, much of which was inspired by nature, he greatly admired. Davies collected autographs, and was particularly keen to obtain Lawrence's. Georgian poetry publisher Edward Marsh was able to secure an autograph (probably as part of a signed poem), and invited Lawrence and Frieda to meet Davies in London on 28 July, under his supervision. Lawrence was immediately captivated by the poet and later invited Davies to join Frieda and himself in Germany. Despite his early enthusiasm for Davies' work, however, Lawrence's opinion changed after reading "Foliage" and he commented after reading "Nature Poems" in Italy that they seemed ".. so thin, one can hardly feel them".
Lawrence and Weekley soon went back to Italy, staying in a cottage in Fiascherino on the Gulf of Spezia. Here he started writing the first draft of a work of fiction that was to be transformed into two of his better-known novels, "The Rainbow" and "Women in Love". While writing "Women in Love" in Cornwall during 1916–17, Lawrence developed a strong and possibly romantic relationship with a Cornish farmer named William Henry Hocking. Although it is not absolutely clear if their relationship was sexual, Frieda said she believed it was. Lawrence's fascination with the theme of homosexuality, which is overtly manifested in "Women in Love", could be related to his own sexual orientation. In a letter written during 1913, he writes, "I should like to know why nearly every man that approaches greatness tends to homosexuality, whether he admits it or not ..." He is also quoted as saying, "I believe the nearest I've come to perfect love was with a young coal-miner when I was about 16."
Eventually, Frieda obtained her divorce. The couple returned to Britain shortly before the outbreak of World War I and were married on 13 July 1914. At this time, Lawrence worked with London intellectuals and writers such as Dora Marsden and the people involved with "The Egoist" (T. S. Eliot, Ezra Pound, and others). "The Egoist", an important Modernist literary magazine, published some of his work. He was also reading and adapting Marinetti's "Futurist Manifesto". He also met at this time the young Jewish artist Mark Gertler, and they became for a time good friends; Lawrence would describe Gertler's 1916 anti-war painting, "Merry-Go-Round" as 'the best "modern" picture I have seen: I think it is great and true.' Gertler would inspire the character Loerke (a sculptor) in "Women in Love". Weekley's German parentage and Lawrence's open contempt for militarism caused them to be viewed with suspicion in wartime Britain and to live in near destitution. "The Rainbow" (1915) was suppressed after an investigation into its alleged obscenity in 1915. Later, they were accused of spying and signalling to German submarines off the coast of Cornwall where they lived at Zennor. During this period he finished writing "Women in Love" in which he explored the destructive features of contemporary civilization through the evolving relationships of four major characters as they reflect upon the value of the arts, politics, economics, sexual experience, friendship and marriage. The novel is a bleak, bitter vision of humanity and proved impossible to publish in wartime conditions. Not published until 1920, it is now widely recognised as an English novel of great dramatic force and intellectual subtlety.
In late 1917, after constant harassment by the armed forces authorities, Lawrence was forced to leave Cornwall at three days' notice under the terms of the Defence of the Realm Act (DORA). This persecution was later described in an autobiographical chapter of his Australian novel "Kangaroo", published in 1923. He spent some months in early 1918 in the small, rural village of Hermitage near Newbury, Berkshire. He then lived for just under a year (mid-1918 to early 1919) at Mountain Cottage, Middleton-by-Wirksworth, Derbyshire, where he wrote one of his most poetic short stories, "The White Peacock". Until 1919 he was compelled by poverty to shift from address to address and barely survived a severe attack of influenza.
Exile.
After the traumatic experience of the war years, Lawrence began what he termed his 'savage pilgrimage', a time of voluntary exile. He escaped from Britain at the earliest practical opportunity, to return only twice for brief visits, and with his wife spent the remainder of his life travelling. This wanderlust took him to Australia, Italy, Ceylon (now called Sri Lanka), the United States, Mexico and the South of France.
Lawrence abandoned Britain in November 1919 and headed south, first to the Abruzzi region in central Italy and then onwards to Capri and the Fontana Vecchia in Taormina, Sicily. From Sicily he made brief excursions to Sardinia, Monte Cassino, Malta, Northern Italy, Austria and Southern Germany. Many of these places appeared in his writings. New novels included "The Lost Girl" (for which he won the James Tait Black Memorial Prize for fiction), "Aaron's Rod" and the fragment titled "Mr Noon" (the first part of which was published in the Phoenix anthology of his works, and the entirety in 1984). He experimented with shorter novels or novellas, such as "The Captain's Doll," "The Fox" and "The Ladybird." In addition, some of his short stories were issued in the collection "England, My England and Other Stories." During these years he produced a number of poems about the natural world in "Birds, Beasts and Flowers." Lawrence is widely recognised as one of the finest travel writers in the English language. "Sea and Sardinia," a book that describes a brief journey undertaken in January 1921, is a recreation of the life of the inhabitants of Sardinia. Less well known is the memoir of Maurice Magnus, "Memoirs of the Foreign Legion", in which Lawrence recalls his visit to the monastery of Monte Cassino. Other non-fiction books include two responses to Freudian psychoanalysis and "Movements in European History," a school textbook that was published under a pseudonym, a reflection of his blighted reputation in Britain.
Later life and career.
In late February 1922 the Lawrences left Europe behind with the intention of migrating to the United States. They sailed in an easterly direction, first to Ceylon and then on to Australia. A short residence in Darlington, Western Australia, which included an encounter with local writer Mollie Skinner, was followed by a brief stop in the small coastal town of Thirroul, New South Wales, during which Lawrence completed "Kangaroo," a novel about local fringe politics that also revealed a lot about his wartime experiences in Cornwall.
The Lawrences finally arrived in the US in September 1922. Here they encountered Mabel Dodge Luhan, a prominent socialite, and considered establishing a utopian community on what was then known as the 160 acre Kiowa Ranch near Taos, New Mexico. After arriving in Lamy, New Mexico via train, they bought the property, now called the D. H. Lawrence Ranch, in 1924, in exchange for the manuscript of "Sons and Lovers". He stayed in New Mexico for two years, with extended visits to Lake Chapala and Oaxaca in Mexico. While Lawrence was in New Mexico, he was visited by Aldous Huxley.
While in the U.S., Lawrence rewrote and published "Studies in Classic American Literature", a set of critical essays begun in 1917, and later described by Edmund Wilson as "one of the few first-rate books that have ever been written on the subject." These interpretations, with their insights into symbolism, New England Transcendentalism and the puritan sensibility, were a significant factor in the revival of the reputation of Herman Melville during the early 1920s. In addition, Lawrence completed a number of new fictional works, including "The Boy in the Bush", "The Plumed Serpent", "St Mawr", "The Woman who Rode Away", "The Princess" and assorted short stories. He also found time to produce some more travel writing, such as the collection of linked excursions that became "Mornings in Mexico."
A brief voyage to England at the end of 1923 was a failure and he soon returned to Taos, convinced that his life as an author now lay in America. However, in March 1925 he suffered a near fatal attack of malaria and tuberculosis while on a third visit to Mexico. Although he eventually recovered, the diagnosis of his condition obliged him to return once again to Europe. He was dangerously ill and the poor health limited his ability to travel for the remainder of his life. The Lawrences made their home in a villa in Northern Italy, living near Florence while he wrote "The Virgin and the Gipsy" and the various versions of "Lady Chatterley's Lover" (1928). The latter book, his last major novel, was initially published in private editions in Florence and Paris and reinforced his notoriety. Lawrence responded robustly to those who claimed to be offended, penning a large number of satirical poems, published under the title of "Pansies" and "Nettles", as well as a tract on "Pornography and Obscenity".
The return to Italy allowed Lawrence to renew old friendships; during these years he was particularly close to Aldous Huxley, who was to edit the first collection of Lawrence's letters after his death, along with a memoir. With artist Earl Brewster, Lawrence visited a number of local archaeological sites in April 1927. The resulting essays describing these visits to old tombs were written up and collected together as "Sketches of Etruscan Places," a book that contrasts the lively past with Benito Mussolini's fascism. Lawrence continued to produce fiction, including short stories and "The Escaped Cock" (also published as "The Man Who Died"), an unorthodox reworking of the story of Jesus Christ's Resurrection. During these final years Lawrence renewed a serious interest in oil painting. Official harassment persisted and an exhibition of some of these pictures at the Warren Gallery in London was raided by the police in mid 1929 and a number of works were confiscated.
Death.
Lawrence continued to write despite his failing health. In his last months he wrote numerous poems, reviews and essays, as well as a robust defence of his last novel against those who sought to suppress it. His last significant work was a reflection on the Book of Revelation, "Apocalypse". After being discharged from a sanatorium, he died 2 March 1930 at the Villa Robermond in Vence, France, from complications of tuberculosis. Frieda Weekley commissioned an elaborate headstone for his grave bearing a mosaic of his adopted emblem of the phoenix. After Lawrence's death, Frieda lived with Angelo Ravagli on the ranch in Taos and eventually married him in 1950. In 1935 Ravaglio arranged, on Frieda's behalf, to have Lawrence's body exhumed and cremated and his ashes brought back to the ranch to be interred there in a small chapel amid the mountains of New Mexico.
Philosophy, religion and politics.
Critic Terry Eagleton situates Lawrence on the radical right wing, as hostile to democracy, liberalism, socialism, and egalitarianism, though never formally embracing fascism, as he died before it reached its zenith. Lawrence's opinion of the masses is discussed in detail by Professor John Carey in "The Intellectuals and the Masses" (1992), and he quotes a 1908 letter from Lawrence to Blanche Jennings:
If I had my way, I would build a lethal chamber as big as the Crystal Palace, with a military band playing softly, and a Cinematograph working brightly; then I'd go out in the back streets and main streets and bring them in, all the sick, the halt, and the maimed; I would lead them gently, and they would smile me a weary thanks; and the band would softly bubble out the "Hallelujah Chorus".
More of Lawrence's political ideas can be seen in his letters to Bertrand Russell around the year 1915, where he voices his opposition to enfranchising the working class and his hostility to the burgeoning labour movements, and disparages the French Revolution, referring to "Liberty, Equality, and Fraternity" as the "three-fanged serpent". Rather than a republic, Lawrence called for an absolute Dictator and equivalent Dictatrix to lord over the lower peoples. Earlier, Harrison had drawn attention to the vein of sadism that runs through Lawrence's writing.
Written works.
Novels.
Lawrence is perhaps best known for his novels "Sons and Lovers", "The Rainbow", "Women in Love" and "Lady Chatterley's Lover". Within these Lawrence explores the possibilities for life within an industrial setting. In particular Lawrence is concerned with the nature of relationships that can be had within such a setting. Though often classed as a realist, Lawrence in fact uses his characters to give form to his personal philosophy. His depiction of sexual activity, though seen as shocking when he first published in the early 20th century, has its roots in this highly personal way of thinking and being. It is worth noting that Lawrence was very interested in the sense of touch and that his focus on physical intimacy has its roots in a desire to restore an emphasis on the body, and re-balance it with what he perceived to be Western civilisation's over-emphasis on the mind.
In his later years Lawrence developed the potentialities of the short novel form in "St Mawr", "The Virgin and the Gypsy" and "The Escaped Cock".
Short stories.
Lawrence's best-known short stories include "The Captain's Doll", "The Fox", "The Ladybird", "Odour of Chrysanthemums", "The Princess", "The Rocking-Horse Winner", "St Mawr", "The Virgin and the Gypsy" and "The Woman who Rode Away". ("The Virgin and the Gypsy" was published as a novella after he died.) Among his most praised collections is "The Prussian Officer and Other Stories", published in 1914. His collection "The Woman Who Rode Away and Other Stories", published in 1928, develops the theme of leadership that Lawrence also explored in novels such as "Kangaroo, The Plumed Serpent" and "Fanny and Annie".
Poetry.
Although best known for his novels, Lawrence wrote almost 800 poems, most of them relatively short. His first poems were written in 1904 and two of his poems, "Dreams Old" and "Dreams Nascent", were among his earliest published works in "The English Review". His early works clearly place him in the school of Georgian poets, a group not only named after the reigning monarch but also to the romantic poets of the previous Georgian period whose work they were trying to emulate. What typified the entire movement, and Lawrence's poems of the time, were well-worn poetic tropes and deliberately archaic language. Many of these poems displayed what John Ruskin referred to as the pathetic fallacy, the tendency to ascribe human emotions to animals and even inanimate objects.
Just as the First World War dramatically changed the work of many of the poets who saw service in the trenches, Lawrence's own work saw a dramatic change, during his years in Cornwall. During this time, he wrote free verse influenced by Walt Whitman. He set forth his manifesto for much of his later verse in the introduction to "New Poems". "We can get rid of the stereotyped movements and the old hackneyed associations of sound or sense. We can break down those artificial conduits and canals through which we do so love to force our utterance. We can break the stiff neck of habit […] But we cannot positively prescribe any motion, any rhythm."
Lawrence rewrote many of his novels several times to perfect them and similarly he returned to some of his early poems when they were collected in 1928. This was in part to fictionalise them, but also to remove some of the artifice of his first works. As he put in himself: "A young man is afraid of his demon and puts his hand over the demon's mouth sometimes and speaks for him." His best-known poems are probably those dealing with nature such as those in the collection "Birds, Beasts and Flowers", including the Tortoise poems, and "Snake", one of his most frequently anthologised, displays some of his most frequent concerns; those of man's modern distance from nature and subtle hints at religious themes.
<poem>
In the deep, strange-scented shade of the great dark carob tree
I came down the steps with my pitcher
And must wait, must stand and wait, for there he was at the trough before me.
</poem>
"Look! We have come through!" is his other work from the period of the end of the war and it reveals another important element common to much of his writings; his inclination to lay himself bare in his writings. Although Lawrence could be regarded as a writer of love poems, his usually deal in the less romantic aspects of love such as sexual frustration or the sex act itself. Ezra Pound in his "Literary Essays" complained of Lawrence's interest in his own "disagreeable sensations" but praised him for his "low-life narrative." This is a reference to Lawrence's dialect poems akin to the Scots poems of Robert Burns, in which he reproduced the language and concerns of the people of Nottinghamshire from his youth.
<poem>
Tha thought tha wanted ter be rid o' me.
'Appen tha did, an' a'.
Tha thought tha wanted ter marry an' se
If ter couldna be master an' th' woman's boss,
Tha'd need a woman different from me,
An' tha knowed it; ay, yet tha comes across
Ter say goodbye! an' a'.
</poem>
Although Lawrence's works after his Georgian period are clearly in the modernist tradition, they were often very different from those of many other modernist writers, such as Pound. Pound's poems were often austere, with every word carefully worked on. Lawrence felt all poems had to be personal sentiments, and that a sense of spontaneity was vital. He called one collection of poems "Pansies", partly for the simple ephemeral nature of the verse, but also as a pun on the French word "panser", to dress or bandage a wound. "Pansies", as he made explicit in the introduction to "New Poems", is also a pun on Blaise Pascal's "Pensées". "The Noble Englishman" and "Don't Look at Me" were removed from the official edition of "Pansies" on the grounds of obscenity, which wounded him. Even though he lived most of the last ten years of his life abroad, his thoughts were often still on England. Published in 1930, just eleven days after his death, his last work "Nettles" was a series of bitter, nettling but often wry attacks on the moral climate of England.
<poem>
O the stale old dogs who pretend to guard
the morals of the masses,
how smelly they make the great back-yard
wetting after everyone that passes.
</poem>
Two notebooks of Lawrence's unprinted verse were posthumously published as "Last Poems" and "More Pansies". These contain two of Lawrence's most famous poems about death, "Bavarian Gentians" and "The Ship of Death".
Literary criticism.
Lawrence's criticism of other authors often provides insight into his own thinking and writing. Of particular note is his "Study of Thomas Hardy and Other Essays". In "Studies in Classic American Literature" Lawrence's responses to writers like Walt Whitman, Herman Melville and Edgar Allan Poe also shed light on his craft.
Lady Chatterley trial.
A heavily censored abridgement of "Lady Chatterley's Lover" was published in the United States by Alfred A. Knopf in 1928. This edition was posthumously re-issued in paperback there both by Signet Books and by Penguin Books in 1946. When the full unexpurgated edition of Lady Chatterley's Lover was published by Penguin Books in Britain in 1960, the trial of Penguin under the Obscene Publications Act of 1959 became a major public event and a test of the new obscenity law. The 1959 act (introduced by Roy Jenkins) had made it possible for publishers to escape conviction if they could show that a work was of literary merit. One of the objections was to the frequent use of the word "fuck" and its derivatives and the word "cunt".
Various academic critics and experts of diverse kinds, including E. M. Forster, Helen Gardner, Richard Hoggart, Raymond Williams and Norman St John-Stevas, were called as witnesses, and the verdict, delivered on 2 November 1960, was "not guilty". This resulted in a far greater degree of freedom for publishing explicit material in the UK. The prosecution was ridiculed for being out of touch with changing social norms when the chief prosecutor, Mervyn Griffith-Jones, asked if it were the kind of book "you would wish your wife or servants to read".
The Penguin second edition, published in 1961, contains a publisher's dedication, which reads: "For having published this book, Penguin Books were prosecuted under the Obscene Publications Act, 1959 at the Old Bailey in London from 20 October to 2 November 1960. This edition is therefore dedicated to the twelve jurors, three women and nine men, who returned a verdict of 'Not Guilty' and thus made D. H. Lawrence's last novel available for the first time to the public in the United Kingdom."
Posthumous reputation.
The obituaries shortly after Lawrence's death were, with the notable exception of E. M. Forster, unsympathetic or hostile. However, there were those who articulated a more favourable recognition of the significance of this author's life and works. For example, his longtime friend Catherine Carswell summed up his life in a letter to the periodical "Time and Tide" published on 16 March 1930. In response to his critics, she claimed:
Aldous Huxley also defended Lawrence in his introduction to a collection of letters published in 1932. However, the most influential advocate of Lawrence's contribution to literature was the Cambridge literary critic F. R. Leavis who asserted that the author had made an important contribution to the tradition of English fiction. Leavis stressed that "The Rainbow", "Women in Love", and the short stories and tales were major works of art. Later, the Lady Chatterley Trial of 1960, and subsequent publication of the book, ensured Lawrence's popularity (and notoriety) with a wider public.
Lawrence held seemingly contradictory views of feminism. The evidence of his written works indicates an overwhelming commitment to representing women as strong, independent and complex; he produced major works in which young, self-directing female characters were central. A number of feminist critics, notably Kate Millett, have criticised, indeed ridiculed Lawrence's sexual politics, Millett claiming that he uses his female characters as mouthpieces to promote his creed of male supremacy. This damaged his reputation in some quarters, although Norman Mailer came to Lawrence's defence in "The Prisoner of Sex" in 1971. Yet Lawrence continues to find an audience, and the ongoing publication of a new scholarly edition of his letters and writings has demonstrated the range of his achievement.
Painting.
D. H. Lawrence had a lifelong interest in painting, which became one of his main forms of expression in his last years. His paintings were exhibited at the Warren Gallery in London's Mayfair in 1929. The exhibition was extremely controversial, with many of the 13,000 people visiting mainly to gawk. The "Daily Express" claimed, ""Fight with an Amazon" represents a hideous, bearded man holding a fair-haired woman in his lascivious grip while wolves with dripping jaws look on expectantly, [this] is frankly indecent". But several artists and art experts praised the paintings. Gwen John, reviewing the exhibition in "Everyman", spoke of Lawrence's "stupendous gift of self-expression" and singled out "The Finding of Moses", "Red Willow Trees" and "Boccaccio Story" as "pictures of real beauty and great vitality". Others singled out "Contadini" for special praise. After a complaint, the police seized thirteen of the twenty-five paintings (including "Boccaccio Story" and "Contadini"). Despite declarations of support from many writers, artists and members of Parliament, Lawrence was able to recover his paintings only by agreeing never to exhibit them in England again. The largest collection of the paintings is now at La Fonda de Taos hotel in Taos, New Mexico. Several others, including "Boccaccio Story" and "Resurrection", are at the Humanities Research Centre of the University of Texas at Austin.
Works.
Paintings.
</dl>
Further reading.
Literary criticism.
</dl>

</doc>
<doc id="44193" url="http://en.wikipedia.org/wiki?curid=44193" title="DocBook">
DocBook

DocBook is a semantic markup language for technical documentation. It was originally intended for writing technical documents related to computer hardware and software but it can be used for any other sort of documentation.
As a semantic language, DocBook enables its users to create document content in a presentation-neutral form that captures the logical structure of the content; that content can then be published in a variety of formats, including HTML, XHTML, EPUB, PDF, man pages, Web help and HTML Help, without requiring users to make any changes to the source. In other words, when a document is written in DocBook format it becomes easily portable into other formats. It solves the problem of reformatting by writing it once using XML tags.
Overview.
DocBook is an XML language. In its current version (5.x), DocBook's language is formally defined by a RELAX NG schema with integrated Schematron rules. (There are also W3C XML Schema+Schematron and Document Type Definition (DTD) versions of the schema available, but these are considered non-standard.)
As a semantic language, DocBook documents do not describe what their contents "look like", but rather the "meaning" of those contents. For example, rather than explaining how the abstract for an article might be visually formatted, DocBook simply says that a particular section "is" an abstract. It is up to an external processing tool or application to decide where on a page the abstract should go and what it should look like or whether or not it should be included in the final output at all.
DocBook provides a vast number of semantic element tags. They are divided into three broad categories: structural, block-level, and inline. 
Structural tags specify broad characteristics of their contents. The book element, for example, specifies that its child elements represent the parts of a book. This includes a title, chapters, glossaries, appendices, and so on. DocBook's structural tags include, but are not limited to:
Structural elements can contain other structural elements. Structural elements are the only permitted top-level elements in a DocBook document. 
Block-level tags are elements like paragraph, lists, etc. Not all these elements can directly contain text. Sequential block-level elements render one "after" another. After, in this case, can differ depending on the language. In most Western languages, "after" means below: text paragraphs are printed down the page. Other languages' writing systems can have different directionality; for example, in Japanese, paragraphs are often printed in downward columns, with the columns running from right to left, so "after" in that case would be to the left. DocBook semantics are entirely neutral to these kinds of language-based concepts.
Inline-level tags are elements like emphasis, hyperlinks, etc. They wrap text within a block-level element. These elements do not cause the text to break when rendered in a paragraph format, but typically they cause the document processor to apply some kind of distinct typographical treatment to the enclosed text, by changing the font, size, or similar attributes. (The DocBook specification "does" say that it expects different typographical treatment, but it does not offer specific requirements as to what this treatment may be.) That is, a DocBook processor doesn't have to transform an emphasis tag into "italics". A reader-based DocBook processor could increase the size of the words, or, a text-based processor could use bold instead of italics.
Sample document.
Semantically, this document is a "book", with a "title", that contains two "chapters" each with their own "titles". Those "chapters" contain "paragraphs" that have text in them. The markup is fairly readable in English.
In more detail, the root element of the document is book. All DocBook elements are in an XML Namespace, so the root element has an "xmlns" attribute to set the current namespace. Also, the root element of a DocBook document must have a "version" that specifies the version of the format that the document is built on.
A book element must contain a title, or an info element containing a title. This must be before any child structural elements. Following the title are the structural children, in this case, two chapter elements. Each of these must have a title. They contain para block elements, which can contain free text and other inline elements like the emphasis in the second paragraph of the first chapter.
Schemas and validation.
Rules such as the ones alluded to in the preceding paragraph ("a book element must contain a title, or an info element containing a title," etc.) are formally defined in the DocBook "schema". Appropriate programming tools can "validate" an XML document (DocBook or otherwise), against its corresponding schema, to determine if (and where) the document fails to conform to that schema. XML editing tools can also use schema information to avoid creating non-conforming documents in the first place.
DocBook authoring.
Because DocBook is XML, documents can be created and edited with any text editor. A dedicated XML editor is likewise a functional DocBook editor. DocBook provides schema files for popular XML schema languages, so any XML editor that can provide content completion based on a schema can do so for DocBook. Many graphical or WYSIWYG XML editors come with the ability to edit DocBook like a word processor. 
DocBook processing.
Because DocBook is an XML format, conforming to a well-defined schema, documents can be validated and processed using any tool or programming language that includes XML support.
DocBook files are used to prepare output files in a wide variety of formats. Nearly always, this is accomplished using DocBook XSL stylesheets. These are XSLT stylesheets that transform DocBook documents into a number of formats (HTML, XSL-FO for later conversion into PDF, etc.). These stylesheets can be sophisticated enough to generate tables of contents, glossaries, and indexes. They can oversee the selection of particular designated portions of a master document to produce different versions of the same document (such as a "tutorial" or a "quick-reference guide", where each of these consist of a subset of the material).
Because the standard DocBook XSL stylesheets "are" well-formed XSL stylesheets, and DocBook "is" well-formed XML, users can write their own customized stylesheets or even a full-fledged program to process the DocBook into an appropriate output format as their needs dictate.
Web help.
Web help is a chunked HTML output format in the DocBook XSL stylesheets that was introduced in version 1.76.1. The documentation for web help also provides an example of web help and is part of the DocBook XSL distribution. Its major features include CSS-based page layout without frameset, multilingual full content search, table of contents (TOC) pane with collapsible TOC tree, auto-synchronization of content pane and TOC. This web help format was originally implemented by Kasun Gajasinghe as part of the Google Summer of Code 2010 program.
History.
DocBook began in 1991 in discussion groups on Usenet and eventually became a joint project of HAL Computer Systems and O'Reilly & Associates and eventually spawned its own maintenance organization (the Davenport Group) before moving in 1998 to the "SGML Open" consortium, which subsequently became OASIS. DocBook is currently maintained by the "DocBook Technical Committee" at OASIS.
DocBook is available in both SGML and XML forms, as a DTD. RELAX NG and W3C XML Schema forms of the XML version are available. Starting with DocBook 5, the RELAX NG version is the "normative" form from which the other formats are generated.
DocBook originally started out as an SGML application, but an equivalent XML application was developed and has now replaced the SGML one for most uses. (Starting with version 4 of the SGML DTD, the XML DTD continued with this version numbering scheme.) Initially, a key group of software companies used DocBook since their representatives were involved in its initial design. Eventually, however, DocBook was adopted by the open source community where it has become a standard for creating documentation for many projects, including FreeBSD, KDE, GNOME desktop documentation, the GTK+ API references, the Linux kernel documentation, and the work of the Linux Documentation Project.
Norman Walsh and the DocBook Project development team maintain the key application for producing output from DocBook source documents: A set of XSL stylesheets (as well as a legacy set of DSSSL stylesheets) that can generate high-quality HTML and print (FO/PDF) output, as well as output in other formats, including RTF, man pages and HTML Help.
Walsh is also the principal author of the book , the official documentation of DocBook. This book is available online under the GFDL, and also as a print publication.
Pre DocBook v5.0.
The current version of DocBook, 5.1, is fairly recent. Prior versions have been and still are in widespread use, so this section provides an overview of the changes to the older 4.x formats.
Until DocBook 5, DocBook was defined normatively by a Document Type Definition (DTD). Because DocBook was built originally as an application of SGML, the DTD was the only available schema language. DocBook 4.x formats can be SGML or XML, but the XML version does not have its own namespace.
DocBook 4.x formats had to live within the restrictions of being defined by a DTD. The most significant restriction was that an element name uniquely defines its possible contents. That is, an element named info must contain the same information no matter where it is in the DocBook file. As such, there are many kinds of info elements in DocBook 4.x: bookinfo, chapterinfo, etc. Each has a slightly different content model, but they do share some of their content model. Additionally, they repeat context information. The book's info element is that, because it is a direct child of the book; it does not need to be named specially for a human reader. However, because the format was defined by a DTD, it did have to be named as such. The root element does not have or need a "version", as the version is built into the DTD declaration at the top of a pre-DocBook 5 document.
DocBook 4.x documents are not compatible with DocBook 5, but can be converted into DocBook 5 documents via an XSLT stylesheet. One (codice_1) is provided as part of the distribution of the DocBook 5 schema and specification package.
Simplified DocBook.
DocBook offers a large number of features that may be overwhelming to a new user. For those who want the convenience of DocBook without a steep learning curve, Simplified DocBook was designed. It is a small subset of DocBook designed for single documents such as articles or white papers (i.e., "books" are not supported). The Simplified DocBook DTD is currently at version 1.1.
Further reading.
</dl>

</doc>
<doc id="44195" url="http://en.wikipedia.org/wiki?curid=44195" title="Friends of the Earth">
Friends of the Earth

Friends of the Earth International (FoEI) is an international network of environmental organizations in 74 countries.
Friends of the Earth was founded in 1969 as an anti-nuclear group by Robert O Anderson who contributed $200,000 in personal funds to launch FOTE with David Brower, Donald Aitken and Jerry Mander after Brower's split with the Sierra Club. FOTE main mission was to lock up and prevent further development of nuclear energy.
Their first employee was Amory Lovins, who kicked off FOE in the UK. It became an international network in 1971 with a meeting of representatives from the U.S., Sweden, the UK and France. For further historical details, see articles on the national FOE organizations.
FoEI is assisted by a small secretariat (based in Amsterdam, Netherlands) which provides support for the network and its agreed major campaigns. The executive committee of elected representatives from national groups sets policy and oversees the work of the secretariat. In 2010, Nigerian activist Nnimmo Bassey was elected to serve as chair of Friends of the Earth International.
Campaign issues.
Friends of the Earth considers environmental issues in their social, political and human rights contexts. Their campaigns stretch beyond the traditional arena of the conservation movement and seek to address the economic and development aspects of sustainability. Originally based largely in North America and Europe, its membership is now heavily weighted toward groups in the developing world.
The current campaign priorities of Friends of the Earth internationally are:
The campaign priorities are set at the bi-annual general meeting of Friends of the Earth International.
In addition to the priority campaign areas Friends of the Earth International has a number of other campaign areas which are active internationally. They include:
All FoE International campaigns incorporate elements of three core themes which are:
Friends of the Earth groups.
The Friends of the Earth in each country are themselves many-tiered networks reaching from individual activists up to the national pressure group which campaigns for environmentally progressive and sustainable policies. The groups and activists at all levels also carry out educational and research activities.
Friends of the Earth groups are required to act independently of party political, religious or other influences; be open, democratic and non-discriminatory in their internal structures; and be willing to cooperate with other organizations who are working for the same goals. These are conditions of remaining a member of FOEI.
The national groups work on the main issues affecting their own country and choose to participate in the international campaigns of FoEI which are relevant to them. In turn, the local campaigners can work on local, national and/or international campaigns.
Structure of the network.
The member organization in a particular country may name itself Friends of the Earth or an equivalent translated phrase in the national language, e.g., Friends of the Earth (US), Friends of the Earth (EWNI) (England Wales and Northern Ireland), Amigos de la Tierra (Spain and Argentina). However, roughly half of the member groups work under their own names, sometimes reflecting an independent origin and subsequent accession to the network, such as Korean Federation for Environmental Movement (KFEM), Environmental Rights Action (FOE Nigeria) and WALHI (FOE Indonesia).
Friends of the Earth International (FoEI) is supported by a secretariat based in Amsterdam, and an executive committee known as ExCom. The ExCom is elected by all member groups at a general meeting held every two years, and it is the ExCom which employs the secretariat. At the same general meeting, overall policies and priority activities are agreed.
In addition to work which is coordinated at the FoEI level, national member groups are free to carry out their own campaigns and to work bi- or multi-laterally as they see fit, as long as this does not go against agreed policy at the international level.
Publications.
The "Meat Atlas" is an annual report on the methods and impact of industrial animal agriculture. The publication consists of 27 short essays and, with the help of graphs, visualises facts about the production and consumption of meat. The Meat Atlas is jointly published by Friends of the Earth and Heinrich Böll Foundation.
Notable supporters.
Support for The Big Ask.
Among those present at the launch of Friends of the Earth (EWNI)'s climate change campaign The Big Ask were: Jude Law, Edith Bowman, Sian Lloyd, Ross Burden, David Cameron, David Miliband, Thom Yorke, Stephen Merchant, Michael Eavis, and Emily Eavis.

</doc>
<doc id="44199" url="http://en.wikipedia.org/wiki?curid=44199" title="Document Style Semantics and Specification Language">
Document Style Semantics and Specification Language

The Document Style Semantics and Specification Language (DSSSL) is an international standard developed to provide a stylesheets for SGML documents.
DSSSL consists of two parts, a tree transformation process that can be used to manipulate the tree structure of documents prior to presentation, and a formatting process that associates the elements in the source document with specific nodes in the target representation — the flow object tree. DSSSL specifications are device-independent pieces of information that can be interchanged between different platforms. The back-end formatters needed to generate the final form of the document (e.g. PostScript or Rich Text Format, or a presentation on a computer display) are not standardized by DSSSL.
Based on a subset of the Scheme programming language, it is specified by the standard ISO/IEC 10179:1996. It was developed by ISO/IEC JTC1/SC34 (ISO/IEC Joint Technical Committee 1, Subcommittee 34 - Document description and processing languages).
SGML contains information in a machine-readable but not very human-readable format. A "stylesheet" is used to present the information stored in SGML in a more pleasing or accessible way. DSSSL can convert to a wide range of formats, including RTF, HTML, and LaTeX.
Although compatible with any SGML, DSSSL was most often used with DocBook. In 1997, a syntax highlighting language definition for KEDIT was published. 
With the appearance of XML as an alternative to SGML, XML's associated stylesheet language XSL was also widely and rapidly adopted, from around 1999. Although DSSSL continued in use within the shrinking SGML field, XSL was very soon in use more extensively, and by more coders, than DSSSL had ever achieved. This was emphasised when previous SGML strongholds such as DocBook converted from SGML to XML, and also converted their favoured stylesheet language from DSSSL to XSL.
DSSSL was thought to be too complex for the World Wide Web, and the World Wide Web Consortium thought about creating a "DSSSL-Lite".

</doc>
<doc id="44202" url="http://en.wikipedia.org/wiki?curid=44202" title="Statecraft">
Statecraft

Statecraft may refer to:

</doc>
