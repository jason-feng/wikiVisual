<doc id="55974" url="http://en.wikipedia.org/wiki?curid=55974" title="U.S. Federal Communications Commission">
U.S. Federal Communications Commission

The Federal Communications Commission (FCC) is an independent agency of the United States government, created by Congressional statute (see #redirect and #redirect ) to regulate interstate communications by radio, television, wire, satellite, and cable in all 50 states, the District of Columbia and U.S. territories. The FCC works towards six goals in the areas of broadband, competition, the spectrum, the media, public safety and homeland security. The Commission is also in the process of modernizing itself.
The FCC was formed by the Communications Act of 1934 to replace the radio regulation functions of the Federal Radio Commission. The FCC took over wire communication regulation from the Interstate Commerce Commission. The FCC's mandated jurisdiction covers the 50 states, the District of Columbia, and U.S. possessions. The FCC also provides varied degrees of cooperation, oversight, and leadership for similar communications bodies in other countries of North America. The FCC is funded entirely by regulatory fees. It has an estimated fiscal-2011 budget of US$335.8 million and a proposed fiscal-2012 budget of $354.2 million. It has 1,720 federal employees.
Mission and strategy.
The FCC's mission, specified in Section One of the Communications Act of 1934 and amended by the Telecommunications Act of 1996 (amendment to 47 U.S.C. §151) is to "make available so far as possible, to all the people of the United States, without discrimination on the basis of race, color, religion, national origin, or sex, rapid, efficient, Nation-wide, and world-wide wire and radio communication services with adequate facilities at reasonable charges." The Act furthermore provides that the FCC was created "for the purpose of the national defense" and "for the purpose of promoting safety of life and property through the use of wire and radio communications."
Consistent with the objectives of the Act as well as the 1993 Government Performance and Results Act (GPRA), the FCC has identified six goals in its 2006–2011 Strategic Plan. These are:
Organization.
Commissioners.
The FCC is directed by five commissioners appointed by the President of the United States and confirmed by the U.S. Senate for five-year terms, except when filling an unexpired term. The President designates one of the commissioners to serve as chairman. Only three commissioners may be members of the same political party. None of them may have a financial interest in any FCC-related business.
Bureaus.
The FCC is organized into seven Bureaus, which process applications for licenses and other filings, analyze complaints, conduct investigations, develop and implement regulations, and participate in hearings.
Offices.
The FCC has eleven Staff Offices.
The FCC's Offices provide support services to the Bureaus. 
History.
Communications Act of 1934.
In 1934, Congress passed the Communications Act, which abolished the Federal Radio Commission and transferred jurisdiction over radio licensing to a new Federal Communications Commission, including in it also the telecommunications jurisdiction previously handled by the Interstate Commerce Commission. Title II of the Communications Act focused on telecommunications using many concepts borrowed from railroad legislation and Title III contained provisions very similar to the Radio Act of 1927.
Report on Chain Broadcasting.
In 1940, the Federal Communications Commission issued the "Report on Chain Broadcasting" which was led by new FCC Chairman James Lawrence Fly. The major point in the report was the breakup of NBC (National Broadcasting Company), which ultimately led to the creation of ABC (American Broadcasting Company), but there were two other important points. One was network option time, the culprit here being CBS. The report limited the amount of time during the day, and what times the networks may broadcast. Previously a network could demand any time it wanted from an affiliate. The second concerned artist bureaus. The networks served as both agents and employers of artists, which was a conflict of interest the report rectified.
Freeze of 1948.
In assigning television stations to various cities after World War II, the FCC found that it placed many stations too close to each other, resulting in interference. At the same time, it became clear that the designated VHF channels, 2 through 13, were inadequate for nationwide television service. As a result, the FCC stopped giving out construction permits for new licenses in October 1948. Most expected this "Freeze" to last six months, but as the allocation of channels to the emerging UHF technology and the eagerly awaited possibilities of color television were debated, the FCC's re-allocation map of stations did not come until April 1952, with July 1, 1952 as the official beginning of licensing new stations.
Other FCC actions hurt the fledgling DuMont and ABC networks. AT&T Corporation forced television coaxial cable users to rent additional radio long lines, discriminating against DuMont, which had no radio network operation. DuMont and ABC protested AT&T's television policies to the FCC, which regulated AT&T's long-line charges, but the commission took no action. The result was that financially marginal DuMont was spending as much in long-line charge as CBS or NBC while using only about 10 to 15 percent of the time and mileage of either larger network.
The FCC's "Sixth Report & Order" ended the Freeze. It would take five years for the U.S. to grow from 108 stations to more than 550. New stations came on line slowly, only five by the end of November 1952. The Sixth Report and Order required some existing TV stations to change channels, but only a few existing VHF stations were required to move to UHF, and a handful of VHF channels were deleted altogether in smaller media markets like Peoria, Fresno, Bakersfield and Fort Wayne, Indiana to create markets which were UHF "islands." The report also set aside a number of channels for the newly emerging field of educational television, which hindered struggling ABC and DuMont's quest for affiliates in the more desirable markets where VHF channels were reserved for non-commercial use.
The Sixth Report and Order also provided for the "intermixture" of VHF and UHF channels in most markets; UHF transmitters in the 1950s were not yet powerful enough, nor receivers sensitive enough (if they included UHF tuners at all - they were not formally required until the 1960s All-Channel Receiver Act), to make UHF viable against entrenched VHF stations. In markets where there were no VHF stations and UHF was the only TV service available, UHF survived. In other markets, which were too small to financially support a television station, too close to VHF outlets in nearby cities, or where UHF was forced to compete with more than one well-established VHF station, UHF had little chance for success.
Denver had been the largest U.S. city without a TV station by 1952. Senator Edwin Johnson (D-Colorado), chair of the Senate's Interstate and Foreign Commerce Committee, had made getting Denver the first post-Freeze station his personal mission. He had pressured the FCC, and proved ultimately successful as the first new station (a VHF station) came on-line a remarkable ten days after the Commission formally announced the first post-Freeze construction permits. KFEL (now KWGN-TV)'s first regular telecast was on July 21, 1952.
Telephone monopoly to competition.
The important relationship of the FCC and the American Telephone and Telegraph (AT&T) Company has evolved over several years. For many years, the FCC and state officials agreed to regulate the telephone systems as a natural monopoly. The FCC controlled telephone rates to limit the profits of AT&T and ensure nondiscriminatory pricing. In the 1960s, the FCC began allowing other long-distance companies, namely MCI, to offer specialized services. In the 1970s, the FCC allowed other companies to expand offerings to the public. A lawsuit in 1982 led by the Justice Department after AT&T underpriced other companies, resulted in the split of the Bells from AT&T. Beginning in 1984, the FCC implemented a new goal that all long-distance companies had equal access to the local phone companies' customers.
Telecommunications Act of 1996.
In 1996, Congress enacted the Telecommunications Act of 1996, in the wake of the break-up of AT&T resulting from the U.S. Justice Department's antitrust suit against AT&T. The legislation attempted to create more competition in local telephone service by requiring Incumbent Local Exchange Carriers to provide access to their facilities for Competitive Local Exchange Carriers. This policy has thus far had limited success and much criticism.
The development of the Internet, cable services and wireless services has raised questions whether new legislative initiates are needed as to competition in what has come to be called 'broadband' services. Congress has monitored developments but as of 2009 has not undertaken a major revision of applicable regulation. The Local Community Radio Act in the 111th Congress has gotten out of committee and will go before the house floor with bi-partisan support, and unanimous support of the FCC.
By passing the Telecommunications Act of 1996, Congress also eliminated the cap on the number of radio stations one entity could own nationwide and substantially loosened local radio station ownership restrictions. Substantial radio consolidation followed. Restrictions on ownership of television stations were also loosened. Public comments to the FCC indicated that the public largely believed that the severe consolidation of media ownership had resulted in harm to diversity, localism, and competition in media, and was harmful to the public interest.
Connection permissivity, indecency crackdowns.
The inauguration of Ronald Reagan as President of the United States in 1981 accelerated an already ongoing shift in the FCC towards a decidedly more market-oriented stance. A number of regulations felt to be outdated were removed, most controversially the Fairness Doctrine in 1987. The FCC also took steps to increase competition to broadcasters, fostering broadcast alternatives such as cable television. In terms of indecency fines, there was no action taken by the FCC from FCC v. Pacifica until 1987, about ten years later.
In the early 2000s, the FCC began stepping up censorship and enforcement of indecency regulations again, most notably following the Janet Jackson "wardrobe malfunction" that occurred during the halftime show of Super Bowl XXXVIII. However, the FCC's regulatory domain with respect to indecency remains restricted to the public airwaves, notably VHF and UHF television and AM/FM radio.
On June 15, 2006, President George W. Bush signed into law the Broadcast Decency Enforcement Act of 2005 sponsored by then-Senator Sam Brownback (now Governor of Kansas), a former broadcaster himself, and endorsed by Congressman Fred Upton of Michigan who authored a similar bill in the United States House of Representatives. The new law stiffens the penalties for each violation of the Act. The Federal Communications Commission will be able to impose fines in the amount of $325,000 for each violation by each station that violates decency standards. The legislation raised the fine ten times over the previous maximum of $32,500 per violation.
Past chairs and notable commissioners.
A complete list of commissioners is available on the FCC website. In addition to chairmen, Frieda B. Hennock (D-NY) was the first female commissioner of the FCC.
Broadcast licensing.
Regulatory powers and enforcement.
The FCC regulates broadcast stations, amateur radio operators, and repeater stations as well as commercial broadcasting operators who operate and repair certain radiotelephone, television, radar, and Morse code radio stations. In recent years it has also licensed people who maintain or operate GMDSS stations. Broadcast licenses are to be renewed if the station meets the "public interest, convenience, or necessity".
The FCC's enforcement powers include fines and broadcast license revocation (see FCC MB Docket 04-232). Burden of proof would be on the complainant in a petition to deny. Fewer than 1% of station renewals are not immediately granted, and only a small fraction of those are ultimately denied.
While the FCC maintains control of the written and Morse testing standards, it no longer administers the exams, having delegated that function to private organizations.
Broadcasting tower database.
An FCC database provides information about the height and year built of broadcasting towers in the US. It does not contain information about the structural types of towers or about the height of towers used by Federal agencies, such as most NDBs, LORAN-C transmission towers or VLF transmission facilities of the US Navy, or about most towers not used for transmission like the BREN Tower. These are instead tracked by the Federal Aviation Administration as obstructions to air navigation.
Internet.
In North America, the FCC made its original Internet policy statement containing four principles "subject to reasonable network management" in 2005. The Commission established the following principles: To encourage broadband deployment and preserve and promote the open and interconnected nature of the public Internet, Consumers are entitled to access the lawful Internet content of their choice; Consumers are entitled to run applications and use services of their choice, subject to the needs of law enforcement; Consumers are entitled to connect their choice of legal devices that do not harm the network; Consumers are entitled to competition among network providers, application and service providers, and content providers.
In December 2010, the FCC revised the principles from the original Internet policy statement and created The Open Internet Order consisting of regarding the Internet: Transparency. Fixed and mobile broadband providers must disclose the network management practices, performance characteristics, and terms and conditions of their broadband services; No blocking. Fixed broadband providers may not block lawful content, applications, services, or non-harmful devices; mobile broadband providers may not block lawful websites, or block applications that compete with their voice or video telephony services; and No unreasonable discrimination.
In April 2014, the FCC created a Notice of Proposed Rulemaking that called for reform regarding The Open Internet Order. In February 2015, the FCC passed new through Congress into law.
Controversies.
Unreleased reports.
2003 study of commercial radio concentration.
In 2003, the FCC Media Bureau produced a draft report analyzing the impact of deregulation in the radio industry. The report stated that from March 1996 through March 2003, the number of commercial radio stations on the air rose 5.9 percent while the number of station owners fell 35 percent. The concentration of ownership followed a 1996 rewrite of telecommunications law that eliminated a 40-station national ownership cap.
The report was never made public, nor have any similar analyses followed, despite the fact that radio industry reports were released in 1998, 2001 and 2002. In September 2006, Senator Barbara Boxer, who had received a copy of the report, released it.
2004 study of television media concentration.
In 2004, the FCC ordered its staff to destroy all copies of a draft study by Keith Brown and Peter Alexander, two economists in the FCC's Media Bureau. The two had analyzed a database of 4,078 individual news stories broadcast in 1998, and showed local ownership of television stations adds almost five and one-half minutes of total news to broadcasts and more than three minutes of "on-location" news.
The conclusion of the study was at odds with FCC arguments made when it voted in 2003 to increase the number of television stations a company could own in a single market. (In June 2004, a federal appeals court rejected the agency's reasoning on most of the rules and ordered it to try again.)
In September 2006, Senator Barbara Boxer, who had received a copy of the report "indirectly from someone within the FCC who believed the information should be made public," wrote a letter to FCC Chairman Kevin Martin, asked whether any other commissioners "past or present" knew of the report's existence and why it was never made public. She also asked whether it was "shelved because the outcome was not to the liking of some of the commissioners and/or any outside powerful interests?" Boxer's office said if she does not receive adequate answers to her questions, she will push for an investigation by the FCC inspector general.
Action by FCC Chairman.
In a letter in response to Senator Boxer, FCC Chairman Martin said "I want to assure you that I too am concerned about what happened to these two draft reports." The letter also said "I have asked the inspector general of the FCC to conduct an investigation into what happened to these draft documents and will cooperate fully with him." Martin added that he was not chairman at the time the reports were drafted, and that neither he nor his staff had seen them.
NSA wiretapping.
When it emerged in 2006 that AT&T, BellSouth and Verizon may have broken U.S. laws by aiding the National Security Agency in possible illegal wiretapping of its customers, Congressional representatives called for an FCC investigation into whether or not those companies broke the law. The FCC declined to investigate, however, claiming that it could not investigate due to the classified nature of the program– a move that provoked the criticism of members of Congress.
"Today the watchdog agency that oversees the country's telecommunications industry refused to investigate the nation's largest phone companies' reported disclosure of phone records to the NSA," said Rep. Edward Markey (D-Mass.) in response to the decision. "The FCC, which oversees the protection of consumer privacy under the Communications Act of 1934, has taken a pass at investigating what is estimated to be the nation's largest violation of consumer privacy ever to occur. If the oversight body that monitors our nation's communications is stepping aside then Congress must step in."
Diversity.
With the major demographic shifts occurring in the country in terms of the racial-ethnic composition of the population, the FCC has also been criticized for ignoring the issue of decreasing racial-ethnic diversity of the media. This includes charges that the FCC has been watering down the limited affirmative action regulations it had on the books, including no longer requiring stations to make public their data on their minority staffing and hiring. In the second half of 2006, groups such as the National Hispanic Media Coalition, the National Latino Media Council, the National Association of Hispanic Journalists, the National Institute for Latino Policy, the League of United Latin American Citizens (LULAC) and others held town hall meetings in California, New York and Texas on media diversity as its effects Latinos and minority communities. They documented widespread and deeply felt community concerns about the negative effects of media concentration and consolidation on racial-ethnic diversity in staffing and programming. At these Latino town hall meetings, the issue of the FCC's lax monitoring of obscene and pornographic material in Spanish-language radio and the lack of racial and national-origin diversity among Latino staff in Spanish-language television were other major themes.
President Barack Obama appointed Mark Lloyd to the FCC in the newly created post of Associate General Counsel/Chief Diversity Officer.
Use of white space.
"White spaces" are radio frequencies that went unused after the federally mandated transformation of analog TV signal to digital. On October 15, 2008, FCC Chairman Kevin Martin announced his support for the unlicensed use of white spaces. Martin said he was "hoping to take advantage of utilizing these airwaves for broadband services to allow for unlicensed technologies and new innovations in that space."
Google, Microsoft and other companies are vying for the use of this white-space to support innovation in Wi-Fi technology. Broadcasters and wireless microphone manufacturers fear that the use of white space would "disrupt their broadcasts and the signals used in sports events and concerts." Cell phone providers such as T-Mobile USA have mounted pressure on the FCC to instead offer up the white space for sale to boost competition and market leverage.
On November 4, 2008, the FCC commissioners unanimously agreed to open up unused broadcast TV spectrum for unlicensed use.
Net neutrality.
The FCC has claimed some jurisdiction over the issue of net neutrality and has laid down guideline rules that it expects the telecommunications industry to follow. On February 11, 2008 Rep. Ed Markey and Rep. Chip Pickering introduced HR5353 "To establish broadband policy and direct the Federal Communications Commission to conduct a proceeding and public broadband summit to assess competition, consumer protection, and consumer choice issues relating to broadband Internet access services, and for other purposes." On 1 August 2008 the FCC formally voted 3-to-2 to upholding a complaint against Comcast, the largest cable company in the US, ruling that it had illegally inhibited users of its high-speed Internet service from using file-sharing software. The FCC imposed no fine, but required Comcast to end such blocking in 2008. FCC chairman Kevin J. Martin said the order was meant to set a precedent that Internet providers, and indeed all communications companies, could not prevent customers from using their networks the way they see fit unless there is a good reason. In an interview Martin stated that "We are preserving the open character of the Internet" and "We are saying that network operators can't block people from getting access to any content and any applications." Martin's successor, Julius Genachowski has maintained that the FCC has no plans to regulate the internet, saying: "I've been clear repeatedly that we're not going to regulate the Internet." The Comcast case highlighted broader issues of whether new legislation is needed to force Internet providers to maintain net neutrality, i.e. treat all uses of their networks equally. The legal complaint against Comcast related to BitTorrent, software that is commonly used for downloading larger files. On November 10, 2014, President Obama recommended the FCC reclassify broadband Internet service as a telecommunications service in order to preserve net neutrality. On January 16, 2015, Republicans presented legislation, in the form of a U. S. Congress H. R. discussion draft bill, that makes concessions to net neutrality but prohibits the FCC from accomplishing the goal or enacting any further regulation affecting Internet service providers (ISPs). On January 31, 2015, AP News reported the FCC will present the notion of applying ("with some caveats") Title II (common carrier) of the Communications Act of 1934 to the internet in a vote expected on February 26, 2015. Adoption of this notion would reclassify internet service from one of information to one of telecommunications 
 and, according to Tom Wheeler, chairman of the FCC, ensure net neutrality. The FCC is expected to enforce net neutrality in its vote, according to the New York Times.
On February 26, 2015, the FCC ruled in favor of net neutrality by applying Title II (common carrier) of the Communications Act of 1934 and Section 706 of the Telecommunications act of 1996 to the Internet. The FCC Chairman, Tom Wheeler, commented, "This is no more a plan to regulate the Internet than the First Amendment is a plan to regulate free speech. They both stand for the same concept." Net neutrality is supported by 81% of Americans, according to a Washington Post poll. According to the poll, 81% of Democrats and 85% of Republicans said they opposed fast lanes.
On March 12, 2015, the FCC released the specific details of the net neutrality rules. On April 13, 2015, the FCC published the final rule on its new "Net Neutrality" regulations.
Proprietary standards.
The FCC has also been criticized for ignoring international open standards, and instead choosing proprietary closed standards, or allowing communications companies to do so and implement the anticompetitive practice of vendor lock-in, thereby preventing a free market.
In the case of digital TV, it chose the ATSC standard, even though DVB was already in use around the world, including DVB-S satellite TV in the U.S. Unlike competing standards, the ATSC system is encumbered by numerous patents, and therefore royalties that make TV sets and DTV converters much more expensive than in the rest of the world. Additionally, the claimed benefit of better reception in rural areas is more than negated in urban areas by multipath interference, which other systems are nearly immune to. It also cannot be received while in motion for this reason, while all other systems can, even without dedicated mobile TV signals or receivers.
For digital radio, the FCC chose proprietary HD Radio, which crowds the existing FM broadcast band and even AM broadcast band with in-band adjacent-channel sidebands, which create noise in other stations. This is in contrast to worldwide DAB, which uses unused TV channels in the VHF band III range. This too has patent fees, while DAB does not. While there has been some effort by iBiquity to lower them, the fees for HD Radio are still an enormous expense when converting each station, and this fee structure presents a potentially high cost barrier to entry for community radio and other non-commercial educational stations when entering the HD Radio market.
Satellite radio (also called SDARS by the FCC) uses two proprietary standards instead of DAB-S, which requires users to change equipment when switching from one provider to the other, and prevents other competitors from offering new choices as stations can do on terrestrial radio. Had the FCC picked DAB-T for terrestrial radio, no separate satellite receiver would have been needed at all, and the only difference from DAB receivers in the rest of the world would be in software, where it would need to tune S band instead of L band.
In mobile telephony, the FCC abandoned the "any lawful device" principle decided against AT&T landlines, and has instead allowed each mobile phone company to dictate what its customers can use.
DTV controversy.
The FCC has been criticized for awarding a digital TV (DTV) channel to each holder of an analog TV station license without an auction, as well as trading auctionable spectrum to Nextel to resolve public safety RF interference problems. Conversely, it has also been criticized for forcing stations to buy and install all new equipment (transmitters, TV antennas, and even entirely new broadcast towers), and operate for years on both channels at once.
After delaying the original deadlines of 2006, 2008, and eventually February 17, 2009, on concerns about elderly and rural folk, on June 12 all full-power analog terrestrial TV licenses in the U.S. were terminated as part of the DTV transition, leaving terrestrial television available only from digital channels and a few low-power LPTV stations. To help U.S. consumers through the conversion, Congress established a federally sponsored DTV Converter Box Coupon Program for two free converters per household.
Modernization of the FCC's information technology systems.
David A. Bray joined the Commission in 2013 as Chief Information Officer and quickly announced goals of modernizing the FCC's legacy information technology (IT) systems, citing 200 different systems for only 1750 people a situation he found "perplexing". He was named as one of the top 70 Most Social U.S. federal technology professionals in 2014, openly discussing new opportunities with the public as and about FCC efforts.
Bray has openly encouraged opportunities to broaden the definition of public service to include citizen-led initiatives and great public-private partnerships. He also has emphasized: "As IT accelerates its global distribution and ubiquitous availability, the importance of assuring the integrity of digital communications become paramount" and that communications support "national growth, prosperity, security, safety, and freedom."
Local broadcasting.
After being successful in opening the FM band as a superior alternative to the AM band by allowing colleges and other schools to start ten-watt LPFM stations, the FCC banned new ones around 1980.
Numerous controversies have surrounded the city of license concept as the internet has made it possible to broadcast a single signal to every owned station in the nation at once, particularly when Clear Channel became the largest FM broadcasting corporation in the US after the Telecommunications Act of 1996 became law - owning over 1200 stations at its peak. As part of its license to buy more radio stations, Clear Channel was forced to divest all TV stations.
Public consultation.
As the public interest standard has always been important to the FCC when determining and shaping policy, so too has the relevance of public involvement in U.S. communication policy making. The "FCC Record" is the comprehensive compilation of decisions, reports, public notices, and other documents of the FCC, published since 1986.
History of the issue.
1927 Radio Act.
In the 1927 Radio Act, which was formulated by the predecessor of the FCC (the Federal Radio Commission), section 4(k) stipulated that the commission was authorized to hold hearings for the purpose of developing a greater understanding of the issues for which rules were being crafted. Section 4(k) stated that:
Thus, it is clear that public consultation, or at least consultation with outside bodies was regarded as central to the Commission's job from early on. Though it should not be surprising, the Act also stipulated that the Commission should verbally communicate with those being assigned licenses. Section 11 of the Act noted:
Public hearings.
As early as 1927, there is evidence that public hearings were indeed held; among them, hearings to assess the expansion of the radio broadcast band. At these early hearings, the goal of having a broad range of viewpoints presented was evident, as not only broadcasters, but also radio engineers and manufacturers were in attendance. Numerous groups representing the general public appeared at the hearings as well, including amateur radio operators and inventors as well as representatives of radio listeners' organizations. Interestingly,
Including members of the general public in the discussion was regarded (or at least articulated) as very important to the Commission's deliberations. In fact, FCC Commissioner Bellows noted at the time that "it is the radio listener we must consider above everyone else." Though there were numerous representatives of the general public at the hearing, some expressing their opinions to the commission verbally, overall there was not a great turnout of everyday listeners at the hearings.
Though not a constant fixture of the communications policy-making process, public hearings were occasionally organized as a part of various deliberation processes as the years progressed. For example, seven years after the enactment of the Radio Act, the Communications Act of 1934 was passed, creating the FCC. That year the Federal Government's National Recovery Agency (associated with the New Deal period) held public hearings as a part of its deliberations over the creation of new broadcasting codes.
A few years later , the FCC held hearings to address early cross-ownership issues; specifically, whether newspaper companies owning radio stations was in the public interest. These "newspaper divorcement hearings" were held between 1941 and 1944, though it appears that these hearings were geared mostly towards discussion by industry stakeholders. Around the same time, the Commission held hearings as a part of its evaluation of the national television standard, and in 1958 held additional hearings on the television network broadcasting rules. Though public hearings were organized somewhat infrequently, there was an obvious public appeal. In his now famous "vast wasteland" speech in 1961, FCC Chairman Newton Minow noted that the commission would hold a "well advertised public hearing" in each community to assure broadcasters were serving the public interest, clearly a move to reconnect the Commission with the public interest (at least rhetorically).
Media ownership review 2003.
In September 2002, the FCC issued a Notice of Proposed Rulemaking stating that the Commission would re-evaluate its media ownership rules pursuant to the obligation specified in the Telecommunications Act of 1996. As 2003 was approaching, a battle of words (and perhaps actions) developed between Chairman Powell and Democratic Commissioner Michael Copps. Commissioner Copps felt that the Republican FCC was too focused on the neo-liberal agenda, and not focused enough on hearing the public's voice regarding the issues at hand, noting, "We need a much wider participation … this is not an inside-the-Beltway issue." Copps repeatedly called for the FCC to hold public hearings with time devoted to public input. Powell responded by noting that the public had already taken advantage of the online comment submission process and that no public hearings would be necessary. A spokesman for Powell noted, "if Commissioner Copps thinks something more can be gained from having hearings, he should feel free to do so." In the end, Commissioner Copps and Commissioner Jonathan Adelstein organized a number of "unofficial" FCC hearings.
On January 16, 2003, the FCC held an "unofficial" public hearing on media ownership at Columbia University; Chairman Michael Powell was in attendance. His opening remarks, however, certainly reflected the lack of interest the Commission had displayed towards public hearings in recent years:
The Chief of the Media Bureau and some other associates would be there all day to hear a full report on the event.
Copps remained adamant that all Commissioners should attend an official FCC hearing before any decisions were made. An editorial in Broadcasting and Cable articulated the heated nature of the eventual decision regarding an official hearing (at least from the Republican standpoint). The article is quoted at length as it includes a variety of points that are relevant:
Headquarters.
The FCC leases space in the Portals building in southwest Washington, D.C. Construction of the Portals building was scheduled to begin on March 1, 1996. In January 1996 the General Services Administration signed a lease with the building's owners, agreeing to let the FCC lease 450000 sqft of space in Portals for 20 years, at a cost of $17.3 million per year in 1996 dollars. Prior to its current arrangement, the FCC had space in six buildings by 19th Street NW and M Street NW. The FCC first solicited bids for a new headquarters complex in 1989. In 1991 the GSA selected the Portals site. The FCC had wanted to move into a more expensive area along Pennsylvania Avenue.

</doc>
<doc id="55976" url="http://en.wikipedia.org/wiki?curid=55976" title="Makefile">
Makefile

Makefile(s) are text files written in a certain prescribed syntax. Together with Make Utility, it helps build a software from its source files, a way to organize code, and its compilation and linking.
Overview.
Most often, the Makefile tells make how to compile and link a program. Using C/C++ as an example, when a C/C++ source file is changed, it must be recompiled. If a header file has changed, each C/C++ source file that includes the header file must be recompiled to be safe. Each compilation produces an object file corresponding to the source file. Finally, if any source file has been recompiled, all the object files, whether newly made or saved from previous compilations, must be linked together to produce the new executable program. These instructions with their dependencies are specified in a Makefile. If none of the files that are prerequisites have been changed since the last time the program was compiled, no actions take place. For large software projects, using Makefiles can substantially reduce build times if only a few source files have changed.
Operating Systems.
Unix-like.
Makefiles originated in Unix like systems and is still the primary software build mechanism.
Microsoft Windows.
Windows supports a variation of makefiles with its nmake utility. Standard Unix like makefiles can be executed in Windows in a Cygwin environment.
However, Visual Studio is a very popular software development environment in Windows which does not use makefiles. The equivalent of a Makefile is managed by Visual Studio Project and Solution files.
Contents.
Makefiles contain five kinds of things: "explicit rules", "implicit rules", "variable definitions", "directives", and "comments".
Rules.
A makefile consists of “rules” with the following construct.
 target: dependencies
 Tab ↹ system command(s)
Note: It is important to insert a Tab ↹ character before the commands.
A target is usually the name of a file that is generated by a program; examples of targets are executable or object files. A target can also be the name of an action to carry out, such as 'clean'.
A dependency (also called prerequisite) is a file that is used as input to create the target. A target often depends on several files. However, the rule that specifies a recipe for the target need not have any prerequisites. For example, the rule containing the delete command associated with the target 'clean' does not have prerequisites.
The system command(s) (also called recipe) is an action that make carries out. A recipe may have more than one command, either on the same line or each on its own line.
Execution.
A Makefile is executed with the make command.
make [options] [target1 target2 ...]
By default, when make looks for the makefile, if a makefile name was not included as a parameter, it tries the following names, in order: makefile and Makefile.
Example.
Here is a simple makefile that describes the way an executable file called "edit" depends on four object files which, in turn, depend on four C source and two header files.
edit : main.o kbd.o command.o display.o 
 cc -o edit main.o kbd.o command.o display.o
main.o : main.c defs.h
 cc -c main.c
kbd.o : kbd.c defs.h command.h
 cc -c kbd.c
command.o : command.c defs.h command.h
 cc -c command.c
display.o : display.c defs.h
 cc -c display.c
clean :
 rm edit main.o kbd.o command.o display.o
To use this makefile to create the executable file called "edit", type: make
To use this makefile to delete the executable file and all the object files from the directory, type: make clean

</doc>
<doc id="55979" url="http://en.wikipedia.org/wiki?curid=55979" title="Capsicum">
Capsicum

Capsicum (commonly known as peppers or bell peppers) is a genus of flowering plants in the nightshade family Solanaceae. Its species are native to the Americas, where they have been cultivated for thousands of years. In modern times, it is cultivated worldwide, and has become a key element in many regional cuisines. In addition to use as spices and food vegetables, "Capsicum" species have also found use in medicines.
The fruit of "Capsicum" plants have a variety of names depending on place and type. The piquant (spicy) varieties are commonly called chili peppers, or simply "chilies". The large, mild form is called red pepper, green pepper, or bell pepper in North America and typically just "capsicum" in New Zealand, Australia, and India. The fruit is called paprika in some other countries (although paprika can also refer to the powdered spice made from various capsicum fruit).
The generic name is derived from the Greek word κάπτω ("kapto"), meaning "to bite" or "to swallow". The name "pepper" came into use because of their similar flavour to the condiment black pepper, "Piper nigrum", although there is no botanical relationship with this plant, or with the Sichuan pepper. The original term, "chilli" (now "chile" in Mexico) came from the Nahuatl word "chilli" or "xilli", referring to a larger "Capsicum" variety cultivated at least since 3000 BC, as evidenced by remains found in pottery from Puebla and Oaxaca.
Capsaicin in capsicum.
The fruit of most species of "Capsicum" contains capsaicin (methyl vanillyl nonenamide), a lipophilic chemical that can produce a strong burning sensation (pungency or spiciness) in the mouth of the unaccustomed eater. Most mammals find this unpleasant, whereas birds are unaffected. The secretion of capsaicin protects the fruit from consumption by insects and mammals, while the bright colours attract birds that will disperse the seeds.
Capsaicin is present in large quantities in the placental tissue (which holds the seeds), the internal membranes, and to a lesser extent, the other fleshy parts of the fruits of plants in this genus. The seeds themselves do not produce any capsaicin, although the highest concentration of capsaicin can be found in the white pith around the seeds.
The amount of capsaicin in the fruit is highly variable and dependent on genetics and environment, giving almost all types of "Capsicum" varied amounts of perceived heat. The most recognizable "Capsicum" without capsaicin is the bell pepper, a cultivar of "Capsicum annuum", which has a zero rating on the Scoville scale. The lack of capsaicin in bell peppers is due to a recessive gene that eliminates capsaicin and, consequently, the "hot" taste usually associated with the rest of the "Capsicum" family. There are also other peppers without capsaicin, mostly within the "Capsicum annuum" species, such as the cultivars Giant Marconi, Yummy Sweets, Jimmy Nardello, and Italian Frying peppers.
Chili peppers are of great importance in Native American medicine, and capsaicin is used in modern medicine—mainly in topical medications—as a circulatory stimulant and analgesic. In more recent times, an aerosol extract of capsaicin, usually known as capsicum or pepper spray, has become used by law enforcement as a nonlethal means of incapacitating a person, and in a more widely dispersed form for riot control, or by individuals for personal defense. Pepper in vegetable oils, or as an horticultural product can be used in gardening as a natural insecticide.
Although black pepper causes a similar burning sensation, it is caused by a different substance—piperine.
Cuisine.
"Capsicum" fruits and peppers can be eaten raw or cooked. Those used in cooking are generally varieties of the "C. annuum" and "C. frutescens" species, though a few others are used, as well. They are suitable for stuffing with fillings such as cheese, meat, or rice.
They are also frequently used both chopped and raw in salads, or cooked in stir-fries or other mixed dishes. They can be sliced into strips and fried, roasted whole or in pieces, or chopped and incorporated into salsas or other sauces, of which they are often a main ingredient.
They can be preserved in the form of a jam, or by drying, pickling, or freezing. Dried peppers may be reconstituted whole, or processed into flakes or powders. Pickled or marinated peppers are frequently added to sandwiches or salads. Frozen peppers are used in stews, soups, and salsas. Extracts can be made and incorporated into hot sauces.
The Spanish "conquistadores" soon became aware of their culinary properties, and brought them back to Europe, together with cocoa, potatoes, sweet potatoes, tobacco, maize, beans, and turkeys. They also brought it to the Spanish Philippines colonies, whence it spread to Asia. The Portuguese brought them to their African and Asiatic possessions such as India.
All the varieties were appreciated, but the hot ones are particularly appreciated because they can enliven otherwise monotonous diets. This was of some importance during dietary restrictions for religious reasons, such as Lent in Christian countries.
Spanish cuisine soon benefited from the discovery of chiles in the New World, and it would be very difficult to untangle Spanish cooking from chiles, garlic, and olive oil. Ground chiles, or paprika, hot or otherwise, are a key ingredient in chorizo, which is then called picante (if hot chile is added) or dulce (if otherwise). Paprika is also an important ingredient in rice dishes, and plays a definitive role in squid Galician style ("polbo á feira"). Chopped chiles are used in fish or lamb dishes such as "ajoarriero" or "chilindrón". "Pisto" is a vegetarian stew with chilies and zucchini as main ingredients. They can also be added, finely chopped, to "gazpacho" as a garnish. In some regions, bacon is salted and dusted in paprika for preservation. Cheese can also be rubbed with paprika to lend it flavour and colour. Dried round chiles called "ñoras" are used for "arroz a banda".
According to Richard Pankhurst, "C. frutescens" (known as "barbaré") was so important to the national cuisine of Ethiopia, at least as early as the 19th century, "that it was cultivated extensively in the warmer areas wherever the soil was suitable." Although it was grown in every province, "barbaré" was especially extensive in Yejju, "which supplied much of Showa, as well as other neighbouring provinces." He mentions the upper Golima River valley as being almost entirely devoted to the cultivation of this plant, where it was harvested year round.
In 2005, a poll of 2,000 people revealed the pepper to be Britain's fourth-favourite culinary vegetable.
In Hungary, sweet yellow peppers – along with tomatoes – are the main ingredient of "lecsó".
In Bulgaria, South Serbia, and Macedonia, peppers are very popular, too. They can be eaten in salads, like "shopska salata"; fried and then covered with a dip of tomato paste, onions, garlic, and parsley; or stuffed with a variety of products, such as minced meat and rice, beans, or cottage cheese and eggs. Peppers are also the main ingredient in the traditional tomato and pepper dip "lyutenitsa" and "ajvar". They are in the base of different kinds of pickled vegetables dishes, "turshiya".
Peppers are also used widely in Italian cuisine, and the hot species are used all around the southern part of Italy as a common spice (sometimes served with olive oil). "Capsicum" peppers are used in many dishes; they can be cooked by themselves in a variety of ways (roasted, fried, deep-fried) and are a fundamental ingredient for some delicatessen specialities, such as "nduja".
Capsicums are also used extensively in Sri Lankan cuisine as side dishes.
The Maya and Aztec people of Mesoamerica used "Capsicum" fruit in cocoa drinks as a flavouring.
Species and varieties.
"Capsicum" consists of 20–27 species, five of which are domesticated: "C. annuum", "C. baccatum", "C. chinense", "C. frutescens", and "C. pubescens". Phylogenetic relationships between species were investigated using biogeographical, morphological, chemosystematic, hybridization, and genetic data. Fruits of "Capsicum" can vary tremendously in color, shape, and size both between and within species, which has led to confusion over the relationships between taxa. Chemosystematic studies helped distinguish the difference between varieties and species. For example, "C. baccatum" var. "baccatum" had the same flavonoids as "C. baccatum" var. "pendulum", which led researchers to believe the two groups belonged to the same species.
Many varieties of the same species can be used in many different ways; for example, "C. annuum" includes the "bell pepper" variety, which is sold in both its immature green state and its red, yellow, or orange ripe state. This same species has other varieties, as well, such as the Anaheim chiles often used for stuffing, the dried ancho chile used to make chili powder, the mild-to-hot jalapeño, and the smoked, ripe jalapeño, known as chipotle.
Most of the capsaicin in a pungent (hot) pepper is concentrated in blisters on the epidermis of the interior ribs (septa) that divide the chambers of the fruit to which the seeds are attached. A study on capsaicin production in fruits of "C. chinense" showed that capsaicinoids are produced only in the epidermal cells of the interlocular septa of pungent fruits, that blister formation only occurs as a result of capsaicinoid accumulation, and that pungency and blister formation are controlled by a single locus, "Pun1", for which there exist at least two recessive alleles that result in non-pungency of "C. chinense" fruits.
The amount of capsaicin in hot peppers varies significantly between varieties, and is measured in Scoville heat units (SHU). The world's current hottest known pepper as rated in SHU is the 'Carolina Reaper' which had been measured at over 2,200,000 SHU.
Genetics.
Most "Capsicum" species are 2n=24. A few of the nondomesticated species are 2n=32.
Breeding.
Several breeding programs are being conducted by corporations and universities. New Mexico State University has released several varieties in the last few years. Cornell has worked to develop regionally adapted varieties. Many types of peppers have been bred for heat, size, and yield.
GRAS.
Only Capsicum frutescens L. and Capsicum annuum L. are in the GRAS.
Synonyms and common names.
The name given to the "Capsicum" fruits varies between English-speaking countries.
In Australia, New Zealand, and India, heatless varieties are called "capsicums", while hot ones are called "chilli"/"chillies" (double L). Pepperoncini are also known as "sweet capsicum". The term "bell peppers" is almost never used, although "C. annuum" and other varieties which have a bell shape and are fairly hot, are often called "bell chillies".
In Ireland and the United Kingdom, the heatless varieties are commonly known simply as "peppers" (or more specifically "green peppers", "red peppers", etc.), while the hot ones are "chilli"/"chillies" (double L) or "chilli peppers".
In the United States and Canada, the common heatless varieties are referred to as "bell peppers", "sweet peppers", "red/green/etc. peppers", or simply "peppers", additionally in Indiana they may be referred to as "mangoes/mango peppers", while the hot varieties are collectively called "chile"/"chiles", "chili"/"chilies", or "chili"/"chile peppers" (one L only), "hot peppers", or named as a specific variety (e.g., banana pepper). 
In Polish and in Hungarian, the term "papryka" and "paprika" (respectively) is used for all kinds of capsicums (the sweet vegetable, and the hot spicy), as well as for dried and ground spice made from them (named paprika in both U.S. English and Commonwealth English). Also, fruit and spice can be attributed as "papryka ostra" (hot pepper) or "papryka słodka" (sweet pepper). The term "pieprz" (pepper) instead means only grains or ground black pepper (incl. the green, white, and red forms), but not capsicum. Sometimes, the hot capsicum spice is also called "chilli".
In Italy and the Italian- and German-speaking parts of Switzerland, the sweet varieties are called "peperone" and the hot varieties "peperoncino" (literally "small pepper"). In Germany, the heatless varieties as well as the spice are called "Paprika" while the hot types are primarily called "Peperoni" or "Chili" while in Austria, "Pfefferoni" is more common for these; in Dutch, this word is also used exclusively for bell peppers, whereas "chilli" is reserved for powders, and hot pepper variants are referred to as "Spaanse pepers" (Spanish peppers). In Switzerland, though, the condiment powder made from capsicum is called "Paprika" (German language regions) and "paprica" (French and Italian language region). In French, capsicum is called "poivron".
In Spanish-speaking countries, many different names are used for the varieties and preparations. In Mexico, the term "chile" is used for "hot peppers", while the heatless varieties are called "pimiento" (the masculine form of the word for pepper, which is "pimienta"). Several other countries, such as Chile, whose name is unrelated, Perú, Puerto Rico, and Argentina, use "ají". In Spain, heatless varieties are called "pimiento" and hot varieties "guindilla". Also, in Argentina and Spain, the variety "C. chacoense" is commonly known as "putaparió", a slang expression equivalent to "damn it", probably due to its extra-hot flavour.
In Indian English, the word "capsicum" is used exclusively for "Capsicum annuum". All other varieties of hot capsicum are called chilli. In northern India and Pakistan, "C. annuum" is also commonly called "shimla mirch" in the local language and as "Kodai Mozhagai" in Tamil which roughly translates to "umbrella chilli" due to its appearance. "Shimla", incidentally, is a popular hill-station in India (and "mirch" means chilli in local languages).
In Japanese, "tōgarashi" (唐辛子, トウガラシ "Chinese mustard") refers to hot chili peppers, and particularly a spicy powder made from them which is used as a condiment, while bell peppers are called "pīman" (ピーマン, from the French "piment" or the Spanish "pimiento").

</doc>
<doc id="55982" url="http://en.wikipedia.org/wiki?curid=55982" title="Dirty bomb">
Dirty bomb

A dirty bomb or radiological dispersal device (RDD) is a speculative radiological weapon that combines radioactive material with conventional explosives. The purpose of the weapon is to contaminate the area around the dispersal agent/conventional explosion with radioactive material, serving primarily as an area denial device against civilians. It is however not to be confused with a nuclear explosion, such as a fission bomb, which by releasing nuclear energy produces blast effects far in excess of what is achievable by the use of conventional explosives.
Though a radiological dispersal device (RDD) would be designed to disperse radioactive material over a large area, a bomb that uses conventional explosives and produces a blast wave would be far more lethal to people than the hazard posed by radioactive material that may be mixed with the explosive. At levels created from probable sources, not enough radiation would be present to cause severe illness or death. A test explosion and subsequent calculations done by the United States Department of Energy found that assuming nothing is done to clean up the affected area and everyone stays in the affected area for one year, the radiation exposure would be "fairly high", but not fatal. Recent analysis of the nuclear fallout from the Chernobyl disaster confirms this, showing that the effect on many people in the surrounding area, although not those in close proximity, was almost negligible.
Since a dirty bomb is unlikely to cause many deaths by radiation exposure, many do not consider this to be a weapon of mass destruction. Its purpose would presumably be to create psychological, not physical, harm through ignorance, mass panic, and terror. For this reason dirty bombs are sometimes called "weapons of mass disruption". Additionally, containment and decontamination of thousands of victims, as well as decontamination of the affected area might require considerable time and expense, rendering areas partly unusable and causing economic damage.
Dirty bombs and terrorism.
Since the 9/11 attacks the fear of terrorist groups using dirty bombs has increased significantly, which has been frequently reported in the media. The meaning of terrorism used here, is described by the U.S. Department of Defense's definition, which is "the calculated use of unlawful violence or threat of unlawful violence to inculcate fear; intended to coerce or to intimidate governments or societies in the pursuit of goals that are generally political, religious, or ideological objectives".
There have only ever been two cases of caesium-containing bombs, and neither was detonated. Both involved Chechnya. The first attempt of radiological terror was carried out in November 1995 by a group of Chechen separatists, who buried a caesium-137 source wrapped in explosives at the Izmaylovsky Park in Moscow. A Chechen rebel leader alerted the media, the bomb was never activated, and the incident amounted to a mere publicity stunt.
In December 1998, a second attempt was announced by the Chechen Security Service, who discovered a container filled with radioactive materials attached to an explosive mine. The bomb was hidden near a railway line in the suburban area Argun, ten miles east of the Chechen capital of Grozny. The same Chechen separatist group was suspected to be involved.
Despite the increased fear of a dirty bombing attack, it is hard to assess whether the actual risk of such an event has increased significantly. The following discussions on implications, effects and probability of an attack, as well as indications of terror groups planning such, are based mainly on statistics, qualified guessing and a few comparable scenarios.
Effect of a dirty bomb explosion.
When dealing with the implications of a dirty bomb attack, there are two main areas to be addressed: (i) the civilian impact, not only dealing with immediate casualties and long term health issues, but also the psychological effect and then (ii) the economic impact. With no prior event of a dirty bomb detonation, it is considered difficult to predict the impact. Several analyses have predicted that RDDs will neither sicken nor kill many people.
Accidents with radioactives.
The effects of uncontrolled radioactive contamination have been reported several times.
One example is the radiological accident occurring in Goiânia, Brazil, between September 1987 and March 1988: Two metal scavengers broke into an abandoned radiotherapy clinic and removed a teletherapy source capsule containing powdered caesium-137 with an activity of 50 TBq. They brought it back to the home of one of the men to take it apart and sell as scrap metal. Later that day both men were showing acute signs of radiation illness with vomiting and one of the men had a swollen hand and diarrhea. A few days later one of the men punctured the 1 mm thick window of the capsule, allowing the caesium chloride powder to leak out and when realizing the powder glowed blue in the dark, brought it back home to his family and friends to show it off. After 2 weeks of spread by contact contamination causing an increasing number of adverse health effects, the correct diagnosis of acute radiation sickness was made at a hospital and proper precautions could be put into procedure. By this time 249 people were contaminated, 151 exhibited both external and internal contamination of which 20 people were seriously ill and 5 people died.
The Goiânia incident to some extent predicts the contamination pattern if it is not immediately realized that the explosion spread radioactive material, but also how fatal even very small amounts of ingested radioactive powder can be. This raises worries of terrorists using powdered alpha emitting material, that if ingested can pose a serious health risk, as in the case of deceased former K.G.B. spy Alexander Litvinenko, who either ate, drank or inhaled polonium-210. "Smoky bombs" based on alpha emitters might easily be just as dangerous as beta or gamma emitting dirty bombs.
Public perception of risks.
For the majority involved in an RDD incident, the radiation health risks (i.e. increased probability of developing cancer later in life due to radiation exposure) are comparatively small, comparable to the health risk from smoking five packages of cigarettes on a daily basis. The fear of radiation is not always logical. Although the exposure might be minimal, many people find radiation exposure especially frightening because it is something they cannot see or feel, and it therefore becomes an unknown source of danger. Dealing with public fear may prove the greatest challenge in case of an RDD event. Policy, science and media may inform the public about the real danger and thus reduce the possible psychological and economic effects.
Statements from the U.S. government after 9/11 may have contributed unnecessarily to the public fear of a dirty bomb. When United States Attorney General John Ashcroft on June 10, 2002, announced the arrest of José Padilla, allegedly plotting to detonate such a weapon, he said:
 [A] radioactive "dirty bomb" (...) spreads radioactive material that is highly toxic to humans and can cause mass death and injury.
 — Attorney General John Ashcroft
This public fear of radiation also plays a big role in why the costs of an RDD impact on a major metropolitan area (such as lower Manhattan) might be equal to or even larger than that of the 9/11 attacks. Assuming the radiation levels are not too high and the area does not need to be abandoned such as the town of Pripyat near the Chernobyl reactor, an expensive and time consuming cleanup procedure will begin. This will mainly consist of tearing down highly contaminated buildings, digging up contaminated soil and quickly applying sticky substances to remaining surfaces to adhere the radioactive particles before radioactivity penetrates the building materials. These procedures are the current state of the art for radioactive contamination cleanup, but some experts say that a complete cleanup of external surfaces in an urban area to current decontamination limits may not be technically feasible. Loss of working hours will be vast during cleanup, but even after the radiation levels reduce to an acceptable level, there might be residual public fear of the site including possible unwillingness to conduct business as usual in the area. Tourist traffic is likely never to resume.
There is also a psychological warfare element to radioactive substances, visceral fear is not widely aroused by, for example, the daily emissions from coal burning, although, as a National Academy of Sciences study found, this causes 10,000 premature deaths a year in the US population of 317,413,000. Medical errors leading to death in U.S. hospitals are estimated to be between 44,000 and 98,000. It is "only nuclear radiation that bears a huge psychological burden — for it carries a unique historical legacy".
Constructing and obtaining material for a dirty bomb.
In order for a terrorist organization to construct and detonate a dirty bomb, they must acquire radioactive material by stealing it or buying it through legal or illegal channels. Possible RDD material could come from the millions of radioactive sources used worldwide in the industry, for medical purposes and in academic applications mainly for research. Of these sources, only nine reactor produced isotopes stand out as being suitable for radiological terror: americium-241, californium-252, caesium-137, cobalt-60, iridium-192, plutonium-238, polonium-210, radium-226 and strontium-90, and even from these it is possible that radium-226 and polonium-210 do not pose a significant threat. Of these sources the U.S. Nuclear Regulatory Commission has estimated that within the U.S., approximately one source is lost, abandoned or stolen every day of the year. Within the European Union the annual estimate is 70. There exist thousands of such "orphan" sources scattered throughout the world, but of those reported lost, no more than an estimated 20 percent can be classified as a potential high security concern if used in a RDD. Especially Russia is believed to house thousands of orphan sources, which were lost following the collapse of the Soviet Union. A large but unknown number of these sources probably belong to the high security risk category. Noteworthy are the Russian very strong beta emitting strontium-90 sources used as radioisotope thermoelectric generators for beacons in lighthouses in remote areas. In December 2001, three Georgian woodcutters stumbled over such a power generator and dragged it back to their camp site to use it as a heat source. Within hours they suffered from acute radiation sickness and sought hospital treatment. The International Atomic Energy Agency (IAEA) later stated that it contained approximately 40 kCi of strontium, equivalent to the amount of radioactivity released immediately after the Chernobyl accident (though the total radioactivity release from Chernobyl was 2500 times greater at around 100 MCi).
Although a terrorist organization might obtain radioactive material through the "black market", and there has been a steady increase in illicit trafficking of radioactive sources from 1996 to 2004, these recorded trafficking incidents mainly refer to rediscovered orphan sources without any sign of criminal activity, and it has been argued that there is no conclusive evidence for such a market. In addition to the hurdles of obtaining usable radioactive material, there are several conflicting requirements regarding the properties of the material the terrorists need to take into consideration: First, the source should be "sufficiently" radioactive to create direct radiological damage at the explosion or at least to perform societal damage or disruption. Second, the source should be transportable with enough shielding to protect the carrier, but not so much that it will be too heavy to maneuver. Third, the source should be sufficiently dispersible to effectively contaminate the area around the explosion.
An example of a worst-case scenario is a terror organization possessing a source of very highly radioactive material, e.g. a strontium-90 thermal generator, with the ability to create an incident comparable to the Chernobyl accident. Although the detonation of a dirty bomb using such a source might seem terrifying, it would be hard to assemble the bomb and transport it without severe radiation damage and possible death of the perpetrators involved. Shielding the source effectively would make it almost impossible to transport and a lot less effective if detonated.
Due to the three constraints of making a dirty bomb, RDDs might still be defined as "high-tech" weapons and this is probably why they have not been used up to now.
Possibility of terrorist groups using dirty bombs.
The present assessment of the possibility of terrorists using a dirty bomb is based on cases involving Al-Qaeda. This is because the attempts by this group to acquire a dirty bomb are the most well-described in the literature, in part due to the attention this group received for their involvement in the 9/11 attacks.
On 8 May 2002, José Padilla (a.k.a. Abdulla al-Muhajir) was arrested on suspicion that he was an Al-Qaeda terrorist planning to detonate a dirty bomb in the U.S. This suspicion was raised by information obtained from an arrested top Al-Qaeda official in U.S. custody, Abu Zubaydah, who under interrogation revealed that the organization was close to constructing a dirty bomb. Although Padilla had not obtained radioactive material or explosives at the time of arrest, law enforcement authorities uncovered evidence that he was on reconnaissance for usable radioactive material and possible locations for detonation. It has been doubted whether José Padilla was preparing such an attack, and it has been claimed that the arrest was highly politically motivated, given the pre-9/11 security lapses by the CIA and FBI.
Later, these charges against José Padilla were dropped. Although there was no hard evidence for Al-Qaeda possessing a dirty bomb, there is a broad agreement that Al-Qaeda poses a potential dirty bomb attack threat because they need to overcome the alleged image that the U.S. and its allies are winning the war against terror. A further concern is the argument, that "if suicide bombers are prepared to die flying airplanes into building, it is also conceivable that they are prepared to forfeit their lives building dirty bombs". If this would be the case, both the cost and complexity of any protective systems needed to allow the perpetrator to survive long enough to both build the bomb and carry out the attack, would be significantly reduced.
Several other captives were alleged to have played a role in this plot.
Guantanamo captive Binyam Mohammed has alleged he was subjected to extraordinary rendition, and that his confession of a role in the plot was coerced through torture.
He sought access through the American and United Kingdom legal systems to provide evidence he was tortured.
Guantanamo military commission prosecutors continue to maintain the plot was real, and charged Binyam for his alleged role in 2008. However they dropped this charge in October 2008, but maintain they could prove the charge and were only dropping the charge to expedite proceedings.
US District Court Judge Emmet G. Sullivan insisted that the administration still had to hand over the evidence that justified the dirty bomb charge, and admonished United States Department of Justice lawyers that dropping the charge "raises serious questions in this court's mind about whether those allegations were ever true."
In 2006, Dhiren Barot from North London pleaded guilty of conspiring to murder innocent people within the United Kingdom and United States using a radioactive dirty bomb. He planned to target underground car parks within the UK and buildings in the U.S. such as the International Monetary Fund, World Bank buildings in Washington D.C., the New York Stock Exchange, Citigroup buildings and the Prudential Financial buildings in Newark, New Jersey. He also faces 12 other charges including, conspiracy to commit public nuisance, seven charges of making a record of information for terrorist purposes and four charges of possessing a record of information for terrorist purposes. Experts say if the plot to use the dirty bomb was carried out "it would have been unlikely to cause deaths, but was designed to affect about 500 people."
In January 2009, a leaked FBI report described the results of a search of the Maine home of James G. Cummings, a white supremacist who had been shot and killed by his wife. Investigators found four one-gallon containers of 35 percent hydrogen peroxide, uranium, thorium, lithium metal, aluminum powder, beryllium, boron, black iron oxide and magnesium as well as literature on how to build dirty bombs and information about cesium-137, strontium-90 and cobalt-60, radioactive materials. Officials confirmed the veracity of the report but stated that the public was never at risk.
In April 2009, the Security Service of Ukraine announced the arrest of a legislator and two businessmen from the Ternopil Oblast. Seized in the undercover sting operation was 3.7 kilograms of what was claimed by the suspects during the sale as plutonium-239, used mostly in nuclear reactors and nuclear weapons, but was determined by experts to be probably americium, a "widely used" radioactive material which is commonly used in amounts of less than 1 milligram in smoke detectors, but can also be used in a dirty bomb. The suspects reportedly wanted US$ 10 million for the material, which the Security Service determined was produced in Russia during the era of the Soviet Union and smuggled into Ukraine through a neighboring country.
In July 2014, ISIS militants seized 88 pounds of uranium compounds from Mosul University. The material was unenriched and so could not be used to build a conventional fission bomb, but a dirty bomb is a theoretical possibility. However, uranium's relatively low radioactivity makes it a poor candidate for use in a dirty bomb.
Little is known about civil preparedness to respond to a dirty bomb attack. The Boston Marathon appeared to many to be a situation with high potential for use of a dirty bomb as a terrorist weapon. Yet the bombing attack that occurred on April 15, 2013 did not involve use of dirty bombs. Any radiological testing or inspections that may have occurred following the attack were either conducted sub rosa or not at all. Also, there was no official dirty bomb "all clear" issued by the Obama Administration. Massachusetts General Hospital had, apparently under their own disaster plan, issued instructions to their emergency room to be prepared for incoming radiation poisoning cases.
Other uses of the term.
The term has also been used historically to refer to certain types of nuclear weapons. Due to the inefficiency of early nuclear weapons, only a small amount of the nuclear material would be consumed during the explosion. Little Boy had an efficiency of only 1.4%. Fat Man, which used a different design and a different fissile material, had an efficiency of 14%. Thus, they tended to disperse large amounts of unused fissile material, and the fission products, which are on average much more dangerous, in the form of nuclear fallout. During the 1950s, there was considerable debate over whether "clean" bombs could be produced and these were often contrasted with "dirty" bombs. "Clean" bombs were often a stated goal and scientists and administrators said that high-efficiency nuclear weapon design could create explosions which generated almost all of their energy in the form of nuclear fusion, which does not create harmful fission products.
But the "Castle Bravo" accident of 1954, in which a thermonuclear weapon produced a large amount of fallout which was dispersed among human populations, suggested that this was not what was actually being used in modern thermonuclear weapons, which derive around half of their yield from a final fission stage. While some proposed producing "clean" weapons, other theorists noted that one could make a nuclear weapon intentionally "dirty" by "salting" it with a material, which would generate large amounts of long-lasting fallout when irradiated by the weapon core. These are known as salted bombs; a specific subtype often noted is a cobalt bomb.

</doc>
<doc id="55983" url="http://en.wikipedia.org/wiki?curid=55983" title="Black rat">
Black rat

The black rat ("Rattus rattus", also known as the ship rat, roof rat, house rat, Alexandrine rat, old English rat, and other names) is a common long-tailed rodent of the genus "Rattus" (rats) in the subfamily Murinae (murine rodents). The species originated in tropical Asia and spread through the Near East in Roman times before reaching Europe by the 1st century and spreading with Europeans across the world.
Black rats are generalist omnivores. They are serious pests to farmers as they eat a wide range of agricultural crops.
Taxonomy.
The black rat was one of the many species originally described by Linnaeus in his 18th century work, "Systema Naturae", and it still bears its original tautonym of "Rattus rattus". It is the type species of the genus "Rattus".
Description.
A typical adult black rat is 12.75 to(-) long, including a 6.5 to(-) tail, and weighs 4 to(-). Despite its name, the black rat exhibits several colour forms. It is usually black to light brown in colour with a lighter underside. In the 1920s in England, several variations were bred and shown alongside domesticated brown rats. This included an unusual green tinted variety. The black rat also has a scraggly coat of black fur, and is slightly smaller than the brown (Norway) rat.
Origin of "Rattus rattus".
"Rattus rattus" bone remains that date back to the Norman Period have been discovered in Britain. Evidence also suggests that "R. rattus" existed in prehistoric Europe as well as the Levant during post-glacial periods. The specific origin of the black rat is uncertain due to the rat's disappearance and reintroduction. Evidence such as DNA and bone fragments also suggests that rats did not originally come from Europe, but migrated from southeast Asia.
Rats are resilient vectors for many diseases because of their ability to hold so many infectious bacteria in their blood. Rats played a primary role in spreading bacteria, such as "Yersinia pestis", which is responsible for the Justinianic plague and bubonic plague. However, a recent study points to the giant gerbil as the source of the plague rather than the rat. According to epidemiological models, "Yersinia pestis" originated outside of Europe, which indicates that Western and central Europe have never had any natural rodent plagues.
The modern black rat was probably spread across Europe in the wake of the Roman conquest and arose from an ancestor that originated in southeast Asia, possibly Malaysia. The Mediterranean black rats differ genetically from their southeast Asian ancestors by having 38 instead of 42 chromosomes. Therefore, it seems that speciation could have occurred when the rats colonized southwest India, which was a primary country where Romans obtained their spices. Because "Rattus rattus" is a passive traveler, they could have easily traveled to Europe during the trading between Rome and the southwestern Asian countries. Evidence also suggests that, in 321–331 BC, Egyptian birds were preying on Mediterranean rats, though this is not enough to prove that Egypt was the source of the rats.
Diet.
Black rats are considered omnivores and eat a wide range of foods, including seeds, fruit, stems, leaves, fungi, and a variety of invertebrates and vertebrates. They are generalists, and thus not very specific in their food preferences, which is indicated by their tendency to feed on any meal provided for cows, swine, chickens, cats, and dogs. They are similar to the tree squirrel in their preference of fruits and nuts. They eat about 15 g per day and drink about 15 ml per day. Their diet is high in water content. They are a threat to many natural habitats because they feed on birds and insects. They are also a threat to many farmers since they feed on a variety of agricultural-based crops, such as cereals, sugar cane, coconuts, cocoa, oranges, and coffee beans.
Distribution and habitat.
The black rat originated in India and Southeast Asia, and spread to the Near East and Egypt, and then throughout the Roman Empire, reaching Great Britain as early as the 1st century. Europeans subsequently spread it throughout the world. The black rat is again largely confined to warmer areas, having been supplanted by the brown rat ("Rattus norvegicus") in cooler regions and urban areas. In addition to being larger and more aggressive, the change from wooden structures and thatched roofs to bricked and tiled buildings favored the burrowing brown rats over the arboreal black rats. In addition, brown rats eat a wider variety of foods, and are more resistant to weather extremes.
Black rat populations can explode under certain circumstances, perhaps having to do with the timing of the fruiting of the bamboo plant, and cause devastation to the plantings of subsistence farmers; this phenomenon is known as "Mautam" in parts of India.
Black rats are thought to have arrived in Australia with the First Fleet, and subsequently spread to many coastal regions in the country.
In New Zealand, black rats have an unusual distribution and importance, in that they are utterly pervasive through native forests, scrublands, and urban parklands. This is typical only of oceanic islands that lack native mammals, especially other rodents. Throughout most of the world, black rats are found only in disturbed habitats near people, mainly near the coast. Black rats are the most frequent predator of small forest birds, invertebrates, and perhaps lizards in New Zealand forests, and are key ecosystem changers. Controlling their abundance on usefully large areas of the New Zealand mainland is a crucial current challenge for conservation managers.
Black rats adapt to a wide range of habitats. In urban areas they are found around warehouses, residential buildings, and other human settlements. They are also found in agricultural areas, such as in barns and crop fields. In urban areas they prefer to live in dry upper levels of buildings, so they are commonly found in wall cavities and false ceilings. In the wild, black rats live in cliffs, rocks, the ground, and trees. They are great climbers and prefer to live in trees, such as pines and palm trees. Their nests are typically spherical and made of shredded material, including sticks, leaves, other vegetation, and cloth. In the absence of trees, they can burrow into the ground. Black rats are also found around fences, ponds, riverbanks, streams, and reservoirs.
The black rat, along with the brown rat, is one of the most widespread rats and animal species in the world.
Home range.
Home range refers to the area in which an animal travels and spends most of its time. It is thought that male and female rats have similar sized home ranges during the winter, but male rats increase the size of their home range during the breeding season. Along with differing between rats of different gender, home range also differs depending on the type of forest in which the black rat inhabits. For example, home ranges in the southern beech forests of the South Island, New Zealand appear to be much larger than the non-beech forests of the North Island. Due to the limited number of rats that are studied in home range studies, the estimated sizes of rat home ranges in different rat demographic groups are inconclusive.
Nesting behavior.
Through the usage of tracking devices such as radio transmitters, rats have been found to occupy dens located in trees, as well as on the ground. In Puketi, a forest in Kauri, New Zealand, rats have been found to form dens together. Rats appear to den and forage in separate areas in their home range depending on the availability of food resources. Research shows that in New South Wales, the black rat prefers to inhabit lower leaf litter of forest habitat. There is also an apparent correlation between the canopy height and logs and the presence of black rats. This correlation may be a result of the distribution of the abundance of prey as well as available refuges for rats to avoid predators. As found in North Head, New South Wales, there is positive correlation between rat abundance, leaf litter cover, canopy height, and litter depth. All other habitat variables showed little to no correlation. While this species' relative, the brown (Norway) rat prefers to nest near the ground of a building the black rat will prefer the upper floors and roof. Because of this habit they have been given the common name roof rat.
Foraging behavior.
As generalists, black rats express great flexibility in their foraging behavior. They are predatory animals and adapt to different micro-habitats. They often meet and forage together in close proximity within and between sexes. Rats tend to forage after sunset. If the food cannot be eaten quickly, they will search for a place to carry and hoard to eat at a later time.
Although black rats eat a broad range of foods, they are highly selective feeders; only a restricted number of the foods they eat are dominant foods. When black rat populations are presented with a wide diversity of foods, they eat only a small sample of each of the available foods. This allows them to monitor the quality of foods that are present year round, such as leaves, as well as seasonal foods, such as herbs and insects. This method of operating on a set of foraging standards ultimately determines the final composition of their meals. Also, by sampling the available food in an area, the rats maintain a dynamic food supply, balance their nutrient intake, and avoid intoxication by secondary compounds.
Ecology.
Black rats (or their ectoparasites) are able to carry a number of pathogens, of which bubonic plague (via the rat flea), typhus, Weil's disease, toxoplasmosis and trichinosis are the best known. It has been hypothesized that the displacement of black rats by brown rats led to the decline of the Black Death. This theory has, however, been deprecated, as the dates of these displacements do not match the increases and decreases in plague outbreaks.
Predators and diseases.
The black rat serves as prey to cats and owls in domestic settings. In less urban settings, rats are preyed upon by weasels, foxes, and coyotes. These predators have little effect on the control of the black rat population because black rats are agile and fast climbers. In addition to agility, the black rat also makes use of its keen sense of hearing to detect danger and quickly evade mammalian and avian predators.
Rats serve as outstanding vectors for transmittance of diseases because they can carry bacteria and viruses in their systems. A number of bacterial diseases are common to rats, and these include "Streptococcus pneumoniae", "Corynebacterium kutsheri," "Bacillus piliformis", "Pasteurella pneumotropica", and "Streptobacillus moniliformis", to name a few. All of these bacteria are disease causing agents in humans. In some cases, these diseases are incurable.
"R. rattus" as an invasive species.
Damage caused by "R. rattus".
After "Rattus rattus" was introduced into the northern islands of New Zealand they fed on the seedlings adversely affecting the ecology of the islands. Even after eradication of "R. rattus" the negative effects may take decades to reverse. When consuming these seabirds and seabird eggs, these rats reduce the pH of the soil. This harms plant species by reducing nutrient availability in soil, thus decreasing the probability of seed germination. For example, research conducted by Hoffman et al. indicates a large impact on sixteen indigenous plant species directly preyed on by "R. rattus". These plants displayed a negative correlation in germination and growth in the presence of black rats.
Rats prefer to forage in forest habitats. In the Ogasawara islands, they prey on the indigenous snails and seedlings. Snails that inhabit the leaf litter of these islands showed a significant decline in population upon the introduction of "Rattus rattus". The black rat shows preference for snails with larger shells (greater than 10mm), and this led to a great decline in the population of snails with larger shells. A lack of prey refuges makes it more difficult for the snail to avoid the rat.
Complex pest.
The black rat is a complex pest, defined as one that influences the environment in both harmful and beneficial ways. In many cases, after the black rat is introduced into a new area, the population size of some native species declines or goes extinct. This is because the black rat is a good generalist with a wide dietary niche and a preference for complex habitats; this causes strong competition for resources among small animals. This has led to the black rat completely displacing many native species in Madagascar, the Galapagos, and the Florida Keys. In a study by Stokes "et al.", habitats suitable for the native bush rat, "Rattus fuscipes", of Australia are often invaded by the black rat and are eventually occupied by only the black rat. When the abundances of these two rat species were compared in different micro-habitats, both were found to be affected by micro-habitat disturbances, but the black rat was most abundant in areas of high disturbance; this indicates it has a better dispersal ability.
Despite the black rat's tendency to displace native species, it can also aid in increasing species population numbers and maintaining species diversity. The bush rat, a common vector for spore dispersal of truffles, has been extirpated from many micro-habitats of Australia. In the absence of a vector, the diversity of truffle species would be expected to decline. In a study in New South Wales, Australia it was found that although the bush rat consumes a diversity of truffle species, the black rat consumes as much of the diverse fungi as the natives and is an effective vector for spore dispersal. Since the black rat now occupies many of the micro-habitats that were previously inhabited by the bush rat, the black rat plays an important ecological role in the dispersal of fungal spores. By eradicating the black rat populations in Australia, the diversity of fungi would decline, potentially doing more harm than good.
Control methods.
Large-scale rat control programs have been taken to maintain a steady level of the invasive predators in order to conserve the native species in New Zealand such as kokako and mohua. Pesticides, such as pindone and 1080 (sodium fluoroacetate), are commonly distributed via aerial spray by helicopter as a method of mass control on islands infested with invasive rat populations. Bait, such as brodifacoum, is also used along with coloured dyes in order to kill and identify rats for experimental and tracking purposes. Another method to track rats is the use of wired cage traps, which are used along with bait, such as rolled oats and peanut butter, to tag and track rats to determine population sizes through methods like mark-recapture and radio-tracking. Poison control methods are effective in reducing rat populations to nonthreatening sizes, but rat populations often rebound to normal size within months. Besides their highly adaptive foraging behavior and fast reproduction, the exact mechanisms for their rebound is unclear and are still being studied.
In 2010, the Sociedad Ornitológica Puertorriqueña (Puerto Rican Bird Society) and the Ponce Yacht and Fishing Club launched a campaign to eradicate the black rat from the Isla Ratones (Mice Island) and Isla Cardona (Cardona Island) islands off the municipality of Ponce, Puerto Rico.
Endangerment and conservation.
"Rattus rattus" populations were common in Great Britain, but began to decline after the introduction of the brown rat in the 18th century. "R. rattus" populations remained common in seaports and major cities until the late 19th century, but have been decreased due to rodent control and sanitation measures. There is currently only one wild population of "R. rattus" left in Britain, in the Shiant Islands in the Outer Hebrides in Scotland. Although rats pose a threat to native seabird species and their eggs, seabird populations have remained stable. There are no definite studies about their impact on sea birds in the Shiant Islands.

</doc>
<doc id="55986" url="http://en.wikipedia.org/wiki?curid=55986" title="Xanana Gusmão">
Xanana Gusmão

Kay Rala Xanana Gusmão GCL CNZM (], born José Alexandre Gusmão, ], on 20 June 1946) is an East Timorese politician. A former militant, he was the first President of East Timor, serving from May 2002 to May 2007. He then became the fourth Prime Minister of East Timor, serving from 8 August 2007 to 16 February 2015. He has been Minister of Planning and Strategic Investment since February 2015.
Early life and career.
Gusmão was born to "mestiço" school teacher parents (of mixed Portuguese-Timorese ancestry) in Manatuto in what was then Portuguese Timor, and attended a Jesuit high school just outside of Dili. After leaving high-school for financial reasons at the age of fifteen, he held a variety of unskilled jobs, while continuing his education at night school. In 1965, at the age of 19, he met Emilia Batista, who was later to become his wife. His nickname, "Xanana", was taken from the name of the American rock and roll band Sha Na Na, who in turn were named after a lyric from a doo-wop song of 1957 by the Silhouettes.
In 1966, Gusmão obtained a position with the public service, which allowed him to continue his education. This was interrupted in 1968 when Gusmão was recruited by the Portuguese Army for national service. He served for three years, rising to the rank of corporal. During this time, he married Emilia Batista, with whom he had a son Eugenio, and a daughter Zenilda. He has since divorced Emilia, and in 2000, he married Australian Kirsty Sword, with whom he had three sons: Alexandre, Kay Olok and Daniel. In 1971, Gusmão completed his national service, his son was born, and he became involved with a nationalist organization headed by José Ramos-Horta. For the next three years he was actively involved in peaceful protests directed at the colonial system.
It was in 1974 that a coup in Portugal resulted in the beginning of decolonization for Portuguese Timor, and shortly afterwards the Governor Mário Lemos Pires announced plans to grant the colony independence. Plans were drawn up to hold general elections with a view to independence in 1978. During most of 1975 a bitter internal struggle occurred between two rival factions in Portuguese Timor. Gusmão became deeply involved with the FRETILIN faction, and as a result he was arrested and imprisoned by the rival faction the Timorese Democratic Union (UDT) in mid-1975. Taking advantage of the internal disorder, and with an eye to absorbing the colony, Indonesia immediately began a campaign of destabilization, and frequent raids into Portuguese Timor were staged from Indonesian West Timor. By late 1975 the Fretilin faction had gained control of Portuguese Timor and Gusmão was released from prison. He was given the position of Press Secretary within the FRETILIN organization. On 28 November 1975, Fretilin declared the independence of Portuguese Timor as "The Democratic Republic of East Timor", and Gusmão was responsible for filming the ceremony. Nine days later, Indonesia invaded East Timor. At the time Gusmão was visiting friends outside of Dili and he witnessed the invasion from the hills. For the next few days he searched for his family.
Indonesian occupation.
After the appointment of the "Provisional Government of East Timor" by Indonesia, Gusmão became heavily involved in resistance activities. Gusmão was largely responsible for the level of organization that evolved in the resistance, which ultimately led to its success. The early days featured Gusmão walking from village to village to obtain support and recruits. But after FRETILIN suffered some major setbacks in the early 1980s Gusmão left FRETILIN and supported various centrist coalitions, eventually becoming a leading opponent of FRETILIN. By the mid-1980s, he was a major leader. During the early 1990s, Gusmão became deeply involved in diplomacy and media management, and was instrumental in alerting the world to the massacre in Dili that occurred in Santa Cruz on 12 November 1991. Gusmão was interviewed by many major media channels and obtained worldwide attention.
As a result of his high profile, Gusmão became a prime target of the Indonesian government. A campaign for his capture was finally successful in November 1992. In May 1993, Gusmão was tried, convicted and sentenced to life imprisonment by the Indonesian government. He was found guilty under Article 108 of the Indonesian Penal Code (rebellion), Law no. 12 of 1951 (illegal possession of firearms) and Article 106 (attempting to separate part of the territory of Indonesia). He spoke in his own defense and he was appointed with a defense lawyers before the commencement of his trial. The sentence was commuted to 20 years by Indonesian President Suharto in August 1993. Although not released until late 1999, Gusmão successfully led the resistance from within prison. By the time of his release, he was regularly visited by United Nations representatives, and dignitaries such as Nelson Mandela.
Transition to independence.
On 30 August 1999, a referendum was held in East Timor and an overwhelming majority voted for independence. The Indonesian military commenced a campaign of terror as a result, with terrible consequences. Although the Indonesian government denied ordering this offensive, they were widely condemned for failing to prevent it. As a result of overwhelming diplomatic pressure from the United Nations, promoted by Portugal since the late 1970s and also by the United States and Australia in the 1990s, a UN-sanctioned, Australian-led international peace-keeping force (INTERFET) entered East Timor, and Gusmão was finally released. Upon his return to his native East Timor, he began a campaign of reconciliation and rebuilding.
Gusmão was appointed to a senior role in the UN administration that governed East Timor until 20 May 2002. During this time he continually campaigned for unity and peace within East Timor, and was generally regarded as the "de facto" leader of the emerging nation. Elections were held in late 2001 and Gusmão, endorsed by nine parties but not by Fretilin, ran as an independent and was comfortably elected leader. As a result he became the first President of East Timor when it became formally independent on 20 May 2002. Gusmão has published an autobiography with selected writings entitled "To Resist Is to Win". He is the main narrator of the film "A Hero's Journey"/"Where the Sun Rises", a 2006 documentary about him and East Timor. According to director Grace Phan, it's an "intimate insight into the personal transformation" of the man who helped shape and liberate East Timor.
Independent East Timor.
 On 21 June 2006, Gusmão called for Prime Minister Mari Alkatiri to resign or else he would, as allegations that Alkatiri had ordered a hit squad to threaten and kill his political opponents led to a large backlash. Senior members of the Fretilin party met on 25 June to discuss Alkatiri's future as the Prime Minister, amidst a protest involving thousands of people calling for Alkatiri to resign instead of Gusmão. Despite receiving a vote of confidence from his party, Alkatiri resigned on 26 June 2006 to end the uncertainty. In announcing this he said, "I declare I am ready to resign my position as prime minister of the government...so as to avoid the resignation of His Excellency the President of the Republic [Xanana Gusmão]." The 'hit squad' accusations against Alkatiri were subsequently rejected by a UN Commission, which also criticised Gusmão for making inflammatory statements during the crisis.
Gusmão declined to run for another term in the April 2007 presidential election. In March 2007 he said that he would lead the new National Congress for Timorese Reconstruction (CNRT) into the parliamentary election planned to be held later in the year, and said that he would be willing to become prime minister if his party won the election. He was succeeded as President by José Ramos-Horta on 20 May 2007. The CNRT placed second in the June 2007 parliamentary election, behind FRETILIN, taking 24.10% of the vote and 18 seats. He won a seat in parliament as the first name on the CNRT's candidate list. The CNRT allied with other parties to form a coalition that would hold a majority of seats in parliament. After weeks of dispute between this coalition and FRETILIN over who should form the government, Ramos-Horta announced on 6 August that the CNRT-led coalition would form the government and that Gusmão would become Prime Minister on 8 August. Gusmão was sworn in at the presidential palace in Dili on 8 August.
On 11 February 2008, a motorcade containing Gusmão came under gunfire one hour after President José Ramos-Horta was shot in the stomach. Gusmão's residence was also occupied by rebels. According to the Associated Press, the incidents raised the possibility of a coup attempt; they have also described as possible assassination attempts and kidnap attempts.
Awards and prizes.
While still in in prison, in 1993, he was awarded the Great-Cross of the Portuguese Order of Liberty.
In 1999, Gusmão was awarded the Sakharov Prize for Freedom of Thought.
In July 2000 Gusmão was appointed as a Honorary Companion of the New Zealand Order of Merit for "the furtherance of New Zealand–East Timor relations".
In 2000, he was awarded the Sydney Peace Prize for being "Courageous and principled leader for the independence of the East Timorese people".
Also in 2000, he won the first Gwangju Prize for Human Rights, created to honor "individuals, groups or institutions in Korea and abroad that have contributed in promoting and advancing human rights, democracy and peace through their work."
In 2002, he was awarded the North-South Prize by the Council of Europe.
Mr Gusmão is an Eminent Member of the Sergio Vieira de Mello Foundation.
More recently in 2006 he was awarded the Grande-Collar of the Portuguese Order of Prince Henry.

</doc>
<doc id="55988" url="http://en.wikipedia.org/wiki?curid=55988" title="Tell Abu Hureyra">
Tell Abu Hureyra

Tell Abu Hureyra (Arabic: تل أبو هريرة‎) is an archaeological site located in the Euphrates valley in modern Syria. The remains of the villages within the tell come from over 4,000 years of habitation, spanning the Epipaleolithic and Neolithic periods. Ancient Abu Hureyra was occupied between 13,000 and 9,500 years ago in radio carbon years. The site is significant because the inhabitants of Abu Hureyra started out as hunter-gatherers but gradually transitioned to farming, making them the earliest known farmers in the world.
History of research.
The site was excavated as a rescue operation before it would be flooded by Lake Assad, the reservoir of the Tabqa Dam which was being built at that time. The site was excavated by Andrew Moore in 1972 and 1973. It was limited to only two seasons of fieldwork, because the site was due to be flooded by Lake Assad. However, despite the limited time frame, a large amount of material was recovered and studied over the following decades. It was one of the first archaeological sites to use modern methods of excavation such as 'flotation,' which preserved even the tiniest and most fragile plant remains. A preliminary report was published in 1983 and a final report in 2000.
Location and description.
Abu Hureyra is a tell, or ancient settlement mound, located in modern-day Ar-Raqqah Governorate in northern Syria. It is located on a plateau near the south bank of the Euphrates, 120 km east of Aleppo. The tell is actually a massive accumulation of collapsed houses, debris, and lost objects accumulated over the course of the habitation of the ancient village. The mound is nearly 500 m across, 8 m deep, and contains over 1000000 m3 of archaeological deposits. Today the tell is inaccessible, drowned beneath the waters of Lake Assad.
Occupation history.
First occupation.
The village of Abu Hureyra had two separate periods of occupation: An Epipalaeolithic settlement, and a Neolithic settlement. The Epipaleolithic, or Natufian, settlement was established around 13,500 years ago. During the first settlement, c. 13,000 BP, the village consisted of small round huts, cut into the soft sandstone of the terrace. The roofs were supported with wooden posts, and roofed with brushwood and reeds. Huts contained underground storage areas for food. The population was small, housing a few hundred people at most.
The hunter-gatherers of Abu Hureyra obtained food by hunting, fishing, and gathering of wild plants. Gazelle was hunted primarily during the summer, when vast herds passed by the village during their annual migration. Other prey included large wild animals such as onager, sheep, and cattle, and smaller animals such as hare, fox and birds, which were hunted throughout the year. Plant foods were harvested from "wild gardens," with species gathered including wild cereal grasses such as einkorn wheat, emmer wheat, and two varieties of rye.
Depopulation.
After 1300 years the hunter-gatherers of the first occupation mostly abandoned Abu Hureyra, probably because of the Younger Dryas, an intense and relatively abrupt return to glacial climate conditions which lasted over 1,000 years. The drought disrupted the migration of the gazelle, and decimated the forageable plant food sources. It is likely that most of the inhabitants had to give up sedentism and returned to life on the move. Many of them might have gone to Mureybet - just 50 km upstream on the other side of the Euphrates - which expanded dramatically at this time. It seems that a small population managed to hang on at Abu Hureyra, which thereby was continuously inhabited for 4500 years.
Second occupation.
It is from the early part of the Younger Dryas that the first indirect evidence of agriculture was detected in the excavations at Abu Hureyra, although the cereals themselves were still of the wild variety (see PPNA). It was during the intentional sowing of cereals in more favourable refugees like Mureybet that these first farmers developed domesticated strains during the centuries of drought and cold of the Younger Dryas (see the Khiamian). When the climate abated from 9500 B.C. they spread all over the Middle East with this new bio-technology, and Abu Hureyra grew to a large village eventually with several thousand people. The second occupation grew domesticated varieties of rye, wheat and barley, and kept sheep as livestock (see PPNB). The hunting of gazelle decreased sharply, probably due to an overexploitation that eventually left them extinct in the Middle East. At Abu Hureyra they were replaced by meat from domesticated animals.
Transition from foraging to farming.
Some evidence has been found for cultivation of rye from 11,050 BC. Peter Akkermans and Glenn Schwartz found this claim about epipaleolithic rye "difficult to reconcile with the absence of cultivated cereals at Abu Hureyra and elsewhere for thousands of years afterwards". It has been suggested that drier climate conditions resulting from the beginning of the Younger Dryas caused wild cereals to become scarce, leading people to begin cultivation as a means of securing a food supply. Results of recent analysis of the rye grains from this level suggest that they may actually have been domesticated during the Epipalaeolithic. It is speculated that the permanent population of the first occupation was fewer than 200 individuals. These individuals occupied several tens of square kilometers, comprising a rich resource base of several different ecosystems (river, forest and steppe). On this land they hunted, harvested food and wood, made charcoal, and may have cultivated cereals and grains for food and fuel.

</doc>
<doc id="55989" url="http://en.wikipedia.org/wiki?curid=55989" title="Radiological weapon">
Radiological weapon

A radiological weapon or radiological dispersion device (RDD) is any weapon that is designed to spread radioactive material with the intent to kill and cause disruption. According to the U.S. Department of Defense, an RDD is "any device, including any weapon or equipment, other than a nuclear explosive device, specifically designed to employ radioactive material by disseminating it to cause destruction, damage, or injury by means of the 
radiation produced by the decay of such material”.
One type of RDD is a "conventional explosive combined with some type of radiological material", also known as a dirty bomb. It is not a true nuclear weapon and does not yield the same explosive power. It uses conventional explosives to spread radioactive material, most commonly the spent fuels from nuclear power plants or radioactive medical waste. "It is not a Weapon of Mass Destruction (WMD), but rather, as researcher Peter Probst 
calls it, a “weapon of mass disruption” (Hughes, 2002). In fact, effective dispersal ranges are 
rather limited. Most deaths (if any) would come from the initial explosion (non-nuclear), but it 
does depend on the type of radiological material used. (Department of Homeland Security 
[DHS], 2003)."
Another version is the salted bomb, a true nuclear weapon designed to produce larger amounts of nuclear fallout than a regular nuclear weapon.
Explanation.
Radiological weapons of mass destruction have been suggested as a possible weapon of terrorism used to create panic and casualties in densely populated areas. They could also render a great deal of property uninhabitable for an extended period, unless costly remediation were undertaken. The radiological source and quality greatly impacts the effectiveness of a radiological weapon.
Factors such as: energy and type of radiation, half-life, longevity, availability, shielding, portability, and the role of the environment will determine the effect of the radiological weapon. Radioisotopes that pose the greatest security risk include: 137Cs, used in radiological medical equipment, 60Co, 241Am, 252Cf, 192Ir, 238Pu, 90Sr, 226Ra, and 238U.
All of these isotopes, except for the final one, are created in nuclear power plants. While the amount of radiation dispersed from the event will likely be minimal, the fact of any radiation may be enough to cause panic and disruption.
History.
The professional history of radioactive weaponry may be traced to a 1940 science fiction story, "Solution Unsatisfactory" by Robert A. Heinlein and a 1943 memo from James Bryant Conant, Arthur Holly Compton and Harold Urey to Brigadier General Leslie Groves, head of the Manhattan Project.
Transmitting a report entitled, "Use of Radioactive Materials as a Military Weapon," the Groves memo states:
As a gas warfare instrument the material would ... be inhaled by personnel. The amount necessary to cause death to a person inhaling the material is extremely small. It has been estimated that one millionth of a gram accumulating in a person's body would be fatal. There are no known methods of treatment for such a casualty... It cannot be detected by the senses; It can be distributed in a dust or smoke form so finely powdered that it will permeate a standard gas mask filter in quantities large enough to be extremely damaging...
Radioactive warfare can be used [...] To make evacuated areas uninhabitable; To contaminate small critical areas such as rail-road yards and airports; As a radioactive poison gas to create casualties among troops; Against large cities, to promote panic, and create casualties among civilian populations.
Areas so contaminated by radioactive dusts and smokes, would be dangerous as long as a high enough concentration of material could be maintained... they can be stirred up as a fine dust from the terrain by winds, movement of vehicles or troops, etc., and would remain a potential hazard for a long time.
These materials may also be so disposed as to be taken into the body by ingestion instead of inhalation. Reservoirs or wells would be contaminated or food poisoned with an effect similar to that resulting from inhalation of dust or smoke. Four days production could contaminate a million gallons of water to an extent that a quart drunk in one day would probably result in complete incapacitation or death in about a month's time.
The United States, however, chose not to pursue radiological weapons during World War II, though early on in the project considered it as a backup plan in case nuclear fission proved impossible to tame. Some US policymakers and scientists involved in the project felt that radiological weapons would qualify as chemical weapons and thus violate international law.
Deployment.
One possible way of dispersing the material is by using a dirty bomb, a conventional explosive which disperses radioactive material. Dirty bombs are not a type of nuclear weapon, which requires a nuclear chain reaction and the creation of a critical mass. Whereas a nuclear weapon will usually create mass casualties immediately following the blast, a dirty bomb scenario would initially cause only minimal casualties from the conventional explosion.
Means of radiological warfare that do not rely on any specific weapon, but rather on spreading radioactive contamination via a food chain or water table, seem to be more effective in some ways, but share many of the same problems as chemical warfare.
Military uses.
Radiological weapons are widely considered to be militarily useless for a state-sponsored army and are initially not hoped to be used by any military forces. Firstly, the use of such a weapon is of no use to an occupying force, as the target area becomes uninhabitable (due to the fallout caused by radioactive poisoning of the involved environment).
Furthermore, area-denial weapons are generally of limited use to an attacking army, as it slows the rate of advance.
Dirty bombs.
A dirty bomb is a radiological weapon dispersed with conventional explosives.
There is currently (as of 2007) an ongoing debate about the damage that terrorists using such a weapon might inflict. Many experts believe that a dirty bomb such that terrorists might reasonably be able to construct would be unlikely to harm more than a few people and hence it would be no more deadly than a conventional bomb. Furthermore, the casualties would be a result of the initial explosion, because alpha and beta emitting material needs to be inhaled to do damage to the human body. Gamma radiation emitting material is so radioactive that it can't be deployed without wrapping an amount of shielding material around the bomb that would make transport by car or plane impossible without risking detection.
Because of this a dirty bomb with radioactive material around an explosive device would be almost useless, unless said shielding was removed shortly before detonation. This is not only because of the effectiveness but also because this material would be easy to clean up. Furthermore, the possibility of terrorists making a gas or aerosol that is radioactive is very unlikely because of the complex chemical work to achieve this goal.
Hence, this line of argument goes, the objectively dominant effect would be the moral and economic damage due to the massive fear and panic such an incident would spur. On the other hand, some believe that the fatalities and injuries might be in fact much more severe. This point was made by physicist Peter D. Zimmerman (King's College London) who reexamined the Goiânia accident which is arguably comparable. and popularized in a subsequent fictionalized account produced by the BBC and broadcast in the United States by PBS. The latter program showed how shielding might be used to minimize the detection risk.
Salted bomb.
A salted bomb is a theoretical nuclear weapon designed to produce enhanced quantities of radioactive fallout, rendering a large area uninhabitable. As far as is publicly known none have ever been built.

</doc>
<doc id="55992" url="http://en.wikipedia.org/wiki?curid=55992" title="Tommy Cooper">
Tommy Cooper

Thomas Frederick "Tommy" Cooper (19 March 1921 – 15 April 1984) was a British prop comedian and magician. Cooper was a member of the Magic Circle, and respected by traditional magicians. He was famed for his red fez, and his appearance was large and lumbering, at 6 ft and more than 15 st in weight. On 15 April 1984, Cooper collapsed, and died soon afterwards, from a heart attack on national television.
Biography.
Born in Caerphilly, Glamorgan, South Wales, at 19 Llwyn Onn Street, Trecenydd, Cooper was delivered by the woman who owned the house in which the family was lodging. His parents were Thomas H. (Tom) Cooper, a Welsh-born recruiting sergeant in the British Army, and Gertrude (née Gertrude C. Wright), his English-born wife from Crediton, Devon.
To escape from the heavily polluted air of Caerphilly, his father accepted the offer of a new job and the family moved to Exeter, Devon, when Cooper was three. It was in Exeter that he acquired the West Country accent that became part of his act. The family lived in the back of Haven Banks, where Cooper attended Mount Radford School for Boys. He also helped his parents run their ice cream van, which attended fairs at weekends. When he was eight an aunt bought him a magic set and he spent hours perfecting the tricks. His brother David (born 1930) opened a magic shop in the 1960s in Slough High Street (then Buckinghamshire now Berkshire) called D. & Z. Cooper's Magic Shop.
Cooper was influenced by Laurel and Hardy, Max Miller, Bob Hope, and Robert Orben.
Second World War.
After school Cooper became a shipwright in Hythe, Hampshire, and in 1940 he was called up as a trooper in the Royal Horse Guards. He served initially in Montgomery's Desert Rats in Egypt. Cooper became a member of a NAAFI entertainment party and developed an act around his magic tricks interspersed with comedy. One evening in Cairo, during a sketch in which he was supposed to be in a costume that required a pith helmet, having forgotten the prop, Cooper reached out and borrowed a fez from a passing waiter, which got huge laughs.
Development of the act.
When he was demobbed after seven years of military service Cooper took up show business on Christmas Eve, 1947. He later developed a popular monologue about his military experience as "Cooper the Trooper". He worked in variety theatres around the country and at many of London's top night spots, performing as many as 52 shows in one week.
Cooper had developed his conjuring skills and was a member of the Magic Circle, but there are various stories about how and when he developed his delivery of "failed" magic tricks:
To keep the audience on their toes Cooper threw in an occasional trick that worked when it was least expected.
Career.
In 1947 Cooper got his big break with Miff Ferrie, at that time trombonist in a band called the Jackdaws, who booked him to appear as the second-spot comedian in a show starring the sand dance act Marqueeze and the Dance of the Seven Veils. Cooper then began two years of arduous performing, including a tour of Europe and a stint in pantomime, playing one of Cinderella's ugly sisters. The period culminated in a season-long booking at the Windmill Theatre, where he doubled up doing cabaret. One week he performed in 52 shows. Ferrie remained Cooper's sole agent for 37 years, until Cooper's death in 1984. Cooper was supported by a variety of acts, including the vocal percussionist Frank Holder.
Cooper rapidly became a top-liner in variety with his turn as the conjurer whose tricks never succeeded, but it was his television work that raised him to national prominence. After his debut on the BBC talent show "New to You" in March 1948 he started starring in his own shows, and was popular with audiences for nearly 40 years, notably through his work with London Weekend Television from 1968 to 1972 and with Thames Television from 1973 to 1980. Thanks to his many television shows during the mid-1970s, he was one of the most recognisable comedians in the world.
Cooper was a heavy drinker and smoker, and experienced a decline in health during the late 1970s, suffering a heart attack in 1977 while in Rome, where he was performing a show. Three months later he was back on television in "Night Out at the London Casino". By 1980, however, his drinking meant that Thames Television would not give him another starring series, and "Cooper's Half Hour" was his last. He did continue to appear as a guest on other television shows, however, and worked with Eric Sykes on two Thames productions in 1982.
John Fisher writes in his biography of Cooper: "Everyone agrees that he was mean. Quite simply he was acknowledged as the tightest man in show business, with a pathological dread of reaching into his pocket." One of Cooper's stunts was to pay the exact taxi fare and when leaving the cab to slip something into the taxi driver's pocket saying, "Have a drink on me." That something would turn out to be a tea bag.
By the mid-1970s alcohol had started to erode Cooper's professionalism and club owners complained that he turned up late or rushed through his show in five minutes. In addition he suffered from chronic indigestion, lumbago, sciatica, bronchitis and severe circulation problems in his legs. When Cooper realised the extent of his maladies he cut down on his drinking, and the energy and confidence returned to his act. However, he never stopped drinking and could be fallible: on an otherwise triumphant appearance with Michael Parkinson he forgot to set the safety catch on the guillotine illusion into which he had cajoled Parkinson, and only a last-minute intervention by the floor manager saved Parkinson from serious injury or worse.
Death on a live television show.
On 15 April 1984 Cooper collapsed from a heart attack in front of millions of television viewers, midway through his act on the London Weekend Television variety show "Live From Her Majesty's", transmitted live from Her Majesty's Theatre.
An assistant had helped him put on a cloak for his sketch, while Jimmy Tarbuck, the host, was hiding behind the curtain waiting to pass him different props that he would then appear to pull from inside his gown. The assistant smiled at him as he collapsed, believing that it was a part of the act. Likewise, the audience gave "uproarious" laughter as he fell, gasping for air. At this point Alasdair MacMillan, the director of the television production, cued the orchestra to play music for an unscripted commercial break (noticeable because of several seconds of blank screen while LWT's master control contacted regional stations to start transmitting advertisements) and Tarbuck's manager tried to pull Cooper back through the curtains. It was decided to continue with the show. Dustin Gee and Les Dennis were the act that had to follow Cooper, and other stars proceeded to present their acts in the limited space in front of the stage. While the show continued efforts were being made backstage to revive Cooper, not made easier by the darkness. It was not until a second commercial break that ambulancemen were able to move his body to Westminster Hospital, where he was pronounced dead on arrival. His death was not officially reported until the next morning, although the incident was the leading item on the news programme that followed the show. Cooper was cremated at Mortlake Crematorium in London.
The video of Tommy Cooper suffering a heart attack on stage has been uploaded to numerous video sharing websites. YouTube drew criticism from a number of sources when footage of the incident was posted on the website in May 2009. John Beyer of the pressure group Mediawatch UK said: "This is very poor taste. That the broadcasters have not repeated the incident shows they have a respect for him and I think that ought to apply also on YouTube." On 28 December 2011 segments of the "Live From Her Majesty's" clip, including Cooper collapsing on stage, were screened on Channel 4 in the UK, on a programme titled "The Untold Tommy Cooper".
Personal life.
From 1967 until his death Cooper had a relationship with his personal assistant, Mary Fieldhouse. She wrote about it in her book, "For the Love of Tommy" (1986). His son Thomas (a.k.a. Thomas Henty) died in 1988 and his wife, Gwen, died in 2002.
Legacy.
A statue of Cooper was unveiled in his hometown of Caerphilly, Wales, in 2008 by fellow entertainer Sir Anthony Hopkins, who is patron of the Tommy Cooper Society. The statue was sculpted by James Done.
In 2009 for Red Nose Day, a charity Red Nose was put on the statue, but the nose was stolen.
In a 2005 poll "The Comedians' Comedian", Cooper was voted the sixth greatest comedy act ever by fellow comedians and comedy insiders. He is commonly cited as one of the best comedians of all time, with several polls placing him at number one. He has been cited as an influence by Jason Manford and John Lydon.
In 2012 the British Heart Foundation ran a series of adverts featuring Tommy Cooper to raise awareness of heart conditions. These included posters bearing his image together with radio adverts featuring classic Cooper jokes.
Jerome Flynn has toured with his own tribute show to Cooper called "Just Like That". In February 2007, "The Independent" reported that Andy Harries, a producer of "The Queen", was working on a dramatisation about the last week of Tommy Cooper's life. Harries described Cooper's death as "extraordinary" in that the whole thing was broadcast live on national television. The film subsequently went into production over six years later as a television drama for ITV. From a screenplay by Simon Nye, "" was directed by Benjamin Caron and the title role was played by David Threlfall; it was broadcast 21 April 2014.
"Being Tommy Cooper", a new play written by Tom Green and starring Damian Williams, was produced by Franklin Productions and toured the UK in 2013.
Hip-hop duo dan le sac vs Scroobius Pip wrote the song "Tommy C", about Cooper's career and death, which appears on their 2008 album, "Angles".
In 2014, with the support of The Tommy Cooper Estate and Cooper's daughter Victoria, a new tribute show "Just Like That! The Tommy Cooper Show" commemorating 30 years since the comedian's death was produced by "Hambledon Productions". The production moves to the "Museum of Comedy" in Bloomsbury, London from September 2014.

</doc>
<doc id="55994" url="http://en.wikipedia.org/wiki?curid=55994" title="Ménière's disease">
Ménière's disease

Ménière's disease , also called endolymphatic hydrops, is a disorder of the inner ear that can affect hearing and balance to a varying degree. It is characterized by episodes of vertigo, tinnitus, and hearing loss. The hearing loss comes and goes for some time, alternating between ears, then becomes permanent with no return to normal function. It is named after the French physician Prosper Ménière, who, in an article published in 1861, first reported that vertigo was caused by inner ear disorders. The condition affects people differently; it can range in intensity from being a mild annoyance to a lifelong condition.
Signs and symptoms.
Ménière's often begins with one symptom, and gradually progresses. However, not all symptoms must be present to confirm the diagnosis although several symptoms at once is more conclusive than different symptoms at separate times. Other conditions can present themselves with Ménière's-like symptoms, such as syphilis, Cogan's syndrome, autoimmune inner ear disease, dysautonomia, perilymph fistula, multiple sclerosis, acoustic neuroma, and both hypo- and hyperthyroidism.
The symptoms of Ménière's are variable; not all sufferers experience the same symptoms. However, so-called "classic Ménière's" is considered to have the following four symptoms:
Some may have parasympathetic symptoms, which aren't necessarily symptoms of Ménière's, but rather side effects from other symptoms. These are typically nausea, vomiting, and sweating which are typically symptoms of vertigo, and not of Ménière's. Vertigo may induce nystagmus, or uncontrollable rhythmical and jerky eye movements, usually in the horizontal plane, reflecting the essential role of non-visual balance in coordinating eye movements. Sudden, severe attacks of dizziness or vertigo, called "Tumarkin's otolithic crises" but known informally as "drop attacks," can cause someone who is standing to suddenly fall. Drop attacks are likely to occur later in the disease, but can occur at any time.
Migraine.
There is an increased prevalence of migraine in patients with Ménière’s disease, with some clinical samples showing about one third of patients experiencing migraines. An association with familial history of vestibular migraine has also been demonstrated.
Cause.
Ménière's disease is linked to "endolymphatic hydrops", an excess of fluid in the inner ear. The membranous labyrinth, a system of membranes in the ear, contains a fluid called endolymph. In Ménière's disease, endolymph bursts from its normal channels in the ear and flows into other areas, causing damage. This is called "hydrops." The membranes can become dilated like a balloon when pressure increases and drainage is blocked. This may be related to swelling of the endolymphatic sac or other tissues in the vestibular system of the inner ear, which is responsible for the body's sense of balance. In some cases, the endolymphatic duct may be obstructed by scar tissue, or may be narrow from birth. In some cases there may be too much fluid secreted by the stria vascularis. The symptoms may occur in the presence of a middle ear infection, head trauma, or an upper respiratory tract infection, or by using aspirin, smoking cigarettes, or drinking alcohol. They may be further exacerbated by excessive consumption of salt in some patients. It has also been proposed that Ménière's symptoms in many patients are caused by the deleterious effects of a herpes virus.
Ménière's disease affects about 190 people per 100,000. Recent gender predominance studies show that Ménière's tends to affect women more often than men. Age of onset typically occurs in adult years, with prevalence increasing with age.
Recent research has found that Ménière's disease may potentially be influenced and worsened by obstructive sleep apnea, and that risk factors for reduced vascular function in the brain such as smoking, migraines, and atherosclerosis may play an important role in triggering attacks.
Diagnosis.
Doctors establish a diagnosis with complaints and medical history. However, a detailed otolaryngological examination, audiometry, and head MRI scan should be performed to exclude a vestibular schwannoma or superior canal dehiscence which would cause similar symptoms. Some of the same symptoms also occur with benign paroxysmal positional vertigo (BPPV), and with cervical spondylosis (which can affect blood supply to the brain and cause vertigo). Ménière's disease is an idiopathic and therefore a diagnosis of exclusion, meaning there is no definitive test for Ménière's; it is only diagnosed when all other possible causes of the patient's symptom have been ruled out.
History.
Ménière's disease had been recognized as early as the 1860s, but it was still relatively vague and broad at the time. The American Academy of Otolaryngology-Head and Neck Surgery Committee on Hearing and Equilibrium (AAO HNS CHE) set criteria for diagnosing Ménière's, as well as defining two sub categories of Ménière's: cochlear (without vertigo) and vestibular (without deafness).
In 1972, the academy defined criteria for diagnosing Ménière's disease as:
In 1985, this list changed to alter wording, such as changing "deafness" to "hearing loss associated with tinnitus, characteristically of low frequencies" and requiring more than one attack of vertigo to diagnose. Finally in 1995, the list was again altered to allow for degrees of the disease:
Management.
Several environmental and dietary changes are thought to reduce the frequency or severity of symptom outbreaks. It is believed that since high sodium intake causes water retention, a diet high in salt can lead to an increase (or at least prevent the decrease) of fluid within the inner ear, although the relationship between salt and the inner ear is not fully understood. Thus, a low sodium diet is often prescribed, with sodium intake reduced to one to two grams of sodium per day (equivalent to approximately 2.5 to 5 grams of table salt, or a little more than one third to two thirds of a teaspoon). By comparison, the recommended Upper Limit (UL) for sodium intake is 2.3 grams per day, and most people are recommended to consume less than 1.5 grams, but on average people in the United States consume 3.4 grams per day. Diuretics have traditionally been prescribed to increase sodium excretion through the urine and thus (it is thought) enhance the effect of sodium restriction, although there is no definite supportive evidence. Some sources recommend taking two grams of potassium or more daily for similar reasons.
Additionally, patients may be advised to avoid alcohol, caffeine, and tobacco, all of which can aggravate symptoms of Ménière's. Many patients will have allergy testing done to see if they are candidates for allergy desensitization, as allergies have been shown to aggravate Ménière's symptoms.
Both prescription and over-the-counter medicine can be used to reduce nausea and vomiting during an episode. Included are antihistamines such as meclozine or dimenhydrinate, trimethobenzamide and other antiemetics, betahistine, diazepam, or ginger root. Betahistine, specifically, is of note because it is the only drug listed that has been proposed to prevent symptoms due to its vasodilation effect on the inner ear.
The antiherpes virus drug acyclovir has been used with some success to treat Ménière's disease. The likelihood of the effectiveness of the treatment was found to decrease with increasing duration of the disease, probably because viral suppression does not reverse damage. Morphological changes to the inner ear of Ménière's sufferers have also been found in which it was considered likely to have resulted from attack by a herpes simplex virus. It was considered possible that long term treatment with acyclovir (greater than six months) would be required to produce an appreciable effect on symptoms. Herpes viruses have the ability to remain dormant in nerve cells by a process known as HHV Latency Associated Transcript. Continued administration of the drug should prevent reactivation of the virus and allow for the possibility of an improvement of symptoms. Another consideration is that different strains of a herpes virus can have different characteristics which may result in differences in the precise effects of the virus. Further confirmation that acyclovir can have a positive effect on Ménière's symptoms has been reported.
Studies done over the use of transtympanic micropressure pulses have indicated promise with patients who had not been previously treated by gentamicin or surgery. Other studies suggest less clear results and propose that micropressure devices are simply placebos.
Coping.
Sufferers tend to have high stress and anxiety which may be caused directly by the disease and not merely a secondary effect. Vestibular injuries are known to increase levels of anxiety directly by affecting signal processing in the brain, and vice versa, i.e. anxiety negatively affects vestibular signal processing. Some patients benefit from non-specific yoga, t'ai chi, and meditation. Greenberg and Nedzelski recommend education to alleviate feelings of depression or helplessness.
Surgery.
If symptoms do not improve with typical treatment, more permanent surgery is considered. Unfortunately, because the inner ear deals with both balance and hearing, few surgeries guarantee no hearing loss.
Nondestructive surgeries include those which do not actively remove any functionality, but rather aim to improve the way the ear works. Intratympanic steroid treatments involve injecting steroids (commonly dexamethasone) into the middle ear in order to reduce inflammation and alter inner ear circulation. Surgery to decompress the endolymphatic sac has shown to be effective for temporary relief from symptoms. Most patients see a decrease in vertigo occurrence, while their hearing may be unaffected. This treatment, however, does not address the long-term course of vertigo in Ménière's disease and may require repeated surgery. Danish studies even link this surgery to a very strong placebo effect, and that very little difference occurred in a 9-year followup, but could not deny the efficacy of the treatment.
Conversely, destructive surgeries are irreversible and involve removing entire functionality of most, if not all, of the affected ear. The inner ear itself can be surgically removed via labyrinthectomy although hearing is always completely lost in the affected ear with this operation. Alternatively, a chemical labyrinthectomy, in which a drug (such as gentamicin) that "kills" the vestibular apparatus is injected into the middle ear can accomplish the same results while retaining hearing. In more serious cases surgeons can cut the nerve to the balance portion of the inner ear in a vestibular neurectomy. Hearing is often mostly preserved, however the surgery involves cutting open into the lining of the brain, and a hospital stay of a few days for monitoring would be required. Vertigo (and the associated nausea and vomiting) typically accompany the recovery from destructive surgeries as the brain learns to compensate.
Physiotherapy.
Physiotherapists also have a role in the management of Ménière's disease. In vestibular rehabilitation, physiotherapists use interventions aimed at stabilizing gaze, reducing dizziness and increasing postural balance within the context of activities of daily living. After a vestibular assessment is conducted, the physiotherapist tailors the treatment plan to the needs of that specific patient.
The central nervous system (CNS) can be re-trained because of its plasticity, or alterability, as well as its repetitious pathways. During vestibular rehabilitation, physiotherapists take advantage of this characteristic of the CNS by provoking symptoms of dizziness or unsteadiness with head movements while allowing the visual, somatosensory and vestibular systems to interpret the information. This leads to a continuous decrease in symptoms.
Although a significant amount of research has been done regarding vestibular rehabilitation in other disorders, substantially less has been done specifically on Ménière's disease. However, vestibular physiotherapy is currently accepted as part of best practices in the management of this condition.
The Merck Manual has added head trauma as a risk factor due to the research on 300 Ménière's patients over the past fourteen years. Michael Burcon, BPh, DC has established a link between whiplash as a result of vehicular accidents or falling on one's head and Ménière's disease. It takes an average of fifteen years after the trauma before the onset of symptoms. Case history, thermography, MRI, CScan, and/or cervical x-ray and modified Prill relative leg length tests are used for diagnosis and upper cervical specific adjustments are performed for treatment to reduce or eliminate vertigo.
Prognosis.
Ménière's disease usually starts confined to one ear, but it often extends to involve both ears over time. The number of patients who end up with bilaterial Ménière's is debated, with ranges spanning from 17% to 75%.
Some Ménière's disease sufferers, in severe cases, find themselves unable to work. However, a majority (60-80%) of sufferers recover with or without medical help.
Hearing loss usually fluctuates in the beginning stages and becomes more permanent in later stages, although hearing aids and cochlear implants can help remedy damage. Tinnitus can be unpredictable, but patients usually get used to it over time.
Ménière's disease, being unpredictable, has a variable prognosis. Attacks could come more frequently and more severely, less frequently and less severely, and anywhere in between. However, Ménière's is known to "burn out" when vestibular function has been destroyed to a stage where vertigo attacks cease.
Studies done on both right and left ear sufferers show that patients with their right ear affected tend to do significantly worse in cognitive performance. General intelligence was not hindered, and it was concluded that declining performance was related to how long the patient had been suffering from the disease.
References.
</dl>

</doc>
<doc id="55995" url="http://en.wikipedia.org/wiki?curid=55995" title="K. M. Peyton">
K. M. Peyton

Kathleen Wendy Herald Peyton, MBE (born 2 August 1929), who writes primarily as K. M. Peyton, is a British author of fiction for children and young adults. 
She has written more than fifty novels including the much loved "Flambards" series of pony stories, for which she won both the 1969 Carnegie Medal from the Library Association and the 1970 Guardian Children's Fiction Prize, judged by a panel of British children's writers. In 1979 the Flambards trilogy was adapted by Yorkshire Television as a 13-part TV series, "Flambards", starring Christine McKenna as the heroine Christina Parsons.
Biography.
Kathleen Herald was born in Birmingham, began writing when she was nine, and was first published when she was fifteen. She "never decided to become a writer ... [she] just was one." Growing up in London where she could not have a horse she was obsessed with them: all her early books are about girls who have ponies. After school, she went to Kingston Art School, then Manchester Art School. There she met another student, Mike Peyton, an ex-serviceman who had been a military artist and prisoner of war. He shared her love of walking in the Pennines. They married when she was twenty-one and went travelling around Europe.
When they returned to Britain, Peyton completed a teaching diploma. However, after the birth of her second daughter, she turned to writing full-time: mostly boys' adventure stories that she sold as serials to "The Scout", magazine of The Scout Association, and later published in full. She began writing as 'K. M. Peyton' at this time; 'M' represented her husband Mike who helped create the plots.
The Peytons loved sailing, and her first books were on that subject; soon, however, she returned to her 'first love', horses, and began to write what became the "Flambards" series. When Peyton became involved with horse racing, she used those experiences as further inspiration for writing.
Fidra Books has reissued "Fly-By-Night" and its sequel, "The Team" (Ruth Hollis series). Oxford University Press, Usborne Publishing and David Fickling Books also publish her work.
Writers who cite K M Peyton as an influence include Linda Newbery, whose young adult novel "The Damage Done" (2001, Scholastic) is dedicated "to Kathleen Peyton, who made me want to try".
"Flambards" was published in Italian, German, Finnish, and Swedish-language editions during the 1970s. WorldCat lists eight other languages of publication for her works in all.
Awards.
Peyton won the Guardian Prize for the Flambards trilogy, exceptionally, and won the Carnegie Medal for its second book.
She was also a commended runner-up for the Carnegie Medal six times in eight years during the 1960s. One of the books was the first Flambards book, another was the third Flambards book in competition with the Medal-winning second. The others were "Windfall" (1962), "The Maplin Bird" (1964), "The Plan for Birdmarsh" (1965), and "Thunder in the Sky" (1966).
Peyton was appointed Member of the Order of the British Empire (MBE) in the 2014 New Year Honours for services to children's literature.
Adaptations.
The Flambards trilogy was adapted by Yorkshire Television in 1978 as a TV series comprising 13 episodes broadcast 1979 in the UK, 1980 in the US: "Flambards", starring Christine McKenna as the heroine Christina Parsons. 
"The Right-Hand Man" (1977), a historical novel featuring an English stagecoach driver, was adapted as a feature film shot in Australia during 1985 and released there in 1987.
"Who, Sir? Me, Sir?" (1985) was adapted as a BBC TV series.
Works.
The bibliography of Peyton's "pony books only" by Jane Badger Books includes all nineteen series books and many "other books" (‡) listed here.
Flambards.
Peyton's extension of the trilogy followed its television adaptation and reversed the original ending.
Pennington.
The Pennington series continues the story of Ruth Hollis.
Jonathan Meredith.
See also the Ruth Hollis series.
Minna.
Set in Roman Britain.
Other books.
By age fifteen, Kathleen Herald had written "about ten more" novels that publishers rejected with "very nice letters".
‡ Jane Badger Books lists these titles among Peyton's "pony books only" – as well as all nineteen series books listed above.

</doc>
<doc id="55998" url="http://en.wikipedia.org/wiki?curid=55998" title="Prosper Ménière">
Prosper Ménière

Prosper Ménière (18 June 1799 – 7 February 1862) was born in Angers, France. He was lycée- and university-educated and he excelled at humanities and classics. He completed his gold medal in medical studies at Hôtel-Dieu de Paris in 1826, and his M.D. in 1828. He then assisted Guillaume Dupuytren.
Ménière was originally set to be an assistant professor in faculty, but political tensions disturbed his professorship and he was sent to control the spread of cholera. He received a legion of honor for his work, but never gained professorship. After securing the position of physician-in-chief at the Institute for deaf-mutes, he focused on the diseases of the ear.
Ménière's studies at the deaf-mute institute helped formulate his paper, "On a particular kind of hearing loss resulting from lesions of the inner ear" which ultimately led to the recognition of Ménière's disease.
There is debate as to how Ménière's name is spelled. Prosper himself was known to write his name as "Menière" while his son used the spelling "Ménière." Many people omit the accent marks.
References.
</dl>

</doc>
<doc id="55999" url="http://en.wikipedia.org/wiki?curid=55999" title="Tongue">
Tongue

The tongue is a muscular hydrostat on the floor of the mouth of most vertebrates which manipulates food for mastication. It is the primary organ of taste (gustation), as much of its upper surface is covered in taste buds. The tongue's upper surface is also covered in numerous lingual papillae. It is sensitive and kept moist by saliva, and is richly supplied with nerves and blood vessels. In humans a secondary function of the tongue is phonetic articulation. The tongue also serves as a natural means of cleaning one's teeth. The ability to perceive different tastes is not localised in different parts of the tongue, as is widely believed. This error arose because of misinterpretation of some 19th-century research (see tongue map).
Structure.
The tongue is a muscular structure that forms part of the floor of the oral cavity. The human tongue is divided into anterior and posterior parts. The anterior part is the visible part situated at the front, and roughly two-thirds the length of the tongue. The posterior part is the part closest to the throat, and roughly one-third of length. These parts differ in terms of their embryological development and nerve supply. The two parts of the tongue are divided by the terminal sulcus.:989–990
The left and right sides of the tongue are separated by the lingual septum.
The anterior tongue is, at its apex, thin and narrow, it is directed forward against the lingual surfaces of the lower incisor teeth. The posterior part is, at its root, directed backward, and connected with the hyoid bone by the hyoglossi and genioglossi muscles and the hyoglossal membrane; with the epiglottis by three folds (glossoepiglottic) of mucous membrane; with the soft palate by the glossopalatine arches; and with the pharynx by the superior pharyngeal constrictor muscle and the mucous membrane. It also forms the anterior wall of the oropharynx
In phonetics and phonology, a distinction is made between the tip of the tongue and the blade (the portion just behind the tip). Sounds made with the tongue tip are said to be apical, while those made with the tongue blade are said to be laminal.
Muscles.
The eight muscles of the human tongue are classified as either "intrinsic" or "extrinsic". The four intrinsic muscles act to change the shape of the tongue, and are not attached to any bone. The four extrinsic muscles act to change the position of the tongue, and are anchored to bone.
Extrinsic.
The extrinsic muscles originate from bone and extend to the tongue. Their main functions are altering the tongue's position allowing for protrusion, retraction, and side-to-side movement.:991
Intrinsic.
Four paired intrinsic muscles of the tongue originate and insert within the tongue, running along its length. These muscles alter the shape of the tongue by: lengthening and shortening it, curling and uncurling its apex and edges, and flattening and rounding its surface. This provides shape, and helps facilitate speech, swallowing, and eating.:991
Blood supply.
The tongue receives its blood supply primarily from the lingual artery, a branch of the external carotid artery. The lingual veins, drain into the internal jugular vein. The floor of the mouth also receives its blood supply from the lingual artery.:993–994 There is also a secondary blood supply to the tongue from the tonsillar branch of the facial artery and the ascending pharyngeal artery.
An area in the neck sometimes called Pirogov's triangle is formed by the intermediate tendon of the digastric muscle, the posterior border of the mylohyoid muscle, and the hypoglossal nerve. The lingual artery is a good place to stop severe hemorrage from the tongue.
Innervation.
Innervation of the tongue consists of motor fibers, special sensory fibers for taste, and general sensory fibers for sensation.:994–5
Innervation of taste and sensation is different for the anterior and posterior part of the tongue because they are derived from different embryological structures (pharyngeal arch 1 and pharyngeal arch 3 and 4, respectively).
Histology.
The tongue is covered with numerous taste buds, and filiform, fungiform, vallate and foliate, lingual papillae.:990
Length.
The average length of the human tongue from the oropharynx to the tip is 10cms in length.
Development.
The anterior tongue is derived primarily from the first pharyngeal arch. The posterior tongue is derived primarily from the third pharyngeal arch. The second arch however has a substantial contribution during fetal development, but this later atrophies. The fourth arch may also contribute, depending upon how the boundaries of the tongue are defined.
The terminal sulcus, which separates the anterior and posterior tongue, is shaped like a V, with the tip of the V situated posteriorly. At the apex is the foramen caecum, which is the point where the embryological thyroid begins to descend.:990
Function.
Taste.
Chemicals that stimulate taste receptor cells are known as tastants. Once a tastant is dissolved in saliva, it can make contact with the plasma membrane of the gustatory hairs, which are the sites of taste transduction.
The tongue is equipped with many taste buds on its dorsal surface, and each taste bud is equipped with taste receptor cells that can sense particular classes of tastes. Distinct types of taste receptor cells respectively detect substances that are sweet, bitter, salty, sour, spicy, or taste of umami. Umami receptor cells are the least understood and accordingly are the type most intensively under research.
Mastication.
The tongue is also used for crushing food against the hard palate, during mastication. The epithelium on the tongue’s upper, or dorsal surface is keratinised. Consequently, the tongue can grind against the hard palate without being itself damaged or irritated. 
Clinical relevance.
Disease.
After the gums, the tongue is the second most common soft tissue site for various pathologies in the oral cavity. Examples of pathological conditions of the tongue include glossitis (e.g. geographic tongue, median rhomboid glossitis), burning mouth syndrome, oral hairy leukoplakia, oral candidiasis and squamous cell carcinoma. Food debris, desquamated epithelial cells and bacteria often form a visible tongue coating. This coating has been identified as a major contributing factor in bad breath (halitosis), which can be managed by brushing the tongue gently with a toothbrush or using special oral hygiene instruments such as tongue scrapers or mouth brushes.
Medical delivery.
The sublingual region underneath the front of the tongue is a location where the oral mucosa is very thin, and underlain by a plexus of veins. This is an ideal location for introducing certain medications to the body. The sublingual route takes advantage of the highly vascular quality of the oral cavity, and allows for the speedy application of medication into the cardiovascular system, bypassing the gastrointestinal tract. This is the only convenient and efficacious route of administration (apart from I.V. administration) of nitroglycerin to a patient suffering chest pain from angina pectoris.
Society and culture.
Figures of speech.
The tongue can be used as a metonym for "language", as in the phrase "mother tongue". Many languages have the same word for "tongue" and "language".
A common temporary failure in word retrieval from memory is referred to as the "tip-of-the-tongue" phenomenon. The expression "tongue in cheek" refers to a statement that is not to be taken entirely seriously – something said or done with subtle ironic or sarcastic humour. A "tongue twister" is a phrase made specifically to be very difficult to pronounce. Aside from being a medical condition, "tongue-tied" means being unable to say what you want to due to confusion or restriction. The phrase "cat got your tongue" refers to when a person is speechless. To "bite one's tongue" is a phrase which describes holding back an opinion to avoid causing offence. A "slip of the tongue" refers to an unintentional utterance, such as a Freudian slip. Speaking in tongues is a common phrase used to describe "glossolalia", which is to make smooth, language-resembling sounds that is no true spoken language itself. A deceptive person is said to have a forked tongue, and a smooth-talking person said to have a silver tongue.
Gestures.
Sticking one's tongue out at someone is considered a childish gesture of rudeness and/or defiance in many countries; the act may also have sexual connotations, depending on the way in which it is done. However, in Tibet it is considered a greeting. In 2009, a farmer from Fabriano, Italy was convicted and fined by the country's highest court for sticking his tongue out at a neighbor with whom he had been arguing. Proof of the affront had been captured with a cell phone camera. Blowing a raspberry can also be meant as a gesture of derision.
Body art.
Being a cultural custom for long time, tongue piercing and splitting has become quite common in western countries in recent decades, with up to one-fifth of young adults having at least one piece of body art in the tongue.
As food.
The tongues of some animals are consumed and sometimes considered delicacies. Hot tongue sandwiches are frequently found on menus in Kosher delicatessens in America. Taco de lengua ("lengua" being Spanish for tongue) is a taco filled with beef tongue, and is especially popular in Mexican cuisine. As part of Colombian gastronomy, Tongue in Sauce (Lengua en Salsa), is a dish prepared by frying the tongue, adding tomato sauce, onions and salt. Tongue can also be prepared as birria. Pig and beef tongue are consumed in Chinese cuisine. Duck tongues are sometimes employed in Szechuan dishes, while lamb's tongue is occasionally employed in Continental and contemporary American cooking. Fried cod "tongue" is a relatively common part of fish meals in Norway and Newfoundland. In Argentina and Uruguay cow tongue is cooked and served in vinegar ("lengua a la vinagreta"). In the Czech Republic and Poland, a pork tongue is considered a delicacy, and there are many ways of preparing it. In Eastern Slavic countries, pork and beef tongues are commonly consumed, boiled and garnished with horseradish or jelled; beef tongues fetch a significantly higher price and are considered more of a delicacy. In Alaska, cow tongues are among the more common.
Tongues of seals and whales have been eaten, sometimes in large quantities, by sealers and whalers, and in various times and places have been sold for food on shore.
History.
Etymology.
The word tongue derives from the Old English "tunge", which comes from Proto-Germanic *"tungōn". It has cognates in other Germanic languages — for example "tonge" in West Frisian, "tong" in Dutch/Afrikaans, "Zunge" in German, "tunge" in Danish/Norwegian and "tunga" in Icelandic/Faroese/Swedish. The "ue" ending of the word seems to be a fourteenth-century attempt to show "proper pronunciation", but it is "neither etymological nor phonetic". Some used the spelling "tunge" and "tonge" as late as the sixteenth century.
Other animals.
Most vertebrate animals have tongues. In mammals such as dogs and cats, the tongue is often used to clean the fur and body. The tongues of these species have a very rough texture which allows them to remove oils and parasites. A dog's tongue also acts as a heat regulator. As a dog increases its exercise the tongue will increase in size due to greater blood flow. The tongue hangs out of the dog's mouth and the moisture on the tongue will work to cool the bloodflow.
Some animals have tongues that are specially adapted for catching prey. For example, chameleons, frogs, and anteaters have prehensile tongues.
Many species of fish have small folds at the base of their mouths that might informally be called tongues, but they lack a muscular structure like the true tongues found in most tetrapods.
Other animals may have organs that are analogous to tongues, such as a butterfly's proboscis or a radula on a mollusc, but these are not homologous with the tongues found in vertebrates, and often have little resemblance in function, for example, butterflies do not lick with their proboscides; they suck through them, and the proboscis is not a single organ, but two jaws held together to form a tube.
References.
"This article incorporates text in the public domain from the 20th edition of Gray's Anatomy (1918)"

</doc>
<doc id="56001" url="http://en.wikipedia.org/wiki?curid=56001" title="Conformance testing">
Conformance testing

Conformance testing or type testing is testing to determine whether a product or system or just a medium complies with the requirements of a specification, contract or regulation. This may apply to various technical terms as well as to pure formal terms with respect to obligations of the contractors.
Testing is often either logical testing or physical testing. The test procedures may involve other criteria from mathematical testing or chemical testing. Beyond simple conformance other requirements for efficiency, interoperability or compliance may apply. 
To aid in the aim towards a conformance proof, various test procedures and test setups have been developed, either by the standard's maintainers or external auditing organizations, specifically for testing conformance to standards. Conformance testing is performed preferably by independent organizations, which may be the standards body itself, to give sound assurance of compliance. 
Products tested conformamnce may then become advertised as being certified by the testing organization as complying with the referred technical standard. Service providers, equipment manufacturers, and equipment suppliers rely on such qualified data to ensure Quality of Service (QoS) through this conformance process.
Typical areas of application.
Conformance testing is applied to various areas of application, as e.g.
In all such testing the subject of test is not just the formal conformance in aspects of e.g.
but especially the aspects of e.g.
Hence conformance testing leads to a vast set of documents and files that allow for re-iterating all performed tests.
Software engineering.
In software testing, conformance testing verifies that a product performs according to its specified standards. Compilers, for instance, are extensively tested to determine whether they meet the recognized standard for that language.
Electronic and electrical engineering.
In electronic engineering and electrical engineering, some countries and business environments (such as telecommunication companies) require that an electronic product meet certain requirements before they can be sold. Standards for telecommunication products written by standards organizations such as ANSI, the FCC, and IEC, etc., have certain criteria that a product must meet before compliance is recognized. In countries such as Japan, China, Korea, and some parts of Europe, products cannot be sold unless they are known to meet those requirements specified in the standards. Usually, manufacturers set their own requirements to ensure product quality, sometimes with levels much higher than what the governing bodies require. Compliance is realized after a product passes a series of tests without occurring some specified mode of failure. Failure levels are usually set depending on what environment the product will be sold in. For instance, test on a product for used in an industrial environment will not be as stringent as a product used in a residential area. A failure can include data corruption, loss of communication, and irregular behavior.
Compliance test for electronic devices include emissions tests, immunity tests, and safety tests. Emissions tests ensure that a product will not emit harmful electromagnetic interference in communication and power lines. Immunity tests ensure that a product is immune to common electrical signals and Electromagnetic interference (EMI) that will be found in its operating environment, such as electromagnetic radiation from a local radio station or interference from nearby products. Safety tests ensure that a product will not create a safety risk from situations such as a failed or shorted power supply, blocked cooling vent, and powerline voltage spikes and dips. 
For example, the telecommunications research and development company Telcordia Technologies publishes conformance standards for telecommunication equipment to pass the following tests: 
Telecom and datacom protocols.
In protocol testing, TTCN-3 has been used successfully to deploy a number of test systems, including protocol conformance testers for SIP, WiMAX, and DSRC.
Based on 3GPP and non-3GPP specification, the test equipment vendors develops the test cases and validated by the bodies.

</doc>
<doc id="56045" url="http://en.wikipedia.org/wiki?curid=56045" title="606 BC">
606 BC


</doc>
<doc id="56046" url="http://en.wikipedia.org/wiki?curid=56046" title="538 BC">
538 BC


</doc>
<doc id="56061" url="http://en.wikipedia.org/wiki?curid=56061" title="Discrete space">
Discrete space

In topology, a discrete space is a particularly simple example of a topological space or similar structure, one in which the points form a "discontinuous sequence", meaning they are "isolated" from each other in a certain sense. The discrete topology is the finest topology that can be given on a set, i.e., it defines all subsets as open sets. In particular, each singleton is an open set in the discrete topology.
Definitions.
Given a set "X":
for any formula_3. In this case formula_4 is called a discrete metric space or a space of isolated points.
A metric space formula_16 is said to be "uniformly discrete" if there exists a "packing radius" formula_17 such that, for any formula_18, one has either formula_19 or formula_20. The topology underlying a metric space can be discrete, without the metric being uniformly discrete: for example the usual metric on the set {1, 1/2, 1/4, 1/8, ...} of real numbers.
Properties.
The underlying uniformity on a discrete metric space is the discrete uniformity, and the underlying topology on a discrete uniform space is the discrete topology.
Thus, the different notions of discrete space are compatible with one another.
On the other hand, the underlying topology of a non-discrete uniform or metric space can be discrete; an example is the metric space "X" := {1/"n" : "n" = 1,2,3...} (with metric inherited from the real line and given by d("x","y") = |"x" − "y"|).
Obviously, this is not the discrete metric; also, this space is not complete and hence not discrete as a uniform space.
Nevertheless, it is discrete as a topological space.
We say that "X" is "topologically discrete" but not "uniformly discrete" or "metrically discrete".
Additionally:
Any function from a discrete topological space to another topological space is continuous, and any function from a discrete uniform space to another uniform space is uniformly continuous. That is, the discrete space "X" is free on the set "X" in the category of topological spaces and continuous maps or in the category of uniform spaces and uniformly continuous maps. These facts are examples of a much broader phenomenon, in which discrete structures are usually free on sets.
With metric spaces, things are more complicated, because there are several categories of metric spaces, depending on what is chosen for the morphisms. Certainly the discrete metric space is free when the morphisms are all uniformly continuous maps or all continuous maps, but this says nothing interesting about the metric structure, only the uniform or topological structure. Categories more relevant to the metric structure can be found by limiting the morphisms to Lipschitz continuous maps or to short maps; however, these categories don't have free objects (on more than one element). However, the discrete metric space is free in the category of bounded metric spaces and Lipschitz continuous maps, and it is free in the category of metric spaces bounded by 1 and short maps. That is, any function from a discrete metric space to another bounded metric space is Lipschitz continuous, and any function from a discrete metric space to another metric space bounded by 1 is short.
Going the other direction, a function "f" from a topological space "Y" to a discrete space "X" is continuous if and only if it is "locally constant" in the sense that every point in "Y" has a neighborhood on which "f" is constant.
Uses.
A discrete structure is often used as the "default structure" on a set that doesn't carry any other natural topology, uniformity, or metric; discrete structures can often be used as "extreme" examples to test particular suppositions. For example, any group can be considered as a topological group by giving it the discrete topology, implying that theorems about topological groups apply to all groups. Indeed, analysts may refer to the ordinary, non-topological groups studied by algebraists as "discrete groups" . In some cases, this can be usefully applied, for example in combination with Pontryagin duality. A 0-dimensional manifold (or differentiable or analytical manifold) is nothing but a discrete topological space. We can therefore view any discrete group as a 0-dimensional Lie group.
A product of countably infinite copies of the discrete space of natural numbers is homeomorphic to the space of irrational numbers, with the homeomorphism given by the continued fraction expansion. A product of countably infinite copies of the discrete space {0,1} is homeomorphic to the Cantor set; and in fact uniformly homeomorphic to the Cantor set if we use the product uniformity on the product. Such a homeomorphism is given by using ternary notation of numbers. (See Cantor space.)
In the foundations of mathematics, the study of compactness properties of products of {0,1} is central to the topological approach to the ultrafilter principle, which is a weak form of choice.
Indiscrete spaces.
In some ways, the opposite of the discrete topology is the trivial topology (also called the "indiscrete topology"), which has the fewest possible open sets (just the empty set and the space itself). Where the discrete topology is initial or free, the indiscrete topology is final or cofree: every function "from" a topological space "to" an indiscrete space is continuous, etc.

</doc>
<doc id="56075" url="http://en.wikipedia.org/wiki?curid=56075" title="Northrop Grumman E-2 Hawkeye">
Northrop Grumman E-2 Hawkeye

The Northrop Grumman E-2 Hawkeye is an American all-weather, carrier-capable tactical airborne early warning (AEW) aircraft. This twin-turboprop aircraft was designed and developed during the late 1950s and early 1960s by the Grumman Aircraft Company for the United States Navy as a replacement for the earlier, radial piston-engined E-1 Tracer, which was rapidly becoming obsolete. The aircraft's performance has been upgraded with the E-2B, and E-2C versions, where most of the changes were made to the radar and radio communications due to advances in electronic integrated circuits and other electronics. The fourth version of the Hawkeye is the E-2D, which first flew in 2007. The E-2 was the first aircraft designed specifically for its role, as opposed to a modification of an existing airframe, such as the Boeing E-3 Sentry. Variants of the Hawkeye have been in continuous production since 1960, giving it the longest production run of any carrier-based aircraft.
The E-2 also received the nickname ""Super Fudd" because it replaced the E-1 Tracer "Willy Fudd"". In recent decades, the E-2 has been commonly referred to as the "Hummer" because of the distinctive sounds of its turboprop engines, quite unlike that of turbojet and turbofan jet engines. In addition to U.S. Navy service, smaller numbers of E-2s have been sold to the armed forces of Egypt, France, Israel, Japan, Mexico, Singapore and Taiwan.
Development.
Background.
Continual improvements in airborne radars through 1956 led to the construction of AEW airplanes by several different countries and several different armed forces. The functions of command and control and sea & air surveillance were also added. The first carrier-based aircraft to perform these missions for the U.S. Navy and its allies was the Douglas AD Skyraider, which was replaced in US Navy service by the Grumman E-1 Tracer, which was a modified version of the S-2 Tracker twin-engine anti-submarine warfare aircraft, where the radar was carried in an aerofoil-shaped radome carried above the aircraft's fuselage. The E-1 was used by the U.S. Navy from 1958 to 1977.
E-2A and E-2B Hawkeye.
In 1956, the U.S. Navy developed a requirement for an airborne early warning aircraft where its data could be integrated into the Naval Tactical Data System aboard the Navy's ships, with a design from Grumman being selected to meet this requirement in March 1957. Its design, initially designated W2F-1, but later redesignated the E-2A Hawkeye, was the first carrier plane that had been designed from its wheels up as an AEW and command and control airplane. The problems facing the design engineers at Grumman were immense, and were compounded by having to constrain the design to enable the aircraft to operate from the older modified "Essex"-class aircraft carriers. These ‘smaller’ carriers were built during World War II and later modified to allow them to operate jet aircraft. Consequently, various height, weight and length restrictions had to be factored into the E-2A design, resulting in some handling characteristics which were less than ideal. The E-2A actually never operated from the modified Essex class carriers, and it is likely the design would have benefited considerably if this requirement had never been imposed.
The first prototype, acting as an aerodynamic testbed only, flew on 21 October 1960. The first fully equipped aircraft followed it on 19 April 1961, and entered service with the US Navy as the E-2A in January 1964. By 1965 the major development problems delaying the E-2A Hawkeye got so bad that the aircraft was actually cancelled after 59 aircraft had already been built. Particular difficulties were being experienced due to inadequate cooling in the closely packed avionics compartment. Early computer and complex avionics systems generated considerable heat; without proper ventilation this would lead to system failures. These failures continued long after the aircraft entered service and at one point reliability was so bad the entire fleet of aircraft was grounded. The airframe was also prone to corrosion, a serious problem in a carrier based aircraft.
After Navy officials had been forced to explain to Congress why four production contracts had been signed before avionics testing had been completed, action was taken; Grumman and the US Navy scrambled to improve the design. The unreliable rotary drum computer was replaced by a Litton L-304 digital computer and various avionics systems were replaced – the upgraded aircraft were designated E-2Bs. In total, 49 of the 59 E-2As were upgraded to E-2B standard. These aircraft replaced the E-1B Tracers in the various US Navy AEW squadrons and it was the E-2B that was to set a new standard for carrier based AEW aircraft.
E-2C Hawkeye and developments.
Although the upgraded E-2B was a vast improvement on the unreliable E-2A, it was an interim measure. The US Navy knew the design had much greater capability and had yet to achieve the performance and reliability parameters set out in the original 1957 design. In April 1968 a reliability improvement program was instigated. In addition, now that the capabilities of the aircraft were starting to be realized, more were desired; 28 new E-2Cs were ordered to augment the 49 E-2Bs that would be upgraded. Improvements in the new and upgraded aircraft were concentrated in the radar and computer performance.
Two E-2A test machines were modified as E-2C prototypes, the first flying on 20 January 1971. Trials proved satisfactory and the E-2C was ordered into production, the first production machine performed its initial flight on 23 September 1972. The original E-2C, known as Group 0, consisted of 55 aircraft; the first aircraft became operational in 1973 and serving on carriers in the 1980s and 1990s, until they were replaced in first-line service by Group II aircraft. US Navy Reserve used some aircraft for tracking drug smugglers. The type was commonly used in conjunction with Grumman F-14 Tomcat fighters; monitoring airspace and then vectoring Tomcats over the Link-4A datalink to destroy potential threats with long range AIM-54C Phoenix missiles.
The next production run, between 1988 and 1991, saw 18 aircraft built to the Group I standard. Group I aircraft replaced the E-2's older APS-125 radar and T56-A-425 turboprops with their successors, the APS-139 radar system and T56-A-427 turboprops. The first Group I aircraft entered service on August 1981. Upgrading the Group 0 aircraft to Group 1 specifications was considered, but the cost was comparable to a new production aircraft, so upgrades were not conducted. Group 1 aircraft were only flown by the Atlantic fleet squadrons. This version was followed within a few years by the Group II, which had the improved APS-145 radar. A total of 50 Group II aircraft were delivered, 12 being upgraded Group I aircraft. This new version entered service in June 1992 and served with the Pacific and Atlantic Fleet squadrons.
By 1997 the US Navy intended that all front line squadrons would be equipped, for a total of 75 Group II aircraft. Grumman merged with Northrop in 1994 and plans began on the next upgrade, known as the Group II Plus, Also known as the Group II / NAV upgrade. The Group II Plus kept the same computer and radar as the Group II, but significantly upgraded the pilot avionics. New improvements included replacing the old mechanical Inertial Navigation System (INS) with a more reliable and more accurate laser Ring Gyroscope driven INS, the inclusion of dual Multifunction Display Units (MFCDUs) (vice one in the Group II), and the integration of GPS into the weapon system. A variant of the Group II with upgrades to the mission computer and CIC workstations is referred to as the MCU/ACIS. However these were produced in small numbers due to the Hawkeye 2000 being produced soon after its introduction. All Group II aircraft have had their 1960s vintage computer processors replaced by a mission computer with the same functionality that used more modern computer technology, referred to as the GrIIM RePr (Group II Mission Computer Replacement Program, pronounced "grim reaper").
Another upgrade to the Group II was the Hawkeye 2000 which featured the same APS-145 radar but incorporated an upgraded mission computer and CIC (Combat Information Center) workstations (Advanced Control Indicator Set or ACIS), and carries the U.S. Navy’s new CEC (cooperative engagement capability) data-link system. It is also fitted with a larger capacity vapor cycle avionics cooling system. Starting in 2007 a hardware and software upgrade package began to be added to existing Hawkeye 2000 aircraft. This upgrade allows faster processing, double current trackfile capacity and access to satellite information networks. Hawkeye 2000 cockpits being upgraded include solid-state glass displays, and a GPS-approach capability. The remaining Hawkeye Group II NAV Upgrade aircraft received GPS approach capability, but did not get the solid-state glass displays.
In 2004, the E-2C's propeller system was changed; a new eight-bladed propeller system named NP2000 was developed by the Hamilton-Sundstrand company to replace the old four-bladed design. Improvements included reduced vibrations and better maintainability as a result of the ability to remove prop blades individually instead of having to remove the entire prop and hub assembly. The system had previously been used in the C-130 Hercules, which also uses the T-56 engine, to great effect. However one major difference between the C-130J and the E-2C is that the C-130J uses a six-bladed propeller. The E-2C needed to use a new eight-bladed configuration in order to maintain harmonic compatibility with the electronics that were designed for a four-bladed propeller. The propeller blades are of carbon fiber construction with steel leading edge inserts and de-icing boots at the root of the blade.
E-2D Advanced Hawkeye.
Once considered for replacement by the "Common Support Aircraft", this concept was abandoned. The latest E-2 version is the "E-2D Advanced Hawkeye", which features an entirely new avionics suite including the new AN/APY-9 radar, radio suite, mission computer, integrated satellite communications, flight management system, improved T56-A-427A engines, a glass cockpit and changes later enable aerial refueling. The APY-9 radar features an active electronically scanned array, which adds electronic scanning to the mechanical rotation of the radar in its radome. The E-2D will include provisions for the copilot to act as a "Tactical 4th Operator" (T4O), who can reconfigure his main cockpit display to show radar, IFF, and Link 16 (JTIDS)/CEC, and access all acquired data. The E-2D's first flight occurred on 3 August 2007. On 8 May 2009, an E-2D used its Cooperative Engagement Capability system to engage an overland cruise missile with a Standard Missile SM-6 fired from another platform in an integrated fire-control system test. These two systems will form the basis of the Naval Integrated Fire Control – Counter Air (NIFC-CA) when fielded in 2015; the USN is investigating adding other systems to the NIFC-CA network in the future.
The APY-9 radar has been suspected of being capable of detecting fighter-sized stealth aircraft, which are typically optimized against high frequencies like Ka, Ku, X, C, and parts of the S-bands. Small aircraft lack the size or weight allowances for all-spectrum low-observable features, leaving a vulnerability to detection by the UHF-band APY-9 radar, potentially detecting fifth-generation fighters like the Russian Sukhoi PAK FA and the Chinese Chengdu J-20 and Shenyang J-31. Historically, UHF radars had resolution and detection issues that made them ineffective for accurate targeting and fire control; Northrop Grumman and Lockheed claim that the APY-9 has solved these shortcomings in the APY-9 using advanced electronic scanning and high digital computing power via space/time adaptive processing. According to the Navy's NIFC-CA concept, the E-2D could guide fleet weapons, such as AIM-120 AMRAAM and SM-6 missiles, onto targets beyond a launch platform's detection range or capabilities.
Deliveries of initial production E-2Ds began in 2010. On 4 February 2010, Delta One conducted the first E-2D carrier landing aboard USS "Harry S. Truman" as a part of carrier suitability testing. On 27 September 2011, an E-2D was successfully launched by the prototype Electromagnetic Aircraft Launch System (EMALS) at Naval Air Engineering Station Lakehurst. On 12 February 2013, the Office of the Secretary of Defense approved the E-2D to enter full-rate production. The Navy plans for an initial operational capability by 2015. In June 2013, the 10th E-2D was delivered to the Navy, with an additional 10 aircraft in various stages of manufacturing and predelivery flight testing. On 18 July 2013, Northrop Grumman was awarded a $113.7 million contract for five full-rate production Lot 2 E-2D Advanced Hawkeye aircraft. On 13 August 2013, Northrop Grumman was awarded a $617 million contract for five E-2Ds until full-rate production Lot 1. On 30 June 2014, Northrop Grumman was awarded a $3.6 billion contract to supply 25 more E-2Ds, for a total contracted number of 50 aircraft; 13 E-2D models had been delivered by that point. The Navy announced in October 2014 that the first E-2D squadron, VAW-125, was operational. The first five aircraft were assigned to Carrier Air Wing One aboard the aircraft . On 11 March 2015, the "Theodore Roosevelt" Carrier Strike Group departed Naval Station Norfolk for around the world tour with deployments to the U.S. 5th, 6th and 7th Fleets. 
Design.
The E-2 is a high-wing airplane, with one turboprop engine on each wing and retractable tricycle landing gear. As with most carrier-borne airplanes, the E-2 is equipped with a tail hook for recovery (landing), and the nose gear can attach to a shuttle of the aircraft carrier's catapults for launch (takeoff). A distinguishing feature of the Hawkeye is its 24-foot (7.3 m) diameter rotating dome that is mounted above its fuselage and wings. This carries the E-2's primary antennas for its long-range radar and IFF systems. No other carrier-borne aircraft possesses one of these, and among land-based aircraft, they are mostly seen atop the Boeing E-3 Sentry, a larger AWACS airplane operated by the U.S. Air Force and NATO air forces in large numbers.
The aircraft is operated by a crew of five, with the pilot and co-pilot on the flight deck and the combat information center officer, air control officer and radar operator stations located in the rear fuselage directly beneath the rotodome.
In U.S. service, the E-2 Hawkeye provides all-weather airborne early warning and command and control capabilities for all aircraft-carrier battle groups. In addition, its other purposes include sea and land surveillance, the control of the aircraft carrier's fighter planes for air defense, the control of strike aircraft on offensive missions, the control of search and rescue missions for naval aviators and sailors lost at sea, and for the relay of radio communications, air-to-air and ship-to-air. It can also serve in an air traffic control capacity in emergency situations when land-based ATC is unavailable.
The E-2C and E-2D Hawkeyes use advanced electronic sensors combined with digital computer signal processing, especially its radars, for early warning of enemy aircraft attacks and anti-ship missile attacks, and the control of the carrier's combat air patrol (CAP) fighters, and secondarily for surveillance of the surrounding sea and land for enemy warships and guided-missile launchers, and any other electronic surveillance missions as directed.
Operational history.
US Navy.
The E-2A entered U.S. Navy service on January 1964, and in April 1964 with VAW-11 at NAS North Island. The first deployment was aboard USS "Kitty Hawk" (CVA-63) during 1965.
Since entering combat during the Vietnam War, the E-2 has served the US Navy around the world, acting as the electronic "eyes of the fleet". In August 1981, a Hawkeye from VAW-124 "Bear Aces" directed two F-14 Tomcats from VF-41 "Black Aces" in an intercept mission in the Gulf of Sidra that resulted in the downing of two Libyan Sukhoi Su-22s. Hawkeyes from VAW-123 aboard the aircraft carrier directed a group of F-14 Tomcat fighters flying the Combat Air Patrol during Operation El Dorado Canyon, the joint strike of two Carrier Battle Groups in the Mediterranean Sea against Libyan terrorist targets during 1986. More recently, E-2Cs provided the command and control for both aerial warfare and land-attack missions during the Persian Gulf War. Hawkeyes have supported the U.S. Coast Guard, the U.S. Customs Service, and American federal and state police forces during anti-drug operations.
In the mid-1980s, several E-2Cs were borrowed from the U.S. Navy and given to the U.S. Coast Guard and the U.S. Customs Service for counternarcotics (CN) and maritime interdiction operations (MIO). This also led to the Coast Guard building a small cadre of Naval Flight Officers (NFOs), starting with the recruitment and interservice transfer of Navy flight officers with E-2 flight experience and the flight training of other junior Coast Guard officers as NFOs. A fatal aircraft mishap on 24 August 1990 involving a Coast Guard E-2C at the former Naval Station Roosevelt Roads in Puerto Rico prompted the Coast Guard to discontinue flying E-2Cs and to return its E-2Cs to the Navy. The U.S Customs Service also returned its E-2Cs to the Navy and concentrated on the use of former U.S. Navy P-3 Orion aircraft in the CN role.
E-2C Hawkeye squadrons played a critical role in air operations during Operation Desert Storm. In one instance, a Hawkeye crew provided critical air control direction to two F/A-18 Hornet aircrew, resulting in the shootdown of two Iraqi MiG-21s. During Operations Southern Watch and Desert Fox, Hawkeye crews continued to provide thousands of hours of air coverage, while providing air-to-air and air-to-ground command and control in a number of combat missions.
The E-2 Hawkeye is a crucial component of all U.S. Navy carrier air wings; each carrier is equipped with four Hawkeyes (five in some situations), allowing for continuous 24-hour-a-day operation of at least one E-2 and for one or two to undergo maintenance in the aircraft carrier's hangar deck at all times. Until 2005. the US Navy Hawkeye’s were organised into East and West coast wings, supporting the respective fleets. However, the East coast wing was disestablished, all aircraft were organised into a single wing based at Point Mugu, California. Six E-2C aircraft were deployed by the US Naval Reserve for drug interdiction and homeland security operations until 9 March 2013, when the sole Reserve squadron, VAW-77 'Nightwolves', was decommissioned and its six aircraft sent to other squadrons.
During Operation Enduring Freedom and Operation Iraqi Freedom all ten Regular Navy Hawkeye squadrons flew overland sorties. They provided battle management for attack of enemy ground targets, close-air-support coordination, combat search and rescue control, airspace management, as well as datalink and communication relay for both land and naval forces. During the aftermath of Hurricane Katrina, three Hawkeye squadrons (two Regular Navy and one Navy Reserve) were deployed in support of civilian relief efforts including Air Traffic Control responsibilities spanning three states, and the control of U.S. Army, U.S. Navy, U.S. Air Force, U.S. Marine Corps, U.S. Coast Guard, and Army National Guard and Air National Guard helicopter rescue units.
Hawkeye 2000s first deployed in 2003 aboard with VAW-117, the "Wallbangers", and CVW-11. U.S. Navy E-2C Hawkeyes have been upgraded with eight-bladed propellers as part of the NP2000 program; the first squadron to cruise with the new propellers was VAW-124 "Bear Aces". The Hawkeye 2000 version can track more than 2,000 targets simultaneously (while at the same time, detecting 20,000 simultaneously) to a range greater than 400 mi and simultaneously guide 40–100 air-to-air intercepts or air-to-surface engagements. In 2014, several E-2C Hawkeyes from the Bear Aces of VAW-124 were deployed from as flying command posts and air traffic controllers over Iraq during Operation Inherent Resolve against the Islamic State.
VAW-120, the E-2C fleet replacement squadron began receiving E-2D Advanced Hawkeyes for training use in July 2010. On 27 March 2014, the first E-2Ds were delivered to the Airborne Early Warning Squadron 125 (VAW-125). The E-2D achieved Initial Operational Capability (IOC) in October 2014 when VAW-125 was certified to have five operational aircraft. This began training on the aircraft for its first operational deployment, scheduled for 2015 aboard . The E-2D will play a larger role than that of the E-2C, with five E-2Ds aboard each carrier instead of the current four C-models, requiring the acquisition of 75 more E-2Ds.
Other operators.
E-2 Hawkeyes have been sold by the U.S. Federal Government under Foreign Military Sales (FMS) procedures to the armed forces of Egypt, France, Israel, Japan, Singapore and Taiwan.
French Naval Aviation.
The French Naval Aviation (Aeronavale) operates three E-2C Hawkeyes and has been the only operator of the E-2 Hawkeye from an aircraft carrier besides the U.S. Navy. The French nuclear-powered carrier, "Charles De Gaulle" (R 91), currently carries two E-2C Hawkeyes on her combat patrols offshore. The third French E-2C Hawkeye have been upgraded with eight-bladed propellers as part of the NP2000 program. In April 2007, France requested the foreign military sale (FMS) of an additional aircraft.
The Flotille 4F of the French Navy's Aeronavale was stood up on 2 July 2000 and flies its E-2C Hawkeyes from its naval air station at Lann-Bihoue or aboard "Charles de Gaulle". They took part in operations in Afghanistan and Libya.
Japan Air Self-Defense Force.
The Japan Air Self-Defense Force bought thirteen E-2C to improve its Early warning capabilities. The E-2C was put into service with the Airborne Early Warning Group (AEWG) at Misawa Air Base in January 1987.
On 6 September 1976, Soviet Air Force pilot Viktor Belenko successfully defected, landing his MiG-25 'Foxbat' at Hakodate Airport, Japan. During this incident, the Japan Self-Defense Forces' radar lost track of the aircraft when Belenko flew his MiG-25 at a low altitude, prompting the JASDF to consider procurement of airborne early warning aircraft.
Initially, the E-3 Sentry airborne warning and control system aircraft was considered to be the prime candidate for the airborne early warning mission by the JASDF. However, the Japanese Defense Agency realized that the E-3 would not be readily available due to USAF needs and opted to procure E-2 Hawkeye aircraft.
On 21 November 2014, the Japanese Ministry of Defense officially decided to procure the E-2D version of the Hawkeye, beating out the Boeing 737 AEW&C design.
Mexico.
In 2004, three former Israel Air Force E-2C aircraft were sold to the Mexican Navy to perform maritime and shore surveillance missions. These aircraft were upgraded locally by IAI. The first Mexican E-2C was rolled out in January 2004.
Singapore.
The Republic of Singapore Air Force acquired four Grumman E-2C Hawkeye airborne early warning aircraft in 1987, which are assigned to the 111 Squadron "Jaeger" based at Tengah Air Base.
In April 2007, it was announced that the four E-2C Hawkeyes were to be replaced with four Gulfstream G550s which would become the primary early warning aircraft of the Singapore Air Force. On 13 April 2012, the newer G550 AEWs officially took over duty from the former.
Israel.
Israel was the first export customer, its four Hawkeyes were delivered during 1981, complete with the folding wings characteristic of carrier-borne aircraft. The four examples were soon put into active service before and during the 1982 Lebanon War during which they won a resounding victory over Syrian air defenses and fighter control. They were central to the Israeli victory in the air battles over the Bekaa Valley during which more than 90 Syrian fighters were downed. The Hawkeyes were also the linchpins of the operation in which the IAF destroyed the SAM array in the Bekaa, coordinating the various stages of the operation, vectoring planes into bombing runs and directing intercepts. Under the constant defense of F-15 Eagles, there were always two Hawkeyes on station off the Lebanese coast, controlling the various assets in the air and detecting any Syrian aircraft upon their takeoff, eliminating any chance of surprise.
The Israeli Air Force (IAF) operated four E-2s for its homeland AEW protection through 1994. The IAF was the first user of the E-2 to install air-to-air refueling equipment.
Three of the four Israeli-owned Hawkeyes were sold to Mexico in 2002 after they had been upgraded with new systems; the remaining example was sent to be displayed in the Israeli Air Force Museum. In 2010, Singapore began retiring its E-2Cs as well. Both Israel and Singapore now employ the IAI "Eitam", a Gulfstream G550-based platform utilizing Elta's EL/W-2085 sensor package (a newer derivative of the airborne Phalcon system) for their national AEW programmes.
Republic of China (Taiwan).
Taiwan acquired four E-2T aircraft from the US on 22 November 1995. On 15 April 2006 Taiwan commissioned two new E-2K Hawkeyes at an official ceremony at the Republic of China Air Force (ROCAF) base in Pingtung Airport in southern Taiwan.
The four E-2Ts were approved for upgrade to Hawkeye 2000 configuration in a 2008 arms deal. The four E-2T aircraft were upgraded to what became known as E-2K standard in two batches, the first batch of two aircraft were sent to the United States in June 2010, arriving home in late 2011; on their return the second batch of two aircraft were sent for upgrade returning, to Taiwan, in March 2013.
Egypt.
Egypt purchased five E-2C Hawkeyes, that entered service in 1987 and were upgraded to Hawkeye 2000 standard. One additional upgraded E-2C was purchased. The first upgraded aircraft was delivered in March 2003 and deliveries were concluded in late 2008. Egypt requested two additional excess E-2C aircraft in October 2007, that were not sold. They all operate in 601 AEW Brigade, Cairo-West.
Egypt used the E-2C Hawkeye in Libya bombing operation in 2015 against ISIL.
Offers.
In August 2009, the U.S. Navy and Northrop Grumman briefed the Indian Navy on the E-2D Advanced Hawkeye on its potential use to satisfy its current shore-based and future carrier-based Airborne Early Warning and Control (AEW&C) requirements. The Indian Navy has reportedly expressed interest in acquiring up to six Hawkeyes.
References.
</dl>

</doc>
<doc id="56079" url="http://en.wikipedia.org/wiki?curid=56079" title="Krull dimension">
Krull dimension

In commutative algebra, the Krull dimension of a commutative ring "R", named after Wolfgang Krull, is the supremum of the lengths of all chains of prime ideals. The Krull dimension need not be finite even for a Noetherian ring. More generally the Krull dimension can be defined for modules over possibly non-commutative rings as the deviation of the poset of submodules.
The Krull dimension has been introduced to provide an algebraic definition of the dimension of an algebraic variety: the dimension of the affine variety defined by an ideal "I" in a polynomial ring "R" is the Krull dimension of "R"/"I".
A field "k" has Krull dimension 0; more generally, "k"["x"1, ..., "x""n"] has Krull dimension "n". A principal ideal domain that is not a field has Krull dimension 1. A local ring has Krull dimension 0 if and only if every element of its maximal ideal is nilpotent.
Explanation.
We say that a chain of prime ideals of the form
formula_1 
has length n. That is, the length is the number of strict inclusions, not the number of primes; these differ by 1. We define the Krull dimension of formula_2 to be the supremum of the lengths of all chains of prime ideals in formula_2. 
Given a prime formula_4 in "R", we define the height of formula_4, written formula_6, to be the supremum of the lengths of all chains of prime ideals contained in formula_4, meaning that formula_8. In other words, the height of formula_4 is the Krull dimension of the localization of "R" at formula_4. A prime ideal has height zero if and only if it is a minimal prime ideal. The Krull dimension of a ring is the supremum of the heights of all maximal ideals, or those of all prime ideals.
In a Noetherian ring, every prime ideal has finite height. Nonetheless,
Nagata gave an example of a Noetherian ring of infinite Krull dimension. A ring is called catenary if any inclusion formula_11 of prime ideals can be extended to a maximal chain of prime ideals between formula_4 and formula_13, and any two maximal chains between formula_4
and formula_13 have the same length. A ring is called universally catenary if any finitely generated algebra over it is catenary. Nagata gave an example of a Noetherian ring which is not catenary.
In a Noetherian ring, Krull's height theorem says that the height of an ideal generated by "n" elements is no greater than "n".
More generally, the height of an ideal I is the infimum of the heights of all prime ideals containing I. In the language of algebraic geometry, this is the codimension of the subvariety of Spec(formula_2) corresponding to I.
Krull dimension and schemes.
It follows readily from the definition of the spectrum of a ring Spec("R"), the space of prime ideals of "R" equipped with the Zariski topology, that the Krull dimension of "R" is equal to the dimension of its spectrum as a topological space, meaning the supremum of the lengths of all chains of irreducible closed subsets. This follows immediately from the Galois connection between ideals of "R" and closed subsets of Spec("R") and the observation that, by the definition of Spec("R"), each prime ideal formula_4 of "R" corresponds to a generic point of the closed subset associated to formula_4 by the Galois connection.
Krull dimension of a module.
If "R" is a commutative ring, and "M" is an "R"-module, we define the Krull dimension of "M" to be the Krull dimension of the quotient of "R" making "M" a faithful module. That is, we define it by the formula:
where Ann"R"("M"), the annihilator, is the kernel of the natural map R → End"R"(M) of "R" into the ring of "R"-linear endomorphisms of "M". 
In the language of schemes, finitely generated modules are interpreted as coherent sheaves, or generalized finite rank vector bundles.
Krull dimension for non-commutative rings.
The Krull dimension of a module over a possibly non-commutative ring is defined as the deviation of the poset of submodules ordered by inclusion. For commutative Noetherian rings, this is the same as the definition using chains of prime ideals. The two definitions can be different for commutative rings which are not Noetherian.

</doc>
<doc id="56083" url="http://en.wikipedia.org/wiki?curid=56083" title="Public school">
Public school

Public school may refer to:

</doc>
<doc id="56085" url="http://en.wikipedia.org/wiki?curid=56085" title="Japanese school uniform">
Japanese school uniform

The Japanese school uniform is modeled on European-style naval uniforms and was first used in Japan in the late 19th century. Today, school uniforms are common in many of the Japanese public and private school systems. The Japanese word for this type of uniform is seifuku (制服).
History.
An official from Tombow Co., a manufacturer of the sailor fuku (sailor outfits), said that the Japanese took the idea from scaled down sailor suits worn by children of royal European families. The official said "In Japan, they were probably seen as adorable Western-style children’s outfits, rather than navy gear." Sailor suits were adopted in Japan for girls because the uniforms were easy to sew. The sides of the uniform had similarity to existing styles of Japanese dressmaking, and the collar had straight lines. Many home economics classes in Japan up until the 1950s gave sewing sailor outfits as assignments. Girls sewed sailor outfits for younger children in their communities.
In the 1980s "sukeban" gangs began modifying uniforms by making skirts longer and shortening the tops, and so schools began switching to blazer style uniforms to try to combat the effect. As of 2012, 50% of Japanese junior high schools and 20% of senior high schools use sailor suit uniforms.
The "Asahi Shimbun" stated in 2012 that "The sailor suit is changing from adorable and cute, a look that 'appeals to the boys,' to a uniform that 'girls like to wear for themselves.'" As of that year, contemporary sailor suits have front closures with zippers or snaps and more constructed bodices. The "Asahi Shimbun" stated that "[t]he form is snug to enhance the figure—the small collar helps the head look smaller, for better balance".
Usage.
The Japanese junior- and senior-high-school uniform traditionally consists of a military style uniform for boys and a sailor outfit for girls. These uniforms are based on Meiji era formal military dress, themselves modeled on European-style naval uniforms. The sailor outfits replace the undivided hakama (andon bakama 行灯袴) designed by Utako Shimoda between 1920 and 1930. While this style of uniform is still in use, many schools have moved to more Western-pattern parochial school uniform styles. These uniforms consist of a white shirt, tie, blazer with school crest, and tailored trousers (often not of the same color as the blazer) for boys and a white blouse, tie, blazer with school crest, and tartan culottes or skirt for girls.
Regardless of what type of uniform any particular school assigns its students, all schools have a summer version of the uniform (usually consisting of just a white dress shirt and the uniform slacks for boys and a reduced-weight traditional uniform or blouse and tartan skirt with tie for girls) and a sports-activity uniform (a polyester track suit for year-round use and a T-shirt and short pants for summer activities). Depending on the discipline level of any particular school, students may often wear different seasonal and activity uniforms within the same classroom during the day. Individual students may attempt to subvert the system of uniforms by wearing their uniforms incorrectly or by adding prohibited elements such as large loose socks or badges. Girls may shorten their skirts, permanently or by wrapping up the top to decrease length; boys may wear trousers about the hips, omit ties, or keep their shirts unbuttoned.
Since some schools do not have sex-segregated changing- or locker-rooms, students may change for sporting activities in their classrooms. As a result, such students may wear their sports uniforms under their classroom uniforms. Certain schools also regulate student hairstyles, footwear, and book bags; but these particular rules are usually adhered to only on special occasions, such as trimester opening and closing ceremonies and school photo days.
It is normal for uniforms to be worn outside of school areas, however this is going out of fashion and many students wear a casual dress. While not many public elementary schools in Japan require uniforms, many private schools and public schools run by the central government still do so.
Gakuran.
The "gakuran" (学ラン) or the "tsume-eri" (詰襟) are the uniforms for many middle school and high school boys in Japan. The color is normally black, but some schools use navy and dark blue as well.
The top has a standing collar buttoning down from top-to-bottom. Buttons are usually decorated with the school emblem to show respect to the school. Pants are straight leg and a black or dark-colored belt is worn with them. Boys usually wear penny loafers or sneakers with this uniform. Some schools may require the students to wear collar-pins representing the school and/or class rank.
The second button from the top of a male's uniform is often given away to a female he is in love with, and is considered a way of confession. The second button is the one closest to the heart and is said to contain the emotions from all three years attendance at the school. This practice was apparently made popular by a scene in a novel by Taijun Takeda.
Traditionally, the gakuran is also worn along with a matching (usually black) student cap, although this custom is less common in modern times.
The gakuran is derived from French Army uniforms. The term is a combination of "gaku" (学) meaning "study" or "student", and "ran" (らん or 蘭) meaning the Netherlands or, historically in Japan, the West in general; thus, "gakuran" translates as "Western student (uniform)". Such clothing was also worn by school children in South Korea and pre-1949 China.
Sailor fuku.
The sailor fuku (セーラー服, sērā fuku, Sailor outfits) is a common style of uniform worn by female middle school students, traditionally by high school students, and occasionally, elementary school students. It was introduced as a school uniform in 1920 in Heian Jogakuin (平安女学院) and 1921 by the principal of Fukuoka Jo Gakuin University (福岡女学院), Elizabeth Lee. It was modeled after the uniform used by the British Royal Navy at the time, which Lee had experienced as an exchange student in the United Kingdom.
Much like the male uniform, the gakuran, the sailor outfits bears a similarity to various military styled naval uniforms. The uniform generally consists of a blouse attached with a sailor-style collar and a pleated skirt. There are seasonal variations for summer and winter: sleeve length and fabric are adjusted accordingly. A ribbon is tied in the front and laced through a loop attached to the blouse. Several variations on the ribbon include neckties, bolo ties, neckerchiefs, and bows. Common colors are navy blue, white, grey, light green and black.
Shoes, socks, and other accessories are sometimes included as part of the uniform. These socks are typically navy or white. The shoes are typically brown or black penny loafers. Although not part of the prescribed uniform, alternate forms of legwear (such as loose socks, knee-length stockings, or similar) are also commonly matched by more fashionable girls with their sailor outfits.
The sailor uniform today is generally associated with junior high schools. A majority of (though by no means all) high schools having changed to more western style plaid skirts or blazers. A large part of the motivation for this change was as a response to the fetishisation of the sailor outfits as well as the desire of modern high school students to differentiate themselves in a more grown up way from junior high students.
Cultural significance.
Various schools are known for their particular uniforms. Uniforms can have a nostalgic characteristic for former students, and are often associated with relatively carefree youth. Uniforms are sometimes modified by students as a means of exhibiting individualism, including lengthening or shortening the skirt, removing the ribbon, hiding patches or badges under the collar, etc. In past decades, brightly coloured variants of the sailor outfits were also adopted by Japanese yankii and Bōsōzoku biker gangs.
Because school uniforms are a popular fetish item, second-hand sailor outfits and other items of school wear are brokered through underground establishments known as burusera, although changes to Japanese law have made such practices difficult. The pop group Onyanko Club had a provocative song called "Don't Strip Off the Sailor Suit!".

</doc>
<doc id="56086" url="http://en.wikipedia.org/wiki?curid=56086" title="Territorial dispute">
Territorial dispute

A territorial dispute is a disagreement over the possession/control of land between two or more territorial entities or over the possession or control of land, usually between a new state and the occupying power.
Context and definitions.
Territorial disputes are often related to the possession of natural resources such as rivers, fertile farmland, mineral or oil resources although the disputes can also be driven by culture, religion and ethnic nationalism. Territorial disputes result often from vague and unclear language in a treaty that set up the original boundary.
Territorial disputes are a major cause of wars and terrorism as states often try to assert their sovereignty over a territory through invasion, and non-state entities try to influence the actions of politicians through terrorism. International law does not support the use of force by one state to annex the territory of another state. The says: "All Members shall refrain in their international relations from the threat or use of force against the territorial integrity or political independence of any state, or in any other manner inconsistent with the Purposes of the United Nations."
In some cases, where the boundary is not demarcated, such as the Taiwan Strait, and Kashmir, involved parties define a line of control that serves as the "de facto" international border.
Basis in international law.
Territorial disputes have significant meaning in the international society, both because it is related to the fundamental right of states, sovereignty, and also because it is important for international peace. 
International law has significant relations with territorial disputes because territorial disputes tackles the basis of international law; the state territory. International law is based on the 'persons' of international law, which requires a 'defined territory' as mentioned in the Montevideo convention of 1933. 
Article 1 of Montevideo Convention on the Rights and Duty of States
"a person of international law should possess the following qualifications: (a) a permanent population; (b) a defined territory; (c) government; and (d) capacity to enter into relations with other States"
Also, as mentioned in B.T.Sumner's article, "In international law and relations, ownership of territory is significant because sovereignty over land defines what constitutes a state."
Therefore, the breach of a country's borders or territorial disputes pose a threat to a state's very sovereignty and the right as a person of international law. 
In addition, territorial disputes are sometimes brought upon the International Court of Justice, as was the case in Costa Rica and Nicaragua (2005). Territorial disputes cannot be separated from international law, because its basis is on the law of state borders, and because its potential settlement also relies on the international law and court.

</doc>
<doc id="56092" url="http://en.wikipedia.org/wiki?curid=56092" title="Trieste">
Trieste

Trieste (; ]   ; Triestine Venetian: "Trièst"; Slovene, Serbo-Croatian: Trst; German: "Triest") is a city and seaport in northeastern Italy. It is situated towards the end of a narrow strip of Italian territory lying between the Adriatic Sea and Slovenia, which lies almost immediately south and east of the city. Trieste is located at the head of the Gulf of Trieste and throughout history it has been influenced by its location at the crossroads of Latin, Slavic, and Germanic cultures. In 2009, it had a population of about 205,000 and it is the capital of the autonomous region Friuli-Venezia Giulia and the Province of Trieste.
Trieste was one of the oldest parts of the Habsburg Monarchy. In the 19th century, it was the most important port of one of the Great Powers of Europe. As a prosperous seaport in the Mediterranean region, Trieste became the fourth largest city of the Austro-Hungarian Empire (after Vienna, Budapest, and Prague). In the fin-de-siecle period, it emerged as an important hub for literature and music. It underwent an economic revival during the 1930s, and Trieste was an important spot in the struggle between the Eastern and Western blocs after the Second World War. Today, the city is in one of the richest regions of Italy, and has been a great centre for shipping, through its port (Port of Trieste), shipbuilding and financial services.
In 2012, LonelyPlanet.com listed the city of Trieste as the world's most underrated travel destination.
Name.
The original pre-Roman name of the city, "Tergeste", with the "-est-" suffix typical of Illyrian, is speculated to be derived from a hypothetical Venetic word "*terg-" "market", etymologically related to Old Church Slavonic "tьrgъ" "market" (whence Slovenian and Croatian "trg" / "trg" / "tržište" and the Scandinavian borrowing "torg"). Roman authors also transliterated the name as "Tergestum". Modern names of the city include: Italian: "Trieste", Slovene: "Trst", German: "Triest", Hungarian: "Trieszt", Croatian: "Trst", Serbian: Трст, Trst, and .
Geography.
Trieste lies in the northernmost part of the high Adriatic in northeastern Italy, near the border with Slovenia. The city lies on the Gulf of Trieste.
 Built mostly on a hillside that becomes a mountain, Trieste's urban territory lies at the foot of an imposing escarpment that comes down abruptly from the Karst Plateau towards the sea. The karst landforms close to the city reach an elevation of 458 metres (1,502 ft) above sea level.
It lies on the borders of the Italian geographical region, the Balkan Peninsula, and the Mitteleuropa.
Climate.
The territory of Trieste is composed of several different climate zones depending on the distance from the sea and elevation. The average temperatures are 5.4 °C in January and 23.3 °C in July. The climatic setting of the city is humid subtropical climate ("Cfa" according to Köppen climate classification) with some Mediterranean influences. On average, humidity levels are pleasantly low (~65%), while only two months (January & February) receive slightly less than 60 mm of precipitation. Trieste along with the Istrian peninsula enjoys evenly distributed rainfall above 1000 mm in total; it is noteworthy that no true summer drought occurs. Snow occurs on average 2 days per year. Temperatures are very mild - lows below zero are somewhat rare (with just 9 days per a year) and highs above 30 °C similarly can be expected 15 days a year only. Winter maxima are lower than in typical Mediterranean zone (~ 5 - 11 °C) with quite high minima (~2 - 8 °C). Two basic weather patterns interchange - sunny, sometimes windy but often very cold days (max. +7, min. 0; frequently connected to an occurrence of northeast wind called "Bora" ) and rainy days with temperatures about 6 to. Absolute minimal temperature came with Arctic winter of 1956, -14.6 °C. Summer is very warm with maxima about 28 degrees and lows above 20 degrees. Absolute maximum from 2003 is 37.2 °C. Average year temperature, 14.1 °C, is nearly the same as that of Earth.
City districts.
Trieste is administratively divided in seven districts:
The iconic city center is Piazza Unità d'Italia, which is between the large 19th-century avenues and the old medieval city, composed of many narrow and crooked streets.
History.
Ancient era.
Originally an Illyrian settlement, it was later captured by the Carni, a tribe of the Eastern Alps, before becoming part of the Roman republic in 177 BC. It was granted the status of colony under Julius Caesar, who recorded its name as "Tergeste" in "Commentarii de Bello Gallico" (51 BC), his work which recounts events of the Gallic Wars.
In imperial times the border of Roman Italy moved from the Timavo river to Formione (today Risano (fiume)). Roman Tergeste flourished due to its position on the road from Aquileia, the main Roman city in the area, to Istria, and as a port, some ruins of which are still visible. Emperor Augustus built a line of walls around the city in 33–32 BC, while Trajan built a theatre in the 2nd century.
In the Early Christian era Trieste continued to flourish. It was retained by Constantinople after the ascension of Odoacer in Italy (476) and became a Byzantine military outpost. In 567 the city was destroyed by the Lombards in the course of their invasion of northern Italy. In 788 it became part of the Frankish kingdom, under the authority of their count-bishop. From 1081 the city came loosely under the Patriarchate of Aquileia, developing into a free commune by the end of the 12th century.
Habsburg Empire.
After two centuries of war against the nearby major power, the Republic of Venice (which briefly occupied it in 1283–87, and again in 1368–72), the leading citizens of Trieste petitioned Leopold III of Habsburg, Duke of Austria, to make Trieste part of his domains. The agreement of voluntary submission ("dedizione") was signed at the castle of Graz on 30 September 1382. The citizens, however, maintained a certain degree of autonomy up until the 17th century. Following an unsuccessful Habsburg invasion of Venice in the prelude to the 1508–16 War of the League of Cambrai, the Venetians occupied Trieste again in 1508, and were allowed to keep the city under the terms of the peace treaty. However, the Habsburg Empire recovered Trieste a little over one year later, when the conflict resumed. By the 18th century Trieste became an important port and trade hub. In 1719, it was made a free port within the Habsburg Empire by Emperor Charles VI, and remained a free port until 1 July 1891. The reign of his successor, Maria Theresa of Austria, marked the beginning of a very prosperous era for the city.
In the following decades, Trieste was briefly occupied by troops of the French Empire during the Napoleonic Wars on several occasions, in 1797, 1805 and 1809. From 1809 to 1813, Trieste was annexed into Illyrian Provinces, interrupting its status of free port and losing its autonomy. The municipal autonomy was not restored after the return of the city to the Austrian Empire in 1813. Following the Napoleonic Wars, Trieste continued to prosper as the Free Imperial City of Trieste (German: "Reichsunmittelbare Stadt Triest"), a status that granted economic freedom, but limited its political self-government. The city's role as Austria's main trading port and shipbuilding centre was later emphasized with the foundation of the merchant shipping line Austrian Lloyd in 1836, whose headquarters stood at the corner of the Piazza Grande and Sanità (today's Piazza Unità d'Italia). By 1913 Austrian Lloyd had a fleet of 62 ships comprising a total of 236,000 tons. With the introduction of the constitutionalism in the Austrian Empire in 1860, the municipal autonomy of the city was restored, with Trieste becoming capital of the Austrian Littoral crown land (German: "Österreichisches Küstenland").
The particular Friulian dialect, called "Tergestino", spoken until the beginning of the 19th century, was gradually overcome by the Triestine dialect of Venetian (a language deriving directly from Vulgar Latin) and other languages, including standard Italian, Slovene, and German. While Triestine and Italian were spoken by the largest part of the population, German was the language of the Austrian bureaucracy and Slovene was predominantly spoken in the surrounding villages. From the last decades of the 19th century, the number of speakers of Slovene grew steadily, reaching 25% of the overall population of Trieste municipality in 1911 (30% of the Austro-Hungarian citizens in Trieste).
According to the 1911 census, the proportion of Slovene speakers amounted to 12.6% in the city centre (15.9% counting only Austrian citizens), 47.6% in the suburbs (53% counting only Austrian citizens), and 90.5% in the surroundings. They were the largest ethnic group in 9 of the 19 urban neighborhoods of Trieste, and represented a majority in 7 of them. The Italian speakers, on the other hand, made up 60.1% of the population in the city center, 38.1% in the suburbs, and 6.0% in the surroundings. They were the largest linguistic group in 10 of the 19 urban neighborhoods, and represented the majority in 7 of them (including all 6 in the city center). Of the 11 villages included within the city limits, the Slovene speakers had an overwhelming majority in 10, and the German speakers in one (Miramare).
German speakers amounted to 5% of the city's population, with the highest proportions in the city center. A small proportion of Trieste's population spoke Croatian (about 1.3% in 1911), and the city also had several other smaller ethnic communities, including Czechs, Istro-Romanians, Serbs, and Greeks, who mostly assimilated either into the Italian or the Slovene-speaking communities.
In the later part of the 19th century, Pope Leo XIII considered moving his residence to Trieste or Salzburg because of what he considered a hostile anti-Catholic climate in Italy following the 1870 Capture of Rome by the newly established Kingdom of Italy. However, the Austrian monarch, Franz Josef I, rejected the idea. The modern Austro-Hungarian Navy used Trieste as a base and for shipbuilding. The construction of the first major trunk railway in the Empire, the Vienna-Trieste Austrian Southern Railway, was completed in 1857, a valuable asset for trade and the supply of coal.
In 1882 an Irredentist activist, Guglielmo Oberdan, attempted to assassinate Emperor Franz Joseph, who was visiting Trieste. Oberdan was caught, convicted, and executed. He was regarded as a martyr by radical Irredentists, but as a cowardly villain by the supporters of the Austro-Hungarian monarchy. Franz Joseph, who reigned another thirty-five years, never visited Trieste again.
At the beginning of the 20th century, Trieste was a bustling cosmopolitan city frequented by artists and philosophers such as James Joyce, Italo Svevo, Sigmund Freud, Dragotin Kette, Ivan Cankar, Scipio Slataper, and Umberto Saba. The city was the major port on the Austrian Riviera, and perhaps the only real enclave of Mitteleuropa (i.e. Central Europe) south of the Alps. Viennese architecture and coffeehouses dominate the streets of Trieste to this day.
World War I, annexation to Italy and the Fascist era.
Italy, in return for entering World War I on the side pf the Allied Powers, had been promised substantial territorial gains, which included the former Austrian Littoral and western Inner Carniola. Italy therefore annexed the city of Trieste at the war's end, in accordance with the provisions of the 1915 Treaty of London and the Italian-Yugoslav 1920 Treaty of Rapallo. While only a few hundred Italians remained in the newly established South Slavic state, a population of half a million Slavs, of which the annexed Slovenes were cut off from the remaining three-quarters of total Slovene population at the time, were subjected to forced Italianization. Trieste had a large Italian majority, but it had more ethnic Slovene inhabitants than even Slovenia's capital of Ljubljana at the end of 19th century.
The Italian lower middle class — who felt most threatened by the city's Slovene middle class — sought to make Trieste a "città italianissima", committing a series of attacks led by Black Shirts against Slovene-owned shops, libraries, and lawyers' offices, and even the Trieste National Hall, the building central to the Slovene community. By the mid-1930s several thousand Slovenes, especially members of the middle class and the intelligentsia from Trieste, emigrated to the Kingdom of Yugoslavia or to South America. Despite the exodus of the Slovene and German speakers, the city's population increased because of the migration of Italians from other parts of Italy. Several thousand ethnic Italians from Dalmatia also moved to Trieste from the newly created Yugoslavia.
In the late 1920s, resistance began with the Slovene militant anti-fascist organization TIGR, which carried out several bomb attacks in the city centre. In 1930 and 1941, two trials of Slovene activists were held in Trieste by the Fascist "Special Tribunal for the Security of the State". Among the notable Slovene émigrés from Trieste were the author Vladimir Bartol, the legal theorist Boris Furlan and the Argentine architect Viktor Sulčič. The political leadership of the around 70,000 émigrés from the Julian March in Yugoslavia was mostly composed by Trieste Slovenes: Lavo Čermelj, Josip Vilfan and Ivan Marija Čok. During the 1920s and 1930s, several monumental building were built in the Fascist architectural style, including the impressive University of Trieste and the almost 70 m tall Victory Lighthouse ("Faro della Vittoria"), which became one of the city's landmarks. The economy experienced improvement in the late 1930s, and several large infrastructure projects were carried out.
The Fascist government encouraged some of the artistic and intellectual subcultures that emerged in the 1920s and the city became home to an important avant-garde movement in visual arts, centered around the futurist Tullio Crali and the constructivist Avgust Černigoj. In the same period, Trieste consolidated its role as one of the centres of modern Italian literature, with authors such as Umberto Saba, Biagio Marin, Giani Stuparich, and Salvatore Satta. Intellectuals were frequently associated with the historic Caffè San Marco, a cafè in the city still open today. Some non-Italian intellectuals remained in the city, such as the Austrian author Julius Kugy, the Slovene writer and poet Stanko Vuk, the lawyer and human rights activist Josip Ferfolja and the anti-fascist clergyman Jakob Ukmar.
The promulgation of the anti-Jewish racial laws in 1938 was a severe blow to the city's Jewish community, at the time the third largest in Italy. The fascist anti-semitic campaign resulted in a series of attacks on Jewish property and individuals, culminating in July 1942 when the Synagogue of Trieste was raided and devastated by the Fascist Squads and the mob.
World War II and its aftermath.
With the annexation of Province of Ljubljana by Italy and the subsequent deportation of 25,000 Slovenes, which equaled 7.5% of the total population of the Province, the operation, one of the most drastic in Europe, filled up Rab concentration camp, Gonars concentration camp, Monigo (Treviso), Renicci d'Anghiari, Chiesanuova, and other Italian concentration camps where altogether 9,000 Slovenes died, World War II came close to Trieste. Following trisection of Slovenia, starting from the winter of 1941, the first Slovene Partisans appeared in Trieste province although the resistance movement did not become active in the city itself until late 1943.
After the Italian armistice in September 1943, the city was occupied by Wehrmacht troops. Trieste became nominally part of the newly constituted Italian Social Republic, but it was de facto ruled by Germany, who created the Operation Zone of the Adriatic Littoral out of former Italian north-eastern regions, with Trieste as the administrative center. The new administrative entity was headed by Friedrich Rainer. Under German occupation, the only concentration camp with a crematorium on Italian soil was built in a suburb of Trieste, at the Risiera di San Sabba on 4 April 1944. About 3,000 Jews, South Slavs and Italian anti-Fascists were died the "Risiera", while thousands were imprisoned before being transferred to other concentration camps.
The city saw intense Italian and Yugoslav partisan activity and suffered from Allied bombings. The city's Jewish community was deported to extermination camps, where most of them died.
On 30 April 1945, the Italian anti-Fascist National Liberation Committee ("Comitato di Liberazione Nazionale", or CLN) of Marzari and Savio Fonda, made up of approximately 3,500 volunteers, incited a riot against the Nazi occupiers. On May 1, Allied members of the Yugoslav Partisans' 8th Dalmatian Corps took over most of the city, except for the courts and the castle of San Giusto, where the German garrisons refused to surrender to anyone except than New Zealanders.)(The Yugoslavs had a reputation for shooting German and Italian prisoners.) The 2nd New Zealand Division continued to advance towards Trieste along Route 14 around the northern coast of the Adriatic sea and arrived in the city the following day (see official histories "The Italian Campaign" and "Through the Venetian Line"). The German forces surrendered on the evening of May 2, but were then turned over to the Yugoslav forces.
The Yugoslavs held full control of the city until June 12, a period known in the Italian historiography as the "forty days of Trieste".
During this period, hundreds of local Italians and anti-Communist Slovenes were arrested by the Yugoslav authorities, and many of them were never seen again.
These included not only former Fascist and German collaborators, but also Italian nationalists and any other real or potential opponents of Yugoslav Communism. Some were interned in Yugoslav concentration camps (in particular at Borovnica, Slovenia), while others were simply murdered and thrown into potholes ("foibe") on the Karst plateau.
After an agreement between the Yugoslav leader Josip Broz Tito and the British Field Marshal Harold Alexander, the Yugoslav forces withdrew from Trieste, which came under a joint British-U.S. military administration. The Julian March was divided between Anglo-American and Yugoslav military administration until September 1947 when the Paris Peace Treaty established the Free Territory of Trieste.
Zone A of the Free Territory of Trieste (1947–54).
In 1947, Trieste was declared an independent city state under the protection of the United Nations as the Free Territory of Trieste. The territory was divided into two zones, A and B, along the Morgan Line established in 1945.
From 1947 to 1954, the A Zone was governed by the Allied Military Government, composed of the American "Trieste United States Troops" (TRUST), commanded by Major General Bryant E. Moore, the commanding general of the American 88th Infantry Division, and the "British Element Trieste Forces" (BETFOR), commanded by Sir Terence Airey, who were the joint forces commander and also the military governors. Zone A covered almost the same area of the current Italian Province of Trieste, except for four small villages south of Muggia, which were given to Yugoslavia after the dissolution (see London Memorandum of 1954) of the Free Territory in 1954. Zone B, which was under the administration of Miloš Stamatović, then collonel of the Yugoslav People's Army, was composed of the north-westernmost portion of the Istrian peninsula, between the river Mirna and the Debeli Rtič cape.
In 1954, in accordance with the Memorandum of London the vast majority of Zone A, including the city of Trieste, was given as civil administration to Italy. Zone B was given as a civil administration to Yugoslavia along with four villages from Zone A (Plavje, Spodnje Škofije, Hrvatini, and Jelarji), and was divided among the Socialist Republic of Slovenia and Croatia. The final border line with Yugoslavia, and the status of the ethnic minorities in the areas, was settled bilaterally in 1975 with the Treaty of Osimo. This line now constitutes the border between Italy and Slovenia.
Economy.
During the Austro-Hungarian era, Trieste became a leading European city in economy, trade and commerce, and was the fourth-largest and most important centre in the empire, after Vienna, Budapest and Prague. The economy of Trieste, however, fell into a decline after the city's annexation to Italy after World War I. But Fascist Italy promoted a huge development of Trieste in the 1930s, with new manufacturing activities related even to naval and armament industries (like the famous "Cantieri Aeronautici Navali Triestini (CANT)"). Allied bombings during World War II destroyed the industrial section of the city (mainly the shipyards). As a consequence, Trieste was a mainly peripheral city during the Cold War. However, since the 1970s, Trieste has experienced a certain economic revival.
Today, Trieste is a lively and cosmopolitan city, with about 8% of its population hailing from a cultural community, and is a major centre in the EU for trade, politics, culture, shipbuilding, education, transport and commerce. The city is part of the "Corridor 5" project to establish closer transport connections between Western and Eastern Europe, via countries such as Slovenia, Croatia, Hungary, Ukraine and Bosnia.
The Port of Trieste is a trade hub with a significant commercial shipping business, busy container and oil terminals, and steel works.
The oil terminal feeds the Transalpine Pipeline which covers 40% of Germany's energy requirements (100% of the states of Bavaria and Baden-Württemberg), 90% of Austria and more than 30% of the Czech Republic's. The sea highway connecting the ports of Trieste and Istanbul is one of the busiest RO/RO [roll on roll-off] routes in the Mediterranean.The port is also Italy's and the Mediterranean's (and one of Europe's) greatest coffee ports, supplying more than 40% of Italy's coffee. The thriving coffee industry in Trieste began under Austria-Hungary, with the Austro-Hungarian government even awarding tax-free status to the city in order to encourage more commerce. Some remnants of Austria-Hungary's coffee-driven economic ambition remain, such as the Hausbrandt Trieste coffee company. As a result, present-day Trieste boasts many cafes, and is still known to this day as "the coffee capital of Italy". Companies active in the coffee sector have given birth to the Trieste Coffee Cluster as their main umbrella organization, but also as an economic actor in its own right.
Two Fortune Global 500 companies have their global or national headquarters in the city, respectively: Assicurazioni Generali and Allianz. Other megacompanies based in Trieste are Fincantieri, one of the world's leading shipbuilding companies and the Italian operations of Wärtsilä. Prominent companies from Trieste include: AcegasApsAmga (Hera Group), Autamarocchi SpA, Banca Generali SpA, Genertel, HERA Trading, Illy, Italia Marittima, Nuovo Arsenale Cartubi Srl, Jindal Steel and Power Italia SpA; Pacorini SpA, TBS Group, Telit, and polling and marketing company SWG.
With two main banking institutions, the Zadružna Kraška Banka (ZKB), and a branch of the Nova Ljubljanska Banka the local Slovene community contributes vigorously to the economy.
Demographics.
s of 2009[ [update]], there were 205,507 people residing in Trieste, located in the province of Trieste, Friuli-Venezia Giulia, of whom 46.7% were male and 53.3% were female. Trieste had lost roughly ⅓ of its population since the 1970s, due to the crisis of the historical industrial sectors of steel and shipbuilding, a dramatic drop in fertility rates and fast population aging. Minors (children aged 18 and younger) totalled 13.78% of the population compared to pensioners who number 27.9%. This compares with the Italian average of 18.06% (minors) and 19.94% (pensioners). The average age of Trieste residents is 46 compared to the Italian average of 42. In the five years between 2002 and 2007, the population of Trieste declined by 3.5%, while Italy as a whole grew by 3.85%. However, in the last two years the city has shown signs of stabilizing thanks to growing immigration fluxes. The crude birth rate in Trieste is only 7.63 per 1,000, one of the lowest in eastern Italy, while the Italian average is 9.45 births. 
Since the annexation to Italy after World War I, there has been a steady decline in the Trieste's demographic weight compared to other cities. In 1911, Trieste was the 4th largest city in the Austro-Hungarian Empire (3rd largest in the Austrian part of the Monarchy). In 1921, Trieste was the 8th largest city in the country, in 1961 the 12th largest, in 1981 the 14th largest, while in 2011 it dropped to the 15th place.
The dominant local dialect of Trieste is "Triestine" ("Triestin", pronounced ]), influenced by a form of Venetian. This dialect and the official Italian language are spoken in the city, while Slovene is spoken in some of the immediate suburbs. There are also small numbers of Serbian, Croatian, German, and Hungarian speakers. 
An estimated 19% of the province's population (49,000 out of 260,000 from the last census) belong to the autochthonous Slovene language community. In total, the city's ethnic Slavic minority makes up about 30 percent of the population.
At the end of 2012, ISTAT estimated that there were 16,279 foreign-born residents in Trieste, representing 7.7% of the total city population. The largest autochthonous minority are Slovenes, but there is also a large immigrant group from Balkan nations (particularly nearby Serbia, Albania and Romania): 4.95%, Asia: 0.52%, and sub-saharan Africa: 0.2%. Serbian community consists of both autochthonous and immigrant groups. Trieste is predominantly Roman Catholic, but also has large numbers of Orthodox Christians, mainly Serbs, due to the city's large migrant population from Eastern Europe and its Balkan influence. 
Main sights.
Castles.
"Castello Miramare" (Miramare Castle).
The "Castello Miramare", or Miramare Castle, on the waterfront 8 km from Trieste, was built between 1856 and 1860 from a project by Carl Junker working under Archduke Maximilian. The Castle gardens provide a setting of beauty with a variety of trees, chosen by and planted on the orders of Maximilian, that today make a remarkable collection. Features of particular attraction in the gardens include two ponds, one noted for its swans and the other for lotus flowers, the Castle annexe ("Castelletto"), a bronze statue of Maximilian, and a small chapel where is kept a cross made from the remains of the "Novara", the flagship on which Maximilian, brother of Emperor Franz Josef, set sail to become Emperor of Mexico. Much later, the castle was also the home of Prince Amedeo, Duke of Aosta, the last commander of Italian forces in East Africa during the Second World War. During the period of the application of the Instrument for the Provisional Regime of the Free Territory of Trieste, as establish in the Treaty of Peace with Italy (Paris 10/02/1947), the castle served as headquarters for the United States Army's TRUST force.
"Castel San Giusto" (Castle of San Giusto).
The "Castel San Giusto", or Castle of San Giusto, was designed on the remains of previous castles on the site, and took almost two centuries to build. The stages of the development of the Castle's defensive structures are marked by the central part built under Frederick III (1470-1), the round Venetian bastion (1508-9), the Hoyos-Lalio bastion and the Pomis, or "Bastione fiorito" dated 1630. 
Archaeological remains.
The ruins of the temple dedicated to Zeus are next to the Forum, those of Athena's temple are under the basilica, visitors can see its basement.
Roman theatre.
The Roman theatre lies at the foot of the San Giusto hill, facing the sea. The construction partially exploits the gentle slope of the hill, and much of the theatre is made of stone. The topmost portion of the steps and the stage were supposedly made of wood.
The statues that adorned the theatre, brought to light in the 1930s, are now preserved at the Town Museum. Three inscriptions from the Trajanic period mention a certain Q. Petronius Modestus, someone closely connected to the development of the theatre, which was erected during the second half of the 1st century.
Caves.
In the entire Province of Trieste, there are 10 speleological groups out of 24 in the whole "Friuli-Venezia Giulia" region. The Trieste plateau (Altopiano Triestino), called Kras or the "Carso" and covering an area of about 200 km2 within Italy has approximately 1,500 caves of various sizes (like that of Basovizza, now a monument to the Foibe massacres).
Among the most famous are the Grotta Gigante, the largest tourist cave in the world, with a single cavity large enough to contain St Peter's in Rome, and the "Cave of Trebiciano", 350 m deep, at the bottom of which flows the "Timavo River". This river dives underground at Škocjan Caves in Slovenia (they are on UNESCO list and only a few kilometres from Trieste) and flows about 30 km before emerging about 1 km from the sea in a series of springs near Duino, reputed by the Romans to be an entrance to Hades ("the world of the dead").
Culture.
Trieste has a lively cultural scene with various theatres. Among these are the Opera Teatro Lirico Giuseppe Verdi, Politeama Rossetti, the Teatro La Contrada, the Slovene theatre in Trieste ("Slovensko stalno gledališče", since 1902), Teatro Miela, and a several smaller ones.
There are also numerous museums. Among these are:
Two important national monuments:
The "Slovenska gospodarsko-kulturna zveza" - "Unione Economica-Culturale Slovena" is the umbrella organization bringing together cultural and economic associations belonging to the Slovene minority.
Education.
The University of Trieste is a medium-size state supported institution that consists of 12 faculties, boasts a wide and almost complete range of university courses and currently has about 23,000 students enrolled and 1,000 professors. It was founded in 1924.
Trieste also hosts the Scuola Internazionale Superiore di Studi Avanzati (SISSA), a leading graduate and postgraduate teaching and research institution in the study of mathematics, theoretical physics, and neuroscience, and the MIB School of Management Trieste, a private, ASFOR accredited business school.
There are three international schools offering primary and secondary education programs in English in the greater metropolitan area: the International School of Trieste, the European School of Trieste, and the United World College of the Adriatic. Liceo scientifico statale "France Prešeren" offers public secondary education in the Slovene language.
The city hosts numerous national and international scientific research institutions, among which: AREA Science Park, which comprises ELETTRA, a synchrotron particle accelerator with free-electron laser capabilities for research and industrial applications; the International Centre for Theoretical Physics, which operates under a tripartite agreement among the Italian Government, UNESCO, and International Atomic Energy Agency (IAEA); the Trieste Astronomical Observatory; the Istituto Nazionale di Oceanografia e Geofisica Sperimentale (OGS), which carries out research on oceans and geophysics; the International Centre for Genetic Engineering and Biotechnology, a United Nations centre of excellence for research and training in genetic engineering and biotechnology for the benefit of developing countries; ICS-UNIDO, a UNIDO research centre in the areas of renewable energies, biofuels, medicinal plants, food safety and sustainable development; the Carso Center for Advanced Research in Space Optics; and the secretariats of the Third World Academy of Sciences (TWAS) and of the InterAcademy Panel: The Global Network of Science Academies (IAP).
Sports.
The local "calcio" (football) club in Trieste is "Triestina", one of the oldest clubs in Italy. Notably, Triestina was runner-up in the 1947/1948 season of the Italian first division (Serie A), losing the championship to Torino.
Trieste is notable for having had two football clubs participating in the championships of two different nations at the same time during the period of the Free Territory of Trieste, due to the schism within the city and region created by the post-war demarcation. Triestina played in the Italian first division (Serie A). Although it faced relegation after the first season after the Second World War, the FIGC changed the rules to keep it in, as it was seen as important to keep a club of the city in the Italian league, while Yugoslavia had its eye on the city. In the championship of next season the club played its best season with a 3rd-place finish. Meanwhile, Yugoslavia bought A.S.D. Ponziana, a small team in Trieste, which under a new name, "Amatori Ponziana Trst", played in the Yugoslavian league for 3 years. Triestina went bankrupt in the 1990s, but after being re-founded regained a position in the Italian second division (Serie B) in 2002. Ponziana was renamed as "Circolo Sportivo Ponziana 1912" and currently plays in Friuli-Venezia Giulia Group of Promozione, which is 7th level of the Italian league.
Trieste also boasts a famous basketball team, Pallacanestro Trieste, which reached its zenith in the 1990s when, with large financial backing from sponsors Stefanel, it was able to sign players such as Dejan Bodiroga, Fernando Gentile and Gregor Fučka, all stars of European basketball.
Many sailing clubs have roots in the city which contribute to Trieste's strong tradition in that sport. The Barcolana regatta, which had its first edition in 1969, is the world's largest sailing race by number of participants.
Local sporting facilities include the Stadio Nereo Rocco, a UEFA-certified stadium with seating capacity of 32,500; the Palatrieste, an indoor sporting arena sitting 8,000 people, and Piscina Bruno Bianchi, a large olympic size swimming pool.
Film.
Trieste has been portrayed on screen a number of times, with films often shot on location in the area. In 1942 the early neorealist "Alfa Tau!" was filmed partly in the city.
Cinematic interest in Trieste peaked during the height of the "Free Territory" era between 1947 and 1954 with international films such as "Sleeping Car to Trieste" and "Diplomatic Courier" portraying it as a hotbed of espionage. These conveyed an impression of the city as a cosmopolitan place of conflict between Great Powers, a portrayal which resembled that of "Casablanca" (1943). Italian filmmakers, by contrast, portrayed Trieste as unquestionably Italian in a series of patriotic films including "Trieste mia!" and "Ombre su Trieste".
Transport.
Maritime transport.
Trieste's maritime location and its former long term status as part of the Austrian and Austro-Hungarian empires made the Port of Trieste the major commercial port for much of the landlocked areas of central Europe. In the 19th century, a new port district known as the "Porto Nuovo" was built northeast to the city centre.
There is significant commercial shipping to the container terminal, steel works and oil terminal, all located to the south of the city centre. After many years of stagnation, a change in the leadership placed the port on a steady growth path, recording a 40% increase in shipping traffic as of 2007.
Rail transport.
Railways came early to Trieste, due to the importance of its port and the need to transport people and goods inland. The first railroad line to reach Trieste was the "Südbahn", launched by the Austrian government in 1857. This railway stretches for 1400 km to Lviv, Ukraine, via Ljubljana, Slovenia; Sopron, Hungary; Vienna, Austria; and Kraków, Poland, crossing the backbone of the Alps mountains through the Semmering Pass near Graz. It approaches Trieste through the village of Villa Opicina, a few kilometres from the big city but over 300 m higher in elevation. Due to this, the line takes a 32 km detour to the north, gradually descending before terminating at the Trieste Centrale railway station.
A second trans-Alpine railway was dedicated in 1906, with the opening of the Transalpina Railway from Vienna, Austria via Jesenice and Nova Gorica. This railway also approached Trieste via Villa Opicina, but it took a rather shorter loop southwards towards Trieste's other main railway station, the Trieste Campo Marzio railway station, south of the central station. This line no longer operates, and the Campo Marzio station is now a railway museum.
To facilitate freight traffic between the two stations and the nearby dock areas, a temporary railway line known as the "Rivabahn" was built along the waterfront in 1887. This railway survived until 1981, when it was replaced by the "Galleria di Circonvallazione", a 5.7 km railway tunnel route, to the east of the city. Freight services from the dock area include container services to northern Italy and to Budapest, Hungary, together with rolling highway services to Salzburg, Austria and Frankfurt, Germany.
Passenger rail service to Trieste mostly consists of trains to and from Venice, connecting there with high-speed trains to Rome and Milan at Mestre. There are also direct trains to Verona, Turin, Milan, Rome, Florence, Naples and Bologna. These trains reach the Trieste central station bypassing the Gulf of Trieste, connecting with the Südbahn's northern loop. s of 2012[ [update]], there are no passenger trains between Italy and Slovenia.
Trieste could in the remote future be connected to the Italian TAV railway network: a 300 km/h fast train route would possibly connect Trieste with Venice. As a matter of fact, this project will not be completed earlier than 2020.
Air transport.
Trieste is served by the Friuli Venezia Giulia Airport (IATA code: TRS), located 30 minutes away from the city, at Ronchi near Monfalcone at the head of the Gulf of Trieste. There are many national and international destinations available.
Local transport.
Local public transport is operated by Trieste Trasporti, which operates a network of around 60 bus routes and two boat services. They also operate the Opicina Tramway, a hybrid between tramway and funicular railway providing a more direct link between the city centre and Opicina.
International relations.
Trieste hosts the Secretariat of the Central European Initiative, an intergovernmental organization among Central and South-Eastern European states.
In recent years, Trieste was chosen to host a number of high level bilateral and multilateral meetings such as: the Italo-Russian Bilateral Summit in 2013 (Letta-Putin) and the Italo-German Bilateral Summit in 2008 (Berlusconi-Merkel); the G8 meetings of Foreign Affairs and Environment Ministers respectively in 2009 and 2001.
Sister cities / Twin towns.
Trieste is twinned with:
 Graz, Austria (since 1973)
 Santos, Brazil (since 1977)
 Le Havre, France

</doc>
<doc id="56093" url="http://en.wikipedia.org/wiki?curid=56093" title="Kerchief">
Kerchief

A kerchief (from the French "couvre-chef", "cover the head") also known as a bandana, is a triangular or square piece of cloth tied around the head or around the neck for protective or decorative purposes. The popularity of head kerchiefs may vary by culture or religion, as among Orthodox Christian women, Amish women, Orthodox Jewish women and Muslim women.
Handkerchief.
A "handkerchief" or "hanky" primarily refers to a napkin made of cloth, used to dab away perspiration, clear the nostrils, or, in Victorian times, as a means of flirtation. A woman could intentionally drop a dainty square of lacy or embroidered fabric to give a favored man a chance to pick it up as an excuse to speak to her while returning it. Handkerchiefs were sometimes scented to be used like a nosegay or tussy-mussy, a way of protecting those who could afford them from the obnoxious scents in the street.
Society.
Subculture.
The popularity of the bandana and kerchief was at it highest point in the 1970s, 1980s and 1990s depending on one's location. After that its popularity started waning in the west, but some eastern cultures maintained its usage for a while, such as in the Persian Gulf countries. It is largely seen as gender neutral and can be worn by both men and women. Its usage when wrapped up was partially replaced by the headband.
Bandana.
A bandana (from the Tamil: பந்தம் Bandham, "a bond") is a type of large, usually colorful, kerchief, usually worn on the head or around the neck of a person or pet and is not considered to be a hat. Bandanas are frequently printed in a paisley pattern and are most often used to hold hair back, either as a fashionable head accessory, or for practical purposes.
Urban.
Colors, and sometimes designs, can be worn as a means of communication or identification, as with the prominent California criminal gangs, the Bloods, the Crips, the Norteños, and the Sureños. 
In gang subcultures, the bandana could be worn in a pocket or, in some cases, around the leg. In the late 1960s/early 1970s, the Bloods and the Crips wore red or blue paisley bandanas as a signifier of gang affiliation.

</doc>
<doc id="56096" url="http://en.wikipedia.org/wiki?curid=56096" title="Pioneer movement">
Pioneer movement

A pioneer movement is an organization for children operated by a communist party. Typically children enter into the organization in elementary school and continue until adolescence. The adolescents then typically join the Young Communist League. Prior to the 1990s there was a wide cooperation between pioneer and similar movements of about 30 countries, coordinated by the international organization, "International Committee of Children's and Adolescents' Movements" (French: "Comité international des mouvements d'enfants et d'adolescents", CIMEA), founded in 1958, with headquarters in Budapest.
Overview.
Pioneer movements exist in countries where the Communist Party is in power as well as in some countries where the Communist Party is in opposition, if the party is large enough to support a children's organization. In countries ruled by Communist Parties, membership of the pioneer movement is officially optional. However, membership provides many benefits, so the vast majority of children typically join the movement (although at different ages). During the existence of the Soviet Union, thousands of Young Pioneer camps and Young Pioneer Palaces were built exclusively for Young Pioneers, which were free of charge, sponsored by the government and trade unions. There were many newspapers and magazines published for Young Pioneers in millions of copies.
The Pioneer movement was modeled on the Scout movement, but there are some distinct differences. Most notably, the Scout movement is independent of government control and political parties. Some features, however, are reminiscent of the Scout movement. The two movements share some principles like preparedness and promotion of sports and outdoor skills. The pioneer movement also includes teaching of communist principles. Opponents of Communist states claim that this is a form of indoctrination.
A member of the movement is known as a pioneer, and a kerchief or necktie — typically red, but sometimes light blue — is the traditional item of clothing worn by a pioneer. The pioneer organization is often named after a famous party member that is considered a suitable role model for young communists. In the Soviet Union it was Vladimir Lenin; in East Germany, it was Ernst Thälmann. The Thälmann pioneers were taught the slogan "Ernst Thälmann is my role model. We wear our red scarf with pride." Albania, which severed diplomatic relations with the USSR in 1961, also had a variant of Pioneer organization, called Pioneers of Enver, named after the communist ruler of Albania, Enver Hoxha.
Countries with Pioneer movements.
The Pioneer Movement now exists in these countries:
Older children could continue in other communist organizations, but that would typically be done only by a limited number of people.
The communist parties in Russia and other countries continue to run a pioneer organization, but membership tends to be quite limited.

</doc>
<doc id="56097" url="http://en.wikipedia.org/wiki?curid=56097" title="History of Styria">
History of Styria

The history of Styria concerns the region roughly corresponding to the modern Austrian state of Styria and the Slovene region of Styria ("Štajerska") from its settlement by Germans and Slavs in the Dark Ages until the present. This mountainous and scenic region, which became a centre for mountaineering in the 19th century, is often called the "Green March", because half of the area is covered with forests and one quarter with meadows, grasslands, vineyards and orchards. Styria is also rich in minerals, soft coal and iron, which has been mined at Erzberg since the time of the Romans. The Slovenske gorice/Windisch Büheln ("Slovene Hills") is a famous wine-producing district, stretching between Slovenia and Austria. Styria was for long the most densely populated and productive mountain region in Europe.
Styria's population before World War I was 68% German-speaking, 32% Slovene, bordered on (clockwise) Lower Austria, Hungary, Croatia, Carniola, Carinthia, Salzburg, and Upper Austria. In 1918 after World War I the southern, Slovene-speaking third south of the river Mur was incorporated into Slovenia in the Kingdom of Serbs, Croats and Slovenes. The remaining two-thirds became the Austrian federal state of Styria, while the Slovene-speaking third (Lower Styria) formed the informal region of Štajerska in Slovenia, now divided up into the statistical EU regions of Podravska, Savinjska and the major part of Slovenian Carinthia. The capital both of the duchy and the Austrian state has always been Graz, which is now also the residence of the governor and the seat of the administration of the land.
Political history.
Prehistory to Charlemagne.
The Roman history of Styria is as part of Noricum and Pannonia, with the romanized Celtic population of the Taurisci. During the great migrations, various Germanic tribes settled and/or traversed the region using the river valleys and low passes, but about 600 CE the Slavs took possession of the area and settled assimilating the remaining autochthonous romanized population.
When Styria came under the hegemony of Charlemagne as a part of Carantania (Carinthia), erected as a border territory against the Avars and Slavs, there was a large influx of Bavarii and other Christianized Germanic peoples, whom the bishops of Salzburg and the patriarchs of Aquileia kept faithful to Rome. Bishop Vergilius of Salzburg (745-84), was largely instrumental in establishing a church hierarchy in the Duchy and gained for himself the name of "Apostle of Carantania." In 811 Charlemagne made the Drave river the boundary between the Dioceses of Salzburg and Aquileia.
Middle Ages.
The March of Styria was created in the Duchy of Carinthia in the late 10th century as a defence against the Magyars. Long called the Carantanian or Carinthian March it was soon ruled by a margravial dynasty called the Otakars that originated from Steyr in Upper Austria thus giving the land its name: "Steiermark". This march was raised to become a duchy by the Emperor Frederick Barbarossa in 1180 after the fall of Henry the Lion of Bavaria.
With the death of Ottokar the first line of rulers of Styria became extinct; the region fell successively to the Babenberg family, rulers of Austria, as stipulated in the Georgenberg Pact; after their extinction to the control of Hungary (1254–60); to King Ottokar of Bohemia; in 1276 to the Habsburgs, who provided it with Habsburgs for Styrian dukes during the years 1379-1439 and 1564-1619.
At the time of the Ottoman invasions in the 16th and 17th centuries the land suffered severely and was depopulated. The Turks made incursions into Styria nearly twenty times; churches, monasteries, cities, and villages were destroyed and plundered, while the population was either killed or carried away into slavery.
Modern era.
The Semmering Railway, completed in 1854, was a triumph of engineering in its time, the oldest of the great European mountain railways. It was remarkable for its numerous and long tunnels and viaducts spanning mountain valleys, running from Gloggnitz in Lower Austria to Mürzzuschlag in Styria, and passing through some exceedingly beautiful scenery. The railway brought tourists to alpine lake resorts and mineral springs at Rohitsch (today's Rogaška Slatina) and Bad Gleichenberg, the brine springs of Bad Aussee, and the thermal springs of Tuffer (today's Laško), Neuhaus am Klausenbach and Tobelbad.
Following World War I, Styria was divided by the Treaty of Saint Germain. Lower Styria with the cities of Celje and Maribor became part of the Kingdom of Serbs, Croats and Slovenes, while the rest remained with Austria as the State of Styria. Other than in Carinthia, no fighting resulted from this, in spite of a German minority in Slovenia (the larger cities of Lower Styria were largely German-speaking).
Lower Styria was reattached to the Reichsgau Steiermark from 1942 to 1945, whence it was returned to Germany. After World War II, the lower third was granted to Yugoslavia. Today, it makes up about the eastern third of Slovenia.
Religious history.
The Protestant Reformation made its way into the country about 1530. Duke Karl (ruling 1564-90), whose wife was the Catholic Duchess Maria of Bavaria, introduced the Counter-Reformation into the country; in 1573 he invited the Jesuits into Styria and in 1586 he founded the Catholic University of Graz. In 1598 his son and successor Ferdinand suppressed all Protestant schools and expelled the teachers and preachers: Protestant doctrines were maintained only in a few isolated mountain valleys, as in the valley of the Inn and the valley of the Mur. On a narrow reading of the Peace of Augsburg, 1555, with its principle of "cuius regio, eius religio", only the nobility were not forced to return to the Roman Church; each could have Protestant services privately in his own house.
After Ferdinand had become Holy Roman Emperor in 1619 and had defeated his Protestant opponents in the Battle of White Mountain near Prague in 1620, he forbade all Protestant church services whatsoever (1625). In 1628 he commanded the nobility also to return to the Catholic faith. A large number of noble families, consequently, emigrated from the country. But most of them either returned, or their descendants did so, becoming Catholics and recovering their possessions.
In the second half of the 17th century renewed action against the Protestants in the isolated mountain valleys resulted in the expulsion of Protestant ministers with the peasants who would not give up Protestantism; about 30,000 chose compulsory emigration to Transylvania over conversion. Only an Edict of Toleration issued by Emperor Joseph II as late as 1781 put an end to religious repression. The Protestants then received the right to found parish communities and to exercise their religion in those enclaves undisturbed.
In 1848, all the provinces of the Habsburg Monarchy received complete liberty of religion and of conscience, parity of religions, and the right to the public exercise of religion.
Ecclesiastically the province was historically divided into two Catholic prince-bishoprics, Seckau and Lavant. From the time of their foundation both were suffragans of the Archdiocese of Salzburg. The Prince-Bishopric of Seckau was established in 1218; since 1786 the see of the prince-bishop has been Graz. The Prince-Bishopric of Lavant with its bishop's seat at Sankt Andrä in the Carinthian Lavant valley was founded as a bishopric in 1228 and raised to a prince-bishopric in 1446. In 1847 the bishop's seat was transferred from St. Andrä to "Marburg an der Drau" (Maribor), and after World War I the see's boundaries were adapted to the new political frontiers. A short-lived third Salzburg suffragan diocese of Leoben comprising 157 parishes in the districts of Leoben and Bruck an der Mur existed on Styrian soil from 1786 but was incorporated into the diocese of Graz-Seckau in 1856 Today the see of the bishop of Graz-Seckau is identical in territory with the Austrian State of Styria.

</doc>
<doc id="56098" url="http://en.wikipedia.org/wiki?curid=56098" title="Monte Carlo method">
Monte Carlo method

Monte Carlo methods (or Monte Carlo experiments) are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. They are often used in physical and mathematical problems and are most useful when it is difficult or impossible to use other mathematical methods. Monte Carlo methods are mainly used in three distinct problem classes: optimization, numerical integration, and generating draws from a probability distribution.
In physics-related problems, Monte Carlo methods are quite useful for simulating systems with many coupled degrees of freedom, such as fluids, disordered materials, strongly coupled solids, and cellular structures (see cellular Potts model). Other examples include modeling phenomena with significant uncertainty in inputs such as the calculation of risk in business and, in math, evaluation of multidimensional definite integrals with complicated boundary conditions. In application to space and oil exploration problems, Monte Carlo–based predictions of failure, cost overruns and schedule overruns are routinely better than human intuition or alternative "soft" methods.
The modern version of the Monte Carlo method was invented in the late 1940s by Stanislaw Ulam, while he was working on nuclear weapons projects at the Los Alamos National Laboratory. Immediately after Ulam's breakthrough, John von Neumann understood its importance and programmed the ENIAC computer to carry out Monte Carlo calculations.
Introduction.
Monte Carlo methods vary, but tend to follow a particular pattern:
For example, consider a circle inscribed in a unit square. Given that the circle and the square have a ratio of areas that is π/4, the value of π can be approximated using a Monte Carlo method:
In this procedure the domain of inputs is the square that circumscribes our circle. We generate random inputs by scattering grains over the square then perform a computation on each input (test whether it falls within the circle). Finally, we aggregate the results to obtain our final result, the approximation of π.
There are two important points to consider here: Firstly, if the grains are not uniformly distributed, then our approximation will be poor. Secondly, there should be a large number of inputs. The approximation is generally poor if only a few grains are randomly dropped into the whole square. On average, the approximation improves as more grains are dropped.
History.
Before the Monte Carlo method was developed, simulations tested a previously understood deterministic problem and statistical sampling was used to estimate uncertainties in the simulations. Monte Carlo simulations invert this approach, solving deterministic problems using a probabilistic analog (see Simulated annealing).
An early variant of the Monte Carlo method can be seen in the Buffon's needle experiment, in which π can be estimated by dropping needles on a floor made of parallel and equidistant strips. In the 1930s, Enrico Fermi first experimented with the Monte Carlo method while studying neutron diffusion, but did not publish anything on it.
In 1946, physicists at Los Alamos Scientific Laboratory were investigating radiation shielding and the distance that neutrons would likely travel through various materials. Despite having most of the necessary data, such as the average distance a neutron would travel in a substance before it collided with an atomic nucleus, and how much energy the neutron was likely to give off following a collision, the Los Alamos physicists were unable to solve the problem using conventional, deterministic mathematical methods. Stanislaw Ulam had the idea of using random experiments. He recounts his inspiration as follows:
Being secret, the work of von Neumann and Ulam required a code name. A colleague of von Neumann and Ulam, Nicholas Metropolis, suggested using the name "Monte Carlo", which refers to the Monte Carlo Casino in Monaco where Ulam's uncle would borrow money from relatives to gamble. Using lists of "truly random" random numbers was extremely slow, but von Neumann developed a way to calculate pseudorandom numbers, using the middle-square method. Though this method has been criticized as crude, von Neumann was aware of this: he justified it as being faster than any other method at his disposal, and also noted that when it went awry it did so obviously, unlike methods that could be subtly incorrect.
Monte Carlo methods were central to the simulations required for the Manhattan Project, though severely limited by the computational tools at the time. In the 1950s they were used at Los Alamos for early work relating to the development of the hydrogen bomb, and became popularized in the fields of physics, physical chemistry, and operations research. The Rand Corporation and the U.S. Air Force were two of the major organizations responsible for funding and disseminating information on Monte Carlo methods during this time, and they began to find a wide application in many different fields.
Uses of Monte Carlo methods require large amounts of random numbers, and it was their use that spurred the development of pseudorandom number generators, which were far quicker to use than the tables of random numbers that had been previously used for statistical sampling.
Definitions.
There is no consensus on how "Monte Carlo" should be defined. For example, Ripley defines most probabilistic modeling as "stochastic simulation", with "Monte Carlo" being reserved for Monte Carlo integration and Monte Carlo statistical tests. Sawilowsky distinguishes between a simulation, a Monte Carlo method, and a Monte Carlo simulation: a simulation is a fictitious representation of reality, a Monte Carlo method is a technique that can be used to solve a mathematical or statistical problem, and a Monte Carlo simulation uses repeated sampling to determine the properties of some phenomenon (or behavior). Examples:
Kalos and Whitlock point out that such distinctions are not always easy to maintain. For example, the emission of radiation from atoms is a natural stochastic process. It can be simulated directly, or its average behavior can be described by stochastic equations that can themselves be solved using Monte Carlo methods. "Indeed, the same computer code can be viewed simultaneously as a 'natural simulation' or as a solution of the equations by natural sampling."
Monte Carlo and random numbers.
Monte Carlo simulation methods do not always require truly random numbers to be useful — while for some applications, such as primality testing, unpredictability is vital. Many of the most useful techniques use deterministic, pseudorandom sequences, making it easy to test and re-run simulations. The only quality usually necessary to make good simulations is for the pseudo-random sequence to appear "random enough" in a certain sense.
What this means depends on the application, but typically they should pass a series of statistical tests. Testing that the numbers are uniformly distributed or follow another desired distribution when a large enough number of elements of the sequence are considered is one of the simplest, and most common ones. Weak correlations between successive samples is also often desirable/necessary.
Sawilowsky lists the characteristics of a high quality Monte Carlo simulation:
Pseudo-random number sampling algorithms are used to transform uniformly distributed pseudo-random numbers into numbers that are distributed according to a given probability distribution.
Low-discrepancy sequences are often used instead of random sampling from a space as they ensure even coverage and normally have a faster order of convergence than Monte Carlo simulations using random or pseudorandom sequences. Methods based on their use are called quasi-Monte Carlo methods.
Monte Carlo simulation versus "what if" scenarios.
There are ways of using probabilities that are definitely not Monte Carlo simulations — for example, deterministic modeling using single-point estimates. Each uncertain variable within a model is assigned a “best guess” estimate. Scenarios (such as best, worst, or most likely case) for each input variable are chosen and the results recorded.
By contrast, Monte Carlo simulations sample probability distribution for each variable to produce hundreds or thousands of possible outcomes. The results are analyzed to get probabilities of different outcomes occurring. For example, a comparison of a spreadsheet cost construction model run using traditional “what if” scenarios, and then run again with Monte Carlo simulation and Triangular probability distributions shows that the Monte Carlo analysis has a narrower range than the “what if” analysis. This is because the “what if” analysis gives equal weight to all scenarios (see quantifying uncertainty in corporate finance), while Monte Carlo method hardly samples in the very low probability regions. The samples in such regions are called "rare events".
Applications.
Monte Carlo methods are especially useful for simulating phenomena with significant uncertainty in inputs and systems with a large number of coupled degrees of freedom. Areas of application include:
Physical sciences.
Monte Carlo methods are very important in computational physics, physical chemistry, and related applied fields, and have diverse applications from complicated quantum chromodynamics calculations to designing heat shields and aerodynamic forms as well as in modeling radiation transport for radiation dosimetry calculations. In statistical physics Monte Carlo molecular modeling is an alternative to computational molecular dynamics, and Monte Carlo methods are used to compute statistical field theories of simple particle and polymer systems. Quantum Monte Carlo methods solve the many-body problem for quantum systems. In experimental particle physics, Monte Carlo methods are used for designing detectors, understanding their behavior and comparing experimental data to theory. In astrophysics, they are used in such diverse manners as to model both galaxy evolution and microwave radiation transmission through a rough planetary surface. Monte Carlo methods are also used in the ensemble models that form the basis of modern weather forecasting.
Engineering.
Monte Carlo methods are widely used in engineering for sensitivity analysis and quantitative probabilistic analysis in process design. The need arises from the interactive, co-linear and non-linear behavior of typical process simulations. For example,
Computational biology.
Monte Carlo methods are used in various fields of computational biology, for example for Bayesian inference in phylogeny, or for studying biological systems such as genomes, proteins, or membranes.
The systems can be studied in the coarse-grained or "ab initio" frameworks depending on the desired accuracy. 
Computer simulations allow us to monitor the local environment of a particular molecule to see if some chemical
reaction is happening for instance. In cases where it is not feasible to conduct a physical experiment, thought experiments can be conducted (for instance: breaking bonds, introducing impurities at specific sites, changing the local/global structure, or introducing external fields).
Computer graphics.
Path Tracing, occasionally referred to as Monte Carlo Ray Tracing, renders a 3D scene by randomly tracing samples of possible light paths. Repeated sampling of any given pixel will eventually cause the average of the samples to converge on the correct solution of the rendering equation, making it one of the most physically accurate 3D graphics rendering methods in existence.
Applied statistics.
In applied statistics, Monte Carlo methods are generally used for two purposes:
Monte Carlo methods are also a compromise between approximate randomization and permutation tests. An approximate randomization test is based on a specified subset of all permutations (which entails potentially enormous housekeeping of which permutations have been considered). The Monte Carlo approach is based on a specified number of randomly drawn permutations (exchanging a minor loss in precision if a permutation is drawn twice – or more frequently—for the efficiency of not having to track which permutations have already been selected).
Artificial intelligence for games.
Monte Carlo methods have been developed into a technique called Monte-Carlo tree search that is useful for searching for the best move in a game. Possible moves are organized in a search tree and a large number of random simulations are used to estimate the long-term potential of each move. A black box simulator represents the opponent's moves.
The Monte Carlo Tree Search (MCTS) method has four steps:
The net effect, over the course of many simulated games, is that the value of a node representing a move will go up or down, hopefully corresponding to whether or not that node represents a good move.
Monte Carlo Tree Search has been used successfully to play games such as Go, Tantrix, Battleship, Havannah, and Arimaa.
Design and visuals.
Monte Carlo methods are also efficient in solving coupled integral differential equations of radiation fields and energy transport, and thus these methods have been used in global illumination computations that produce photo-realistic images of virtual 3D models, with applications in video games, architecture, design, computer generated films, and cinematic special effects.
Finance and business.
Monte Carlo methods in finance are often used to evaluate investments in projects at a business unit or corporate level, or to evaluate financial derivatives. They can be used to model project schedules, where simulations aggregate estimates for worst-case, best-case, and most likely durations for each task to determine outcomes for the overall project.
Use in mathematics.
In general, Monte Carlo methods are used in mathematics to solve various problems by generating suitable random numbers (see also Random number generation) and observing that fraction of the numbers that obeys some property or properties. The method is useful for obtaining numerical solutions to problems too complicated to solve analytically. The most common application of the Monte Carlo method is Monte Carlo integration.
Integration.
Deterministic numerical integration algorithms work well in a small number of dimensions, but encounter two problems when the functions have many variables. First, the number of function evaluations needed increases rapidly with the number of dimensions. For example, if 10 evaluations provide adequate accuracy in one dimension, then 10100 points are needed for 100 dimensions—far too many to be computed. This is called the curse of dimensionality. Second, the boundary of a multidimensional region may be very complicated, so it may not be feasible to reduce the problem to a series of nested one-dimensional integrals. 100 dimensions is by no means unusual, since in many physical problems, a "dimension" is equivalent to a degree of freedom.
Monte Carlo methods provide a way out of this exponential increase in computation time. As long as the function in question is reasonably well-behaved, it can be estimated by randomly selecting points in 100-dimensional space, and taking some kind of average of the function values at these points. By the central limit theorem, this method displays formula_1 convergence—i.e., quadrupling the number of sampled points halves the error, regardless of the number of dimensions.
A refinement of this method, known as importance sampling in statistics, involves sampling the points randomly, but more frequently where the integrand is large. To do this precisely one would have to already know the integral, but one can approximate the integral by an integral of a similar function or use adaptive routines such as stratified sampling, recursive stratified sampling, adaptive umbrella sampling or the VEGAS algorithm.
A similar approach, the quasi-Monte Carlo method, uses low-discrepancy sequences. These sequences "fill" the area better and sample the most important points more frequently, so quasi-Monte Carlo methods can often converge on the integral more quickly.
Another class of methods for sampling points in a volume is to simulate random walks over it (Markov chain Monte Carlo). Such methods include the Metropolis-Hastings algorithm, Gibbs sampling and the Wang and Landau algorithm.
Simulation and optimization.
Another powerful and very popular application for random numbers in numerical simulation is in numerical optimization. The problem is to minimize (or maximize) functions of some vector that often has a large number of dimensions. Many problems can be phrased in this way: for example, a computer chess program could be seen as trying to find the set of, say, 10 moves that produces the best evaluation function at the end. In the traveling salesman problem the goal is to minimize distance traveled. There are also applications to engineering design, such as multidisciplinary design optimization. It has been applied to solve particle dynamics simulation model Quasi-one-dimensional models to efficiently explore large configuration space.
The traveling salesman problem is what is called a conventional optimization problem. That is, all the facts (distances between each destination point) needed to determine the optimal path to follow are known with certainty and the goal is to run through the possible travel choices to come up with the one with the lowest total distance. However, let's assume that instead of wanting to minimize the total distance traveled to visit each desired destination, we wanted to minimize the total time needed to reach each destination. This goes beyond conventional optimization since travel time is inherently uncertain (traffic jams, time of day, etc.). As a result, to determine our optimal path we would want to use simulation - optimization to first understand the range of potential times it could take to go from one point to another (represented by a probability distribution in this case rather than a specific distance) and then optimize our travel decisions to identify the best path to follow taking that uncertainty into account.
Inverse problems.
Probabilistic formulation of inverse problems leads to the definition of a probability distribution in the model space. This probability distribution combines prior information with new information obtained by measuring some observable parameters (data). As, in the general case, the theory linking data with model parameters is nonlinear, the posterior probability in the model space may not be easy to describe (it may be multimodal, some moments may not be defined, etc.).
When analyzing an inverse problem, obtaining a maximum likelihood model is usually not sufficient, as we normally also wish to have information on the resolution power of the data. In the general case we may have a large number of model parameters, and an inspection of the marginal probability densities of interest may be impractical, or even useless. But it is possible to pseudorandomly generate a large collection of models according to the posterior probability distribution and to analyze and display the models in such a way that information on the relative likelihoods of model properties is conveyed to the spectator. This can be accomplished by means of an efficient Monte Carlo method, even in cases where no explicit formula for the "a priori" distribution is available.
The best-known importance sampling method, the Metropolis algorithm, can be generalized, and this gives a method that allows analysis of (possibly highly nonlinear) inverse problems with complex "a priori" information and data with an arbitrary noise distribution.
Petroleum reservoir management.
Monte Carlo methods are very popular in hydrocarbon reservoir management in the context of nonlinear inverse problems. This includes generating computational models of oil and gas reservoirs for consistency with observed production data. For the goal of decision making and uncertainty assessment, Monte Carlo methods are used for generating multiple geological realizations.

</doc>
<doc id="56099" url="http://en.wikipedia.org/wiki?curid=56099" title="Red dwarf">
Red dwarf

Hertzsprung–Russell diagram
Spectral type
Brown dwarfs
White dwarfs
Red dwarfs
Subdwarfs
Main sequence<br>("dwarfs")
Subgiants
Giants
Bright giants
Supergiants
Hypergiants
absolute
magni-
tude
A red dwarf is a small and relatively cool star on the main sequence, either late K or M spectral type. Red dwarfs range in mass from a low of 0.075 solar masses (M☉) to about 0.50 M☉ and have a surface temperature of less than 4,000 K.
Red dwarfs are by far the most common type of star in the Milky Way, at least in the neighborhood of the Sun, but because of their low luminosity, individual red dwarfs cannot easily be observed. From Earth, not one is visible to the naked eye. Proxima Centauri, the nearest star to the Sun, is a red dwarf (Type M5, apparent magnitude 11.05), as are twenty of the next thirty nearest.
According to some estimates, red dwarfs make up three-quarters of the stars in the Milky Way.
Stellar models indicate that red dwarfs less than 0.35 M☉ are fully convective. Hence the helium produced by the thermonuclear fusion of hydrogen is constantly remixed throughout the star, avoiding a buildup at the core. Red dwarfs therefore develop very slowly, having a constant luminosity and spectral type for, in theory, some trillions of years, until their fuel is depleted. Because of the comparatively short age of the universe, no red dwarfs of advanced evolutionary stages exist.
Description and characteristics.
Red dwarfs are very-low-mass stars. Consequently they have relatively low temperatures in their cores and energy is generated at a slow rate through nuclear fusion of hydrogen into helium by the proton–proton (PP) chain mechanism. Hence these stars emit little light, sometimes as little as 1⁄10,000 that of the Sun. Even the largest red dwarfs (for example HD 179930, HIP 12961 and Lacaille 8760) have only about 10% of the Sun's luminosity. In general, red dwarfs less than 0.35 M☉ transport energy from the core to the surface by convection. Convection occurs because of opacity of the interior, which has a high density compared to the temperature. As a result, energy transfer by radiation is decreased, and instead convection is the main form of energy transport to the surface of the star. Above this mass, the red dwarfs will have a region around their core where convection does not occur.
Because late-type red dwarfs are fully convective, helium does not accumulate at the core and, compared to larger stars such as the Sun, they can burn a larger proportion of their hydrogen before leaving the main sequence. As a result, red dwarfs have estimated lifespans far longer than the present age of the universe, and stars less than 0.8 M☉ have not had time to leave the main sequence. The lower the mass of a red dwarf, the longer the lifespan. It is believed that the lifespan of these stars exceeds the expected 10 billion year lifespan of our Sun by the third or fourth power of the ratio of the solar mass to their masses; thus a 0.1 M☉ red dwarf may continue burning for 10 trillion years. As the proportion of hydrogen in a red dwarf is consumed, the rate of fusion declines and the core starts to contract. The gravitational energy released by this size reduction is converted into heat, which is carried throughout the star by convection.
According to computer simulations, the minimum mass a red dwarf must have in order to become a red giant is 0.25 M☉; less massive objects, as they age, increase their surface temperatures and luminosities becoming blue dwarfs and finally become white dwarfs.
The less massive the star, the longer this evolutionary process takes; for example, it has been calculated that a 0.16 M☉ red dwarf (approximately the mass of the nearby Barnard's Star) would stay on the main sequence during 2.5 trillion years that would be followed by five billion years as a blue dwarf, in which the star would have 1/3 of the Sun's luminosity (L☉) and a surface temperature of 6,500‒8,500 Kelvin.
The fact that red dwarfs and other low-mass stars still remain on the main sequence when more massive stars have moved off the main sequence allows the age of star clusters to be estimated by finding the mass at which the stars turn off the main sequence. This provides a lower, stellar, age limit to the Universe and also allows formation timescales to be placed upon the structures within the Milky Way, namely the Galactic halo and Galactic disk.
One mystery which has not been solved as of 2009[ [update]] is the absence of red dwarfs with no metals. (In astronomy, a metal is any element heavier than hydrogen or helium.) The Big Bang model predicts the first generation of stars should have only hydrogen, helium, and trace amounts of lithium. If such stars included red dwarfs, they should still be observable today, but none have yet been identified. The preferred explanation is that without heavy elements only large and not yet observed population III stars can form, and these rapidly burn out, leaving heavy elements which then allow for the formation of red dwarfs. Alternative explanations, such as the idea that zero-metal red dwarfs are dim and could be few in number, are considered much less likely because they seem to conflict with stellar evolution models.
Planets.
Many red dwarfs are orbited by extrasolar planets but large Jupiter-sized planets are comparatively rare. Doppler surveys around a wide variety of stars indicate about 1 in 6 stars having twice the mass of the Sun are orbited by one or more Jupiter-sized planets, vs. 1 in 16 for Sun-like stars and only 1 in 50 for red dwarfs. On the other hand, microlensing surveys indicate that long-period Neptune-mass planets are found around 1 in 3 red dwarfs.
Observations with HARPS further indicate 40% of red dwarfs have a "super-Earth" class planet orbiting in the habitable zone where liquid water can exist on the surface of the planet.
At least four and possibly up to six extrasolar planets were discovered orbiting the red dwarf Gliese 581 between 2005–2010. One planet has about the mass of Neptune, or 16 Earth masses (M⊕). It orbits just 6 million kilometers (0.04 AU) from its star, and so is estimated to have a surface temperature of 150 °C, despite the dimness of the star. In 2006, an even smaller extrasolar planet (only 5.5 M⊕) was found orbiting the red dwarf OGLE-2005-BLG-390L; it lies 390 million km (2.6 AU) from the star and its surface temperature is −220 °C (56 K).
In 2007, a new, potentially habitable extrasolar planet, Gliese 581 c, was found, orbiting Gliese 581. If the minimum mass estimated by its discoverers (a team led by Stephane Udry), namely 5.36 M⊕, is correct, it is the smallest extrasolar planet revolving around a main-sequence star discovered to date and since then Gliese 581 d, which is also potentially habitable, was discovered. (There are smaller planets known around a neutron star, named PSR B1257+12.) The discoverers estimate its radius to be 1.5 times that of Earth (R⊕).
Gliese 581 c and d are within the habitable zone of the host star, and are two of the most likely candidates for habitability of any extrasolar planets discovered so far. Gliese 581 g, detected September 2010, has a near-circular orbit in the middle of the star's habitable zone. However, the planet's existence is contested.
Habitability.
Planetary habitability of red dwarf systems is subject to some debate. In spite of their great numbers and long lifespans, there are several factors which may make life difficult on planets around a red dwarf. First, planets in the habitable zone of a red dwarf would be so close to the parent star that they would likely be tidally locked. This would mean that one side would be in perpetual daylight and the other in eternal night. This could create enormous temperature variations from one side of the planet to the other. Such conditions would appear to make it difficult for forms of life similar to those on Earth to evolve. And it appears there is a great problem with the atmosphere of such tidally locked planets: the perpetual night zone would be cold enough to freeze the main gases of their atmospheres, leaving the daylight zone nude and dry. On the other hand, recent theories propose that either a thick atmosphere or planetary ocean could potentially circulate heat around such a planet. Alternatively, a moon in orbit around a gas giant planet may be habitable. It would circumvent the tidal lock problem by becoming tidally locked to its planet. This way there would be a day/night cycle as the moon orbited its primary, and there would be distribution of heat.
In addition, red dwarfs emit most of their radiation as infrared light, while on Earth plants use energy mostly in the visible spectrum. Red dwarfs emit almost no ultraviolet light, which would be a problem, should this kind of light be required for life to exist. Variability in stellar energy output may also have negative impacts on development of life. Red dwarfs are often covered by starspots, reducing stellar output by as much as 40% for months at a time. At other times, some red dwarfs, called flare stars, can emit gigantic flares, doubling their brightness in minutes. This variability may also make it difficult for life to develop and persist near a red dwarf. Gibor Basri of the University of California, Berkeley claims a planet orbiting close to a red dwarf could keep its atmosphere even if the star flares.
Spectral standard stars.
The spectral standards for M-type stars have changed slightly over the years, but settled down somewhat since the early 1990s. Part of this is due to the fact that even the nearest M dwarfs are fairly faint, and the study of mid- to late-M dwarfs has only taken off in the past few decades due to evolution of astronomical techniques, from photographic plates to charged-couple devices (CCDs) to infrared-sensitive arrays.
The revised Yerkes Atlas system (Johnson & Morgan 1953) listed only 2 M-type spectral standard stars: HD 147379 (M0 V)
and HD 95735/Lalande 21185 (M2 V). While HD 147379 was not considered a standard by expert classifiers in later compendia of standards, Lalande 21185 is still a primary standard for M2 V. Robert Garrison does not list any "anchor" standards among the M dwarf stars, but Lalande 21185 has survived as a M2 V standard through many compendia. The review on MK classification by Morgan & Keenan (1973) did not contain M dwarf standards. In the mid-1970s, M dwarf standard stars were published by Keenan & McNeil (1976) and Boeshaar (1976), but unfortunately there was little agreement among the standards. As later cooler stars were identified through the 1980s, it was clear that an overhaul of the M dwarf standards was needed. Building primarily upon the Boeshaar standards, a group at Steward Observatory (Kirkpatrick, Henry, & McCarthy 1991) filled in the spectral sequence from K5 V to M9 V. It is these M type dwarf standard stars which have largely survived intact as the main standards to the modern day. There have been negligible changes in the M dwarf spectral sequence since 1991. Additional M dwarf standards were compiled by Henry et al. (2002), and D. Kirkpatrick has recently
reviewed the classification of M dwarf stars and standard stars in Gray & Corbally's 2009 monograph. The M-dwarf primary spectral standards are: GJ 270 (M0 V), GJ 229A (M1 V), Lalande 21185 (M2 V), Gliese 581 (M3 V), GJ 402 (M4 V), GJ 51 (M5 V), Wolf 359 (M6 V), Van Biesbroeck 8 (M7 V), VB 10 (M8 V), LHS 2924 (M9 V).
References.
</dl>

</doc>
<doc id="56100" url="http://en.wikipedia.org/wiki?curid=56100" title="Ouagadougou">
Ouagadougou

Ouagadougou (; Mossi: ]) is the capital of Burkina Faso and the administrative, communications, cultural and economic centre of the nation. It is also the country's largest city, with a population of 1,475,223 ("as of 2006"). The city's name is often shortened to "Ouaga". The inhabitants are called "ouagalais". The spelling of the name "Ouagadougou" is derived from the French orthography common in former French African colonies.
Ouagadougou's primary industries are food processing and textiles. It is served by an international airport, rail links to Abidjan in the Ivory Coast. There is no rail service to Kaya. There is a paved highway to Niamey, Niger, south to Ghana, and Southwest to Ivory Coast. Ouagadougou was the site of Ouagadougou grand market, one of the largest markets in West Africa, which burned in 2003 and has since been reopened. Other attractions include the National Museum of Burkina Faso, the Moro-Naba Palace (site of the Moro-Naba Ceremony), the National Museum of Music, and several craft markets.
History.
The name "Ouagadougou" dates back to the 15th century when the Ninsi tribes inhabited the area. They were in constant conflict until 1441 when Wubri, a Yonyonse hero and an important figure in Burkina Faso's history, led his tribe to victory. He then renamed the area from "Kumbee-Tenga", as the Ninsi had called it, to "Wage sabre soba koumbem tenga", meaning "head war chief's village". "Ouagadougou" is a Francophone spelling of the name.
The city became the capital of the Mossi Empire in 1441 and was the permanent residence of the Mossi emperors (Moro-Naba) from 1681. The Moro-Naba Ceremony is still performed every Friday by the Moro-Naba and his court. The French made Ouagadougou the capital of the Upper Volta territory (basically the same area as contemporary independent Burkina Faso) in 1919. In 1954 the railroad line from Ivory Coast reached the city. The population of Ouagadougou doubled from 1954 to 1960 and has been doubling since about every ten years.
Geography.
Ouagadougou, situated on the central plateau (12.4° N 1.5° W), grew around the imperial palace of the Mogho Naaba. Being an administrative center of colonial rule, it became an important urban center in the post-colonial era. First the capital of the Mossi Kingdoms and later of Upper Volta and Burkina Faso, Ouagadougou became a veritable communal center in 1995.
Government.
The first municipal elections were held in 1956.
Ouagadougou is governed by a mayor, who is elected for a five-year term, two senior councillors, and 90 councillors.
The city is divided into five arrondissements, consisting of 30 sectors, which are subdivided into districts. Districts of Ouagadougou include Gounghin, Kamsaoghin, Koulouba, Moemmin, Niogsin, Paspanga, Peuloghin, Bilbalogho, and Tiendpalogo. Seventeen villages comprise the Ouagadougou metropolitan area, which is about 219.3 km².
The city is clean, pleasant, well-organized and relatively safe. There are good restaurants and hotels, clean water, and dependable electricity. Streets are paved, there are stoplights, which most drivers respect. Traffic is congested in some neighborhoods in the mornings and late afternoons.
The population of this area is estimated to be 1,475,000 inhabitants, 48% of which are men and 52% women. The rural population is about 5% and the urban population about 95% of the total, and the density is 6,727 inhabitants per square kilometer, according to 2006 census.
Concerning city management, the communes of Ouagadougou have made the decision to invest in huge projects. This is largely because Ouagadougou constitutes a 'cultural centre' by merit of holding the SIAO (International Arts and Crafts fair) and the FESPACO (Panafrican Film and Television Festival of Ouagadougou). Moreover, the growing affluence of the villages allow for such investment, and the population's rapid growth necessitates it.
Climate.
The climate of Ouagadougou is hot semi-arid (BSh) under Köppen-Geiger classification, that closely borders with tropical wet and dry (Aw). The city is part of the Sudano-Sahelian area, with a rainfall of about 800 mm per year. The rainy season stretches from May to October, its height from June to September, with a mean average temperature of 28 °C. The cold season runs from December to January, with a minimum average temperature of 16 °C. The maximum temperature during the hot season, which runs from March to May, can reach 43 °C. The harmattan (a dry wind) and the monsoon are the two main factors that determine Ouagadougou's climate. Even though Ouagadougou is farther from the equator, its hottest months' temperatures are slightly hotter than those of Bobo-Dioulasso, the second most populous city.
Social life and education.
Education.
Though literacy in Ouagadougou is not high, there are three universities in the city. The largest college is the state University of Ouagadougou which was founded in 1974 and the UUPN, University of United Popular Nations based on international learning system. In 2010 it had around 40,000 students in 2010 (83% of the national population of university students). The official language in the city is French and the principal local languages are More, Dyula and Fulfulde. The bilingual program in schools (French plus one of the local languages) was established in 1994. But this so-called bilingual program has never been effective. The local authorities have failed to implement it.
Sport, culture, and leisure.
A wide array of sports, including association football, basketball, and volleyball, is played by Ouagadougou inhabitants. There are sports tournaments and activities organized by the local authorities. 
There are a number of cultural and art venues, such as the Maison du Peuple and Salle des Banquets, in addition to performances of many genres of music, including traditional folk music, modern music, and rap.
Art and crafts.
Several international festivals and activities are organized within the municipality, such as FESPACO (Panafrican Film and Television Festival of Ouagadougou), which is Africa's largest festival of this type, SIAO (International Art and Craft Fair), FESPAM (Pan-African Music Festival), FITMO (International Theatre and Marionnette Festival) and FESTIVO.
Health.
Ouagadougou has both state and private hospitals. The two state hospitals in the city are the Centre hospitalier national Yalgado Ouedraogo (CHNYO) and the Centre hospitalier national pédiatrique Charles de Gaulle (CHNP-CDG), but there are also private hospitals. Despite that, the local population still largely can only afford traditional local medicine and the "pharmacopée".
Transport.
Many residents travel on motorcycles and mopeds. The large private vendor of motorcycles JC Megamonde sells 50,000 motorbikes and mopeds every year.
Ouagadougou's citizens also travel in green cabs, which take their passengers anywhere in town for 200 to 400 CFA, but the price is higher after 10:00 pm and can then reach 1000 CFA. 
Air transport.
Ouagadougou Airport (code OUA) serves the area with flights to West Africa and Europe. Air Burkina has its head office in the Air Burkina Storey Building (French: "Immeuble Air Burkina") in Ouagadougou.
Rail.
Ouagadougou is connected by passenger rail service to Bobo-Dioulasso, Koudougou and Ivory Coast. As of June 2014 "Sitarail" operates a passenger train three times a week along the route from Ouagadougou to Abidjan. There are freight services to Kaya in the north of Burkina Faso and in 2014 plans were announced to revive freight services to the Manganese mine at Tambao starting in 2016.
Economy.
The Copromof workshop in Ouagadougou sews cotton lingerie for the French label "Atelier Augusti."
International relations.
Twin towns – Sister cities.
Ouagadougou is twinned with:
Tourism.
Parks.
The Bangr-Weoogo urban park (area: 2.63 km2), before colonialism, belonged to the Mosse chiefs. Considering it a sacred forest, many went there for traditional initiations or for refuge. The French colonists, disregarding its local significance and history, established it as a park in the 1930s. In 1985, renovations were done in the park. In January 2001, the park was renamed "Parc Urbain Bangr-Weoogo", meaning "the urban park of the forest of knowledge".
Another notable park in Ouagadougou is the "L'Unité Pédagogique", which shelters animals in a semi-free state. This botanic garden/biosphere system stretches over 8 ha and also serves as a museum for the country's history.
"Jardin de l'amitié Ouaga-Loudun" (Garden of Ouaga-Loudun Friendship), with a green space that was renovated in 1996, is a symbol of the twin-city relationship between Ouagadougou and Loudun in France. It is situated in the center of the city, near the "Nation Unies' crossroads".
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="56101" url="http://en.wikipedia.org/wiki?curid=56101" title="Ethnic conflict">
Ethnic conflict

An ethnic conflict or ethnic war is an armed conflict between ethnic groups.
It contrasts with civil war on one hand (where a single nation or ethnic group is fighting among itself) and regular warfare on the other, where two or more sovereign states (which may or may not be nation states) are in conflict.
Examples of ethnic wars since the 1990s were typically caused by secessionist movements leading to the breakup of multi-ethnic states along ethnic lines: the Yugoslav Wars, the First Chechen War, the Nagorno-Karabakh War, the Rwandan Civil War, and War in Darfur among others.
Academic explanations of ethnic conflict generally fall into one of three schools of thought: primordialist, instrumentalist or constructivist. Intellectual debate has also focused around the issue of whether ethnic conflict has become more prevalent since the end of the Cold War, and on devising ways of managing conflicts, through instruments such as consociationalism and federalisation.
Theories of ethnic conflict.
The causes of ethnic conflict are debated by political scientists and sociologists who generally fall into one of three schools of thought: primordialist, instrumentalist, and constructivist. More recent scholarship draws on all three schools.
Primordialist accounts.
Proponents of primordialist accounts of ethnic conflict argue that “[e]thnic groups and nationalities exist because there are traditions of belief and action towards primordial objects such as biological features and especially territorial location”. The primordialist account relies on a concept of kinship between members of an ethnic group. Donald L. Horowitz argues that this kinship “makes it possible for ethnic groups to think in terms of family resemblances”.
There are a number of political scientists who refer to the concept of ethnic wars as a myth because they argue that the root causes of ethnic conflict do not involve ethnicity but rather institutional, political, and economic factors. These political scientists argue that the concept of ethnic war is misleading because it leads to an essentialist conclusion that certain groups are doomed to fight each other when in fact the wars between them are the result of political decisions. Opposing groups may substitute ethnicity for the underlying factors to simplify identification of friend and foe.
Instrumentalist accounts.
Anthony Smith notes that the instrumentalist account “came to prominence in the 1960s and 1970s in the United States, in the debate about (white) ethnic persistence in what was supposed to have been an effective melting pot”. This new theory sought to explain such persistence as the result of the actions of community leaders, “who used their cultural groups as sites of mass mobilization and as constituencies in their competition for power and resources, because they found them more effective than social classes”. In this account of ethnic identification, “[e]thnicity and race are viewed as instrumental identities, organized as means to particular ends”.
Whether ethnicity is a fixed perception is not crucial in the instrumentalist accounts. Moreover, the scholars of this school do generally not oppose neither that ethnic difference is a part of many conflicts nor that a lot of belligerent human beings believe that they are fighting over such difference. Instrumentalists simply claim that ethnic difference is not sufficient to explain conflicts.
Constructivist accounts.
A third, constructivist, set of accounts stress the importance of the socially constructed nature of ethnic groups, drawing on Benedict Anderson's concept of the imagined community. Proponents of this account point to Rwanda as an example since the Tutsi/Hutu distinction was codified by the Belgian colonial power in the 1930s on the basis of cattle ownership, physical measurements and church records. Identity cards were issued on this basis, and these documents played a key role in the genocide of 1994.
Scholars of ethnic conflict and civil wars have introduced theories that draw insights from all three traditional schools of thought. In "The Geography of Ethnic Violence", for example, Monica Duffy Toft shows how ethnic group settlement patterns, socially constructed identities, charismatic leaders, issue indivisibility, and state concern with precedent setting can lead rational actors to escalate a dispute to violence, even when doing so is likely to leave contending groups much worse off. Such research addresses empirical puzzles that are difficult to explain using primordialist, instrumentalist, or constructivist approaches alone.
Ethnic conflict in the post–Cold War world.
The term "ethnicity" as used today arose in the mid-20th century, replacing the terminology of "races" or "nations" used for the concept in the 19th century. Regular warfare was formerly conceived as conflicts between nations, and only with the rise of multi-ethnic societies and the shift to asymmetric warfare did the concept of "ethnic conflict" arise as separate from generic "war".
This has been the case especially since the collapse of the multi-ethnic Soviet Union and of the relatively more homogeneous Yugoslavia in the 1990s, both of which were followed by ethnic conflicts that escalated to violence and civil war.
The end of the Cold War thus sparked interest in two important questions about ethnic conflict: was ethnic conflict on the rise; and given that some ethnic conflicts had escalated into serious violence, what, if anything, could scholars of large-scale violence (security studies, strategic studies, interstate politics) offer by way of explanation?
One of the most debated issues relating to ethnic conflict is whether it has become more or less prevalent in the post–Cold War period. At the end of the Cold War, academics including Samuel P. Huntington and Robert D. Kaplan predicted a proliferation of conflicts fuelled by civilisational clashes, tribalism, resource scarcity and overpopulation.
The post–Cold War period has witnessed a number of ethnically-informed secessionist movements, predominantly within the former communist states. Conflicts have involved secessionist movements in the former Yugoslavia, Transnistria in Moldova, Armenians in Azerbaijan, Abkhaz and Ossetians in Georgia. Outside the former communist bloc, ethno-seperatist strife in the same period has occurred in areas such as Sri Lanka, West Papua, Chiapas, East Timor, the Basque Country and Southern Sudan.
However, some theorists contend that this does not represent a rise in the incidence of ethnic conflict, seeing many of the proxy wars fought during the Cold War as ethnic conflicts masked as hot spots of the Cold War. Research shows that the fall of Communism and the increase in the number of capitalist states were accompanied by a decline in total warfare, interstate wars, ethnic wars, revolutionary wars, and the number of refugees and displaced persons. Indeed, some scholars have questioned whether the concept of ethnic conflict is useful at all. Others have attempted to test the "clash of civilisations" thesis, finding it to be difficult to operationalise and that civilisational conflicts have not risen in intensity in relation to other ethnic conflicts since the end of the Cold War.
On the question of whether scholars deeply invested in theories of interstate violence could adapt their theories to explain or predict large-scale ethnic violence, a key issue proved to be whether ethnic groups could be considered "rational" actors.
Prior to the end of the Cold War, the consensus among students of large-scale violence was that ethnic groups should be considered irrational actors, or semi-rational at best. If true, general explanations of ethnic violence would be impossible. In the years since, however, scholarly consensus has shifted to consider that ethnic groups may in fact be counted as rational actors, and the puzzle of their apparently irrational actions (for example, fighting over territory of little or no intrinsic worth) must therefore be explained in some other way. As a result, the possibility of a general explanation of ethnic violence has grown, and collaboration between comparativist and international-relations subfields has resulted in increasingly useful theories of ethnic conflict.
The book "" by Amy Chua argues that democratization may give political power to an ethnic majority that is poor compared to an ethnic minority that has become more economically successful. This may cause conflict, persecution, and even genocide of the minority.
Ethnic conflict regulation.
A number of scholars have attempted to synthesize the methods available for the resolution, management or transformation of ethnic conflict. John Coakley, for example, has developed a typology of the methods of conflict resolution that have been employed by states, which he lists as: indigenization, accommodation, assimilation, acculturation, population transfer, boundary alteration, genocide and ethnic suicide. Greg Meyjes suggests that the degree to which ethnic tensions stem from inter-group disparity, dominance, discrimination, and repression has been critically unaccounted -- and proposes a cultural rights approach to understanding and managing ethnic conflicts.
John McGarry and Brendan O'Leary have developed a taxonomy of eight macro-political ethnic conflict regulation methods, which they note are often employed by states in combination with each other. They include a number of methods that they note are clearly morally unacceptable.

</doc>
<doc id="56102" url="http://en.wikipedia.org/wiki?curid=56102" title="Manneken Pis">
Manneken Pis

Manneken Pis ( ] ; "Little man Pee" in Dutch; French: "le Petit Julien") is a landmark small bronze sculpture in Brussels, depicting a naked little boy urinating into a fountain's basin. It was designed by Hiëronymus Duquesnoy the Elder and put in place in 1618 or 1619. 
Location.
The famous statue is located at the junction of Rue de l'Étuve/Stoofstraat and Rue du Chêne/Eikstraat. To find it, one takes the left lane next to the Brussels Town Hall from the famous Grand Place and walks a few hundred metres southwest via Rue Charles Buls/Karel Bulsstraat.
History and legends.
The 61 cm tall bronze statue on the corner of Rue de l'Etuve and Rue des Grands Carmes was made in 1619 by Brussels sculptor Hieronimus Duquesnoy the Elder, father of the more famous François Duquesnoy. The figure has been repeatedly stolen: the current statue dates from 1965. The original restored version is kept at the Maison du Roi/Broodhuis on the Grand Place.
There are several legends behind this statue, but the most famous is the one about Duke Godfrey III of Leuven. In 1142, the troops of this two-year-old lord were battling against the troops of the Berthouts, the lords of Grimbergen, in Ransbeke (now Neder-Over-Heembeek). The troops put the infant lord in a basket and hung the basket in a tree to encourage them. From there, the boy urinated on the troops of the Berthouts, who eventually lost the battle.
Another legend states that in the 14th century, Brussels was under siege by a foreign power. The city had held its ground for some time, so the attackers conceived of a plan to place explosive charges at the city walls. A little boy named Julianske happened to be spying on them as they were preparing. He urinated on the burning fuse and thus saved the city. There was at the time (middle of the 15th century, perhaps as early as 1388) a similar statue made of stone. The statue was stolen several times.
Another story (told often to tourists) tells of a wealthy merchant who, during a visit to the city with his family, had his beloved young son go missing. The merchant hastily formed a search party that scoured all corners of the city until the boy was found happily urinating in a small garden. The merchant, as a gift of gratitude to the locals who helped out during the search, had the fountain built.
Another legend was that a small boy went missing from his mother when shopping in the centre of the city. The woman, panic-stricken by the loss of her child, called upon everyone she came across, including the mayor of the city. A city-wide search began and when at last the child was found, he was urinating on the corner of a small street. The story was passed down over time and the statue erected as a tribute to the well-known legend.
Another legend tells of the young boy who was awoken by a fire and was able to put out the fire with his urine, in the end this helped stop the king's castle from burning down.
Traditions.
The statue is dressed in costume several times each week, according to a published schedule which is posted on the railings around the fountain. His wardrobe consists of several hundred different costumes, many of which may be viewed in a permanent exhibition inside the City Museum, located in the Grand Place, immediately opposite the Town Hall. The costumes are managed by the non-profit association "The Friends of Manneken-Pis", who review hundreds of designs submitted each year, and select a small number to be produced and used.
Although the proliferation of costumes is of twentieth-century origin, the occasional use of costumes dates back almost to the date of casting, the oldest costume on display in the City Museum being of seventeenth-century origin. The changing of the costume on the figure is a colourful ceremony, often accompanied by brass band music. Many costumes represent the national dress of nations whose citizens come to Brussels as tourists; others are the uniforms of assorted trades, professions, associations, and branches of the civil and military services.
On occasion, the statue is hooked up to a keg of beer. Cups will be filled up with the beer flowing from the statue and given out to people passing by.
The statue has been stolen seven times, the last time in January 1963, by students of the Antwerp student association "De Wikings" of the Sint-Ignatius Handelshogeschool (Higher Business Education), now part of the Antwerp University, who "hijacked" Manneken Pis for five days before handing it over to the Antwerp authorities. The local and international press covered the story, contributing to the students' collection of funds donated to two orphanages.
There is also a statue of Manneken Pis in Tokushima, Japan, which was a present from the Belgian embassy (Tokushima being twinned with Brussels).
Since 1987, the Manneken has had a female equivalent, Jeanneke Pis, located on the east side of the "Impasse de la Fidélité/Getrouwheidsgang".
Replicas.
Although the "Manneken Pis" in Brussels is the best-known, others exist. There is an ongoing dispute over which Manneken Pis is the oldest - the one in Brussels or the one in Geraardsbergen. Similar statues can also be found in the Belgian cities of Koksijde, Hasselt, Ghent, Bruges, in the town of Braine-l'Alleud (where it is called "Il Gamin Quipiche"), and in the French Flemish village of Broxeele, a town with the same etymology as "Brussels".
In Bali, Indonesia, there is a Belgian restaurant called Mannekepis. It even has the exact replica of the statue standing in front of the restaurant, urinating.
In many countries, replicas in brass or fiberglass are commonplace swimming or garden-pool decorations. Many copies exist worldwide as garden ornaments. Manneken Pis has also been adapted into such risqué souvenir items as ashtrays and corkscrews.
In September 2002, a Belgian-born waffle-maker in Florida, named Assayag, set up a replica in front of his waffle stand in the Orlando Fashion Square mall in Orlando, Florida. He recalled the legend as 'the boy who saved Brussels from fire by extinguishing it with his urine' (confusing the legend with an incident in "Gulliver's Travels" perhaps). Some shocked shoppers made a formal complaint. Mall officials said that the waffle-shop owner did not follow procedures when he put up the statue and was therefore in violation of his lease.
In contrast, there is a similar statue in Rio de Janeiro in front of the quarters of Botafogo de Futebol e Regatas, a famous football club from Brazil. There, the presence of the statue is taken lightly, and it has even been adopted as a mascot by the club. Fans usually dress it with the club's jersey after important wins.
A working replica of Manneken Pis stands on the platform of Hamamatsuchō Station in Tokyo, Japan. The statue is a great source of pride for station workers who dress it in various costumes—traditional and otherwise—at different times of year.
In popular culture.
A promotional expansion for the board game "7 Wonders" allows a player to build an eighth wonder of the world: Manneken-Pis.
"Manneken Pis" is also the name of a book by Vladimir Radunsky.
"The Party (film)" starring Peter Sellers, includes a reproduction of the statue in the house's extended water feature. The statue's peeing can be changed at an extended intercom panel, and Sellers as Hrundi V. Bakshi, soaks a guest when he hits the wrong button.
In the 1986 film "The Money Pit", the lead character, Walter Fielding, played by Tom Hanks, accidentally falls in a construction area where workers are renovating his home. In a sequence reminiscent of a Rube Goldberg machine, he stumbles through a window, across the roof, down a scaffold, finally into a wheeled bin in which he rolls down a hill and is dumped into a fountain resting directly under a replica of the Manneken Pis.

</doc>
<doc id="56103" url="http://en.wikipedia.org/wiki?curid=56103" title="Stevie Smith">
Stevie Smith

Florence Margaret Smith, known as Stevie Smith (20 September 1902 – 7 March 1971) was an English poet and novelist.
Life.
Stevie Smith, born Florence Margaret Smith in Kingston upon Hull, was the second daughter of Ethel and Charles Smith. She was called "Peggy" within her family, but acquired the name "Stevie" as a young woman when she was riding in the park with a friend who said that she reminded him of the jockey Steve Donaghue. Her father was a shipping agent, a business that he had inherited from his father. As the company and his marriage began to fall apart, he ran away to sea and Smith saw very little of her father after that. He appeared occasionally on 24-hour shore leave and sent very brief postcards ("Off to Valparaiso, Love Daddy"). When she was three years old she moved with her mother and sister to Palmers Green in North London where Smith would live until her death in 1971. She resented the fact that her father had abandoned his family. Later, when her mother became ill, her aunt Madge Spear (whom Smith called "The Lion Aunt") came to live with them, raised Smith and her elder sister Molly and became the most important person in Smith's life. Spear was a feminist who claimed to have "no patience" with men and, as Smith wrote, "she also had 'no patience' with Hitler". Smith and Molly were raised without men and thus became attached to their own independence, in contrast to what Smith described as the typical Victorian family atmosphere of "father knows best". When Smith was five she developed tubercular peritonitis and was sent to a sanatorium near Broadstairs, Kent, where she remained for three years. She related that her preoccupation with death began when she was seven, at a time when she was very distressed at being sent away from her mother. Death and fear fascinated her and provide the subjects of many of her poems. When suffering from the depression to which she was subject all her life she was so consoled by the thought of death as a release that, as she put it, she did not have to commit suicide. She wrote in several poems that death was "the only god who must come when he is called". Smith suffered throughout her life from an acute nervousness, described as a mix of shyness and intense sensitivity. Her mother died when Smith was 16.
In the Poem "A House of Mercy", she wrote of her childhood house in North London:
<poem>
It was a house of female habitation,
Two ladies fair inhabited the house,
And they were brave. For although Fear knocked loud
Upon the door, and said he must come in,
They did not let him in.
</poem>
Smith was educated at Palmers Green High School and North London Collegiate School for Girls. She spent the remainder of her life with her aunt, and worked as private secretary to Sir Neville Pearson with Sir George Newnes at Newnes Publishing Company in London from 1923 to 1953. Despite her secluded life, she corresponded and socialised widely with other writers and creative artists, including Elisabeth Lutyens, Sally Chilver, Inez Holden, Naomi Mitchison, Isobel English and Anna Kallin. After she retired from Sir Neville Pearson's service following a nervous breakdown she gave poetry readings and broadcasts on the BBC that gained her new friends and readers among a younger generation. Sylvia Plath became a fan of her poetry, "a desperate Smith-addict", and made an appointment to meet her but killed herself before the meeting could occur.
Smith died of a brain tumour on 7 March 1971. Her last collection, "Scorpion and other Poems" was published posthumously in 1972, and the "Collected Poems" followed in 1975. Three novels were republished and there was a successful play based on her life, "Stevie", written by Hugh Whitemore. It was filmed in 1978 by Robert Enders and starred Glenda Jackson and Mona Washbourne.
She was described by her friends as being naive and selfish in some ways and formidably intelligent in others, having been raised by her aunt as both a spoiled child and a resolutely autonomous woman. Likewise, her political views vacillated between her aunt's Toryism and her friends' left-wing tendencies. Smith was celibate for most of her life, although she rejected the idea that she was lonely as a result, alleging that she had a number of intimate relationships with friends and family that kept her fulfilled. She never entirely abandoned or accepted the Anglican faith of her childhood, describing herself as a "lapsed atheist", and wrote sensitively about theological puzzles;"There is a God in whom I do not believe/Yet to this God my love stretches." Her 14-page essay of 1958, "The Necessity of Not Believing", concludes: "There is no reason to be sad, as some people are sad when they feel religion slipping off from them. There is no reason to be sad, it is a good thing."
Career.
Smith wrote three novels, the first of which, "Novel on Yellow Paper", was published in 1936. All her novels are lightly fictionalised accounts of her own life, which got her into trouble at times as people recognised themselves. Smith said that two of the male characters in her last book are different aspects of George Orwell, who was close to Smith (there were even rumours that they were lovers; he was married to his first wife at the time).
Apart from death, common subjects include loneliness; myth and legend; absurd vignettes, usually drawn from middle-class British life, war, human cruelty and religion. Though her poems were remarkably consistent in tone and quality throughout her life, their subject matter changed over time, with less of the outrageous wit of her youth and more reflection on suffering, faith and the end of life. Her best-known poem is "Not Waving but Drowning". She was awarded the Cholmondeley Award for Poets in 1966 and won the Queen's Gold Medal for poetry in 1969.
A good time was had by all.
She also wrote nine volumes of poetry. The first, "A Good Time Was Had By All", established her as a poet: soon her poems were found in periodicals. Her style was often very dark; her characters were perpetually saying "goodbye" to their friends or welcoming death. At the same time her work has an eerie levity and can be very funny though it is neither light nor whimsical. "Stevie Smith often uses the word 'peculiar' and it is the best word to describe her effects" (Hermione Lee). She was never sentimental, undercutting any pathetic effects with the ruthless honesty of her humour.
The title of her first poetry book, "A Good Time was Had By All", itself became a catch phrase, still occasionally used to this day. Smith said she got the phrase from parish magazines, where descriptions of church picnics often included this phrase.
This saying has become so familiar that it is recognised even by those who are unaware of its origin. Variations appear in pop culture, including Being for the Benefit of Mr Kite by the Beatles.
Fiction.
"Novel on Yellow Paper" (Cape, 1936).
Smith's first novel is structured as the random typings of a bored secretary, Pompey. She plays word games, retells stories from classical and popular culture, remembers events from her childhood, gossips about her friends and describes her family, particularly her beloved Aunt. As with all Smith's novels, there is an early scene where the heroine expresses feelings and beliefs which she will later feel significant, although ambiguous, regret for. In "Novel on Yellow Paper" that belief is anti-Semitism, where she feels elation at being the "only Goy" at a Jewish party. This apparently throwaway scene acts as a timebomb, which detonates at the centre of the novel when Pompey visits Germany as the Nazis are gaining power. With horror, she acknowledges the continuity between her feeling "Hurray for being a Goy" at the party and the madness that is overtaking Germany. The German scenes stand out in the novel, but perhaps equally powerful is her dissection of failed love. She describes two unsuccessful relationships, first with the German Karl and then with the suburban Freddy. The final section of the novel describes with unusual clarity the intense pain of her break-up with Freddy.
"Over the Frontier" (Cape, 1938).
Smith herself dismissed her second novel as a failed experiment, but its attempt to parody popular genre fiction to explore profound political issues now seems to anticipate post-modern fiction. If anti-Semitism was one of the key themes of "Novel on Yellow Paper", "Over the Frontier" is concerned with militarism. In particular, she asks how the necessity of fighting Fascism can be achieved without descending into the nationalism and dehumanisation that fascism represents. After a failed romance the heroine, Pompey, suffers a breakdown and is sent to Germany to recuperate. At this point the novel changes style radically, as Pompey becomes part of an adventure/spy yarn in the style of John Buchan or Dornford Yates. As the novel becomes increasingly dreamlike, Pompey crosses over the frontier to become a spy and soldier. If her initial motives are idealistic, she becomes seduced by the intrigue and, ultimately, violence. The vision Smith offers is a bleak one: "Power and cruelty are the strengths of our lives, and only in their weakness is there love."
"The Holiday" (Chapman and Hall, 1949).
Smith's final novel is her own favourite, and most fully realised. It is concerned with personal and political malaise in the immediate post-war period. Most of the characters are either employed in the army or civil service in post-war reconstruction, and its heroine, Celia, works for the Ministry as a cryptographer and propagandist. "The Holiday" describes a series of hopeless relationships. Celia and her cousin Caz are in love, but cannot pursue their affair since it is believed that, because of their parents' adultery, they are half-brother and sister. Celia's other cousin Tom is in love with her, Basil is love with Tom, Tom is estranged from his father, Celia's beloved Uncle Heber, who pines for a reconciliation; and Celia's best friend Tiny longs for the married Vera. These unhappy, futureless but intractable relationships are mirrored by the novel's political concerns. The unsustainability of the British Empire and the uncertainty over Britain's post-war role are constant themes, and many of the characters discuss their personal and political concerns as if they were seamlessly linked. Caz is on leave from Palestine and is deeply disillusioned, Tom goes mad during the war, and it is telling that the family scandal that blights Celia and Caz's lives took place in India. Just as Pompey's anti-semitism was tested in "Novel on Yellow Paper", so Celia's traditional nationalism and sentimental support for colonialism is challenged throughout "The Holiday".

</doc>
<doc id="56106" url="http://en.wikipedia.org/wiki?curid=56106" title="Wildfire">
Wildfire

A wildfire is an uncontrolled fire in an area of combustible vegetation that occurs in the countryside area Other names such as brush fire, bush fire, forest fire, desert fire, grass fire, hill fire, peat fire, vegetation fire, and veldfire may be used to describe the same phenomenon depending on the type of vegetation being burned, and the regional variant of English being used. A wildfire differs from other fires by its extensive size, the speed at which it can spread out from its original source, its potential to change direction unexpectedly, and its ability to jump gaps such as roads, rivers and fire breaks. Wildfires are characterized in terms of the cause of ignition, their physical properties such as speed of propagation, the combustible material present, and the effect of weather on the fire.
Bushfires in Australia are a common occurrence; because of the generally hot and dry climate, they pose a great risk to life and infrastructure during all times of the year, though mostly throughout the hotter months of summer and spring. In the United States, there are typically between 60,000 and 80,000 wildfires that occur each year, burning 3 million to 10 million acres (12,000 to 40,000 square kilometres) of land depending on the year. Fossil records and human history contain accounts of wildfires, as wildfires can occur in periodic intervals. Wildfires can cause extensive damage, both to property and human life, but they also have various beneficial effects on wilderness areas. Some plant species depend on the effects of fire for growth and reproduction, although large wildfires may also have negative ecological effects.
Strategies of wildfire prevention, detection, and suppression have varied over the years, and international wildfire management experts encourage further development of technology and research. One of the more controversial techniques is controlled burning: permitting or even igniting smaller fires to minimize the amount of flammable material available for a potential wildfire. While some wildfires burn in remote forested regions, they can cause extensive destruction of homes and other property located in the wildland-urban interface: a zone of transition between developed areas and undeveloped wilderness.
The name "wildfire" was once a synonym for Greek fire but now refers to any large or destructive conflagration. Wildfires differ from other fires in that they take place outdoors in areas of grassland, woodlands, bushland, scrubland, peatland, and other wooded areas that act as a source of fuel, or combustible material. Buildings may become involved if a wildfire spreads to adjacent communities. While the causes of wildfires vary and the outcomes are always unique, all wildfires can be characterized in terms of their physical properties, their fuel type, and the effect that weather has on the fire.
Wildfire behaviour and severity result from the combination of factors such as available fuels, physical setting, and weather. While wildfires can be large, uncontrolled disasters that burn through 0.4 to or more, they can also be as small as 0.001 km2 or less. Although smaller events may be included in wildfire modeling, most do not earn press attention. This can be problematic because public fire policies, which relate to fires of all sizes, are influenced more by the way the media portrays catastrophic wildfires than by small fires.
Causes.
Wildfires are 'quasi-natural' hazards, meaning that they are not entirely natural features (like volcanoes, earthquakes and tropical storms). This is because they are caused by human activity as well. The four major natural causes of wildfire ignitions are lightning, volcanic eruption, sparks from rockfalls, and spontaneous combustion. The thousands of coal seam fires that are burning around the world, such as those in Centralia, Burning Mountain, and several coal-sustained fires in China, can also flare up and ignite nearby flammable material. The most common human sources of wildfires are arson, discarded cigarettes, sparks from equipment, and power line arcs (as detected by arc mapping). Ignition of wildland fires via contact with hot rifle bullet fragments is possible under the right conditions. In societies experiencing shifting cultivation where land is cleared quickly and farmed until the soil loses fertility, slash and burn clearing is often considered the least expensive way to prepare land for future use. Forested areas cleared by logging encourage the dominance of flammable grasses, and abandoned logging roads overgrown by vegetation may act as fire corridors. Annual grassland fires in southern Vietnam can be attributed in part to the destruction of forested areas by US military herbicides, explosives, and mechanical land clearing and burning operations during the Vietnam War.
The most common cause of wildfires varies throughout the world. In Canada and northwest China, for example, lightning is the major source of ignition. In other parts of the world, human involvement is a major contributor. In Mexico, Central America, South America, Africa, Southeast Asia, Fiji, and New Zealand, wildfires can be attributed to human activities such as animal husbandry, agriculture, and land-conversion burning. Human carelessness is a major cause of wildfires in China and in the Mediterranean Basin. In the United States and Australia, the source of wildfires can be traced to both lightning strikes and human activities such as machinery sparks and cast-away cigarette butts."
On a yearly basis in the United States, typically more than six times the number of wildfires are caused by human means such as campfires and controlled agricultural burns than by natural means. However, in any given year there could be far more acres burned by wildfires that are started by natural means than by human means as well as vice versa. For example, in 2010, almost 1.4 million acres were burned by human-caused wildfires, and over 2 million acres were burned by naturally-caused wildfires. However, far more acres were burned by human-caused fires in 2011, when almost 5.4 million acres were burned by human-caused wildfires, and only about 3.4 million acres were caused by naturally-derived wildfires.
Fuel type.
The spread of wildfires varies based on the flammable material present and its vertical arrangement. For example, fuels uphill from a fire are more readily dried and warmed by the fire than those downhill, yet burning logs can roll downhill from the fire to ignite other fuels. Fuel arrangement and density is governed in part by topography, as land shape determines factors such as available sunlight and water for plant growth. Overall, fire types can be generally characterized by their fuels as follows:
Physical properties.
Wildfires occur when all of the necessary elements of a fire triangle come together in a susceptible area: an ignition source is brought into contact with a combustible material such as vegetation, that is subjected to sufficient heat and has an adequate supply of oxygen from the ambient air. A high moisture content usually prevents ignition and slows propagation, because higher temperatures are required to evaporate any water within the material and heat the material to its fire point. Dense forests usually provide more shade, resulting in lower ambient temperatures and greater humidity, and are therefore less susceptible to wildfires. Less dense material such as grasses and leaves are easier to ignite because they contain less water than denser material such as branches and trunks. Plants continuously lose water by evapotranspiration, but water loss is usually balanced by water absorbed from the soil, humidity, or rain. When this balance is not maintained, plants dry out and are therefore more flammable, often a consequence of droughts.
A wildfire "front" is the portion sustaining continuous flaming combustion, where unburned material meets active flames, or the smoldering transition between unburned and burned material. As the front approaches, the fire heats both the surrounding air and woody material through convection and thermal radiation. First, wood is dried as water is vaporized at a temperature of 100 C. Next, the pyrolysis of wood at 230 C releases flammable gases. Finally, wood can smoulder at 380 C or, when heated sufficiently, ignite at 590 C. Even before the flames of a wildfire arrive at a particular location, heat transfer from the wildfire front warms the air to 800 C, which pre-heats and dries flammable materials, causing materials to ignite faster and allowing the fire to spread faster. High-temperature and long-duration surface wildfires may encourage flashover or "torching": the drying of tree canopies and their subsequent ignition from below. 
Wildfires have a rapid "forward rate of spread" (FROS) when burning through dense, uninterrupted fuels. They can move as fast as 10.8 km/h in forests and 22 km/h in grasslands. Wildfires can advance tangential to the main front to form a "flanking" front, or burn in the opposite direction of the main front by "backing". They may also spread by "jumping" or "spotting" as winds and vertical convection columns carry "firebrands" (hot wood embers) and other burning materials through the air over roads, rivers, and other barriers that may otherwise act as firebreaks. Torching and fires in tree canopies encourage spotting, and dry ground fuels that surround a wildfire are especially vulnerable to ignition from firebrands. Spotting can create "spot fires" as hot embers and firebrands ignite fuels downwind from the fire. In Australian bushfires, spot fires are known to occur as far as 20 km from the fire front.
Especially large wildfires may affect air currents in their immediate vicinities by the stack effect: air rises as it is heated, and large wildfires create powerful updrafts that will draw in new, cooler air from surrounding areas in thermal columns. Great vertical differences in temperature and humidity encourage pyrocumulus clouds, strong winds, and fire whirls with the force of tornadoes at speeds of more than 80 km/h. Rapid rates of spread, prolific crowning or spotting, the presence of fire whirls, and strong convection columns signify extreme conditions.
The thermal heat from wildfire can cause significant weathering of rocks and boulders, heat can rapidly expand a boulder and thermal shock can occur, which may result in an object's structure to fail.
Effect of weather.
Heat waves, droughts, cyclical climate changes such as El Niño, and regional weather patterns such as high-pressure ridges can increase the risk and alter the behavior of wildfires dramatically. Years of precipitation followed by warm periods can encourage more widespread fires and longer fire seasons. Since the mid-1980s, earlier snowmelt and associated warming has also been associated with an increase in length and severity of the wildfire season in the Western United States. However, one individual element does not always cause an increase in wildfire activity. For example, wildfires will not occur during a drought unless accompanied by other factors, such as lightning (ignition source) and strong winds (mechanism for rapid spread).
Intensity also increases during daytime hours. Burn rates of smoldering logs are up to five times greater during the day due to lower humidity, increased temperatures, and increased wind speeds. Sunlight warms the ground during the day which creates air currents that travel uphill. At night the land cools, creating air currents that travel downhill. Wildfires are fanned by these winds and often follow the air currents over hills and through valleys. Fires in Europe occur frequently during the hours of 12:00 p.m. and 2:00 p.m. Wildfire suppression operations in the United States revolve around a 24-hour "fire day" that begins at 10:00 a.m. due to the predictable increase in intensity resulting from the daytime warmth.
Ecology.
Wildfires are common in climates that are sufficiently moist to allow the growth of vegetation but feature extended dry, hot periods. Such places include the vegetated areas of Australia and Southeast Asia, the veld in southern Africa, the fynbos in the Western Cape of South Africa, the forested areas of the United States and Canada, and the Mediterranean Basin. Fires can be particularly intense during days of strong winds, periods of drought, and during warm summer months. Global warming may increase the intensity and frequency of droughts in many areas, creating more intense and frequent wildfires.
Although some ecosystems rely on naturally occurring fires to regulate growth, many ecosystems suffer from too much fire, such as the chaparral in southern California and lower elevation deserts in the American Southwest. The increased fire frequency in these ordinarily fire-dependent areas has upset natural cycles, destroyed native plant communities, and encouraged the growth of fire-intolerant vegetation and non-native weeds. Invasive species, such as "Lygodium microphyllum" and "Bromus tectorum", can grow rapidly in areas that were damaged by fires. Because they are highly flammable, they can increase the future risk of fire, creating a positive feedback loop that increases fire frequency and further destroys native growth.
In the Amazon Rainforest, drought, logging, cattle ranching practices, and slash-and-burn agriculture damage fire-resistant forests and promote the growth of flammable brush, creating a cycle that encourages more burning. Fires in the rainforest threaten its collection of diverse species and produce large amounts of CO2. Also, fires in the rainforest, along with drought and human involvement, could damage or destroy more than half of the Amazon rainforest by the year 2030. Wildfires generate ash, destroy available organic nutrients, and cause an increase in water runoff, eroding away other nutrients and creating flash flood conditions. A 2003 wildfire in the North Yorkshire Moors destroyed 2.5 km2 of heather and the underlying peat layers. Afterwards, wind erosion stripped the ash and the exposed soil, revealing archaeological remains dating back to 10,000 BC. Wildfires can also have an effect on climate change, increasing the amount of carbon released into the atmosphere and inhibiting vegetation growth, which affects overall carbon uptake by plants.
In tundra there is a natural pattern of accumulation of fuel and wildfire which varies depending on the nature of vegetation and terrain. Research in Alaska has shown fire-event return intervals, (FRIs) that typically vary from 150 to 200 years with dryer lowland areas burning more frequently than wetter upland areas.
Plant adaptation.
Plants in wildfire-prone ecosystems often survive through adaptations to their local fire regime. Such adaptations include physical protection against heat, increased growth after a fire event, and flammable materials that encourage fire and may eliminate competition. For example, plants of the genus "Eucalyptus" contain flammable oils that encourage fire and hard sclerophyll leaves to resist heat and drought, ensuring their dominance over less fire-tolerant species. Dense bark, shedding lower branches, and high water content in external structures may also protect trees from rising temperatures. Fire-resistant seeds and reserve shoots that sprout after a fire encourage species preservation, as embodied by pioneer species. Smoke, charred wood, and heat can stimulate the germination of seeds in a process called "serotiny". Exposure to smoke from burning plants promotes germination in other types of plants by inducing the production of the orange butenolide.
Grasslands in Western Sabah, Malaysian pine forests, and Indonesian "Casuarina" forests are believed to have resulted from previous periods of fire. Chamise deadwood litter is low in water content and flammable, and the shrub quickly sprouts after a fire. Cape lilies lie dormant until flames brush away the covering, then blossom almost overnight. Sequoia rely on periodic fires to reduce competition, release seeds from their cones, and clear the soil and canopy for new growth. Caribbean Pine in Bahamian pineyards have adapted to and rely on low-intensity, surface fires for survival and growth. An optimum fire frequency for growth is every 3 to 10 years. Too frequent fires favor herbaceous plants, and infrequent fires favor species typical of Bahamian dry forests.
Atmospheric effects.
Most of the Earth's weather and air pollution resides in the troposphere, the part of the atmosphere that extends from the surface of the planet to a height of about 10 km. The vertical lift of a severe thunderstorm or pyrocumulonimbus can be enhanced in the area of a large wildfire, which can propel smoke, soot, and other particulate matter as high as the lower stratosphere. Previously, prevailing scientific theory held that most particles in the stratosphere came from volcanoes, but smoke and other wildfire emissions have been detected from the lower stratosphere. Pyrocumulus clouds can reach 6100 m over wildfires. Increased fire byproducts in the stratosphere can increase ozone concentration beyond safe levels. Satellite observation of smoke plumes from wildfires revealed that the plumes could be traced intact for distances exceeding 1600 km. Computer-aided models such as CALPUFF may help predict the size and direction of wildfire-generated smoke plumes by using atmospheric dispersion modeling.
Wildfires can affect climate and weather and have major impacts on atmospheric pollution. Wildfire emissions contain fine particulate matter which can cause cardiovascular and respiratory problems. Forest fires in Indonesia in 1997 were estimated to have released between 0.81 and 2.57 gigatonnes (0.89 and 2.83 billion short tons) of CO2 into the atmosphere, which is between 13%–40% of the annual global carbon dioxide emissions from burning fossil fuels. Atmospheric models suggest that these concentrations of sooty particles could increase absorption of incoming solar radiation during winter months by as much as 15%.
Smoke trail from a fire seen while looking towards Dargo from Swifts Creek, Victoria, Australia, 11 January 2007
History.
In the Welsh Borders, the first evidence of wildfire is rhyniophytoid plant fossils preserved as charcoal, dating to the Silurian period (about million years ago). Smoldering surface fires started to occur sometime before the Early Devonian period million years ago. Low atmospheric oxygen during the Middle and Late Devonian was accompanied by a decrease in charcoal abundance. Additional charcoal evidence suggests that fires continued through the Carboniferous period. Later, the overall increase of atmospheric oxygen from 13% in the Late Devonian to 30-31% by the Late Permian was accompanied by a more widespread distribution of wildfires. Later, a decrease in wildfire-related charcoal deposits from the late Permian to the Triassic periods is explained by a decrease in oxygen levels.
Wildfires during the Paleozoic and Mesozoic periods followed patterns similar to fires that occur in modern times. Surface fires driven by dry seasons are evident in Devonian and Carboniferous progymnosperm forests. Lepidodendron forests dating to the Carboniferous period have charred peaks, evidence of crown fires. In Jurassic gymnosperm forests, there is evidence of high frequency, light surface fires. The increase of fire activity in the late Tertiary is possibly due to the increase of C4-type grasses. As these grasses shifted to more mesic habitats, their high flammability increased fire frequency, promoting grasslands over woodlands. However, fire-prone habitats may have contributed to the prominence of trees such as those of the genus "Pinus", which have thick bark to withstand fires and employ serotiny.
Human involvement.
The human use of fire for agricultural and hunting purposes during the Paleolithic and Mesolithic ages altered the preexisting landscapes and fire regimes. Woodlands were gradually replaced by smaller vegetation that facilitated travel, hunting, seed-gathering and planting. In recorded human history, minor allusions to wildfires were mentioned in the Bible and by classical writers such as Homer. However, while ancient Hebrew, Greek, and Roman writers were aware of fires, they were not very interested in the uncultivated lands where wildfires occurred. Wildfires were used in battles throughout human history as early thermal weapons. From the Middle ages, accounts were written of occupational burning as well as customs and laws that governed the use of fire. In Germany, regular burning was documented in 1290 in the Odenwald and in 1344 in the Black Forest. In the 14th century Sardinia, firebreaks were used for wildfire protection. In Spain during the 1550s, sheep husbandry was discouraged in certain provinces by Philip II due to the harmful effects of fires used in transhumance. As early as the 17th century, Native Americans were observed using fire for many purposes including cultivation, signaling, and warfare. Scottish botanist David Douglas noted the native use of fire for tobacco cultivation, to encourage deer into smaller areas for hunting purposes, and to improve foraging for honey and grasshoppers. Charcoal found in sedimentary deposits off the Pacific coast of Central America suggests that more burning occurred in the 50 years before the Spanish colonization of the Americas than after the colonization. In the post-World War II Baltic region, socio-economic changes led more stringent air quality standards and bans on fires that eliminated traditional burning practices.
Wildfires typically occurred during periods of increased temperature and drought. An increase in fire-related debris flow in alluvial fans of northeastern Yellowstone National Park was linked to the period between AD 1050 and 1200, coinciding with the Medieval Warm Period. However, human influence caused an increase in fire frequency. Dendrochronological fire scar data and charcoal layer data in Finland suggests that, while many fires occurred during severe drought conditions, an increase in the number of fires during 850 BC and 1660 AD can be attributed to human influence. Charcoal evidence from the Americas suggested a general decrease in wildfires between 1 AD and 1750 compared to previous years. However, a period of increased fire frequency between 1750 and 1870 was suggested by charcoal data from North America and Asia, attributed to human population growth and influences such as land clearing practices. This period was followed by an overall decrease in burning in the 20th century, linked to the expansion of agriculture, increased livestock grazing, and fire prevention efforts. A meta-analysis found that 17 times as much land burned annually in California before 1800 compared to today (1,800,000 hectares/year compared to 102,000 hectares/year).
Prevention.
Wildfire prevention refers to the preemptive methods of reducing the risk of fires as well as lessening its severity and spread. Effective prevention techniques allow supervising agencies to manage air quality, maintain ecological balances, protect resources, and to limit the effects of future uncontrolled fires. North American firefighting policies may permit naturally caused fires to burn to maintain their ecological role, so long as the risks of escape into high-value areas are mitigated. However, prevention policies must consider the role that humans play in wildfires, since, for example, 95% of forest fires in Europe are related to human involvement. Sources of human-caused fire may include arson, accidental ignition, or the uncontrolled use of fire in land-clearing and agriculture such as the slash-and-burn farming in Southeast Asia.
In the mid-19th century, explorers from the HMS "Beagle" observed Australian Aborigines using fire for ground clearing, hunting, and regeneration of plant food in a method later named fire-stick farming. Such careful use of fire has been employed for centuries in the lands protected by Kakadu National Park to encourage biodiversity. In 1937, U.S. President Franklin D. Roosevelt initiated a nationwide fire prevention campaign, highlighting the role of human carelessness in forest fires. Later posters of the program featured Uncle Sam, leaders of the Axis powers of World War II, characters from the Disney movie "Bambi", and the official mascot of the U.S. Forest Service, Smokey Bear.
Wildfires are caused by a combination of natural factors such as topography, fuels, and weather. Other than reducing human infractions, only fuels may be altered to affect future fire risk and behavior. Wildfire prevention programs around the world may employ techniques such as "wildland fire use" and "prescribed or controlled burns". "Wildland fire use" refers to any fire of natural causes that is monitored but allowed to burn. "Controlled burns" are fires ignited by government agencies under less dangerous weather conditions.
Vegetation may be burned periodically to maintain high species diversity, and frequent burning of surface fuels limits fuel accumulation, thereby reducing the risk of crown fires. Using strategic cuts of trees, fuels may also be removed by handcrews in order to clean and clear the forest, prevent fuel build-up, and create access into forested areas. Chain saws and large equipment can be used to thin out ladder fuels and shred trees and vegetation to a mulch. Multiple fuel treatments are often needed to influence future fire risks, and wildfire models may be used to predict and compare the benefits of different fuel treatments on future wildfire spread.
However, controlled burns are reportedly "the most effective treatment for reducing a fire’s rate of spread, fireline intensity, flame length, and heat per unit of area" according to Jan Van Wagtendonk, a biologist at the Yellowstone Field Station. Additionally, while fuel treatments are typically limited to smaller areas, effective fire management requires the administration of fuels across large landscapes in order to reduce future fire size and severity.
Building codes in fire-prone areas typically require that structures be built of flame-resistant materials and a defensible space be maintained by clearing flammable materials within a prescribed distance from the structure. Communities in the Philippines also maintain fire lines 5 to wide between the forest and their village, and patrol these lines during summer months or seasons of dry weather. Fuel buildup can result in costly, devastating fires as new homes, ranches, and other development are built adjacent to wilderness areas. Continued growth in fire-prone areas and rebuilding structures destroyed by fires has been met with criticism.
However, the population growth along the wildland-urban interface discourages the use of current fuel management techniques. Smoke is an irritant and attempts to thin out the fuel load is met with opposition due to desirability of forested areas, in addition to other wilderness goals such as endangered species protection and habitat preservation. The ecological benefits of fire are often overridden by the economic and safety benefits of protecting structures and human life. For example, while fuel treatments decrease the risk of crown fires, these techniques destroy the habitats of various plant and animal species. Additionally, government policies that cover the wilderness usually differ from local and state policies that govern urban lands.
A ponderosa pine stand in the Bitterroot National Forest in Montana in 1909, 1948, and 1989. The increase in vegetation density was attributed to fire prevention efforts since 1895.
Policy.
History of wildfire policy in the U.S..
Since the turn of the 20th century, various federal and state agencies have been involved in wildland fire management in one form or another. In the early 20th century, for example, the federal government, through the U.S. Army and the U.S. Forest Service, solicited fire suppression as a primary goal of managing the nation’s forests. At this time in history fire was viewed as a threat to timber, an economically important natural resource. As such, rational decisions were made to devote public funds to fire suppression and fire prevention efforts. For example, the Forest Fire Emergency Fund Act of 1908 permitted deficit spending in the case of emergency fire situations. As a result, the U.S. Forest Service was able to acquire a deficit of over $1 million in 1910 due to emergency fire suppression efforts. Following the same tone of timber resource protection, the U.S. Forest Service adopted the “10 AM Policy” in 1935. Through this policy the agency advocated the control of all fires by 10 o’clock of the morning following the discovery of a wildfire. Fire prevention was also heavily advocated through public education campaigns such as Smokey the Bear. Through these and similar public education campaigns the general public was, in a sense, trained to perceive all wildfire as a threat to civilized society and natural resources. The negative sentiment towards wildland fire prevailed and helped to shape wildland fire management objectives throughout most of the 20th century.
Beginning in the 1970s public perception of wildland fire management began to shift. Despite portly funding for fire suppression in the first half of the 20th century, massive wildfires continued to be prevalent across the landscape of North America. Natural resource professionals and ordinary citizens alike became curious about the ecological effects of wildfire. Ecologists were beginning to recognize the presence and ecological importance of natural lightning-ignited wildfires across the United States. Along with this new discovery of fire knowledge and the emergence of fire ecology as a science came an effort to apply fire to land in a controlled manner. It was learned that suppression of fire in certain ecosystems actually increases the likelihood that a wildfire will occur and increases the intensity of those wildfires. This was in fact happening across the United States. However, suppression is still the main tactic when a fire is set by a human or if it threatens life or property.
By the 1980s funding efforts began to support prescribed burning. In light of emerging information about wildland fire, rational thought justified funding prescribed burning in order to prevent catastrophic wildfire events. In 2001, the United States Government implemented a National Fire Plan and the budget increased from $108 million in 2000 to $401 million for the reduction of hazardous fuels. In this way, the costs of implementing prescribed burns were thought to be less than the costs imposed on society by catastrophic wildfires. In addition to using prescribed fire to reduce the chance of catastrophic wildfires, mechanical methods have recently been adopted as well. Mechanical methods include the use of chippers and other machinery to remove hazardous fuels and thereby reduce the risk of wildfire events. Today the United States philosophy remains that, “fire, as a critical natural process, will be integrated into land and resource management plans and activities on a landscape scale, and across agency boundaries. Response to wildfire is based on ecological, social and legal consequences of fire. The circumstance under which a fire occurs, and the likely consequences and public safety and welfare, natural and cultural resources, and values to be protected dictate the appropriate management response to fire” (United States Department of Agriculture Guidance for Implementation of Federal Wildland Fire Management Policy, 13 February 2009). The five federal regulatory agencies managing forest fire response and planning for 676 million acres in the United States are the Department of the Interior, the Bureau of Land Management, the Bureau of Indian Affairs, the National Park Service, the United States Department of Agriculture-Forest Service and the United States Fish and Wildlife Services. Several hundred million U.S. acres of wildfire management are also conducted by state, county, and local fire management organizations. In 2014, legislators proposed The Wildfire Disaster Funding Act to provide $2.7 billion fund appropriated by congress for the USDA and Department of Interior to use in fire suppression. The bill is a reaction to United States Forest Service and Department of Interior costs of Western Wildfire suppression appending that amounted to $3.5 billion in 2013.
The Condition Class System.
The Condition Class System is used in the United States to provide “national-level data on the current condition of fuel and vegetation.” The USDA Forest Service developed this for the purpose of allocating fire funding and resources, prioritizing fuel usage and restoration activities, and evaluating wildfire management progress. There are primary and secondary determinants used to rank forest systems into condition class and fire regimes. Condition Class “indicates the departure from normal fire return intervals” and is categorized as low, medium, or high. The more a fire departs from normal pattern, the higher is its condition class. A fire regime is the “historical pattern of fire in forests” and the Roman numerals I, II, III, IV and V are used for the classification. Primary determinants are the structure of the forest, the amount of trees, tree density and the characteristics of the combustible fuel. The United States Department of Agriculture and the United States Department of Interior use the Condition Class System in the LANDFIRE project to make assessments of federal land. However, the LANDFIRE project revealed in 2003 that this type of analysis is not detailed enough to use at a local level. Federal agencies are required to take record and report "acres treated", using different prevention tactics, under the National Fire Plan Operations Reporting System (NFPORS).
Wildland-urban interface policy.
An aspect of wildfire policy that is gaining attention is the wildland-urban interface (WUI). More and more people are living in “red zones,” or areas that are at high risk of wildfires. FEMA and the NFPA develop specific policies to guide homeowners and builders in how to build and maintain structures at the WUI and how protect against catastrophic losses. For example, NFPA-1141 is a standard for fire protection infrastructure for land development in wildland, rural and suburban areas and NFPA-1144 is a standard for reducing structure ignition hazards from wildland fire. For a full list of these policies and guidelines, see http://www.nfpa.org/categoryList.asp?categoryID=124&URL=Codes%20&%20Standards. Compensation for losses in the WUI are typically negotiated on an incident-by-incident basis. This is generating discussion about the burden of responsibility for funding and fighting a fire in the WUI, in that, if a resident chooses to live in a known red zone, should he or she retain a higher level of responsibility for funding home protection against wildfires.
Economics of fire management policy.
Similar to that of military operations, fire management is often very expensive in the U.S. Today, it is not uncommon for suppression operations for a single wildfire to exceed costs of $1 million in just a few days. The United States Department of Agriculture allotted $2.2 billion for wildfire management in 2012. Although fire suppression offers many benefits to society, other options for fire management exist. While these options cannot completely replace fire suppression as a fire management tool, other options can play an important role in overall fire management and can therefore affect the costs of fire suppression.
The application of fire management tools requires making certain tradeoffs. Below is a sample of some costs and benefits associated with the tools currently used in fire management. Current approaches to fire management are an almost complete turnaround compared to historic approaches. In fact, it is commonly accepted that past fire suppression, along with other factors, has resulted in larger, more intense wildfire events which are seen today. In economic terms, expenditures used for wildfire suppression in the early 20th century have contributed to increased suppression costs which are being realized today. As is the case with many public policy issues, costs and benefits associated with particular fire management tools are difficult to accurately quantify. Ultimately, costs and benefits should be weighed against one another on a case-by-case basis in planning wildland fire management operations.
Depending on the tradeoffs that a land manager is willing to make, a combination of the following fire management tools could be used. For instance, prescribed fire and/or mechanical fuels reduction could be used to help prevent or lessen the intensity of a wildfire thereby reducing or eliminating suppression costs. In addition, prescribed fire and/or mechanical fuels reduction could be used to improve soil conditions in fields or in forests to the benefit of wildlife or natural resources. On the other hand, the use of prescribed fire requires much advanced planning and can have negative impacts on human health in nearby communities.
Costs and Benefits of Wildland Fire Management Tools
Detection.
Fast and effective detection is a key factor in wildfire fighting. Early detection efforts were focused on early response, accurate results in both daytime and nighttime, and the ability to prioritize fire danger. Fire lookout towers were used in the United States in the early 20th century and fires were reported using telephones, carrier pigeons, and heliographs. Aerial and land photography using instant cameras were used in the 1950s until infrared scanning was developed for fire detection in the 1960s. However, information analysis and delivery was often delayed by limitations in communication technology. Early satellite-derived fire analyses were hand-drawn on maps at a remote site and sent via overnight mail to the fire manager. During the Yellowstone fires of 1988, a data station was established in West Yellowstone, permitting the delivery of satellite-based fire information in approximately four hours.
Currently, public hotlines, fire lookouts in towers, and ground and aerial patrols can be used as a means of early detection of forest fires. However, accurate human observation may be limited by operator fatigue, time of day, time of year, and geographic location. Electronic systems have gained popularity in recent years as a possible resolution to human operator error. A government report on a recent trial of three automated camera fire detection systems in Australia did, however, conclude "...detection by the camera systems was slower and less reliable than by a trained human observer". These systems may be semi- or fully automated and employ systems based on the risk area and degree of human presence, as suggested by GIS data analyses. An integrated approach of multiple systems can be used to merge satellite data, aerial imagery, and personnel position via Global Positioning System (GPS) into a collective whole for near-realtime use by wireless Incident Command Centers.
A small, high risk area that features thick vegetation, a strong human presence, or is close to a critical urban area can be monitored using a local sensor network. Detection systems may include wireless sensor networks that act as automated weather systems: detecting temperature, humidity, and smoke. These may be battery-powered, solar-powered, or "tree-rechargeable": able to recharge their battery systems using the small electrical currents in plant material. Larger, medium-risk areas can be monitored by scanning towers that incorporate fixed cameras and sensors to detect smoke or additional factors such as the infrared signature of carbon dioxide produced by fires. Additional capabilities such as night vision, brightness detection, and color change detection may also be incorporated into sensor arrays.
Satellite and aerial monitoring through the use of planes, helicopter, or UAVs can provide a wider view and may be sufficient to monitor very large, low risk areas. These more sophisticated systems employ GPS and aircraft-mounted infrared or high-resolution visible cameras to identify and target wildfires. Satellite-mounted sensors such as Envisat's Advanced Along Track Scanning Radiometer and European Remote-Sensing Satellite's Along-Track Scanning Radiometer can measure infrared radiation emitted by fires, identifying hot spots greater than 39 C. The National Oceanic and Atmospheric Administration's Hazard Mapping System combines remote-sensing data from satellite sources such as Geostationary Operational Environmental Satellite (GOES), Moderate-Resolution Imaging Spectroradiometer (MODIS), and Advanced Very High Resolution Radiometer (AVHRR) for detection of fire and smoke plume locations. However, satellite detection is prone to offset errors, anywhere from 2 to for MODIS and AVHRR data and up to 12 km for GOES data. Satellites in geostationary orbits may become disabled, and satellites in polar orbits are often limited by their short window of observation time. Cloud cover and image resolution and may also limit the effectiveness of satellite imagery.
Suppression.
Wildfire suppression depends on the technologies available in the area in which the wildfire occurs. In less developed nations the techniques used can be as simple as throwing sand or beating the fire with sticks or palm fronds. In more advanced nations, the suppression methods vary due to increased technological capacity. Silver iodide can be used to encourage snow fall, while fire retardants and water can be dropped onto fires by unmanned aerial vehicles, planes, and helicopters. Complete fire suppression is no longer an expectation, but the majority of wildfires are often extinguished before they grow out of control. While more than 99% of the 10,000 new wildfires each year are contained, escaped wildfires can cause extensive damage. Worldwide damage from wildfires is in the billions of euros annually. Wildfires in Canada and the US burn an average of 54500 km2 per year.
Above all, fighting wildfires can become deadly. A wildfire's burning front may also change direction unexpectedly and jump across fire breaks. Intense heat and smoke can lead to disorientation and loss of appreciation of the direction of the fire, which can make fires particularly dangerous. For example, during the 1949 Mann Gulch fire in Montana, USA, thirteen smokejumpers died when they lost their communication links, became disoriented, and were overtaken by the fire. In the Australian February 2009 Victorian bushfires, at least 173 people died and over 2,029 homes and 3,500 structures were lost when they became engulfed by wildfire.
Wildland firefighting safety.
Wildland fire fighters face several life-threatening hazards including heat stress, fatigue, smoke and dust, as well as the risk of other injuries such as burns, cuts and scrapes, animal bites, and even rhabdomyolysis.
Especially in hot weather condition, fires present the risk of heat stress, which can entail feeling heat, fatigue, weakness, vertigo, headache, or nausea. Heat stress can progress into heat strain, which entails physiological changes such as increased heart rate and core body temperature. This can lead to heat-related illnesses, such as heat rash, cramps, exhaustion or heat stroke. Various factors can contribute to the risks posed by heat stress, including strenuous work, personal risk factors such as age and fitness, dehydration, sleep deprivation, and burdensome personal protective equipment. Rest, cool water, and occasional breaks are crucial to mitigating the effects of heat stress.
Smoke, ash, and debris can also pose serious respiratory hazards to wildland fire fighters. The smoke and dust from wildfires can contain gases such as carbon monoxide, sulfur dioxide and formaldehyde, as well as particulates such as ash and silica. To reduce smoke exposure, wildfire fighting crews should, whenever possible, rotate firefighters through areas of heavy smoke, avoid downwind firefighting, use equipment rather than people in holding areas, and minimize mop-up. Camps and command posts should also be located upwind of wildfires. Protective clothing and equipment can also help minimize exposure to smoke and ash.
Firefighters are also at risk of cardiac events including strokes and heart attacks. Fire fighters should maintain good physical fitness. Fitness programs, medical screening and examination programs which include stress tests can minimize the risks of firefighting cardiac problems. Other injury hazards wildland fire fighters face include slips, trips and falls, burns, scrapes and cuts from tools and equipment, being struck by trees, vehicles, or other objects, plant hazards such as thorns and poison ivy, snake and animal bites, vehicle crashes, electrocution from power lines or lightning storms, and unstable building structures.
Fire retardant.
Fire retardants are used to help slow wildfires, coat fuels, and lessen oxygen availability as required by various firefighting situations. They are composed of nitrates, ammonia, phosphates and sulfates, as well as other chemicals and thickening agents. The choice of whether to apply retardant depends on the magnitude, location and intensity of the wildfire. Fire retardants are used to reach inaccessible geographical regions where ground firefighting crews are unable to reach a wildfire or in any occasion where human safety and structures are in endangered. In certain instances, fire retardant may also be applied ahead of wildfires for protection of structures and vegetation as a precautionary fire defense measure.
The application of aerial fire retardants creates an atypical appearance on land and water surfaces and has the potential to change soil chemistry. Fire retardant can decrease the availability of plant nutrients in the soil by increasing the acidity of the soil and reducing soil pH. Fire retardant may also affect water quality through leaching, eutrophication, or misapplication. Fire retardant’s effects on drinking water remain inconclusive. Dilution factors, including water body size, rainfall, and water flow rates lessen the concentration and potency of fire retardant. Wildfire debris (ash and sediment) clog rivers and reservoirs increasing the risk for floods and erosion that ultimately slow and/or damage water treatment systems. There is continued concern of fire retardant effects on land, water, wildlife habitats, and watershed quality, additional research is needed. However, on the positive side, fire retardant (specifically its nitrogen and phosphorus components) has been shown to have a fertilizing effect on nutrient-deprived soils and thus creates a temporary increase in vegetation.
Current USDA procedure maintains that the aerial application of fire retardant in the United States must clear waterways by a minimum of 300 feet in order to safeguard effects of retardant runoff. Aerial uses of fire retardant are required to avoid application near waterways and endangered species (plant and animal habitats). After any incident of fire retardant misapplication, the U.S. Forest Service requires reporting and assessment impacts be made in order to determine mitigation, remediation, and/or restrictions on future retardant uses in that area.
Modeling.
Wildfire modeling is concerned with numerical simulation of wildfires in order to comprehend and predict fire behavior. Wildfire modeling can ultimately aid wildfire suppression, increase the safety of firefighters and the public, and minimize damage. Using computational science, wildfire modeling involves the statistical analysis of past fire events to predict spotting risks and front behavior. Various wildfire propagation models have been proposed in the past, including simple ellipses and egg- and fan-shaped models. Early attempts to determine wildfire behavior assumed terrain and vegetation uniformity. However, the exact behavior of a wildfire's front is dependent on a variety of factors, including windspeed and slope steepness. Modern growth models utilize a combination of past ellipsoidal descriptions and Huygens' Principle to simulate fire growth as a continuously expanding polygon. Extreme value theory may also be used to predict the size of large wildfires. However, large fires that exceed suppression capabilities are often regarded as statistical outliers in standard analyses, even though fire policies are more influenced by catastrophic wildfires than by small fires.
Cellular automata models have increasingly been used to model forest fire events. The initial model proposed by Drossel-Schwarbl (1992) identified forest fires as self-organised critical systems because the frequency-size distribution adheres to a power law relationship. More highly parameterised models have since supported this initial power law claim within forest fire models and real data obtained from forest fires in Australia and the US have shown the power law relationship to hold over a certain range. This has implications for predicting the size of forest fires based on observed frequency in areas where environmental conditions make the area susceptible to wildfire. Cellular automata models have also been used to model the response of forest fire events to changes in type, amount and distribution of combustible material, as well as weather conditions.
Human risk and exposure.
2009 California Wildfires at NASA/JPL - Pasadena, California
Wildfire risk is the chance that a wildfire will start in or reach a particular area and the potential loss of human values if it does. Risk is dependent on variable factors such as human activities, weather patterns, availability of wildfire fuels, and the availability or lack of resources to suppress a fire. Wildfires have continually been a threat to human populations. However, human induced geographical and climatic changes are exposing populations more frequently to wildfires and increasing wildfire risk. It is speculated that the increase in wildfires arises from a century of wildfire suppression coupled with the rapid expansion of human developments into fire-prone wildlands. Wildfires are naturally occurring events that aid in promoting forest health. The consequence of suppressing wildfires has led to an overgrowth in forest vegetation, which provides excess fuel that increases the severity, range, and duration of a wildfire. Global warming and climate changes are causing an increase in temperatures and more droughts nationwide which also contributes to an increase in wildfire risk.
Regional burden of wildfires in the United States.
Nationally, the burden of wildfires is disproportionally heavily distributed in the southern and western regions. The Geographic Area Coordinating Group (GACG) divides the United States and Alaska into 11 geographic areas for the purpose of emergency incident management. One particular area of focus is wildland fires. A national assessment of wildfire risk in the United States based on GACG identified regions (with the slight modification of combining Southern and Northern California, and the West and East Basin); indicate that California (50.22% risk) and the Southern Area (15.53% risk) are the geographic areas with the highest wildfire risk. The western areas of the nation are experiencing an expansion of human development into and beyond what is called the wildland-urban interface (WUI). When wildfires inevitably occur in these fire-prone areas, often communities are threatened due to their proximity to fire-prone forest. The south is one of the fastest growing regions with 88 million acres classified as WUI. The south consistently has the highest number of wildfires per year. More than 50, 000 communities are estimated to be at high to very high risk of wildfire damage. These statistics are greatly attributable to the South’s year-round fire season.
Wildfires risk to human health.
The most noticeable adverse effect of wildfires is the destruction of property and biomass. However, the release of hazardous chemicals from the burning of wildland fuels significantly impacts health in humans. Wildfire smoke is composed primarily of carbon dioxide and water vapor. Other common smoke components present in lower concentrations are carbon monoxide, formaldehyde, acrolein, polyaromatic hydrocarbons, and benzene. Small particulates suspended in air which come in solid form or in liquid droplets are also present in smoke. 80 -90% of wildfire smoke, by mass, is within the fine particle size class of 2.5 micrometers in diameter or smaller. Despite carbon dioxides high concentration in smoke, it poses low health risk due to its low toxicity. Carbon monoxide and fine particulate matter, particularly 2.5 µm in diameter and smaller, have been identified as the major health threats. Other chemicals are considered to be significant hazards but are found in concentrations that are too low to cause detectable health effects.
The degree of wildfire smoke exposure to an individual is dependent on the length, severity, duration, and proximity of the fire. People are exposed directly to smoke via the respiratory tract though inhalation of air pollutants. Indirectly, communities are exposed to wildfire debris that can contaminate soil and water supplies. Firefighters are at the greatest risk for acute and chronic health effects resulting from wildfire smoke exposure. Due to firefighter’s occupational duties, they are frequently exposed to hazardous chemicals at a close proximity for longer periods of time. A case study on the exposure of wildfire smoke among wildland firefighters, show that firefighters are exposed to significant levels of carbon monoxide and respiratory irritants above OSHA permissible exposure limits (PEL) and ACGIH threshold limit values (TLV). 5-10% are overexposed. The study obtained exposure concentrations for one wildland firefighter over a 10-hour shift spent holding down a fireline. The firefighter was exposed to a wide range of carbon monoxide and respiratory irritant (combination of particulate matter 3.5 µm and smaller, acrolein, and formaldehype) levels. Carbon monoxide levels reached up to 160ppm and the TLV irritant index value reached a high of 10. In contrast, the OSHA PEL for carbon monoxide is 30ppm and for the TLV respiratory irritant index, the calculated threshold limit value is 1; any value above 1 exceeds exposure limits.
Residents in communities surrounding wildfires are exposed to lower concentrations of chemicals, but they are at a greater risk for indirect exposure through water or soil contamination. Exposure to residents is greatly dependent on individual susceptibility. Vulnerable persons such as children (ages 0–4), the elderly (ages 65 and older), smokers, and pregnant women are at an increased risk due to already compromised body systems, even when the exposures are present at low chemical concentrations and for relatively short exposure periods. The U.S. Environmental Protection Agency (EPA) developed the Air Quality Index (AQI), a public resource that provides national air quality standard concentrations for common air pollutants. The public can use this index as a tool to determine their exposure to hazardous air pollutants based on visibility range.
Health effects.
Inhalation of smoke from a wildfire can be a health hazard. Wildfire smoke is primarily composed of carbon dioxide, water vapor, particulate matter, organic chemicals, nitrogen oxides and other compounds. The principle health concern is the inhalation of particulate matter and carbon monoxide.
Particulate matter (PM) is a type of air pollution made up of particles of dust and liquid droplets. They are characterized into two categories based on the diameter of the particle. Coarse particles are between 2.5 micrometers and 10 micrometers and fine particles measure 2.5 micrometers and less. Both sizes can be inhaled. Coarse particles are filtered by the upper airways and can cause eye and sinus irritation as well as sore throat and coughing. The fine particles are more problematic because, when inhaled, they can be deposited deep into the lungs, where they are absorbed into the bloodstream. This is particularly hazardous to the very young, elderly and those with chronic conditions such as asthma, chronic obstructive pulmonary disease (COPD), cystic fibrosis and cardiovascular conditions. The illnesses most commonly with exposure to fine particle from wildfire smoke are bronchitis, exacerbation of asthma or COPD, and pneumonia. Symptoms of these complications include wheezing and shortness of breath and cardiovascular symptoms include chest pain, rapid heart rate and fatigue.
Carbon monoxide (CO) is a colorless, odorless gas that can be found at the highest concentration at close proximity to a smoldering fire. For this reason, carbon monoxide inhalation is a serious threat to the health of wildfire firefighters. CO in smoke can be inhaled into the lungs where it is absorbed into the bloodstream and reduces oxygen delivery to the body's vital organs. At high concentrations, it can cause headache, weakness, dizziness, confusion, nausea, disorientation, visual impairment, coma and even death. However, even at lower concentrations, such as those found at wildfires, individuals with cardiovascular disease may experience chest pain and cardiac arrhythmia. A recent study tracking the number and cause of wildfire firefighter deaths from 1990-2006 found that 21.9% of the deaths occurred from heart attacks.
Another important and somewhat less obvious health effect of wildfires is psychiatric diseases and disorders. Both adults and children from countries ranging from the United States and Canada to Greece and Australia who were directly and indirectly affected by wildfires were found by researchers to demonstrate several different mental conditions linked to their experience with the wildfires. These include post-traumatic stress disorder (PTSD), depression, anxiety, and phobias.
In a new twist to wildfire health effects, former uranium mining sites were burned over in the summer of 2012 near North Fork, Idaho. This prompted concern from area residents and Idaho State Department of Environmental Quality officials over the potential spread of radiation in the resultant smoke, since those sites had never been completely cleaned up from radioactive remains.
Epidemiology.
The EPA has defined acceptable concentrations of particulate matter in the air, through the National Ambient Air Quality Standards and monitoring of ambient air quality has been mandated. Due to these monitoring programs and the incidence of several large wildfires near populated areas, epidemiological studies have been conducted and demonstrate an association between human health effects and an increase in fine particulate matter due to wildfire smoke.
An increase in PM emitted from the Hayman fire in Colorado in June 2002, was associated with an increase in respiratory symptoms in patients with COPD.
Looking at the wildfires in Southern California in October 2003 in a similar manner, investigators have shown an increase in hospital admissions due to asthma during peak concentrations of PM. Children participating in the Children’s Health Study were also found to have an increase in eye and respiratory symptoms, medication use and physician visits. Recently, it was demonstrated that mothers who were pregnant during the fires gave birth to babies with a slightly reduced average birth weight compared to those who were not exposed to wildfire during birth. Suggesting that pregnant women may also be at greater risk to adverse effects from wildfire. Worldwide it is estimated that 339,000 people die due to the effects of wildfire smoke each year.

</doc>
<doc id="56107" url="http://en.wikipedia.org/wiki?curid=56107" title="Metropolis–Hastings algorithm">
Metropolis–Hastings algorithm

In statistics and in statistical physics, the Metropolis–Hastings algorithm is a Markov chain Monte Carlo (MCMC) method for obtaining a sequence of random samples from a probability distribution for which direct sampling is difficult. This sequence can be used to approximate the distribution (i.e., to generate a histogram), or to compute an integral (such as an expected value). Metropolis–Hastings and other MCMC algorithms are generally used for sampling from multi-dimensional distributions, especially when the number of dimensions is high. For single-dimensional distributions, other methods are usually available (e.g. adaptive rejection sampling) that can directly return independent samples from the distribution, and are free from the problem of auto-correlated samples that is inherent in MCMC methods.
History.
The algorithm was named after Nicholas Metropolis, who was an author along with Arianna W. Rosenbluth, Marshall N. Rosenbluth, Augusta H. Teller, and Edward Teller of the 1953 paper "Equation of State Calculations by Fast Computing Machines" which first proposed the algorithm for the specific case of the canonical ensemble; and W. K. Hastings who extended it to the more general case in 1970.
There is controversy over the credit for discovery of the algorithm.
Edward Teller states in his memoirs that the five authors of the 1953 paper worked
together for "days (and nights)".
M. Rosenbluth, in an oral history recorded shortly before his death credits E. Teller with posing the
original problem, himself with solving it, and A.W. Rosenbluth (his wife) with programming the computer.
According to M. Rosenbluth, neither Metropolis nor A.H. Teller participated in any way.
Rosenbluth's account of events is supported by other contemporary recollections.
According to Roy Glauber and Emilio Segrè, the original algorithm was invented by Enrico Fermi
and reinvented by Stan Ulam.
Intuition.
The Metropolis–Hastings algorithm can draw samples from any probability distribution "P(x)", provided you can compute the value of a function "f(x)" which is "proportional" to the density of "P". The lax requirement that "f(x)" should be merely proportional to the density, rather than exactly equal to it, makes the Metropolis–Hastings algorithm particularly useful, because calculating the necessary normalization factor is often extremely difficult in practice.
The Metropolis–Hastings algorithm works by generating a sequence of sample values in such a way that, as more and more sample values are produced, the distribution of values more closely approximates the desired distribution, "P(x)". These sample values are produced iteratively, with the distribution of the next sample being dependent only on the current sample value (thus making the sequence of samples into a Markov chain). Specifically, at each iteration, the algorithm picks a candidate for the next sample value based on the current sample value. Then, with some probability, the candidate is either accepted (in which case the candidate value is used in the next iteration) or rejected (in which case the candidate value is discarded, and current value is reused in the next iteration)−the probability of acceptance is determined by comparing the values of the function "f(x)" of the current and candidate sample values with respect to the desired distribution "P(x)".
For the purpose of illustration, the Metropolis algorithm, a special case of the Metropolis–Hastings algorithm where the proposal function is symmetric, is described below.
Metropolis algorithm (symmetric proposal distribution)
Let "f(x)" be a function that is proportional to the desired probability distribution "P(x)".
This algorithm proceeds by randomly attempting to move about the sample space, sometimes accepting the moves and sometimes remaining in place. Note that the acceptance ratio formula_5 indicates how probable the new proposed sample is with respect to the current sample, according to the distribution formula_6. If we attempt to move to a point that is more probable than the existing point (i.e. a point in a higher-density region of formula_6), we will always accept the move. However, if we attempt to move to a less probable point, we will sometimes reject the move, and the more the relative drop in probability, the more likely we are to reject the new point. Thus, we will tend to stay in (and return large numbers of samples from) high-density regions of formula_6, while only occasionally visiting low-density regions. Intuitively, this is why this algorithm works, and returns samples that follow the desired distribution formula_6.
Compared with an algorithm like adaptive rejection sampling that directly generates independent samples from a distribution, Metropolis–Hastings and other MCMC algorithms have a number of disadvantages:
On the other hand, most simple rejection sampling methods suffer from the "curse of dimensionality", where the probability of rejection increases exponentially as a function of the number of dimensions. Metropolis–Hastings, along with other MCMC methods, do not have this problem to such a degree, and thus are often the only solutions available when the number of dimensions of the distribution to be sampled is high. As a result, MCMC methods are often the methods of choice for producing samples from hierarchical Bayesian models and other high-dimensional statistical models used nowadays in many disciplines.
In multivariate distributions, the classic Metropolis–Hastings algorithm as described above involves choosing a new multi-dimensional sample point. When the number of dimensions is high, finding the right jumping distribution to use can be difficult, as the different individual dimensions behave in very different ways, and the jumping width (see above) must be "just right" for all dimensions at once to avoid excessively slow mixing. An alternative approach that often works better in such situations, known as Gibbs sampling, involves choosing a new sample for each dimension separately from the others, rather than choosing a sample for all dimensions at once. This is especially applicable when the multivariate distribution is composed out of a set of individual random variables in which each variable is conditioned on only a small number of other variables, as is the case in most typical hierarchical models. The individual variables are then sampled one at a time, with each variable conditioned on the most recent values of all the others. Various algorithms can be used to choose these individual samples, depending on the exact form of the multivariate distribution: some possibilities are adaptive rejection sampling, a one-dimensional Metropolis–Hastings step, or slice sampling.
Formal derivation of the Metropolis-Hastings algorithm.
The purpose of the Metropolis–Hastings algorithm is to generate a collection of states according to a desired distribution P(x). To accomplish this, the algorithm uses a Markov process which asymptotically reaches a unique stationary distribution π(x) such that π(x)=P(x) .
A Markov process is uniquely defined by its transition probabilities, the probability formula_11 of transitioning between any two states x to x'. It has a unique stationary distribution π(x) when the following two conditions are met:
Metropolis–Hastings algorithm resides in designing a Markov process (by constructing transition probabilities) which fulfils the two above conditions, such that its stationary distribution π(x) is chosen to be "P(x)". The derivation of the algorithm starts with the condition of detailed balance:
formula_13
which is re-written as
formula_14.
The approach is to separate the transition in two sub-steps; the proposal and the acceptance-rejection. The proposal distribution formula_15 is the conditional probability of proposing a state x' given x, and the acceptance distribution formula_16 the conditional probability to accept the proposed state x'. The transition probability can be written as the product of them:
formula_17 .
Inserting this relation the previous equation, we have
formula_18 .
The next step in the derivation is to choose an acceptance that fulfills detailed balance. One common choice is the Metropolis choice:
formula_19
i.e., we always accept when the acceptance is bigger than 1, and we reject accordingly when the acceptance is smaller than 1.
This constitutes the required quantity for implementing the algorithm.
The Metropolis–Hastings algorithm thus consists in the following:
The saved states are in principle drawn from the distribution formula_22, as step 4 ensures they are de-correlated.
The value of T must be chosen according to different factors such as the proposal distribution and, formally, it has to be of the order of the autocorrelation time of the Markov process.
It is important to notice that it is not clear, in a general problem, which distribution formula_15 one should use; it is a free parameter of the method which has to be adjusted to the particular problem in hand.
Step-by-step instructions.
Suppose the most recent value sampled is formula_24. To follow the Metropolis–Hastings algorithm, we next draw a new proposal state formula_25 with probability density formula_26, and calculate a value
where
is the probability (e.g., Bayesian posterior) ratio between the proposed sample formula_25 and the previous sample formula_24, and
is the ratio of the proposal density in two directions (from formula_24 to formula_25 and "vice versa").
This is equal to 1 if the proposal density is symmetric.
Then the new state formula_34 is chosen according to the following rules.
The Markov chain is started from an arbitrary initial value formula_37 and the algorithm is run for many iterations until this initial state is "forgotten". 
These samples, which are discarded, are known as "burn-in". The remaining set of accepted values of formula_38 represent a sample from the distribution formula_22.
The algorithm works best if the proposal density matches the shape of the target distribution formula_6 from which direct sampling is difficult, that is formula_41.
If a Gaussian proposal density formula_42 is used the variance parameter formula_43 has to be tuned during the burn-in period.
This is usually done by calculating the "acceptance rate", which is the fraction of proposed samples that is accepted in a window of the last formula_44 samples.
The desired acceptance rate depends on the target distribution, however it has been shown theoretically that the ideal acceptance rate for a one-dimensional Gaussian distribution is approx 50%, decreasing to approx 23% for an formula_44-dimensional Gaussian target distribution.
If formula_43 is too small the chain will "mix slowly" (i.e., the acceptance rate will be high but successive samples will move around the space slowly and the chain will converge only slowly to formula_6). On the other hand,
if formula_43 is too large the acceptance rate will be very low because the proposals are likely to land in regions of much lower probability density, so formula_49 will be very small and again the chain will converge very slowly.

</doc>
<doc id="56108" url="http://en.wikipedia.org/wiki?curid=56108" title="Penrose triangle">
Penrose triangle

The Penrose triangle, also known as the Penrose tribar, is an impossible object. It was first created by the Swedish artist Oscar Reutersvärd in 1934. The psychologist Lionel Penrose and his mathematician son Roger Penrose independently devised and popularised it in the 1950s, describing it as "impossibility in its purest form". It is featured prominently in the works of artist M. C. Escher, whose earlier depictions of impossible objects partly inspired it.
The tribar appears to be a solid object, made of three straight beams of square cross-section which meet pairwise at right angles at the vertices of the triangle they form. The beams may be broken, forming cubes or cuboids; the logo of National Westminster Bank, comprising three chevrons that form a broken hexagon, has sometimes been depicted with the chevrons as two sides of cubes forming the corners of a Penrose triangle.
This combination of properties cannot be realized by any 3-dimensional object in ordinary Euclidean space. Such an object can exist in certain Euclidean 3-manifolds. There also exist 3-dimensional solid shapes each of which, when viewed from a certain angle, appears the same as the 2-dimensional depiction of the Penrose triangle on this page (such as - for example - the image to the left depicting a sculpture in Perth, Australia). The term "Penrose triangle" can refer to the 2-dimensional depiction or the impossible object itself.
M.C. Escher's lithograph "Waterfall" depicts a watercourse that flows in a zigzag along the long sides of two elongated Penrose triangles, so that it ends up two stories higher than it began. The resulting waterfall, forming the short sides of both triangles, drives a water wheel. Escher helpfully points out that in order to keep the wheel turning some water must occasionally be added to compensate for evaporation.
If a line is traced around the Penrose triangle, a 3-loop Möbius strip is formed.
Other Penrose polygons.
While it is possible to construct analogies to the Penrose triangle with other regular polygons to create a Penrose polygon, the visual effect is not as striking, and as the sides increase, the object seems merely to be warped or twisted.

</doc>
<doc id="56109" url="http://en.wikipedia.org/wiki?curid=56109" title="Brown rat">
Brown rat

The brown rat, also referred to as common rat, street rat, sewer rat, Hanover rat, Norway rat, brown Norway rat, Norwegian rat, or wharf rat ("Rattus norvegicus") is one of the best known and most common rat.
One of the largest muroids, it is a brown or grey rodent with a body up to 25 cm long, and a similar tail length; the male weighs on average 350 g and the female 250 g. Thought to have originated in northern China, this rodent has now spread to all continents except Antarctica, and is the dominant rat in Europe and much of North America—making it by at least this particular definition the most successful mammal on the planet after humans. With rare exceptions, the brown rat lives wherever humans live, particularly in urban areas.
Selective breeding of "Rattus norvegicus" has produced the laboratory rat, a model organism in biological research, as well as pet rats.
Naming and etymology.
Originally called the "Hanover rat" by people wishing to link problems in 18th century England with the House of Hanover, it is not known for certain why the brown rat is named "Rattus norvegicus" (Norwegian rat), as it did not originate from Norway. However, the English naturalist John Berkenhout, author of the 1769 book "Outlines of the Natural History of Great Britain", is most likely responsible for popularizing the misnomer. Berkenhout gave the brown rat the binomial name "Rattus norvegicus", believing it had migrated to England from Norwegian ships in 1728, although no brown rat had entered Norway at that time.
By the early to middle part of the 19th century, British academics were aware that the brown rat was not native to Norway, hypothesizing (incorrectly) that it may have come from Ireland, Gibraltar or across the English Channel with William the Conqueror. As early as 1850, however, a more correct understanding of the rat's origins was beginning to develop. The British novelist Charles Dickens acknowledged the misnomer in his weekly journal, "All the Year Round," writing:
"Now there is a mystery about the native country of the best known species of rat, the common brown rat. It is frequently called, in books and otherwise, the 'Norway rat', and it is said to have been imported into this country in a ship-load of timber from Norway. Against this hypothesis stands the fact that when the brown rat had become common in this country, it was unknown in Norway, although there was a small animal like a rat, but really a lemming, which made its home there."
Academics began to understand the origins and corrected etymology of the brown rat towards the end of the 19th century, as seen in the 1895 text "Natural History" by American scholar Alfred Henry Miles:
"The brown rat is the species common in England, and best known throughout the world. It is said to have travelled from Persia to England less than two hundred years ago and to have spread from thence to other countries visited by English ships."
Though the assumptions surrounding this species' origins were not yet entirely accurate, by the 20th century, it was established among naturalists that the brown rat did not originate in Norway, rather the species came from central Asia and (likely) China. Despite this, this species' common name of "Norway rat" is still in use today.
Description.
The fur is coarse and usually brown or dark grey, while the underparts are lighter grey or brown. The brown rat is a rather large true murid and can weigh twice as much as a black rat and many times more than a house mouse. The length is commonly in the range of 20 to, with the tail a further 18 to, thus being roughly the same length as the body. Adult body weight averages 350 g in males and about 250 g in females. Exceptionally large individuals can reportedly reach 900 to but are not expected outside of domestic specimens. Stories of rats attaining sizes as big as cats are exaggerations, or misidentifications of other rodents, such as the coypu and muskrat. In fact it is common for breeding wild brown rats to weigh (sometimes considerably) less than 300 g.
Brown rats have acute hearing, are sensitive to ultrasound, and possess a very highly developed olfactory sense. Their average heart rate is 300 to 400 beats per minute, with a respiratory rate of around 100 per minute. The vision of a pigmented rat is poor, around 20/600, while a non-pigmented (albino) with no melanin in its eyes has both around 20/1200 vision and a terrible scattering of light within its vision. Brown rats are dichromates which perceive colors rather like a human with red-green colorblindness, and their colour saturation may be quite faint. Their blue perception, however, also has UV receptors, allowing them to see ultraviolet lights that some species cannot.
Biology and behavior.
The brown rat is nocturnal and is a good swimmer, both on the surface and underwater, and has been observed climbing slim round metal poles several feet in order to reach garden bird feeders. Brown rats dig well, and often excavate extensive burrow systems. A 2007 study found brown rats to possess metacognition, a mental ability previously only found in humans and some primates, but further analysis suggested they may have been following simple operant conditioning principles.
Communication.
Brown rats are capable of producing ultrasonic vocalizations. As pups, young rats use different types of ultrasonic cries to elicit and direct maternal search behavior, as well as to regulate their mother's movements in the nest. Although pups will produce ultrasounds around any other rats at 7 days old, by 14 days old they significantly reduce ultrasound production around male rats as a defensive response. Adult rats will emit ultrasonic vocalizations in response to predators or perceived danger; the frequency and duration of such cries depends on the sex and reproductive status of the rat. The female rat will also emit ultrasonic vocalizations during mating.
Chirping.
Rats may also emit short, high frequency, ultrasonic, socially induced vocalization during rough and tumble play, before receiving morphine, or mating, and when tickled. The vocalization, described as a distinct "chirping", has been likened to laughter, and is interpreted as an expectation of something rewarding. Like most rat vocalizations, the chirping is too high in pitch for humans to hear without special equipment. Bat detectors are often used by pet owners for this purpose.
In clinical studies, the chirping is associated with positive emotional feelings, and social bonding occurs with the tickler, resulting in the rats becoming conditioned to seek the tickling. However, as the rats age, the tendency to chirp appears to decline.
Rat chirp also can be used for mosquito control.
Other ultrasonic vocalizations, including a lower-frequency 'boom' or 'whoom' noise can be produced by bucks in a calm state, when grooming or settling down to sleep.
Audible communication.
Brown rats also produce communicative noises capable of being heard by humans. The most commonly heard in domestic rats is bruxing, or teeth-grinding, which is most usually triggered by happiness, but can also be 'self-comforting' in stressful situations, such as a visit to the vet. The noise is best described as either a quick clicking or 'burring' sound, varying from animal to animal.
In addition, they commonly squeak along a range of tones from high, abrupt pain squeaks to soft, persistent 'singing' sounds during confrontations.
Diet.
The brown rat is a true omnivore and will consume almost anything, but cereals form a substantial part of its diet. Martin Schein, founder of the Animal Behavior Society in 1964, studied the diet of brown rats and came to the conclusion that the most-liked foods of brown rats include scrambled eggs, macaroni and cheese, and cooked corn kernels. According to Schein, the least-liked foods were raw beets, peaches, and raw celery.
Foraging behavior is often population-specific, and varies by environment and food source. Brown rats living near a hatchery in West Virginia catch fingerling fish.
Some colonies along the banks of the Po River in Italy will dive for mollusks, a practice demonstrating social learning among members of this species. Rats on the island of Norderoog in the North Sea stalk and kill sparrows and ducks.
Reproduction and life cycle.
The brown rat can breed throughout the year if conditions are suitable, with a female producing up to five litters a year. The gestation period is only 21 days, and litters can number up to 14, although seven is common. They reach sexual maturity in about five weeks. Under ideal conditions (for the rat), this means the population of females could increase by a factor of three and a half (half a litter of 7) in 8 weeks (5 weeks for sexual maturity and 3 weeks of gestation), corresponding to a population growing by a factor of 10 in just 15 weeks. The maximum life span is up to three years, although most barely manage one. A yearly mortality rate of 95% is estimated, with predators and interspecies conflict as major causes.
When lactating, female rats display a 24-hour rhythm of maternal behavior, and will usually spend more time attending to smaller litters than large ones.
Brown rats live in large, hierarchical groups, either in burrows or subsurface places, such as sewers and cellars. When food is in short supply, the rats lower in social order are the first to die. If a large fraction of a rat population is exterminated, the remaining rats will increase their reproductive rate, and quickly restore the old population level.
Social behavior.
Rats commonly groom each other and sleep together. As with dogs, rats create a social hierarchy, and each rat has its own place in the pack. Rats are said to establish an order of hierarchy, so one rat will be dominant over another one. Groups of rats tend to "play fight", which can involve any combination of jumping, chasing, tumbling, and "boxing". Play fighting involves rats going for each other's necks, while serious fighting involves strikes at the others' back ends. If living space becomes limited, rats may turn to aggressive behavior, which may result in the death of some animals, reducing the burden over the living space.
Rats like most mammals also form family groups, a mother and her young. This applies to both groups of males and females. However, rats are territorial animals, meaning that they usually act aggressively or scared of strange rats. Rats will fluff up their hair, hiss, squeal, and move their tails around when defending their territory. Rats will chase each other, groom each other, sleep in group nests, wrestle with each other, have dominance squabbles, communicate, and play in various other ways with each other. Huddling is an additional important part of rat socialization. Huddling is often supposed to have a heat-conserving function. Nestling rats especially depend on heat from their mother, since they cannot regulate their own temperature. Huddling is an extreme form of herding. Other forms of interaction include, crawling under, which is literally the act of crawling underneath one another,walking over, also explained in the name, then there is allo-grooming, so-called to distinguish it from self-grooming. And lastly there is another type of contact called nosing, where a rat gently pushes with its nose at another rat near the neck.
Burrowing.
Rats are known to burrow extensively, both in the wild and in captivity, if given access to a suitable substrate. Rats generally begin a new burrow adjacent to an object or structure, as this provides a sturdy "roof" for the section of the burrow nearest to the ground's surface. Burrows usually develop to eventually include multiple levels of tunnels, as well as a secondary entrance. Older male rats will generally not burrow, while young males and females will burrow vigorously.
Burrows provide rats with shelter and food storage, as well as safe, thermo-regulated nest sites. Rats use their burrows to escape from perceived threats in the surrounding environment; for example, rats will retreat to their burrows following a sudden, loud noise or while fleeing an intruder. Burrowing can therefore be described as a "pre-encounter defensive behavior", as opposed to a "post-encounter defensive behavior", such as flight, freezing, or avoidance of a threatening stimulus.
Distribution and habitat.
Likely originating from the plains of Asia, northern China and Mongolia, the brown rat spread to other parts of the world sometime in the Middle Ages. The question of when brown rats became commensal with humans remains unsettled, but as a species, they have spread and established themselves along routes of human migration and now live almost everywhere humans are.
The brown rat may have been present in Europe as early as 1553, a conclusion drawn from an illustration and description by Swiss naturalist Conrad Gesner in his book "Historiae animalium", published 1551–1558. Though Gesner's description could apply to the black rat, his mention of a large percentage of albino specimens—not uncommon among wild populations of brown rats—adds credibility to this conclusion. Reliable reports dating to the 18th century document the presence of the brown rat in Ireland in 1722, England in 1730, France in 1735, Germany in 1750, and Spain in 1800, becoming widespread during the Industrial Revolution. It did not reach North America until around 1750–1755.
As it spread from Asia, the brown rat generally displaced the black rat in areas where humans lived. In addition to being larger and more aggressive, the change from wooden structures and thatched roofs to bricked and tiled buildings favored the burrowing brown rats over the arboreal black rats. In addition, brown rats eat a wider variety of foods, and are more resistant to weather extremes.
In the absence of humans, brown rats prefer damp environments, such as river banks. However, the great majority are now linked to man-made environments, such as sewage systems. 
It is often said that there are as many rats in cities as people, but this varies from area to area depending on climate, living conditions, etc. Brown rats in cities tend not to wander extensively, often staying within 20 m of their nest if a suitable concentrated food supply is available, but they will range more widely where food availability is lower. There is great debate over the size of the population of rats in New York City, with estimates from almost 100 million rats to as few as 250,000. Experts suggest New York is a particularly attractive place for rats because of its aging infrastructure, high moisture, and high poverty rates. In addition to sewers, rats are very comfortable living in alleyways and residential buildings, as there is usually a large and continuous food source in those areas.
In the United Kingdom, some figures show the rat population has been rising, with estimations that 81 million rats reside in the UK. Those figures would mean there are 1.3 rats per person in the country. High rat populations in the UK are often attributed to the mild climate, which allow them higher survival rates during the winter months.
The only brown rat-free zones in the world are the continent of Antarctica, some (although not all) parts of the Arctic, some especially isolated islands, the province of Alberta in Canada, and certain conservation areas in New Zealand.
Antarctica is almost completely covered by ice and has no permanent human inhabitants, making it uninhabitable by rats. The Arctic has extremely cold winters that rats cannot survive outdoors, and the human population density is extremely low, making it difficult for rats to travel from one habitation to another, although they have arrived in many coastal areas by ship. When the occasional rat infestation is found and eliminated, the rats are unable to reinfest it from an adjacent one. Isolated islands are also able to eliminate rat populations because of low human population density and geographic distance from other rat populations.
Alaska.
Rat Island in Alaska was infested with brown rats after a Japanese shipwreck in 1780. They had a devastating effect on the native bird life. An eradication program was started in 2007 and the island was declared rat free in June 2009.
Alberta, Canada.
Alberta, Canada, is the largest rat-free populated area in the world. Rat invasions of Alberta were stopped and eliminated by very aggressive government rat control measures, starting during the 1950s.
The only species of "Rattus" which is capable of surviving the climate of Alberta is the brown rat, which can only survive in the prairie region of the province, and even then must overwinter in buildings. Although it is a major agricultural area, Alberta is far from any seaport and only a portion of its eastern boundary with Saskatchewan provides a favorable entry route for rats. Brown rats cannot survive in the wild boreal forest to the north, the Rocky Mountains to the west, nor can they safely cross the semiarid High Plains of Montana to the south. The first brown rat did not reach Alberta until 1950, and in 1951, the province launched a rat-control program that included shooting, poisoning, and gassing rats, and bulldozing, burning down, and blowing up rat-infested buildings. The effort was backed by legislation that required every person and every municipality to destroy and prevent the establishment of designated pests. If they failed, the provincial government could carry out the necessary measures and charge the costs to the landowner or municipality.
In the first year of the program, 64 t of arsenic trioxide were spread throughout 8,000 buildings on farms along the Saskatchewan border. In 1953, warfarin, an anticoagulant which is much safer for humans and other large animals, and much more effective at killing rats than arsenic, was introduced. By 1960, the number of rat infestations in Alberta dropped to below 200 per year. In 2002 the province finally recorded its first year with zero rat infestations, and from 2002 to 2007 there were only two infestations found. After an infestation of rats in the Medicine Hat landfill was found in 2012, the province's rat-free status was questioned, but provincial government rat control specialists brought in excavating machinery, dug out, shot, and poisoned 147 rats in the landfill, and no live rats were found thereafter. In 2013 the number of rat infestations in Alberta dropped to zero again. Alberta defines an infestation as two or more rats found at the same location, since a single rat cannot reproduce. About a dozen single rats enter Alberta in an average year and are killed by provincial rat control specialists before they can reproduce.
Only zoos, universities, and research institutes are allowed to keep caged rats in Alberta, and possession of unlicensed rats (including pet rats) by anyone else is punishable by a penalty of up to $5,000 or up to 60 days in jail.
The adjacent and similarly landlocked province of Saskatchewan initiated a rat control program in 1972, and has managed to reduce the number of rats in the province substantially, although they have not been eliminated. The Saskatchewan rat control program has considerably reduced the number of rats trying to enter Alberta.
New Zealand.
First arriving before 1800 (perhaps on James Cook's vessels), brown rats have posed a serious threat to many of New Zealand's native animals. Rat eradication programmes within New Zealand have led to rat-free zones on offshore islands and even on fenced "ecological islands" on the mainland. Before an eradication effort was launched in 2001, the sub-Antarctic Campbell Island had the highest population density of brown rats in the world.
Diseases.
Similar to other rodents, brown rats may carry a number of pathogens, which can result in disease, including Weil's disease, rat bite fever, cryptosporidiosis, viral hemorrhagic fever, Q fever and hantavirus pulmonary syndrome. In the United Kingdom, brown rats are an important reservoir for "Coxiella burnetii," the bacterium that causes Q fever, with seroprevalence for the bacteria found to be as high as 53% in some wild populations.
This species can also serve as a reservoir for "Toxoplasma gondii", the parasite that causes toxoplasmosis, though the disease usually spreads from rats to humans when domestic cats feed on infected brown rats. The parasite has a long history with the brown rat, and there are indications that the parasite has evolved to alter an infected rat's perception to cat predation, making it more susceptible to predation and increasing the likelihood of transmission.
Surveys and specimens of brown rat populations throughout the world have shown this species is often associated with outbreaks of trichinosis, but the extent to which the brown rat is responsible in transmitting "Trichinella" larvae to humans and other synanthropic animals is at least somewhat debatable. "Trichinella pseudospiralis", a parasite previously not considered to be a potential pathogen in humans or domestic animals, has been found to be pathogenic in humans and carried by brown rats.
Brown rats are sometimes mistakenly thought to be a major reservoir of bubonic plague, a possible cause of the Black Death. However, the bacterium responsible, "Yersinia pestis", is commonly endemic in only a few rodent species and is usually transmitted zoonotically by rat fleas—common carrier rodents today include ground squirrels and wood rats. However, brown rats may suffer from plague, as can many nonrodent species, including dogs, cats, and humans. The original carrier for the plague-infected fleas thought to be responsible for the Black Death was the black rat, and it has been hypothesized that the displacement of black rats by brown rats led to the decline of bubonic plague. This theory has, however, been deprecated, as the dates of these displacements do not match the increases and decreases in plague outbreaks.
In captivity.
Uses in science.
Selective breeding of albino brown rats rescued from being killed in a now-outlawed sport called rat baiting has produced the albino laboratory rat. Like mice, these rats are frequently subjects of medical, psychological and other biological experiments, and constitute an important model organism. This is because they grow quickly to sexual maturity and are easy to keep and to breed in captivity. When modern biologists refer to "rats", they almost always mean "Rattus norvegicus".
As pets.
The brown rat is kept as a pet in many parts of the world. Australia, the United Kingdom, and the United States are just a few of the countries that have formed fancy rat associations similar in nature to the American Kennel Club, establishing standards, orchestrating events, and promoting responsible pet ownership.
The many different types of domesticated brown rats include variations in coat patterns, as well as the style of the coat, such as Hairless or Rex, and more recently developed variations in body size and structure, including dwarf and tailless fancy rats.
External links.
Overviews
"Rattus norvegicus" genome and use as model animal

</doc>
<doc id="56110" url="http://en.wikipedia.org/wiki?curid=56110" title="Impossible object">
Impossible object

An impossible object (also known as an impossible figure or an undecidable figure) is a type of optical illusion. It consists of a two-dimensional figure which is instantly and subconsciously interpreted by the visual system as representing a projection of a three-dimensional object.
In most cases the impossibility becomes apparent after viewing the figure for a few seconds. However, the initial impression of a 3D object remains even after it has been contradicted. There are also more subtle examples of impossible objects where the impossibility does not become apparent spontaneously and it is necessary to consciously examine the geometry of the implied object to determine that it is impossible.
The unsettling nature of impossible objects occurs because of our natural desire to interpret 2D drawings as three-dimensional objects. This is why a drawing of a Necker cube would be most likely seen as a cube, rather than "two squares connected with diagonal lines, a square surrounded by irregular planar figures, or any other planar figure." With an impossible object, looking at different parts of the object makes one reassess the 3D nature of the object, which confuses the mind.
Impossible objects are of interest to psychologists, mathematicians and artists without falling entirely into any one discipline.
Notable examples.
Notable impossible objects include:
History.
An early example of an impossible object comes from "Apolinère Enameled", a 1916 advertisement painted by Marcel Duchamp. It depicts a girl painting a bed-frame with white enamelled paint, and deliberately includes conflicting perspective lines, to produce an impossible object. To emphasise the deliberate impossibility of the shape, a piece of the frame is missing.
Swedish artist Oscar Reutersvärd was one of the first to deliberately design many impossible objects. He has been called "the father of impossible figures". In 1934 he drew the Penrose triangle, some years before the Penroses. In Reutersvärd's version the sides of the triangle are broken up into cubes.
In 1956, British psychiatrist Lionel Penrose and his son, mathematician Roger Penrose, submitted a short article to the "British Journal of Psychology" titled "Impossible Objects: A Special Type of Visual Illusion". This was illustrated with the Penrose triangle and Penrose stairs. The article referred to Escher, whose work had sparked their interest in the subject, but not Reutersvärd, of whom they were unaware. The article was published in 1958.
From the 1930s onwards Dutch artist M.C. Escher produced many drawings featuring paradoxes of perspective gradually working towards impossible objects". In 1957 he produced his first drawing containing a true impossible object: "Cube with Magic Ribbons". He produced many further drawings featuring impossible objects, sometimes with the entire drawing being an impossible object. His work did much to draw the attention of the public to impossible objects.
Some contemporary artists are also experimenting with impossible figures, for example, Jos de Mey, Shigeo Fukuda, Sandro del Prete, István Orosz (Utisz), Guido Moretti, Tamás F. Farkas and Mathieu Hamaekers.
Constructed impossible objects.
Although possible to represent in two dimensions, it is not geometrically possible for such an object to exist in the physical world. However some models of impossible objects have been constructed, such that when they are viewed from a very specific point, the illusion is maintained. Rotating the object or changing the viewpoint breaks the illusion, and therefore many of these models rely on forced perspective or having parts of the model appearing to be further or closer than they actually are.
The notion of an "interactive impossible object" is an impossible object that can be viewed from any angle without breaking the illusion.
Views of a sculpture from different angles. As the view rotates, a Penrose triangle appears to form.

</doc>
<doc id="56111" url="http://en.wikipedia.org/wiki?curid=56111" title="David Stirling">
David Stirling

Colonel Sir Archibald David Stirling, DSO, OBE (15 November 1915 – 4 November 1990) was a British mountaineer, World War II British Army officer, and the founder of the Special Air Service.
Life before the war.
Stirling was born at his family's ancestral home, Keir House in the parish of Lecropt, Perthshire. He was the son of Brigadier General Archibald Stirling, of Keir, and Margaret Fraser, daughter of Simon Fraser, the Lord Lovat, (a descendant of Charles II, King of Scots). His cousin was Simon Fraser, 15th Lord Lovat, and his grandparents were Sir William Stirling-Maxwell, 9th Baronet and Lady Anna Maria Leslie-Melville. Raised in the Roman Catholic faith of his mother, he was educated at the Benedictine Ampleforth College and Trinity College, Cambridge. A tall and athletic figure (he was 6 ft tall). He was training to climb Mount Everest when World War II broke out.
World War II and the founding of the SAS.
Stirling was commissioned into the Scots Guards from Ampleforth College Contingent Officer Training Corps on 24 July 1937. In June 1940 he volunteered for the new No. 8 Commando under Lieutenant-Colonel Robert Laycock which became part of Force Z (later named "Layforce"). After Layforce (and No.8 Commando) were disbanded on 1 August 1941, Stirling remained convinced that due to the mechanised nature of war a small team of highly trained soldiers with the advantage of surprise could exact greater damage to the enemy's ability to fight than an entire platoon. 
Aware that taking his idea up through the chain of command was unlikely to work, Stirling decided to go straight to the top. On crutches following a parachuting accident, he stealthily entered Middle East headquarters in Cairo (under, through or over a fence but spotted by guards) in an effort to see Commander-in-Chief General Claude Auchinleck. He ran into one office, only to come face-to-face with an officer he had previously fallen out with. Retreating rapidly to shouts of "Guards, Guards", he dodged into another office, Stirling came face to face with Deputy Commander Middle East General Ritchie. Stirling explained his plan to Ritchie, the latter immediately convincing Auchinleck (in the office next door) to allow Stirling to form a new Special Forces unit. The unit was given the deliberately misleading name "L Detachment, Special Air Service Brigade" to reinforce an existing deception of a parachute brigade existing in North Africa. Short of equipment at the outset when they set up base at Kibrit Air Base, particularly tents and related gear, the first operation of the new SAS was to relieve a well-equipped New Zealand unit of small tents, a large tent and contents including a bar and a piano. A truck and a series of bluffs managed to convince curious onlookers and the New Zealand unit that all was well. 
After a brief period of training, an initial attempt at attacking a German airfield by parachute landing on 16 November 1941 in support of Operation Crusader was disastrous. 42 of his 61 officers and men were killed, wounded or captured far from the target after being blown off course or landing in the wrong area, during one of the biggest storms for thirty years. Escaping only with the help of the Long Range Desert Group (LRDG) who were designated to pick up the unit after the attack, Stirling agreed that approaching by land under the cover of night would be safer and more effective than parachuting. As quickly as possible he organised raids on ports using this simple method, often bluffing through check posts at night using the language skills of some of his soldiers. 
Under his leadership, the Lewes bomb was invented by Jock Lewes, the first hand-held dual explosive and incendiary device. American jeeps, which were able to deal with the harsh desert terrain better than other transport, were cut down, adapted and fitted with obsolete RAF machine guns. He also pioneered the use of small groups to escape detection. Stirling often led from the front, his SAS units driving through enemy airfields to shoot up aircraft and crew, replacing the early operational strategy of attaching bombs to enemy aircraft on foot. The first jeep-borne airfield raid occurred on the night of 7-8 July 1942 when Stirling's SAS group attacked Bagush airfield along with five other Axis airfields all in the same night. After returning to Cairo on 16 July, Stirling collected a consignment of more Willys Bantam Jeeps for further airfield raids. His biggest success was on the night of 26-27 July 1942 when his SAS squadron with 18 jeeps raided the Sidi Haneish landing strip and destroyed over 20 German aircraft for the loss of one man killed. After a drive through the desert and evading enemy patrols and aircraft, Stirling and his men reached the safety of friendly lines on 29 July.
These hit-and-run operations eventually proved Stirling's undoing; he was captured by the Germans in January 1943. Although he escaped, he was subsequently re-captured by the Italians, who took great delight in the embarrassment this caused to their German allies. A further four escape attempts were made, before Stirling was finally sent to Colditz Castle, where he remained for the rest of the war. After his capture, his own brother Bill Stirling along with Paddy Mayne took command of the SAS. 
In North Africa, in the fifteen months before Stirling's capture, the SAS had destroyed over 250 aircraft on the ground, dozens of supply dumps, wrecked railways and telecommunications, and had put hundreds of enemy vehicles out of action. Field Marshal Montgomery described Stirling as "mad, quite mad" but admitted that men like Stirling were needed in time of war. According to John Aspinal, Stirling reputedly personally strangled 41 men.
Private military company.
Worried that Britain was losing its power after the war, Stirling organised deals to provide British weapons and military personnel to other countries, like Saudi Arabia, for various privatised foreign policy operations. Along with several associates, Stirling formed Watchguard International Ltd, formerly with offices in Sloane Street (where the Chelsea Hotel later opened) before moving to South Audley Street in Mayfair.
Business was chiefly with the Gulf States. He was linked, along with Denys Rowley, to a failed attempt to the overthrow Libyan ruler Muammar Gaddafi in 1970 or 1971. Stirling was the founder of private military company KAS International, also known as KAS Enterprises.
Watchguard International Ltd was a private military company, registered in Jersey in 1965 by Stirling and John Woodhouse. Woodhouse's first assignment was to go to Yemen to report on the state of the royalist forces when a cease-fire was declared. At the same time Stirling was cultivating his contacts in the Iranian government and exploring the chances of obtaining work in Africa. The company operated in Zambia and in Sierra Leone, providing training teams and advising on security matters, but its founders' maverick ways of doing business caused its eventual downfall. Woodhouse resigned as Director of Operations after a series of disagreements and Stirling ceased to take an active part in 1972.
Great Britain 75.
In mid-1970s Great Britain, Stirling became increasingly worried that an "undemocratic event" would occur and decided to take action. He created an organisation called Great Britain 75 and recruited members from the aristocratic clubs in Mayfair; mainly ex-military men (often former SAS members). The plan was simple. Should civil unrest result in the breakdown of normal Government operations, they would take over its running. He describes this in detail in an interview from 1974, part of which is present in Adam Curtis's documentary "The Mayfair Set", episode 1: "Who Pays Wins".
In August 1974, before Stirling was ready to go public with GB75, the pacifist magazine "Peace News" obtained and published his plans, and eventually Stirling – dismayed by the right-wing character of many of those seeking to join GB75 – abandoned the scheme.
Undermining trade unionism.
During the mid to late 1970s, Stirling created a secret organisation designed to undermine trade unionism from within. He recruited like minded individuals from within the trade union movement, with the express intention that they should cause as much trouble during conferences as permissible. One such member was Kate Losinska, who was Head of the Civil and Public Services Association. Funding for this "operation" came primarily from his friend Sir James Goldsmith.
Later life.
Stirling was the founder of the Capricorn Africa Society – a society for promoting an Africa free from racial discrimination. Founded in 1949, while Africa was still under colonial rule, it had its high point at the 1956 Salima Conference. However, because of his emphasis on a qualified and highly elitist voting franchise, similar to Disraeli's "fancy franchises", educated Africans were divided on it. Conversely, many white settlers believed it to be too liberal. Consequently the society's attempt to deal with the problem of different levels of social development in a non-racial way was ineffective, although it received a surprising validation when the South African Communist Party used Stirling's multi-racial elitist model for its 1955 "Congress Alliance" when taking over the African National Congress of South Africa. Stirling resigned as Chairman of the Society in 1959. That year, following gambling losses he was obliged to note "John Aspinall - I owe you £173,500" in the accountant's ledger. One night in 1967 he lost a further £150,000. In 1968 he won substantial damages in libel against Len Deighton, among others.
Honours.
Stirling was knighted in 1990, and died later that year 11 days before his 75th birthday. In 2002 the SAS memorial, a statue of Stirling standing on a rock, was opened on the Hill of Row near his family's estate at Park of Keir. Two bronze plaques were stolen from the statue sometime around the end of May 2014. The current Laird of the Keir estate is his nephew Archie Stirling, a millionaire businessman and former Scots Guards officer.
Additional reading.
 

</doc>
<doc id="56112" url="http://en.wikipedia.org/wiki?curid=56112" title="Necker cube">
Necker cube

The Necker cube is an optical illusion first published as a rhomboid in 1832 by Swiss crystallographer Louis Albert Necker.
Ambiguity.
The Necker cube is an ambiguous line drawing.
The effect is interesting because each part of the picture is ambiguous by itself, yet the human visual system picks an interpretation of each part that makes the whole consistent. The Necker cube is sometimes used to test computer models of the human visual system to see whether they can arrive at consistent interpretations of the image the same way humans do.
Humans do not usually see an inconsistent interpretation of the cube. A cube whose edges cross in an inconsistent way is an example of an impossible object, specifically an impossible cube (compare Penrose triangle).
With the cube on the left, most people see the lower-left face as being in front most of the time. This is possibly because people view objects from above, with the top side visible, far more often than from below, with the bottom visible, so the brain "prefers" the interpretation that the cube is viewed from above. Another reason behind this may be due to the brain's natural preference of viewing things from left to right, therefore seeing the leftmost square as being in front.
There is evidence that by focusing on different parts of the figure one can force a more stable perception of the cube. The intersection of the two faces that are parallel to the observer forms a rectangle, and the lines that converge on the square form a "y-junction" at the two diagonally opposite sides. If an observer focuses on the upper "y-junction" the lower left face will appear to be in front. The upper right face will appear to be in front if the eyes focus on the lower junction. Blinking while being on the second perception will probably cause you to switch to the first one.
The Necker cube has shed light on the human visual system. The phenomenon has served as evidence of the human brain being a neural network with two distinct equally possible interchangeable stable states. Sidney Bradford, blind from the age of ten months but regaining his sight following an operation at age 52, did not perceive the ambiguity that normal-sighted observers do.
Apparent Viewpoint.
The orientation of the Necker cube can also be altered by shifting the observer's point of view. When seen from apparent above, one face tends to be seen closer; and in contrast, when seen from a subjective viewpoint that is below, a different face comes to the fore.
Epistemology.
The Necker cube is used in epistemology (the study of knowledge) and provides a counter-attack against naïve realism. Naïve realism (also known as "direct" or "common-sense" realism) states that the way we perceive the world is the way the world actually is. The Necker cube seems to disprove this claim because we see one or the other of two cubes, but really, there is no cube there at all: only a two-dimensional drawing of twelve lines. We see something which is not really there, thus (allegedly) disproving naïve realism. This criticism of naïve realism supports representative realism.
A rotating Necker cube was used to demonstrate that the human visual system can recruit new visual cues that affect the way things look.
A doctored photograph purporting to be of an impossible cube was published in the June 1966 issue of "Scientific American", where it was called a "Freemish Crate".
References in popular culture.
The Necker cube is discussed to such extent in Robert J. Sawyer's 1998 science fiction novel "Factoring Humanity" that "Necker" becomes a verb, meaning to impel one's brain to switch from one perspective or perception to another.
In Grant Morrison's "Doom Patrol" comic book, issue 36/September 1990 page 14, a character named Mr. Jones constructs a three-dimensional Necker cube out of paper, meditates upon it in order to create new agents for his organization, and uses it as a weapon by forcing others to look into it.
Peter Watts postulates in his novel "Blindsight" that consciousness serves as a set of training wheels for reality; this manifests in our ability to see only one aspect of the Necker cube at a time. In the novel, vampires have a more advanced perception of the universe because they can hold both aspects of the Necker cube in their head simultaneously.

</doc>
<doc id="56114" url="http://en.wikipedia.org/wiki?curid=56114" title="Urbanization">
Urbanization

Urbanization is a population shift from rural to urban areas, "the gradual increase in the proportion of people living in urban areas", and the ways in which each society adapts to the change. It predominantly results in the physical growth of urban areas, be it horizontal or vertical. The United Nations projected that half of the world's population would live in urban areas at the end of 2008. It is predicted that by 2050 about 64% of the developing world and 86% of the developed world will be urbanized.That is equivalent to approximately 3 billion urbanites by 2050, much of which will occur in Africa and Asia. 
Urbanization is relevant to a range of disciplines, including geography, sociology, economics, urban planning, and public health. The phenomenon has been closely linked to modernization, industrialization, and the sociological process of rationalization. Urbanization can be seen as a specific condition at a set time (e.g. the proportion of total population or area in cities or towns) or as an increase in that condition over time. So urbanization can be quantified either in terms of, say, the level of urban development relative to the overall population, or as the rate at which the urban proportion of the population is increasing. Urbanization creates enormous social, economic and environmental changes, which provide an opportunity for sustainability with the “potential to use resources more efficiently, to create more sustainable land use and to protect the biodiversity of natural ecosystems.” 
Urbanization is not merely a modern phenomenon, but a rapid and historic transformation of human social roots on a global scale, whereby predominantly rural culture is being rapidly replaced by predominantly urban culture. The first major change in settlement patterns was the accumulation of hunter-gatherers into villages many thousand years ago. Village culture is characterized by common bloodlines, intimate relationships, and communal behavior whereas urban culture is characterized by distant bloodlines, unfamiliar relations, and competitive behavior. This unprecedented movement of people is forecast to continue and intensify during the next few decades, mushrooming cities to sizes unthinkable only a century ago. Today, in Asia the urban agglomerations of Osaka, Karachi, Jakarta, Mumbai, Shanghai, Manila, Seoul, and Beijing are each already home to over 20 million people, while the Pearl River Delta, Delhi and Tokyo are forecast to approach or exceed 40 million people each within the coming decade. Outside Asia, Mexico City, Sao Paulo, New York City, Lagos, Los Angeles, and Cairo are fast approaching being, or are already, home to over 20 million people.
History.
From the development of the earliest cities in Mesopotamia and Egypt until the 18th century, an equilibrium existed between the vast majority of the population who engaged in subsistence agriculture in a rural context, and small centres of populations in the towns where economic activity consisted primarily of trade at markets and manufactures on a small scale. Due to the primitive and relatively stagnant state of agriculture throughout this period the ratio of rural to urban population remained at a fixed equilibrium.
With the onset of the agricultural and industrial revolution in the late 18th century this relationship was finally broken and an unprecedented growth in urban population took place over the course of the 19th century, both through continued migration from the countryside and due to the tremendous demographic expansion that occurred at that time. In England, the urban population jumped from 17% in 1801 to 72% in 1891 (for other countries the figure was: 37% in France, 41% in Prussia and 28% in the United States).
As labourers were freed up from working the land due to higher agricultural productivity they converged on the new industrial cities like Manchester and Birmingham which were experiencing a boom in commerce, trade and industry. Growing trade around the world also allowed cereals to be imported from North America and refrigerated meat from Australasia and South America. Spatially, cities also expanded due to the development of public transport systems, which facilitated commutes of longer distances to the city centre for the working class.
Urbanization rapidly spread across the Western world and, since the 1950s, it has begun to take hold in the developing world as well. At the turn of the 20th century, just 15% of the world population lived in cities. According to the UN the year 2007 witnessed the turning point when more than 50% of the world population were living in cities, for the first time in human history.
Movement.
As more and more people leave villages and farms to live in cities, urban growth results. The rapid growth of cities like Chicago in the late 19th century, Tokyo in the mid twentieth, and Delhi in the 21st century can be attributed largely to rural-urban migration. This kind of growth is especially commonplace in developing countries. This phenomenal growth can also be attributed to the lure of not just economic opportunities, but also to loss or degradation of farmland and pastureland due to development, pollution, land grabs, or conflict, the attraction and anonymity of hedonistic pleasures of urban areas, proximity and ease of mass transport, as well as the opportunity to assert individualism.
Urban centres are seen by many as an opportunity to “escape traditional patriarchy and experience new freedoms,” this includes greater access to education, health, and employment. However, for many who seek these opportunities the opposite occurs resulting in “extreme poverty, exclusion, vulnerability and marginalization due to urban sprawl where “urban land is expanding much faster than the urban population.” As such a strain on urban efficiencies occurs resulting in the urban poor who are forced to create slums and then ultimately face unhealthy living conditions without access to the very opportunities they sought in the first place. The United Nations Population Fund (UNFPA) estimated that residents in slums had risen to approximately 863 million in 2012 from over 650 million in 1990. 
The rapid urbanization of the world’s population over the twentieth century is described in the 2005 Revision of the UN World Urbanization Prospects report. The global proportion of urban population rose dramatically from 13% (220 million) in 1900, to 29% (732 million) in 1950, to 49% (3.2 billion) in 2005. The same report projected that the figure is likely to rise to 60% (4.9 billion) by 2030.
According to the UNFPA State of the World Population 2007 report, sometime in the middle of 2007, the majority of people worldwide will be living in towns or cities, for the first time in history; this is referred to as the arrival of the "Urban Millennium" or the 'tipping point'. In regard to future trends, it is estimated 93% of urban growth will occur in developing nations, with 80% of urban growth occurring in Asia and Africa.
Urbanization rates vary between countries. The United States and United Kingdom have a far higher urbanization level than India, Swaziland or Niger, but a far slower annual urbanization rate, since much less of the population is living in a rural area. Some nations make a distinction between suburban and urban areas, while others do not; indeed, human conditions within such areas differ greatly.
Causes.
Urbanization occurs as individual, commercial flight, social and governmental efforts reduce time and expense in commuting and transportation and improve opportunities for jobs, education, housing, and transportation. Living in cities permits the advantages of the opportunities of proximity, diversity, and marketplace competition. However, the advantages of urbanization are weighed against alienation issues, stress, increased daily life costs, and negative social aspects that result from mass marginalization. Suburbanization, which is happening in the cities of the largest developing countries, was sold and seen as an attempt to balance these negative aspects of urban life while still allowing access to the large extent of shared resources.
Cities are known to be places where money, services, wealth and opportunities are centralized. Many rural inhabitants come to the city for reasons of seeking fortunes and social mobility. Businesses, which provide jobs and exchange capital are more concentrated in urban areas. Whether the source is trade or tourism, it is also through the ports or banking systems that foreign money flows into a country, commonly located in cities.
Economic opportunities are just one reason people move into cities, though they do not go to fully explain why urbanization rates have exploded only recently in places like China and India. Rural flight is a contributing factor to urbanization. In rural areas, often on small family farms or collective farms in villages, it has traditionally been difficult to access manufactured goods, though overall quality of life is very subjective, and may certainly surpass that of the city. Farm living has always been susceptible to unpredictable environmental conditions, and in times of drought, flood or pestilence, survival may become extremely problematic.
Thai farmers are seen as poor, stupid, and unhealthy. As young people flee the farms, the values and knowledge of rice farming and the countryside are fading, including the tradition of long kek, helping neighbors plant, harvest, or build a house. We are losing what we call Thai-ness, the values of being kind, helping each other, having mercy and gratefulness.— Iam Thongdee, Professor of Humanities, Mahidol University in Bangkok
 
In a New York Times article concerning the acute migration away from farming in Thailand, life as a farmer was described as "hot and exhausting." "Everyone says the farmer works the hardest but gets the least amount of money". In an effort to counter this impression, the Agriculture Department of Thailand is seeking to promote the impression that farming is "honorable and secure".
However, in Thailand, urbanization has also resulted in massive increases in problems such as obesity. City life, especially in modern urban slums of the developing world, is certainly hardly immune to pestilence or climatic disturbances such as floods, yet continues to strongly attract migrants. Examples of this were the 2011 Thailand floods and 2007 Jakarta flood. Urban areas are also far more prone to violence, drugs, and other urban social problems. In the case of the United States, industrialization of agriculture has negatively affected the economy of small and middle-sized farms and strongly reduced the size of the rural labour market.
These are the costs of participating in the urban economy. Your increased income is canceled out by increased expenditure. In the end, you have even less left for food. —Madhura Swaminathan, economist at Kolkata’s Indian Statistical Institute
 
Particularly in the developing world, conflict over land rights due to the effects of globalization has led to less politically powerful groups, such as farmers, losing or forfeiting their land, resulting in obligatory migration into cities. In China, where land acquisition measures are forceful, there has been far more extensive and rapid urbanization (54%) than in India (36%), where peasants form militant groups (e.g. Naxalites) to oppose such efforts. Obligatory and unplanned migration often results in rapid growth of slums. This is also similar to areas of violent conflict, where people are driven off their land due to violence. Bogota, Colombia is one example of this.
Cities offer a larger variety of services, such as specialist services that aren't found in rural areas. Supporting the provision of these services requires workers, resulting in more numerous and varied job opportunities. Elderly individuals may be forced to move to cities where there are doctors and hospitals that can cater for their health needs. Varied and high quality educational opportunities are another factor in urban migration, as well as the opportunity to join, develop, and seek out social communities.
Urbanization also creates greater opportunities for women that are otherwise denied to them living in rural areas. This creates a gender-related transformation where women are engaged in paid employment and have access to education resulting in demographic implications in which fertility levels decline. However, women are still at a disadvantage due to their unequal position in the labour market, their inability to secure assets independently from male relatives and exposure to violence. 
People located in cities are more productive than those working outside dense agglomerations. An important question for the policy makers as well as for clustering people deals with the causality of this relationship, that is whether people become more productive in cities due to certain agglomeration effects or are cities simply attracting those who are more productive. Economists have recently shown that there exists indeed a large productivity gain due to locating in dense agglomerations. It is thus possible that agents locate in cities in order to benefit from these agglomeration effects.
Dominant conurbation.
The dominant conurbation(s) of a country also benefit from even more intense concentrations of the very same things cities offer, making them magnets for not just the non-urban population, but urban and suburban population from other conurbations. Dominant conurbations are quite often primate cities, but do not have to be. Due to cases like Greater Manila, conurbation rather than city is more apt; as a whole Greater Manila's 20 million (over 20% national population) is very much a primate city, yet Quezon City(2.7 million), the largest municipality, or Manila (1.6 million), the capital, is not. Measures of a how dominant a conurbation is can relate to percentage of national output, wealth, and especially population as a percentage of an entire country. Greater Seoul is one conurbation with massive dominance over South Korea, it is home to 50% of the entire national population.
Though Greater Busan-Ulsan (15%, 8 million) and Greater Osaka (14%, 18 million) exhibit strong dominance in their respective countries, yet they are losing population to even more dominant rivals, Seoul and Tokyo.
Economic effects.
As cities develop, effects can include a dramatic increase and change in costs, often pricing the local working class out of the market, including such functionaries as employees of the local municipalities. For example, Eric Hobsbawm's book "The age of revolution: 1789–1848" (published 1962 and 2005) chapter 11, stated "Urban development in our period [1789–1848] was a gigantic process of class segregation, which pushed the new labouring poor into great morasses of misery outside the centres of government and business and the newly specialized residential areas of the bourgeoisie. The almost universal European division into a 'good' west end and a 'poor' east end of large cities developed in this period." This is likely due the prevailing south-west wind which carries coal smoke and other airborne pollutants downwind, making the western edges of towns preferable to the eastern ones. Similar problems now affect the developing world, rising inequality resulting from rapid urbanization trends. The drive for rapid urban growth and often efficiency can lead to less equitable urban development. Think tanks such as the Overseas Development Institute have proposed policies that encourage labor-intensive growth as a means of absorbing the influx of low-skilled and unskilled labor. One problem these migrant workers are involved with is the growth of slums. In many cases, the rural-urban low skilled or unskilled migrant workers, attracted by economic opportunities in urban areas, cannot find a job and afford housing in cities and have to dwell in slums. Urban problems, along with infrastructure developments, are also fueling suburbanization trends in developing nations, though the trend for core cities in said nations tends to continue to become ever denser. Urbanization is often viewed as a negative trend, but there are positives in the reduction of expenses in commuting and transportation while improving opportunities for jobs, education, housing, and transportation. Living in cities permits individuals and families to take advantage of the opportunities of proximity and diversity. While cities have a greater variety of markets and goods than rural areas, infrastructure congestion, monopolization, high overhead costs, and the inconvenience of cross-town trips frequently combine to make marketplace competition harsher in cities than in rural areas.
In many developing countries where economies are growing, the growth is often erratic and based on a small number of industries. For young people in these countries barriers exist such as, lack of access to financial services and business advisory services, difficulty in obtaining credit to start a business, and lack of entrepreneurial skills, in order for them to access opportunities in these industries. Investment in human capital so that young people have access to quality education and infrastructure to enable access to educational facilities is imperative to overcoming economic barriers. 
Environmental effects.
The phenomenon of Urban heat islands has become a growing concern. Incidence of this phenomenon as well as concern about it has increased over the years. An urban heat island is formed when industrial and urban areas are developed resulting in greater production and retention of heat. A large proportion of solar energy that affects rural areas is consumed by the evaporating water from vegetation and soil. In cities, where there is less vegetation and exposed soil, the majority of the sun’s energy is absorbed by urban structures and asphalt. Hence, during warm daylight hours, less evaporative cooling in cities results in higher surface temperatures than in rural areas. Vehicles and factories release additional city heat, as do industrial and domestic heating and cooling units. As a result, cities are often 1.8 to 5.4 °F (1 to 3 °C) warmer than surrounding landscapes. Impacts also include reducing soil moisture and a reduction in re-uptake of carbon dioxide emissions.
In his book "Whole Earth Discipline", Stewart Brand argues that the effects of urbanization are primarily positive for the environment. Firstly, the birth rate of new urban dwellers falls immediately to replacement rate, and keeps falling, reducing the risk of environmental stresses caused by population growth. Secondly, migration away from rural areas reduces the prevalence of destructive subsistence farming techniques, such as improperly implemented slash and burn agriculture.
In July 2013 a report was issued by the United Nations Department of Economic and Social Affairs, however warns that with the additional 2.4 billion people by 2050, the amount of food produced will have to increase by 70 percent straining food resources, especially in countries already facing food insecurity due to changing environmental conditions. The mix of changing environmental conditions and the growing number of people living in urban regions, according to UN experts, will strain basic sanitation systems, health care, and potentially cause a humanitarian and environmental nightmare.
Health effects.
In the developing world, urbanization does not seem to translate into a significant increase in life expectancy. Rapid urbanization has brought increased mortality from non-communicable diseases associated with lifestyle, including cancer and heart disease. Differences in mortality from contagious diseases vary depending on the particular disease.
Urban health levels are better in comparison those in rural areas on average. However, residents in poor areas such as slums and informal settlements suffer “disproportionately from disease, injury, premature death, and the combination of ill-health and poverty entrenches disadvantage over time.” Financial constraints for many of the urban poor also inhibit their access to health services due to an increasing requirement to pay for services resulting in people resorting to seeing less qualified and unregulated providers. 
While urbanization is associated with improvements in public hygiene, sanitation and access to health care, it also entails changes in occupational, dietary and exercise patterns. It can have mixed effects on health patterns, alleviating some problems and accentuating others. For instance, in children urbanization is associated with a lower risk of undernutrition but a higher risk of overweight. Overall, body mass index and cholesterol levels increase sharply with national income and the degree of urbanization. Easier access to non-traditional foods may lead to less healthy dietary patterns. In India prevalence of diabetes in urban areas appears to be more than twice as high as in rural areas. In general, major risk factors for chronic diseases are more prevalent in urban environments.
Changing forms.
Different forms of urbanization can be classified depending on the style of architecture and planning methods as well as historic growth of areas.
In cities of the developed world urbanization traditionally exhibited a concentration of human activities and settlements around the downtown area, the so-called "in-migration". In-migration refers to migration from former colonies and similar places. The fact that many immigrants settle in impoverished city centres led to the notion of the "peripheralization of the core", which simply describes that people who used to be at the periphery of the former empires now live right in the centre.
Recent developments, such as inner-city redevelopment schemes, mean that new arrivals in cities no longer necessarily settle in the centre. In some developed regions, the reverse effect, originally called counter urbanization has occurred, with cities losing population to rural areas, and is particularly common for richer families. This has been possible because of improved communications, and has been caused by factors such as the fear of crime and poor urban environments. It has contributed to the phenomenon of shrinking cities experienced by some parts of the industrialized world.
When the residential area shifts outward, this is called suburbanization. A number of researchers and writers suggest that suburbanization has gone so far to form new points of concentration outside the downtown both in developed and developing countries such as India. This networked, poly-centric form of concentration is considered by some emerging pattern of urbanization. It is called variously exurbia, edge city (Garreau, 1991), network city (Batten, 1995), or postmodern city (Dear, 2000). Los Angeles is the best-known example of this type of urbanization. Interestingly, in the United States, this process has reversed as of 2011, with "re-urbanization" occurring as "suburban flight" due to chronically high transport costs.
Rural migrants are attracted by the possibilities that cities can offer, but often settle in shanty towns and experience extreme poverty. The inability of countries to provide adequate housing for these rural migrants is related to overurbanization, a phenomenon in which the rate of urbanization grows more rapidly that the rate of economic development, leading to high unemployment and high demand for resources. In the 1980s, this was attempted to be tackled with the urban bias theory which was promoted by Michael Lipton.
...the most important class conflict in the poor countries of the world today is not between labour and capital. Nor is it between foreign and national interests. It is between rural classes and urban classes. The rural sector contains most of the poverty and most of the low-cost sources of potential advance; but the urban sector contains most of the articulateness, organization and power. So the urban classes have been able to win most of the rounds of the struggle with the countryside...".— Michael Lipton, author of urban bias theory 
Most of the urban poor in developing countries able to find work can spend their lives in insecure, poorly paid jobs. According to research by the Overseas Development Institute pro-poor urbanization will require labour-intensive growth, supported by labour protection, flexible land use regulation and investments in basic services.'
Urbanization can be planned urbanization or organic. Planned urbanization, i.e.: planned community or the garden city movement, is based on an advance plan, which can be prepared for military, aesthetic, economic or urban design reasons. Examples can be seen in many ancient cities; although with exploration came the collision of nations, which meant that many invaded cities took on the desired planned characteristics of their occupiers. Many ancient organic cities experienced redevelopment for military and economic purposes, new roads carved through the cities, and new parcels of land were cordoned off serving various planned purposes giving cities distinctive geometric designs. UN agencies prefer to see urban infrastructure installed before urbanization occurs. Landscape planners are responsible for landscape infrastructure (public parks, sustainable urban drainage systems, greenways etc.) which can be planned before urbanization takes place, or afterward to revitalize an area and create greater livability within a region. Concepts of control of the urban expansion are considered in the American Institute of Planners.
See also.
Contributors to urbanization:
Historical:
Regional:

</doc>
<doc id="56116" url="http://en.wikipedia.org/wiki?curid=56116" title="Salting the earth">
Salting the earth

Salt of the earth
Salting the earth, or sowing with salt, is the ritual of spreading salt on conquered cities to symbolize a curse on their re-inhabitation. It originated as a symbolic practice in the ancient Near East and became a well-established folkloric motif in the Middle Ages. There is no evidence that sufficient amounts of salt were used to render large tracts of land unusable.
Destroying cities.
The custom of purifying or consecrating a destroyed city with salt and cursing anyone who dared to rebuild it was widespread in the ancient Near East, but historical accounts are unclear as to what the sowing of salt meant in that process.
Various Hittite and Assyrian texts speak of ceremonially strewing salt, minerals, or plants (weeds, "cress", or "kudimmu", which are associated with salt and desolation) over destroyed cities, including Hattusa, Taidu, Arinna, Hunusa, Irridu, and Susa. The "Book of Judges" (9:45) says that Abimelech, the judge of the Israelites, sowed his own capital, Shechem, with salt, c. 1050 BC, after quelling a revolt against him. This may have been part of a ḥērem ritual (compare with "salt in the Bible").
Starting in the 19th century, various texts claim that the Roman general Scipio Aemilianus Africanus plowed over and sowed the city of Carthage with salt after defeating it in the Third Punic War (146 BC), sacking it, and forcing the survivors into slavery. However, no ancient sources exist documenting the salting itself. The Carthage story is a later invention, probably modeled on the story of Shechem. The ritual of symbolically drawing a plow over the site of a city is, however, mentioned in ancient sources, though not in reference to Carthage specifically.
When Pope Boniface VIII destroyed Palestrina in 1299, he ordered that it be plowed "following the old example of Carthage in Africa", and also salted. "I have run the plough over it, like the ancient Carthage of Africa, and I have had salt sown upon it..." The text is not clear as to whether he thought Carthage was salted. Later accounts of other saltings in the destructions of medieval Italian cities are now rejected as unhistorical: Padua by Attila (452)--perhaps in a parallel between Attila and the ancient Assyrians; Milan by Frederick Barbarossa (1162); and Semifonte by the Florentines (1202).
The English epic poem "Siege of Jerusalem" (c. 1370) recounts that Titus commanded the sowing of salt on the Temple, but this episode is not found in Josephus.
Punishing traitors.
In Spain and the Spanish Empire, salt was poured onto the land owned by a convicted traitor (often one who was executed and his head placed on a "picota", or pike, afterwards) after his house was demolished.
Likewise, in Portugal, salt was poured onto the land owned by a convicted traitor. The last known event of this sort was the destruction of the Duke of Aveiro's palace in Lisbon in 1759, due to his participation in the Távora affair (a conspiracy against King Joseph I of Portugal). His palace was demolished and his land was salted. A stone memorial now perpetuates the memory of the shame of the Duke, where it is written:
In this place were put to the ground and salted the houses of José Mascarenhas, stripped of the honours of Duque de Aveiro and others... Put to Justice as one of the leaders of the most barbarous and execrable upheaval that... was committed against the most royal and sacred person of the Lord Joseph I. In this infamous land nothing may be built for all time.
In the Portuguese colony of Brazil, the leader of the Inconfidência Mineira, Tiradentes, was sentenced to death and his house was "razed and salted, so that never again be built up on the floor, ... and even the floor will rise up a standard by which the memory is preserved (preserving) the infamy of this heinous offender..." He suffered further indignities, being hanged and quartered, his body parts carried to various parts of the country where his fellow revolutionaries had met, and his children deprived of their property and honor.
Legends.
An ancient legend says that Odysseus feigned madness by yoking a horse and an ox to his plow and sowing salt.

</doc>
<doc id="56117" url="http://en.wikipedia.org/wiki?curid=56117" title="Shareholder rights plan">
Shareholder rights plan

A shareholder rights plan, colloquially known as a "poison pill", is a type of defensive tactic used by a corporation's board of directors against a takeover. Typically, such a plan gives shareholders the right to buy more shares at a discount if one shareholder buys a certain percentage or more of the company's shares. The plan could be triggered, for instance, if any one shareholder buys 20% of the company's shares, at which point every shareholder (except the one who possesses 20%) will have the right to buy a new issue of shares at a discount. If every other shareholder is able to buy more shares at a discount, such purchases would dilute the bidder's interest, and the cost of the bid would rise substantially. Knowing that such a plan could be activated, the bidder could be disinclined to take over the corporation without the board's approval, and would first negotiate with the board in order to revoke the plan.
The plan can be issued by the board of directors as an "option" or a "warrant" attached to existing shares, and only be revoked at the discretion of the board.
In the field of mergers and acquisitions, shareholder rights plans were devised in the early 1980s as a way to prevent takeover bidders from negotiating a price for sale of shares directly with shareholders, and instead forcing the bidder to negotiate with the board.
Shareholder rights plans, or poison pills, are controversial because they hinder an active market for corporate control. Further, giving directors the power to deter takeovers puts directors in a position to enrich themselves, as they may effectively ask to be compensated for the price of consenting to a takeover. Shareholder rights plans are unlawful without shareholder approval in many jurisdictions such as the United Kingdom, frowned upon in others such as throughout the European Union, and lawful only if used "proportionately" in others, including Delaware in the United States. 
History.
The poison pill was invented by mergers and acquisitions lawyer Martin Lipton of Wachtell, Lipton, Rosen & Katz in 1982, as a response to tender-based hostile takeovers. Poison pills became popular during the early 1980s in response to the wave of takeovers by corporate raiders such as Carl Icahn. The term "poison pill" derives its original meaning from a poison pill physically carried by various spies throughout history, a pill which was taken by the spies when they were discovered to eliminate the possibility of being interrogated by an enemy.
It was reported in 2001 that since 1997, for every company with a poison pill which successfully resisted a hostile takeover, there were 20 companies with poison pills that accepted takeover offers. The trend since the early 2000s has been for shareholders to vote against poison pill authorization, since poison pills are designed to resist takeovers, whereas from the point of view of a shareholder, takeovers can be financially rewarding.
Some have argued that poison pills are detrimental to shareholder interests because they perpetuate existing management. For instance, Microsoft originally made an unsolicited bid for Yahoo!, but subsequently dropped the bid after Yahoo! CEO Jerry Yang threatened to make the takeover as difficult as possible unless Microsoft raised the price to US$37 per share. One Microsoft executive commented, "They are going to burn the furniture if we go hostile. They are going to destroy the place." Yahoo has had a shareholders rights plan in place since 2001. Analysts suggested that Microsoft's raised offer of $33 per share was already too expensive, and that Yang was not bargaining in good faith, which later led to several shareholder lawsuits and an aborted proxy fight from Carl Icahn. Yahoo's stock price plunged after Microsoft withdrew the bid, and Jerry Yang faced a backlash from stockholders that eventually led to his resignation.
Overview.
In publicly held companies, "poison pills" refer to various methods to deter takeover bids. Takeover bids are attempts by a bidder to obtain control of a target company, either by soliciting proxies to get elected to the board or by acquiring a controlling block of shares and using the associated votes to get elected to the board. Once in control of the board, the bidder can manage the target. As discussed below, targets have various takeover defenses available, and several types of defense have been called "poison pills" because they harm not only the bidder, but the target (or its shareholders) as well. Currently, the most common type of takeover defense is a shareholder rights plan.
Because the board of directors of the company can redeem or otherwise eliminate a standard poison pill, it does not typically preclude a proxy fight or other takeover attempts not accompanied by an acquisition of a significant block of the company's stock. It can, however, prevent shareholders from entering into certain agreements that can assist in a proxy fight, such as an agreement to pay another shareholder's expenses. In combination with a staggered board of directors, however, a shareholder rights plan can be a defense.
The goal of a shareholder rights plan is to force a bidder to negotiate with the target's board and not directly with the shareholders. The effects are twofold:
Common types of poison pills.
The target issues a large number of new shares, often preferred shares, to existing shareholders. These new shares usually have severe redemption provisions, such as allowing them to be converted into a large number of common shares if a takeover occurs. This immediately dilutes the percentage of the target owned by the acquirer, and makes it more expensive to acquire 50% of the target's stock.
The target takes on large debts in an effort to make the debt load too high to be attractive -- the acquirer would eventually have to pay the debts.
The company buys a number of smaller companies using a stock swap, diluting the value of the target's stock.
Under this scenario, the target company re-phases all its employees' stock-option grants to ensure they immediately become vested if the company is taken over. Many employees can then exercise their options and then dump the stocks. With the release of the "golden handcuffs", many discontented employees may quit immediately after having cashed in their stock options. This poison pill is designed to create an exodus of talented employees, reducing a corporate value as a target. In many high-tech businesses, attrition of talented human resources may result in a diluted or empty shell being left behind for the new owner.
For instance, Peoplesoft guaranteed its customers in June 2003 that if it were acquired within two years, presumably by its rival Oracle, and product support were reduced within four years, its customers would receive a refund of between two and five times the fees they had paid for their Peoplesoft software licenses. While the acquisition ultimately prevailed, the hypothetical cost to Oracle was valued at as much as US$1.5 billion. 
In a voting plan, a company will charter preferred stock with superior voting rights over that of common shareholders. If an unfriendly bidder acquired a substantial quantity of the target firm's voting common stock, it then still would not be able to exercise control over its purchase. For example, Asarco established a voting plan in which 99% of the company's common stock would only harness 16.5% of the total voting power.
In addition to these pills, a "dead-hand" provision allows only the directors who introduce the poison pill to remove it (for a set period after they have been replaced), thus potentially delaying a new board’s decision to sell a company.
Constraints and legal status.
The legality of poison pills had been unclear when they were first put to use in the early 1980s. However, the Delaware Supreme Court upheld poison pills as a valid instrument of takeover defense in its 1985 decision in Moran v. Household International, Inc. However, many jurisdictions other than the U.S. have held the poison pill strategy as illegal, or place restraints on their use.
In Canada, almost all shareholders rights plans are "chewable", meaning they contain a permitted bid concept such that a bidder who is willing to conform to the requirements of a permitted bid can acquire the company by take-over bid without triggering a flip-in event. Shareholder rights plans in Canada are also weakened by the ability of a hostile acquirer to petition the provincial securities regulators to have the company's pill overturned. Generally, the courts will overturn the pill to allow shareholders to decide whether they want to tender to a bid for the company. However, the company may be allowed to maintain it for long enough to run an auction to see if a white knight can be found. A notable Canadian case before the securities regulators in 2006 involved the poison pill of Falconbridge Ltd. which at the time was the subject of a friendly bid from Inco and a hostile bid from Xstrata plc, which was a 20% shareholder of Falconbridge. Xstrata applied to have Falconbridge's pill invalidated, citing among other things that the Falconbridge had had its pill in place without shareholder approval for more than nine months and that the pill stood in the way of Falconbridge shareholders accepting Xstrata's all cash offer for Falconbridge shares. Despite similar facts with previous cases in which securities regulators had promptly taken down pills, the Ontario Securities Commission ruled that Falconbridge's pill could remain in place for a further limited period as it had the effect of sustaining the auction for Falconbridge by preventing Xstrata increasing its ownership and potentially obtaining a blocking position that would prevent other bidders from obtaining 100% of the shares.
In the United Kingdom, poison pills are not allowed under the Takeover Panel rules. The rights of public shareholders are protected by the Panel on a case-by-case, principles-based regulatory regime. One disadvantage of the Panel's prohibition of poison pills is that it allows bidding wars to be won by hostile bidders who buy shares of their target in the marketplace during "raids". Raids have helped bidders win targets such as BAA plc and AWG plc when other bidders were considering emerging at higher prices. If these companies had poison pills, they could have prevented the raids by threatening to dilute the positions of their hostile suitors if they exceeded the statutory levels (often 10% of the outstanding shares) in the rights plan. The London Stock Exchange itself is another example of a company that has seen significant stakebuilding by a hostile suitor, in this case the NASDAQ. The LSE's ultimate fate is currently up in the air, but NASDAQ's stake is sufficiently large that it is essentially impossible for a third party bidder to make a successful offer to acquire the LSE.
Takeover law is still evolving in continental Europe, as individual countries slowly fall in line with requirements mandated by the European Commission. Stakebuilding is commonplace in many continental takeover battles such as Scania AB. Formal poison pills are quite rare in continental Europe, but national governments hold golden shares in many "strategic" companies such as telecom monopolies and energy companies. Governments have also served as "poison pills" by threatening potential suitors with negative regulatory developments if they pursue the takeover. Examples of this include Spain's adoption of new rules for the ownership of energy companies after E.ON of Germany made a hostile bid for Endesa and France's threats to punish any potential acquiror of Groupe Danone.
Other takeover defenses.
Poison pill is sometimes used more broadly to describe other types of takeover defenses that involve the target taking some action. Although the broad category of takeover defenses (more commonly known as "shark repellents") includes the traditional shareholder rights plan poison pill. Other anti-takeover protections include:
Shareholder input.
More companies are giving shareholders a say on poison pills. According to FactSet SharkRepellent data, so far this year, 21 companies that adopted or extended a poison pill have publicly disclosed they plan to put the poison pill to a shareholder vote within a year. That's already more than 2008's full year total of 18 and in fact is the most in any year since the first poison pill was adopted in the early 1980s.

</doc>
<doc id="56118" url="http://en.wikipedia.org/wiki?curid=56118" title="Tierra Amarilla, New Mexico">
Tierra Amarilla, New Mexico

Tierra Amarilla is a small unincorporated community near the Carson National Forest in the northern part of the U.S. state of New Mexico. It is the county seat of Rio Arriba County.
"Tierra Amarilla" is Spanish for "Yellow Earth". The name refers to clay deposits found in the Chama River Valley and used by Native American peoples.:352–353 Tewa and Navajo toponyms for the area also refer to the yellow clay.:352–353
History.
There is evidence of 5000 years of habitation in the Chama River Valley including pueblo sites south of Abiquiu. The area served as a trade route for peoples in the present-day Four Corners region and the Rio Grande Valley. Navajos later used the valley as a staging area for raids on Spanish settlements along the Rio Grande. Written accounts of the Tierra Amarilla locality by pathfinding Spanish friars in 1776 described it as suitable for pastoral and agricultural use. The route taken by the friars from Santa Fe to California became the Spanish Trail. During the Californian Gold Rush the area became a staging point for westward fortune seekers.
Tierra Amarilla Grant.
The "Tierra Amarilla Grant" was created in 1832 by the Mexican government for Manuel Martinez and settlers from Abiquiu.:352–353 The land grant encompassed a more general area than the contemporary community known as "Tierra Amarilla".:352–353 The grant holders were unable to maintain a permanent settlement due to "raids by Utes, Navajos and Jicarilla Apaches" until early in the 1860s. In 1860 the United States Congress confirmed the land grant as a private grant, rather than a community grant, due to mistranslated and concealed documents. Although a land patent for the grant required the completion of a geographical survey before issuance, some of Manuel Martinez' heirs began to sell the land to Anglo speculators. In 1880 Thomas Catron sold some of the grant to the Denver and Rio Grande Railway for the construction of their San Juan line and a service center at Chama. By 1883 Catron had consolidated the deeds he held for the whole of the grant sans the original villages and their associated fields. In 1950, the descendants of the original grant holder's court petitions to reclaim communal land were rebuked.
Rio Arriba's county seat.
In 1866 the United States Army established Camp Plummer just south of Los Ojos (established in 1860) to rein in already decreased Native American activity on the grant. The military encampment was deserted in 1869.:57, 210, 352–353 Las Nutrias, the site of the contemporary community, was founded nearby c.1862. The first post office in Las Nutrias was established in 1866 and bore the name "Tierra Amarilla", as did the present one which was established in 1870 after an approximately two-year absence.:352–353 In 1877 a U.S. Army lieutenant described the village as "the center of the Mexican population of northwestern New Mexico". The territorial legislature located Rio Arriba's county seat in Las Nutrias and renamed the village in 1880.:352–353 The Denver and Rio Grande Railway's 1881 arrival at Chama, about ten miles to the north, had profound effects on the development of the region by bringing the area out of economic and cultural isolation.
When Tierra Amarilla was designated as the county seat the villagers set about building a courthouse. This structure was demolished to make way for the present one, which was built in 1917 and gained notoriety fifty years later when it was the location of a gunfight between land rights activists and authorities. The neoclassical design by Isaac Rapp is now on the National Register of Historic Places.
Courthouse raid.
The Alianza Federal de Mercedes, led by Reies Tijerina, raided the Rio Arriba County Courthouse in 1967. Attempting to make a citizen's arrest of the district attorney "to bring attention to the unscrupulous means by which government and Anglo settlers had usurped Hispanic land grant properties", an armed struggle in the courthouse ensued resulting in Tijerina and his group fleeing to the south with two prisoners as hostages. Eulogio Salazar, a prison guard, was shot and Daniel Rivera, a sheriff's deputy, was badly injured. The National Guard, FBI and New Mexico State Police successfully pursued Tijerina, who was sentenced to less than three years.
Geography.
The Brazos Cliffs are a prominent nearby landmark and attraction, and a popular destination for rock climbers. Also nearby are the artificial Heron Lake and El Vado Lake.
Tierra Amarillas' Elevation is 7,524 feet above sea level.
Layout.
The settlement is situated in a cluster of villages along United States Route 84 and the Chama River. The layout of the villages, including the one that became Tierra Amarilla, do not follow the urban planning principles of the Laws of the Indies.
Demographics.
Tierra Amarilla has the ZIP code of 87575. The ZIP Code Tabulation Area for ZIP Code 87575 had a population of 750 at the 2000 census.

</doc>
<doc id="56119" url="http://en.wikipedia.org/wiki?curid=56119" title="Takeover">
Takeover

In business, a takeover is the purchase of one company (the "target") by another (the "acquirer", or "bidder"). In UK, the term refers to the acquisition of a public company whose shares are listed on a stock exchange, in contrast to the acquisition of a private company.
Types of takeover.
Friendly takeovers.
A "friendly takeover" is an acquisition which is approved by the management. Before a bidder makes an offer for another company, it usually first informs the company's board of directors. In an ideal world, if the board feels that accepting the offer serves the shareholders better than rejecting it, it recommends the offer be accepted by the shareholders.
In a private company, because the shareholders and the board are usually the same people or closely connected with one another, private acquisitions are usually friendly. If the shareholders agree to sell the company, then the board is usually of the same mind or sufficiently under the orders of the equity shareholders to cooperate with the bidder. This point is not relevant to the UK concept of takeovers, which always involve the acquisition of a public company.
Hostile takeovers.
A "hostile takeover" allows a bidder to take over a target company whose management is unwilling to agree to a merger or takeover. A takeover is considered "hostile" if the target company's board rejects the offer, but the bidder continues to pursue it, or the bidder makes the offer directly after having announced its firm intention to make an offer. Development of the hostile tender is attributed to Louis Wolfson.
A hostile takeover can be conducted in several ways. A tender offer can be made where the acquiring company makes a public offer at a fixed price above the current market price. Tender offers in the United States are regulated by the Williams Act. An acquiring company can also engage in a proxy fight, whereby it tries to persuade enough shareholders, usually a simple majority, to replace the management with a new one which will approve the takeover. Another method involves quietly purchasing enough stock on the open market, known as a "creeping tender offer", to effect a change in management. In all of these ways, management resists the acquisition, but it is carried out anyway.
The main consequence of a bid being considered hostile is practical rather than legal. If the board of the target cooperates, the bidder can conduct extensive due diligence into the affairs of the target company, providing the bidder with a comprehensive analysis of the target company's finances. In contrast, a hostile bidder will only have more limited, publicly available information about the target company available, rendering the bidder vulnerable to hidden risks regarding the target company's finances. An additional problem is that takeovers often require loans provided by banks in order to service the offer, but banks are often less willing to back a hostile bidder because of the relative lack of target information which is available to them.
A well known example of an extremely hostile takeover was Oracle's hostile bid to acquire PeopleSoft.
Reverse takeovers.
A "reverse takeover" is a type of takeover where a private company acquires a public company. This is usually done at the instigation of the larger, private company, the purpose being for the private company to effectively float itself while avoiding some of the expense and time involved in a conventional IPO. However, in the UK under AIM rules, a reverse take-over is an acquisition or acquisitions in a twelve-month period which for an AIM company would:
An individual or organization, sometimes known as corporate raider, can purchase a large fraction of the company's stock and, in doing so, get enough votes to replace the board of directors and the CEO. With a new agreeable management team, the stock is a much more attractive investment, which would likely result in a price rise and a profit for the corporate raider and the other shareholders.
Backflip takeovers.
A "backflip takeover" is any sort of takeover in which the acquiring company turns itself into a subsidiary of the purchased company. This type of takeover can occur when a larger but less well-known company purchases a struggling company with a very well-known brand. Examples include:
Financing a takeover.
Funding.
Often a company acquiring another pays a specified amount for it. This money can be raised in a number of ways. Although the company may have sufficient funds available in its account, remitting payment entirely from the acquiring company's cash on hand is unusual. More often, it will be borrowed from a bank, or raised by an issue of bonds. Acquisitions financed through debt are known as leveraged buyouts, and the debt will often be moved down onto the balance sheet of the acquired company. The acquired company then has to pay back the debt. This is a technique often used by private equity companies. The debt ratio of financing can go as high as 80% in some cases. In such a case, the acquiring company would only need to raise 20% of the purchase price.
Loan note alternatives.
Cash offers for public companies often include a "loan note alternative" that allows shareholders to take a part or all of their consideration in loan notes rather than cash. This is done primarily to make the offer more attractive in terms of taxation. A conversion of shares into cash is counted as a disposal that triggers a payment of capital gains tax, whereas if the shares are converted into other securities, such as loan notes, the tax is rolled over.
All share deals.
A takeover, particularly a reverse takeover, may be financed by an all share deal. The bidder does not pay money, but instead issues new shares in itself to the shareholders of the company being acquired. In a reverse takeover the shareholders of the company being acquired end up with a majority of the shares in, and so control of, the company making the bid. The company has managerial rights.
Mechanics.
In the United Kingdom.
Takeovers in the UK (meaning acquisitions of public companies only) are governed by the City Code on Takeovers and Mergers, also known as the 'City Code' or 'Takeover Code'. The rules for a takeover can be found in what is primarily known as 'The Blue Book'. The Code used to be a non-statutory set of rules that was controlled by city institutions on a theoretically voluntary basis. However, as a breach of the Code brought such reputational damage and the possibility of exclusion from city services run by those institutions, it was regarded as binding. In 2006, the Code was put onto a statutory footing as part of the UK's compliance with the European Takeover Directive (2004/25/EC).
The Code requires that all shareholders in a company should be treated equally. It regulates when and what information companies must and cannot release publicly in relation to the bid, sets timetables for certain aspects of the bid, and sets minimum bid levels following a previous purchase of shares.
In particular:
The Rules Governing the Substantial Acquisition of Shares, which used to accompany the Code and which regulated the announcement of certain levels of shareholdings, have now been abolished, though similar provisions still exist in the Companies Act 1985.
Strategies.
There are a variety of reasons why an acquiring company may wish to purchase another company. Some takeovers are "opportunistic" - the target company may simply be very reasonably priced for one reason or another and the acquiring company may decide that in the long run, it will end up making money by purchasing the target company. The large holding company Berkshire Hathaway has profited well over time by purchasing many companies opportunistically in this manner.
Other takeovers are "strategic" in that they are thought to have secondary effects beyond the simple effect of the profitability of the target company being added to the acquiring company's profitability. For example, an acquiring company may decide to purchase a company that is profitable and has good distribution capabilities in new areas which the acquiring company can use for its own products as well. A target company might be attractive because it allows the acquiring company to enter a new market without having to take on the risk, time and expense of starting a new division. An acquiring company could decide to take over a competitor not only because the competitor is profitable, but in order to eliminate competition in its field and make it easier, in the long term, to raise prices. Also a takeover could fulfill the belief that the combined company can be more profitable than the two companies would be separately due to a reduction of redundant functions.
Agency Problems.
Takeovers may also benefit from principal–agent problems associated with top executive compensation.
For example, it is fairly easy for a top executive to reduce the price of his/her company's stock – due
to information asymmetry. The executive can accelerate accounting of expected expenses, delay accounting of expected revenue,
engage in off-balance-sheet transactions to make the company's profitability appear temporarily poorer, or simply
promote and report severely conservative (e.g. pessimistic) estimates of future earnings. Such seemingly adverse earnings
news will be likely to (at least temporarily) reduce share price. (This is again due to information asymmetries since it
is more common for top executives to do everything they can to window dress their company's earnings forecasts).
There are typically very few legal risks to being 'too conservative' in one's accounting and earnings estimates.
A reduced share price makes a company an easier takeover target. When the company gets bought out (or taken private) – at a dramatically lower price – the takeover artist gains a windfall from the former top executive's actions to surreptitiously
reduce share price. This can represent tens of billions of dollars (questionably) transferred from previous shareholders to
the takeover artist. The former top executive is then rewarded with a golden handshake for presiding over
the fire sale that can sometimes be in the hundreds of millions of dollars for one or two years of work.
(This is nevertheless an excellent bargain for the takeover artist, who will tend to benefit from developing a reputation
of being very generous to parting top executives). This is just one example of some of the principal–agent / perverse incentive issues involved with takeovers.
Similar issues occur when a publicly held asset or non-profit organization undergoes privatization.
Top executives often reap tremendous monetary benefits when a government owned or non-profit entity
is sold to private hands. Just as in the example above, they can facilitate this process by making the
entity appear to be in financial crisis. This perception can reduce the sale price (to the profit of the purchaser) and
make non-profits and governments more likely to sell. It can also contribute to a public
perception that private entities are more efficiently run, reinforcing the political will to sell off public assets.
Pros and cons of takeover.
While pros and cons of a takeover differ from case to case, there are a few reoccurring ones worth mentioning.
Pros:
Cons:
Takeovers also tend to substitute debt for equity. In a sense, any government tax policy of allowing for deduction
of interest expenses but not of dividends, has essentially provided a substantial subsidy to takeovers.
It can punish more-conservative or prudent management that do not allow their companies to leverage themselves
into a high-risk position. High leverage will lead to high profits if circumstances go well, but can lead
to catastrophic failure if circumstances do not go favorably. This can create substantial negative externalities
for governments, employees, suppliers and other stakeholders.
Occurrence.
Corporate takeovers occur frequently in the United States, Canada, United Kingdom, France and Spain. They happen only occasionally in Italy because larger shareholders (typically controlling families) often have special board voting privileges designed to keep them in control. They do not happen often in Germany because of the dual board structure, nor in Japan because companies have interlocking sets of ownerships known as keiretsu, nor in the People's Republic of China because the state owned majority owns most publicly listed companies.
Tactics against hostile takeover.
There are several tactics, or techniques, which can be used to deter a hostile takeover.

</doc>
<doc id="56120" url="http://en.wikipedia.org/wiki?curid=56120" title="Hispanic">
Hispanic

Hispanic (Spanish: "hispano, hispánico" Galician: "hispánico", Basque: "hispaniar", Catalan: "hispà") is an ethnonym to people of country heritage that speak the Spanish language, in some definitions, to ancient Roman Hispania, which roughly comprised the Iberian Peninsula including the contemporary states of Spain, Portugal, Andorra and the Crown Colony or British Overseas Territories of Gibraltar. Today, organizations in the United States use the term as a broad catchall to refer to persons with a historical and cultural relationship with Spain, regardless of race and ethnicity. The U.S. Census Bureau defines the ethnonym "Hispanic or Latino" to refer to "a person of Cuban, Mexican, Puerto Rican, South or Central American (except for Brazil), or other Spanish culture or origin regardless of race" and states that Hispanics or Latinos can be of any race, any ancestry, any ethnicity. Generically, this limits the definition of Hispanic or Latino to people from the Caribbean, Central and South America, or other Spanish culture or origin, regardless of race, distinctly excluding all persons of Portuguese origin.
Because of the technical distinctions involved in defining "race" vs. "ethnicity," there is confusion among the general population about the designation of Hispanic identity. Currently, the United States Census Bureau defines five race categories:
According to census reports, of the above races the largest number of Hispanic or Latinos are of the White race, the second largest number come from the Native American/American Indian race who are the indigenous people of the Americas. The inhabitants of Easter Island are Pacific Islanders and since the island belongs to Chile they are theoretically Hispanic or Latinos.
Because Hispanic roots are considered aligned with a European ancestry (Spain), Hispanic/Latino ancestry is defined solely as an "ethnic "designation (similar to being Norse or Germanic). Therefore, a person of Hispanic descent is typically defined using both race and ethnicity as an identifier—i.e., Black-Hispanic, White-Hispanic, Asian-Hispanic, Amerindian-Hispanic or "other race" Hispanic.
The term "Hispanic" broadly refers to the culture, peoples, or nations with a historical link to Spain. The term commonly applies to countries once colonized by Spain, particularly the countries of Latin America that were colonized by Spain. It could be argued that the term should apply to all Spanish-speaking cultures or countries, as the historical roots of the word specifically pertain to the Iberian region. It is also difficult to label a culture with one term, such as Hispanic, as the customs, traditions, beliefs and art forms (music, literature, dress, architecture, cuisine or others) vary widely depending on country and even within the regions of said country. The Spanish language and culture is the main culture.
Officially, however, the U.S. Government has defined "Hispanic or Latino" persons as being "persons who trace their origin or descent to Mexico, Puerto Rico, Cuba, Central and South America (except for Brazil), and other Spanish cultures". This includes Spain which is the origin of Spanish culture. The United States Census uses the ethnonym "Hispanic or Latino" to refer to "a person of Cuban, Mexican, Puerto Rican, South or Central American, or other Spanish culture or origin regardless of race." These definitions include Spain but fully exclude non-Spanish speaking persons of Portuguese culture or origin based in Europe (as well as in Africa or Asia), but leave room for misinterpretation regarding persons of Brazilian origin (as Brazil is indeed located in South America, even if it does not speak Spanish or derive its culture from Spain, but from Portugal). The same happens to persons who can trace their origins to Suriname and Guyana (also in South America, but deriving their culture from The Netherlands and the United Kingdom). None of these countries regard themselves as Hispanic. Under the aforementioned definitions by the U.S. Government, it is clear that persons of Portuguese culture (Brazil) should not be classified as Hispanic.
Terminology.
The term "Hispanic" derives from "Hispanicus" (which derived from "Hispania"), "Hispania" may in turn derive from Latin "Hispanicus", or from Greek Ισπανία "Hispania" and Ισπανός "Hispanos", probably from Celtiberian or from Basque "Ezpanna". In English the word is attested from the 16th century (and in late 19th century in American English).
The words "Spain", "Spanish", and "Spaniard" are of the same etymology as "Hispanus", ultimately.
"Hispanus" was the Latin name given to a person from Hispania during Roman rule. In English, the term "Hispano-Roman" is sometimes used. The Hispano-Romans were composed of people from many different tribes. Some famous "Hispani" (plural of "Hispanus") were Marcus Annaeus Lucanus, Martial, Prudentius, Theodosius I, and Magnus Maximus and Maximus of Hispania.
Here follows a comparison of several terms related to "Hispanic":
Hispania was the Roman name for the whole territory of the Iberian Peninsula. Initially, Hispania was divided into two provinces: Hispania Citerior and Hispania Ulterior. In 27 b.C, Hispania Ulterior was divided into two new provinces, Hispania Baetica and Hispania Lusitania, while Hispania Citerior was renamed Hispania Tarraconensis. This division of Hispania explains the usage of the singular and plural forms (Spain, and The Spains) used to refer to the peninsula and its kingdoms in the Middle Ages.
Prior to the marriage of Queen Isabella I of Castile and King Ferdinand II of Aragon in 1469, the four Christian kingdoms of the Iberian Peninsula, namely the Kingdom of Portugal, the Crown of Aragon, the Crown of Castile, and the Kingdom of Navarre, were collectively referred to as The Spains. This revival of the old Roman name in the Middle Ages appears to have originated in Provençal, and was first documented at the end of the 11th century. In the Council of Constance, the four kingdoms shared one vote.
The word "Lusitanian", relates to Lusitania or Portugal, also in reference to the Lusitanians, one of the first Indo-European tribes to settle in Europe. From this tribe's name had derived the name of the Roman province of Lusitania, and "" remains Portugal's name in Latin.
The terms "Spain" and " the Spains" were not interchangeable.
Spain was a geographic territory home to several kingdoms (Christian and Muslim), with separate governments, laws, languages, religions, and costumes and was also the historical remnant of the Hispano-Gothic unity. Spain was not a political entity until much later, and when referring to the Middle Ages one should not be confounded with the nation-state of today. The term "The Spains" referred specifically to a collective of juridico-political units, that is, it first referred only to the Christian kingdoms, then to the different kingdoms ruled by the same king.
With the Decretos de Nueva Planta, Philip V started to organize the fusion of his kingdoms that until then were ruled as distinct and independent, but this unification process lacked a formal and juridic proclamation.
Although colloquially and literally the expression "King of Spain" or "King of the Spains" was already widespread, it did not refer to a unified nation-state. It was only in the constitution of 1812 that was adopted the name "Españas" (Spains) for the Spanish nation and the use of the title of "king of the Spains". The constitution of 1876 adopts for the first time the name "Spain" for the Spanish nation and from then on the kings would use the title of "king of Spain".
The expansion of the Spanish Empire between 1492 and 1898 brought thousands of Spanish migrants to the conquered lands, who established settlements, mainly in the Americas but also in other distant parts of the world, like in the Philippines being the lone Spanish territory in Asia, producing a number of multiracial populations. Today the term "Hispanic" is typically applied to the varied populations of these places, including those with little or no Spanish ancestry.
Definitions in ancient Rome.
The Latin gentile adjectives that belong to Hispania are Hispanus, Hispanicus and Hispanienses.
Hispanus is someone who is a native of Hispania with no foreign parents, while children born in Hispania of roman parents were Hispaniensis.
Hispaniensis means connected in some way to Hispania as in "Exercitus Hispaniensis" or "mercatores Hispanienses", that means those who are located in Hispania.
While Hispanicus means "of" or "belonging to" Hispania or the Hispanus or of their fashion as in "glaudius Hispanicus".
The gentile adjectives were not ethnolinguistic but derived primarily on a geographic basis, from the toponym Hispania as the people of Hispania spoke different languages, although Livy said they could all understand each other, not making clear if they spoke dialects of the same language or were polyglots.
The first recorded use of an anthroponym derived from the toponym Hispania is attested in one of the five fragments, of Ennius in 236 B.C. who wrote "Hispane, non Romane memoretis loqui me" (remember that I speak like a Spaniard not a Roman) as having been said by a native of Hispania.
Definitions in Portugal and Spain.
Persons from Portugal or of Portuguese extraction are known as Lusitanians or "Lusófonos" (Lusophone) not Hispanic. In Portugal, Hispanic refers to something related to ancient Hispania, Spain or the Spanish language and culture, not Portugal Portugal and Spain do not have exactly the same definition for the term Hispanic, but they do share the etymology for the word (pt: "hispânico", es: "hispánico").
The Royal Spanish Academy (Spanish: Real Academia Española, RAE), the official royal institution responsible for regulating the Spanish language defines the term "Hispano" (which means "Hispanic" in Spanish) as:
The term signifies the cultural resonance, among other elements and characteristics, of the descendants of the people who inhabited ancient Hispania. It has been used throughout history for many purposes, including drawing a contrast to the Moors and differentiating explorers and settlers.
The correct modern term to identify Portuguese and Spanish cultures under a single nomenclature is "Iberian", and the one to refer to cultures derived from both countries in the Americas is "Iberian-American". These designations can be mutually recognized by people in Portugal and Brazil, unlike "Hispanic", which is totally void of any self-identification in those countries, and quite on the opposite, serves the purpose of marking a clear distinction in relation to neighboring countries´ culture.
In Spanish, the term "hispano" as in "hispanoamericano", refers to the people of Spanish origin who live in the Americas; it also refers to a relationship to Hispania or to the Spanish language. There are people in Hispanic America that are not of Spanish origin, as the original people of these areas are Amerindians.
Definitions in the United States.
The U.S. Office of Management and Budget currently defines "Hispanic or Latino" as "a person of Mexican, Puerto Rican, Cuban, South or Central American (except for Brazil), or other Spanish culture or origin, regardless of race". The 2010 Census asked if the person was "Spanish/Hispanic/Latino". The United States Census uses the ethnonym "Hispanic or Latino" to refer to "a person of Cuban, Mexican, Puerto Rican, South (except for Brazil) or Central American, or other Spanish culture or origin regardless of race." The Census Bureau also explains that "[o]rigin can be viewed as the heritage, nationality group, lineage, or country of birth of the person or the person's ancestors before their arrival in the United States. People who identify their origin as Hispanic, Latino, or Spanish may be of any race."
The U.S. Department of Transportation defines "Hispanic" as, "persons of Mexican, Puerto Rican, Cuban, Dominican, Central or South American, or other Spanish or Portuguese culture or origin, regardless of race." This definition has been adopted by the Small Business Administration as well as by many federal, state, and municipal agencies for the purposes of awarding government contracts to minority owned businesses. 
The Congressional Hispanic Caucus and the Congressional Hispanic Conference include representatives of Spanish and Portuguese, Puerto Rican and Mexican descent. The Hispanic Society of America is dedicated to the study of the arts and cultures of Spain, Portugal, and Latin America. The Hispanic Association of Colleges and Universities, proclaimed champions of Hispanic success in higher education, is committed to Hispanic educational success in the U.S., Puerto Rico, Ibero-America, Spain and Portugal.
The U.S. Equal Employment Opportunity Commission encourages any individual who believes that he or she is Hispanic to self-identify as Hispanic. The United States Department of Labor - Office of Federal Contract Compliance Programs encourages the same self-identification. As a result, any individual who traces his or her origins to part of the Spanish Empire or Portuguese Empire may self-identify as Hispanic, because an employer may not override an individual's self-identification.
The 1970 Census was the first time that a "Hispanic" identifier was used and data collected with the question. The definition of "Hispanic" has been modified in each successive census.
In a recent study, most Spanish-speakers of Spanish or Hispanic American descent do not prefer the term "Hispanic" or "Latino" when it comes to describing their identity. Instead, they prefer to be identified by their country of origin. When asked if they have a preference for either being identified as "Hispanic" or "Latino," the Pew study finds that "half (51%) say they have no preference for either term." A majority (51%) say they most often identify themselves by their family’s country of origin, while 24% say they prefer a pan-ethnic label such as Hispanic or Latino. Among those 24% who have a preference for a pan-ethnic label, "'Hispanic' is preferred over 'Latino' by more than a two-to-one margin—33% versus 14%." Twenty-one percent prefer to be referred to simply as "Americans."
Hispanicization.
Hispanicization is the process by which a place or a person absorbs characteristics of Hispanic society and culture. Modern hispanization of a place, namely in the United States, might be illustrated by Spanish-language media and businesses. Hispanization of a person might be illustrated by speaking Spanish, making and eating Hispanic American food, listening to Spanish language music or participating in Hispanic festivals and holidays - Hispanization of those outside the Hispanic community as opposed to assimilation of Hispanics into theirs.
One reason that some people believe the assimilation of Hispanics in the U.S. is not comparable to that of other cultural groups is that Hispanic and Latino Americans have been living in parts of North America for centuries, in many cases well before the English-speaking culture became dominant. For example, California, Texas, Colorado, New Mexico (1598), Arizona, Nevada, Florida and Puerto Rico have been home to Spanish-speaking peoples since the 16th century, long before the U.S. existed. These and other Spanish-speaking territories were part of the Viceroyalty of New Spain, and later Mexico (with the exception of Florida and Puerto Rico), before these regions joined or were taken over by the United States in 1848. Some cities in the U.S. were founded by Spanish settlers as early as the 16th century, prior to the creation of the Thirteen Colonies. For example, San Miguel de Gualdape, Pensacola and St. Augustine, Florida were founded in 1526, 1559 and 1565 respectively. Santa Fe, New Mexico was founded in 1604, and Albuquerque was established in 1660. El Paso was founded in 1659, San Antonio in 1691, Laredo, Texas in 1755, San Diego in 1769, San Francisco in 1776, San Jose, California in 1777, New Iberia, Louisiana in 1779, and Los Angeles in 1781. Therefore, in many parts of the U.S., the Hispanic cultural legacy predates English/British influence. For this reason, many generations have largely maintained their cultural traditions and Spanish language well before the United States was created. However, Spanish-speaking persons in many Hispanic areas in the U.S. amounted to only a few thousand people when they became part of the United States; a large majority of current Hispanic residents are descended from Hispanics who entered the United States in the mid-to-late 20th and early 21st centuries.
Language retention is a common index to assimilation; according to the 2000 census, about 75 percent of all Hispanics spoke Spanish in the home. Spanish language retention rates vary geographically; parts of Texas and New Mexico have language retention rates over 90 percent, whereas in parts of Colorado and California, retention rates are lower than 30 percent. The degree of retention of Spanish as the native language is based on recent arrival from countries where Spanish is spoken. As is true of other immigrants, those who were born in other countries still speak their native language. Later generations are increasingly less likely to speak the language spoken in the country of their ancestors, as is true of other immigrant groups.
Spanish-speaking countries and regions.
Today, Spanish is among the most commonly spoken first languages of the world. During the period of the Spanish Empire from 1492 and 1898, many people migrated from Spain to the conquered lands. The Spaniards brought with them the Castilian language and culture, and in this process that lasted several centuries, created a global empire with a diverse population.
Miscegenation between peoples in the colonies led to the creation of the new mixed peoples, chiefly half-caste and mulattos, in many countries.
Culturally, Spaniards are typically European, but they also have small traces of many peoples from the rest of Europe, the Near East and the Mediterranean areas of northern Africa.
The Hispanic countries, including Spain, are also inhabited by peoples of non-Spanish ancestry, to widely varying extents.
Music.
Folk and popular dance and music also varies greatly among Hispanics. For instance, the music from Spain is a lot different from the Hispanic American, although there is a high grade of exchange between both continents. In addition, due to the high national development of the diverse nationalities and regions of Spain, there is a lot of music in the different languages of the Peninsula (Catalan, Galician and Basque, mainly). See, for instance, Music of Catalonia or Rock català, Music of Galicia, Cantabria and Asturias, and Basque music. Flamenco is also a very popular music style in Spain, especially in Andalusia. Spanish ballads "romances" can be traced in Mexico as "corridos" or in Argentina as "milongas", same structure but different scenarios.
On the other side of the ocean, Hispanic America is also home to a wide variety of music, even though "Latin" music is often erroneously thought of, as a single genre. Hispanic Caribbean music tends to favor complex polyrhythms of African origin. Mexican music shows combined influences of mostly Spanish and Native American origin, while traditional Northern Mexican music — norteño and banda — is more influenced by country-and-western music and the polka, brought by Central European settlers to Mexico. The music of Hispanic Americans — such as tejano music — has influences in rock, jazz, R&B, pop, and country music as well as traditional Mexican music such as Mariachi. Meanwhile, native Andean sounds and melodies are the backbone of Peruvian and Bolivian music, but also play a significant role in the popular music of most South American countries and are heavily incorporated into the folk music of Ecuador and Chile and the tunes of Colombia, and again in Chile where they play a fundamental role in the form of the greatly followed nueva canción. In U.S. communities of immigrants from these countries it is common to hear these styles. Latin pop, Rock en Español, Latin hip-hop, Salsa, Merengue, colombian cumbia and Reggaeton styles tend to appeal to the broader Hispanic population, and varieties of Cuban music are popular with many Hispanics of all backgrounds.
Literature.
Spanish-language literature and folklore is very rich and is influenced by a variety of countries. There are thousands of writers from many places, and dating from the Middle Ages to the present. Some of the most recognized writers are Miguel de Cervantes Saavedra (Spain), Lope de Vega (Spain), Calderón de la Barca (Spain), Carlos Fuentes (Mexico), Octavio Paz (Mexico), Miguel Ángel Asturias (Guatemala), George Santayana (US), José Martí (Cuba), Sabine Ulibarri (US), Federico García Lorca (Spain), Miguel de Unamuno (Spain), Gabriel García Márquez (Colombia), Rafael Pombo (Colombia), Horacio Quiroga (Uruguay), Rómulo Gallegos (Venezuela), Rubén Darío (Nicaragua), Mario Vargas Llosa (Peru), Giannina Braschi (Puerto Rico), Cristina Peri Rossi (Uruguay), Luisa Valenzuela (Argentina), Roberto Quesada (Honduras), Julio Cortázar (Argentina), Pablo Neruda (Chile), Gabriela Mistral (Chile), Jorge Luis Borges (Argentina), Pedro Henríquez Ureña (Dominican Republic), Ernesto Sabato (Argentina), Juan Tomás Ávila Laurel (Equatorial Guinea), Ciro Alegría (Peru), Joaquin Garcia Monge (Costa Rica), and José Rizal (Philippines).
Religion.
With regard to religious affiliation among Spanish-speakers, Christianity — specifically Roman Catholicism — is usually the first religious tradition that comes to mind . The Spaniards and the Portuguese took the Roman Catholic faith to Ibero-America and the Philippines, and Roman Catholicism remains the predominant religion amongst most Hispanics. A small but growing number of Hispanics belong to a Protestant denomination.
There are also Spanish-speaking Jews, most of whom are the descendants of Ashkenazi Jews who migrated from Europe (German Jews, Russian Jews, Polish Jews, etc.) to Hispanic America, particularly Argentina, Uruguay, Peru and Cuba (Argentina is host to the third largest Jewish population in the Western Hemisphere, after the United States and Canada) in the 19th century and following World War II. Many Spanish-speaking Jews also originate from the small communities of reconverted descendants of anusim — those whose Spanish Sephardi Jewish ancestors long ago hid their Jewish ancestry and beliefs in fear of persecution by the Spanish Inquisition in the Iberian Peninsula and Ibero-America. The Spanish Inquisition led to a large number of forced conversions of Spanish Jews. Genetic studies on the (male) Y-chromosome conducted by the University of Leeds in 2008 appear to support the idea that the number of forced conversions have been previously underestimated significantly. They found that twenty percent of Spanish males have Y-chromosomes associated with Sephardic Jewish ancestry. This may imply that there were more forced conversions than was previously thought. There are also thought to be many Catholic-professing descendants of marranos and Spanish-speaking crypto-Jews in the Southwestern United States and scattered through Hispanic America. Additionally, there are Sephardic Jews who are descendants of those Jews who fled Spain to Turkey, Syria, and North Africa, some of whom have now migrated to Hispanic America, holding on to some Spanish/Sephardic customs, such as the Ladino language, which mixes Spanish, Hebrew, Arabic and others, though written with Hebrew and Latin characters. Though, it should be noted, that Ladinos were also African slaves captive in Spain held prior to the colonial period in the Americas. (See also History of the Jews in Hispanic America and List of Hispanic American Jews.)
Among the Spanish-speaking Catholics, most communities celebrate their homeland's patron saint, dedicating a day for this purpose with festivals and religious services. Some Spanish-speakers syncretize Roman Catholicism and African or Native American rituals and beliefs. Such is the case of Santería, popular with Afro-Cubans, which combines old African beliefs in the form of Roman Catholic saints and rituals. Other syncretistic beliefs include Spiritism and Curanderismo.
While a tiny minority, there are some Muslims in Latin America, in the US, and in the Philippines, living predominantly in Mindanao, the home of Islam in the Philippines.
In the United States, some 65% of Hispanics and Latinos report themselves Catholic and 21% Protestant, with 13% having no affiliation. A minority among the Roman Catholics, about one in five, are charismatics. Among the Protestant, 85% are "Born-again Christians" and belong to Evangelical or Pentecostal churches. Among the smallest groups, less than 4%, are Jewish.

</doc>
<doc id="56121" url="http://en.wikipedia.org/wiki?curid=56121" title="Market liquidity">
Market liquidity

In business, economics or investment, market liquidity is a market's ability to facilitate the purchase or sale of an asset without causing drastic change in the asset's price. Equivalently, an asset's market liquidity (or simply "an asset's liquidity") is the asset's ability to sell quickly without having to reduce its price very much. Liquidity is about how big the trade-off is between the speed of the sale and the price it can be sold for. In a liquid market, the trade-off is mild: selling quickly will not reduce the price much. In a relatively illiquid market, selling it quickly will require cutting its price by some amount.
Money, or cash, is the most liquid asset, because it can be "sold" for goods and services instantly with no loss of value. There is no wait for a suitable buyer of the cash. There is no trade-off between speed and value. It can be used immediately to perform economic actions like buying, selling, or paying debt, meeting immediate wants and needs.
If an asset is moderately (or very) liquid, it has moderate (or high) liquidity. In an alternative definition, liquidity can mean the amount of highly liquid assets. If a business has moderate liquidity, it has a moderate amount of very liquid assets. If a business has sufficient liquidity, it has a sufficient amount of very liquid assets and the ability to meet its payment obligations.
An act of exchanging a less liquid asset for a more liquid asset is called liquidation. Often liquidation is trading the less liquid asset for cash, also known as selling it. An asset's liquidity can change. For the same asset, its liquidity can change through time or between different markets, such as in different countries. The change in the asset's liquidity is just based on the market liquidity for the asset at the particular time or in the particular country, etc. The liquidity of a product can be measured as how often it is bought and sold.
Liquidity is defined formally in many accounting regimes and has in recent years been more strictly defined. For instance, the US Federal Reserve intends to apply quantitative liquidity requirements based on Basel III liquidity rules as of fiscal 2012. Bank directors will also be required to know of, and approve, major liquidity risks personally. Other rules require diversifying counterparty risk and portfolio stress testing against extreme scenarios, which tend to identify unusual market liquidity conditions and avoid investments that are particularly vulnerable to sudden liquidity shifts.
Overview.
A liquid asset has some or all of the following features: It can be sold rapidly, with minimal loss of value, any time within market hours. The essential characteristic of a liquid market is that there are always ready and willing buyers and sellers. It is similar to, but distinct from, market depth, which relates to the trade-off between quantity being sold and the price it can be sold for, rather than the liquidity trade-off between speed of sale and the price it can be sold for. A market may be considered both deep and liquid if there are ready and willing buyers and sellers in large quantities.
An illiquid asset is an asset which is not readily salable (without a drastic price reduction, and sometimes not at any price) due to uncertainty about its value or the lack of a market in which it is regularly traded. The mortgage-related assets which resulted in the subprime mortgage crisis are examples of illiquid assets, as their value was not readily determinable despite being secured by real property. Before the crisis, they had moderate liquidity because it was believed that their value was generally known.
Speculators and market makers are key contributors to the liquidity of a market, or asset. Speculators and market makers are individuals or institutions that seek to profit from anticipated increases or decreases in a particular market price. By doing this, they provide the capital needed to facilitate the liquidity. The risk of illiquidity need not apply only to individual investments: whole portfolios are subject to market risk. Financial institutions and asset managers that oversee portfolios are subject to what is called "structural" and "contingent" liquidity risk. Structural liquidity risk, sometimes called funding liquidity risk, is the risk associated with funding asset portfolios in the normal course of business. Contingent liquidity risk is the risk associated with finding additional funds or replacing maturing liabilities under potential, future stressed market conditions. When a central bank tries to influence the liquidity (supply) of money, this process is known as open market operations.
Effect on asset values.
The market liquidity of assets affects their prices and expected returns. Theory and empirical evidence suggests that investors require higher return on assets with lower market liquidity to compensate them for the higher cost of trading these assets. That is, for an asset with given cash flow, the higher its market liquidity, the higher its price and the lower is its expected return. In addition, risk-averse investors require higher expected return if the asset’s market-liquidity risk is greater. This risk involves the exposure of the asset return to shocks in overall market liquidity, the exposure of the asset own liquidity to shocks in market liquidity and the effect of market return on the asset’s own liquidity. Here too, the higher the liquidity risk, the higher the expected return on the asset or the lower is its price.
One example of this is comparison of assets with and without a liquid secondary market. The liquidity discount is the reduced promised yield or expected return for such assets, like the difference between newly issued U.S. Treasury bonds compared to off the run treasuries with the same term to maturity. Initial buyers know that other investors are less willing to buy off-the-run treasuries, so the newly issued bonds have a higher price (and hence lower yield).
Futures.
In the futures markets, there is no assurance that a liquid market may exist for offsetting a commodity contract at all times. Some future contracts and specific delivery months tend to have increasingly more trading activity and have higher liquidity than others. The most useful indicators of liquidity for these contracts are the trading volume and open interest.
There is also dark liquidity, referring to transactions that occur off-exchange and are therefore not visible to investors until after the transaction is complete. It does not contribute to public price discovery.
Banking.
In banking, liquidity is the ability to meet obligations when they come due without incurring unacceptable losses. Managing liquidity is a daily process requiring bankers to monitor and project cash flows to ensure adequate liquidity is maintained. Maintaining a balance between short-term assets and short-term liabilities is critical. For an individual bank, clients' deposits are its primary liabilities (in the sense that the bank is meant to give back all client deposits on demand), whereas reserves and loans are its primary assets (in the sense that these loans are owed to the bank, not by the bank). The investment portfolio represents a smaller portion of assets, and serves as the primary source of liquidity. Investment securities can be liquidated to satisfy deposit withdrawals and increased loan demand. Banks have several additional options for generating liquidity, such as selling loans, borrowing from other banks, borrowing from a central bank, such as the US Federal Reserve bank, and raising additional capital. In a worst-case scenario, depositors may demand their funds when the bank is unable to generate adequate cash without incurring substantial financial losses. In severe cases, this may result in a bank run. Most banks are subject to legally mandated requirements intended to help banks avoid a liquidity crisis.
Banks can generally maintain as much liquidity as desired because bank deposits are insured by governments in most developed countries. A lack of liquidity can be remedied by raising deposit rates and effectively marketing deposit products. However, an important measure of a bank's value and success is the cost of liquidity. A bank can attract significant liquid funds. Lower costs generate stronger profits, more stability, and more confidence among depositors, investors, and regulators.
Stock market.
In the market, liquidity has a slightly different meaning, although still tied to how easily assets, in this case shares of stock, can be converted to cash. The market for a stock is said to be liquid if the shares can be rapidly sold and the act of selling has little impact on the stock's price. Generally, this translates to where the shares are traded and the level of interest that investors have in the company. Another way to judge liquidity in a company's stock is to look at the bid/ask spread. For liquid stocks, such as Microsoft or General Electric, the spread is often just a few pennies - much less than 1% of the price. For illiquid stocks, the spread can be much larger, amounting to a few percent of the trading. price. In today's stock market, high-frequency trading firms are said to contribute to nearly 50% of all liquidity.
Liquidity positively impacts the stock market. When stock prices rise, it is said to be due to a confluence of extraordinarily high levels of liquidity on household and business balance sheets, combined with a simultaneous normalization of liquidity preferences. On the margin, this drives a demand for equity investments.
Proxies.
One way to calculate the liquidity of the banking system of a country is to divide liquid assets to short term liabilities.

</doc>
<doc id="56122" url="http://en.wikipedia.org/wiki?curid=56122" title="Sorghum">
Sorghum

Sorghum is a genus of plants in the grass family. Most species are native to Australia, with some extending to Africa, Asia, Mesoamerica, and certain islands in the Indian and Pacific Oceans.
One species is grown for grain and many of which are used as fodder plants, either cultivated or as part of pasture. The plants are cultivated in warm climates worldwide and naturalized in many places. "Sorghum" is in the subfamily Panicoideae and the tribe Andropogoneae (the tribe of big bluestem and sugarcane).
Cultivation and uses.
One species, "Sorghum bicolor", native to Africa with many cultivated forms now, is an important crop worldwide, used for food (as grain and in sorghum syrup or "sorghum molasses"), animal fodder, the production of alcoholic beverages, and biofuels. Most varieties are drought- and heat-tolerant, and are especially important in arid regions, where the grain is one of the staples for poor and rural people. These varieties form important components of pastures in many tropical regions. "S. bicolor" is an important food crop in Africa, Central America, and South Asia, and is the "fifth-most important cereal crop grown in the world".
Some species of sorghum can contain levels of hydrogen cyanide, hordenine, and nitrates lethal to grazing animals in the early stages of the plants' growth. When stressed by drought or heat, plants can also contain toxic levels of cyanide and/or nitrates at later stages in growth.
Another "Sorghum" species, Johnson grass ("S. halapense"), is classified as an invasive species in the US by the Department of Agriculture.
Broomcorn.
"S. vulgare" var. "technicum" is commonly called broomcorn. An annual grass like other Sorghums, it grows 6 to tall, although dwarf varieties are only 3 to in height. The upper peduncle is normally 8 to long, topped by a branched inflorescence or panicle, from which the seed-bearing fibers originate. The fibers are usually 12 to long, but can be up to 36 in long; they are branched toward the tip where the flowers and seed grow. The seeds number about 30,000/lb (70,000/kg), with feed value similar to oats. A ton of the fibrous panicle makes 900 to 1200 brooms.
Plants selected for long-panicle branches probably originated in central Africa, but the variety was known to be used for broom-making in the Mediterranean in the Middle Ages. It was first described in Italy in the late 1500s.
Diversity.
many species once considered part of "Sorghum" but now considered better suited to other genera: "Andropogon Arthraxon Bothriochloa Chrysopogon Cymbopogon Danthoniopsis Dichanthium Diectomis Diheteropogon Exotheca Hyparrhenia Hyperthelia Monocymbium Parahyparrhenia Pentameris Pseudosorghum Schizachyrium Sorghastrum"

</doc>
<doc id="56124" url="http://en.wikipedia.org/wiki?curid=56124" title="Lakota people">
Lakota people

The Lakȟóta people (pronounced ]; also known as Teton, Thítȟuŋwaŋ ("prairie dwellers"), and Teton Sioux ("snake, or enemy") are an indigenous people of the Great Plains of North America. They are part of a confederation of seven related Sioux tribes, the Očhéthi Šakówiŋ or seven council fires, and speak Lakota, one of the three major dialects of the Sioux language.
The Lakota are the westernmost of the three Siouan language groups, occupying lands in both North and South Dakota. The seven bands or "sub-tribes" of the Lakota are:
Notable Lakota persons include Tȟatȟáŋka Íyotake (Sitting Bull) from the Húnkpapȟa band; Touch the Clouds from the Miniconjou band; and, Tȟašúŋke Witkó (Crazy Horse), Maȟpíya Lúta (Red Cloud), Heȟáka Sápa (Black Elk), Siŋté Glešká (Spotted Tail), and Billy Mills from the Oglala band.
History.
Siouan language speakers may have originated in the lower Mississippi River region and then migrated to or originated in the Ohio Valley. They were agriculturalists and may have been part of the Mound Builder civilization during the 9th–12th centuries CE. In the late 16th and early 17th centuries, Dakota-Lakota-Nakota speakers lived in the upper Mississippi Region in present day Minnesota, Wisconsin, Iowa, and the Dakotas. Conflicts with Anishnaabe and Cree peoples pushed the Lakota west onto the Great Plains in the mid- to late-17th century.
Early Lakota history is recorded in their Winter counts (Lakota: "waníyetu wówapi"), pictorial calendars painted on hides or later recorded on paper. The Battiste Good winter count records Lakota history back to 900 CE, when White Buffalo Calf Woman gave the Lakota people the White Buffalo Calf Pipe.
Around 1730, Cheyenne people introduced the Lakota to horses, called "šuŋkawakaŋ" ("dog [of] power/mystery/wonder"). After their adoption of horse culture, Lakota society centered on the buffalo hunt on horseback. The total population of the Sioux (Lakota, Santee, Yankton, and Yanktonai) was estimated at 28,000 by French explorers in 1660. The Lakota population was first estimated at 8,500 in 1805, growing steadily and reaching 16,110 in 1881. The Lakota were, thus, one of the few Native American tribes to increase in population in the 19th century. The number of Lakota has now increased to about 70,000, of whom about 20,500 still speak the Lakota language.
After 1720, the Lakota branch of the Seven Council Fires split into two major sects, the Saône who moved to the Lake Traverse area on the South Dakota–North Dakota–Minnesota border, and the Oglála-Sičháŋǧu who occupied the James River valley. However, by about 1750 the Saône had moved to the east bank of the Missouri River, followed 10 years later by the Oglála and Brulé (Sičháŋǧu).
The large and powerful Arikara, Mandan, and Hidatsa villages had long prevented the Lakota from crossing the Missouri. However, the great smallpox epidemic of 1772–1780 destroyed three-quarters of these tribes. The Lakota crossed the river into the drier, short-grass prairies of the High Plains. These newcomers were the Saône, well-mounted and increasingly confident, who spread out quickly. In 1765, a Saône exploring and raiding party led by Chief Standing Bear discovered the Black Hills (the "Paha Sapa"), then the territory of the Cheyenne. Ten years later, the Oglála and Brulé also crossed the river. In 1776, the Lakota defeated the Cheyenne, who had earlier taken the region from the Kiowa. The Cheyenne then moved west to the Powder River country, and the Lakota made the Black Hills their home.
Initial United States contact with the Lakota during the Lewis and Clark Expedition of 1804–1806 was marked by a standoff. Lakota bands refused to allow the explorers to continue upstream, and the expedition prepared for battle, which never came. Nearly half a century later, after the United States Army had built Fort Laramie without permission on Lakota land, the Fort Laramie Treaty of 1851 was negotiated to protect travelers on the Oregon Trail. The Cheyenne and Lakota had previously attacked emigrant parties in a competition for resources, and also because some settlers had encroached on their lands. The Fort Laramie Treaty acknowledged Lakota sovereignty over the Great Plains in exchange for free passage on the Oregon Trail for "as long as the river flows and the eagle flies".
The United States government did not enforce the treaty restriction against unauthorized settlement. Lakota and other bands attacked settlers and even emigrant trains, causing public pressure on the US Army to punish the hostiles. On September 3, 1855, 700 soldiers under American General William S. Harney avenged the Grattan Massacre by attacking a Lakota village in Nebraska, killing about 100 men, women, and children. A series of short "wars" followed, and in 1862–1864, as refugees from the "Dakota War of 1862" in Minnesota fled west to their allies in Montana and Dakota Territory. Increasing illegal settlement after the American Civil War caused war once again.
The Black Hills were considered sacred by the Lakota, and they objected to mining. In 1868, the United States signed the Fort Laramie Treaty of 1868, exempting the Black Hills from all white settlement forever. Four years later gold was discovered there, and prospectors descended on the area.
The attacks on settlers and miners were met by military force conducted by army commanders such as Lieutenant Colonel George Armstrong Custer. General Philip Sheridan encouraged his troops to hunt and kill the buffalo as a means of "destroying the Indians' commissary."
The allied Lakota and Arapaho bands and the unified Northern Cheyenne were involved in much of the warfare after 1860. They fought a successful delaying action against General George Crook's army at the Battle of the Rosebud, preventing Crook from locating and attacking their camp, and a week later defeated the U.S. 7th Cavalry in 1876 at the Battle of the Greasy Grass. Custer attacked a camp of several tribes, much larger than he realized. Their combined forces, led by Chief Crazy Horse killed 258 soldiers, wiping out the entire Custer battalion in the Battle of the Little Bighorn, and inflicting more than 50% casualties on the regiment.
Their victory over the U.S. Army would not last, however. The US Congress authorized funds to expand the army by 2500 men. The reinforced US Army defeated the Lakota bands in a series of battles, finally ending the Great Sioux War in 1877. The Lakota were eventually confined onto reservations, prevented from hunting buffalo and forced to accept government food distribution.
In 1877 some of the Lakota bands signed a treaty that ceded the Black Hills to the United States; however, the nature of this treaty and its passage were controversial. The number of Lakota leaders that actually backed the treaty is highly disputed. Low-intensity conflicts continued in the Black Hills.. Fourteen years later, Sitting Bull was killed at Standing Rock reservation on December 15, 1890. The US Army attacked Spotted Elk (aka Bigfoot), Mnicoujou band of Lakota at the Wounded Knee Massacre on December 29, 1890 at Pine Ridge.
Today, the Lakota are found mostly in the five reservations of western South Dakota: Rosebud Indian Reservation (home of the Upper Sičhánǧu or Brulé), Pine Ridge Indian Reservation (home of the Oglála), Lower Brule Indian Reservation (home of the Lower Sičhaŋǧu), Cheyenne River Indian Reservation (home of several other of the seven Lakota bands, including the Mnikȟówožu, Itázipčho, Sihásapa and Oóhenumpa), and Standing Rock Indian Reservation (home of the Húŋkpapȟa), also home to people from many bands. Lakota also live on the Fort Peck Indian Reservation in northeastern Montana, the Fort Berthold Indian Reservation of northwestern North Dakota, and several small reserves in Saskatchewan and Manitoba. Their ancestors fled to "Grandmother's [i.e. Queen Victoria's] Land" (Canada) during the Minnesota or Black Hills War.
Large numbers of Lakota live in Rapid City and other towns in the Black Hills, and in metro Denver. Lakota elders joined the Unrepresented Nations and Peoples Organization (UNPO) to seek protection and recognition for their cultural and land rights.
Government.
United States.
Legally and by treaty a semi-autonomous "nation" within the United States, the Lakota Sioux are represented locally by officials elected to councils for the several reservations and communities in the Dakotas, Minnesota, Nebraska. They are represented on the state and national level by the elected officials from the political districts of their respective states and Congressional Districts. Band or reservation members living both on and off the individual reservations are eligible to vote in periodic elections for that reservation. Each reservation has a unique local government style and election cycle based on its own constitution or articles of incorporation. Most follow a multi-member tribal council model with a chairman or president elected directly by the voters.
Tribal governments have significant leeway, as semi-autonomous political entities, in deviating from state law (e.g. Indian gaming.) They are ultimately subject to supervisory oversight by the United States Congress and executive regulation through the Bureau of Indian Affairs. The nature and legitimacy of those relationships continue to be a matter of dispute.
Canada.
There are nine bands of Dakota and Lakota in Manitoba and southern Saskatchewan, with a total of 6,000 registered members. They are recognized as First Nations but are not considered "treaty Indians". As First Nations they receive rights and entitlements through the Indian and Northern Affairs Canada department. However as they are not recognized as treaty Indians, they did not participate in the land settlement and natural resource revenues. The Dakota rejected a $60 million land rights settlement in 2008.
Independence movement.
Beginning in 1974, some Lakota activists have taken steps to become independent from the United States, in an attempt to form their own fully independent nation. These steps have included drafting their own "declaration of continuing independence" and using Constitutional and International Law to solidify their legal standing.
A 1980 U.S. Supreme Court decision awarded $122 million to eight bands of Sioux Indians as compensation for land claims, but the court did not award land. The Lakota have refused the settlement.
In September 2007, the United Nations passed a non-binding Resolution on the Rights of Indigenous Peoples. Canada, the United States, Australia and New Zealand refused to sign.
On December 20, 2007, a group of Lakota under the name Lakota Freedom Delegation traveled to Washington D.C. to announce a withdrawal of the Lakota Sioux from all treaties with the United States government. These activists had no standing under any elected BIA tribal government. The group claimed official standing under the traditional Lakota Treaty Councils, representing the traditional "Tiyóšpaye" (matriarchal family units). These have been the traditional form of Lakota governance.
Longtime political activist Russell Means said, "We have 33 treaties with the United States that they have not lived by." He was part of the delegation's declaring the Lakota a sovereign nation with property rights over thousands of square miles in South Dakota, North Dakota, Nebraska, Wyoming and Montana. The group stated that they do not act for or represent the tribal governments set up by the BIA or those Lakota who support the BIA system of government.
The Lakota Freedom Delegation did not include any elected leaders from any of the tribes. Russell Means had previously run for president of the Oglala Sioux tribe and twice been defeated. Several elected BIA tribal governments issued statements distancing themselves from the independence declaration, with some saying they were watching the independent movement closely. Although some Indigenous nations and groups around the world made statements in support, no elected Lakota tribal governments endorsed the declaration.
In January 2008, the Lakota Freedom Delegation split into two groups. One group was led by "Canupa Gluha Mani" (Duane Martin Sr.). He is a leader of "Cante Tenza", the traditional Strongheart Warrior Society, that has included leaders such as Sitting Bull and Crazy Horse. This group is called "Lakota Oyate". The other group is called the "Republic of Lakotah" and is led by Russell Means. In December 2008, Lakota Oyate received the support and standing of the traditional treaty council of the Oglala Tiospayes.
Current Activism.
The Lakota People made national news when NPR's investigative story aired. It exposed what many critics consider to be the "kidnapping" of Lakota children from their homes by the state of South Dakota's Department of Social Services (D.S.S.). Lakota activists such as Madonna Thunder Hawk and Chase Iron Eyes, along with the , have alleged that Lakota grandmothers are illegally denied the right to foster their own grandchildren. They are currently working to redirect federal funding away from the state of South Dakota's D.S.S. to new tribal foster care programs. This would be an historic shift away from the state's traditional control over Lakota foster children.
Ethnonyms.
The name "Lakota" comes from the Lakota autonym, "Lakota" "feeling affection, friendly, united, allied". The early French historic documents did not distinguish a separate Teton division, instead grouping them with other "Sioux of the West," Santee and Yankton bands.
The names "Teton" and "Tetuwan" come from the Lakota name "thítȟuŋwaŋ", the meaning of which is obscure. This term was used to refer to the Lakota by non-Lakota Sioux groups. Other derivations include: ti tanka, Tintonyanyan, Titon, Tintonha, Thintohas, Tinthenha, Tinton, Thuntotas, Tintones, Tintoner, Tintinhos, Ten-ton-ha, Thinthonha, Tinthonha, Tentouha, Tintonwans, Tindaw, Tinthow, Atintons, Anthontans, Atentons, Atintans, Atrutons, Titoba, Tetongues, Teton Sioux, Teeton, Ti toan, Teetwawn, Teetwans, Ti-t’-wawn, Ti-twans, Tit’wan, Tetans, Tieton, and Teetonwan.
Early French sources call the Lakota "Sioux" with an additional modifier, such as Sioux of the West, West Schious, Sioux des prairies, Sioux occidentaux, Sioux of the Meadows, Nadooessis of the Plains, Prairie Indians, Sioux of the Plain, Maskoutens-Nadouessians, Mascouteins Nadouessi, and Sioux nomades.
Today many of the tribes continue to officially call themselves "Sioux". In the 19th and 20th centuries, this was the name which the US government applied to all Dakota/Lakota people. However, some of the tribes have formally or informally adopted traditional names: the Rosebud Sioux Tribe is also known as the Sičháŋǧu Oyáte (Brulé Nation), and the Oglala often use the name Oglála Lakȟóta Oyáte, rather than the English "Oglala Sioux Tribe" or OST. (The alternate English spelling of Ogallala is deprecated, even though it is closer to the correct pronunciation.) The Lakota have names for their own subdivisions. The Lakota also are Western of the three Sioux groups, occupying lands in both North and South Dakota.
Reservations.
Today, one half of all enrolled Sioux live off the Reservation.
Lakota reservations recognized by the U.S. government include:
Some Lakota also live on other Sioux reservations in eastern South Dakota, Minnesota, and Nebraska:
In addition several Lakota live on Wood Mountain Indian Reserve often Wood Mountain First Nation northwest of Wood Mountain Post now a Saskatchewan historic site.

</doc>
<doc id="56125" url="http://en.wikipedia.org/wiki?curid=56125" title="Sioux">
Sioux

The Sioux are a Native American and First Nations people in North America. The term can refer to any ethnic group within the Great Sioux Nation or any of the nation's many language dialects. The Sioux comprise three major divisions based on Siouan dialect and subculture: the Santee, the Yankton-Yanktonai, and the Lakota.
The Santee ("Isáŋyathi"; "Knife"), also called Eastern Dakota, reside in the extreme east of the Dakotas, Minnesota and northern Iowa. The Yankton and Yanktonai ("Iháŋktȟuŋwaŋ" and "Iháŋktȟuŋwaŋna"; "Village-at-the-end" and "Little village-at-the-end"), collectively also referred to as the Western Dakota or by the endonym "Wičhíyena", reside in the Minnesota River area. They are considered to be the middle Sioux, and have in the past been erroneously classified as Nakota. The Lakota, also called Teton ("Thítȟuŋwaŋ"; possibly "Dwellers on the prairie"), are the westernmost Sioux, known for their hunting and warrior culture.
Today, the Sioux maintain many separate tribal governments scattered across several reservations, communities, and reserves in North Dakota, South Dakota, Nebraska, Minnesota, and Montana in the United States; and Manitoba and southern Saskatchewan in Canada.
Etymology.
The name "Sioux" is an abbreviated form of "Nadouessioux" borrowed into Canadian French from "Nadoüessioüak" from the early Odawa exonym: "naadowesiwag" "Sioux". Jean Nicolet recorded the use in 1640. The Proto-Algonquian form "*na·towe·wa", meaning "Northern Iroquoian", has reflexes in several daughter languages that refer to a small rattlesnake (massasauga, "Sistrurus"). This information was interpreted by some that the Odawa borrowing was an insult. However, this Proto-Algonquian term most likely was ultimately derived from a form "*-a·towe·", meaning simply "to speak a foreign language", which would make it similar to the etymology of the Greek "Barbarian". Later this was extended in meaning in some Algonquian languages to refer to the massasauga. Thus, contrary to many accounts, the old Odawa word "naadowesiwag" did not equate the Sioux with snakes. This is not confirmed though, since usage over the previous decades has led to this term having negative connotations to those tribes to which it refers. This would explain why many tribes have rejected this term as an exonym. One source states that the name "Sioux" derives from a Chippewa word meaning "little snake"; Chippewa, or Ojibwa, is a dialectic variant of Odawa.
Some of the tribes have formally or informally adopted traditional names: the Rosebud Sioux Tribe is also known as the "Sičháŋǧu Oyáte", and the Oglala often use the name "Oglála Lakȟóta Oyáte", rather than the English "Oglala Sioux Tribe" or OST. The alternative English spelling of Ogallala is considered improper.
Očhéthi Šakówiŋ.
The historical Sioux referred to the Great Sioux Nation as the Očhéthi Šakówiŋ (pronounced ]), meaning "Seven Council Fires". Each fire was a symbol of an oyate (people or nation). The seven nations that comprise the Sioux are: Bdewákaŋthuŋwaŋ (Mdewakanton), Waȟpéthuŋwaŋ (Wahpeton), Waȟpékhute (Wahpekute), Sisíthuŋwaŋ (Sisseton), the Iháŋkthuŋwaŋ (Yankton), Iháŋkthuŋwaŋna (Yanktonai), and the Thítȟuŋwaŋ (Teton or Lakota). The Seven Council Fires would assemble each summer to hold council, renew kinships, decide tribal matters, and participate in the Sun Dance. The seven divisions would select four leaders known as Wičháša Yatápika from among the leaders of each division. Being one of the four leaders was considered the highest honor for a leader; however, the annual gathering meant the majority of tribal administration was cared for by the usual leaders of each division. The last meeting of the Seven Council Fires was in 1850.
Today the Teton, Santee (mixture of the four Dakota tribes) and the Minnesota Dakota, and Yankton/Yanktonai are usually known, respectively, as the Lakota, Eastern Dakota, or Western Dakota. In any of the three main dialects, "Lakota" or "Dakota" translate to mean "friend," or more properly, "ally." Usage of Lakota or Dakota may then refer to the alliance that once bound the Great Sioux Nation.
History.
First contacts with Europeans.
The Dakota are first recorded to have resided at the source of the Mississippi River during the seventeenth century. By 1700 some had migrated to present-day South Dakota. Late in the 17th century, the Dakota entered into an alliance with French merchants. The French were trying to gain advantage in the struggle for the North American fur trade against the English, who had recently established the Hudson's Bay Company.
Relationship with French traders.
The first recorded encounter between the Sioux and the French occurred when Radisson and Groseilliers reached what is now Wisconsin during the winter of 1659-60. Later visiting French traders and missionaries included Claude-Jean Allouez, Daniel Greysolon Duluth, and Pierre-Charles Le Sueur who wintered with Dakota bands in early 1700. In 1736 a group of Sioux killed Jean Baptiste de La Vérendrye and twenty other men on an island in Lake of the Woods. However, trade with the French continued until after the French gave up North America in 1763.
Relationship with Pawnees.
Author and historian Mark van de Logt wrote: "Although military historians tend to reserve the concept of “total war” for conflicts between modern industrial nations, the term nevertheless most closely approaches the state of affairs between the Pawnees and the Sioux and Cheyennes. Both sides directed their actions not solely against warrior-combatants but against the people as a whole. Noncombatants were legitimate targets. ... It is within this context that the military service of the Pawnee Scouts must be viewed."
The battle of Massacre Canyon on August 5, 1873, was the last major battle between the Pawnee and the Sioux.
Dakota War of 1862.
By 1862, shortly after a failed crop the year before and a winter starvation, the federal payment was late. The local traders would not issue any more credit to the Santee and one trader, Andrew Myrick, went so far as to say, "If they're hungry, let them eat grass." On August 17, 1862 the Dakota War began when a few Santee men murdered a white farmer and most of his family. They inspired further attacks on white settlements along the Minnesota River. The Santee attacked the trading post. Later settlers found Myrick among the dead with his mouth stuffed full of grass.
On November 5, 1862 in Minnesota, in courts-martial, 303 Santee Sioux were found guilty of rape and murder of hundreds of American settlers. They were sentenced to be hanged. No attorneys or witness were allowed as a defense for the accused, and many were convicted in less than five minutes of court time with the judge. President Abraham Lincoln commuted the death sentence of 284 of the warriors, while signing off on the execution of 38 Santee men by hanging on December 26, 1862 in Mankato, Minnesota. It was the largest mass-execution in U.S. history.
Afterwards, the US suspended treaty annuities to the Dakota for four years and awarded the money to the white victims and their families. The men remanded by order of President Lincoln were sent to a prison in Iowa, where more than half died.
During and after the revolt, many Santee and their kin fled Minnesota and Eastern Dakota to Canada, or settled in the James River Valley in a short-lived reservation before being forced to move to Crow Creek Reservation on the east bank of the Missouri. A few joined the Yanktonai and moved further west to join with the Lakota bands to continue their struggle against the United States military.
Others were able to remain in Minnesota and the east, in small reservations existing into the 21st century, including Sisseton-Wahpeton, Flandreau, and Devils Lake (Spirit Lake or Fort Totten) Reservations in the Dakotas. Some ended up in Nebraska, where the Santee Sioux Tribe today has a reservation on the south bank of the Missouri.
Those who fled to Canada now have descendants residing on nine small Dakota Reserves, five of which are located in Manitoba (Sioux Valley, Long Plain, Dakota Tipi, Birdtail Creek, and Oak Lake [Pipestone]) and the remaining four (Standing Buffalo, Moose Woods [White Cap], Round Plain [Wahpeton], and Wood Mountain) in Saskatchewan.
Red Cloud's War.
Red Cloud's War (also referred to as the Bozeman War) was an armed conflict between the Lakota and the United States Army in the Wyoming Territory and the Montana Territory from 1866 to 1868. The war was fought over control of the Powder River Country in north central Wyoming.
The war is named after Red Cloud, a prominent Sioux chief who led the war against the United States following encroachment into the area by the U.S. military. The war ended with the Treaty of Fort Laramie. The Sioux victory in the war led to their temporarily preserving their control of the Powder River country.
Great Sioux War of 1876–77.
The Great Sioux War comprised a series of battles between the Lakota and allied tribes such as the Cheyenne against the United States military. The earliest engagement was the Battle of Powder River, and the final battle was the Wolf Mountain. Included are the Battle of the Rosebud, Battle of the Little Bighorn, Battle of Warbonnet Creek, Battle of Slim Buttes, Battle of Cedar Creek, and the Dull Knife Fight. The Great Sioux War of 1876–77 was also known as the Black Hills War, and was centered on the Lakota tribes of the Sioux, although several natives believe that the primary target of the United States military was the Northern Cheyenne tribe. The series of battles occurred in Montana territory, Dakota territory, and Wyoming territory, and resulted in a victory for the United States military.
Wounded Knee Massacre.
The massacre at Wounded Knee Creek was the last major armed conflict between the Lakota and the United States. It was described as a " by General Nelson A. Miles in a letter to the Commissioner of Indian Affairs.
On December 29, 1890, five hundred troops of the U.S. 7th Cavalry, supported by four Hotchkiss guns (a lightweight artillery piece capable of rapid fire), surrounded an encampment of the Lakota bands of the Miniconjou and Hunkpapa with orders to escort them to the railroad for transport to Omaha, Nebraska.
By the time it was over, 25 troopers and more than 150 Lakota Sioux lay dead, including men, women, and children. It remains unknown which side was responsible for the first shot; some of the soldiers are believed to have been the victims of "friendly fire" because the shooting took place at point-blank range in chaotic conditions. Around 150 Lakota are believed to have fled the chaos, many of whom may have died from hypothermia.
Reserves and First Nations.
Later in the 19th century, the railroads hired hunters to exterminate the buffalo herds, the Indians' primary food supply. The Santee and Lakota were forced to accept white-defined reservations in exchange for the rest of their lands, and domestic cattle and corn in exchange for buffalo. They became dependent upon annual federal payments guaranteed by treaty.
In Minnesota, the treaties of Traverse des Sioux and Mendota in 1851 left the Sioux with a reservation twenty miles (32 km) wide on each side of the Minnesota River.
Today, one half of all enrolled Sioux in the United States live off the reservation. Enrolled members in any of the Sioux tribes in the United States are required to have ancestry that is at least 1/4 degree Sioux (the equivalent to one grandparent).
In Canada, the Canadian government recognizes the tribal community as First Nations. The land holdings of the these First Nations are called Indian Reserves.
20th century activism.
Wounded Knee incident.
Beginning in the late 1960s, young Native Americans began to agitate for improved conditions, respect for their civil rights, and better programs in education and economic development. Dramatic protests were conceived and carried out, such as the occupation of Alcatraz Island in California.
The Wounded Knee incident began February 27, 1973 when the town of Wounded Knee, South Dakota was seized by followers of the American Indian Movement. The occupiers controlled the town for 71 days while various state and federal law enforcement agencies such as the Federal Bureau of Investigation and the United States Marshals Service laid siege. Two members of A.I.M. were killed by gunfire during the incident.
Republic of Lakota.
The "Lakota Freedom Delegation", a group of controversial Native American activists, declared on December 19, 2007 the Lakota were withdrawing from all treaties signed with the United States to regain sovereignty over their nation. One of the activists, Russell Means, claimed that the action is legal and cites Natural, International and U.S. law. The group considers Lakota to be a sovereign nation, although as yet the state is generally unrecognized. The proposed borders reclaim thousands of square kilometres of North and South Dakota, Wyoming, Nebraska and Montana.
Current activism.
The Lakota Sioux made national news when NPR's  investigative story aired. It exposed what many critics consider to be the "kidnapping" of Lakota children from their homes by the state of South Dakota's Department of Social Services (D.S.S.). Lakota activists such as Madonna Thunder Hawk and Chase Iron Eyes, along with the , have alleged that Lakota grandmothers are illegally denied the right to foster their own grandchildren. They are currently working to redirect federal funding away from the state of South Dakota's D.S.S. to new tribal foster care programs. This would be an historic shift away from the state's traditional control over Lakota foster children.
In early 2014 a Lakota group launched Mazacoin, a digital currency that is claimed to be the "national currency of the traditional Lakota Nation".
Political organization.
The historical political organization was based on individual participation and the cooperation of many to sustain the tribe’s way of life. Leaders were chosen based upon noble birth and demonstrations of chiefly virtues, such as bravery, fortitude, generosity, and wisdom.
Linguistics.
The Sioux comprise three closely related language groups:
The earlier linguistic three-way division of the Sioux language identified "Lakota", "Dakota", and "Nakota" as dialects of a single language, where Lakota = Teton, Dakota = Santee-Sisseton and Nakota = Yankton-Yanktonai. However, the latest studies show that Yankton-Yanktonai never used the autonym "Nakhóta", but pronounced their name roughly the same as the Santee (i.e. "Dakȟóta").
These later studies identify Assiniboine and Stoney as two separate languages, with Sioux being the third language. Sioux has three similar dialects: Lakota, Western Dakota (Yankton-Yanktonai) and Eastern Dakota (Santee-Sisseton). Assiniboine and Stoney speakers refer to themselves as "Nakhóta" or "Nakhóda" (cf. Nakota).
The term "Dakota" has also been applied by anthropologists and governmental departments to refer to all Sioux groups, resulting in names such as "Teton Dakota", "Santee Dakota", etc. This was mainly because of the misrepresented translation of the Ottawa word from which "Sioux" is derived.
Modern geographic divisions.
The Sioux maintain many separate tribal governments scattered across several reservations and communities in North America: in the Dakotas, Minnesota, Nebraska, and Montana in the United States; and in Manitoba, southern Saskatchewan and Alberta in Canada.
The earliest known European record of the Sioux identified them in Minnesota, Iowa, and Wisconsin. After the introduction of the horse in the early 18th century, the Sioux dominated larger areas of land—from present day Central Canada to the Platte River, from Minnesota to the Yellowstone River, including the Powder River country.
Santee (Isáŋyathi or Eastern Dakota).
The Santee migrated north and westward from the Southeast United States, first into Ohio, then to Minnesota. Some came up from the Santee River and Lake Marion, area of South Carolina. The Santee River was named after them, and some of their ancestors' ancient earthwork mounds have survived along the portion of the dammed-up river that forms Lake Marion. In the past, they were a Woodland people who thrived on hunting, fishing and farming.
Migrations of Anishinaabe/Chippewa (Ojibwe) people from the east in the 17th and 18th centuries, with muskets supplied by the French and British, pushed the Dakota further into Minnesota and west and southward. The US gave the name "Dakota Territory" to the northern expanse west of the Mississippi River and up to its headwaters.
Iháŋkthuŋwaŋ-Iháŋkthuŋwaŋna (Yankton-Yanktonai or Western Dakota).
The Iháŋkthuŋwaŋ-Iháŋkthuŋwaŋna, also known by the anglicized spelling Yankton (Iháŋkthuŋwaŋ: "End village") and Yanktonai (Iháŋkthuŋwaŋna: "Little end village") divisions consist of two bands or two of the seven council fires. According to "Nasunatanka" and "Matononpa" in 1880, the Yanktonai are divided into two sub-groups known as the Upper Yanktonai and the Lower Yanktonai (Hunkpatina).
They were involved in quarrying pipestone. The Yankton-Yanktonai moved into northern Minnesota. In the 18th century, they were recorded as living in the Mankato region of Minnesota.
Lakota (Teton or Thítȟuŋwaŋ).
The Sioux likely obtained horses sometime during the seventeenth century (although some historians date the arrival of horses in South Dakota to 1720, and credit the Cheyenne with introducing horse culture to the Lakota). The Teton (Lakota) division of the Sioux emerged as a result of this introduction. Dominating the northern Great Plains with their light cavalry, the western Sioux quickly expanded their territory further to the Rocky Mountains (which they call "Heska", "white mountains"). The Lakota once subsisted on the buffalo hunt, and on corn. They acquired corn mostly through trade with the eastern Sioux and their linguistic cousins, the Mandan and Hidatsa along the Missouri. The name Teton or Thítȟuŋwaŋ is archaic among the people, who prefer to call themselves "Lakȟóta".
Ethnic divisions.
The Sioux are divided into three ethnic groups, the larger of which are divided into sub-groups, and further branched into bands.
Today, many Sioux also live outside their reservations.
Notable Sioux.
Contemporary.
Contemporary Sioux people are listed under the tribes to which they belong.
Legacy.
A Manitoba Historical Plaque was erected at the Spruce Woods Provincial Park by the province to commemorate Assiniboin (Nakota) First Nation's role in Manitoba's heritage.
Further reading.
</dl>

</doc>
<doc id="56126" url="http://en.wikipedia.org/wiki?curid=56126" title="Battle of the Little Bighorn">
Battle of the Little Bighorn

The Battle of the Little Bighorn, known to Lakota as the Battle of the Greasy Grass, and commonly referred to as Custer's Last Stand, was an armed engagement between combined forces of the Lakota, Northern Cheyenne, and Arapaho tribes, against the 7th Cavalry Regiment of the United States Army. The battle, which occurred June 25–26, 1876, near the Little Bighorn River in eastern Montana Territory, was the most prominent action of the Great Sioux War of 1876. It was an overwhelming victory for the Lakota, Northern Cheyenne, and Arapaho, led by several major war leaders, including Crazy Horse and Chief Gall, inspired by the visions of Sitting Bull (Tȟatȟáŋka Íyotake). The U.S. 7th Cavalry, including the Custer Battalion, a force of 700 men led by George Armstrong Custer, suffered a severe defeat. Five of the 7th Cavalry's twelve companies were annihilated; Custer was killed, as were two of his brothers, a nephew, and a brother-in-law. The total U.S. casualty count, including scouts, was 268 dead and 55 injured.
Public response to the Great Sioux War varied at the time. The battle, and Custer's actions in particular, have been studied extensively by historians.
Background.
Tension between the native inhabitants of the Great Plains and the encroaching settlers resulted in a series of conflicts known as the Sioux Wars. Even though many agreed to relocate to ever-shrinking reservations, some resisted. In 1875, Sitting Bull created the Sun Dance alliance between the Lakota and the Cheyenne and a large number of "Agency Indians" who had slipped away from their reservations to join them. During a Sun Dance around June 5, 1876, on the Rosebud Creek in Montana, Sitting Bull reportedly had a vision of "soldiers falling into his camp like grasshoppers from the sky." At the same time, military officials were conducting a summer campaign to force the Lakota and Cheyenne back to their reservations, using infantry and cavalry in a three-pronged approach.
Col. John Gibbon's column of six companies (A, B, E, H, I, and K) of the 7th Infantry and four companies (F, G, H, and L) of the 2nd Cavalry marched east from Fort Ellis in western Montana on March 30, to patrol the Yellowstone River. Brig. Gen. George Crook's column of ten companies (A, B, C, D, E, F, G, I, L, and M) of the 3rd Cavalry, five (A, B, D, E, and I) of the 2nd Cavalry, two companies (D and F) of the 4th Infantry, and three companies (C, G, and H) of the 9th Infantry, moved north from Fort Fetterman in the Wyoming Territory on May 29, marching toward the Powder River area. Brig. Gen. Alfred Terry's column, including twelve companies (A, B, C, D, E, F, G, H, I, K, L, and M) of the 7th Cavalry under Lieutenant Colonel George Armstrong Custer's immediate command, Companies C and G of the 17th U.S. Infantry, and the Gatling gun detachment of the 20th Infantry departed westward from Fort Abraham Lincoln in the Dakota Territory on May 17. They were accompanied by teamsters and packers with 150 wagons and a large contingent of pack mules that reinforced Custer. Companies C, D, and I of the 6th U.S. Infantry, moved along the Yellowstone River from Fort Buford on the Missouri River to set up a supply depot, and joined Terry on May 29 at the mouth of the Powder River. They were later joined there by the steamboat "Far West", which was loaded with 200 tons of supplies from Fort Lincoln.
The coordination and planning began to go awry on June 17, 1876, when Crook's column retreated after the Battle of the Rosebud. Surprised and, according to some accounts, astonished by the unusually large numbers of Native Americans in the battle, Crook held the field at the end of the battle but felt compelled by his losses to pull back, regroup and wait for reinforcements. Unaware of Crook's battle, Gibbon and Terry proceeded, joining forces in early June near the mouth of the Rosebud Creek. They reviewed Terry's plan calling for Custer's regiment to proceed south along the Rosebud, while Terry and Gibbon's united forces would move in a westerly direction toward the Bighorn and Little Bighorn rivers. As this was the likely location of Native encampments, all army elements were to converge around June 26 or 27, attempting to engulf the Native Americans. On June 22, Terry ordered the 7th Cavalry, composed of 31 officers and 566 enlisted men under Custer, to begin a reconnaissance and pursuit along the Rosebud, with the prerogative to "depart" from orders upon seeing "sufficient reason." Custer had been offered the use of Gatling guns, but declined, believing they would slow his command.
While the Terry/Gibbon column was marching toward the mouth of the Little Bighorn, on the evening of June 24, Custer's scouts arrived at an overlook known as the Crow's Nest, 14 mi east of the Little Bighorn River. At sunrise on June 25, Custer's scouts reported they could see a massive pony herd and signs of the Native American village roughly 15 mi in the distance. After a night's march, the tired officer who was sent with the scouts could see neither, and when Custer joined them, he was also unable to make the sighting. Custer's scouts also spotted the regimental cooking fires that could be seen from 10 mi away, disclosing the regiment's position.
Custer contemplated a surprise attack against the encampment the following morning of June 26, but he then received a report informing him several hostiles had discovered the trail left by his troops. Assuming his presence had been exposed, Custer decided to attack the village without further delay. On the morning of June 25, Custer divided his 12 companies into three battalions in anticipation of the forthcoming engagement. Three companies were placed under the command of Major Marcus Reno (A, G, and M); and three were placed under the command of Capt. Frederick Benteen (H, D, and K). Five companies (C, E, F, I, and L) remained under Custer's immediate command. The 12th, Company B, under Capt. Thomas McDougall, had been assigned to escort the slower pack train carrying provisions and additional ammunition.
Unknown to Custer, the group of Native Americans seen on his trail were actually leaving the encampment on the Big Horn and did not alert the village. Custer's scouts warned him about the size of the village, with Mitch Bouyer reportedly saying, "General, I have been with these Indians for 30 years, and this is the largest village I have ever heard of." Custer's overriding concern was that the Native American group would break up and scatter. The command began its approach to the village at noon and prepared to attack in full daylight.
Prelude.
7th Cavalry organization.
The 7th Cavalry was created just after the American Civil War. Many men were veterans of the war, including most of the leading officers. A significant portion of the regiment had previously served four-and-a-half years at Ft. Riley, Kansas, during which time it fought one major engagement and numerous skirmishes, experiencing casualties of 36 killed and 27 wounded. Six other troopers had died of drowning and 51 from cholera epidemics.
Half of the 7th Cavalry's companies had just returned from 18 months of constabulary duty in the Deep South, having been recalled to Fort Abraham Lincoln to reassemble the regiment for the campaign. About 20 percent of the troopers had been enlisted in the prior seven months (139 of an enlisted roll of 718), were only marginally trained, and had no combat or frontier experience. A sizable number of these recruits were immigrants from Ireland, England and Germany, just as many of the veteran troopers had been before their enlistments. Archaeological evidence suggests that many of these troopers were malnourished and in poor physical condition, despite being the best-equipped and supplied regiment in the army.
Of the 45 officers and 718 troopers then assigned to the 7th Cavalry (including a second lieutenant detached from the 20th Infantry and serving in Company L), 14 officers (including the regimental commander, Col. Samuel D. Sturgis) and 152 troopers did not accompany the 7th during the campaign. The ratio of troops detached for other duty (approximately 22%) was not unusual for an expedition of this size, and part of the officer shortage was chronic, due to the Army's rigid seniority system: three of the regiment's 12 captains were permanently detached, and two had never served a day with the 7th since their appointment in July 1866. Three second lieutenant vacancies (in E, H, and L Companies) were also unfilled.
Military assumptions prior to the battle.
Number of Indian warriors.
As the Army moved into the field on its expedition, it was operating with incorrect assumptions as to the number of Indians it would encounter. The Army's assumptions were based on inaccurate information provided by the Indian Agents that no more than 800 hostiles were in the area. The Indian Agents based the 800 number on the number of Lakota led by Sitting Bull and other leaders off the reservation in protest of US Government policies. This was a correct estimate until several weeks before the battle, when the "reservation Indians" joined Sitting Bull's ranks for the summer buffalo hunt. However, the agents did not take into account the many thousands of "reservation Indians" who had "unofficially" left the reservation to join their "uncooperative non-reservation cousins led by Sitting Bull". The latter were those groups who had indicated that they were not going to cooperate with the US Government and live on reservation lands. Thus, Custer unknowingly faced thousands of Indians, in addition to the 800 non-reservation "hostiles". All Army plans were based on the incorrect numbers. While after the battle, Custer was severely criticized for not having accepted reinforcements and for dividing his forces, it must be understood that he had accepted the same official Government estimates of hostiles in the area which Terry and Gibbon also accepted. Historian James Donovan states that when Custer asked interpreter Fred Gerard for his opinion on the size of the opposition, he estimated the force at between 1,500 to 2,500 warriors.
Additionally, Custer was more concerned with preventing the escape of the Lakota and Cheyenne than with fighting them. From his own observation, as reported by his bugler John Martin (Martini), Custer assumed the warriors had been sleeping in on the morning of the battle, to which virtually every native account attested later, giving Custer a false estimate of what he was up against. When he and his scouts first looked down on the village from Crow's Nest across the Little Bighorn River, they could only see the herd of ponies. Looking from a hill 2.5 mi away after parting with Reno's command, Custer could observe only women preparing for the day, and young boys taking thousands of horses out to graze south of the village. Custer's Crow scouts told him it was the largest native village they had ever seen. When the scouts began changing back into their native dress right before the battle, Custer released them from his command. While the village was enormous in size, Custer thought there were far fewer warriors to defend the village. He assumed most of the warriors were still asleep in their tipis.
Finally, Custer may have assumed that in the event of his encountering Native Americans, his subordinate Benteen with the pack train would quickly come to his aid. Rifle volleys were a standard way of telling supporting units to come to another unit's aid. In a subsequent official 1879 Army investigation requested by Major Reno, the Reno Board of Inquiry (RCOI), Benteen and Reno's men testified that they heard distinct rifle volleys as late as 4:30 pm during the battle.
Custer had wanted to take a day and scout the village before attacking; however, when men went back after supplies dropped by the pack train, they discovered they were being back-trailed by Indians. Reports from his scouts also revealed fresh pony tracks from ridges overlooking his formation. It became apparent that the warriors in the village were either aware of or would soon be aware of his approach. Fearing that the village would break up into small bands that he would have to chase, Custer began to prepare for an immediate attack.
The role of Indian noncombatants in Custer's strategy.
Lt. Colonel George A. Custer's field strategy was designed to engage noncombatants at the encampments at the Battle of the Little Big Horn, so as to capture women, children, the elderly or disabled :297 to serve as hostages and human shields. Custer's battalions were poised to "ride into the camp and secure noncombatant hostages" and "forc[e] the warriors to surrender". Author Evan S. Connell observed that if Custer could occupy the village before widespread resistance developed, the Sioux and Cheyenne warriors "would be obliged to surrender, because if they started to fight, they would be shooting their own families.":312
Custer asserted in his book "My Life on the Plains", published just two years before the Battle of the Little Big Horn, that:
 "Indians contemplating a battle, either offensive or defensive, are always anxious to have their women and children removed from all danger…For this reason I decided to locate our [military] camp as close as convenient to [Chief Black Kettle's Cheyenne] village, knowing that the close proximity of their women and children, and their necessary exposure in case of conflict, would operate as a powerful argument in favor of peace, when the question of peace or war came to be discussed."
On Custer's decision to advance up the bluffs and descend on the village from the east, Lt. Edward Godfrey of Company K surmised:
 "[Custer] expected to find the squaws and children fleeing to the bluffs on the north, for in no other way do I account for his wide detour. He must have counted upon Reno's success, and fully expected the 'scatteration' of the non-combatants with the pony herds. The probable attack upon the families and capture of the herds were in that event counted upon to strike consternation in the hearts of the warriors, and were elements for success upon which General Custer fully counted".:379
The Sioux and Cheyenne fighters were acutely aware of the danger posed by the military engagement of noncombatants and that "even a semblance of an attack on the women and children" would draw the warriors back to the village, according to historian John S. Gray. Such was their concern that a "feint" by Capt. Yates' E and F Companies at the mouth of Medicine Tail Coulee (Minneconjou Ford) caused hundreds of warriors to disengage from the Reno valley fight and return to deal with the threat to the village.
Some authors and historians, based on archeological evidence and reviews of native testimony, speculate that Custer attempted to cross the river at a point they refer to as Ford D. According to Richard A.Fox, James Donovan, and others, Custer proceeded with a wing of his battalion (Yates' Troops E and F) north and opposite the Cheyenne circle at that crossing:176–77 which provided "access to the [women and children] fugitives.":306 Yates's force "posed an immediate threat to fugitive Indian families…" gathering at the north end of the huge encampment.:299then persisted in his efforts to "seize women and children" even as hundreds of warriors were massing around Keogh's wing on the bluffs. Yates' wing, descending to the Little Bighorn River at Ford D, encountered "light resistance",:297 undetected by the Indian forces ascending the bluffs east of the village.:298 Custer was almost within "striking distance of the refugees" before being repulsed by Indian defenders and forced back to Custer Ridge. That hypothesis, by Fox’s own admission, is not universally accepted,:173 and as a result of conflicting physical evidence and variations in differing Lakota accounts, Custer’s precise movements remain impossible to ascertain.
Battle engagements.
Reno's attack.
The first group to attack was Major Reno's second detachment (Companies A, G and M), conducted after receiving orders from Custer written out by Lt. William W. Cooke, as Custer's Crow scouts reported Sioux tribe members were alerting the village. Ordered to charge, Reno began that phase of the battle. The orders, made without accurate knowledge of the village's size, location, or the warriors' propensity to stand and fight, had been to pursue the Native Americans and "bring them to battle." Reno's force crossed the Little Bighorn at the mouth of what is today Reno Creek around 3:00 pm. They immediately realized that the Lakota and Northern Cheyenne were present "in force and not running away."
Reno advanced rapidly across the open field towards the northwest, his movements masked by the thick bramble of trees that ran along the southern banks of the Little Bighorn river. The same trees on his front right shielded his movements across the wide field over which his men rapidly rode, first with two approximately forty-man companies abreast and eventually with all three charging abreast. The trees also obscured Reno's view of the Native American village until his force had passed that bend on his right front and was suddenly within arrow shot of the village. The tepees in that area were occupied by the Hunkpapa Sioux. Neither Custer nor Reno had much idea of the length, depth and size of the encampment they were attacking, as the village was hidden by the trees. When Reno came into the open in front of the south end of the village, he sent his Arikara/Ree and Crow Indian scouts forward on his exposed left flank. Realizing the full extent of the village's width, Reno quickly suspected what he would later call "a trap" and stopped a few hundred yards short of the encampment.
He ordered his troopers to dismount and deploy in a skirmish line, according to standard army doctrine. In this formation, every fourth trooper held the horses for the troopers in firing position, with five to ten yards separating each trooper, officers to their rear and troopers with horses behind the officers. This formation reduced Reno's firepower by 25 percent. As Reno's men fired into the village and killed, by some accounts, several wives and children of the Sioux leader, Chief Gall (in Lakota, "Phizí"), mounted warriors began streaming out to meet the attack. With Reno's men anchored on their right by the impassable tree line and bend in the river, the Indians rode hard against the exposed left end of Reno's line. After about 20 minutes of long-distance firing, Reno had taken only one casualty, but the odds against him had risen (Reno estimated five to one) and Custer had not reinforced him. Trooper Billy Jackson reported that by then, the Indians had begun massing in the open area shielded by a small hill to the left of the Reno's line and to the right of the Indian village. From this position the Indians mounted an attack of more than 500 warriors against the left and rear of Reno's line, turning Reno's exposed left flank. They forced a hasty withdrawal into the timber along the bend in the river. Here the Indians pinned Reno and his men down and set fire to the brush to try to drive the soldiers out of their position.
After giving orders to mount, dismount and mount again, Reno told his men, "All those who wish to make their escape follow me," and led a disorderly rout across the river toward the bluffs on the other side. The retreat was immediately disrupted by Cheyenne attacks at close quarters. Later Reno reported that three officers and 29 troopers had been killed during the retreat and subsequent fording of the river, with another officer and 13–18 men missing. Most of these men were left behind in the timber, although many eventually rejoined the detachment. Reno's hasty retreat may have been precipitated by the death of Reno's Arikara Scout Bloody Knife, who had been shot in the head as he sat on his horse next to Reno, his blood and brains splattering the side of Reno's face.
Reno and Benteen on Reno Hill.
Atop the bluffs, known today as Reno Hill, Reno's shaken troops were joined by Captain Benteen's column (Companies D, H and K), arriving from the south. This force had been on a lateral scouting mission when it had been summoned by Custer's messenger, Italian bugler John Martin (Giovanni Martini) with the hand-written message "Benteen. Come on, Big Village, Be quick, Bring packs. P.S. Bring Packs.". Benteen's coincidental arrival on the bluffs was just in time to save Reno's men from possible annihilation. Their detachments were reinforced by McDougall's Company B and the pack train. The 14 officers and 340 troopers on the bluffs organized an all-around defense and dug rifle pits using whatever implements they had among them, including knives. This practice had become standard during the last year of the American Civil War, with both Union and Confederate troops utilizing knives, eating utensils, mess plates and pans, to dig effective battlefield fortifications.
Despite hearing heavy gunfire from the north, including distinct volleys at 4:20 pm, Benteen concentrated on reinforcing Reno's badly wounded and hard-pressed detachment, rather than continuing on toward Custer. Benteen's apparent reluctance to reach Custer prompted later criticism that he had failed to follow orders. Around 5:00 pm, Capt. Thomas Weir and Company D moved out to make contact with Custer. They advanced a mile, to what is today Weir Ridge or Weir Point, and could see in the distance Native warriors on horseback shooting at objects on the ground. By this time, roughly 5:25 pm, Custer's battle may have concluded. The conventional historical understanding is that what Weir witnessed was most likely warriors killing the wounded soldiers and shooting at dead bodies on the "Last Stand Hill" at the northern end of the Custer battlefield. Some contemporary historians have suggested that what Weir witnessed was a fight on what is now called Calhoun Hill. The destruction of Keogh's battalion may have begun with the collapse of L, I and C Company (half of it) following the combined assaults led by Crazy Horse, White Bull, Hump, Chief Gall and others.:240 Other Native accounts contradict this understanding, however, and the time element remains a subject of debate. The other entrenched companies eventually followed Weir by assigned battalions, first Benteen, then Reno, and finally the pack train. Growing Native attacks around Weir Ridge forced all seven companies to return to the bluff before the pack train, with the ammunition, had moved even a quarter mile. There, they remained pinned down for another day, but the Natives were unable to breach this tightly held position.
Benteen displayed calmness and courage by exposing himself to Native fire and was hit in the heel of his boot by a Native bullet. At one point, he personally led a counterattack to push back Natives who had continued to crawl through the grass closer to the soldier's positions.
Custer's fight.
The precise details of Custer's fight are largely conjectural since none of his men (the five companies under his immediate command) survived the battle. The accounts of surviving Indians are conflicting and unclear.
While the gunfire heard on the bluffs by Reno and Benteen's men was probably from Custer's fight, the soldiers on Reno Hill were unaware of what had happened to Custer until General Terry's arrival on June 27. They were reportedly stunned by the news. When the army examined the Custer battle site, soldiers could not determine fully what had transpired. Custer's force of roughly 210 men had been engaged by the Lakota and Northern Cheyenne about 3.5 miles (6 km) to the north. Evidence of organized resistance included apparent breastworks made of dead horses on Custer Hill. By this time, the Lakota and Cheyenne had already removed most of their dead from the field. The soldiers identified the 7th Cavalry's dead as best as possible and hastily buried them where they fell. By the time troops came to recover the bodies, they found most of the dead stripped of their clothing, ritually mutilated, and in an advanced state of decomposition, making identification of many impossible.
Custer was found with shots to the left chest and left temple. Either wound would have been fatal, though he appeared to have bled from only the chest wound, meaning his head wound may have been delivered post-mortem. He also suffered a wound to the arm. Some Lakota oral histories assert that Custer committed suicide to avoid capture and subsequent torture, though this is usually discounted since the wounds were inconsistent with his known right-handedness. (Other Native accounts note several soldiers committing suicide near the end of the battle.) His body was found near the top of Custer Hill, which also came to be known as "Last Stand Hill." There the United States erected a tall memorial obelisk inscribed with the names of the 7th Cavalry's casualties.
Several days after the battle, Curley, Custer's Crow scout who had left Custer near Medicine Tail Coulee, recounted the battle, reporting that Custer had attacked the village after attempting to cross the river. He was driven back, retreating toward the hill where his body was found. As the scenario seemed compatible with Custer's aggressive style of warfare and with evidence found on the ground, it was the basis of many popular accounts of the battle.
According to Pretty Shield, the wife of Goes-Ahead (another Crow scout for the 7th Cavalry), Custer was killed while crossing the river: "...and he died there, died in the water of the Little Bighorn, with Two-bodies, and the blue soldier carrying his flag".:136 In this account, Custer was allegedly killed by a Lakota called Big-nose.:141 However, in Chief Gall's version of events, as recounted to Lt. Edward Settle Godfrey, Custer did not attempt to ford the river and the nearest that he came to the river or village was his final position on the ridge.:380 Chief Gall's statements were corroborated by other Indians, notably the wife of Spotted Horn Bull.:379 Given that no bodies of men or horses were found anywhere near the ford, Godfrey himself concluded "that Custer did not go to the ford with any body of men".:380
Cheyenne oral tradition credits Buffalo Calf Road Woman with striking the blow that knocked Custer off his horse before he died.
Custer at Minneconjou Ford.
"Hurrah boys, we've got them! We'll finish them up and then go home to our station." 
— Reported words of Lieutenant Colonel Custer at the battle's outset. 
Having isolated Reno's force and driven them away from the encampment, the bulk of the native warriors were free to pursue Custer. The route taken by Custer to his "Last Stand" remains a subject of debate. One possibility is that after ordering Reno to charge, Custer continued down Reno Creek to within about a half mile (800 m) of the Little Bighorn, but then turned north, and climbed up the bluffs, reaching the same spot to which Reno would soon retreat. From this point on the other side of the river, he could see Reno charging the village. Riding north along the bluffs, Custer could have descended into a drainage called Medicine Tail Coulee, which led to the river. Some historians believe that part of Custer's force descended the coulee, going west to the river and attempting unsuccessfully to cross into the village. According to some accounts, a small contingent of Indian sharpshooters opposed this crossing.
White Cow Bull claimed to have shot a leader wearing a buckskin jacket off his horse in the river. While no other Indian account supports this claim, if White Bull did shoot a buckskin-clad leader off his horse, some historians have argued that Custer may have been seriously wounded by him. Some Indian accounts claim that besides wounding one of the leaders of this advance, a soldier carrying a company guidon was also hit. Troopers had to dismount to help the wounded men back onto their horses.:117–19 The fact that each of the non-mutilation wounds to Custer's body (a bullet wound below the heart and a shot to the left temple) would have been instantly fatal casts doubt on his being wounded and remounted.
Reports of an attempted fording of the river at Medicine Tail Coulee might explain Custer's purpose for Reno's attack, that is, a coordinated "hammer-and-anvil" maneuver, with Reno's holding the Indians at bay at the southern end of the camp, while Custer drove them against Reno's line from the north. Other historians have noted that if Custer did attempt to cross the river near Medicine Tail Coulee, he may have believed it was the north end of the Indian camp, although it was only the middle. Some Indian accounts, however, place the Northern Cheyenne encampment and the north end of the overall village to the left (and south) of the opposite side of the crossing.:10–20 The location of the north end of the village remains in dispute, however.
Edward Curtis, the famed ethnologist and photographer of the Native American Indians, made a detailed personal study of the Battle, interviewing many of those who had fought or taken part in it. First he went over the ground covered by the troops with the three Crow scouts White Man Runs Him, Goes Ahead, and Hairy Moccasin, and then again with Two Moons and a party of Cheyenne warriors. He also visited the Lakota country and interviewed Red Hawk "whose recollection of the fight seemed to be particularly clear".:44 Finally, he went over the battlefield once more with the three Crow scouts, but also accompanied by General Charles Woodruff "as I particularly desired that the testimony of these men might be considered by an experienced army officer". Finally, Curtis visited the country of the Arikara and interviewed the scouts of that tribe who had been with Custer's command.:44 Based on all the information he gathered, Curtis concluded that Custer had indeed ridden down the Medicine Tail Coulee and then towards the river where he probably planned to ford it. However, "the Indians had now discovered him and were gathered closely on the opposite side".:48 They were soon joined by a large force of Sioux who (no longer engaging Reno) rushed down the valley. This was the beginning of their attack on Custer who was forced to turn and head for the hill where he would make his famous 'last stand'. Thus, wrote Curtis, "Custer made no attack, the whole movement being a retreat".:49
Other views of Custer's actions at Minneconjou Ford.
Other historians claim that Custer never approached the river, but rather continued north across the coulee and up the other side, where he gradually came under attack. According to this theory, by the time Custer realized he was badly outnumbered, it was too late to break back to the south where Reno and Benteen could have provided assistance. Two men from the 7th Cavalry, the young Crow scout "Ashishishe" (known in English as Curley) and the trooper Peter Thompson, claimed to have seen Custer engage the Indians. The accuracy of their recollections remains controversial, as accounts by battle participants and assessments by historians almost universally discredit Thompson's claim.
Archaeological evidence and reassessment of Indian testimony has led to a new interpretation of the battle. In the 1920s, battlefield investigators discovered hundreds of .45–70 shell cases along the ridge line, known today as Nye-Cartwright Ridge, between South Medicine Tail Coulee and the next drainage at North Medicine Tail (also known as Deep Coulee). Some historians believe Custer divided his detachment into two (and possibly three) battalions, retaining personal command of one while presumably delegating Captain George W. Yates to command the second.
The 1920s' evidence supports the theory that at least one of the companies made a feint attack southeast from Nye-Cartwright Ridge straight down the center of the "V" formed by the intersection at the crossing of Medicine Tail Coulee on the right and Calhoun Coulee on the left. The intent may have been to relieve pressure on Reno's detachment (according to the Crow scout Curley, possibly viewed by both Mitch Bouyer and Custer) by withdrawing the skirmish line into the timber on the edge of the Little Bighorn River. Had the US troops come straight down Medicine Tail Coulee, their approach to the Minneconjou Crossing and the northern area of the village would have been masked by the high ridges running on the northwest side of the Little Bighorn River.
That they might have come southeast, from the center of Nye-Cartwright Ridge, seems to be supported by Northern Cheyenne accounts of seeing the approach of the distinctly white-colored horses of Company E, known as the Grey Horse Company. Its approach was seen by Indians at that end of the village. Behind them, a second company, further up on the heights, would have provided long-range cover fire. Warriors could have been drawn to the feint attack, forcing the battalion back towards the heights, up the north fork drainage, away from the troops' providing cover fire above. The covering company would have moved towards a reunion, delivering heavy volley fire and leaving the trail of expended cartridges discovered 50 years later.
The "Last Stand".
In the end, the hilltop was probably too small to accommodate the survivors and wounded. Fire from the southeast made it impossible for Custer's men to secure a defensive position all around Last Stand Hill where the soldiers put up their most dogged defense. According to Lakota accounts, far more of their casualties occurred in the attack on Last Stand Hill than anywhere else. The extent of the soldiers' resistance indicated they had few doubts about their prospects for survival. According to Cheyenne and Sioux testimony, the command structure rapidly broke down, although smaller "last stands" were apparently made by several groups. Custer's remaining companies (E, F, and half of C,) were soon eradicated.
By almost all accounts, the Lakota annihilated Custer's force within an hour of engagement. David Humphreys Miller, who between 1935 and 1955 interviewed the last Lakota survivors of the battle, wrote that the Custer fight lasted less than one-half hour. Other Native accounts said the fighting lasted only "as long as it takes a hungry man to eat a meal." The Lakota asserted that Crazy Horse personally led one of the large groups of warriors who overwhelmed the cavalrymen in a surprise charge from the northeast, causing a breakdown in the command structure and panic among the troops. Many of these men threw down their weapons while Cheyenne and Sioux warriors rode them down, "counting coup" with lances, coup sticks, and quirts. Some Native accounts recalled this segment of the fight as a "buffalo run."
 I went over the battlefield carefully with a view to determine how the battle was fought. I arrived at the conclusion I [hold] now – that it was a rout, a panic, until the last man was killed…<br>
There was no line formed on the battlefield. You can take a handful of corn and scatter [the kernels] over the floor, and make just such lines. There were none...The only approach to a line was where 5 or 6 [dead] horses found at equal distances, like skirmishers [part of Lt. Calhoun’s Company L]. That was the only approach to a line on the field. There were more than 20 [troopers] killed [in one group]; there were [more often] four or five at one place, all within a space of 20 to 30 yards [of each other]…I counted 70 dead [cavalry] horses and 2 Indian ponies.<br><br>
I think, in all probability, that the men turned their horses loose without any orders to do so. Many orders might have been given, but few obeyed. I think that they were panic stricken; it was a rout, as I said before.”
 — Captain Frederick Benteen, Battalion leader, Companies D, H and K, recalling his observations on the Custer Battlefield, June 27, 1876
Custer's final resistance.
Recent archaeological work at the battlefield indicates that officers on Custer Hill restored some tactical control. E. Company, rushed off Custer Hill toward Little Big Horn River but failed and resulted in total destruction, leaving behind some 50 to 60 men. The remainder of the battle took on the nature of a running fight. Modern archaeology and historical Indian accounts indicate that Custer's force may have been divided into three groups, with the Indians' attempting to prevent them from effectively reuniting. Indian accounts describe warriors (including women) running up from the village to wave blankets in order to scare off the soldiers' horses. One 7th cavalry trooper claimed finding a number of stone "mallets" consisting of a round cobble weighing 8-10 pounds (about 4 kg) with a rawhide handle, which he believed had been used by the Indian women to finish off the wounded. Fighting dismounted, the soldiers' skirmish lines were overwhelmed. Army doctrine would have called for one man in four to be a horseholder behind the skirmish lines and, in extreme cases, one man in eight. Later, the troops would have bunched together in defensive positions and are alleged to have shot their remaining horses as cover. As individual troopers were wounded or killed, initial defensive positions would have been abandoned as untenable.
Under threat of attack the first US soldiers on the battlefield three days later hurriedly buried the troopers in shallow graves, more or less where they had fallen. A couple of years after the battle, markers were placed where men were believed to have fallen, so the placement of troops has been roughly construed. The troops evidently died in several groups, including on Custer Hill, around Captain Myles Keogh, and strung out towards the Little Big Horn River.
Last break-out attempt by 28 troopers.
Modern documentaries suggest that there may not have been a "Last Stand", as traditionally portrayed in popular culture. Instead, archaeologists suggest that, in the end, Custer's troops were not surrounded but rather overwhelmed by a single charge. This scenario corresponds to several Indian accounts stating Crazy Horse's charge swarmed the resistance, with the surviving soldiers fleeing in panic. At this point, the fight would have become a rout with warriors riding down the fleeing troopers and hitting them with lances and coup sticks. Many of these troopers may have ended up in a deep ravine 300–400 yards away from what is known today as Custer Hill. At least 28 bodies (the most common number associated with burial witness testimony), including that of scout Mitch Bouyer, were discovered in or near that gulch, their deaths possibly the battle's final actions. Although the marker for Mitch Bouyer has been accounted for as being accurate through archaeological and forensic testing, it is some 65 yards away from Deep Ravine. Other archaeological explorations done in Deep Ravine have found no human remains associated with the battle. According to Indian accounts, about 40 men made a desperate stand around Custer on Custer Hill, delivering volley fire.:284–85 The great majority of the Indian casualties were probably suffered during this closing segment of the battle, as the soldiers and Indians on Calhoun Ridge were more widely separated and traded fire at greater distances for most of their portion of the Battle than did the soldiers and Indians on Custer Hill.:282
Aftermath.
After the Custer force was annihilated, the Lakota and Northern Cheyenne regrouped to attack Reno and Benteen. The fight continued until dark (approximately 9:00 pm) and for much of the next day, with the outcome in doubt. Reno credited Benteen's leadership with repulsing a severe attack on the portion of the perimeter held by Companies H and M. On June 27, the column under General Terry approached from the north, and the Indians drew off in the opposite direction. The Crow scout White Man Runs Him was the first to tell General Terry's officers that Custer's force had "been wiped out." Reno and Benteen's wounded troops were given what treatment was available at that time; five later died of their wounds. One of the regiment's three surgeons had been with Custer's column, while another, Dr. DeWolf, had been killed during Reno's retreat. The only remaining doctor was Assistant Surgeon Henry R. Porter.
News of the defeat arrived in the East as the U.S. was observing its centennial. The Army began to investigate, although its effectiveness was hampered by a concern for survivors, and the reputation of the officers.
From the Indian perspective, the aftermath of the Battle of the Little Bighorn had far-reaching consequences. It was the beginning of the end of the Indian Wars, and has even been referred to as "the Indians' last stand" in the area. Within 48 hours after the battle, the large encampment on the Little Bighorn broke up into smaller groups as the resources of grass for the horses and game could not sustain a large congregation of people.
Oglala Sioux Black Elk recounted the exodus this way: "We fled all night, following the Greasy Grass. My two younger brothers and I rode in a pony-drag, and my mother put some young pups in with us. They were always trying to crawl out and I was always putting them back in, so I didn't sleep much."
The scattered Sioux and Cheyenne feasted and celebrated during July with no threat from soldiers. After their celebrations, many of the Indians slipped back to the reservation. Soon, the number of warriors who still remained at large and hostile amounted to only about 600. Both Crook and Terry remained immobile for seven weeks after the Bighorn battle, awaiting reinforcements and unwilling to venture out against the Indians until they had at least 2,000 men. Crook and Terry finally took the field against the Indians in August. General Nelson A. Miles took command of the effort in October 1876. In May 1877, Sitting Bull escaped to Canada. Within days, Crazy Horse surrendered at Fort Robinson. The Great Sioux War ended on May 7 with Miles' defeat of a remaining band of Miniconjou Sioux.
As for the Black Hills, the Manypenny Commission structured an arrangement in which the Sioux would cede the land to United States or the government would cease to supply rations to the reservations. Threatened with starvation, the Indians ceded "Paha Sapa" to the United States, but the Sioux never accepted the legitimacy of the transaction. After lobbying Congress to create a forum to decide their claim, and subsequent litigation spanning 40 years, the United States Supreme Court in the 1980 decision United States v. Sioux Nation of Indians acknowledged the United States had taken the Black Hills without just compensation. The Sioux refused the money offered, and continue to insist on their right to occupy the land.
Battle participants.
Notable scouts/interpreters in the battle.
The 7th Cavalry was accompanied by a number of scouts and interpreters:
Arapaho participation.
Modern-day accounts include Arapaho warriors in this fight, but the five Arapaho men were at the encampments only by accident. While on a hunting trip they came close to the village by the river and were captured and almost killed by the Lakota who believed the hunters were scouts for the US Army. Two Moon, a Northern Cheyenne leader, interceded to save their lives.
Order of battle.
Native Americans
United States Army, Lieutenant Colonel George A. Custer, 7th United States Cavalry Regiment, Commanding.
Casualties.
Native American casualties.
Native American casualties have never been determined and estimates vary widely, from as few as 36 dead (from Native American listings of the dead by name) to as many as 300. The Sioux chief Red Horse told Col. W. H. Wood in 1877 that the Native American suffered 136 dead and 160 wounded during the battle. In 1881, Red Horse told Dr. C. E. McChesney the same numbers but in a series of drawings done by Red Horse to illustrate the battle, Red Horse drew only sixty figures representing Lakota and Cheyenne casualties. Of those sixty figures only thirty some are portrayed with a conventional Plains Indian method of indicating death. Many historians do not agree with these categorical numbers, since Native Americans did not keep such statistics.
7th Cavalry casualties.
The 7th Cavalry suffered 52 percent casualties: 16 officers and 242 troopers killed or died of wounds, 1 officer and 51 troopers wounded. Every soldier in the five companies with Custer was killed (3 Indian scouts and several troopers had left that column before the battle; an Indian scout, Curley, was the only survivor to leave after the battle had begun), although for years rumors persisted of survivors. Among the dead were Custer's brothers Boston and Thomas, his brother-in-law James Calhoun, and his nephew Henry Reed. The sole surviving animal reportedly discovered on the battlefield by General Terry's troops was Captain Keogh's horse, Comanche, although other horses were believed to have been taken by the Indians.
In 1878, the army awarded 24 Medals of Honor to participants in the fight on the bluffs for bravery, most for risking their lives to carry water from the river up the hill to the wounded. Few on the non-Indian side questioned the conduct of the enlisted men, but many questioned the tactics, strategy and conduct of the officers. Indian accounts spoke of soldiers' panic-driven flight and suicide by those unwilling to fall captive to the Indians. While such stories were gathered by Thomas Bailey Marquis in a book in the 1930s, it was not published until 1976 because of the unpopularity of such assertions. Although soldiers may have believed captives would be tortured, Indians usually killed men outright and took as captive for adoption only young women and children. Indian accounts also noted the bravery of soldiers who fought to the death.
Legacy.
Reconstitution of the 7th Cavalry – July 1876.
Beginning in July, the 7th Cavalry was assigned new officers and recruiting efforts begun to fill the depleted ranks. The regiment, reorganized into eight companies, remained in the field as part of the Terry Expedition, now based on the Yellowstone River at the mouth of the Big Horn and reinforced by Gibbon's column. On August 8, 1876, after Terry was further reinforced with the 5th Infantry, the expedition moved up Rosebud Creek in pursuit of the Lakota. It met with Crook's command, similarly reinforced, and the combined force, almost 4,000 strong, followed the Lakota trail northeast toward the Little Missouri River. Persistent rain and lack of supplies forced the column to dissolve and return to its varying starting points. The 7th Cavalry returned to Fort Abraham Lincoln to reconstitute.
The expansion of the US Army.
The US Congress authorized appropriations to expand the Army by 2,500 men to meet the emergency after the defeat of the 7th Cavalry. For a session, the Democratic Party-controlled House of Representatives abandoned its campaign to reduce the size of the Army. Word of Custer's fate reached the 44th United States Congress as a conference committee was attempting to reconcile opposing appropriations bills approved by the House and the Republican Senate. They approved a measure to increase the size of cavalry companies to 100 enlisted men on July 24. The committee temporarily lifted the ceiling on the size of the Army by 2,500 on August 15.
"Sell or Starve".
As a result of the defeat in June 1876, Congress responded by attaching what the Sioux call the "sell or starve" rider (19 Stat. ) to the Indian Appropriations Act of 1876 (enacted August 15, 1876) which cut off all rations for the Sioux until they terminated hostilities and ceded the Black Hills to the United States. The Agreement of 1877 (19 Stat. , enacted February 28, 1877) officially took away Sioux land and permanently established Indian reservations.
Battle controversies.
Reno's conduct.
The Battle Of The Little Bighorn was the subject of an 1879 U.S. Army Court of Inquiry in Chicago, held at Reno's request, during which his conduct was scrutinized. Some testimony by non-Army officers suggested that he was drunk and a coward. The court found Reno's conduct to be without fault. Since the battle, Thomas Rosser, James O'Kelly, and others continued to question the conduct of Reno due to his hastily ordered retreat. Defenders of Reno at the trial noted that, while the retreat was disorganized, Reno did not withdraw from his position until it became apparent that he was outnumbered and outflanked by the Indians. Contemporary accounts also point to the fact that Reno's scout, Bloody Knife, was shot in the head, spraying him with blood, possibly increasing his own panic and distress.
Custer's errors.
General Terry and others claimed that Custer made strategic errors from the start of the campaign. For instance, he refused to use a battery of Gatling guns, and turned down General Terry's offer of an additional battalion of the 2nd cCavalry. Custer believed that the Gatling guns would impede his march up the Rosebud and hamper his mobility. His rapid march en route to the Little Big Horn averaged nearly 30 mi a day, so his assessment appears to have been accurate. Custer planned “to live and travel like Indians; in this manner the command will be able to go wherever the Indians can,” he wrote in his "Herald" dispatch.
By contrast, each Gatling gun had to be hauled by four horses, and soldiers often had to drag the heavy guns by hand over obstacles. Each of the heavy, hand-cranked weapons could fire up to 350 rounds a minute, an impressive rate, but they were known to jam frequently. During the Black Hills Expedition two years earlier, a Gatling gun had turned over, rolled down a mountain, and shattered to pieces. Lieutenant William Low, commander of the artillery detachment, was said to have almost wept when he learned he had been excluded from the strike force.
Custer believed that the 7th Cavalry could handle any Indian force and that the addition of the four companies of the 2nd would not alter the outcome. When offered the 2nd Cavalry, he reportedly replied that the 7th "could handle anything." There is evidence that Custer suspected that he would be outnumbered by the Indians, although he did not know by how much. By dividing his forces, Custer could have caused the defeat of the entire column, had it not been for Benteen's and Reno's linking up to make a desperate yet successful stand on the bluff above the southern end of the camp.
The historian James Donovan believed that Custer's dividing his force into four smaller detachments (including the pack train) can be attributed to his inadequate reconnaissance; he also ignored the warnings of his Crow scouts and Charley Reynolds. By the time the battle began, Custer had already divided his forces into three battalions of differing sizes, of which he kept the largest. His men were widely scattered and unable to support each other. Wanting to prevent any escape by the combined tribes to the south, where they could disperse into different groups, Custer believed that an immediate attack on the south end of the camp was the best course of action.
Admiration for Custer.
Criticism of Custer was not universal. While investigating the battlefield, Lieutenant General Nelson A. Miles wrote in 1877, "The more I study the moves here [on the Little Big Horn], the more I have admiration for Custer." Facing major budget cutbacks, the U.S. Army wanted to avoid bad press and found ways to exculpate Custer. They blamed the defeat on the Indians' alleged possession of numerous repeating rifles and the overwhelming numerical superiority of the warriors.
The widowed Elizabeth Bacon Custer, who never remarried, wrote three popular books in which she fiercely protected her husband's reputation. She lived until 1933, thus preventing much serious research until most of the evidence was long gone. In addition, Captain Frederick Whittaker's 1876 book idealizing Custer was hugely successful. Custer as a heroic officer fighting valiantly against savage forces was an image popularized in "Wild West" extravaganzas hosted by showman "Buffalo Bill" Cody, Pawnee Bill, and others.
Weapons used at the Battle of the Little Bighorn.
Lakota and Cheyenne.
The Lakota and Cheyenne warriors that opposed Custer’s forces possessed a wide array of weaponry, from Stone Age war clubs and lances to the most advanced firearms of the day. The typical firearms carried by the Lakota and Cheyenne combatants were muzzleloaders, more often a cap-lock smoothbore, the so-called Indian trade musket or Leman guns distributed to Indians by the US government at treaty conventions. Less common were surplus .58 caliber rifled muskets of American Civil War vintage such as the Enfield and Springfield. Metal cartridge weapons were prized by native combatants, such as the Henry and the Spencer lever-action rifles, as well as Sharps breechloaders. Bow and arrows were utilized by younger braves in lieu of the more potent firearms; effective up to 30 yards (27 meters) the arrows could readily maim or disable an opponent.
Sitting Bull’s forces had no assured means to supply themselves with firearms and ammunition. Nonetheless, they could usually procure these through post-traders, licensed or unlicensed, and from gunrunners who operated in the Dakota Territory: “…a horse or a mule for a repeater…buffalo hides for ammunition.” Custer's highly regarded guide, "Lonesome" Charley Reynolds, informed his superior in early 1876 that Sitting Bull's forces were amassing weapons, including numerous Winchester repeating rifles and abundant ammunition.
Of the guns owned by Lakota and Cheyenne fighters at the Little Bighorn, approximately 200 were repeating rifles corresponding to about 1 of 10 of the encampment’s two thousand able-bodied fighters who participated in the battle
7th Cavalry.
The troops under Custer’s command carried two regulation firearms authorized and issued by the U.S. Army in early 1876: the breech-loading, single-shot Springfield Model 1873 carbine, and the 1873 Colt single-action revolver. The regulation M1860 saber or “Long Knives” were not carried by troopers upon Custer’s order.
With the exception of a number of officers and scouts who opted for personally owned and more expensive rifles and handguns, the 7th Cavalry was uniformly armed.
Ammunition allotments provided 100 carbine rounds per trooper, carried on an cartridge belt and in saddlebags on their mounts. An additional 50 carbine rounds per man were reserved on the pack train that accompanied the regiment to the battlefield. Each trooper had 24 rounds for his Colt handgun.
The opposing forces, though not equally matched in the number and type of arms, were comparably outfitted, and neither side held a overwhelming advantage in weaponry.
Lever-action Repeaters vs. Single-action Breechloaders.
Two hundred or more Lakota and Cheyenne combatants are known to have been armed with Henry, Winchester, or similar lever-action repeating rifles at the battle. Virtually every trooper in the 7th Cavalry fought with the single-shot, breech-loading Springfield carbine and the Colt revolver.
Historians have asked whether the repeating rifles conferred a distinct advantage on Sitting Bull’s villagers that contributed to their victory over Custer’s carbine-armed soldiers.
Historian Michael L. Lawson offers a scenario based on archaeological collections at the "Henryville" site, which yielded plentiful Henry rifle cartridge casings from approximately 20 individual guns. Lawson speculates that, though less powerful than the Springfield carbines, the Henry repeaters provided a barrage of fire at a critical point, driving Lieutenant James Calhoun's L Company from Calhoun Hill and Finley Ridge, forcing them to flee in disarray back to Captain Myles Keogh's I Company, and leading to the disintegration of that wing of Custer's Battalion.
Model 1873 Springfield carbine and the US Army.
After exhaustive testing – including comparisons to domestic and foreign single-shot and repeating rifles – the Army Ordnance Board (whose members included officers Marcus Reno and Alfred Terry) authorized the Springfield as the official firearm for the United State Army.
The Springfield, manufactured in a .45-70 long rifle version for the infantry and a .45-55 light carbine version for the cavalry, was judged a solid firearm that met the long-term and geostrategic requirements of the United States fighting forces.
British historian Mark Gallear maintains that US government experts rejected the lever-action repeater designs, deeming them ineffective in the event of a clash with fully equipped European armies, or in case of an outbreak of another American civil conflict. Gallear’s analysis minimizes the allegation that rapid depletion of ammunition in lever-action models influenced the decision in favor of the single-shot Springfield. The Indian War, in this context, appears as a minor theatre of conflict, whose contingencies were unlikely to govern the selection of standard weaponry for an emerging industrialized nation.
The Springfield carbine is praised for its “superior range and stopping power” by historian James Donovan, and author Charles M. Robinson reports that the rifle could be “loaded and fired much more rapidly than its muzzle loading predecessors, and had twice the range of repeating rifles such as the Winchester, Henry and Spencer.”
Gallear points out that lever-action rifles, after a burst of rapid discharge, still required a reloading interlude that lowered their overall rate of fire; Springfield breechloaders “in the long run, had a higher rate of fire, which was sustainable throughout a battle.”
The breechloader design patent for the Springfield’s Erskine S. Allin “trapdoor” system was owned by the US government and the firearm could be easily adapted for production with existing machinery at the Springfield Armory in Massachusetts.
 At time when funding for the post-war Army had been slashed, the prospect for economical production influenced the Ordnance Board member selection of the Springfield option.
Malfunction of the Springfield Carbine extractor mechanism.
The question as to whether the reported malfunction of the Model 1873 Springfield carbine issued to the 7th Cavalry contributed to their defeat has been debated for years.
That the weapon experienced jamming of the extractor is not contested, but its contribution to Custer’s defeat is considered negligible. This conclusion is supported by evidence from archaeological studies performed at the battlefield, where the recovery of Springfield cartridge casing, bearing tell-tale scratch marks indicating manual extraction, were rare.
The flaw in the ejector mechanism was known to the Army Ordnance Board at the time of the selection of the Model 1873 rifle and carbine, and was not considered a significant shortcoming in the overall worthiness of the shoulder arm. With the ejector failure in US Army tests as low as 1:300, the Springfield carbine was vastly more reliable than the muzzle-loading Springfields used in the Civil War.
Gallear addresses the post-battle testimony concerning the copper .45-55 cartridges supplied to the troops in which an officer is said to have cleared the chambers of spent cartridges for a number of Springfield carbines. This testimony of widespread fusing of the casings offered to the Chief of Ordnance at the Reno Court of Inquiry in 1879 conflicts with the archaeological evidence collected at the battlefield. Field data showed that possible extractor failures occurred at a rate of approximately 1:30 firings at the Custer Battlefield and at a rate of 1:37 at the Reno-Benteen Battlefield.
Historian Thom Hatch observes that the Model 1873 Springfield, despite the known ejector flaw, remained the standard issue shoulder arm for US troops until the early 1890s. when the copper-cased, inside-primed cartridges were replaced with brass.
The Gatling gun controversy.
General Alfred Terry’s Dakota column included a single battery of artillery, comprising two Rodman guns (3-inch Ordnance rifle) and two Gatling guns. (According to historian Evan S. Connell, the precise number of Gatlings has not been established, ranging from two to three).
Custer’s decision to reject Terry’s offer of the rapid-fire Gatlings has raised questions among historians as to why he refused them and what advantage their availability might have conferred on his forces at the Battle of the Little Bighorn.
One factor concerned Major Marcus Reno’s recent 8-day reconnaissance-in-force of the Powder-Tongue-Rosebud Rivers, June 10 to 18. This deployment had demonstrated that artillery pieces mounted on gun carriages and hauled by horses no longer fit for cavalry mounts (so-called condemned horses) were cumbersome over mixed terrain and vulnerable to breakdowns. Custer, valuing the mobility of the 7th Cavalry and recognizing Terry’s acknowledgement of the regiment as “the primary strike force” preferred to remain unencumbered by the Gatling guns. Custer insisted that the artillery was superfluous to his success, in that the 7th Cavalry alone was sufficient to cope with any force they should encounter, informing Terry: "The 7th can handle anything it meets". In addition to these practical concerns, a strained relationship with Major James Brisbin induced Custer’s polite refusal to integrate Brisbin’s Second Cavalry unit – and the Gatling guns – into his strike force, as it would disrupt any hierarchical arrangements that Custer presided over.
Historians have acknowledged the fire power inherent in the Gatling gun: they were capable of firing 350 .45-70 caliber rounds per minute. Jamming caused by black powder residue could lower that rate, raising questions as to their reliability under combat conditions. Researchers have further questioned the effectiveness of the guns under the tactics that Custer was likely to face with the Lakota and Cheyenne warriors. The Gatlings, mounted high on carriages, required the battery crew to stand upright during its operation, making them easy targets for Lakota and Cheyenne sharpshooters.
Historian Robert M. Utley, in a section entitled “Would Gatling Guns had Saved Custer?” presents two judgments from Custer’s contemporaries: General Henry J. Hunt, expert in the tactical use of artillery in Civil War, stated that Gatlings “would probably have saved the command”, whereas General Nelson A. Miles, participant in the Great Sioux War declared “[Gatlings] were useless for Indian fighting.” 
Battle survivor claims.
Soldiers under Custer's direct command were annihilated on the first day of battle. However, over 120 men and women would come forward over the course of the next 70 years claiming they were "the lone survivor" of Custer's Last Stand. The phenomena became so widespread that one historian remarked, "Had Custer had all of those who claimed to be “the lone survivor” of his two battalions he would have had at least a brigade behind him when he crossed the Wolf Mountains and rode to the attack."
The historian Earl Alonzo Brininstool suggested he had collected at least 70 "lone survivor" stories. Michael Nunnally, an amateur Custer historian, wrote a booklet describing 30 such accounts. It has been said that even Mrs. Libby Custer received dozens of letters from men, in shocking detail, about their sole survivor experience. At least 125 alleged "single survivor" tales have been confirmed in the historical record as of July 2012.
Frank Finkel, from Dayton, Washington, had such a convincing story that historian Charles Kuhlman believed the alleged survivor, going so far as to write a lengthy defense of Finkel's participation in the battle. Douglas Ellison—mayor of Medora, North Dakota, and an amateur historian—also wrote a book in support of the veracity of Finkel's claim. Most scholars reject Finkel's claim. 
Some of these survivors held a form of celebrity status in the United States, among them Raymond Hatfield “Arizona Bill” Gardner and Frank Tarbeaux. A few even published their own autobiographies including their deeds at the Little Bighorn.
Almost as soon as men came forward implying or directly pronouncing their unique role in the battle, there were others who were equally opposed to any such claims. Theodore Goldin, a battle participant who later became a controversial historian on the event, wrote that
 The Indians always insisted that they took no prisoners. If they did—a thing I firmly believe—they were tortured and killed the night of the 25th. As an evidence of this I recall the three charred and burned heads we picked up in the village near the scene of the big war dance, when we visited the village with Capt. Benteen and Lieut. Wallace on the morning of the 27th...
I'm sorely afraid, Tony, that we will have to class Hayward's story, like that of so many others, as pure, unadulterated B. S.
As a clerk at headquarters I had occasion to look over the morning reports of at least the six troops at Lincoln almost dally, and never saw his name there, or among the list of scouts employed from time to time...I am hoping that some day all of these damned fakirs will die and it will be safe for actual participants in the battle to admit and insist that they were there, without being branded and looked upon as a lot of damned liars. Actually, there have been times when I have been tempted to deny that I ever heard of the 7th Cavalry, much less participated with it in that engagement...My Medal of Honor and its inscription have served me as proof positive that I was at least in the vicinity at the time in question, otherwise I should be tempted to deny all knowledge of the event.
Battlefield preservation.
The site was first preserved as a United States national cemetery in 1879, to protect the graves of the 7th Cavalry troopers. In 1946 it was redesignated as the "Custer Battlefield National Monument", reflecting its association with the general. In 1967, Major Marcus Reno was reinterred in the cemetery with honors, including an eleven-gun salute. Beginning in the early 1970s there was concern within the National Park Service over the name Custer Battlefield National Monument, recognizing the larger history of the battle between two cultures, hearings on the name change were held in Billings on June 10 and during the following months in 1991 Congress renamed the site the "Little Bighorn Battlefield National Monument".
United States memorialization on the battlefiepeld began in 1879 with a temporary monument to U.S. dead. In 1881 the current marble obelisk was erected in their honor. In 1890 marble blocks were added to mark the places where the U.S. cavalry soldiers fell.
Nearly 100 years later, ideas about the meaning of the battle have become more inclusive. The United States government acknowledged that Native American sacrifices also deserved recognition at the site. The 1991 bill changing the name of the national monument also authorized an Indian Memorial to be built near Last Stand Hill in honor of Lakota and Cheyenne warriors. The commissioned work by Native artist Colleen Cutschall is shown in the photograph at right. On Memorial Day 1999, in consultation with tribal representatives, the US added two red granite markers to the battlefield to note where Native American warriors fell. As of December 2006, a total of ten warrior markers have been added (three at the Reno-Benteen Defense Site, seven on the Little Bighorn Battlefield).
The Indian Memorial, themed "Peace Through Unity" l is an open circular structure that stands 75 yards from the 7th Cavalry obelisk. Its walls have some of the names of Indians who died at the site, as well as Native accounts of the battle. The open circle of the structure is symbolic, as for many tribes, the circle is sacred. The "spirit gate" window facing the Cavalry monument is symbolic as well, welcoming the dead cavalrymen into the memorial. 
Further reading.
</dl>

</doc>
<doc id="56127" url="http://en.wikipedia.org/wiki?curid=56127" title="Cleopatra Selene">
Cleopatra Selene

Cleopatra Selene may refer to:

</doc>
<doc id="56128" url="http://en.wikipedia.org/wiki?curid=56128" title="Cheyenne">
Cheyenne

The Cheyenne ( ) are one of the groups of indigenous people of the Great Plains and their language is of the Algonquian language family. The Cheyenne comprise two Native American groups, the Só'taeo'o or Só'taétaneo'o (more commonly spelled as Suhtai or Sutaio) and the Tsétsêhéstâhese (also spelled Tsitsistas). These tribes merged in the early 19th century. Today the Cheyenne people are split into two federally recognized groups: Southern Cheyenne, who are enrolled in the Cheyenne and Arapaho Tribes in Oklahoma, and the Northern Cheyenne, who are enrolled in the Northern Cheyenne Tribe of the Northern Cheyenne Indian Reservation in Montana.
The Cheyenne lived in the area of what is now Minnesota at the time of their first contact with the Europeans. Also, they were at times allied with the Lakota and Arapaho. They migrated west across the Mississippi River and into North and South Dakota in the early 18th century. They adopted the horse culture and developed a more centralized authority through ritual ceremonies and structure than other Plains Indians of the 19th century. Having settled the Black Hills of South Dakota and the Powder River Country of present-day Montana, they introduced the horse culture to Lakota bands about 1730. Allied with the Arapaho, the Cheyenne pushed the Kiowa to the Southern Plains. In turn, they were pushed west by the more numerous Lakota.
The Cheyenne Nation or Tsêhéstáno was at one time composed of ten bands which spread across the Great Plains from southern Colorado to the Black Hills in South Dakota. When they gathered, the bands leaders would meet in formal council. They performed an annual Arrow Renewal ceremony and Sun Dance. They fought their traditional enemies, the Crow and later (1856–79) the United States Army forces. In the mid-19th century, the bands began to split, with some bands choosing to remain near the Black Hills, while others chose to remain near the Platte Rivers of central Colorado.
The Northern Cheyenne, known in Cheyenne either as Notameohmésêhese meaning "Northern Eaters" or simply as Ohmésêhese meaning "Eaters", live in southeastern Montana on the Northern Cheyenne Indian Reservation. Tribal enrollment figures, as of late 2014, indicate that there are approximately 10,840 enrolled tribal members, of which about 4,939 reside on the reservation. Approximately 91% of the population are Native Americans (full or part race), with 72.8% identifying themselves as Cheyenne. Slightly more than one quarter of the population five years or older spoke a language other than English.
The Southern Cheyenne, known in Cheyenne as Heévâhetaneo'o meaning "Roped People", together with the Southern Arapaho, form the Cheyenne and Arapaho Tribes, in western Oklahoma. Their combined population is 12,130, as of 2008[ [update]]. In 2003, approximately 8,000 of these identified themselves as Cheyenne, although with continuing intermarriage it has become increasingly difficult to separate the tribes.
Name.
The Cheyenne Nation is composed of two tribes, the Só'taeo'o or Só'taétaneo'o (more commonly as Suhtai or Sutaio; singular: Só'taétane) and the Tsétsêhéstâhese (more commonly as the Tsitsistas; singular: Tsétsêhéstaestse), which translates to "those who are like this". These two tribes had always traveled together, becoming fully merged sometime after 1831, when they were still noted as having separate camps. The Suhtai were said to have originally had slightly different speech and customs from their traveling companions.
The name "Cheyenne" may be derived from Dakota Sioux exonym for them, "Šahíyena" (meaning "little "Šahíya""). Though the identity of the "Šahíya" is not known, many Great Plains tribes assume it means Cree or some other people who spoke an Algonquian language related to Cree and Cheyenne. The Cheyenne word for Ojibwe" is "Sáhea'eo'o," a word that sounds similar to the "Dakota" word "Šahíya"."
Another of the common etymologies for "Cheyenne" is "a bit like the [people of an] alien speech" (literally, "red-talker"). According to George Bird Grinnell, the Dakota had referred to themselves and fellow Siouan-language bands as "white talkers", and those of other language families, such as the Algonquian Cheyenne, as "red talkers" ("Šahíyena").
Etymology of the name Tsitsistas (technically Tsétsėhéstȧhese) the Cheyennes call themselves is uncertain. According to the Cheyenne dictionary, offered online by Chief Dull Knife College, there is no definitive consensus and various studies of the origins and the translation of the word has been suggested. Grinnell's record is typical in which he states "They call themselves Tsistsistas [sic, Tsitsistas is the correct pronunciation], which the books commonly give as meaning "people." It most likely means related to one another, similarly bred, like us, our people, or us. The term for the Cheyenne homeland is "Tsiihistano".
Language.
The Cheyenne of Montana and Oklahoma speak the Cheyenne language, known as "Tsêhésenêstsestôtse" (common spelling: Tsisinstsistots). Approximately 800 people speak Cheyenne in Oklahoma. Only a handful of vocabulary differs between the two locations. The Cheyenne alphabet contains 14 letters. The Cheyenne language is one of the larger Algonquian-language group. Formerly the Só'taeo'o (Só'taétaneo'o) or Suhtai (Sutaio) bands of Southern and Northern Cheyennes spoke "Só'taéka'ękóne" or "Só'taenęstsestôtse", a language so close to "Tsêhésenêstsestôtse" (Cheyenne language), that it is sometimes termed a Cheyenne dialect.
History.
Early history.
The earliest known written historical record of the Cheyenne comes from the mid-17th century, when a group of Cheyenne visited the French Fort Crevecoeur, near present-day Chicago, Illinois. The Cheyenne at this time lived between the Mississippi River and Mille Lacs Lake in present-day Minnesota. The Cheyenne economy was based on collection of wild rice and hunting, especially of bison which lived in the prairies 70–80 miles west of the Cheyenne villages.
According to tribal history, during the 17th century the Cheyenne had been driven by the Assiniboine (Hóheeheo'o - ″wrapped ones or Swaddled″, adaptive from the Lakota/Dakota word "Hóhe", meaning “rebells”) from the Great Lakes region to present-day Minnesota and North Dakota, where they established villages. The most prominent of the ancient Cheyenne villages is Biesterfeldt Village, in eastern North Dakota along the Sheyenne River. The tribal history also relates that they first reached the Missouri River in 1676. A more recent analysis of early records posits that at least some of the Cheyenne remained in the Mille Lac region of Minnesota until c. 1765, when the Ojibwe defeated the Dakota with firearms — pushing the Cheyenne in turn to the Minnesota River, where they were reported in 1766.
On the Missouri River, the Cheyenne came into contact with the neighboring Mandan, Hidatsa (Tsé-heše'émâheónese - „People, who have dirt houses (that is, Earth lodges“) and Arikara people (Ónoneo'o), and they adopted many of their cultural characteristics. They were first of the later Plains tribes into the Black Hills and Powder River Country. About 1730 they introduced the horse to Lakota bands (Ho'óhomo'eo'o - “the invited ones (to Cheyenne lands i.e. the Black Hills)”). Conflict with migrating Lakota and Ojibwe people forced the Cheyenne further west, and they in turn pushed the Kiowa to the south.
By 1776 the Lakota had overwhelmed the Cheyenne and taken over much of their territory near the Black Hills. In 1804, Lewis and Clark visited a surviving Cheyenne village in North Dakota. Such European American explorers learned many different names for the Cheyenne, and did not realize how the different sections were forming a unified tribe.
The Cheyenne Nation is descended from two related tribes, the Tsétsêhéstâhese / Tsitsistas (Cheyenne proper) and Só'taeo'o / Só'taétaneo'o (better known as Suhtai or Sutaio), the latter may have joined the Tsétsêhéstâhese in the early 18th century. Their oral history relays that both tribal peoples are characterized, and represented by two cultural heroes or prophets who received divine articles from their god Ma'heo'o (″Sacred Being, God″, commonly in English Maheo, Mahiu, this is a post-missionary term, formerly the plural Ma'heono was used), which the Só'taeo'o called He'emo (″Goddess, Female Sacred Being, God″, equivalent to "Ma'heo'o" in the Tsétsêhéstâhese dialect).
The Tsétsêhéstâhese / Tsitsistas prophet Motsé'eóeve (Sweet Medicine Standing, Sweet Root Standing, commonly called Sweet Medicine) had received the "Maahótse" (in English known as "Mahuts", a bundle of (Sacred) Arrows or the (Sacred) Arrows Bundle) at "Nóávóse" (″medicine(sacred)-hill″, name for Bear Butte, northwest of Rapid City, South Dakota), which they carried when they waged tribal-level war and were kept in the "maahéome" (Arrow Lodge or Arrow Tepee). He organized the structure of Cheyenne society, their military or war societies led by prominent warriors, their system of legal justice, and the Council of Forty-four peace chiefs, the latter was formed from four "véhoo'o" (chiefs or leaders) of the ten principal "manaho" (bands) and an additional four ″Old Man″ meeting to deliberate at regular tribal gatherings, centered around the Sun Dance. Sweet Medicine is the Cheyenne prophet who predicted the coming of the horse, cow, whiteman, etc. to the Cheyennes (1987:6-16; 1987:99); he was named for "motsé'eonȯtse" (sweetgrass), which they used as an Incense and for purification, as oblations to ancestors, for protection of spirits, and keeping out of evil and harm as well to paint pipes in the Sun Dance and the Sacred Arrow ceremonies. The "Maahótse (Sacred Arrows)" are cared until today by the Southern Cheyenne and Southern Só'taeo'o. The "Keeper of the Sacred Arrows" (called by the whites also: "Keeper of the Medicine Arrows") must be a Southern Cheyenne and cannot be of the Só'taeo'o (Northern or Southern alike).
The Só'taeo'o / Só'taétaneo'o prophet Tomȯsévėséhe ("Tomosevsehe", former English-spelling "Tomsivsi", commonly called Erect Horns) had received the "Ésevone (old term, also meaning ″buffalo herd, female buffalo″, therefore the Só'taeo'o were also known as "Buffalo People")" or "Hóhkėha'e (new term)" (former English-spelling "Is'siwun" - Sacred (Buffalo) Hat, also known as Buffalo Hat, Sacred Hat or Sacred (Buffalo) Hat Bundle) at "Toh'nihvoos" (″Stone Hammer Mountain″) near the Great Lakes in the present state of Minnesota, the "Ésevone / Hóhkėha'e (Sacred Buffalo Hat)" is kept in the "vonȧhéome (old term)" or "hóhkėha'éome (new term)" (Sacred Hat Lodge, Sacred Hat Tepee), Erect Horns gave them the accompanying ceremonies and the Sun Dance (Hestȯsenestȯtse or Hoxéhevėhomó'hestȯtse), his vision convinced the tribe to abandon their earlier sedentary agricultural traditions to adopt nomadic Plains horse culture (therefore the Só'taeo'o were earlier on the plains in the west than the Tsétsêhéstâhese). They replaced their earth lodges with portable tipis and switched their diet from fish and agricultural produce, to mainly bison and wild fruits and vegetables. Their lands ranged from the upper Missouri River into what is now Wyoming, Montana, Colorado, and South Dakota. The "Ésevone / Hóhkėha'e (Sacred Buffalo Hat)" is kept among the Northern Cheyenne and Northern Só'taeo'o. The "Tséá'enōvȧhtse" (″Sacred (Buffalo) Hat Keeper″ or ″Keeper of the Sacred (Buffalo) Hat″) must belong to the Só'taeo'o (Northern or Southern alike).
The "Maahótse (Sacred Arrows)" are symbols of male power and the power of the "Ésevone / Hóhkėha'e (Sacred Buffalo Hat)" is female. The Sacred Buffalo Hat and the Sacred Arrows together form the two great covenants of the Cheyenne Nation. Through these two bundles Ma'heo'o assures continual life and blessings for the people.
Historical Cheyenne bands.
Northern Cheyenne (known in Cheyenne either as Notameohmésêhese or Notameohmésėhétaneo'o meaning "Northern Eaters" or simply as Ohmésêhese / Ôhmésêheseo'o meaning "Eaters")
lesser northern bands (not represented in the Council of Forty-Four):
Southern Cheyenne (known in Cheyenne as Heévâhetaneo'o meaning "Roped People" - after the most populous band, also commonly known as Sówoniá - "the Southern People")
lesser southern bands (not represented in the Council of Forty-Four):
The Heviksnipahis (Iviststsinihpah, also known as the Tsétsêhéstâhese / Tsitsistas proper), Heévâhetaneo'o (Hevhaitaneo), Masikota (in Lakotiyapi: Sheo), Omísis (Ôhmésêheseo'o, the Notameohmésêhese proper), Só'taeo'o / Só'taétaneo'o (Suhtai or Sutaio, Northern and Southern), Wotápio (Wutapai), Oévemanaho (Oivimána or Oévemana, Northern and Southern), Hesé'omeétaneo'o (Hisiometaneo or Issiometaniu), Oo'kóhta'oná (Ohktounna or Oqtóguna) and the Hónowa (Háovȯhnóvȧhese or Nėstamenóoheo'o) were the ten principal bands that had the right to send four chief delegates representing them in the Council of Forty-Four.
After the "Masikota" and "Oo'kóhta'oná" bands had been almost wiped out through a cholera epidemic in 1849, the remaining Masikota joined the Dog Soldiers warrior society ("Hotamétaneo'o"). They effectively became a separate band and in 1850 took over the position in the camp circle formerly occupied by the Masikota. The members often opposed policies of peace chiefs such as Black Kettle. Over time the Dog Soldiers took a prominent leadership role in the wars against the whites. In 1867, most of the band were killed by United States Army forces in the Battle of Summit Springs.
Due to an increasing division between the Dog Soldiers and the council chiefs with respect to policy towards the whites, the Dog Soldiers became separated from the other Cheyenne bands. They effectively became a "third division" of the Cheyenne people, between the Northern Cheyenne, who ranged north of the Platte River, and the Southern Cheyenne, who occupied the area north of the Arkansas River.
Expansion on the Plains.
After being pushed south and westward by the Lakota, the unified Cheyenne people began to create and expand a new territory of their own. Sometime around 1811 the Cheyenne made a formal alliance with the Arapaho people (Hetanevo'eo'o - „People of the Sky“, „Cloud People“, because of their close interaction also known as Héstanėheo'o - “people, mankind, tribe of people”), which would remain strong throughout their history and into modern times. The alliance helped the Cheyenne expand their territory which stretched from southern Montana, through most of Wyoming, the eastern half of Colorado, far western Nebraska, and far western Kansas. As early as 1820, traders and explorers reported contact with Cheyenne at present-day Denver, Colorado and on the Arkansas River. They were probably hunting and trading in that area earlier. They may have migrated to the south for winter. The Hairy Rope band is reputed to have been the first band to move south, capturing wild horses as far south as the Cimarron River Valley. In response to the construction of Bent’s Fort by Charles Bent, a friend of the Cheyenne who established a popular trading area for the Cheyenne, a large portion of the tribe moved further south and stayed around the area. The other part of the tribe continued to live along the headwaters of the North Platte and Yellowstone rivers. The groups became the Southern Cheyenne, known as Sówoníă (Southerners) and the Northern Cheyenne, known as O'mǐ'sǐs (Eaters). The separation of the tribe was only a geographic one and the two divisions had regular and close contact.
In the southern portion of their territory the Cheyenne and Arapaho warred with the allied Comanche, Kiowa, and Plains Apache. Numerous battles were fought including a notable fight along the Washita River in 1836 with the Kiowa which resulted in the death of 48 Cheyenne warriors of the Bowstring society. In summer 1838, many Cheyenne and Arapaho attacked a camp of Kiowa and Comanche along Wolf Creek in Oklahoma resulting in heavy losses from both sides. Conflict with the Comanche, Kiowa, and Plains Apache ended in 1840 when the tribes made an alliance with each other. The new alliance allowed the Cheyenne to enter the Llano Estacado in the Texas and Oklahoma panhandles and northeastern New Mexico to hunt bison and trade. Their expansion in the south and alliance with the Kiowa led to their first raid into Mexico in 1853. The raid ended in disaster with heavy resistance from Mexican lancers, resulting in all but 3 of the war party being killed. To the north the Cheyenne made a strong alliance with the Lakota Sioux, which allowed them to expand their territory into part of their former lands around the Black Hills. They managed to escape the smallpox epidemics which swept across the plains from white settlements in 1837-39 by heading into the Rocky Mountains but were greatly affected by the Cholera epidemic in 1849. Contact with Euro-Americans was mostly light, with most contact involving mountain men, traders, explorers, treaty makers, and painters.
Enemies and warrior culture.
Like many other plains Indian nations the Cheyenne were a horse and warrior people who developed as skilled and powerful mounted warriors. A warrior was viewed by the people not as a maker of war but as a protector, provider, and leader. Warriors gained rank in Cheyenne society by performing and accumulating various acts of bravery in battle known as coups. The title of war chief can be earned by any warrior who performs enough of the specific coups required to become a war chief. Specific warrior societies developed among the Cheyenne as with other plains nations. Each society had selected leaders which would invite those they see worthy enough to join their society to their society lodge for initiation. Often societies would have minor rivalry or would work together as a unit when warring with an enemy. Military societies played an important role in Cheyenne government, society leaders were often in charge of organizing hunts and raids as well as ensuring proper discipline and enforcement of laws within the nation. Each of the six distinct warrior societies of the Cheyenne would take turns assuming the leadership role within the nation. The four original military societies of the Cheyenne were the Swift Fox Society, Elk Horn Scrapper or Crooked Lance Society, Shield Society, and the Bowstring Men Society. The fifth society is split between the Crazy Dog Society and the famous Dog Soldiers. The sixth society is the Contrary Warrior Society, most notable for riding backwards into battle as a sign of bravery. All six societies and their various branches exist among the Southern and Northern Cheyenne Nations in present times. Warriors use a combination of traditional weapons such as various types of war clubs, tomahawks, bows and arrows, and lances as well as non-traditional weapons such as revolvers, rifles, and shotguns acquired through raid and trade.
The enemies of the Cheyenne included the Crow (Óoetaneo'o - “crow (bird) people”), Shoshone (Sósone'eo'o), Blackfeet (Mo'ôhtávêhahtátaneo'o, same literally meaning), Flathead (Kȧhkoestséataneo'o - “flat-headed-people”), Nez Perce (Otaesétaneo'o - “pierced nose people”), Arikara, Gros Ventre (Hestóetaneo'o - “beggars for meat”, “spongers” or Môhónooneo'o - lit. “scouting all over ones”), Assiniboine, and Plains Cree (Vóhkoohétaneo'o - “rabbit people”) to the north and west of Cheyenne territory. To the east of Cheyenne Territory they fought with the Sioux, Pawnee (Ho'néhetaneo'o - “wolf people”, possibly an adaptive from the Skiri/Skidi Pawnee or Wolf Pawnee), Ponca (Onéhao'o), Kaw (Oo'kóhtâxétaneo'o - “cut hair people”), Iowa, Ho-Chunk and Omaha (Onéhao'o). South of Cheyenne territory they fought with the Kiowa (Vétapâhaetó'eo'o - “greasy wood ones”), Comanche (Šé'šenovotsétaneo'o - “snake people”), Ute (Mo'ȯhtávėhetaneo'o - “black (skinned) people”), Plains Apache (Mȯhtséheonetaneo'o - “occupied.comp-people”), Osage (Oo'kóhtâxétaneo'o - “cut hair people”), Wichita people, various Apache tribes and Navajo (Hotamó'keeho - „Indians from out west“; collective name for tribes of the Southswest and Great Basin). Many of the enemies the Cheyenne fought were only encountered occasionally, such as on a long distance raid or hunt. Some of their enemies, particularly the Indian peoples of the eastern great plains such as the Pawnee and Osage would act as Indian Scouts for the US Army, providing valuable tracking skills and information regarding Cheyenne habits and fighting strategies to US soldiers. Some of their enemies such as the Lakota would latter in their history become their strong allies, helping the Cheyenne fight against the United States Army during Red Cloud's War and the Great Sioux War of 1876. The Comanche, Kiowa and Plains Apache became allies of the Cheyenne towards the end of the Indian wars on the southern plains, fighting together during conflicts such as the Red River War.
Relationship with the Arapaho.
The Cheyenne and Arapaho people formed an alliance together around 1811 which helped them expand their territories and strengthen their presence on the plains. Like the Cheyenne, the Arapaho language is part of the Algonquian group, although the two languages are not mutually intelligible. The Arapaho remained strong allies with the Cheyenne and helped them fight alongside the Sioux during Red Cloud's War and the Great Sioux War of 1876, also known commonly as the Black Hills War. On the southern plains the Arapaho and Cheyenne allied with the Comanche, Kiowa, and Plains Apache to fight invading settlers and US soldiers. The Arapaho were present with the Cheyenne at the Sand Creek Massacre when a peaceful encampment of mostly women, children, and the elderly were attacked and massacred by US soldiers. Both major divisions of the Cheyenne, the Northern Cheyenne and Southern Cheyenne were allies to the Arapaho who like the Cheyenne are split into northern and southern divisions. The Southern Cheyenne and Southern Arapaho were assigned to the same reservation in Oklahoma Indian Territory and remained together as the federally recognized Cheyenne and Arapaho Tribes after the reservation was opened to American settlement and into modern times. The Northern Arapaho were to be assigned a reservation of their own or share one with the Cheyenne however the government failed to provide them with either and placed them on the already established Wind River Indian Reservation in Wyoming with their former enemies the Shoshone.
Treaty of 1825.
In the summer of 1825, the tribe was visited on the upper Missouri by a US treaty commission consisting of General Henry Atkinson and Indian agent Benjamin O'Fallon, accompanied by a military escort of 476 men. General Atkinson and his fellow commissioner left Fort Atkinson on May 16, 1825. Ascending the Missouri, they negotiated treaties of friendship and trade with tribes of the upper Missouri, including the Arikara, the Cheyenne, the Crow, the Mandan, the Ponca, and several bands of the Sioux. At that time the US had competition from British traders on the upper Missouri, who came down from Canada.
The treaties acknowledged that the tribes lived within the United States, vowed perpetual friendship between the US and the tribes, and, recognizing the right of the United States to regulate trade, the tribes promised to deal only with licensed traders. The tribes agreed to forswear private retaliation for injuries, and to return or indemnify the owner of stolen horses or other goods. The commission's efforts to contact the Blackfoot and the Assiniboine were unsuccessful. Along their return to Fort Atkinson at the Council Bluff in Nebraska, the commission had successful negotiations with the Ota, the Pawnee and the Omaha.
Effects of the Emigrant Trail.
Increased traffic of emigrants along the related Oregon, Mormon and California trails, beginning in the early 1840s, heightened competition with Native Americans for scarce resources of water and game in arid areas. With resource depletion along the trails, the Cheyenne became increasingly divided into the Northern Cheyenne and Southern Cheyenne, where they could have adequate territory for sustenance.
During the California Gold Rush, emigrants brought in cholera. It spread in mining camps and waterways due to poor sanitation. The disease was generally a major cause of death for emigrants, about one-tenth of whom died during their journeys.
Perhaps from traders, the cholera epidemic reached the Plains Indians in 1849, resulting in severe loss of life during the summer of that year. Historians estimate about 2,000 Cheyenne died, one-half to two-thirds of their population. There were significant losses among other tribes as well, which weakened their social structures. Perhaps because of severe loss of trade during the 1849 season, Bent's Fort was abandoned and burned.
Fort Laramie Treaty of 1851.
In 1846 Thomas Fitzpatrick was appointed US Indian agent for the upper Arkansas and Platte River. His efforts to negotiate with the Northern Cheyenne, the Arapaho and other tribes led to a great council at Fort Laramie in 1851. Treaties were negotiated by a commission consisting of Fitzpatrick and David Dawson Mitchell, US Superintendent of Indian Affairs, with the Indians of the northern plains.
To reduce intertribal warfare on the Plains, the government officials "assigned" territories to each tribe and had them pledge mutual peace. In addition, the government secured permission to build and maintain roads for European-American travelers and traders through Indian country on the Plains, such as the Emigrant Trail and the Santa Fe Trail, and to maintain forts to guard them. The tribes were compensated with annuities of cash and supplies for such encroachment on their territories. The Fort Laramie Treaty of 1851 affirmed the Cheyenne and Arapaho territory on the Great Plains between the North Platte River and the Arkansas. This territory included what is now Colorado, east of the Front Range of the Rockies and north of the Arkansas River; Wyoming and Nebraska, south of the North Platte River; and extreme western Kansas.
Punitive US expedition of 1857.
In April 1856, an incident at the Platte River Bridge (near present-day Casper, Wyoming), resulted in the wounding of a Cheyenne warrior. He returned to the Cheyenne on the plains. During the summer of 1856, Indians attacked travelers along the Emigrant Trail near Fort Kearny. In retaliation, the US Cavalry attacked a Cheyenne camp on Grand Island in Nebraska. They killed ten Cheyenne warriors and wounded eight or more.
Cheyenne parties attacked at least three emigrant settler parties before returning to the Republican River. The Indian agent at Fort Laramie negotiated with the Cheyenne to reduce hostilities, but the Secretary of War ordered the 1st Cavalry Regiment (1855) to carry out a punitive expedition under the command of Colonel Edwin V. Sumner. He went against the Cheyenne in the spring of 1857. Major John Sedgwick led part of the expedition up the Arkansas River, and via Fountain Creek to the South Platte River. Sumner's command went west along the North Platte to Fort Laramie, then down along the Front Range to the South Platte. The combined force of 400 troops went east through the plains searching for Cheyenne.
Under the influence of the medicine man White Bull (also called Ice) and Grey Beard (also called Dark), the Cheyenne went into battle believing that strong spiritual medicine would prevent the soldiers' guns from firing. They were told that if they dipped their hands in a nearby lake, they had only to raise their hands to repel army bullets. Hands raised, the Cheyenne surrounded the advancing troops as they advanced near the Solomon River. Sumner ordered a cavalry charge and the troops charged with drawn sabers; the Cheyenne fled. With tired horses after long marches, the cavalry could not engage more than a few Cheyenne, as their horses were fresh.
This was the first battle which the Cheyenne fought against the US Army. Casualties were few on each side; J.E.B. Stuart, then a young lieutenant, was shot in the breast while attacking a Cheyenne warrior with a sabre. The troops continued on and two days later burned a hastily abandoned Cheyenne camp; they destroyed lodges and the winter supply of buffalo meat.
Sumner continued to Bent's Fort. To punish the Cheyenne, he distributed their annuities to the Arapaho. He intended further punitive actions, but the Army ordered him to Utah because of an outbreak of trouble with the Mormons. (This became known as the Utah War.) The Cheyenne moved below the Arkansas into Kiowa and Comanche country. In the fall, the Northern Cheyenne returned to their country north of the Platte.
The Pikes Peak Gold Rush.
Starting in 1859 with the Colorado Gold Rush, European-American settlers moved into lands reserved for the Cheyenne and other Plains Indians. Travel greatly increased along the Emigrant Trail along the South Platte River and some emigrants stopped before going on to California. For several years there was peace between settlers and Indians. The only conflicts were related to the endemic warfare between the Cheyenne and Arapaho of the plains and the Utes of the mountains.
US negotiations with Black Kettle and other Cheyenne favoring peace resulted in the Treaty of Fort Wise: it established a small reservation for the Cheyenne in southeastern Colorado in exchange for the territory agreed to in the Fort Laramie Treaty of 1851. Many Cheyenne did not sign the treaty, and they continued to live and hunt on their traditional grounds in the Smokey Hill and Republican basins, between the Arkansas and the South Platte, where there were plentiful buffalo.
Efforts to make a wider peace continued, but in the spring of 1864, John Evans, governor of Colorado Territory, and John Chivington, commander of the Colorado Volunteers, a citizens militia, began a series of attacks on Indians camping or hunting on the plains. They killed any Indian on sight and initiated the Colorado War. General warfare broke out and Indians made many raids on the trail along the South Platte which Denver depended on for supplies. The Army closed the road from August 15 until September 24, 1864.
On November 29, 1864, the Colorado Militia attacked a Cheyenne and Arapaho encampment under Chief Black Kettle, although it flew a flag of truce and indicated its allegiance to the US government. The Sand Creek massacre, as it came to be known, resulted in the death of between 150 and 200 Cheyenne, mostly unarmed women and children. The survivors fled northeast and joined the camps of the Cheyenne on the Smokey Hill and Republican rivers. There warriors smoked the war pipe, passing it from camp to camp among the Sioux, Cheyenne and Arapaho.
In January 1865 they planned and carried out an attack with about 1000 warriors on Camp Rankin, a stage station and fort at Julesburg. The Indians made numerous raids along the South Platte, both east and west of Julesburg, and raided the fort again in early February. They captured much loot and killed many European Americans. Most of the Indians moved north into Nebraska on their way to the Black Hills and the Powder River. (See Battle of Julesburg, Battle of Mud Springs, Battle of Rush Creek, Powder River Expedition, Battle of Platte Bridge)
Black Kettle continued to desire peace and did not join in the second raid or in the plan to go north to the Powder River country. He left the large camp and returned with 80 lodges of his tribesmen to the Arkansas River, where he intended to seek peace with the US.
Battle of Washita River.
Four years later, on November 27, 1868, George Armstrong Custer and his troops attacked Black Kettle's band at the Battle of Washita River. Although his band was camped on a defined reservation, complying with the government's orders, some of its members had been linked to raiding into Kansas by bands operating out of the Indian Territory. Custer claimed 103 Cheyenne "warriors" and an unspecified number of women and children killed whereas different Cheyenne informants named between 11 and 18 men (mostly 10 Cheyenne, 2 Arapaho, 1 Mexican trader) and between 17 and 25 women and children killed in the village.
There are conflicting claims as to whether the band was hostile or friendly. Historians believe that Chief Black Kettle, head of the band, was not part of the war party but the peace party within the Cheyenne nation. But, he did not command absolute authority over members of his band and the European Americans did not understand this. When younger members of the band took part in raiding parties, European Americans blamed the entire band for the incidents and casualties..
Battle of the Little Bighorn.
The Northern Cheyenne fought in the Battle of the Little Bighorn, which took place on June 25, 1876. The Cheyenne, together with the Lakota, other Sioux warriors and a small band of Arapaho, killed General George Armstrong Custer and much of his 7th Cavalry contingent of soldiers. Historians have estimated the population of the Cheyenne, Lakota and Arapaho encampment along the Little Bighorn River was approximately 10,000, making it one of the largest gatherings of Native Americans in North America in pre-reservation times. News of the event traveled across the United States and reached Washington, D.C., just as the nation was celebrating its Centennial. Public reaction arose in outrage against the Cheyenne.
Northern Cheyenne Exodus.
Following the Battle of the Little Bighorn, the US Army increased attempts to capture the Cheyenne. In 1879, after the Dull Knife Fight, when Crazy Horse surrendered at Fort Robinson, a few Cheyenne chiefs and their people surrendered as well. They were Dull Knife, Standing Elk and Wild Hog with around 130 Cheyenne. Later that year Two Moons surrendered at Fort Keogh, with 300 Cheyenne. The Cheyenne wanted and expected to live on the reservation with the Sioux in accordance to an April 29, 1868 treaty of Fort Laramie, which both Dull Knife and Little Wolf had signed.
As part of a US increase in troops following the Battle of the Little Bighorn, the Army reassigned Colonel Ranald S. Mackenzie and his Fourth Cavalry to the Department of the Platte. Stationed initially at Camp Robinson, they formed the core of the Powder River Expedition. It departed in October 1876 to locate the northern Cheyenne villages. On November 25, 1876, his column discovered and defeated a village of Northern Cheyenne in the Dull Knife Fight in Wyoming Territory. After the soldiers destroyed the lodges and supplies, and confiscated the horses, the Northern Cheyenne soon surrendered. They hoped to remain with the Sioux in the north but the US pressured them to locate with the Southern Cheyenne on their reservation in Indian Territory. After a difficult council, the Northern Cheyenne eventually agreed to go South.
When the Northern Cheyenne arrived at Indian Territory, conditions were very difficult: rations were inadequate, there were no buffalo near the reservation and according to several sources, there was malaria among the people. On 9 September 1878, a portion of the Northern Cheyenne, led by Little Wolf and Dull Knife started their trek back to the north. Upon reaching the northern area, they split into two bands. That led by Dull Knife (mostly women, children and elders) surrendered and were taken to Fort Robinson, where subsequent events became known as the Fort Robinson tragedy. Dull Knife's group was first offered food and firewood, and then after a week and a half, they were to told to go back to Indian territory. When they said no, they were then locked in the wooden barracks with no food, water or firewood for heat for four days. Most escaped in an estimated forty degrees below zero on January 9, 1879, but all were recaptured or killed.
Eventually the US forced the Northern Cheyenne onto a reservation, in southern Montana.
Northern Cheyenne Indian Reservation.
The Cheyenne who traveled to Fort Keogh (present day Miles City, Montana), including Little Wolf, settled near the fort. Many of the Cheyenne worked with the army as scouts. The Cheyenne scouts were pivotal in helping the Army find Chief Joseph and his band of Nez Percé in northern Montana. Fort Keogh became a staging and gathering point for the Northern Cheyenne. Many families began to migrate south to the Tongue River watershed area, where they established homesteads.
The US established the Tongue River Indian Reservation, now named the Northern Cheyenne Indian Reservation, of 371200 acre by the executive order of President Chester A. Arthur November 16, 1884. It excluded Cheyenne who had homesteaded further east near the Tongue River. The western boundary is the Crow Indian Reservation. On March 19, 1900, President William McKinley extended the reservation to the west bank of the Tongue River, making a total of 444157 acre. Those who had homesteaded east of the Tongue River were relocated to the west of the river.
The Northern Cheyenne, who were sharing the Lakota land at Pine Ridge Indian Reservation were finally allowed to return to the Tongue River on their own reservation. Along with the Lakota and Apache, the Cheyenne were the last nations to be subdued and placed on reservations. (The Seminole tribe of Florida never made a treaty with the US government.)
The Northern Cheyenne were given the right to remain in the north, near the Black Hills, land which they consider sacred. The Cheyenne also managed to retain their culture, religion and language. Today, the Northern Cheyenne Nation is one of the few American Indian nations to have control over the majority of its land base, currently 98%.
Culture.
Over the past 400 years, the Cheyenne have changed their lifestyles. In the 16th century they lived in the regions near the Great Lakes. They farmed corn, squash, and beans, and harvested wild rice like other indigenous peoples of the Northeastern Woodlands. They migrated west in the 18th century and hunted bison on the Great Plains. By the mid-19th century, the US forced them onto reservations.
The traditional Cheyenne government system is a politically unified system. The central traditional government system of the Cheyenne is the Arrow Keeper, followed by the Council of Forty-Four. Early in Cheyenne history, three related tribes known as the "Heviqsnipahis", the "Só'taeo'o" and the "Masikota", unified themselves to form the "Tsé-tsêhéstâhese" or the "Like Hearted People" who are known today as the "Cheyenne." The unified tribe then divided themselves into ten principal bands:
Each of the ten bands had four seated chief delegates; the remaining four chiefs were the principal advisers of the other delegates. Smaller bands or sub-bands had no right to send delegates to the council. This system also regulated the Cheyenne military societies that developed for planning warfare, enforcing rules, and conducting ceremonies.
Anthropologists debate about Cheyenne society organization. On the plains, it appears they had a bilateral band kinship system. However, some anthropologists reported that the Cheyenne had a matrilineal band system. Studies into whether, and if so, how much the Cheyenne developed a matrilineal clan system are continuing.
A Cheyenne sun dance gathering, c. 1909.
Traditional Cheyenne plains culture.
While they participated in nomadic Plains horse culture, men hunted and occasionally fought with and raided other tribes. The women tanned and dressed hides for clothing, shelter, and other uses. They also gathered roots, berries, and other useful plants. From the products of hunting and gathering, the women also made lodges, clothing, and other equipment. Their lives were active and physically demanding. The range of the Cheyenne was first the area in and near the Black Hills, but later all the Great Plains from Dakota to the Arkansas River.
Role models.
A Cheyenne woman has a higher status if she is part of an extended family with distinguished ancestors. Also, if she is friendly and compatible with her female relatives and does not have members in her extended family who are alcoholics or otherwise in disrepute. It is expected of all Cheyenne women to be hardworking, chaste, modest, skilled in traditional crafts, knowledgeable about Cheyenne culture and history and speak Cheyenne fluently. Tribal powwow princesses are expected to have these characteristics.

</doc>
<doc id="56129" url="http://en.wikipedia.org/wiki?curid=56129" title="Noetherian ring">
Noetherian ring

In mathematics, more specifically in the area of abstract algebra known as ring theory, a Noetherian ring is a ring that satisfies the ascending chain condition on ideals; that is, given any chain:
there exists an "n" such that:
There are other equivalent formulations of the definition of a Noetherian ring and these are outlined later in the article. 
Noetherian rings are named after Emmy Noether. 
The notion of a Noetherian ring is of fundamental importance in both commutative and noncommutative ring theory, due to the role it plays in simplifying the ideal structure of a ring. For instance, the ring of integers and the polynomial ring over a field are both Noetherian rings, and consequently, such theorems as the Lasker–Noether theorem, the Krull intersection theorem, and the Hilbert's basis theorem hold for them. Furthermore, if a ring is Noetherian, then it satisfies the descending chain condition on "prime ideals". This property suggests a deep theory of dimension for Noetherian rings beginning with the notion of the Krull dimension.
Characterizations.
For noncommutative rings, it is necessary to distinguish between three very similar concepts:
For commutative rings, all three concepts coincide, but in general they are different. There are rings that are left-Noetherian and not right-Noetherian, and vice versa.
There are other, equivalent, definitions for a ring "R" to be left-Noetherian:
Similar results hold for right-Noetherian rings.
For a commutative ring to be Noetherian it suffices that every prime ideal of the ring is finitely generated. (The result is due to I. S. Cohen.)
Examples.
Rings that are not Noetherian tend to be (in some sense) very large. Here are three examples of non-Noetherian rings:
However, a non-Noetherian ring can be a subring of a Noetherian ring: trivially because any integral domain is a subring of a field. For a less trivial, 
Indeed, there are rings that are left Noetherian, but not right Noetherian, so that one must be careful in measuring the "size" of a ring this way. 
A unique factorization domain is not necessarily a noetherian ring. It does satisfy a weaker condition: the ascending chain condition on principal ideals.
A valuation ring is not Noetherian unless it is a principal ideal domain. It gives an example of a ring that arises naturally in algebraic geometry but is not Noetherian.
Primary decomposition.
In the ring Z of integers, an arbitrary ideal is of the form ("n") for some integer "n" (where ("n") denotes the set of all integer multiples of "n"). If "n" is non-zero, and is neither 1 nor −1, by the fundamental theorem of arithmetic, there exist primes "pi", and positive integers "ei", with formula_5. In this case, the ideal ("n") may be written as the intersection of the ideals ("piei"); that is, formula_6. This is referred to as a "primary decomposition" of the ideal ("n"). 
In general, an ideal "Q" of a ring is said to be "primary" if "Q" is proper and whenever "xy" ∈ "Q", either "x" ∈ "Q" or "yn" ∈ "Q" for some positive integer "n". In Z, the primary ideals are precisely the ideals of the form ("pe") where "p" is prime and "e" is a positive integer. Thus, a primary decomposition of ("n") corresponds to representing ("n") as the intersection of finitely many primary ideals. 
Since the fundamental theorem of arithmetic applied to a non-zero integer "n" that is neither 1 nor −1 also asserts uniqueness of the representation formula_5 for "pi" prime and "ei" positive, a primary decomposition of ("n") is essentially "unique". 
For all of the above reasons, the following theorem, referred to as the "Lasker–Noether theorem", may be seen as a certain generalization of the fundamental theorem of arithmetic:
Lasker-Noether Theorem. Let "R" be a commutative Noetherian ring and let "I" be an ideal of "R". Then "I" may be written as the intersection of finitely many primary ideals with distinct radicals; that is:
with "Qi" primary for all "i" and Rad("Qi") ≠ Rad("Qj") for "i" ≠ "j". Furthermore, if:
is decomposition of "I" with Rad("Pi") ≠ Rad("Pj") for "i" ≠ "j", and both decompositions of "I" are "irredundant" (meaning that no proper subset of either {"Q"1, ..., "Qt"} or {"P"1, ..., "Pk"} yields an intersection equal to "I"), "t" = "k" and (after possibly renumbering the "Qi") Rad("Qi") = Rad("Pi") for all "i".
For any primary decomposition of "I", the set of all radicals, that is, the set {Rad("Q"1), ..., Rad("Qt")} remains the same by the Lasker–Noether theorem. In fact, it turns out that (for a Noetherian ring) the set is precisely the assassinator of the module "R"/"I"; that is, the set of all annihilators of "R"/"I" (viewed as a module over "R") that are prime.

</doc>
<doc id="56130" url="http://en.wikipedia.org/wiki?curid=56130" title="Artinian">
Artinian

In mathematics, Artinian, named for Emil Artin, is an adjective that describes objects that satisfy particular cases of the descending chain condition.

</doc>
<doc id="56133" url="http://en.wikipedia.org/wiki?curid=56133" title="Battle Angel Alita">
Battle Angel Alita

Battle Angel Alita, known in Japan as Gunnm (銃夢, Ganmu, a portmanteau of "Gun" and "Mu" the "onyomi" for the Japanese kanji for "Dream"), is a manga series created by Yukito Kishiro in 1990 and originally published in Shueisha's "Business Jump" magazine. Two of the nine-volume comics were adapted into two anime original video animation episodes titled "Battle Angel" for North American release by ADV Films and the UK and Australian release by Manga Entertainment. Manga Entertainment also dubbed "Battle Angel Alita" into English.
The series is set in the post-apocalyptic future and focuses on Alita, a cyborg who has lost all memories and is found in a garbage heap by a cybernetics doctor who rebuilds and takes care of her. She discovers that there is one thing she remembers, the legendary cyborg martial art Panzer Kunst, which leads to her becoming a Hunter Warrior or bounty hunter. The story traces Alita's attempts to rediscover her past and the characters whose lives she impacts on her journey. The manga series is continued in "" and "Gunnm Mars Chronicle".
Plot.
"Battle Angel Alita" tells the story of Alita ("Gally" in the original Japanese version), an amnesiac female cyborg. Her intact head and chest, in suspended animation, are found by cybermedic expert Daisuke Ido in the local dump. Ido manages to revive her, and finding she has lost her memory, names her Alita after his deceased cat. The rebuilt Alita soon discovers that she remembers the legendary martial art Panzer Kunst, although she does not recall anything else. Alita uses her Panzer Kunst to first become a mercenary Hunter-Warrior, killing cyborg criminals in the Scrapyard, and then as a player in the brutal sport of Motorball. While in combat, Alita awakens memories of her earlier life on Mars. She becomes involved with the floating city of Tiphares as one of their agents, and is sent to hunt criminals down. Foremost is the mad genius Desty Nova, who clashes with Alita before becoming her ally.
The futuristic dystopian world of "Battle Angel Alita" revolves around the city of Scrapyard, grown up around a massive scrap heap that rains down from Tiphares (Salem in the anime). Ground dwellers have no access to Tiphares and are forced to make a living in the sprawl below. Many are heavily modified by cybernetics to better cope with their hard life.
Tiphares exploits the Scrapyard and surrounding farms, paying mercenaries (called Hunter-Warriors) to hunt criminals and arranging violent sports to keep the population entertained. Massive tubes connect the Scrapyard to Tiphares, and the city uses robots for carrying out errands and providing security on the ground. Occasionally, Tiphareans (such as Ido Daisuke and Desty Nova) are exiled and sent to the ground. Aside from the robots and exiles, there is little contact between the two cities.
The story takes place in the former United States. According to a map, printed in the eighth volume, Scrapyard/Tiphares is near Kansas City, Missouri, and the Necropolis is Colorado Springs, Colorado. Radio KAOS is at Dallas. Figure's coastal hometown is Alhambra. Desty Nova's Granite Inn is built out of a military base – NORAD at Cheyenne Mountain Complex, Colorado.
"Battle Angel Alita" is revealed to take place in the 26th century. The characters refer to years after ES (Era Sputnik), based on the launch of "Sputnik 1" in 1957. About fourteen years pass after Daisuke Ido discovers Alita in the first graphic novel. This places the entire "Battle Angel Alita" manga mostly between ES 577 and ES 591, or 2533 and 2547 AD.
Characters.
"Battle Angel Alita" features a diverse cast of characters, many of whom shift in and out of focus as the story progresses. Some are never to be seen again following the conclusion of a story arc, while others make recurring appearances. The one character who remains a constant throughout is Alita, the protagonist and title character, a young cyborg with amnesia struggling to uncover her forgotten past through the only thing she remembers from it: by fighting. Early on in the story, Daisuke Ido, a bounty-hunting cybernetic doctor who finds and revives Alita plays a major role as well, but midway through the manga he becomes marginalized as focus begins to increasingly shift to Desty Nova, an eccentric nanotechnology scientist who has fled from Tiphares. Nova is the mastermind behind many of the enemies and trials that Alita faces, but does not make an actual appearance until more than two years into the story, although he is alluded to early on. Finally, Kaos, Desty Nova's son, a frail and troubled radio DJ with psychometric powers, also begins to play a crucial role after he comes in contact with Alita. He broadcasts his popular radio show from the wastelands outside the Scrapyard, staying away from the increasing conflict between Tiphares and the rebel army Barjack.
Production.
Besides renaming "Gally" to "Alita", the North American version of the manga also changed the city of "Salem" to "Tiphares", after Tiferet. Since Kishiro also used the name "Jeru" for the facility atop "Salem", "Jeru" was renamed "Ketheres" in the translation, after Keter. To further develop the Biblical theme in the original series, "Salem"'s main computer was named "Melchizedek", "the king of Salem" and "priest to the Most High God".
Media.
Manga.
The manga was first published in Shueisha's "Business Jump" magazine. It was then serialized from 1990 to 1995 in nine "tankōbon". In the U.S., Viz originally released the story in a 25 page comic book, it then followed the same volume format as its Japanese counterpart. "Battle Angel Alita" was licensed for international release in a number of languages and regions. It was published in Spain by Planeta DeAgostini, in Brazil by Editora JBC, in France and Netherlands by Glenat, in Poland by JPF, in Germany by Carlsen and in Taiwan by Tong Li Publishing.
A second manga titled, "Gunnm: Another Stories" (銃夢外伝, Ganmu Gaiden) published in "Ultra Jump" from January 24, 1997 to December 19, 2006. It was released in a single volume on December 19, 2007. The manga contains four short side stories: "Furusato", "Seiyakyoku", "Sonic Finger", and "Bashaku Ondo".
A 6-volume special edition titled "Gunnm: Complete Edition" was released in Japan on December 23, 1998. The series was released in B5 format and contains the original story, but with a different ending accommodating for the continuation of the story in "". Included are also rough sketches, a timeline and three short stories published later as "Gunnm: Another Stories".
A novel spin-off was released on April 4, 1997 by JUMP j-BOOKS, as part of the Japanese publisher Shueisha.
OVA.
A two episode OVA was released in 1993, incorporating elements from the first two volumes of the manga with changes to the characters and storyline. According to Kishiro, only two episodes were originally planned. At the time, he was too busy with the manga "to review the plan coolly", nor was he serious about an anime adaptation. It remains the only anime adaptation of "Battle Angel Alita" to date and there are no plans to revive it.
A 3 minute 3D-CGI rendered movie clip is included in volume 6 of the Japanese "Gunnm: Complete Edition". It showcases Alita in a Third League motorball race with players from two of her races such as "Armor" Togo, Degchalev, and Valdicci, and depicts events from both of those races.
Film.
Director James Cameron has rights to the film adaptation of "Battle Angel". Cameron is said to be a big fan of the manga, and he was waiting until CGI technology was sufficiently advanced to make a live-action 3D film with effects comparable to "Avatar". However, he is working on "Avatar" sequels before starting Alita. On April 18, 2012, Cameron confirmed that Battle Angel is a low priority for him right now, and he doesn't know when he'll get to it.
Cameron's producer Jon Landau says “I am sure you will get to see Battle Angel. It is one of my favourite stories, a great story about a young woman’s journey to self-discovery. It is a film that begs the question: What does it mean to be human?' Are you human if you have a heart, a brain or a soul? I look forward to giving the audience the film.” The film will likely not hit screens before 2017. Landau half-jokingly stated that the project may be titled ""Alita: The Battle Angel"," because of Cameron's tradition in naming his films with either an "A" or a "T."
Cameron's film would be a live-action adaption of the first four volumes of the manga series; "What I’m going to do is take the spine story and use elements from the first four books. So, the Motorball from books three and four, and parts of the story of one and two will all be in the movie". He has also stated that he has no one in mind for casting yet.
Video game.
' is an action RPG video game for the PlayStation by Banpresto. It is an adaptation of the manga, following Alita (Gally) from her discovery in the Tiphares dump heap by Daisuke Ido up through and beyond her career as a TUNED agent. The story includes additional elements that Kishiro had conceived when he ended the original manga in 1995, but was unable to implement at the time, which involved Alita going into outer space. He then expanded the story, which formed the basis for the manga '.

</doc>
<doc id="56134" url="http://en.wikipedia.org/wiki?curid=56134" title="List of U.S. state songs">
List of U.S. state songs

Forty-nine of the fifty U.S. states that make up the United States of America have one or more state songs, which are selected by each state legislature, and/or state governor, as a symbol (or emblem) of that particular U.S. state. New Jersey does not have an official state song, while Virginia's state song, "Carry Me Back to Old Virginny", adopted in 1940, is now considered the "emeritus" state song and is scheduled to be replaced, having been rescinded by the Virginia General Assembly. In 2015, "Our Great Virginia" as a new state song was enacted.
Some U.S. states have more than one official state song, and may refer to some of their official songs by other names; for example, Arkansas officially has two state songs, plus a state anthem, and a state historical song. Arizona has a song that was written specifically as a state anthem in 1915, as well as the 1981 country hit "Arizona", which it adopted as the alternate state anthem in 1982.
A few of these songs are among the best-known songs in the U.S., including "Old Folks at Home" (better known as "Swanee Ribber" or "Suwannee River"), "Yankee Doodle", "You Are My Sunshine", "My Old Kentucky Home", "Rocky Top", and "Home on the Range"; a number of others are popular standards, including "Oklahoma!" (from the Rodgers and Hammerstein musical), Hoagy Carmichael's "Georgia on My Mind", "Tennessee Waltz", "Missouri Waltz", and "On the Banks of the Wabash, Far Away". Many of the others are much less well-known, especially outside the state.
Territories.
Some American overseas territories, although not U.S. states, have songs and marches of their own.

</doc>
<doc id="56135" url="http://en.wikipedia.org/wiki?curid=56135" title="Touchstone (assaying tool)">
Touchstone (assaying tool)

A touchstone is a small tablet of dark stone such as fieldstone, slate, or lydite, used for assaying precious metal alloys. It has a finely grained surface on which soft metals leave a visible trace.
History.
The touchstone was used in ancient Greece. Its role in the introduction of monetary economy was explored by science historian James Burke in the second episode of his 1978 BBC television series "Connections". It was also used by the Indus Valley Civilization about 3500 BC for testing the purity of soft metals.
Usage.
Drawing a line with gold on a touchstone will leave a visible trace. Because different alloys of gold have different colours (see gold) the unknown sample can be compared to samples of known purity. This method has been used since ancient times. In modern times, additional tests can be done. The trace will react in different ways to specific concentrations of nitric acid or aqua regia, thereby identifying the quality of the gold. Thus, 24 carat gold is not affected but 14 carat gold will show chemical activity.

</doc>
<doc id="56136" url="http://en.wikipedia.org/wiki?curid=56136" title="Yukito Kishiro">
Yukito Kishiro

Yukito Kishiro (木城 ゆきと, Kishiro Yukito, born March 20, 1967, Ōta, Tokyo) is a Japanese manga artist. He is best known for "Battle Angel Alita".
Kishiro began his career at age 17, with his debut manga, "Kikai" in the "Weekly Shonen Sunday".

</doc>
<doc id="56139" url="http://en.wikipedia.org/wiki?curid=56139" title="Butanol">
Butanol

Butanol (also butyl alcohol) refers to a four-carbon alcohol with a formula of C4H9OH. There are four possible isomeric structures for butanol, from a straight-chain primary alcohol to a branched-chain tertiary alcohol. It is primarily used as a solvent, as an intermediate in chemical synthesis, and as a fuel. It is sometimes also called "biobutanol" when produced biologically.
Isomers.
The unmodified term "butanol" usually refers to the straight chain isomer with the alcohol functional group at the terminal carbon, which is also known as "n"-butanol or 1-butanol. The straight chain isomer with the alcohol at an internal carbon is "sec"-butanol or 2-butanol. The branched isomer with the alcohol at a terminal carbon is isobutanol or 2-methyl-1-propanol, and the branched isomer with the alcohol at the internal carbon is "tert"-butanol or 2-methyl-2-propanol.
Butanol isomers, due to their different structures, have somewhat different melting and boiling points. "n"-Butanol and isobutanol have limited solubility, "sec"-butanol has substantially greater solubility, while "tert"-butanol is fully miscible with water. This is because all alcohols have a hydroxyl group which makes them polar which in turn tends to promote solubility in water. At the same time, the carbon chain of the alcohol resists solubility in water. Methanol, ethanol, and propanol are fully miscible with water, while n-butanol is only moderately soluble because of the balance between the two opposing solubility trends.
Toxicity.
Like many alcohols, butanol is considered toxic. It has shown low order of toxicity in single dose experiments to laboratory animals. and is considered safe enough for use in cosmetics. Brief, repeated overexposure with the skin can result in depression of the Central nervous system, as with other short-chain alcohols. Exposure may also cause severe eye irritation and moderate skin irritation. The main dangers are from prolonged exposure to fumes. In extreme cases this includes suppression of the central nervous system and even death. Under most circumstances, butanol is quickly metabolized to carbon dioxide.
It has not been shown to damage DNA or cause cancer.
Uses.
Biobutanol.
Butanol is considered as a potential biofuel (butanol fuel). Butanol at 85 percent strength can be used in cars designed for gasoline (petrol) without any change to the engine (unlike 85% ethanol), and it contains more energy for a given volume than ethanol and almost as much as gasoline, so a vehicle using butanol would return fuel consumption more comparable to gasoline than ethanol. Butanol can also be used as a blended additive to diesel fuel to reduce soot emissions.
Other uses.
Butanol sees use as a solvent for a wide variety of chemical and textile processes, in organic synthesis and as a chemical intermediate. It is also used as a paint thinner and a solvent in other coating applications where it is used as a relatively slow evaporating latent solvent in lacquers and ambient-cured enamels. It finds other uses such as a component of hydraulic and brake fluids.
Butanol is used in the synthesis of 2-Butoxyethanol
It is also used as a base for perfumes, but on its own has a highly alcoholic aroma.
Salts of butanol are chemical intermediates; for example, alkali metal salts of "tert"-butanol are "tert"-butoxides.
Production.
Since the 1950s, most butanol in the United States is produced commercially from fossil fuels. The most common process starts with propene (propylene), which is run through a hydroformylation reaction to form butyraldehyde, which is then reduced with hydrogen to 1-butanol and/or 2-butanol. Tert-butanol is derived from isobutane as a co-product of propylene oxide production. Butanol can also be produced by fermentation of biomass by bacteria. Prior to the 1950s, "Clostridium acetobutylicum" was used in industrial fermentation processes producing butanol. Research in the past few decades showed results of other microorganisms that can produce butanol through fermentation.

</doc>
<doc id="56140" url="http://en.wikipedia.org/wiki?curid=56140" title="History of Norway">
History of Norway

The history of Norway has been influenced to an extraordinary degree by the terrain and the climate of the region. About 10,000 BC, following the retreat of the great inland ice sheets, the earliest inhabitants migrated north into the territory which is now Norway. They traveled steadily northwards along the coastal areas, warmed by the Gulf Stream, where life was more bearable. In order to survive they fished and hunted reindeer (and other prey). Between 5,000 BC and 4,000 BC the earliest agricultural settlements appeared around the Oslofjord. Gradually, between 1500 BC and 500 BC, these agricultural settlements spread into the southern areas of Norway - whilst the inhabitants of the northern regions continued to hunt and fish.
The Neolithic period started 4000 BC. The Migration Period caused the first chieftains to take control and the first defenses to be made. From the last decades of the 8th century Norwegians started expanding across the seas to the British Isles and later Iceland and Greenland. The Viking Age also saw the unification of the country. Christianization took place during the 11th century and Nidaros became an archdiocese. The population expanded quickly until 1349 (Oslo: 3,000; Bergen: 7,000; Trondheim: 4,000) when it was halved by the Black Death and successive plagues. Bergen became the main trading port, controlled by the Hanseatic League. Norway entered the Kalmar Union with Denmark and Sweden in 1397.
After Sweden left the union in 1523, Norway became the junior partner in Denmark–Norway. The Reformation was introduced in 1537 and absolute monarchy imposed in 1661. In 1814 Norway was ceded from Denmark to Sweden and a constitution was passed. Norway declared its independence but was then occupied by Sweden, although the Parliament was allowed to continue to exist. Industrialization started in the 1840s and from the 1860s large-scale emigration to North America took place. In 1884 the king appointed Johan Sverdrup as prime minister, thus establishing parliamentarism. The union with Sweden was dissolved in 1905. From the 1880s to the 1920s, Norwegians such as Roald Amundsen carried out a series of important polar expeditions.
Shipping and hydroelectricity were important sources of income for the country. The following decades saw a fluctuating economy and the rise of the labor movement. Germany occupied Norway between 1940 and 1945 during the Second World War, after which Norway joined NATO and underwent a period of reconstruction under public planning. Oil was discovered in 1969 and by 1995 Norway was the world's second-largest exporter. This resulted in a large increase of wealth. From the 1980s Norway started deregulation in many sectors and experienced a banking crisis.
Prehistory.
Norway's coastline rose from glaciation with the end of the last glacial period about 12,000 BC. The first immigration took place during this period as the Norwegian coast offered good conditions for sealing, fishing and hunting. They were nomadic and by 9300 BC they were at Magerøya. Increased ice receding from 8000 BC caused settlement along the entire coastline. The Stone Age consisted of the Komsa culture in Troms and Finnmark and the Fosna culture further south. The Nøstvet culture took over from the Fosna culture ca. 7000 BC, which adapted to a warmer climate which gave increased forestation and new mammals for hunting. The oldest human skeleton ever discovered in Norway was found in shallow water off Sogne in 1994 and has been carbon dated to 6,600 BC. Ca. 4000 BC people in the north started using slate tools, earthenware, skis, sleds and large skin boats.
The first farming and thus the start of the Neolithic period, began ca. 4000 BC around the Oslofjord, with the technology coming from southern Scandinavia. The break-through occurred between 2900 and 2500 BC, when oats, barley, pigs, cattle, sheep and goats became common and spread as far north as Alta. This period also saw the arrival of the Corded Ware culture, who brought new weapons, tools and the Indo-European language, from which the Norwegian language developed.
Nordic Bronze Age (1700–500 BC).
The Bronze Age started in 1800 BC and involved innovations such as plowing fields with ards, permanents farms with houses and yards, especially in the fertile areas around the Oslofjord, Trondheimsfjord, Mjøsa and Jæren. Some yields were so high that it allowed farmers to trade furs and skins for luxury items, especially with Jutland. Ca. 1000 BC speakers of Uralic languages arrived in the north and assimilated with the indigenous population, becoming the Sami people.
A climate shift with colder weather started about 500 BC. The forests, which had previously consisted of elm, lime, ash and oak, were replaced with birch, pine and spruce. The climate changes also meant that farmers started building more structures for shelter. Knowledge of iron was introduced by Celts, resulting in better weapons and tools.
Nordic Iron Age (500 BC-800 AD).
The Iron Age allowed for easier cultivation and thus new areas were cleared as the population grew with the increased harvests. A new social structure evolved: when sons married, they would remain in the same house; such an extended family was a clan. They would offer protection from other clans; if conflicts arose, the issue would be decided at a "thing", a sacred place where all freemen from the surrounding area would assemble and could determine punishments for crimes, such as paying fines in food.
From the first century AD a cultural influence from the Roman Empire took place. Norwegians adapted letters and created their own alphabet, runes. Trading with Romans also took place, largely furs and skins in exchange for luxury goods. Some Scandinavians also served as Roman mercenaries. Some of the most powerful farmers became chieftains. They functioned as priests and accepted sacrifices from farmers which were again used to pay soldiers, creating a hird. Thus they were able to rule an area of several settlements and tribes.
The chieftains' power increased during the Migration Period between 400 to 550 as other Germanic tribes migrated northwards and local farmers wanted protection. This also resulted in the construction of simple fortifications. A plague hit southern Norway in the 6th century, with hundreds of farms being depopulated. Most were repopulated in the 7th century, which also saw the construction of several fishing hamlets and a boom in trade of iron and soapstone across the North Sea. Some chieftains were able to control most of the trade and grew in power throughout the 8th century.
Viking Age.
The Viking Age was a period of Scandinavian expansion through trade, colonization and raids. The first raid was against Lindisfarne in 793 and is considered the beginning of the Viking Age. This was possible because of the development of the longship, suitable for travel across the sea, and advanced navigation techniques.
Vikings were well-equipped, had chain mail armor, were well-trained and had a psychological advantage over Christian counterparts since they believed that being killed in combat would result in them going to Valhalla. In addition to gold and silver, an important outcome from the raids were thralls, which were brought to the Norwegian farms as a slave workforce. While the men were out at sea, the management of the farm was under the control of the women.
The lack of suitable farming land in Western Norway caused Norwegians to travel to the sparsely populated areas such as Shetland, Orkney, the Faroe Islands and the Hebrides to colonize—the latter which became the Kingdom of the Isles. Norwegian Vikings settled on the west coast of Ireland ca. 800 and founded the island's first cities, including Dublin. Their arrival caused the petty Celtic kings to ally and by 900 they had driven out the Norwegians.
Norwegians discovered Iceland in ca. 870 and within sixty years the island had been divided between four hundred chieftains. Led by Erik the Red, a group of Norwegians settled on Greenland in the 980s. His son, Leif Ericson, discovered Newfoundland in ca. 1000, naming it Vinland. Unlike Greenland, no permanent settlement was established there.
In the mid 9th century the largest chieftains of the petty kingdoms started a major power struggle. Harald Fairhair started the process of unifying Norway when he entered an alliance with the Earls of Lade and was able to unify the country after the decisive Battle of Hafrsfjord. He set up the very basics of a state administration with stewards in the most important former chieftain estates. His son Håkon the Good, who assumed the crown in 930, established two large things, Gulating for Western Norway and Frostating for Trøndelag, in which the king met with the freemen to make decisions. He also established the ledang, a conscription-based military. After his death in 960, war broke out between the Fairhair dynasty and the Earls of Lade in alliance with Danish kings.
Middle Ages.
Christianization and abolishing the rites in Norse mythology was first attempted by Olav Tryggvason, but he was killed in the Battle of Svolder in 1000. Olav Haraldsson, starting in 1015, made the "things" pass church laws, destroyed heathen hofs, built churches and created an institution of priests. Many chieftains feared that the Christianization would rob them of power in lieu of their roles as "Goðar" in traditional Norse Paganism. The two sides met in the Battle of Stiklestad, where Haraldsson was killed. The church elevated Haraldsson to sainthood, allowing Nidaros (today Trondheim) to become the Christian center of Norway. Within a few years the Danish rule had become sufficiently unpopular that Norway again became united.
From the 1040s to 1130 the country was at peace. In 1130 the civil war era broke out on the basis of unclear succession laws, which allowed all the king's sons to rule jointly. For periods there could be peace, before a lesser sons allied himself with a chieftain and started a new conflict. The Archdiocese of Nidaros was created in 1152 and attempted to control the appointment of kings. The church inevitably had to take sides in the conflicts, with the civil was also becoming an issue regarding the church's influence of the king. The wars ended in 1217 with the appointment of Håkon Håkonsson, who introduced clear law of succession.
From 1000 to 1300 the population increased from 150,000 to 400,000, resulting both in more land being cleared and the subdivision of farms. While in the Viking Age all farmers owned their own land, by 1300 seventy percent of the land was owned by the king, the church or the aristocracy. This was a gradual process which took place because of farmers borrowing money in poor times and not being able to repay. However, tenants would always remain free men and the large distances and often scattered ownership meant that they enjoyed much more freedom than their continental peers. In the 13th century about twenty percent of a farmer's yield went to the king, church and landowners.
14th century.
The 14th century is described as Norway's Golden Age, with peace and increase in trade, especially with the British Islands, although Germany became increasingly important towards the end of the century. Throughout the High Middle Ages the king established Norway as a state with a central administration with local representatives.
In 1349 the Black Death spread to Norway and had within a year killed a third of the population. Later plagues reduced the population to half the starting point by 1400. Many communities were entirely wiped out, resulting in an abundance of land, allowing farmers to switch to more animal husbandry. The reduction in taxes weakened the king's position, and many aristocrats lost the basis for their surplus, reducing some to mere farmers. High tithes to church made it increasingly powerful and the archbishop became a member of the Council of State.
The Hanseatic League took control over Norwegian trade during the 14th century and established a trading center in Bergen. In 1380 Olaf Haakonsson inherited both the Norwegian and Danish thrones, creating a union between the two countries. In 1397, under Margaret I, the Kalmar Union was created between the three Scandinavian countries. She waged war against the Germans, resulting in a trade blockade and higher taxation on Norwegians, which resulted in a rebellion. However, the Norwegian Council of State was too weak to pull out of the union.
Margaret pursued a centralising policy which inevitably favoured Denmark, because it had a greater population than Norway and Sweden combined. Margaret also granted trade privileges to the Hanseatic merchants of Lübeck in Bergen in return for recognition of her right to rule, and these hurt the Norwegian economy. The Hanseatic merchants formed a state within a state in Bergen for generations. Even worse were the pirates, the "Victual Brothers", who launched three devastating raids on the port (the last in 1427).
Norway slipped ever more to the background under the Oldenburg dynasty (established 1448). There was one revolt under Knut Alvsson in 1502. Norwegians had some affection for King Christian II, who resided in the country for several years. Norway took no part in the events which led to Swedish independence from Denmark in the 1520s.
Denmark–Norway.
Sweden was able to pull out of the Kalmar Union in 1523, thus creating Denmark–Norway under the rule of a king in Copenhagen. Frederick I of Denmark favoured Martin Luther's Reformation, but it was not popular in Norway, where the Church was the one national institution and the country was too poor for the clergy to be very corrupt. Initially, Frederick agreed not to try to introduce Protestantism to Norway but in 1529 he changed his mind. Norwegian resistance was led by Olav Engelbrektsson, Archbishop of Trondheim, who invited the old king Christian II back from his exile in Holland. Christian returned but was ambushed and spent the rest of his life in prison. Then Frederick died and a three-way war of succession broke out between the supporters of his eldest son Christian (III), his younger Catholic brother Hans and the followers of Christian II. Olaf Engelbrektsson again tried to lead a Catholic Norwegian resistance movement but he found little support. Christian III triumphed and sent him into exile and in 1536 Christian demoted Norway from a kingdom to a mere Danish province. The Reformation was imposed in 1537, strengthening the king's power. All church valuables were sent to Copenhagen and the forty percent of the land which was owned by the church came under the control of the king. Danish was introduced as a written language, although Norwegian remained distinct dialects. Professional administration was now needed and power shifted from the provincial nobility to the royal administration: district stipendiary magistrates were appointed as judges and the sheriffs became employees of the crown rather than of the local nobility. In 1572 a governor-general was appointed for Norway with a seat at Akershus Fortress in Oslo. From the 1620s professional military officers were employed.
The 17th century saw a series of wars between Denmark–Norway and Sweden. The Kalmar War between 1611–13 saw 8,000 Norwegian peasants conscripted. Despite lack of training, Denmark–Norway won and Sweden abandoned its claims to the area between Tysfjord and Varangerfjord. With the Danish participation in the Thirty Years' War in 1618–48, a new conscription system was created in which the country was subdivided into 6,000 "ledg", each required to support one soldier. Denmark–Norway lost the war and was forced to cede Jämtland and Härjedalen to Sweden. The Second Northern War in 1657 to 1660 resulted in Bohuslän being ceded to Sweden. The Danish monarchy became an absolutist and hereditary one in Norway in 1661. A new administrative system was introduced. Departments organized by portfolio were established in Copenhagen, while Norway was divided into counties, each led by a district governor, and further subdivided into bailiwicks. About 1,600 government officials were appointed throughout the country. Ulrik Fredrik Gyldenløve was the most famous viceroy of Norway (1664-1699).
The population of Norway increased from 150,000 in 1500 to 900,000 in 1800. By 1500 most deserted farms were repossessed. The period under absolutism increased the ratio of self-owning farmers from twenty to fifty percent, largely through sales of crown land to fiance the lost wars. Crofts became common in the absolutism period, especially in Eastern Norway and Trøndelag, with the smallholder living at the mercy of the farmer. There were 48,000 smallholders in 1800. Compared to Denmark, taxes were very low in Norway, typically at four to ten percent of the harvest, although the number of farms per "legd" decreased from four to two in the 1670s. Confirmation was introduced in 1736; as it required people to read, elementary education was introduced. The Norwegian economy improved with the introduction of the water-driven saw in the early 16th century. Norway had huge resources of timber but did not have the means to exploit much of it in the Middle Ages as only hand-tools were available. The new saw mills which sprang up in the fjords changed this. In 1544 a deal was struck with the Netherlands (then part of the Holy Roman Empire) and the Dutch controlled the export of Norwegian timber for the next 150 years. Amsterdam was built on piles from Norway. Tree-felling was done in the winter when farm-work was impossible and it was easy to get the felled trees across the snow to the rivers. In the spring, the logs floated down the rivers to the saw mills by the sea. By the mid-16th century the power of the Hanseatic League in Bergen was broken; though German craftsmen remained, they had to accept Danish rule. Many Norwegians earned a living as sailors in foreign ships, especially Dutch ones. The crews in both sides of the Anglo-Dutch Wars contained Norwegians. Norway benefitted from the many European wars of the 18th century. As a neutral power it was able to expand its share of the shipping market. It also supplied timber to foreign navies.
The entire period saw mercantilism as the basis for commerce, which involved import regulations and tariffs, monopolies and privileges throughout the county granted to burghers. The lumber industry became important in the 17th century through exports especially to England. To avoid deforestation, a royal decree closed a large number of sawmills in 1688; because this mostly affected farmers with small mills, by the mid 18th century only a handful of merchants controlled the entire lumber industry. Mining increased in the 17th century, the largest being the silver mines in Kongsberg and the copper mines in Røros. Fishing continued to be an important income for farmers along the coast, but from the 18th century dried cod started being salted, which required fishermen to buy salt from merchants. The first important period of Norwegian shipping was between 1690 and 1710, but the advantage was lost with Denmark–Norway entering the Great Northern War in 1709. However, Norwegian shipping regained its strength towards the end of the century.
Throughout the period, Bergen was the largest town in the country; its population of 14,000 in the mid 18th century was twice the size of Christiania (later Oslo) and Trondheim combined. Eight townships with privileges existed in 1660—by 1800 this had increased to twenty-three. During this period up to two-thirds of the country's audited national income was transferred to Copenhagen. In the last decades of the century, Hans Nielsen Hauge started the Haugean movement, which demanded the right to preach the word of God freely. The University of Oslo was established in 1811.
Union with Sweden.
Denmark–Norway entered the Napoleonic Wars on France's side in 1807. This had a devastating effect on the Norwegian economy as the Royal Navy hindered export by ship and import of food. Sweden invaded Norway the following year, but after several Norwegian victories a cease-fire was signed in 1809. After pressure from Norwegian merchants license trade was permitted with corn from Denmark to Eastern Norway in exchange for Norwegian timber export to the United Kingdom. Following the Battle of Leipzig in 1813, the Treaty of Kiel signed on 14 January 1814 ceded Norway to Sweden.
Christian Frederik, heir to the Danish crown, had since 1813 been governor-general of Norway. He traveled to Trondheim to gain support for his person and assembled twenty-one prominent citizens at Eidsvoll on 16 February 1814 where he laid claim to the throne. They rejected a new absolute monarch and instead wanted a liberal constitution; therefore, representatives from the entire country would be elected to create a constitution. The 112 members of the Constituent Assembly gathered and, after six weeks of discussion, concluded the work on the Constitution of Norway on 17 May 1814. Power would be split between the king—a position to which Christian Frederik was appointed—and the Parliament of Norway. King Carl Johan of Sweden invaded Norway in late July; at the Convention of Moss on 14 August Norway surrendered while Sweden accepted the constitution. The union between Sweden and Norway under Carl Johan was approved by Parliament on 4 November.
The Napoleonic Wars sent Norway into an economic crisis, as nearly all the merchants had gone bankrupt during the blockade. Recovery was difficult because of export tariffs and the country underwent strong inflation. The Norwegian speciedaler was established as a currency by the Bank of Norway when it was established in 1816, financed through a silver tax which lasted until 1842. Under threat of a coup d'état by Carl Johan, Norway reluctantly paid the debt stated in the Treaty of Kiel, despite never having ratified it. Constitution Day on 17 May became an important political rally every year; in 1829 the Swedish governor-general Baltzar von Platen resigned after he used force against demonstrators in the Battle of the Square. The first half of the century was dominated by the ca. 2,000 officials, as there were few bourgeois and no aristocracy following an 1821 decision to abolish nobility. From the 1832 election, farmers became more conscious of electing themselves, resulting in a majority of farmers in Parliament. This resulted in rural tax cuts and higher import tariffs, shifting the tax burden to the cities. They also passed the Local Committees Act, which established elected municipal councils from 1838. Cultural expression from the 1840s to the 1870s was dominated by the romantic nationalism, which emphasized the uniqueness of Norway.
The textile industry started in the 1840s, which was followed up with mechanical workshops to build new machinery as the British embargo hindered import of textile machinery. An economic crisis hit the country from 1848, resulting in Marcus Thrane establishing the first trade unions and demanding that quality for the law independent of social class. Parliament passed a series of laws abandoning economic privileges and easing domestic trade during the 1840s and 1850s. Population increase forced the clearing of new land, although some of the growth came in the cities. The population of Christiania reached 40,000 in 1855. By 1865 the population reached 1.7 million; the large increase was largely caused by better nutrition from herring and potatoes, a sharp decrease of infant mortality and increased hygiene. Emigration to North America started in 1825, with the first mass emigration commencing in the 1860s. By 1930, 800,000 people had emigrated, the majority settling in the Midwestern United States.
The population decrease resulted in a labor shortage in the agriculture, which again resulted in increased use of machinery and thus capital. The government stimulated the process through the creation of the Mortgage Bank in 1851 and the State Agricultural College eight years later. The 19th century saw a large increase of road construction and steamship services commenced along the coast. The first railway, the Trunk Line between Christiania and Eidsvoll opened in 1854, followed a year later by the first telegraph line. Export industry commenced with steam-powered sawmills in the 1860s, followed by canned herring, wood pulp and cellulose. From 1850 to 1880 the Norwegian shipping industry enjoyed a large boom, stimulated by the abolishing of the British Navigation Acts. By 1880 there were 60,000 Norwegian seamen and the country had the world's third-largest merchant marine. As the first coast-to-coast railway, the Røros Line connected the capital to Trondheim in 1877. Norway joined the Scandinavian Monetary Union in 1875 and introduced the Norwegian krone with a gold standard, along with the metric system being introduced.
Annual parliamentary sessions were introduced from 1869 and in 1872 ministers were, though a constitutional amendment, required to meet in Parliament to defend their policies. The king, despite having no constitutional right to do so, vetoed the amendment in three successive parliaments. The 1882 election saw the first two parties, the Liberals and Conservatives, run for election, and subsequently the majority succeeded at impeaching the cabinet. In 1884 the king appointed majority leader Johan Sverdrup as prime minister, thus establishing parliamentarism as the first European country. The Liberal Party introduced a series of legal reforms, such as increasing the voting rights to about half of all men, settling the language conflict by establishing two official written standards, Riksmål and Landsmål, introduced juries, seven years of compulsory education and, as the first European country, universal suffrage for men in 1889.
The 1880s and 1890s saw the rise of the labor movement and trade unions became common; the Norwegian Confederation of Trade Unions was established in 1899 and the Norwegian Employers' Confederation the following year. The Labor Party had its first parliamentary members elected in 1903. The women's issue became increasingly dominant through the 1880s and they were gradually permitted to take secondary and tertiary education. Norwegian support of the union decreased towards the end of the 1890s, especially following the 1897 Swedish abolition of the free trade agreement and the lack of a Norwegian foreign minister. Negotiations of independence commenced, but were not effective because of shifting governments and the Swedish threat of war.
Independence.
With the four-party Michelsen's Cabinet appointed in 1905, Parliament voted to establish a Norwegian consular service. This was rejected by the king and on 7 June Parliament unanimously approved the dissolution of the union. In the following dissolution referendum, only 184 people voted in favor of a union. The government offered the Norwegian crown to Denmark's Prince Carl, who after a plebiscite became Haakon VII. The following ten years, Parliament passed a series of social reforms, such as sick pay, factory inspection, a ten-hour working day and worker protection laws. Waterfalls for hydroelectricity became an important resource in this period and the government secured laws to hinder foreigners from controlling waterfalls, mines and forests. Large industrial companies established in these years were Elkem, Norsk Hydro and Sydvaranger. The Bergen Line was completed in 1909, the Norwegian Institute of Technology was established the following year and women's suffrage was introduced in 1913—as the second country in the world. From the 1880s to the 1920s, Norwegians carried out a series of polar expeditions. The most important explorers were Fridtjof Nansen, Roald Amundsen and Otto Sverdrup. Amundsen's expedition in 1911 became the first to reach the South Pole.
Norway adopted a policy of neutrality from 1905; during World War I the Norwegian merchant marine was largely used in support of the British, resulting in Norway being classified as The Neutral Ally. Half the Norwegian fleet and 2,000 seamen were killed by the German Atlantic U-boat Campaign. Some merchants made huge profits from trade and shipping during the war, resulting in an increased division between the classes. The interwar period was dominated by economic instability caused among other by strikes, lock-outs and the monetary policy causing deflation to compensate for too much money having been issued during the war and thus hindering investments. Especially fishermen were hit hard in the period, while farmers retained market prices through organizing regulations. Unemployment peaked at ten percent between 1931 and 1933. Although industrial production increased by eighty percent from 1915 to 1939, the number of jobs remained stable. The Norwegian School of Economics was established in 1936.
Norway had nine governments between 1918 and 1935, nearly all minority and lasting an average eighteen months. The Agrarian Party was established in 1920, although this period saw a rise of support for the Conservatives. The Labor Party split in 1921, with the left wing establishing the Communist Party. Although strong during the 1920s, they were marginalized through the 1930s. A short-lived Labor Government reigned in 1928, but did not establish a sound parliamentary support until the 1935 Nygaardsvold's Cabinet, based on an alliance with the Agrarian Party. During the 1920s and 1930s, Norway established three dependencies, Bouvetøya, Peter I Island and Queen Maud Land, annexed Jan Mayen and secured sovereignty of Svalbard through the Svalbard Treaty. Norway's first civil airport, Stavanger, opened in 1937.
World War II.
From the start of World War II in 1939, Norway retained a strict neutrality. Both Britain and Germany realized the strategic location; both made plans to invade Norway, regardless of Norwegian opposition. The Germans struck first and attacked Norway on April 9, 1940. After furious battles with the Norwegians and British forces, Germany prevailed and controlled Norway until the end of the war. The German goal was to use Norway to control access to the North Sea and the Atlantic, and to station air and naval forces to stop convoys from Britain to the USSR.
Government in exile.
The government in exile, including the royal family, escaped to London. Politics were suspended and the government coordinated action with the Allies, retained control of a world-wide diplomatic and consular service, and operated the huge Norwegian merchant marine. It organized and supervised the resistance within Norway. One long-term impact was the abandonment of a traditional Scandinavian policy of neutrality; Norway became a founding member of NATO in 1949. Norway at the start of the war had the world's fourth largest merchant fleet, at 4.8 million tons, including a fifth of the world's oil tankers. The Germans captured about 20% of the fleet but the remainder, about 1000 ships, were taken over by the government. Although half the ships were sunk, the earnings paid the expenses of the government.
Quisling regime.
Vidkun Quisling proclaimed himself prime minister and appointed a government with members from the National Unity Party. He was quickly set aside and replaced by Josef Terboven, but reinstated in 1942. The Norwegian Campaign continued in Northern Norway and the government fled to London on 7 June. The German occupation resulted in a brutalization of society and 30,000 people were imprisoned. 55,000 people joined the National Unity Party, which became the only legal party. But the nazification process failed after the Supreme Court resigned and both organized sports and bishops boycotted the new regime. A resistance movement was established and became coordinate from London from 1943. Stokker reports that hostile humour against the Germans helped maintain morale and build a wall against collaboration. Jokes made the rounds dripping with contempt for the oppressors, ridicule of Nazi ideology, stressing the cruelty of the Nazis and mocking their inflated self-image. People on the street asked, "Do you know the difference between the Nazis and a bucket of manure? The bucket." In Post Office lines they explained, "It's rumored that we're getting new stamps bearing Quisling's likeness, but distribution has been delayed because no one knows which side to spit on." The jokes worked to educate Norwegians about the occupation, and encourage a sense of solidarity. At the time of German surrender on 8 May 1945, there were 360,000 German soldiers in the country.
Postwar.
In the following legal purge, 53,000 people were sentenced for treason and 25 were executed. The post-war years saw an increased interest in Scandinavism, resulting in Scandinavian Airlines System in 1946, the Nordic Council in 1952 and the Nordic Passport Union along with the metric system being introduced. Reconstruction after the war gave Norway the highest economic growth in Europe until 1950, partially created through rationing private consumption allowing for higher industrial investments. The Labor Party retained power throughout the period and enforced a policy of public planning. The University of Bergen was created in 1946. The 1950s saw a boom in construction of hydroelectricity and the state built the steel mill Norsk Jernverk and two aluminum works. State banks such as the State Housing Bank, the State Educational Loan Fund and Postbanken allowed for governmental control over private debt. Oslo hosted the 1952 Winter Olympics.
Norway retained its neutrality policy until 1947, focusing on its membership in the United Nations, where Trygve Lie had become the first secretary-general. Norway joined the Marshall Plan in 1947, receiving US$400 million in American support. Anti-communism grew with a Soviet proposal for joint control over Svalbard and especially after the 1948 Czechoslovak coup d'état, after which the Communist Party lost all influence. Norway started negotiation the creation of a Scandinavian defense union, but instead opted to become a founding member of the North Atlantic Treaty Organization (NATO). However, Norway never allowed permanently stationed foreign troops or nuclear weapons on Norwegian soil to avoid agitating the Soviet Union, with which Norway from 1944 shared a land border with. NATO financed large parts of the Norwegian military investments, which ultimately resulted in a numerous airports being built throughout the 1950s and 1960s.
Sales of cars were deregulated in October 1960, the same year as the Norwegian Broadcasting Corporation introduced Norway's first television broadcasts. Norway feared competition from Swedish industry and Danish agriculture and chose to not join any free trade organizations until 1960, when it joined the European Free Trade Association. Throughout the post-war period both fishing and agriculture became more mechanized, the agricultural subsidies rose to the third-highest in the world and the number of small-scale farms and fishermen fell dramatically. The Socialist People's Party was created in 1961 by former Labor politicians who disagreed with the Labor Party's NATO, nuclear and European policies. Following the Kings Bay Affair the right-winged Lyng's Cabinet ruled for a month. The right-wing coalition Borten's Cabinet won the 1965 election, sat for six years and started a trend of shifting Labor and right-winged governments. Norwegianization of Samis halted after the war and Sami rights became an increasing issue, with a council being established in 1964.
The completion of the Nordland Line to Bodø in 1962 concluded the construction of new railway routes, while the first part of the Oslo Metro opened in 1966. A social security net was gradually introduced after the war: child allowance was introduced in 1946 and the Social Care Act was introduced in 1964. The 1960s saw good times for the heavy industry and Norway became Europe's largest exporter of aluminum and the world's largest exporter of ferroalloys. The University of Trondheim and the University of Tromsø both opened in 1968, one year before a network of regional colleges started being opened. Influenced by American culture and similar actions abroad, youth and students started an uproar against cultural norms. The 1960s saw an increased focus on environmentalism, especially through activism, based on ever-more conversion of waterfalls to hydro stations, pollution and the dilapidation of herring stocks. Rondane National Park was created as the country's first in 1962 and the Ministry of the Environment was the first in the world when it was established in 1972. A network of regional airports were built in Western and Northern Norway in the late 1960s and early 1970s. Membership in the European Economic Community was rejected in a 1972 referendum.
Oil Age.
Prospecting in the North Sea started in 1966 and in 1969 Phillips Petroleum found oil in the Ekofisk field—which proved to be among the ten largest fields in the world. Operations of the fields was split between foreign operators, the state-owned Statoil, the partially state-owned Norsk Hydro and Saga Petroleum. Ekofisk experienced a major blowout in 1977 and 123 people were killed when the Alexander Kielland accommodation rig capsized in 1980; these incidents led to a strengthening of petroleum safety regulations. The oil industry not only created jobs in production, but a large number of supply and technology companies were established. Stavanger became the center of this industry. High petroleum taxes and dividends from Statoil gave high income from the oil industry to the government.
Norway established its exclusive economic zone in the 1970s, receiving an area of 2000000 km2. A series of border disputes followed; agreements were reached with Denmark and Iceland in the 1990s, but the border in the Barents Sea was not agreed upon until 2010. Between 1973 and 1981 the country was ruled by the Labor Party, who carried out a series of reforms such as new school system. Farmers received increased subsidies and from 1974 women were permitted to inherit farms. Abortion on demand was legalized in 1978. Loans guaranteed in future oil income allowed Norway to avoid a recession during the mid-1970s. But by 1977 high wages had made Norwegian industry uncompetitive and a soaring forced cut-backs in public and private spending. Fish farming became a new, profitable industry along the coast.
An immigration surplus was established in the late 1960s, largely from Western Europe and the United States—from the 1970s increasingly expertise in oil. The period also saw an increased immigration of unskilled labor from developing countries, especially Pakistan, although regulations from 1975 slowed this significantly. Oslo became the center-point of immigration. The Alta controversy started in the 1970s when Statkraft planned to dam the Alta River. The case united the environmental and Sami interest groups; although Alta Power Station was built, the issue shifted the political climate and made large-scale hydroelectricity project difficult to built. The Sami Parliament was established in 1989.
The Conservative Party won the 1981 elections and carried out a large deregulation reform: taxes were cut, local private radio stations were permitted, cable television was established by private companies, regulations on borrowing money were removed and foreigners were permitted to buy securities. An economic crisis hit in 1986 when foreigners started selling Norwegian krone, which ultimately forced an increase in taxes and Prime Minister Kåre Willoch was forced to resign. The Progress Party, located to the left of the Conservatives, had its break-through in the late 1980s. The high wages in the oil industry made low-skill manufacturing industries uncompetitive and the Labor Party closed a number of public industrial companies which were receiving large subsidies. The 1980s saw a trebling of people on disability, largely amongst the oldest in the workforce. Crime rates rose.
The subsea Vardø Tunnel opened in 1982 and since the country has built subsea tunnels to connect island communities to the mainland. From the 1980s, the largest cities introduced toll rings to finance new road projects. A banking crisis hit Norway in the late 1980s, causing the largest banks, such as Den norske Bank, Christiania Bank and Fokus Bank, to be nationalized. Norsk Data, a manufacturer of minicomputers, became Norway's second largest company by 1985, just to go bankrupt by 1993. Unemployment reached record-high levels in the early 1990s.
By 1990 Norway was Europe's largest oil producer and by 1995 it was the world's second-largest oil exporter. Membership in the European Union was rejected in a 1994 referendum, with Norway instead joining the European Economic Area and later also the Schengen Area. Large public investments in the 1990s were a new National Hospital and Oslo Airport, Gardermoen—connected to the capital with Norway's first high-speed railway, the Gardermoen Line. A number of large government companies, such as Statoil, Telenor and Kongsberg were privatized. Lillehammer hosted the 1994 Winter Olympics. The end of the Cold War resulted in cooperation with Russia and reduced military activity. The Norwegian Armed Forces shifted their focus from defending an invasion to being mobile for use in NATO operations abroad and participated in the NATO bombing of Yugoslavia, the War in Afghanistan and the Libyan Civil War. The 2011 Norway attacks saw a Norwegian far-right terrorist bomb the Government Headquarters and Workers' Youth League camp at Utøya, killing 77 people.
In national elections in September 2013, voters ended eight years of Labor rule. A coalition of the Conservative Party and the populist anti-immigration Progress Party, was elected on promises of tax cuts, more spending on infrastructure and education, better services and stricter rules on immigration. The transition comes as Norway's economy is in good condition with low unemployment. Center-right leader Erna Solberg will form a new government after Labor Prime Minister Jens Stoltenberg admitted defeat. Solberg said her win was "a historic election victory for the right-wing parties".

</doc>
<doc id="56142" url="http://en.wikipedia.org/wiki?curid=56142" title="Thiruvananthapuram">
Thiruvananthapuram

Thiruvananthapuram (]), also known as Trivandrum, is the capital of the Indian state of Kerala the largest and the most populous corporation in kerala and fifth largest Urban Agglomeration in Kerala. The Etymology of the city's name can be broken down in Malayalam as, Thiru (signification of respect)- Anantha-Puram (signifying, "Place of"). It is located on the west coast of India near the extreme south of the mainland. Referred to by Mahatma Gandhi as the "evergreen city of India", the city is characterised by its undulating terrain of low coastal hills and busy commercial alleys. The city with a population of 957,730 inhabitants is the largest in Kerala. Thiruvananthapuram contributes 80% of the state's software exports and is a major IT hub.
The city is home to central and state government offices and organisations. Apart from being the political nerve centre of Kerala, it is also an academic hub and is home to several educational institutions including the University of Kerala, and to many science and technology institutions, the most prominent being the Indian Space Research Organisation (ISRO), Vikram Sarabhai Space Centre (VSSC), College of Engineering Thiruvananthapuram (CET),the Jawaharlal Nehru Tropical Botanic Garden and Research Institute (JNTBGRI), Central Tuber Crops Research Institute, Technopark, the Indian Institute of Space Science and Technology (IIST), the Indian Institute of Information Technology and Management, Kerala, Indian Institute of Science, Education and Research (IISER), the Centre for Development Studies, the Centre for Development of Imaging Technology (C-DIT), the National Institute for Interdisciplinary Science and Technology, the International Centre for Free and Open Source Software (ICFOSS), the Centre for Earth Science Studies, Rajiv Gandhi Centre for Biotechnology and the Sree Chitira Thirunal Institute for Medical Science and Technology. Considered one of the 10 greenest cities in India, Thiruvananthapuram is classified as a tier-II Indian city along with Kochi and was ranked as the best city in Kerala to live in a 2012 Times of India survey. The city is also ranked as the best city in India for Housing and Transport by a survey conducted by India Today.
Toponymy.
The city gets its name from the Malayalam word "thiru-anantha-puram" ], meaning the "City of Lord Ananta". The name derives from the deity of the Sri Padmanabhaswamy temple at the centre of the city. Anantha is the serpent Shesha on whom Padmanabha or Vishnu reclines. This temple of Vishnu reclining on Anantha remains the iconic landmark of the city. It is estimated that the value of the monumental items and assets of the temple partially revealed are close to ₹1000 billion (), making it the richest temple in the world. The city was officially referred to as "Trivandrum" until 1991, when the government decided to reinstate the city's original name Thiruvananthapuram.
History.
Thiruvananthapuram is an ancient region with trading traditions dating back to 1000 BCE. It is believed that the ships of King Solomon landed in a port called Ophir (now Poovar) in Thiruvananthapuram in 1036 BCE. The city was the trading post of spices, sandalwood and ivory. However, the ancient political and cultural history of the city was almost entirely independent from that of the rest of Kerala. The early rulers of the city were the Ays. With their fall in the 10th century, the city was taken over by the rulers of Venad.
The rise of modern Thiruvananthapuram began with accession of Marthanda Varma in 1729 as the founding ruler of the princely state of Travancore ("Thiruvithamkoor" in the local vernacular). Thiruvananthapuram was made the capital of Travancore in 1745 after shifting the capital from Padmanabhapuram in Kanyakumari district. The city developed into a major intellectual and artistic centre during this period. The golden age in the city's history was during the mid 19th century under the reign of Maharaja Swathi Thirunal and Maharaja Ayilyam Thirunal. This era saw the establishment of the first English school (1834), the Observatory (1837), the General Hospital (1839), the Oriental Research Institute & Manuscripts Library and the University College (1873). The first mental hospital in the state was started during the same period. Sanskrit College, Ayurveda College, Law College and a second grade college for women were started by Moolam Thirunal (1885–1924).
The early 20th century was an age of tremendous political and social changes in the city. The Sree Moolam Assembly, established in 1904, was the first democratically elected legislative council in any Indian state. Despite not being under direct control of the British Empire at any time, the city featured prominently in India's freedom struggle. The Indian National Congress had a very active presence in Thiruvananthapuram. A meeting of the Indian National Congress presided by Dr. Pattabhi Sitaramaiah was held here in 1938.
The Thiruvananthapuram Municipality came into existence in 1920. The municipality was converted into a Corporation on 30 October 1940, during the period of Chitra Thirunal Bala Rama Varma, who took over in 1931. The city witnessed many-sided progress during his period. The promulgation of "Temple Entry Proclamation" (1936) was an act that underlined social emancipation. This era also saw the establishment of the University of Travancore in 1937, which later became Kerala University.
With the end of the British rule in 1947, Travancore chose to join the Indian union,after toying with the idea of independence till as late as 1949. In fact, it had declared itself to be independent on 18 June 1947. An assassination attempt on the Dewan, Sir C P Ramaswamy Iyer and his exit turned the tables on the votaries of an "American Model" Travancore. The first popular ministry headed by Pattom Thanu Pillai was installed in office on 24 March 1948. In 1949, Thiruvananthapuram became the capital of Thiru-Kochi, the state formed by the integration of Travancore with its northern neighbour Kochi,which incidentally was the first princely state to accede to the Indian Union. The king of Travancore, Chitra Thirunal Bala Rama Varma, became the Rajpramukh of the Travancore-Cochin Union from 1 July 1949 until 31 October 1956. When the state of Kerala was formed on 1 November 1956, Thiruvananthapuram became its capital.
With the establishment of Thumba Equatorial Rocket Launching Station (TERLS) in 1962, Thiruvananthapuram became the cradle of India's ambitious space programme. The first Indian space rocket was developed and launched from the Vikram Sarabhai Space Centre (VSSC) in the outskirts of the city in 1963. Several establishments of the Indian Space Research Organization (ISRO) were later established in Thiruvananthapuram.
A major milestone in the city's recent history was the establishment of Technopark—India's first IT park—in 1995. Technopark has developed into the largest IT park in India in geographical area, employing around 48,000 people in 300 companies. This placed Thiruvananthapuram on the IT map of India.
Geography and climate.
Thiruvananthapuram is built on seven hills by the sea shore and is located at on the west coast, near the southern tip of mainland India. The city situated on the west coast of India, and is bounded by Laccadive Sea to its west and the Western Ghats to its east. The city spans an area of 214.86 km2 and the greater metropolitan area spans an area of 250 km2. The average elevation of the city is 16 ft above sea level. The Geological Survey of India has identified Thiruvananthapuram as a moderately earthquake-prone urban centre and categorised the city in the Seismic III Zone.
Thiruvananthapuram lies on the shores of Karamana and Killi rivers. Vellayani, Thiruvallam and Aakulam backwaters lies in the city.
Climate.
The city has a climate that borders between a tropical savanna climate and a tropical monsoon climate. As a result it does not experience distinct seasons. The mean maximum temperature 34 °C and the mean minimum temperature is 21 °C. The humidity is high and rises to about 90% during the monsoon season. Thiruvananthapuram is the first city along the path of the south-west monsoons and gets its first showers in early June. The city gets heavy rainfall of around 1700 mm per year. The city also gets rain from the receding north-east monsoons which hit the city by October. The dry season sets in by December. December, January and February are the coldest months while March, April and May are the hottest. The lowest temperature in the city core recorded during winter was 16.4 °C on, and the highest temperature recorded in summer is 38.0 °C.
Economy.
The economy of Thiruvananthapuram city was earlier based on the tertiary sector with about 60% of the workforce being employed as government servants. Large-scale industrial establishments are low compared to other south Indian state capitals like Bangalore and Chennai. Currently the economy is growing with the contributions from more professionals in the field of IT.
Thiruvananthapuram was listed as one of the top 10 cites in India on Vibrancy Index and Consumption Index by a study conducted by global financial services firm Morgan Stanley. The opening of many private television channels in the state made Thiruvananthapuram the home of several studios and related industries. India's first animation park Kinfra Film and Video Park is situated here.
The city contributes 80% of software exports from the state, and was selected as the fourth hottest IT destination in India by Rediff. Since the establishment of Technopark in 1995, Thiruvananthapuram has steadily grown into a competitive IT centre. The city was rated as the best 2nd tier metro with IT/ITES infrastructure, and second in terms of availability of human talent. Technopark is home to several companies including Oracle Corporation, Infosys, ITC Infotech, TCS, Capgemini, Visual Graphics Computing Services, Ernst & Young Global Shared Services Center, Allianz Cornhill, RR Donnelley, UST Global, Tata Elxsi, IBS Software Services, NeST Software, SunTec Business Solutions etc. The park has around 285 companies employing over 40,000 professionals. This is the first CMMI Level 4 assessed Technology Park which spreads over 330 acres, and about 4000000 sqft. of built-up space As Phase IV expansion, Technopark is developing 450 acres of land in Pallippuram, 5 km north from the main campus as Technocity.
Tourism has also contributed heavily to the economy of Thiruvananthapuram. A large number of foreign tourists visit the city every year.
There are around 20 government owned and 60 privately owned medium and large-scale industrial units in Thiruvanathapuram. The major employers are the KSIDC, Milma, Keltron, VSSC, ISRO LPSC, Travancore Titanium and Hindustan Latex, all government owned. There are also about 30,000 small scale industrial units employing around 115,000 people. Traditional industries include handloom and coir.
Commercial activity is low mainly due to the underdevelopment of ports. However, this is expected to change with the construction of the Deep Water Container Transshipment Port at Vizhinjam. Situated close to the city, Vizhinjam is very close to international shipping routes and the east-west shipping axis and hardly require maintenance dredging. Other major organisations of economic interest are the BrahMos Aerospace, Chithranjali Film Complex, Kinfra Apparel Park, Kinfra Film and Video Park, Kerala Hitech Industries (KELTECH), Kerala Automobiles Limited and the English Indian Clays Ltd.
Administration and law.
The state legislative assembly and Secretariat are located here as Thiruvananthapuram is the capital of Kerala. The city is also the headquarters of the Thiruvananthapuram district. The foreign missions in the city are the Consulate of Maldives and Honorary Consulate of Russia.
There is also a recent plea to reinstate a bench of the Kerala High Court in the city which was earlier cancelled in 1957 due to setting up of the High Court of Kerala at Ernakulam.
The city is administered by the Thiruvananthapuram Corporation which headed by the Mayor and is responsible for the overall, supervision and control of the administrative functions of the Municipal Corporation. The city council is democratically elected and comprises 100 members representing the different city wards. Several agencies work under or in partnership with the Corporation including the Thiruvananthapuram Development Authority (TRIDA) and Thiruvananthapuram Road Development Company Limited (TRDCL).
The ranked 2nd out of 21 Cities for best governance & administrative practices in India in 2014. It scored 3.9 on 10 compared to the national average of 3.3. It is the only city in India with a local ombudsman.
The city comes under the Thiruvananthapuram Lok Sabha constituency. The city corporation area contributes to four legislative assembly seats namely Kazhakuttam, Vattiyoorkavu, Thiruvananthapuram, and Nemom.
The city police is headed by a Police Commissioner, an officer of Deputy Inspector General rank in the Indian Police Service.
The city is divided into three police sub-divisions headed by Assistant Commissioners. There are also two traffic sub-divisions. A women's cell and a narcotics control cell also operate in the city. The other units of Thiruvananthapuram City Police include Crime Detachment, City Special Branch, Dog Squad, Mounted Police, District Crime Records Bureau, Foreigners Registration Office (FRO), Tourist Police and District Armed Reserve. There are two state Armed Police Battalions and a unit of the Central Reserve Police Force (CRPF) based in Thiruvananthapuram. The CRPF has a Group Headquarters (GHQ) located at Pallipuram. There is also a large army cantonment in Pangode where some regiments of the Indian Army are based.
Infrastructure.
The city is fully electrified by Kerala State Electricity Board (KSEB). The district is divided into three circles: Transmission circle, Thiruvananthapuram city and Kattakkada. Domestic consumers account for 43% of the total power consumption, or 90 million units per month. Thiruvananthapuram district has one 220 kV, nine 110 kV and six 66 kV electrical substations. A 400 kV substation has just been commissioned by the Power Grid Corporation and will ensure high-quality power supply to the city.
The water supply schemes cover 100% within the city limits. It is 84% of the urban and 69% of the rural population, when the district is considered. Peppara and Aruvikkara dams are the main sources of water for distribution in the capital city. The new project plan for improving the water supply with Japanese aid covers Thiruvananthapuram city and six suburban panchayats having urban characteristics.
Thiruvananthapuram is the only city in the state to have a scientific sewage treatment facility . The entire sewage is disposed off at the Muttathara Sewage Treatment Plant, which can handle 107 million litres a day (mld). However, only 32 mld of sewage is currently disposed off at the plant. Sewage Plant at Muttathara is also India’s largest and Kerala’s first modern sewage treatment plant.
The sewerage system in the city was implemented at the time of the Travancore Kingdom, and modernised in 1938. This scheme for the disposal of sullage and sewage is an underground system. The whole system is controlled by Kerala Water Authority now. The city area is divided into seven blocks for the execution of the sewerage system, two commissioned in the 1990s and two after 2000. The sewerage used to be pumped to a stilling chamber at the Sewerage Treatment Plant (STP) located at Valiyathura, and was disposed through sewage farming. The Dairy Development Department maintains this sewage farm, and fodder cultivation is done here. There is no revenue generation from this scheme, and the sewerage system in the city is a service provided to the residents. However, now the sewage is treated at the Muttathara STP.
Tourism.
Thiruvananthapuram is also a tourist destination for both domestic and international tourists. There are many tourist destinations in or near the city including Kovalam beach, Sanghumukham Beach, Napier museum and Zoo (Yann Martel wrote his book Life of PI after studying a disabled lion, Simba for months together), Agasthyarkoodam peak, Neyyar Wildlife Sanctuary and Neyyar Dam, Kuthira Malika palace, Sree Padmanabha Swamy temple, Ponmudi, Poovar, Varkala Cliffs and beaches and many others.
Kanyakumari, Thiruvattar, Padmanabhapuram Palace and Tirpparappu waterfalls, are also near the city, in the adjoining Kanyakumari District (Nagercoil), in the state of Tamil Nadu.
The eponymous Sree Padmanabhaswamy Temple circled by the East Fort is at the center of a busy shopping hub of the city. The temple attracts millions of visitors every year. Visitors are required to adhere to special dress code before entering the temple. Recent court battle challenges the custodianship of the Royal family over the temple. The controversy centres on the estimated properties of over $20 billion housed in the vaults of the temple.
Transport.
Road.
The NH-66, which runs from Panvel to Kanyakumari connects the city to Kochi, Kozhikode and Mangalore. The Main Central Road (MC Road) which is an arterial State Highway in Kerala and designated as SH 1 starts from Kesavadasapuram in the city.
The Thiruvananthpuram Road Development Company Limited is an SPV to develop the road network in Thiruvananthapuram city. It is the first intra-city project in the country.
The intra-city public transport in the city is dominated by the state-owned KSRTC, though there are significant numbers of private buses plying within the city limits.
Within the city, city buses, taxis and autorickshaws provide transportation. Scooters, motorcycles and cars are the favoured means of personal transportation. The intra-city public transport is dominated by the state-owned KSRTC (Kerala State Road Transport Corporation).
There are bus services operated by private operators and provides access within city limits and beyond. The city services of KSRTC operate from six depots namely, the City depot, Vikas Bhavan, Peroorkada, Pappanamcode, Kaniyapuram and Vellanad. These services were revamped in 2005 with the introduction of modern buses and electronic ticketing mechanisms. The Central bus station is in Thampanoor, opposite Thiruvananthapuram Central Station. It connects Thiruvananthapuram with other parts of Kerala as well as other states. The central city bus terminal is 1 km away at East Fort (Kizhakke kotta), near the Padmanabha Swamy temple.
Interstate buses : Tamil Nadu State Transport Corporation of Tirunelveli Division ply buses between Nagercoil and Thiruvananthpuram and many other parts of Kanyakumari district. It has a depot of SETC which operates long distance services towards Chennai and Bangalore via Nagercoil, Madurai. Also, KSRTC, private bus operators and the Karnataka SRTC ply services to destinations in Tamil Nadu, Karnataka and Hyderabad.
Rail.
Thiruvananthapuram comes under the Southern Railway zone of the Indian Railways. There are five railway stations within the city limits including the Thiruvananthapuram central station. Thiruvananthapuram Pettah, Kochuveli and Veli stations are located towards north direction and Thiruvananthapuram Nemom is located in south direction from the central station. The Central railway station is located at Thampanoor in the heart of the city, and is about 5 km from the new international air terminal and nearly 8 km from the domestic air terminal. It is the largest and busiest railway station in the state. Kochuveli railway station is developed to ease congestion on central station and it act as satellite station to Thiruvananthapuram Central. Some of the long distance trains from the city operates from this station. The Thiruvananthapuram Rajdhani Express connects the city to New Delhi, the capital of India. The city is well connected by rail to almost all major cities in India such as New Delhi, Mumbai, Chennai, Kolkata, Bangalore, Tirunelveli and Hyderabad. Thiruvananthapuram is also the first major South Indian city on the longest train route in India, Kanyakumari to Dibrugarh.
Light Metro Rail.
The Government of Kerala is considering a proposal to construct two metro systems (light category) in the city of Thiruvananthapuram and Kozhikode. Earlier, the proposal was to construct monorail systems. The monorail proposed was to start from Pallippuram and terminate at Neyyattinkara covering a distance of 41.8 km. Thirty-five stops were proposed with multi-storeyed parking lots at the stations. However, the monorail projects have now been dropped and the government is instead considering the possibility of light metros in these corporation areas with construction starting by the end of 2015.
Sub Urban Rail.
A new suburban corridor proposed by Railways in Thiruvananthapuram – Kollam – Haripad/Chengannur routes for which MRVC is tasked to conduct study and submit report. Ten trains, each with 7 bogies will transport passengers back and forth between Thiruvananthapuram-Kollam-Chengannur-Harippad section. Suburban Corridor is modelled on the lines of the Mumbai Suburban Rail where around 3,000 suburban trains ply every day
Air.
Thiruvananthapuram is served by the Thiruvananthapuram International Airport (IATA: TRV, ICAO: VOTV), which is the first international airport in India outside the four metropolitan cities then. It has direct connectivity to the Middle East, Singapore, Maldives and Sri Lanka and is one of the gateways to the tourism-rich state of Kerala. The airport is qualified for all-weather and night operations. One of the major advantage of the airport is the prevailing weather at the location that does not go to extremes, allowing flight operations without disruption year around. The International terminal of the airport is approximately 3.7 km due west and the domestic terminal is approximately 8.0 km from the central business district. The importance of the airport is also due to the fact that it is the southernmost airport in India and also the closest option for neighbouring countries like Maldives and Sri Lanka, and the only option to Maldives from India. Also, apart from the regular scheduled flights, charter flights, primarily carrying tourists, also serve the airport.
Sea.
The work on infrastructure development for the Deep Water Container Trans-shipment Port at Vizhinjam is expected to begin in 2015. It is to be built in three phases, and expected to be a key competitor in the ports business (especially for container transshipment), with the international shipping lanes between Europe and the Far East lying very close to the port, and also with major ports like Colombo, Kochi and Tuticorin in close proximity.
The exponential growth of the services and IT-based sectors coupled with its prominence as the state capital and tourist center has caused considerable strain on the transport infrastructure of the city. To ease the strain, projects such as TCRIP are underway(First phase is completed) including the construction of flyovers and under passes. In the first phase, 42 km of six-lane and four-lane dual carriage ways are being built.
Demographics.
The city has a population of 2,752,490 according to the 2011 census, and 2,687,406 in the Urban Agglomeration. Within the city, the density of population is about 5,284 people per square kilometre. There are more women in Thiruvananthapuram than men; the sex ratio is 2,064 females to every 2,000 males.
In October 2010, the area of the city was increased from 86 wards to 100 wards by adding Sreekaryam, Vattiyoorkavu, Kudappanakunnu, Vizhinjam and Kazhakuttam panchayats into the corporation. The city has now an area of 214.86 km² and a population of 2,957,730 inhabitants with 867,739 males and 889,991 females.
Hindus comprise 65% of the population, Christians are about 18% of the population, and Muslims are about 15% ,Sikhism The remains 8%,Others remaining 2% practise other religions. The major language spoken is Malayalam. English, Tamil, and Hindi are widely understood. There is a prominent minority of Tamil speakers and a few Tulu , Konkani and Urdu speakers.
Unemployment is a serious issue in Thiruvananthapuram. The increase in the unemployment rate was from 8.8% in 1998 to 34.3% in 2003, thus registering a 25.5% absolute and a 289.7% relative increase in five years,but currently unemployment is decreased in at a high rate of 2.3% because of increased amount of IT companies in city. Thiruvananthapuram taluk ranks third in Kerala with 36.3% of its population unemployed. The in-migration of the unemployed from other districts boosts this high unemployment rate. Thiruvananthapuram has a high suicide rate, which went up from 17.2 per lakh in 1995 to 38.5 per lakh in 2002. In 2004, the rate came down slightly to 36.6 per lakh. As per 2001 census, the populace below the poverty line in the city was 11,667. A BPL survey indicated the urban poor population as 120,367. Majority of these populace lives in slums and coastal fishing areas.
Culture.
The cultural background of Thiruvananthapuram originates from the efforts of the rulers of erstwhile Travancore, who took an active interest in the development of arts and culture. Thiruvananthapuram has produced several great artists, the most famous ones being Maharaja Swathi Thirunal, Irayimman Thampi and Raja Ravi Varma.
Maharaja Swathi Thirunal was a great composer and played a vital role in the development of Carnatic music. There is a music college in his name in the city – "Swathi Thirunal College of Music". Raja Ravi Varma was a famous painter of international renown. His contributions to Indian art are substantial. Most of his famous paintings are preserved at the Sree Chithra Art Gallery in the city. The Padmanabha Swamy Temple and the fort surrounding it, the Napier Museum and Zoo, the VJT hall are among the prominent heritage buildings in the city. The Veli lake and Shankumugham beach are home to various sculptures of the noted sculptor Kanayi Kunhiraman. Many people, including Mahatma Gandhi have admired the city's greenery.
Thiruvananthapuram appears as a laid back and quiet city to a casual observer. However there are considerable cultural activities in the city. The cultural activities are more during the festival season of Onam in August/September, and during the tourist season later in the year. The state government organises the tourism week celebrations every year during the Onam with cultural events conducted at various centres in the city. The other major events include the annual flower show, the Attukal "Pongala", the"Aaraat" of Padmanabha Swamy Temple, Urs at Beemapally,etc. The CVN Kalari at East Fort is a well-known centre for training in Kerala's indigenous martial art—the Kalaripayattu. The Margi centre offers training in many of Kerala's traditional arts including Kathakali.
The general cuisine of the people is Keralite cuisine, which is characterised by an abundance of coconut and spices. This includes predominantly vegetarian Naadan ( country ) and non vegetarian Malabar and Kuttanad recipes. Other South Indian cuisines, as well as Chinese and North Indian cuisines are popular. Arabian, Thai and branded fast food joints are also patronised.
Thiruvananthapuram has numerous libraries, the prominent ones being the State Central Library (Thiruvananthapuram Public library, Est. 1829), the University Library, Thiruvananthapuram Children's Library, Manuscripts Library and the Centre for Development Studies Library. The British Library (Est. 1964) was located very near to the Government Secretariat adjacent to the YMCA Hostel.The British Council closed it down, citing financial constraints.
A shopping mall, Mall of Travancore (MOT), with an area of 600,000 plus sq.feet is under construction on the Chaakka Bypass. Developed by the Malabar Group, it will be the second largest mall in Kerala on completion.
Education.
Thiruvananthapuram is an academic hub. The University of Kerala is located here. The regional headquarters of Indira Gandhi National Open University (IGNOU) is also situated in Thiruvananthapuram. There are many professional education colleges including fifteen engineering colleges, three medical colleges, three Ayurveda colleges, two Homeopathy colleges, six other medical related colleges, and two law colleges in the city and its suburbs. The College of Engineering, Thiruvananthapuram, Government Engineering College, Barton Hill, and Sree Chitra Thirunal College of Engineering are the main engineering colleges in the city. The Asian School of Business and IIITM-K are two of the management study institutions in the city, both situated inside Technopark. The Indian Institute of Space Science and Technology is situated in the city. Centre for Development Studies and Centre for Development of Imaging Technology (C-DIT) are located within city limits.
The schools in the city are classified as Aided, Unaided and Government schools. The government schools are run directly by the state government and follow the syllabus prescribed by the state government. The aided schools also follow the state syllabus. In addition to this, there are five Kendriya Vidyalayas run directly by the Central government, which follow the CBSE syllabus, and private schools run by educational trusts or boards which follow CBSE and/or ICSE syllabus and/or NIOS syllabus and/or state syllabus. In 1961, the first ISC school, Loyola School, was started in the city.The school is located at Srikaryam and is affiliated to the CISCE, CBSE and SCERT and was the first in the city to introduce the ISC course with its Board in Delhi and affiliation to Cambridge University. The first International school in Kerala, The Trivandrum International School, was started in the outskirts of the city in August 2003.
The literacy rate in Thiruvananthapuram, according to the 2001 census, is 89.36 percent; 92.68 percent among males and 86.26 percent among females.
Science and technology.
Thiruvananthapuram is a Research and Development hub in the fields of space science, information technology, bio-technology, and medicine. It is home to the Indian Institute of Science Education and Research, Vikram Sarabhai Space Centre (VSSC), Liquid Propulsion Systems Centre (LPSC), Thumba Equatorial Rocket Launching Station (TERLS), Indian Institute of Space Science and Technology (IIST), Rajiv Gandhi Centre for Biotechnology (RGCB), Jawaharlal Nehru Tropical Botanical Garden and Research Institute (JNTBGRI), ER&DC – CDAC, CSIR – National Institute of Interdisciplinary Science and Technology, Free Software Foundation of India (FSFI), Regional Cancer Centre (RCC), Sree Chitra Thirunal Institute of Medical Sciences and Technology (SCTIMST), Centre for Earth Science Studies (CESS), Central Tuber Crops Research Institute (CTCRI), Kerala Science and Technology Museum, Priyadarsini Planetarium, The Oriental Research Institute & Manuscripts Library, Kerala Highway Research Institute and Kerala Fisheries Research Institute. A scientific institution named National centre for molecular materials, for the research and development of biomedical devices and space electronics is to be established in Thiruvananthapuram. College of Architecture Thiruvananthapuram(CAT), which specialises only on the architecture course, is another institution proposed to set up in the suburbs of the city.
Media.
Daily newspapers are available in English, Malayalam and Tamil. The English newspapers with editions from Thiruvananthapuram are "The New Indian Express", "The Hindu", "The Deccan Chronicle" and "The Times of India". The major Malayalam newspapers are "Mathrubhumi", "Malayala Manorama", "Kerala Kaumudi", "Deshabhimani", Madhyamam, "Janmabhumi", " Chandrika", "Thejas" , "Siraj", Kerala Kaumudi Flash, Deepika and Rashtra Deepika.
Most of the Malayalam TV channels are based in Thiruvananthapuram. The government owned Doordarshan began broadcasting from here in 1981. Asianet, the first private Malayalam channel, began its telecasts in 1991. The other channels now based in Thiruvananthapuram are Amrita TV, Kairali TV, Kairali We (Youth channel of Kairali),[mathrubhoomi news] ,[kaumudi TV], JaiHind TV,Asianet news,Asianet movies, Asianet Plus (Youth channel of Asianet) and People (News and current affairs channel of Kairali TV). The local cable services are provided by Asianet Satellite Communications Limited, Connecttel Communications Pvt Ltd, Trivandrum Cable Network Pvt Ltd and Siti Cable and they provide a bouquet of local channels in addition to all the Indian channels. DTH services are available through Doordarshan Direct Plus, Tata Sky, SUN Direct, Big TV, Airtel digital TV, Videocon d2h and Dish TV.
All India Radio has an AM (1161 MHz) and an FM (Ananthapuri FM; 101.9 MHz) station for the city. Also, it has a Short Wave ( SW ) transmitter relaying the AM programming over various frequencies, intended for listeners in far flung areas of Kerala and beyond. FM radio channels broadcast from Thiruvananthapuram are Ananthapuri FM (AIR) 101.9 MHz, Gyanvani from IGNOU 105.6 MHz, Big FM 92.7 MHz, Club FM 94.3 MHz, Radio Mirchi 98.3 MHz, Red FM 93.5 MHz and Radio DC(Low power CRS) 90.4 MHz.
Thiruvananthapuram city contains the largest number of theatres in Kerala. There are over 18 cinema halls which screen films in Malayalam, Tamil, English and Hindi. There are also two film studios in the city—Chithranjali and Merryland. The Kinfra Film and Video Park, located near the Technopark, is one of the most advanced film and animation production centres in India. Leading firms like Prasad Labs have set up their facilities here. The International Film Festival of Kerala (IFFK) is held in November/December every year and is acknowledged as one of the leading events of its kind in India.
The wireline telephone services are provided by BSNL, Reliance, AirTel and Tata Indicom. The main GSM networks operating in the city are BSNL CellOne, Airtel, Tata Docomo, Idea Cellular, Vodafone, Reliance and Virgin Mobile. The main CDMA providers are Reliance, MTS and Tata Indicom. The number of mobile phone connections has increased exponentially since the late 1990s. Major broadband internet services are provided by BSNL Broadband, Asianet Dataline and Siti Cable. Private providers like Reliance, Tata Communications (VSNL), Airtel and Satyam also have their presence in the city. The major dial-up internet providers are BSNL NetOne, Kerala Online and KelNet among others. Thiruvananthapuram also holds the distinction of having been the first 100% Digital SSA (Secondary Switching Area) in India.
Sports.
The most popular games are Football and Cricket. Basketball, Badminton and Volleyball are also popular, mostly in schools. The Kerala Cricket Association (KCA) is headquartered in Thiruvananthapuram. The HQ complex of KCA, has advance facilities including two practice turfs with nets, bowling machines, gymnasium with multi-gym and equipment for aerobic training, lecture hall and library, an astro-turf indoor coaching facility, fully furnished accommodation for coaches and players, a physiotherapy clinic, functional office facilities and guest rooms.
Trivandrum is the Sports Capital of Kerala.Trivandrum has Asia's best 2015 Multi-functional Cricket Cum Football International Stadium at Kazhakkoottam and India's best Shooting range at Vattiyoorkkavu.
The Trivandrum International Stadium, is a world class International Multipurpose (cricket/football) stadium in Thiruvananthapuram. It is the first stadium in the country coming up on DBOT (Design-Build- Operate and Transfer) basis. It is also the first stadium in the country to be developed on annuity mode. It is the proposed venue for the opening/closing ceremonies of the 35th National Games to be held in Kerala. The playing arena in the stadium is constructed in line with FIFA regulations and ICC norms. It will have facilities for indoor sports like table tennis, basketball, badminton etc., a gymnasium and spa, a club house with five star facilities, tennis court, Olympic size swimming pool, open convention cum trade cum exhibition centre, retail outlets, food courts, club facilities, car parking, etc.
The Chandrasekharan Nair Stadium, in the heart of the city, is a prominent football stadium and has hosted both national and international level matches. The University Stadium has hosted two international cricket matches. This stadium is under the University of Kerala and is equipped with synthetic tracks for athletics games. The Central Stadium, which has facilities for athletics, football, basketball, volleyball and cricket practice nets, is situated on the eastern side of the Government Secretariat. The Jimmy George Indoor Stadium, the GV Raja Sports School and Lakshmi Bhai National College for Physical Education (LNCPE) are the other major sports establishments in the city.
The city has a golf course known as Thiruvananthapuram Golf Club. It is one of the oldest golf course in India, more than 150 years old. The city also has a Tennis Club (Thiruvananthapuram Tennis Club/TTC) both located at Kowdiar. The city fields two football clubs--SBT-Thiruvananthapuram and Titanium—in the second division of the National Football League. The city also has a fully equipped modern swimming pool located near the Jimmy George Sports Complex at Vellayambalam. Many state level and national level swimming competitions are held in this complex. It also holds coaching camps for those who are interested in learning swimming.
Strategic importance.
Thiruvananthapuram is a strategically important city in Southern India. Being the largest city in India's deep south, it is important for both military logistics and civil aviation in the southern part of the country. It is the headquarters of the "Southern Air Command (SAC)" of the Indian Air Force. 
Due to the strategic importance of the city, the Indian Air Force authorities have planned to establish an aerospace command in SAC.
The plan for setting up a new "Tri-Service Command", which will integrate all the three forces under a single command, is also in the pipeline.
Being the Indian city with the closest air link to the small island nation of Maldives and also Sri Lanka, the city's medical and health infrastructure caters to the needs of the patients from both countries, especially Maldives. Thiruvananthapuram also provides a key link in the movement of goods and passengers to and from southern parts of Tamil Nadu into Kerala, the state border being just 30 km from the city centre.
Notable people.
See List of people from Thiruvananthapuram
References.
</dl>

</doc>
<doc id="56143" url="http://en.wikipedia.org/wiki?curid=56143" title="Kochi (disambiguation)">
Kochi (disambiguation)

Kochi or Kōchi may refer to:

</doc>
<doc id="56144" url="http://en.wikipedia.org/wiki?curid=56144" title="Kottayam district">
Kottayam district

Kottayam is one of the 14 districts in the state of Kerala, India. The district has its headquarters at Kottayam town, located at 9.36° N and 76.17° E. According to the 1991 census, it is the first district to achieve 100% literacy rate in the whole of India. On 27 September 2008, Kottayam district also became the first tobacco free districts in India.
Bordered by the Western Ghats on the east and the Vembanad Lake and paddy fields of Kuttanad on the west, Kottayam has many unique characteristics. Panoramic backwater stretches, lush paddy fields, highlands, hills and hillocks, rubber plantations and places associated with many legends given Kottayam District the enviable title: The land of letters, legends, latex and lakes. The district is 15.35% urbanised.
The Headquarters of the Malankara Orthodox Syrian Church is the Catholicate Palace located at Devalokam, Kottayam, in Kerala state of India. It is the official headquarters of the Catholicos Of The East who reigns on the Supreme Throne of St.Thomas the Apostle.
History.
Kottayam literally means the interior of a fort - "Kotta + Akam". Rulers of Munjanad and Thekkumkur had their headquarters at Thazhathangadi near Kottayam town. Marthanda Varma of Travancore attacked Thekkumkur and destroyed the palace and the Thaliyil Fort. The remnants of the palaces and forts are still seen here.
Kottayam has played its role in all the political agitations of modern times. The 'Malayali Memorial' agitation may be said to have had its origin in Kottayam. The Malayali Memorial sought to secure better representation for educated Travancoreans in the Travancore civil service against persons from outside. The Memorial, which was presented to the Maharaja Sri Moolam Thirunal (1891) was drafted at a public meeting held in the Kottayam Public Library. The event marked the beginning of the modern political movement in the state.
It was in Kottayam that the famous Vaikom Satyagraha (1924–25), an epic struggle for eradication of untouchability, took place. Scheduled castes and other backward classes in Travancore were denied not only entry into temples, but also access to temple roads. Vaikom, the seat of a celebrated Siva Temple, was the venue of the symbolic satyagraha. It is of immense historic significance that national leaders like Mahatma Gandhi, C. Rajagopalachari, Acharya Vinoba Bhave and E.V. Ramswami Naykar, associated with this struggle. The ' Nivarthana ' agitation of the early thirties, to secure adequate representation for the non-caste Hindus, Christians and Muslims in the state Legislature, enjoyed considerable support from this district. The district was also a centre of the agitation led by the state Congress for responsible Government in Travancore. The agitation had a triumphant end, with the overthrow of Sir C. P. Ramaswami Iyer, the then Dewan of Travancore.
The present Kottayam district was previously a part of the erstwhile princely state of Travancore. Earlier, the Travancore state consisted of two revenue divisions viz. the southern and northern divisions, under the administrative control of a 'Diwan Peshkar' for each. Later in 1868 two more divisions Quilon (Kollam) and Kottayam were constituted. The fifth division Devikulam came next but only for a short period, which in course of time, was added to Kottayam. At the time of the integration of the state of Travancore and Cochin (Kochi) in 1949, these revenue divisions were renamed as districts and the Diwan Peshkars gave way to District Collectors, paving the way for the birth of the Kottayam District in July 1949 which included Kottayam, Muvattupuzha (including present day Kothamangalam), Thodupuzha, Changanasserry, Vaikkom, Meenachil, Devikulam and Peermade taluks.
Kottayam is also known as the language-capital of Kerala .
Weather.
Kottayam has a tropical climate like that of the rest of Kerala, hence there are no distinct seasons in the area. Humidity is high and rises to about 90% during the rainy season. Kottayam gets rain from two monsoon seasons, the south-west monsoon and the north-east monsoon. The average rainfall is around 3600 mm per year. The south-west monsoon starts in June and ends in September. The north-east monsoon season is from October to November. Pre-monsoon rains during March to May is accompanied by thunder and lightning ; the highest rainfall during this period in Kerala is received in Kottayam. December, January and February are cooler, while March, April and May are warmer. The highest temperature recorded here was 38.5 °C (6 April 1998) and the lowest was 15 °C (13 December 2000). Kottayam district experienced the most intense red rainfall, heavy downpours occurred in 2001 during which the rain was colored red, Yellow, green, and black.
Tourism.
Kottayam has a vast network of rivers, backwaters, ancient religious places, and hillstations. Some of the noted tourist places here are:
Vembanad Lake has a great expanse of water which is a part of the interconnected Kerala Backwaters that run virtually the length one third of the state. Vembanad Lake is 52 miles (84 km) in length and 9 miles (14 km) in width. Traditional cargo boats called Kettuvallams are modified into luxurious cruise boats and house boats. These boats gracefully move around the back waters, providing facilities to tourist to enjoy the beauty of the Vembanad Lake in a relaxed pace.
Pathiramanal (the midnight sands) is located in the Vembanad Lake is a small beautiful island. This island is accessible only by boat.
Kumarakom, located on the coast of Vembanad Lake, is a beautiful village stocked with divine mangroves and coconut groves, lush green paddy fields, gushing waters snaking through the dense forests. Kumarakom bird sanctuary, is home to migratory birds like the Siberian stork, egret, darter, heron and teal. Local birds like the water fowl, cuckoo, owl and water hen and other common varieties like the woodpecker, sky lark, crane and parrot can also be spotted here. Ninety-one species of local and 50 species of migratory birds are found here. The best time to watch local birds is June–August and the best time for migratory birds is November–February. House Boats and motorboats are available on hire for bird watching cruises in the Lake. Vagamon is a hill station in the Kottayam-Idukki district.
During the months of August and September, the rivers in and near Kottayam are turned into festival centres. The serene backwaters come alive during Onam with a spectacular water regatta -the snake boat races. Oarsmen, at least a hundred in each boat, slice their way through the waters to the fast rhythm of their own full-throated singing. Thazhathangadi boat race in Kummanam is over a century old. Boat races are conducted at Kavanar and Kottathodu rivers in Kumarakom. These vallam kali have about 50 boats participating, including Chundan, Churulan, Iruttukuthi(ody) Veppu, and canoes.
Other nearby tourist destinations:
Agriculture.
Kottayam has a mountainous terrain as well as low lying areas very close to sea level. Depending on the location different varieties of food crops as well as cash crops are cultivated. Rice is the principal crop extensively cultivated in low lying regions like Vaikom and Upper Kuttanad. The district occupies the third position in the production of rice behind Palakkad and Alappuzha. Though it is the staple food of the people, unfortunately the area under cultivation is dwindling due to more lucrative cash crops like rubber plantations for which Kottayam significantly contributes to the overall rubber production in India. Kottayam occupies the first position in the production of rubber in India. Rubber trees provide a stable income for the farmers and climate is ideal for rubber plantations. Though highlands are more suitable, the cultivation has spread to almost every where. Apart from these, other crops cultivated are tapioca, coconut, pepper, vegetables etc. In order to enhance the rubber productivity, government of India has set up Rubber board and Rubber research institute in Kottayam.
Industry.
Aside from two public sector companies, Hindustan Newsprint at Velloor and Travancore Cements at Nattakom, industries in the district consist mostly of small and medium scale units. The main activity are in publishing (newspapers and books) and processing of rubber (latex) and manufacturing of rubber based products. Rubber based industries in the district include a unit of MRF Ltd. (Madras Rubber Factory) in Vadavathoor,St.Mary's Rubbers (P)Ltd Koovapplly, Kanjirappally the No.1 centrifuged Latex, Skim rubber Block and Skim crepe rubber exporter in India,(www.stmarysrubbers.com).St.Mary's Rubbers (P) Ltd newly launched their new product gloves under the brand name of "Medismart" on 30 June 2013.(www.medismartglove.com/) . Midas Rubber Ltd. at Ettumanoor, Intermix factory (Neezhoor) and Rubco at Pampady.
Confined more or less to the Vaikom area of the district, is a thriving coir processing industry, processing coir and making coir products. Consisting of more than twenty co-operatives, it employs around 20,000 people. In the hand loom sector, eight co-operative societies provide employment to 2100 persons. The district has a rich forest wealth with good availability of softwood and other varieties of timber providing raw material for a number of small enterprises in the production of plywood, packing cases, splints, veneers and furniture.
Publishing.
The first printing press in Kerala (C.M.S Press) was established here in 1821 by Rev. Benjamin Bailey, a British missionary. Maiden printed Malayalam-English and English-Malayalam dictionaries were published from Kottayam in 1846 and 1847 respectively. The only cooperative society of writers, authors and publishers (SPCS), for publishing books and periodicals was set up here in 1945. Kottayam is the hometown of a vast number of books and periodicals and is the centre of publishing business in the state. Popular publishing houses like Malayala Manorama, Mathrubhumi publications, Labour India Publications Ltd, Mangalam Publications, Deepika, D. C. Books, V Publishers, Vidhyamitram, Kerala Kaumudi daily and kerala kaumudi flash are also publishes from here. Kottayam city hosts a number of book exhibitions every year.
Demographics.
According to the 2011 census Kottayam district has a population of 1,979,384, roughly equal to the nation of Slovenia or the US state of New Mexico. This gives it a ranking of 234th in India (out of a total of 640). The district has a population density of 896 PD/sqkm. Its population growth rate over the decade 2001-2011 was 1.32%. Kottayam has a sex ratio of 1040 females for every 1000 males, and a literacy rate of 96.4%.
As per 2001 Indian Census, population in Kottayam includes Hindus (49.32%), Christians (44.60%) and Muslims (5.97%).
Religion.
Like the rest of Kerala, Hindus, Christians and Muslims form a significant part of the population.In 2001 Indian Census Muslim Population is 5.97%, Hindu 49.32%, Christian 44.60%.
Kottayam, Thiruvalla and Chengannur are the railway stations for pilgrims heading to the Hindu holy site of Sabarimala. During December and January of each year pilgrims from all over India head to Kottayam and its vicinities to start their final journey to the Sabarimala Temple. The temple is located around 110 km from Kottayam, in the district of Pathanamthitta. Panachikad Temple, one of the famous Saraswathy temples in Kerala is located nearly 12 km away from Kottayam. The Siva temple at Thirunakkara is in the heart of Kottayam town. Three festivals are celebrated here in Thulam (October–November), Mithunam (June–July) and Meenam (March–April) of which the last one is the most important. The Aaraattu, conducted on the last day of the festival, is the festival of Kottayam, irrespective of caste and creeds. Ettumanoor Mahadeva Temple is another important temple located near Kottayam town. Thirunakkara Srikrishna temple, and Pallipurathukavu Devi Temple are other important temples located in the town. Kodungoor Devi temple, Kavinpuram Devi Temple (Ezhacherry), Chirakadavu Mahadevar temple, Ponkunnam Devi temple, Cherubally Devi temple, Kidangoor Subramanyaswamy Temple and Vaikom Mahadevar temple are other famous Hindu temples near Kottayam.
Thekkumthala Devi Temple is in another famous temple. It is located in Pallikkathode in Kottayam.
Kottayam is a major center for Syrian Christians in Kerala. Syrian Christians include Malankara Orthodox, Jacobite Syrian Christian Church (Jacobite), Knanaya, Syro-Malankara Catholic, Marthoma, and other members of Syro-Malabar Catholic and Madhya Kerala Diocese of the Church of South India.
About 40% of Kottayam Christians are Orthodox Christians[ Malankara Orthodox, Jacobite Syrian Christian Church (Jacobite), Knanaya ] , 30% are Catholic Christians [ Syro-Malankara Catholic, Knanaya ] and remaining 30% are Protestants. Catholic Church had the highest population among different Christian Denominations with over 500,000 population including Roman Catholic, Knanaya Catholic and Malankara Catholic. Malankara Orthodox Church with over 350,000 and Jacobite Church with its nearly 200,000 population including Jacobite Knanaya population. A number of old and sacred Christian churches are also located in Kottayam.
The most famous Christian church in the district of Kottayam is St Mary's Jacobite Syrian Church in Manarcad. It is known for its celebration of the ancient practice of the 8 Day Lent and the Feast of Virgin Mary's Birth between September 1 and 8 every year. During the days of the lent, millions of people from the far corners of the world reach Manarcad to seek the blessings of Virgin Mary.
St. George Orthodox Church Puthuppally Pally, is a prominent church that belongs to the Malankara Orthodox Syrian Church. This church is situated beside the Puthuppally-Changancherry road about a kilometre from the Puthuppally junction, on the eastern bank of the Kodoorar rivulet.Puthuppally Pally is one of the most important churches among the ancient churches of Malankara.
St. Alphonsa, the first woman saint of India was born on 19 August 1910 in Kudamaloor and died on 28 July 1946 in Bharananganam. St. Mary's Syro-Malabar Catholic Church, Kudamaloor where she was baptized and the St. Mary's Syro-Malabar Catholic Church, Bharananganam where she is buried are pilgrim centers of large interest among Christians.
The Valia Pally, the oldest church in Kottayam,has Persian inscriptions and a stone cross and belongs to the Knanaya Jacobite Church. Old Seminary Orthodox Pazhaya Seminary, a prominent religious institution belonging to the Malankara Syrian Orthodox Church and a seminary for aspiring priests for Syrian Christians in Kerala, is also located in Chungam, Kottayam. The Cheria Pally, an ancient Church belonging to the Malankara Orthodox Church is a well preserved church with wall murals dating back to ancient times. These unique wall murals have been painted using vegetable dyes.
Some of the important Syrian Catholic churches in Kottayam include Christ the King Syro-Malabar Catholic Cathedral of Kottayam Knanaya Archdiocese and Lourde's Syro-Malabar Catholic Forane Church. Important Latin Catholic churches are Good Shepherd Church and Vimalagiri Cathedral. Pope John Paul II visited Kottayam during his visit to India in 1986. In Kottayam he announced the beatification of Kuriakose Elias Chavara and Sister Alphonsa, who hail from Kottayam district.
Cherpunkal Church is a famous Church in the name of Infant Jesus. Lots of people visit the church every First Friday. The Syrian Christians of Kottayam are popularly known as Achayans, who are wealthy and forward thinking.
One of the oldest mosques in Kerala, Thazhathangady Mosque is situated in Thazhathangady at 3 kilometers from Kottayam town. This mosque is considered to be around 1500 years old and was built by early Arab travelers who landed in Kerala during the time of the Cheraman empire.
Education.
The Old Seminary (Orthodox Pazhaya Seminary of the Malankara Orthodox Church at Chungam, is the first institution to start English education in South India.It was founded in 1815 by Colonel John Monroe. 
 C.M.S High School (which later became Church Missionary Society College High School) was founded by the British missionary Rev. Benjamin Bailey. The first college in the state (C.M.S College) was started at Kottayam in 1840. It is also the second college in India established by the British empire. CMS college was previously known as "grammar school". 
Kottayam is a major centre of education. Mahatma Gandhi University, one of the six universities in Kerala is located here. Other prominent educational institutions located in Kottayam include C.M.S College, Baselius College, KG College Pampady, B.C.M College, Government College Nattakom and K.E College(Kuriakose Elias College, Mannanam ). Medical College, Kottayam one of the government medical colleges, is located at Gandhinagar close to Kottayam. Government Dental College, Kottayam, the third and the latest Dental College is also located in Gandhinagar. Rajiv Gandhi Institute of Technology, the government engineering college named after former prime minister of India Mr. Rajiv Gandhi is situated in Pampady. There are also a number of other engineering colleges situated in the district. Theophilus college of Nursing, Kangazha is the first Self-financing Nursing College to be started in the private sector. The Mar Thoma Seminary Higher Secondary School situated on Zion Hill of Kottayam. This school was awarded the best school of the Kerala state in 1976. Kottayam has a Technical Higher Secondary School and College of Applied Science managed by IHRD located at Puthuppally.
Politics.
K R Narayanan, the former President of India hails from Kottayam district. Currently Kottayam is represented in the Lok Sabha by Jose K Mani of Kerala Congress (Mani).
Members representing constituencies in Kottayam in the Kerala state Legislative Assembly
Towns & Villages in Kottayam District.
Ambikamarket, Amayannoor, Amparanirappel, Anchery, Arpookara, Athirampuzha, Ayamkudy, Aymanam, Ayarkunnam, Chemmalamattam, Chengalam, Cheruvally, Chungam, Changanassery, Cherpunkal, Chingavanam, Ettumanoor, Edakkunnam, Elikkulam, Erumely, Erattupetta, Kadaplamattom, Kadappoor, Kaduthuruthy, Kaipuzha, Kalikave, Kallara, Kanakkary, Kanjiramattom, Kanjirappally, Kangazha, Karikkattoor, Karukachal, Kidangoor, Kodungoor, Koottickal, Koodalloor, Kothala, Kothanalloor, Kudamaloor, Kumarakom, Kumaranalloor, Kummannoor, Kuravilangad, Kurichithanam, Kurupanthara, Malloossery, Manarkadu, Manimala, Marangattupilly, Mundakkayam, Mutholy, Muttuchira, Neendoor, Neericadu, Padinjattinkara, Palackattumala, Palai, Pallickathode, Palayam, Pampady, Panachikkad, Panamattom, Parampuzha, Parathodu, Peroor, Ponkunnam, Poonjar, Poovarany, Puthuppally, Puthuvely, Vaikom, Kidangoor, Thalayolaparambu, Thekkumthala, Thiruvanchoor, Uzhavoor, Vadavathoor, Vaikom, Vakathanam, Vechoor, Villoonni, Pathamuttom

</doc>
<doc id="56145" url="http://en.wikipedia.org/wiki?curid=56145" title="Kozhikode">
Kozhikode

Kozhikode (]), also known as Calicut, is a city in the state of Kerala in southern India on the Malabar Coast. Kozhikode is the second largest city in Kerala and is part of the second largest urban agglomeration in Kerala with a metropolitan population of 2,030,519 as per 2011 census. The city lies about 380 km north of the state capital Thiruvananthapuram.
During classical antiquity and the Middle Ages, Kozhikode was dubbed the "City of Spices" for its role as the major trading point of eastern spices. It was the capital of an independent kingdom ruled by the Samoothiris (Zamorins) in the Middle Ages and later of the erstwhile Malabar District under British rule. Arab merchants traded with the region as early as 7th century, and Portuguese explorer Vasco da Gama landed at Kozhikode on 20 May 1498, thus opening a trade route between Europe and Malabar. A Portuguese factory and fort was intact in Kozhikode for short period (1511–1525, until the Fall of Calicut), the English landed in 1615 (constructing a trading post in 1665), followed by the French (1698) and the Dutch (1752). In 1765, Mysore captured Kozhikode as part of its occupation of the Malabar Coast. Kozhikode, once a famous cotton-weaving center, gave its name to the Calico cloth.
On 7 June 2012, Kozhikode was given the tag of "City of Sculptures" (Shilpa Nagaram) because of the various architectural sculptures located in various parts of the city.
According to data compiled by economics research firm "Indicus Analytics" on residences, earnings and investments, Kozhikode ranked as the second best city in India to reside in. It was ranked eleventh among Tier-II Indian cities in job creation by a study conducted by ASSOCHAM in 2007. Kozhikode city continues to be a centre of flourishing domestic and international trade. Its contribution to all round development of the district in trade, commerce and economic development over the years is spectacular. Kozhikode city is the marketing centre for commodities like pepper coconut, coffee, rubber, lemon grass oil etc., produced in Kozhikode and the neighbouring districts of Wayanad, Malappuram and Kannur.
Etymology.
The name "Kozhikode" derives, from "koyil" (palace) plus "kota" (fort), meaning "fortified palace." The place was also referred to as "Chullikkad", meaning "shrubby jungle," probably referring to the marshy nature of the land. Linguistically, "ya" and "zha" are interchangeable in Malayalam, and "kode" stands for fort (kotta). While the city has been known by different names by people of other lands, Malayalees have called it "Kozhikode".
The Arabs called it قَالِقُوط "Qāliqūṭ" (IPA: qˠaːliqˠːuːtˤ). The Tamils called the city "Kallikkottai" while for the Chinese it was "Kalifo", in Kannada ಕಲ್ಲಿಕೋಟೆ (Kallikoate).
Although the city's official name is Kozhikode, in English it is sometimes known by its anglicised version, "Calicut".
The word "calico", a fine variety of hand-woven cotton cloth that was exported from the port of Kozhikode, is thought to have been derived from "Calicut". It is the Historical Capital of Kerala as the history dates back to 1498 AD when Vaso da Gama landed in Kappad near Calicut.
History.
Kozhikode is a town with a long recorded history. From time immemorial, the city has attracted travellers with its prosperity. It has traded in spices like black pepper and cardamom with Jews, Arabs, Phoenicians, and Chinese for more than 500 years. As Kozhikode offered full freedom and security, the Arab and the Chinese merchants preferred it to all other ports. The globe-trotter Ibn Batuta (A.D. 1342–47) said:
"We came next to Kalikut, one of the great ports of the district of Malabar, and in which merchants of all parts are found."
Kozhikode was the capital of Malabar during the time of Zamorins (in Malayalam 'Samoothiri'), who ruled the region before the British took over. The city's first recorded contact with Europe was when Vasco da Gama landed at Kappad (18 km north) in May 1498, as the leaders of a trade mission from Portugal. He was received by the Zamorin himself.
Early Kozhikode in foreign accounts.
Accounts of the city and the conditions prevailing then can be gleaned from the chronicles of travellers who visited the port city.
Ibn Battuta (1342–1347), who visited six times, gives us the earliest glimpses of life in the city. He describes Kozhikode as "one of the great ports of the district of Malabar" where "merchants of all parts of the world are found". The king of this place, he says "shaves his chin just as the Haidari Fakeers of Rome do...The greater part of the Muslim merchants of this place are so wealthy that one of them can purchase the whole freightage of such vessels put here and fit out others like them".
Ma Huan (1403 AD), the Chinese Muslim sailor part of the Imperial Chinese fleet under Cheng Ho (Zheng He) lauds the city as a great emporium of trade frequented by merchants from around the world. He makes note of the 20 or 30 mosques built to cater to the religious needs of the Muslims, the unique system of calculation by the merchants using their fingers and toes (followed to this day) and the matrilineal system of succession.
Abdur Razzak (1442–43) the ambassador of Persian Emperor Sha-Rohk finds the city harbour perfectly secured and notices precious articles from several maritime countries especially from Abyssinia, Zirbad and Zanzibar.
The Italian Niccolò de' Conti (1445), perhaps the first Christian traveller who noticed Kozhikode describes the city as abounding in pepper, lac, ginger, a larger kind of cinnamon, myrobalans and zedary. He calls it a noble emporium for all India, with a circumference of eight miles (13 km).
The Russian traveller Athanasius Nikitn or Afanasy Nikitin(1468–74) calls 'Calecut' a port for the whole Indian sea and describes it as having a "big bazaar."
Other travellers who visited Kozhikode include the Italian Ludovico di Varthema (1503–1508) and Duarte Barbosa.
The Zamorins.
Kozhikode and its suburbs formed part of the "Polanad" kingdom ruled by the "Porlatiri". The Eradis of Nediyirippu in Eranad wanted an outlet to the sea, to initiate trade and commerce with the distant lands. and after fighting with the king Polatthiri for 48 years conquered the area around Panniankara. After this, Menokkis were made as the ruler of "Polanad" and came to terms with the troops and people. After this, the town of Kozhikode was founded close to the palace at Tali. Then, the Eradis shifted their headquarters from Nediyirippu to Kozhikode. The Governor of Ernad built a fort at a place called Velapuram to safeguard his new interests. The fort most likely lent its name to "Koyil Kotta" the precursor to Calicut. Thus the city came into existence sometime in the 13th century CE. The status of Udaiyavar increased and he became known as Swami Nambiyathiri Thirumulpad, and eventually Samuri or Samuthiri. Europeans called him Zamorin.
According to K.V. Krishna Iyer, the rise of Kozhikode is at once a cause and a consequence of Zamorin's ascendancy in Kerala. By the end of the century, Zamorin was at the zenith of his powers with all princes and chieftains of Kerala north of Cochin acknowledging his suzerainty.
Vasco da Gama.
Vasco da Gama arrived at Kozhikode on 20 May 1498 and obtained permission to carry out trade. He landed at a place known as Kappad, near Thiruvangoor. The Arabs sensing the threat posed by Portuguese to their commercial supremacy opposed the Europeans. Bitter fights started between Portuguese and Arabs. The Portuguese went to Cochin for trade and the Raja of Cochin had an alliance with the Portuguese with aim of attaining sovereignty from Zamorin.
The hostilities between the Zamorin and the Portuguese continued for many decades and the role played by the Kunjali Marakkar in these battles can not been forgotten. Kunjali Marakkars were the hereditary admirals of the zamorin and organised a powerful navy to fight the Portuguese.Kunhali II, was one of the greatest of Zamorin's Admirals. Kunjali III built a fort at Kottakkal and enjoyed all the privileges enjoyed by the Nair chiefs. His actions against the Portuguese fleets caused heavy damages to Portuguese shipping and trade from Kozhikode.
The Portuguese built a fort at Chaliyam at the mouth of the Beypore River in the middle of the Zamorin's territory. Due to the prolonged struggle, Zamorin's military strength deteriorated and he entered into a treaty with them in 1540, which allowed the Portuguese to have monopoly over trade at Kozhikode port. The peace was temporary and war broke out again resulting in the demolition of Chaliyom Fort in 1571 by the Zamorin forces.
The battles between the Portuguese and the Zamorin continued till 1588 when the Portuguese were allowed to settle down at Kozhikode. However Kunjali opposed the move. At around this time, Kunjali IV declared himself as the 'King of the Moors' and moved away from the Zamorin. The Zamorins now took the help of the Portuguese to destroy the powerful Kunjalis. In 1600, kunjali surrendered and was executed.
In the meanwhile, the Dutch, English and the French arrived in Kerala. Zamorins allowed the Dutch to trade in Kozhikode and sought their help to drive out the Portuguese. The position of Portuguese weakened gradually due to international events and their position in Kerala deteriorated. the Dutch captured Cochin and Cannanore and established trade. However, by 1721, the Dutch formally withdrew from all interference in native wars.
Geography and climate.
Geography.
The city of Kozhikode is 410 km north of the state capital Thiruvananthapuram. It is located at approximately . It has an elevation of 1 metre (3 ft) along the coast with the city's eastern edges rising to at least 15 metres, with a sandy coastal belt and a lateritic midland. The city has a 15 km long shoreline and small hills dot the terrain in the eastern and central regions. To the city's west is the Laccadive Sea and from approximately 60 km to the east rises the Sahyadri Mountains.
The geographical conditions of city area and suburban areas are similar to the other parts of the district falling in coastal and midland zones. The region comprising Kozhikode Corporation and peri-urban blocks belong to the low- and midlands in the typical classification of land in Kerala as low-, mid- and highlands. Lagoons and backwaters characterise the lowland, which receives runoff from the rivers. The lowland is often subjected to salinity intrusion. The coastal plains exhibit more or less flat, narrow terrain with landforms such as beach ridges, sandbars, and backwater marshes. A few kilometres from the sea to the east, the surface gathers into slopes and clustering hills with numerous valleys in between formed due to floods and sediment transport. The Midlands is represented by hummocky rocky terrain with lateritised denudational hills and intervening valley fills (locally called "elas"). The 'elas' are fairly wide in the lower reaches of midlands and narrow towards the upper parts of the midlands.
A number of rivers originating from the Sahyadri run along the outer reaches of the city. These include the Chaliyar puzha, Kallayi Puzha, Korapuzha river, Poonoor "puzha" (river), and Iravanjhi "puzha". Of these, Kallai river that runs through the southern part of the city has been the most important culturally and historically for Kozhikode. The Kallai River has its origin in Cherikkulathur village. It is connected with Chaliyar on the south by a man-made canal. The river passes through Cherukulathur, Kovur, Olavanna, Manava and Kallai before finally joining the sea near Kozhikode. The length of the river is 22 km.
The Korapuzha river is formed by the confluence of the Agalapuzha with the Punnurpuzha, and it joins the sea at Elathur. The Agalapuzha is more or less a backwater while the Punnurpuzha originates from Arikkankunni. The total length of the river is 40 km. Panurpuzha is a tributary of Korapuzha. It passes through the northern boundary of the study area and joins to the sea. The river is perennial.
Canoly Canal was built in 1848 to connect the Korapuzha river in the north to Kallayi river in the south. It functions as a drain to reduce flooding in the city during the rainy season and as a navigation channel.
A system of wetland (mangrove) forests pervade the city from Kallai river to Eranjikkal.
Climate.
Kozhikode features a tropical monsoon climate (Köppen climate classification "Am"). The city has a highly humid tropical climate with high temperatures recorded from March to May. A brief spell of pre-monsoon Mango showers hits the city sometime during April. However, the primary source of rain is the South-west monsoon that sets in the first week of June and continues until September. The city receives significant precipitation from the North-East Monsoon that sets in from the second half of October through November.
The average annual rainfall is 3,266 mm. The weather is milder from December/January until March when the skies are clear and the air is crisp. Winters are seldom cold. According to climate charts, 12 locations in India are cooler, 26 are warmer, 37 are dryer and only 1 is wetter than Kozhikode The highest temperature recorded was 39.4 °C in March 1975. The lowest was 14 °C recorded on 26 December 1975.
Demographics.
The corporation of Kozhikode has an average literacy rate of 96.8% (national average is 74.85%). The male literacy rate is 97.93% and female literacy rate is 95.78%. Malayalam is the most spoken language. English, Tamil and Hindi are widely understood.
Kozhikode has been a multi-ethnic and multi-religious town since the early medieval period. The Hindus forms largest religious group, followed by Muslims and Christians.
The Hindus engage in beliefs spanning all forms of theism as well as atheism. Brahma, Vishnu, Shiva and other Gods and Goddesses of the Hindu pantheon are worshipped. Many places have temples with local deities, more often a Goddess (Devi). Festivities like Theyyam, Thira and art forms like Ottamthullal, Kathakali are performed in stages attested to temple estates. Many temples have associated oracles called "Velichappad". Serpent and ancestral worship are also practised.
The Muslims of Kozhikode are known as Mappilas, and according to the official Kozhikode website "the great majority of them are Sunnis following the Shafi school of thought. There are also some smaller communities among the Muslims such as Dawoodi Bohras. Many of the Muslims living in the historic part of the city follow matriliny and are noted for their piety. Though Christianity is believed to have been introduced in Kerala in 52 CE, the size of community in Malabar (northern Kerala) began to rise only after the arrival of the Portuguese towards the close of the 15th century. A few Christians of Travancore and Cochin have lately migrated to the hilly regions of the district and are settled there.
Pre-modern Kozhikode was already teeming with people of several communities and regional groups. Most of these communities continued to follow their traditional occupations and customs till the 20th century. These included Kosavan (potter), Vannan (washerman), Pulayan (agricultural worker), Chaliyan (weaver), Chetti (merchant), Thiyya (physicians, militia and toddy tappers), Ganaka (astrologer), Vettuvan (salt-maker), Paanan (sorcerer), Eravallan (firewood and grass carrier), Kammalas, Parayan etc. A number of Brahmins too lived in the city mostly around the Hindu temples. Regional groups like the Tamil Brahmins, Gujaratis and Marwari Jains became part of the city at various periods and lived around their shrines.
The Nairs formed the rulers, warriors and landed gentry of Kozhikode. The Zamorin had a ten thousand strong Nair bodyguard called the Kozhikkottu pathinaayiram (The 10,000 of Kozhikode) who defended the capital and supported the administration within the city. He had a larger force of 30,000 Nairs in his capacity as the Prince of Eranadu, called the Kozhikkottu Muppatinaayiram (The 30,000 of Kozhikode). The Nairs also formed the members of the suicide squad (chaver). The aristocratic Nairs had their "Taravad" houses in and around the capital. Several Nairs in the city were traders too. The Nairs could not be imprisoned or fettered except for serious crimes like cow slaughter, criticising the King etc. The Mappila community of Kozhikode acted as an important support base for the city's military, economic and political affairs. They were settled primarily in Kuttichira and Idiyangara. Their aristocratic dwelling houses were similar to the "tharavad" houses of the Nairs and the Thiyyas. Two Ghazi's were recognised as their spiritual leaders. Travellers like Barbosa were intrigued by the extent to which the Mapillas blended into the local society, who spoke the same language and looked like any other Nair (except for the round caps and long beards).
The Thiyyas formed the "vaidyars"(Physicians), local militia and traders of Kozhikode. Several aristocratic thiyya families such as 'Kallingal madom' were settled in and around the city.
The Tamil Brahmins are primarily settled around the Tali Siva temple. They arrived in Kozhikode as dependants of chieftains, working as cooks, cloth merchants and moneylenders. They have retained their Tamil language and dialects as well as caste rituals. The Gujarati community is settled mostly around the Jain temple in and around the Valliyangadi. They owned a large number of establishments, especially textile and sweet meat shops. They must have arrived in Kozhikode at least from the beginning of the 14th century. They belong to either the Hindu or the Jain community. A few Marwari families are also found in Kozhikode who were basically moneylenders.
Civic administration.
The city is administered by the Kozhikode Corporation, headed by a mayor. For administrative purposes, the city is divided into 75 wards, from which the members of the corporation council are elected for five years. Recently neighbouring suburbs Beypore, Elathur, Cheruvannur and Nallalam were merged within the municipal corporation.
Kozhikode corporation has four assembly constituencies – Kozhikode North, Kozhikode South, Beypore and Elathur – all of which are part of Kozhikode.
Law and order.
The Kozhikode City Police is headed by a commissioner, an Indian Police Service (IPS) officer. The city is divided into six zones each under a circle officer. Apart from regular law and order, the city police comprises the traffic police, bomb squad, dog squad, fingerprint bureau, women's cell, juvenile wing, narcotics cell, riot force, armed reserve camps, district crime records bureau and a women's station. It operates 16 police stations functioning under the Home Ministry of Government of Kerala.
Transport.
Road.
The city has a reasonably well-developed transport infrastructure. A large number of buses, predominantly run by individual owners, ply on the major routes within the city and to nearby locations. City buses are painted green. Kerala State Road Transport Corporation (KSRTC) runs regular services to many destinations in the state and to the neighbouring states. The city has three bus stands. All private buses to the suburban and nearby towns ply from the Palayam Bus Stand. Private buses to adjoining districts start from the Mofussil Bus Stand (one of the largest bus stand in Kerala) on Indira Gandhi Road (Mavoor Road). Buses operated by the KSRTC drive from the KSRTC bus stand on Indira Gandhi Road. KSRTC Bus Stand Kozhikode is the biggest bus stand in Kerala having a size of 36,036.47-meter square. There are also KSRTC depots in Thamarassery, Thottilpalam, Thiruvambady and Vadakara in the district.
There are two routes available to Bangalore. One is Kozhikode–Gundlupet–Mysore–Bangalore; this road is most preferred one but is very busy. Another route, less used, is Kozhikode–Gundlupet–Chamarajanagar–Kollegal–Bangalore.
A coastal route is available to Kochi via Feroke(NH)Kadalundikadav\Chelari(NH), Tirur, Chamravattom, Chavakkad and North Paravur. This road is very narrow in some parts but the easiest one.
Private tour operators maintain regular luxury bus services to Mumbai, Bangalore, Coimbatore, Chennai, Ernakulam, Trivandrum, Ooty etc. and mainly operate from the Palayam area. These are usually night services.
National Highways.
National Highway 66 connects Kozhikode to Mumbai via Mangalore, Udupi and Goa to the north and Kochi and Kanyakumari near Thiruvananthapuram to the south along the west coast of India. This highway connects the city with the other important towns like, Uppala, Kasaragod, Kanhangad, Kannur, Thalassery, Mahe, Vadakara, Koyilandy, Pavangad, Kozhikode, Kottakkal, Kuttippuram, Ponnani, (Guruvayoor)Chavakkad, Kodungallur, North Paravur, Edapally and proceed to Kanyakumari.
National Highway 766 connects Kozhikode with Mysore in Karnataka via Nanjangud, Tirumakudal Narsipur, Gundlupet, Sulthan Bathery, Kalpetta and Thamarassery. This highway also connects the city with the suburbs like Malaparambu, Kunnamangalam and premier institutes like IIM-K, NIT-C, IISR and CWRDM.
National Highway 966 connects Kozhikode with Palakkad. It covers a distance of 125 km. At Ramanattukara, a suburb of Kozhikode, it joins NH 66. It also passes through towns like Kondotty, Malappuram, Perinthalmanna, and Mannarkkad. This stretch also connects the city and Calicut International Airport.
State Highways.
SH 28 is the Kerala section of highway connecting Kozhikode and Gudalur near Ooty. The highway is 103.6 km long. It passes through important towns like Manjeri and Nilambur.
SH 29 passes through the city. It connects NH 212, Malabar Christian College, civil station, Kunnamangalam and also Padanilam, Thamarassery, Chellot, Chitragiri and Road to Gudallor from Kerala border.
SH 54 is connecting city and Kalpetta. The highway is 99.0 km long. The highway passes through Pavangad, Kozhikode, Ulliyeri, Perambra, Poozhithodu, Peruvannamuzhi and Padinjarethara.
SH 68 starts from Kappad and ends in Adivaram. The highway is 68.11 km long.
SH 34 starts from Koyilandy and ends in Edavanna. The highway is 44.0 km long.
Rail.
The history of railways in Malabar dates back to 1861 when the first tracks were laid between Tirur and Beypore. Today, Kozhikode is well connected by rail to cities like Thiruvananthapuram, Kochi, Kollam, Palakkad, Coimbatore, Chennai, Bangalore, Kannur, Mangalore, Mumbai and New Delhi.
Vijayawada, Vishakapatnam, Hyderabad. The needed future development is Kozhikode - Balusssery - Kuttyadi - Mattannur - Cherkala rail line.
Monorail.
The Kozhikode Monorail planned now under implementation stage.The construction will start by the end of 2012. The state government has submitted the proposal for the monorail project to the central government and is awaiting approval. It is an undertaking by DMRC.The routes planned: MedicCalicut Medical College-Meenchanda-Calicut International Airport .The monorail will connect through the railway station. The first phase of the project has an estimated cost of around 20 billion, and is proposed for completion in three phases. It will reach to meenchanda. After completing this project, Kozhikode is the first city have Monorail in south India.The Kozhikode Monorail will be extended to Malapparamba, and plans exist to later extend it to Kunnamangalam.
Air.
Calicut International Airport is 26 km from the city at Karipur in Malappuram. Regular domestic services are operated to major Indian cities. There are frequent international flights to the Middle eastern air hubs like Dubai, Abu Dhabi, Muscat, Dammam, Riyadh, Jiddah, Sharjah, Bahrain, Doha and to Colombo.
Economy.
Kozhikode is one of the main commercial cities of Kerala. The economy is mainly business oriented. The city currently is the major trade hub of North Kerala with good connectivity through road, rail and air. It also has large timber yards along the banks of the Kallayi River. 
Kozhikode District with 8% of the state population makes 12% contribution to the state's income. Kozhikode has witnessed a building boom in recent years. This is particularly evident in the number of malls and buildings built in recent years. Kozhikode is also going to be the first city in Kerala to have a mono rail transporting system.The KSRTC bus terminal which is under construction is the biggest bus terminal in Kerala.
The District has an intermediate port at Kozhikode (including Beypore) and a minor port at Vadakara. In coast line of the Kozhikode port extends from Elathur cape to the south bank of Kadalundi river and treads roughly in straight line. This port has two Piers, but this cannot be used due to the dilapidate condition. Traffic is mainly dealt at Beypore port. Kozhikode Port has a Light House and a Signal Station. The godown at South Pier is used as transit sheds.
Two IT "cyber parks" are under construction in Kozhikode. One is the UL cyber park (constructed and operated by ULCCSC, a Kozhikode-based company). UL cyber park began operation in 2012 and will complete its first phase in 2013. The other park is run by the government, and will complete its first construction phase in 2014.
Cyberpark, is a Government of Kerala organisation planned to build, operate and manage IT parks for the promotion and development of investment in IT and ITES industries in Malabar region of Kerala and will be the third IT hub in the state of Kerala.The two IT park will create a total 100,000(100000) direct job opportunities. It is in the process of setting up IT parks at Kozhikode, at the SEZs approved at Kannur and Kasargod. Its first project is the development of Cyberpark hub in Kozhikode with its spokes at Kannur and Kazargode IT parks. Other planned projects include the Birla IT park (at Mavoor) and Malaysian satellite city (at Kinaloor) where KINFRA has plans to set up a 400 acre industrial park.
Shopping Malls.
The city has a strong mercantile aspect. The main area of business was once 'Valiyangadi' (Big Bazaar) near the railway station. As time progressed, it shifted to other parts of the city. These days, the commercial heart has moved to "Mittai Theruvu" (Sweet Meat Street), a long street crammed with shops that sell everything from saris to cosmetics. It also houses restaurants and sweetmeat shops. Today, the city has multiple shopping malls. Currently, new shopping malls are springing up all over the city. This has changed the consumer habits, shifting the centre of commerce from S M Street (Mitthai Theruvu) to these places.
Places of interest and historical significance.
Kozhikode is famous for its boat-building yard, timber industry and historic temples and churches. There are a large number of tourist locations in the district while tourists visiting Kozhikode are attracted more towards leisure tourism including beaches and historical monuments.Kozhikode functions mostly as a transit point for domestic and foreign tourists.There are 148 classified hotels in Kerala, as listed by the Tourism Department, 22 hotels such as Malabar Palace, The Gateway, Alakapuri, Hyson Heritage and The Westway are located in Kozhikode city and constitute 15% of the state's total classified hotels.
Some of the popular places of interest are Kozhikode Beach, Veliyangadi (big bazaar), Mananchira, S.M. Street, Regional Science Centre and Planetarium, Sarovaram Biopark, Tali Siva Temple, Mishkal Mosque, Panniyankara Bhagavati Temple, Thiruvannur Siva Temple, Kappad Beach, Beypore,Beypore Siva temple,Beypore Beach,Thusharagiri Falls.
Kozhikode Beach.
The Kozhikode Beach is situated near the town of Kozhikode and is known for its old world charm and natural beauty. The beach has two crumbling piers that stand toward the middle of the sea and each of them is more than a hundred years old. The beach also houses a lighthouse, Marine Water Aquarium and Lions Park. The beach is a perfect setting for tourists wanting to enjoy the sunrise or sunset. The beach witnesses huge number of tourists everyday. A large area of the beach was renovated and various statues made by eminent artists are placed here. The coastal area is about 1.2 m above sea level, whereas the eastern part of the city is at about 15 m above sea level. The city has a long seashore of 15 km. Small hills dot the city terrain in the eastern and central portions. In the city nearly 5500 Hectares of land is used for cultivation and nearly 321 Hectares are waterlogged area.
Culture.
In the field of Malayalam language and literature, Kozhikode has made many significant contributions. A 17th century Zamorin king named Manavedan authored the famous 'Krishnattam', a manipravala text describing the childhood of Lord Krishna in eight volumes. The district is famous for folk songs or ballads known as "Vadakkan Pattukal". The most popular songs celebrate the exploits of Thacholi Othenan and Unniyarcha. An intellectual debate for Vedic scholars, where winners receive the title of Pattathanam, takes place at Thali temple during the month of Thulam. Kozhikode also has a strong associations with ghazals and football.
Literature.
Many prominent writers of Malayalam literature hail from Kozhikode. Among them are S. K. Pottekkatt, Thikkodiyan, U. A. Khader, K. T. Muhammed, Akbar Kakkattil, N. V. Krishna Warrier, N.N. Kakkad, M.P.Veerendra Kumar, P. Valsala and M. T. Vasudevan Nair. Sanjayan a known satirist was also from the city. S. K. Pottekkatt was a famous Malayalam writer, author of nearly sixty books which include ten novels, twenty-four collections of short stories, three anthologies of poems, eighteen travelogues, four plays, a collection of essays and a couple of books based on personal reminiscences. His biographical novel Oru Desattinte Katha won the Kerala Sahithya Academy Award in 1972, the Kendra Sahithya Academy Award in 1977, and the Jnanpith Award in 1980.
Music.
In addition to the Malabar Mahotsavam, every year since 1981 the Tyagaraja Aradhana Trust has been conducting a five-day music festival in honour of Sri Tyagaraja. The festival is complete with the Uncchavritti, rendering of Divyanama kritis, Pancharatna Kritis, concerts by professional artistes and students of music from morning to late in the evening.
Kozhikode has a tradition of Ghazal and Hindustani music appreciation. There are many Malayalam Ghazals. The late film director and play back singer M.S. Baburaj, from Kozhikode was influenced by Ghazal and Hindustani.
Apart, Hindi songs are more popular in this city. Mohammed Rafi Foundation, organizes musical nights in the name "Rafi Nite" on birth (on 24 December) and death anniversary of legendary singer Mohammed Rafi. It is estimated that the gathering for this Rafi nite is the largest gathering for the Rafi nite any where in India.
Cuisine.
Kozhikode offers a variety of South Indian, North Indian, European, Chinese, Arab, Gujarati and Jain food. It also has a street food culture, though small and confined to the outskirts. The European and Arab trade influences, from the importance of Kozhikode as a port to east Africa and the Mideast, has influenced the culinary culture of Kozhikode.
'Halwa', a sweet meat so called by British, made out of refined flour, sugar, palm sugar and cooked in coconut oil, is a sought after savoury in Kozhikode.The four traditional halwa varieties are black, white, red (almond) or green (pistachio) but new varieties like beetroot, tomato, mango, jackfruit, sesame, grape, strawberry and chocolate are popular too.
However, a new generation of Kozhikode people are more inclined towards Chinese and American food culture. Chinese food is the most popular and loved among locals. Vegetarianism is increasing as a new trend.
Films.
The film history of Kozhikode dates back to 1950s. Some of the main production companies of Malayalam films like Grihalakshmi productions, Kalpaka, Swargachithra, etc. are Kozhikode based companies. The city was also an important hub of top notch film makers like M. T. Vasudevan Nair, I. V. Sasi and T. Damodaran. Kozhikode produced such notable actors as Ummer, Mammukoya, Balan K. Nair, Santha Devi and Kuthiravattam Pappu. The ever green musician Baburaj, Gireesh Puthenchery, arguably one of the best lyricists in the Malayalam film industry, lyricist and music director Kaithapram Damodaran Namboothiri, director, script writer and actor Ranjith, Hariharan, V. M. Vinu, A. Vincent, Shajoon Kariyal, Anjali Menon and cinematographer P. S. Nivas also hail from Kozhikode.
Some of the other cine actors like Madhupal, Anoop Menon, Nellikode Bhaskaran and Augustine are from Kozhikode.
Famous cine actress from Kozhikode include Swetha Menon, Nithya Menen, Mamta Mohandas, Ann Augustine, Nithya Das, Jomol, Akhila Sasidharan, Parvathi Menon.
The 1947 Douglas Fairbanks Jr. Hollywood thriller, Sindbad the Sailor, mentions Kozhikode.
Kozhikode, the largest city in the Malabar region, also has a vital role in the entertainment segment. City has more than 10 theatres and 1 Multiplex which encourages the entertainment segment to its top. The first multiplex in Malabar became a reality following the efforts of P.S. Nataraj, managing director, P.V.S. Film City, and M.M. Ramachandran, chairman of Atlas Group of Companies with establishment of PVS Film City. Two of the screens have active 3-D facility, which is said to be the first of its kind in the state.
Sports.
Kozhikode is known as the second Mecca of football (after Kolkata). The other most popular games in Kozhikode are cricket, football, basketball, badminton and volleyball. The E. M. S Stadium hosted many international football matches of major football teams in the past. The city is home to many international footballers. One of the famous was Olympian Abdurahman who played for the nation in many international games including Melbourne Olympic games. K.P. Sethu Madhavan, Premnath Phillips, Sudheer etc. are some international footballers from Kozhikode. The seven-a-side form of football is also very famous in the city.
P. T. Usha, is a famous athlete who is regarded as one of the greatest athletes India has ever produced and is often called the "queen of Indian track and field". She is nicknamed Payyoli Express. Currently she runs the Usha School of Athletics at Koyilandy in Kerala.
T. Abdul Rahman, popularly known as Olympian Rahman, was an Indian Olympian footballer from Kozhikode. Rahman was a member of the Indian team that reached the semi-final in 1956 Melbourne Olympics.
Other sports personalities include Tom Joseph (Indian volleyball player and was captain of Indian volleyball team) and Premnath Phillips.
Jaseel P. Ismail, V. Diju, Aparna Balan & Arun Vishnu are international badminton players from the city.
The Sports & Education Promotion Trust (SEPT) was established to promote sports development in India with focus on football. Started in 2004 and based in Kozhikode, the trust has set up 52 centres called "football nurseries" spread across thirteen districts in Kerala. Since 2010, Calicut Mini Marathon runs have been organised by IIM Kozhikode and witness participation of around 7000 people every year.
Media.
Print.
Kozhikode occupies a prominent position in the history of Malayalam journalism. The origin of journalism in the district can be traced back to 1880. The "Kerala Pathrika" is likely the earliest newspaper published from Kozhikode. "Keralam", "Kerala Sanchari" and "Bharath Vilasam" are among the other newspapers that were published from Kozhikode pre-1893.
Kozhikode is the 'birthplace' of the widely circulated Malayalam dailies "Mathrubhumi", "Desabhimani" and "Madhyamam". "Chandrika", "Thejas", "Siraj", "Varthamanam" and "Calicut Times" are the another dailies from Kozhikode. Along with those papers, noted dailies like "Malayala Manorama", "Kerala Kaumudi", "Mangalam", "Deepika", "New Indian Express", The Hindu, Deccan chronicle,"Janmabhumi", "Veekshanam" and evening dailies like "Pradeepam", "Rashtra deepika", "News Kerala" and "Flash" are published from Kozhikode. Nearly all news agencies, other major newspapers published from outside the state are represented in Kozhikode. "The Times of India", the largest-circulating English broadsheet newspaper in the world, started circulation in Kozhikode on 1 February 2012. A large number of weeklies, fortnightlies and monthlies are also published there (such as Information Technology Lokam, a computer magazine in Malayalam). Newspapers in other regional languages like English, Hindi, Kannada, Tamil and Telugu are available.
Radio.
The Kozhikode radio station of All India Radio has two transmitters: Kozhikode AM (100 kilowatt) and Kozhikode FM [Vividh Bharathi] (10 kilowatt). Private FM radio stations: Radio Mango 91.9 operated by Malayala Manorama Co. Ltd.
and Red FM 93.5 of the SUN Network. AIR FM radio station: Kozhikode – 103.6 MHz; AIR MW radio station: Kozhikode – 684 kHz.
Television.
A television transmitter has been functioning in Kozhikode since 3 July 1984, relaying programmes from Delhi and Thiruvananthapuram Doordarshan. Doordarshan has its broadcasting centre in Kozhikode located at Medical College. The Malayalam channels based on Kozhikode are the Shalom Television, Darsana TV and Media One TV. All major channels in Malayalam viz. Manorama News, Asianet, Surya TV, Kairali TV, Amrita TV, Jeevan TV, Indiavision and Jaihind have their studios and news bureaus in the city. Satellite television services are available through DD Direct+, Dish TV, Sun Direct DTH and Tata Sky. Asianet Cable Vision popularly known as ACV telecasts daily city news. Spidernet is another local channel. Other local operators include KCL and Citinet.
The Calicut Press Club came into existence in 1970. It is the nerve centre of all media activities, both print and electronic. Began with around 70 members in the roll, this Press Club, over the years, became a prestigious and alert media center in the state with a present membership of over 280.
Telecommunications.
Telephone services are provided by various players like Airtel, Idea cellular, Vodafone, Reliance Infocomm, Tata Docomo, MTS, Uninor, Tata Indicom and the state owned BSNL and most of them provide 3G services also. The city also has broadband wireless services on WiMAX platform.
Education.
There were reputed centres of learning and culture in Kozhikode even in the early and medieval periods. Under the rule of the enlightened Zamorins, it became famous all over South India as a rendezvous of scholars and men of learning.
The beginning of western education may be traced back to the first half of the 19th century, when in 1848 the basal Evangelical Mission started a primary school at Kallai. In 1877, a school for the young Rajas was started in Kozhikode. This was later thrown open to all caste Hindu boys. In 1879, it was affiliated to the University of Madras as a second grade college and with this, collegiate education in the district received a fillip. Secondary education recorded an appreciable progress since 1915. The erstwhile Malabar district, of which the present Kozhikode district formed a part, holds a high rank among the districts of Madras Presidency in secondary education.
Primary education.
Kerala primary education starts with pre-primary institutions just like Anganvadis and play schools where it is the basic stage in schooling. Primary school is further divided lower primary (LP) [classes I–IV] and into upper primary (UP) [classes V–VII]. The pattern of primary education is essentially the same all over the state. Each school is affiliated with either the Indian Certificate of Secondary Education (ICSE), the Central Board for Secondary Education (CBSE), Kerala State Education Board or the National Institute of Open Schooling (NIOS). English is the language of instruction in most private schools, while government run schools offer English or Malayalam as the medium of instruction.The city is widely known through the functioning of educational institutions like St. Josephs Boys High School (established in 1794), Spring Valley School.
Higher education.
Kozhikode is home to two premier educational institutions of national importance: the Indian Institute of Management Kozhikode (IIMK), and the National Institute of Technology, Calicut (NITC). The NITC and IIMK are institutions with university status under Union Government
The NITC was formerly known as Calicut Regional Engineering College (CREC). CREC was born in September 1961 as the ninth of its kind and the first one established during the Third Five Year Plan period. It become a Deemed University under the name National Institute of Technology Calicut in June 2002. NITC is located about 22 kilometres northeast of Kozhikode. It started management education also (School of management Studies) in the year 2009.
The University of Calicut the main university named after the city, is in Thenjipalam, about 24 km south of Kozhikode, in the district of Malappuram. This university established in 1968 was the second university set up in Kerala. Most of the colleges offering tertiary education are affiliated with this university.
The Calicut Medical College was established in 1957 as the second medical college in Kerala. Since then, the institution has grown into a premier center of medical education in the state. Presently it is the largest medical institute in the state with a yearly intake of 250 candidates for the undergraduate program.
Some of the other major institutes in Kozhikode are the (CUIET), Government Engineering College (GEC), Malabar Christian College, Zamorin's Guruvayurappan College, St. Joseph's College, Devagiri, Farook College, Government Arts and Science College, Providence Women's College, Government Homeopathic Medical College, Government Law College, Government College of Teacher Education, Kerala School of Mathematics, , formerly known as DOEACC, CEDTI etc.
Research institutes.
There are a few research institutes located in or around the city. These include the Indian Institute of Spices Research (IISR), the Centre for Water Resources Development and Management (CWRDM), Western Ghats Field Research Station (Zoological Survey of India) and the Regional Filaria Training and Research Centre, a centre of the National Institute of Communicable Diseases, Centre for Mathematics.
Renowned personalities.
Many famous diplomats and politicians hails from this district. Among them are V.K. Krishna Menon, C. H. Muhammed Koya, K. Kelappan, K.P. Kesava Menon, P.P. Ummer Koya, M. K. Muneer.
Dr. Verghese Kurein was the person who played an outstanding role in the development of Amul. He was known as the 'Father of the white revolution' in India. He is also called as the Milkman of India. Dr. Varghese was the architect behind the success for the largest dairy development programme in the world., christened as Operation Flood.
 is one of the greatest athletes India has ever produced. She has won 101 international medals in her sparkling career. During the 1985 Asian Track and Field Meet at Indonesia, Usha also nicknamed as Payyoli Express secured 5 gold medals, in the 100, 200, and 400-metre sprints, 400m hurdles, 4x400m relay and a bronze ine 4x100m relay. This is the current World Record for the most gold medals earned by a female in a single track meet.

</doc>
<doc id="56148" url="http://en.wikipedia.org/wiki?curid=56148" title="Taranto">
Taranto

"For other uses, see Tarentum (disambiguation)."
Taranto (]; early Italian: "Tarento" from Latin: "Tarentum"; Ancient Greek: Τάρᾱς "Tarās"; Modern Greek: Τάραντας "Tarantas"; Tarantino "Tarde") is a coastal city in Apulia, Southern Italy. It is the capital of the Province of Taranto and is an important commercial port as well as the main Italian naval base.
It is the third-largest continental city of Southern Italy: according to 2011 population census, it has a population of 200,154.
Taranto is an important commercial and military port. It has well-developed steel and iron foundries, oil refineries, chemical works, some shipyards for building warships, and food-processing factories.
Taranto is known as "The Spartan City" because it was founded by the Spartans in 706 BC.
In ancient times around 500 BC the city was one of the largest in Italy with population estimates up to 300,000 people.
Other estimates are lower; the built-up area was 530 hectares (1300 acres).
Overview.
Taranto's pre-history dates back to 706 BC when it was founded as a Greek colony, established by the Spartans. The ancient city was situated on a peninsula; the modern city has been built over the ancient Greek city of which only a few ruins remain, including part of the city wall, two temple columns dating to the 6th century BC, and tombs.
The islets of "S. Pietro" and "S. Paolo" (St. Peter and St. Paul), collectively known as Cheradi Islands, protect the bay, called "Mar Grande" ("Big Sea"), where the commercial port is located. Another bay, called "Mar Piccolo" ("Little Sea"), is formed by the peninsula of the old city, and has flourishing fishing. "Mar Piccolo" is a military port with strategic importance.
At the end of the 19th century, a channel was excavated to allow military ships to enter the "Mar Piccolo" harbour, and the ancient Greek city become an island connected to the mainland by bridges. In addition, the islets and the coast are strongly fortified. Because of the presence of these two bays, Taranto is also called “the city of the two seas”.
The Greek colonists from Sparta called the city Taras (Τάρας), after the mythical hero Taras, while the Romans, who connected the city to Rome with an extension of the Appian way, called it Tarentum.
The natural harbor at Taranto made it a logical home port for the Italian naval fleet before and during the First World War. During World War II, Taranto became famous as a consequence of the November 1940 British air attack on the Regia Marina naval base stationed here, which today is called the Battle of Taranto.
Taranto is also the origin of the common name of the Tarantula spider family, Theraphosidae, even though strictly speaking there are no members of Theraphosidae in the area. In ancient times, residents of the town of Taranto, upon being bitten by the large local Wolf Spider, "Lycosa tarentula", would promptly do a long vigorous dance like a Jig. This was done in order to sweat the venom out of their pores, even though the spider's venom was not fatal to humans. The frenetic dance became known as the Tarantella.
Physical geography.
Taranto faces the Ionian Sea. It is 14.5 m above sea level. It was built on a plain running north/north-west–southeast, and surrounded by the Murgia plateau from the north-west to the east. Its territory extends for 209.64 km2 and is mostly underwater. It is characterised by three natural peninsulas and a man-made island, formed by digging a ditch during the construction of Aragon Castle. The city is known as the "city of two seas" because it is washed by the Great Sea in the bay between Punta Rondinella to the northwest and Capo San Dante to the south, and by the vast reservoir of the Little Sea.
Great Sea and Little Sea.
The Great Sea is frequently known as the "Great Sea bay" as that is where ships harbour. It is separated from the Little Sea by a cape which closes the gulf, leading to the artificial island. This island formed the heart of the original city and it is connected to the mainland by the Ponte di Porta Napoli and the Ponte Girevole. The Great Sea is separated from the Ionian Sea by the Capo San Vito, the Isole Cheradi of St Peter and St Paul, and the three islands of San Nicolicchio, which are completely incorporated by the steel plant. The latter form a little archipelago which closes off the arc creating the natural Great Sea bay.
The Little Sea is considered to be a lagoon so it presents problems of water exchange. It is virtually divided into two by the Ponte Punta Penna Pizzone, which joins the Punta Penna to the Punta Pizzone. The first of these forms a rough triangle, whose corners are the opening to the east and the Porta Napoli channel linking it to the Great Sea in the west. The second half forms an ellipse whose major axis measures almost 5 km from the south-west to the north-east. The Galeso river flows into the first half.
The two seas have slightly different winds and tides and their underwater springs have different salinities. These affect the currents on the surface and in the depths of the Great Sea and the two halves of the Little Sea. In the Great Sea and in the northern part of the Little Sea, there are some underwater springs called citri, which carry undrinkable freshwater together with salt water. This creates the ideal biological conditions for cultivating Mediterranean mussels, known locally as "cozze".
Climate.
The climate of the city, recorded by the weather station situated near the Grottaglie Military Airport, is typical of the Mediterranean with frequent Continental features.
The spring is usually mild and rainy, but it is not uncommon to have sudden cold spells come in from the east, which often cause snowfall.
The summer is very hot and humid, with temperatures reaching up to 40 C.
On 28 November 2012 a large F3 tornado hit the port of Taranto and damaged the Taranto Steel Mill where workers were protesting against the plant's pollution emissions; about 20 people were injured, and another man was reported missing. The tornado is one of nine to hit Italy since 1 October.
It is classified as Geographical zone C and having a degree-day of 30.
History.
Taranto was founded in 706 BC by Dorian Greek immigrants as the only Spartan colony, and its origin is peculiar: the founders were Partheniae ("sons of virgins"), sons of unmarried Spartan women and "Perioeci" (free men, but not citizens of Sparta); these out-of-wedlock unions were permitted extraordinarily by the Spartans to increase the prospective number of soldiers (only the citizens of Sparta could become soldiers) during the bloody Messenian wars, but later they were retroactively nullified, and the sons were then obliged to leave Greece forever. Phalanthus, the Parthenian leader, went to Delphi to consult the oracle: the puzzling answer designated the harbour of Taranto as the new home of the exiles. The Partheniae arrived in Apulia, and founded the city, naming it "Taras" after the son of the Greek sea god, Poseidon, and of a local nymph, Satyrion. According to other sources, Heracles founded the city. Another tradition indicates Taras as the founder of the city; the symbol of the Greek city (as well as of the modern city) depicts the legend of Taras being saved from a shipwreck by riding a dolphin that was sent to him by Poseidon. Taranto increased its power, becoming a commercial power and a sovereign city of Magna Graecia, ruling over the Greek colonies in southern Italy.
Its independence and power came to an end as the Romans expanded throughout Italy. Taranto won the first of two wars against Rome for the control of Southern Italy: it was helped by Pyrrhus, king of Greek Epirus, who surprised Rome with the use of elephants in battle, a thing never seen before by the Romans. Rome won the second war in 272 B.C. This subsequently cut off Taranto from the centre of Mediterranean trade, by connecting the Via Appia directly to the port of Brundisium (Brindisi).
Ancient art.
Like many Greek city states, Taras issued its own coins in the 5th and 4th centuries B.C. The denomination was a Nomos, a die-cast silver coin whose weight, size and purity were controlled by the state. The highly artistic coins presented the symbol of the city, Taras being saved by a dolphin, with the reverse side showing the likeness of a hippocamp, a horse-fish amalgam which is depicted in mythology as the beast that drew Poseidon's chariot.
Taras was also the center of a thriving decorated Greek pottery industry during the 4th century BC. Most of the South Italian Greek vessels known as Basilican ware were made in different workshops in the city.
Unfortunately, none of the names of the artists have survived, so modern scholars have been obliged to give the recognizable artistic hands and workshops nicknames based on the subject matter of their works, museums which possess the works, or individuals who have distinguished the works from others. Some of the most famous of the Apulian vase painters at Taras are now called: the Iliupersis Painter, the Lycurgus Painter, the Gioia del Colle Painter, the Darius Painter, the Underworld Painter, and the White Sakkos Painter, among others.
The wares produced by these workshops were usually large elaborate vessels intended for mortuary use. The forms produced included volute kraters, loutrophoroi, paterai, oinochoai, lekythoi, fish plates, etc. The decoration of these vessels was red figure (with figures reserved in red clay fabric, while the background was covered in a black gloss), with overpainting (sovradipinto) in white, pink, yellow, and maroon slips.
Often the style of the drawings is florid and frilly, as was already the fashion in fourth-century Athens. Distinctive South Italian features also begin to appear. Many figures are shown seated on rocks. Floral motifs become very ornate, including spiraling vines and leaves, roses, lilies, poppies, sprays of laurel, acanthus leaves, etc. Often the subject matter consists of naiskos scenes (scenes showing the statue of a deceased person in a naos, a miniature temple or shrine). Most often the naiskos scene occupies one side of the vase, while a mythological scene occupies the other. Images depicting many of the Greek myths are only known from South Italian vases, since Athenian ones seem to have had more limited repertoires of depiction.
Municipal bankruptcy.
The Municipality of Taranto was declared bankrupt because - as ascertained by Francesco Boccia, chief of the liquidation committee - as of 31 December 2005, it had accrued liabilities in an amount equal to 637 million euros. This is one of the biggest financial crises which has ever hit a municipality.
The fact that the municipality went bankrupt was officially declared on 18 October 2006 by the receiver Tommaso Blonda, appointed further to the resignation of the mayor, Rossana Di Bello, following her one-year-and-four-months imprisonment after her conviction for abuse of office and documental forgery in relation to investigations made on the contract for the management of the city incinerator given to the company Termomeccanica.
Transport.
Taranto railway station connects the city with Rome, Naples, Milan, Bologna, Bari, Reggio di Calabria and Brindisi.
Environment.
In 1991 Taranto was declared a high environmental risk area by the Ministry of Environment. As a consequence of the pollutants discharged into the air by the factories in the area, most notably the ILVA steel plant, part of Gruppo Riva, Taranto is the most polluted city in Italy and western Europe. 7% of Taranto's pollution is produced by the public; 93% is produced by factories. In 2005, the European Pollutant Emission Register estimated dioxin emissions from the Taranto ILVA plant were responsible for 83% of Italy's total reported emissions.
Every year the city is exposed to 2.7 tonnes of carbon monoxide and 57.7 tonnes of carbon dioxide. In 2014, the Italian National Institute of Emissions and their Eources, state that Taranto stands third in the world behind China's Linfen, and Copşa Mică in Romania, the most polluted cities in the world due to factories' emissions.
In particular, the city produces minety-two percent of Italy's dioxin. This is 8.8 percent of the dioxin in Europe. Between 1995 and 2004, leukaemias, myelomas and lymphomas increased by 30 to 40 percent. Dioxin accumulates over the years. Over 9 kilos of dioxin have been discharged into the city's air by its factories. 
Grazing is banned within 20 km of the ILVA plant.
In 2013, the Riva family, owner of the ILVA plant, is being tried for ecological disaster. Emissions of both carbon monoxide, carbon dioxide and dioxin have decreased. Animal species have returned that had left, including as swallows, cranes, dolphins, seahorses and the coral reef.
Architecture and landmarks.
Taranto has a number of sites of historic value. Sitting along the "Little Sea", The Aragonese Castle was built in the 15th century with the intention to protect the town from the Turks' frequent raids. The castle replaced a pre-existing fort which was deemed unfit for 15th century warfare.
The old town, including Piazza Fontana, the church of San Domenico, the "Madonna della Salute" Sanctuary, and a number of old "palazzi", is standing exactly as it did a thousand years ago, when the Byzantines rebuilt what the Saracens had razed to the ground in 927 AD.
There are several Greek temple ruins - some from the 6th century BC - such as the remains of a Doric Temple still visible on Piazza Castello.
There are a number of 18th-century "palazzi" in the town centre. For years, they served as the main residence of local aristocratic families. These include Palazzo Carducci-Artenisio (1650), Palazzo Galeota (1728) and Palazzo Latagliata.
The Ponte Girevole (swing bridge), built in 1887, runs across the navigable ship canal that joins "Mar Piccolo" ("Little Sea") with "Mar Grande" ("Big Sea") and stretches along 89.9 m. When the bridge is open, the two ends of the city are disconnected.
The Promenade ("lungomare"), named after former Italian king Victor Emmanuel III, overlooks the "Mar Grande", the natural harbour and commercial port.
Education.
Among the various school are : IIS Archita, IIS Quinto Ennio (in Literature), IIS Aristosseno (Languages), Galileo Ferraris, and ITIS Pacinotti (in IT) and ITC V. Bachelet (in Commercial and Accounting - famous for the activities at BIT MILANO).
Society.
Demographic evolution.
"Census populations"
Dialect.
The city is the centre of the Tarantino dialect ("Dialètte tarandine") of the Sicilian language. As a result of the city's history, it is influenced by Greek, Vulgar Latin and French.
International relations.
Taranto is twinned with:
 Brest, France; since 1964.
Notable people.
These historical figures have had a relationship with the city. Not all of them were actually born in Taranto.

</doc>
<doc id="56150" url="http://en.wikipedia.org/wiki?curid=56150" title="Mainstream">
Mainstream

Mainstream is the common current thought of the perceived majority. It includes all popular culture and media culture, typically disseminated by mass media. The opposite of the mainstream are subcultures, countercultures and cult followings. It is often used as a pejorative term by subcultures who view ostensibly mainstream culture as not only exclusive but artistically and aesthetically inferior. In the United States, mainline churches are sometimes referred to synonymously as "mainstream."
In film.
Mainstream films are films that are distributed to movie theaters which give these films wide releases. However, the definition of a mainstream film can vary by country. For example, a mainstream film from China wouldn't be considered a mainstream film in India. But from a global perspective, mainstream films could be defined as Hollywood films, because it is these films which make up the majority of the most widely distributed films in the world. This makes Hollywood films the worldwide mainstream.
In the media.
Mainstream media, or mass media, is generally applied to print publications, such as newspapers and magazines that contain the highest readership among the public, along with radio formats and television stations that contain the highest viewing and listener audience, respectively. This is in contrast to various independent media, such as alternative media newspapers, specialized magazines in various organizations and corporations, and various electronic sources such as podcasts and blogs (Though certain blogs are more mainstream than others given their association with a mainstream source.
In music.
Mainstream music denotes music that is familiar to the masses, as for example pop music, middle of the road music, dance-pop, pop rap or pop rock.
Opposing mainstream music is the music of subcultures. This exists in virtually all genres of music and is found commonly in punk rock, indie rock, alternative/underground hip hop, anti-folk and heavy metal, among others. In the 1960s this music was exemplified by the music of the hippie counterculture.
In science.
Mainstream science is scientific inquiry in an established field of study that does not depart significantly from orthodox theories. In the philosophy of science, mainstream science is an area of scientific endeavor that has left the process of becoming established. New areas of scientific endeavor still in the process of becoming established are generally labelled protoscience or fringe science. A definition of mainstream in terms of protoscience and fringe science can be understood from the following table:
By its standard practices of applying good scientific methods, mainstream is distinguished from pseudoscience as a demarcation problem and specific types of inquiry are debunked as junk science, cargo cult science, scientific misconduct, etc.
In sociology.
Mainstream pressure, through actions such as peer pressure, can force individuals to conform to the mores of the group (e.g., an obedience to the mandates of the peer group). Some, such as those of modern Hipster culture, have stated that they see mainstream as the antithesis of individuality.
In religion.
Mainstream Christianity is a term used to collectively refer to the common views of major denominations of Christianity (such as Orthodox Christianity, Roman Catholicism, Anglicanism, Protestantism) as opposed the particular tenets of other Christian denominations. The context is dependent on the particular issues addressed, but usually contrasts a orthodox majority view against a heterodox minority view. In the most common sense, "mainstream" refers to Nicene Christianity, or rather the traditions which continue to claim adherence to the Nicene Creed.
Mainstream American Protestant churches (also called "Mainline Protestant") are a group of Protestant churches in the United States that have stressed social justice and personal salvation, and both politically and theologically, tend to be more liberal than non-mainstream Protestants. Mainstream Protestant churches share a common approach that often leads to collaboration in organizations such as the National Council of Churches, and because of their involvement with the ecumenical movement, they are sometimes given the alternative label of "ecumenical Protestantism" (especially outside the United States). While in 1970 the mainstream Protestant churches claimed most Protestants and more than 30 percent of the American population as members, as of 2009[ [update]] they are a minority among American Protestants, claiming approximately 15 percent of American adults.
Education.
Mainstreaming is the practice of bringing disabled students into the “mainstream” of student life. Mainstreamed students attend some classes with typical students and other classes with students that have similar disabilities. Mainstreaming represents a midpoint between full inclusion (all students spend all day in the regular classroom) and dedicated, self-contained classrooms or special schools (disabled students are isolated with other disabled students).
Gender mainstreaming.
The difference of male and female, in the sense that human beings are distinguished as non-conformant.
Etymology.
The term "mainstream" refers to the principle current of a river or stream. Its figurative use by Thomas Carlyle to indicating the prevailing taste or mode is attested at least as early as 1831.

</doc>
<doc id="56152" url="http://en.wikipedia.org/wiki?curid=56152" title="Boris III of Bulgaria">
Boris III of Bulgaria

Boris III, Tsar of Bulgaria (30 January [O.S. 18 January] 1894 – 28 August 1943), originally "Boris Klemens Robert Maria Pius Ludwig Stanislaus Xaver" ("Boris Clement Robert Mary Pius Louis Stanislaus Xavier"), son of Ferdinand I, came to the throne in 1918 upon the abdication of his father, following the defeat of the Kingdom of Bulgaria during World War I. This was the country's second major defeat in only five years, after the disastrous Second Balkan War (1913). Under the Treaty of Neuilly, Bulgaria was forced to cede new territories and pay crippling reparations to its neighbours, thereby threatening political and economic stability. Two political forces, the Agrarian Union and the Communist Party, were calling for the overthrowing of the monarchy and the change of the government. It was in these circumstances that Boris succeeded to the throne. He distinguished himself during the Second World War by opposing attempts by Adolf Hitler to deport the Jewish population of his country.
Biography.
Boris was born on 30 January 1894 in Sofia. He was the first son of Prince Ferdinand of Bulgaria and his wife Princess Marie Louise.
In February 1896 his father paved the way for the reconciliation of Bulgaria and Russia with the conversion of the infant Prince Boris from Roman Catholicism to Eastern Orthodox Christianity, a move that earned Ferdinand the frustration of his wife, the animosity of his Catholic Austrian relatives (particularly that of his uncle, Franz Joseph I of Austria) and excommunication from the Catholic Church. In order to remedy this difficult situation Ferdinand christened all his remaining children as Catholics. Nicholas II of Russia stood as godfather to Boris and met the young boy during Ferdinand's official visit to Saint Petersburg in July 1898.
He received his initial education in the so-called Palace Secondary School which Ferdinand created in 1908 solely for his sons. Later, Boris graduated from the Military School in Sofia, then took part in the Balkan Wars. During the First World War he served as liaison officer of the General Staff of the Bulgarian Army on the Macedonian front. In 1916 he was promoted to colonel and attached again as liaison officer to Army Group Mackensen and the Bulgarian Third Army for the operations against Romania. Boris worked hard to smooth the sometimes difficult relations between Field Marshal Mackensen and the commander of the 3rd army Lieutenant General Stefan Toshev. Through his courage and personal example he earned the respect of the troops and the senior Bulgarian and German commanders, even that of the Generalquartiermeister of the German Army Erich Ludendorff, who preferred dealing personally with Boris and described him as excellently trained, a thoroughly soldierly person and mature beyond his years. In 1918 Boris was made a major general and with the abdication of his father acceded to the throne as Tsar Boris III on 3 October 1918. 
Early reign.
One year after Boris's accession, Aleksandar Stamboliyski (or "Stambolijski") of the Bulgarian People's Agrarian Union was elected prime minister. Though popular with the large peasant class, Stambolijski earned the animosity of the middle class and military, which led to his toppling in a military coup on 9 June 1923, and his subsequent assassination. On 14 April 1925 an anarchist group attacked Boris's cavalcade as it passed through the Arabakonak Pass. Two days later a bomb killed 150 members of the Bulgarian political and military elite in Sofia as they attended the funeral of a murdered general (see St Nedelya Church assault). Following a further attempt on Boris's life the same year military reprisals killed several thousand communists and agrarians, including representatives of the intelligentsia. Finally, in October 1925, there was a short border war with Greece, known as the Incident at Petrich, which was resolved with the help of the League of Nations. 
In the coup on 19 May 1934, the Zveno military organisation established a dictatorship and abolished the political parties in Bulgaria. King Boris was reduced to the status of a puppet king as a result of the coup. The following year, he staged a counter-coup and assumed control of the country by establishing a regime loyal to him. The political process was controlled by the Tsar, but a form of parliamentary rule was re-introduced, without the restoration of the political parties. With the rise of the "King's government" in 1935, Bulgaria entered an era of prosperity and astounding growth, which deservedly qualify it as the Golden Age of the Third Bulgarian Kingdom. It lasted nearly five years.
Boris married Giovanna of Italy, daughter of Victor Emmanuel III of Italy, first in a Catholic ceremony in Assisi, Italy in October 1930 (attended by Benito Mussolini), and then at an Orthodox ceremony in Sofia. The marriage produced a daughter, Maria Louisa, in January 1933, and a son and heir to the throne, Simeon, in 1937.
World War II.
In the early days of World War II, Bulgaria was neutral, but powerful groups in the country swayed its politics towards Germany (with which Bulgaria had also been allied in World War I). As a result of peace treaties that ended World War I – the Treaty of Versailles and the Treaty of Neuilly—Bulgaria, which had fought on the losing side, lost two important territories to neighboring countries: the northern plain of Dobrudja to Romania and Thrace to Greece. The Bulgarians considered these treaties an insult and wanted the lands restored. When Adolf Hitler rose to power, he tried to win Bulgarian King Boris III’s allegiance. In the summer of 1940, after a year of war, Hitler hosted diplomatic talks between Bulgaria and Romania in Vienna. On September 7, an agreement was signed for the return of South Dobrudja to Bulgaria. The Bulgarian nation rejoiced. In March 1941, Boris allied himself with the Axis powers, thus recovering most of Macedonia and Aegean Thrace back to his kingdom, as well as protecting his country from being crushed by the German Wehrmacht like neighboring Yugoslavia and Greece. For recovering these territories Tsar Boris was called the Unifier (Bulgarian: Цар Обединител).
Tsar Boris appeared on the cover of "Time" on 20 January 1941 wearing a full military uniform.
However he was unwilling to send troops to fight the Soviet Union, although in that war the destinies of Bulgaria and Europe were to be decided. He not only did not send regular troops to the Eastern Front, but also refused to allow a legion of volunteers to go, although the German legation in Sofia received 1500 requests from Bulgarian young men who wanted to fight against Bolshevism.
However, in spite of this strong alliance, Boris was not willing to render full and unconditional cooperation with Germany, despite the German presence in Sofia and along the railway line which passed through the Bulgarian capital to Greece.
But there was a price to be paid for the return of Dobrudja. This was the adoption of the anti-Jewish “Law for Protection of the Nation” (Закон за защита на нацията — ЗЗН) on 24 December 1940. This law was in accordance with the Nuremberg Laws in Nazi Germany and the rest of Hitler's occupied Europe. Bulgarian Prime Minister Bogdan Filov and Interior Minister Petur Gabrovski, both Nazi sympathizers, were the architects of this law, which restricted Jewish rights, imposed new taxes, and established a quota for Jews in some professions. Many Bulgarians protested in letters to their government. In March 1941, Bulgaria signed the Tripartite Pact and joined the Axis coalition in hopes of regaining the territories of Macedonia and Thrace. Tsar Boris signed it into law on 21 January 1941..
The Holocaust.
In early 1943, in Bulgaria arrived the emissary of Hitler – Theodor Dannecker, an SS Hauptsturmführer and one of Adolf Eichmann's associates who guided the campaign for the deportation of the French Jews to death camps. In February 1943, Dannecker met with the Commissar for Jewish Affairs in Bulgaria – Alexander Belev, famous for his antisemitic and strong nationalist views. They both held closed-door meetings and ended with a secret agreement signed on 22 February 1943 for the deportations of 20,000 Jews from Aegean Thrace and Vardar Macedonia. These were the territories conquered by Germany and legally not to be under Bulgarian jurisdiction until after the end of the war. The Jewish people in these territories were citizens of Greece and Yugoslavia. Several days later, it became clear that the number of Jews in Aegean Thrace and Vardar Macedonia was 11,343. The "quota" of 20,000 came short. The revised pact called for sending those 11,343 Jews from Thrace and Macedonia and another 8,000 from Bulgaria proper. The remaining Bulgarian Jews were to be deported later.
The initial roundups were to begin on March 9, 1943. In Kyustendil, a town on the western border, the boxcars were lined up. But as the news about the imminent deportations leaked, protests began throughout Bulgaria. In the morning of March 9, a delegation from Kyustendil, composed of eminent public figures and headed by Dimitar Peshev, the deputy speaker of the National Assembly, met with Interior Minister Petur Gabrovski. Facing strong opposition within the country, Gabrovski relented. The same day he sent telegrams to the roundup centers cancelling the deportations.
In a report of 5 April 1943, Adolph Hoffman, a German government adviser and police attache at the German legation in Sofia (1943–44) wrote: "The Minister of Interior has received instruction from the highest place to stop the planned deportation of Jews from the old borders of Bulgaria". In fact, Gabrovski’s decision was not taken on his own “personal initiative,” but had come from the highest authority— King Boris III, who at the risk of direct confrontation with the Reich, refused to deport the Jews. Four hours before the deadline, the order was cancelled. While Jews living in Bulgaria proper were saved, 11,343 Jews from Vardar Macedonia and Thrace were deported to the death camps of Treblinka and Majdanek. The Jewish subjects of these new territories were considered exiles under Hitler's military command and under Hitler's direct jurisdiction. Bulgaria administered these lands, but Nazi Germany did not formally annex them to Bulgaria and their status were to be resolved only after the war.
Still reluctant to comply with the German deportation request, the Royal Palace utilized Swiss diplomatic channels to inquire whether possible deportations of the Jews could happen to British-controlled Palestine by ships rather than to concentration camps in Poland by trains. However, this attempt was blocked by the British Foreign Secretary, Anthony Eden.
Aware of Bulgaria's unreliability on the Jewish matter, the Nazis grew more suspicious about the quiet activities in aid of European Jewry of an old friend of King Boris, Monsignor Angelo Roncalli, then Apostolic delegate in Istanbul and future Pope John XXIII. Reporting on the humanitarian efforts of Roncalli, his secretary in Venice and in the Vatican, Monsignor Loris F. Capovilla writes: "Through his intervention, and with the help of King Boris III of Bulgaria, thousands of Jews from Slovakia, who had first been sent to Hungary and then to Bulgaria, and who were in danger of being sent to Nazi concentration camps, obtained transit visas for Palestine signed by him." 
Meetings with Hitler.
Nazi pressure on King Boris III continued for the deportation of the Bulgarian Jewry. At the end of March, Hitler invited the king to visit him. Upon returning home, King Boris ordered able-bodied Jewishmen to join hard labor units to build roads within the interior of his kingdom. It is widely believed this was the King's attempt to avoid deporting them. In May 1943, Dannecker and the Commissar for Jewish Affairs Belev headed to plan the deportation of 50,000 Bulgarian Jews, to be loaded on steamers on the River Danube. Boris III continued the cat and mouse game that Bulgarian Jews were needed for the construction of roads and railway lines inside his kingdom. Nazi officials requested that Bulgaria deport its Jewish population to German-occupied Poland. The request caused a public outcry, and a campaign whose most prominent leaders were Parliament vice-chairman Dimitar Peshev and the head of the Bulgarian Orthodox Church, Archbishop Stefan, was organized. Following this campaign, Boris III refused to permit the extradition of Bulgaria's 50,000 Jews.
On June 30, 1943, Apostolic Delegate Angelo Roncalli, the future Pope John XXIII, wrote to King Boris III of Bulgaria, asking for mercy for “the sons of the Jewish people.” He wrote that King Boris should on no account agree to that dishonorable action. On the copy of the letter the future Pope John XXIII noted, by hand, that the King replied verbally to his message. The note goes on: "Il Re ha fatto qualche cosa" ("The king has acted") and also noting the difficult situation of the monarch, Mgr. Roncalli stresses once again: "Però, ripeto, ha fatto" (" But I repeat, he has acted").
An excerpt from the diary of Rabbi Daniel Zion, the spiritual leader of the Jewish community in Bulgaria during the war years, reads: "Do not be afraid, dear brothers and sisters! Trust in the Holy Rock of our salvation ... Yesterday I was informed by Bishop Stephen about his conversation with the Bulgarian king. When I went to see Bishop Stephen, he said: "Tell your people, the King has promised, that the Bulgarian Jews shall not leave the borders of Bulgaria ...". When I returned to the synagogue, silence reigned in anticipation of the outcome of my meeting with Bishop Stephen. When I entered, my words were: "Yes, my brethren, God heard our prayers ..." 
Most irritating for Hitler, however, was the Tsar's refusal to declare war on the Soviet Union or send Bulgarian troops to the Eastern front. On 9 August 1943, Hitler summoned Boris to a stormy meeting at Rastenburg, East Prussia, where Tsar Boris arrived by plane from Vrazhdebna on Saturday, 14 August. At Rastenburg the King asserted his stance once again not to send Bulgarian Jews to death camps in Poland and Germany. While Bulgaria had declared a 'symbolic' war on the distant United Kingdom and the United States, at that meeting Boris once again refused to get involved in the war against the Soviet Union, giving two major reasons for his unwillingness to send troops to Russia. First, many ordinary Bulgarians had strong Russian sentiments; and second, the political and military position of Turkey remained unclear. The 'symbolic' war against the Western Allies, however, turned into a disaster for the citizens of Sofia as the city was heavily bombarded by the USAAF and the British Royal Air Force in 1943 and 1944. Nevertheless, the bombardments started only after Boris' death.
Bulgaria’s opposition came to a head at this last official meeting between Hitler and King Boris III in August 1943. Reports of the meeting indicate that Hitler was furious at the King for refusing to join the war against the USSR and to deport the Jews within his kingdom. At the end of the meeting, it was agreed that “the Bulgarian Jews were not to be deported for King Boris had insisted that the Jews were needed for various laboring tasks including road maintenance." This act of bravery displayed by King Boris saved all 50,000 Jews of Bulgaria. Two weeks later on August, 28th 1943, King Boris III died, aged 49.
Death.
Shortly after returning to Sofia from a meeting with Hitler, Boris died of apparent heart failure on 28 August 1943. According to the diary of the German attache in Sofia at the time, Colonel von Schoenebeck, the two German doctors who attended the king – Sajitz and Hans Eppinger – both believed that the king had died from the same poison that Dr. Eppinger had allegedly found two years earlier in the postmortem examination of the Greek prime minister Ioannis Metaxas, a slow poison which takes weeks to do its work, and which causes the appearance of blotches on the skin of its victim before death.
Boris was succeeded by his six-year-old son Simeon II under a Regency Council headed by Boris's brother, Prince Kiril of Bulgaria.
Following a large and impressive state funeral at the Alexander Nevsky Cathedral, Sofia, where the streets were lined with weeping crowds, the coffin of Tsar Boris III was taken by train to the mountains and buried in Bulgaria's largest and most important monastery, the Rila Monastery. After taking power in September 1944, the Communist-dominated government had his body exhumed and secretly buried in the courtyard of the Vrana Palace near Sofia. At a later time the Communist authorities removed the zinc coffin from Vrana and moved it to a secret location, which remains unknown to this day. After the fall of communism, an excavation attempt was made at the Vrana Palace, in which only Boris's heart was found, as it had been put in a glass cylinder outside the coffin. The heart was taken by his widow in 1993 to Rila Monastery where it was reinterred.
A wood-carving is placed on the left side of his grave in the Rila monastery, made on 10 October 1943 by inhabitants of the village of Osoi, Debar district. The wood-carving has the following inscription:
Titles, styles, honours, patronages and awards.
The United States Congress proclaimed King Boris III savior of fifty thousand Bulgarian Jews on 12 May 1994.
King Boris III was posthumously awarded the Jewish National Fund's Medal of the Legion of Honor Award, the first non-Jew to receive one of the Jewish community's highest honors.
The Anti-Defamation League and Chabad have also honored King Boris III for refusing to sacrifice his Jewish subjects to the Nazi juggernaut.
Tsar Boris III Boulevard is one of the main boulevards in Sofia , Varna and Plovdiv.
Borisova gradina is the largest park in Sofia.
In 1998, to thank Tsar Boris, Bulgarian Jews in the United States and the Jewish National Fund erected a monument in the “The Bulgarian Forest” in Israel, honoring Tsar Boris as a savior of Bulgarian Jews. In July, 2003, a public committee headed by Israeli Chief Justice Dr. Moshe Beiski decided to remove the memorial from the “The Bulgarian Forest," because Bulgaria had consented to the delivery of the Jews from occupied territory of Macedonia and Thrace to the Germans..
External links.
social solidarity

</doc>
<doc id="56153" url="http://en.wikipedia.org/wiki?curid=56153" title="Ophthalmology">
Ophthalmology

Ophthalmology is the branch of medicine that deals with the anatomy, physiology and diseases of the eye. An ophthalmologist is a specialist in medical and surgical eye problems. Since ophthalmologists perform operations on eyes, they are both surgical and medical specialists. A multitude of diseases and conditions can be diagnosed from the eye.
Etymology.
The word ophthalmology comes from the Greek roots ὀφθαλμός, "ophthalmos", i.e., "eye" and -λoγία, -"logia", i.e., "study of, discourse"; ophthalmology literally means "the science of eyes". As a discipline, it applies to animal eyes also, since the differences from human practice are surprisingly minor and are related mainly to differences in anatomy or prevalence, not differences in disease processes.
History.
Ancient India.
The Indian surgeon Sushruta wrote "Sushruta Samhita" in Sanskrit in about 800 which describes 76 ocular diseases (of these 51 surgical) as well as several ophthalmological surgical instruments and techniques. His description of cataract surgery was more akin to extracapsular lens extraction than to couching. He has been described as the first cataract surgeon.
Before Hippocrates.
The pre-Hippocratics largely based their anatomical conceptions of the eye on speculation, rather than empiricism. They recognized the sclera and transparent cornea running flushly as the outer coating of the eye, with an inner layer with pupil, and a fluid at the centre. It was believed, by Alcamaeon and others, that this fluid was the medium of vision and flowed from the eye to the brain by a tube. Aristotle advanced such ideas with empiricism. He dissected the eyes of animals, and discovering three layers (not two), found that the fluid was of a constant consistency with the lens forming (or congealing) after death, and the surrounding layers were seen to be juxtaposed. He and his contemporaries further put forth the existence of three tubes leading from the eye, not one. One tube from each eye met within the skull.
Rufus.
Rufus of Ephesus recognised a more modern eye, with conjunctiva, extending as a fourth epithelial layer over the eye. Rufus was the first to recognise a two-chambered eye, with one chamber from cornea to lens (filled with water), the other from lens to retina (filled with an egg white-like substance). The Greek physician Galen remedied some mistakes including the curvature of the cornea and lens, the nature of the optic nerve, and the existence of a posterior chamber.
Though this model was a roughly correct modern model of the eye, it contained errors. Still, it was not advanced upon again until after Vesalius. A ciliary body was then discovered and the sclera, retina, choroid, and cornea were seen to meet at the same point. The two chambers were seen to hold the same fluid, as well as the lens being attached to the choroid. Galen continued the notion of a central canal, but he dissected the optic nerve and saw that it was solid. He mistakenly counted seven optical muscles, one too many. He also knew of the tear ducts.
Middle Eastern ophthalmology.
Medieval Islamic Arabic and Persian scientists (unlike their classical predecessors) considered it normal to combine theory and practice, including the crafting of precise instruments, and therefore found it natural to combine the study of the eye with the practical application of that knowledge.
Ibn al-Haytham (Alhazen), an Arab scientist with Islamic beliefs, wrote extensively on optics and the anatomy of the eye in his "Book of Optics" (1021).
Ibn al-Nafis, an Arabic native of Damascus, wrote a large textbook, "The Polished Book on Experimental Ophthalmology", divided into two parts, "On the Theory of Ophthalmology" and "Simple and Compounded Ophthalmic Drugs".
17th and 18th centuries.
In the 17th and 18th centuries, hand lenses were used by Malpighi, and microscopes by van Leeuwenhoek, preparations for fixing the eye for study by Ruysch, and later the freezing of the eye by Petit. This allowed for detailed study of the eye and an advanced model. Some mistakes persisted, such as: why the pupil changed size (seen to be vessels of the iris filling with blood), the existence of the posterior chamber, and of course the nature of the retina. In 1722, van Leeuwenhoek noted the existence of rods and cones, though they were not properly discovered until Gottfried Reinhold Treviranus in 1834 by use of a microscope.
Georg Joseph Beer (1763–1821) was an Austrian ophthalmologist and leader of the First Viennese School of Medicine. He introduced a flap operation for treatment of cataracts (Beer's operation), as well as popularizing the instrument used to perform the surgery (Beer's knife).
Ophthalmic surgery in Great Britain.
The first ophthalmic surgeon in Great Britain was John Freke, appointed to the position by the Governors of St Bartholomew's Hospital in 1727. A major breakthrough came with the appointment of Baron Michael Johann Baptist de Wenzel (1724–90), a German who became oculist to King George III of England in 1772. His skill at removing cataracts legitimized the field. The first dedicated ophthalmic hospital opened in 1805 in London; it is now called Moorfields Eye Hospital. Clinical developments at Moorfields and the founding of the Institute of Ophthalmology (now part of the University College London) by Sir Stewart Duke Elder established the site as the largest eye hospital in the world and a nexus for ophthalmic research.
20th century.
The prominent opticians of the late 19th and early 20th centuries included Ernst Abbe (1840–1905), a co-owner of at the Zeiss Jena factories in Germany where he developed numerous optical instruments. Hermann von Helmholtz (1821-1894) was a polymath who made contributions to many fields of science and invented the ophthalmoscope in 1851. They both made theoretical calculations on image formation in optical systems and had also studied the optics of the eye.
Central Europe.
Numerous ophthalmologists fled Germany after 1933 as the Nazis began to persecute those of Jewish descent. A representative leader was Joseph Igersheimer (1879–1965), best known for his discoveries with arsphenamine for the treatment of syphilis. He fled to Turkey in 1933. As one of eight emigrant directors in the Faculty of Medicine at the University of Istanbul, he built a modern clinic and trained students. In 1939, he went to the United States, becoming a professor at Tufts University.
Polish ophthalmology dates to the 13th century. The Polish Ophthalmological Society was founded in 1911. A representative leader was Adam Zamenhof (1888–1940), who introduced certain diagnostic, surgical, and nonsurgical eye-care procedures and was shot by the Nazis in 1940. Zofia Falkowska (1915–93) head of the Faculty and Clinic of Ophthalmology in Warsaw from 1963 to 1976, was the first to use lasers in her practice.
Professional requirements.
Ophthalmologists are physicians (MD/MBBS or D.O., not OD or BOptom) who have completed a college degree, medical school, and residency in ophthalmology. Ophthalmology training equips eye specialists to provide the full spectrum of eye care, including the prescription of glasses and contact lenses, medical treatment, and complex microsurgery. In many countries, ophthalmologists also undergo additional specialized training in one of the many subspecialties. Ophthalmology was the first branch of medicine to offer board certification, now a standard practice among all specialties.
Australia and New Zealand.
In Australia and New Zealand, the FRACO/FRANZCO is the equivalent postgraduate specialist qualification. It is a very competitive speciality to enter training and has a closely monitored and structured training system in place over the five years of postgraduate training. Overseas-trained ophthalmologists are assessed using the pathway published on the RANZCO website. Those who have completed their formal training in the UK and have the CCST/CCT are usually deemed to be comparable.
Bangladesh.
In Bangladesh to be an ophthalmologist the basic degree is an MBBS. Then they have to obtain a postgraduate degree or diploma in specialty ophthalmology. In Bangladesh, these are Diploma in Ophthalmology, Diploma in Community Ophthalmology, Fellow or Member of the College of Physicians and Surgeons in ophthalmology, and Master of Science in ophthalmology.
Canada.
In Canada, an ophthalmology residency after medical school is undertaken. The residency lasts a minimum of five years after the MD degree which culminates in fellowship of the Royal College of Surgeons of Canada (FRCSC). Subspecialty training is undertaken by about 30% of fellows (FRCSC) in a variety of fields from anterior segment, cornea, glaucoma, visual rehabilitation, uveitis, oculoplastics, medical and surgical retina, ocular oncology, ocular pathology. or neuro-ophthalmology. About 35 vacancies open per year for ophthalmology residency training in all of Canada. These numbers fluctuate per year, ranging from 30 to 37 spots. Of these, up to seven spots are often dedicated to French-speaking universities in Quebec, while the rest of the English-speaking spots are competed for by hundreds of applicants each year. At the end of the five years, the graduating ophthalmologist must pass the oral and written portions of the Royal College exam.
Finland.
In Finland, physicians willing to become ophthalmologists must undergo a five-year specialization which includes practical training and theoretical studies.
India.
In India, after completing MBBS degree, postgraduate study in ophthalmology is required. The degrees are Doctor of Medicine, Master of Surgery, Diploma in Ophthalmic Medicine and Surgery, and Diplomate of National Board. The concurrent training and work experience is in the form of a junior residency at a medical college, eye hospital, or institution under the supervision of experienced faculty. Further work experience in form of fellowship, registrar, or senior resident refines the skills of these eye surgeons. All India Ophthalmological Society and various state-level ophthalmological societies hold regular conferences and actively promote continuing medical education.
Nepal.
In Nepal, to become an ophthalmologist, three years postgraduate study is required after completing MBBS degree. The postgraduate degree in ophthalmology is called MD in Ophthalmology. This degree is currently provided by BPKLCO, Institute of Medicine, TU, Kathmandu, BP Koirala Institute of Health Sciences, Dharan, Kathmandu University, Dhulikhel and National Academy of Medical Science, Kathmandu. Few Nepalese citizen also study this subject in Bangladesh, China, India, Pakistan and other countries. All the graduates have to pass Nepal Medical Council Licensing Exam to become a registered Ophthalmology in Nepal. The concurrent residency training is in the form of a PG student (resident) at a medical college, eye hospital, or institution according to the degree providing university's rules and regulations. Nepal Ophthalmic Society hold regular conferences and actively promote continuing medical education.
Ireland.
In Ireland, the Royal College of Surgeons of Ireland grants Membership (MRCSI (Ophth)) and Fellowship (FRCSI (Ophth)) qualifications in conjunction with the Irish College of Ophthalmologists. Total postgraduate training involves an intern year, a minimum of three years of basic surgical training and a further 4.5 years of higher surgical training. Clinical training takes place within public, Health Service Executive-funded hospitals in Dublin, Sligo, Limerick, Galway, Waterford, and Cork. A minimum of 8.5 years of training is required before eligibility to work in consultant posts. Some trainees take extra time to obtain MSc, MD or PhD degrees and to undertake clinical fellowships in the UK, Australia and the United States.
Pakistan.
In Pakistan, after MBBS, a four-year full-time residency program leads to an exit-level FCPS examination in ophthalmology, held under the auspices of the College of Physicians and Surgeons, Pakistan. The tough examination is assessed by both highly qualified Pakistani and eminent international ophthalmic consultants. As a prerequisite to the final examinations, an intermediate module, an optics and refraction module, and a dissertation written on a research project carried out under supervision is also assessed.
Moreover, a two-and-a-half-year residency program leads to an MCPS while a two-year training of DOMS is also being offered. For candidates in the military, a stringent two-year graded course, with quarterly assessments, is held under Armed Forces Post Graduate Medical Institute in Rawalpindi.
The M.S. in ophthalmology is also one of the specialty programs. In addition to programs for doctors, various diplomas and degrees for allied eyecare personnel are also being offered to produce competent optometrists, orthoptists, ophthalmic nurses, ophthalmic technologists, and ophthalmic technicians in this field. These programs are being offered notably by the College of Ophthalmology and Allied Vision Sciences in Lahore and the Pakistan Institute of Community Ophthalmology in Peshawar. Subspecialty fellowships are also being offered in the fields of pediatric ophthalmology and vitreoretinal ophthalmology. King Edward Medical University, Al Shifa Trust Eye Hospital Rawalpindi, and Al- Ibrahim Eye Hospital Karachi have also started a degree program in this field.
Philippines.
Ophthalmology is a considered a medical specialty that uses medicine and surgery to treat diseases of the eye. To become a general ophthalmologist, a candidate must have completed a Doctor of Medicine degree or its equivalent (e.g. MBBS), have passed the physician licensure exam, completed an internship in medicine, and completed residency at any Philippine Academy of Ophthalmology-accredited program. Attainment of board certification in ophthalmology from PBO is optional, but is preferred and required to gain privileges in most major health institutions. Graduates of residency programs can receive further training in subspecialties of ophthalmology such as neuro-ophthalmology, etc. by completing a fellowship program which varies in length depending on each program's requirements. The leading professional organization in the country is the Philippine Academy of Ophthalmology which also regulates ophthalmology residency programs and board certification through its accrediting agency, the Philippine Board of Ophthalmology.
United Kingdom.
In the United Kingdom, three colleges grant postgraduate degrees in ophthalmology. The Royal College of Ophthalmologists (RCOphth) and the Royal College of Surgeons of Edinburgh grant MRCOphth/FRCOphth and MRCSEd/FRCSEd, (although membership is no longer a prequisite for fellowship), the Royal College of Glasgow grants FRCS. Postgraduate work as a specialist registrar and one of these degrees is required for specialization in eye diseases. Such clinical work is within the NHS, with supplementary private work for some consultants.
Only 2.3 ophthalmologists exist per 100,000 population in the UK – fewer "pro rata" than in any other nation in the European Union.
United States.
In the United States, four years of residency training after medical school are required, with the first year being an internship in surgery, internal medicine, pediatrics, or a general transition year. Optional fellowships in advanced topics may be pursued for several years after residency. Most currently practicing ophthalmologists train in medical residency programs accredited by the Accreditation Council for Graduate Medical Education or the American Osteopathic Association and are board-certified by the American Board of Ophthalmology or the American Osteopathic Board of Ophthalmology and Otolaryngology. United States physicians who train in osteopathic medical schools hold the Doctor of Osteopathic Medicine (DO) degree rather than an MD degree. The same residency and certification requirements for ophthalmology training must be fulfilled by osteopathic physicians.
Physicians must complete the requirements of continuing medical education to maintain licensure and for recertification. Professional bodies like the American Academy of Ophthalmology and American Society of Cataract and Refractive Surgery organize conferences, help physician members through continuing medical education programs for maintaining board certification, and provide political advocacy and peer support.
Subspecialties.
Ophthalmology includes subspecialities which deal either with certain diseases or diseases of certain parts of the eye. Some of them are:

</doc>
<doc id="56154" url="http://en.wikipedia.org/wiki?curid=56154" title="Odd molecule">
Odd molecule

Odd molecule is a term invented by Gilbert N. Lewis in 1916 for a molecule containing an odd number of electrons.
Taking the "p-shell" elements, such molecules are rare; they are usually colored and paramagnetic, that is, attracted by a magnet.
Odd molecules are 'radicals.'
A fine example is nitric oxide, q.v.; nitrogen dioxide is another; chlorine dioxide is also an example, being a reddish-yellow gas. They are all fairly reactive.
When including "d-shell" elements, i.e., the transition metals, the concept mostly doesn't apply, and this 'odd' state is not so unusual.

</doc>
<doc id="56162" url="http://en.wikipedia.org/wiki?curid=56162" title="List of architects">
List of architects

The following is a list of notable architects – well-known individuals with a large body of published work or notable structures.
Mythological/fictional architects.
Several architects occur in worldwide mythology, including Daedalus, builder of the Labyrinth, in Greek myth. In the Bible, Nimrod is considered the creator of the Tower of Babel, and King Solomon built Solomon's Temple with the assistance of the architect Hiram. In Hinduism, the palaces of the gods were built by the architect and artisan Vivasvat. Moreover, Indian epic Mahabharata cites amazing work by architect 'Maya.'
Architects also occur in modern fiction. Examples include Howard Roark, protagonist in Ayn Rand's "The Fountainhead"; Bloody Stupid Johnson, a parody of Capability Brown who appears in Terry Pratchett's "Discworld" novels; and Slartibartfast, designer of planets in Douglas Adams's "The Hitchhiker's Guide to the Galaxy".
Several films have included central characters who are architects, including Henry Fonda's character "Juror 8" (Davis) in "12 Angry Men" (1957), Tom Hanks's character in "Sleepless in Seattle" (1993), David Strathairn's character in "The River Wild" (1994), Michael J. Fox's character in "The Frighteners" (1996), John Cassavetes's character in "Tempest" (1982) and Michael Keaton's character in "White Noise" (2005).
In television, Mike Brady, father of "The Brady Bunch", is an architect; as is Wilbur Post, owner of "Mister Ed"; and Ted Mosby, from "How I Met Your Mother". The character George Costanza pretends to be an architect named "Art Vandelay" in "Seinfeld". Architect Halvard Solness is the protagonist of Henrick Ibsen's 1892 play "The Master Builder".

</doc>
<doc id="56163" url="http://en.wikipedia.org/wiki?curid=56163" title="Starship">
Starship

A starship, starcraft or interstellar spacecraft is a theoretical spacecraft designed for traveling between stars, as opposed to a vehicle designed for orbital spaceflight or interplanetary travel.
The term is mostly found in science fiction, because such craft have never been constructed. Whilst the Voyager and Pioneer probes have traveled into local interstellar space, they are not starships because they have not traveled to other stars, they are unmanned and their purpose was specifically interplanetary. Exploratory engineering has been undertaken on several preliminary designs and feasibility studies have been done for starships that could be built with modern technology or technology thought likely to be available in the near future.
Research.
To travel between stars in a reasonable time using rocket-like technology requires very high effective exhaust velocity exhaust jet, and enormous energy to power this, such as might be provided by fusion power or antimatter.
There are very few scientific studies that investigate the issues in building a starship. Some examples of this include:
The Bussard ramjet is an idea to use nuclear fusion of interstellar gas to provide propulsion.
Examined in an October 1973 issue of "Analog", the Enzmann Starship proposed using a 12,000 ton ball of frozen deuterium to power thermonuclear powered pulse propulsion units. Twice as long as the Empire State Building and assembled in-orbit, the proposed spacecraft would be part of a larger project preceded by interstellar probes and telescopic observation of target star systems.
The NASA Breakthrough Propulsion Physics Program (1996–2002), was a professional scientific study examining advanced spacecraft propulsion systems.
Theoretical types.
A common literary device is to posit a faster-than-light propulsion system (such as warp drive) or travel through hyperspace, although some starships may be outfitted for centuries-long journeys of slower-than-light travel. Other designs posit a way to boost the ship to near-lightspeed, allowing relatively "quick" travel (i.e. decades, not centuries) to nearer stars. This results in a general categorization of the kinds of starships:
Fictional elements.
Certain common elements are found in most fiction that discusses starships.
Slower-than-light.
Fiction that discusses slower-than-light starships is relatively rare, since the time scales are so long. Instead of describing the interaction with the outside world, those fictions tend to focus on setting the whole story within the world of the (often very large) starship during its long travels. Sometimes the starship "is" a world, in perception or reality.
Faster-than-light.
Travel at velocities greater than the speed of light is impossible according to the known laws of physics, although apparent FTL is not excluded by general relativity. The alcubierre drive provides a theoretical way of achieving FTL, although it requires negative mass, which has not yet been discovered. Nevertheless Harold G. White (NASA) at NASA had design the White–Juday warp-field interferometer to detect a microscopic instance of a warping of space-time according to the alcubierre drive. An example of a faster-than-light ship is the "Enterprise" from "".
Fictional examples.
The following is a listing of some of the most widely known vessels in various science fiction franchises:
Individual ships.
This list is not exhaustive.

</doc>
<doc id="56166" url="http://en.wikipedia.org/wiki?curid=56166" title="Languages in Star Wars">
Languages in Star Wars

The fictional universe of Star Wars contains many languages. The languages have a role in the story lines. Because of the various languages characters speak in Star Wars they often cannot understand each other. The character C-3PO is a translator fluent in over six million forms of communication who acts as a go-between for other characters in the stories.
Basic - the common galactic language.
The spoken language most often heard, a lingua franca, in the "Star Wars" films and stories is "Galactic Basic" (shortened to Basic), although this name itself is never explicitly mentioned in the films. Basic is heard or printed in the vernacular of the audience (English in English versions, Spanish in Spanish translations, etc.) and most often written in "Aurebesh", an alphabet which has letters corresponding to each of the 26 letters of the basic Latin alphabet, as well as letters representing Latin digraphs (letter combinations) such as "th", "sh", "ng", etc.
Other Languages.
Bocce.
Bocce is a language of trade within the Star Wars Universe. Developed by a merchant fleet, it combines multiple languages to provide trade communication between different species.
Huttese.
Another lingua franca in the Star Wars Universe that is spoken by many groups and species is Huttese, spoken on Nal Hutta, Nar Shaddaa, Tatooine and other worlds in and around Hutt space. It is spoken in the films by both non-humans (Jabba the Hutt, Watto, Sebulba and others) and humans. In fact, the whole Max Rebo Band communicates and sings in Huttese. Its phonology is said to be based on the Quechua language.
Tribal tongue of the Ewoks.
The Ewoks of the forest moon of Endor speak a "primitive dialect" of one of the more than six million other forms of communication that C-3PO is familiar with. Ben Burtt, "Return of the Jedi"’s sound designer, created the Ewok language. Burtt has told rather differing stories about how he developed the language. In "Bantha Tracks" #17 August 1982, he states
Several years later, on the commentary track for the DVD of "Return of the Jedi", Burtt identified the language that he heard in the BBC documentary as Kalmyk, a tongue spoken by the isolated nomadic Kalmyk people. He describes how, after some research, he identified an 80-year old Kalmyk refugee. He recorded her telling folk stories in her native language, and then used the recordings as a basis for sounds that became the Ewok language and were performed by voice actors who imitated the old woman's voice in different styles. For the scene in which C-3PO speaks Ewokese, actor Anthony Daniels worked with Burtt and invented words, based on the Kalmyk recordings.
Marcia Calkovsky of Lethbridge University maintains that Tibetan language contributed to Ewok speech "along" with Kalmyk, starting the story from attempts to use language samples of Native Americans and later turning to 9 Tibetan women living in San Francisco area, as well as one Kalmyk woman. The story of the choice of these languages is referenced to Burtt's 1989 telephone interview, and many of the used Tibetan phrases translated. The initial prayer Ewoks address to C-3PO is actually the beginning part of Tibetan Buddhist prayer for the benefit of all sentient beings, or so called four immeasurables, but also there is a second (out of four) part of refuge prayer. Tibetan diaspora was puzzled as many of the phrases they could make out did not corellate to events on screen.
Wookiee language of Shyriiwook.
"Shyriiwook" (also called Wookiee Speak, or contracted as "Wookieespeak" in the Expanded Universe, and video games) is the native language of the Wookiee race. The language consists of roars and growls. Although it can be understood by members of other species, it is extremely difficult for those with non-Wookiee physiology to speak. Conversely, Wookiee mouthparts physically cannot create the sounds of Galactic Basic, thus while Wookiees such as Chewbacca can understand characters speaking Basic, he cannot speak it. In Timothy Zahn's "Heir to the Empire", Leia Organa Solo encounters a Wookiee with a speech impediment which conveniently renders his Shyriiwook pronunciation much easier to understand by Leia. Another Wookiee language, Xaczik, is indigenous to Wartaki Island on Kashyyyk and several outlying coastal regions.
Ithorians.
Ithorians have two mouths, one on each side of their head. As a result, their native language is extremely complicated and essentially impossible for non-Ithorians to speak. Despite the stereophonic quality of their voices, Ithorians are able to speak Basic, and be understood by others, with ease.
Geonosians.
Geonosians, insectoid species seen in "" whose language includes click consonants.
Tusken Raiders.
The Tusken Raiders of Tatooine, according to the video game "Knights of the Old Republic", speak a language of their own; it is, however, difficult for non-Tuskens to understand this language. In the game, a droid named HK-47 assists the player in communicating with the Tusken Raiders. In the novelizations Junior Jedi Knights and New Jedi Order series, it is revealed that Jedi Knight Tahiri Veila was raised by the Tusken Raiders after they captured her in a raid. Generally, they utter roars and battle cries when seen in public.
Jawaese and Jawa Trade Language.
The Jawas, also found on Tatooine, speak in a high-pitched, squeaky voice. To speak to others of their species along with the voice they emit a smell showing their emotions. However when trading droids and dealing with non-Jawas they speak without the smell because many consider the smell "Foul."
Ryl language of the Twi'leks.
Twi'leks speak their own language, Ryl, which incorporates spoken words and a form of sign language, using subtle manipulations of the tips of their lekku (head tails).
Rodian language.
Rodians have their own language called Rodese.
Hapan language.
Hapan was the language developed by residents of the Hapes Cluster due to their isolation from the rest of the galaxy. Due to their limited contact with residents outside the Cluster, Basic was not commonly known to the average Hapan.
Droids and computers.
Droids (robots) and computers in Star Wars use either the natural languages or machine languages. C-3PO is "fluent in over six million forms of communication" and protocol droids are often employed as translators. Astromech droids such as R2-D2 communicate through an information-dense language of beeps and whistles known as Binary; devices exist that can translate this language into Basic. A few non-droids can also learn to understand it through working with the droids for long periods of time, and protocol droids are able to translate Binary into other languages.
Writing.
Hindu-Arabic numerals appear throughout the films, mainly on computer displays counting down time or distance. At least one instance of the Latin alphabet crops up in the original version of "" ("POWER – TRACTOR BEAM 12 (SEC. N6)"). Text in the other films is either illegible, offscreen, or in fictional scripts. For the 2004 DVD release, this writing was changed to the "Aurebesh" alphabet. In the novel "The Truce at Bakura", the Ssi-ruuk speak some sort of tonal language which involves whistles. A human prisoner devises an orthography for this language.
Language building.
The languages of some fictional worlds have been worked out in great detail, with grammatical rules and large vocabularies, such as J. R. R. Tolkien's Elvish languages, and the Klingon language of "Star Trek". The fictional languages of "Star Wars", in contrast, are not systematically worked out. The Wookiee growls and the beeps of the astromechs mainly carry emotional indicators for the audience via intonation. The language most often heard in the films, Galactic Basic, is itself identical to modern English (or whatever language the film is shown in), with only a few changed idioms and additions of words related to the Star Wars setting. Mando'a, the language of the Mandalorians, is being developed into a working language by "Star Wars" author Karen Traviss.
Other languages heard are also human languages, albeit ones likely unfamiliar to most of the audience. In "A New Hope", for instance, the language spoken by the character Greedo in conversation with Han Solo (in the cantina) is actually a simplified version of Quechua, an indigenous language of the Andean region of South America. In "Return of the Jedi", the voice of Lando Calrissian's copilot, Nien Nunb, was provided by a student speaking Haya, a language from the young man's native Tanzania ("Star Wars Insider" #67, 31). In ', Oola, Jabba's (a name apparently inspired by a Russian word (жаба (zhaba/jaba)) meaning "toad") twi'lek dancer, can also be heard speaking in French language saying ""Non, ne me tuez pas!" (meaning: "No, don't kill me!"") as Jabba is about to kill her. One can also hear some Finnish in '. Similarly, "Teräs Käsi", the name of a martial art in the Expanded Universe, comes from Finnish and translates as "steel hand."
The "Star Wars: Galactic Phrase Book & Travel Guide" summarizes book and movie information pertaining to Huttese, Bocce, Ewok, Shyriiwook, droid, Jawa, and Gungan.

</doc>
<doc id="56167" url="http://en.wikipedia.org/wiki?curid=56167" title="Orleans County, New York">
Orleans County, New York

Orleans County is a county located in the U.S. state of New York. As of the 2010 census, the population was 42,883. The county seat is Albion. The name is in honor of the French Royal House of Orleans.
Orleans County is part of the Rochester, NY Metropolitan Statistical Area.
History.
When counties were established in New York State in 1683, the present Orleans County was part of Albany County. When the county was formed, a dispute arose about naming it after Andrew Jackson or John Adams; the conflict was ended by choosing the name Orleans. This was an enormous county, including the northern part of New York State as well as all of the present State of Vermont and, in theory, extending westward to the Pacific Ocean. This county was reduced in size on July 3, 1766 by the creation of Cumberland County, and further on March 16, 1770 by the creation of Gloucester County, both containing territory now in Vermont.
On March 12, 1772, what was left of Albany County was split into three parts, one remaining under the name Albany County. One of the other pieces, Tryon County, contained the western portion (and thus, since no western boundary was specified, theoretically still extended west to the Pacific). The eastern boundary of Tryon County was approximately five miles west of the present city of Schenectady, and the county included the western part of the Adirondack Mountains and the area west of the West Branch of the Delaware River. The area then designated as Tryon County now includes 37 counties of New York State. The county was named for William Tryon, colonial governor of New York.
In the years prior to 1776, most of the Loyalists in Tryon County fled to Canada. In 1784, following the peace treaty that ended the American Revolutionary War, the name of Tryon County was changed to Montgomery County in order to honor the general, Richard Montgomery, who had captured several places in Canada and died attempting to capture the city of Quebec, replacing the name of the hated British governor.
In 1789, Ontario County was split off from Montgomery.
In 1802, Genesee County was created by a splitting of Ontario County . This was much larger than the present Genesee County, however, containing the present Allegany, Cattaraugus, Chautauqua, Erie, Niagara, Orleans, and Wyoming Counties, and parts of Livingston and Monroe Counties.
In 1806, Genesee County was reduced in size by the splitting off of Allegany County. In 1808, Genesee County was further reduced in size by the splitting off of Cattaraugus, Chautauqua, and Niagara Counties. Niagara County at that time also included the present Erie County.
In 1821, Genesee County was reduced in size by the splitting off of portions which were combined with portions of Ontario County to create Livingston and Monroe Counties. By this time Genesee County had been reduced considerably in size from its original area of 1802, still containing the present Orleans and Wyoming Counties in addition to its present area, however.
In 1824, Orleans County was created from what was left of Genesee County.
Geography.
According to the U.S. Census Bureau, the county has a total area of 817 sqmi, of which 391 sqmi is land and 426 sqmi (52%) is water.
The high proportion of water is due to the extension of Orleans County north into Lake Ontario to the Canadian border (a line of latitude running through the middle of the lake). The distance from the Orleans shore north to the international border is greater than the distance from the shore south to the Genesee County line, meaning the area of Orleans underwater is actually greater than that above water.
Orleans County is in western New York State, northeast of Buffalo and west of Rochester, on the southern shore of Lake Ontario.
The Erie Canal passes (east-west) through the middle of the county.
Government and politics.
Orleans County is governed by a seven–member legislature headed by a chairman.
State and federal government.
Orleans County is part of:
Demographics.
As of the census of 2000, there were 44,171 people, 15,363 households, and 10,846 families residing in the county. The population density was 113 people per square mile (44/km²). There were 17,347 housing units at an average density of 44 per square mile (17/km²). The racial makeup of the county was 89.12% White, 7.31% Black or African American, 0.46% Native American, 0.32% Asian, 0.03% Pacific Islander, 1.54% from other races, and 1.21% from two or more races. 3.89% of the population were Hispanic or Latino of any race. 20.3% were of German, 18.3% English, 10.8% Italian, 10.3% Irish, 9.4% American and 7.3% Polish ancestry according to Census 2000. 96.0% spoke English and 3.0% Spanish as their first language.
There were 15,363 households out of which 35.0% had children under the age of 18 living with them, 54.3% were married couples living together, 11.2% had a female householder with no husband present, and 29.4% were non-families. 23.7% of all households were made up of individuals and 10.7% had someone living alone who was 65 years of age or older. The average household size was 2.65 and the average family size was 3.13.
In the county the population was spread out with 26.2% under the age of 18, 8.2% from 18 to 24, 31.3% from 25 to 44, 21.9% from 45 to 64, and 12.40% who were 65 years of age or older. The median age was 36 years. For every 100 females there were 98.3 males. For every 100 females age 18 and over, there were 95.2 males.
The median income for a household in the county was $37,972, and the median income for a family was $42,830. Males had a median income of $32,450 versus $22,605 for females. The per capita income for the county was $16,457. About 7.7% of families and 10.8% of the population were below the poverty line, including 15.0% of those under age 18 and 5.2% of those age 65 or over.
Education.
The county is considered to have five school districts, although the actual district boundaries can extend into neighboring counties, and the same is true for neighboring counties' districts. The five districts, from west to east, are:
The only post-secondary education available in the county are two branches of Genesee Community College located in Albion and Medina.
External links.
 

</doc>
<doc id="56168" url="http://en.wikipedia.org/wiki?curid=56168" title="Orange County, New York">
Orange County, New York

Orange County is a county located in the U.S. state of New York. As of the 2010 census, the population was 372,813. The county seat is Goshen. This county was first created in 1683 and reorganized with its present boundaries in 1798.
Orange County is included in the New York-Newark-Jersey City, NY-NJ-PA Metropolitan Statistical Area. It is in the state's Mid-Hudson Region of the Hudson Valley.
The County Executive is Steve Neuhaus.
History.
Orange County was officially established on November 1, 1683, when the Province of New York was divided into twelve counties. Each of these was named to honor member of the British royal family, and Orange County took its name from the Prince of Orange, who subsequently became King William III of England. As originally defined, Orange County included only the southern part of its present-day territory, plus all of present-day Rockland County further south. The northern part of the present-day county, beyond Moodna Creek, was then a part of neighbouring Ulster County.
At that date, the only European inhabitants of the area were a handful of Dutch colonists in present-day Rockland County, and the area of modern Orange County was entirely occupied by the native Munsee people. Due to its relatively small population, the original Orange County was not fully independent and was administered by New York County.
The first European settlers in the area of the present-day county arrived in 1685. They were a party of around twenty-five families from Scotland, led by David Toshach, the Laird of Monzievaird, and his brother-in-law Major Patrick McGregor, a former officer of the French Army. They settled in the Hudson Highlands at the place where the Moodna Creek enters the Hudson River, now known as New Windsor. In 1709, a group of German Palatine refugees settled at Newburgh. They were Protestants from a part of Germany along the Rhine that had suffered during the religious wars. Queen Anne's government arranged for passage from England of nearly 3,000 Palatines in ten ships. Many were settled along the Hudson River in work camps on property belonging to Robert Livingston. A group of Dutch and English settlers arrived at Goshen in 1712. Additional immigrants came from Ireland; they were of Scots and English descent who had been settled as planters there. 
In 1798, after the American Revolutionary War, the boundaries of Orange County changed. Its southern corner was used to create the new Rockland County, and in exchange, an area to the north of the Moodna Creek was added, which had previously been in Ulster County. This caused a reorganization of the local administration, as the original county seat had been fixed at Orangetown in 1703, but this was now in Rockland County. Duties were subsequently shared between Goshen, which had been the center of government for the northern part of Orange County, and Newburgh, which played a similar role in the area transferred from Ulster County. The county court was established in 1801. It was not until 1970 that Goshen was named as the sole county seat. 
Due to a boundary dispute between New York and New Jersey, the boundaries of many of the southern towns of the county were not definitively established until the 19th century.
Geography.
According to the U.S. Census Bureau, the county has a total area of 839 sqmi, of which 812 sqmi is land and 27 sqmi (3.2%) is water.
Orange County is in southeastern New York State, directly north of the New Jersey-New York border, west of the Hudson River, east of the Delaware River and northwest of New York City. It borders the New York counties of Dutchess, Putnam, Rockland, Sullivan, Ulster, and Westchester, as well as Passaic and Sussex counties in New Jersey and Pike County in Pennsylvania.
Orange County is the only county in New York State which borders both the Hudson and Delaware Rivers.
Orange County is where the Great Valley of the Appalachians finally opens up and ends. The western corner is set off by the Shawangunk Ridge. The area along the Rockland County border (within Harriman and Bear Mountain state parks) and south of Newburgh is part of the Hudson Highlands. The land in between is the valley of the Wallkill River. In the southern portion of the county the Wallkill valley expands into a wide glacial lake bed known as the Black Dirt Region for its fertility.
The highest point is Schunemunk Mountain, at 1664 ft above sea level. The lowest is sea level along the Hudson.
Demographics.
As of the census of 2000, there were 341,367 people, 114,788 households, and 84,483 families residing in the county. The population density was 418 people per square mile (161/km²). There were 122,754 housing units at an average density of 150 per square mile (58/km²). The racial makeup of the county was 83.70% White, 8.09% Black or African American, 0.35% Native American, 1.51% Asian, 0.04% Pacific Islander, 4.09% from other races, and 2.23% from two or more races. 11.64% of the population were Hispanic or Latino of any race. 18.3% were of Italian, 17.4% Irish, 10.2% German and 5.0% Polish ancestry according to the 2000 census. 9.23% reported speaking Spanish at home, 3.29% Yiddish, and 1.20% Italian.
By 2005, census estimates placed Orange County's non-Hispanic white population at 72.4%. African Americans were 10.2% of the population. Native Americans were at 0.4%, a change that was less than can be measured by the precision of the 2005 estimates being used for these figures. Asians were up to 2.2% of the population. Latinos had made the largest gain as an increase in their percentage of the population, and constituted 14.9% of the county's population.
There were 114,788 households out of which 39.60% had children under the age of 18 living with them, 57.90% were married couples living together, 11.40% had a female householder with no husband present, and 26.40% were non-families. 21.50% of all households were made up of individuals and 8.50% had someone living alone who was 65 years of age or older. The average household size was 2.85 and the average family size was 3.35.
In the county the population was spread out with 29.00% under the age of 18, 8.70% from 18 to 24, 30.00% from 25 to 44, 21.90% from 45 to 64, and 10.30% who were 65 years of age or older. The median age was 35 years. For every 100 females there were 100.30 males. For every 100 females age 18 and over, there were 97.50 males.
The median income for a household in the county was $52,058, and the median income for a family was $60,355. Males had a median income of $42,363 versus $30,821 for females. The per capita income for the county was $21,597. About 7.60% of families and 10.50% of the population were below the poverty line, including 14.80% of those under age 18 and 8.00% of those age 65 or over.
Despite its rural roots, Orange County has been among the fastest-growing regions within the New York City Metropolitan Area.
Law and government.
Originally, like most New York counties, Orange County was governed by a 37-member Board of Supervisors, consisting of the 20 town supervisors, 9 city supervisors elected from the 9 wards of the City of Newburgh, and four each elected from the wards of the cities of Middletown and Port Jervis. In 1968, the board adopted a county charter and a reapportionment plan that created the county legislature and executive. The first county executive and legislature were elected in November, 1969 and took office on January 1, 1970. Today, Orange County is still governed by the same charter; residents elect the county executive and a 21-member county legislature elected from 21 single-member districts. There are also several state constitutional positions that are elected, including a Sheriff, County Clerk and District Attorney. Prior to 1 January 2008 four coroners were also elected; however, on that date, the county switched to a medical examiner system.
The current County Officers are:
The County Legislature and its previous board of supervisors were long dominated by the Republican Party. However, since the late 20th century, the Democrats have closed the gap. During 2008 and 2009 the legislature was evenly split between 10 Republicans, 10 Democrats and 1 Independence Party member. In 2009, the legislature had its first Democratic chairman elected when one member of the Republican caucus voted alongside the 10 Democratic members to elect Roxanne Donnery (D)-Highlands/Woodbury to the post. At the November 2009 election, several Democratic incumbents were defeated. As of the convening of the legislature on January 1, 2012, there are 12 Republicans, 8 Democrats and 1 Independence member.
In 1970, the county switched from government by a Board of Supervisors, consisting of the elected heads of town governments, to having a 21-member elected county legislature and executive. The sheriff, district attorney and county clerk have always been elected. All serve four-year terms, with elections in the year following presidential election years, save the sheriff, whose election is the following year.
The current county executive is Steven Neuhaus, former town supervisor for Chester. Frank Phillips, Donna Benson and Carl DuBois are the incumbent district attorney, clerk and sheriff respectively. All are Republicans, and as of 2012 the legislature has a 13–8 Republican majority.
Only one Democrat, Mary McPhillips, has served as county executive. She failed to win re-election after a single term in the early 1990s. For several years in the late 2000s, one Republican legislator's decision to become an independent and caucus with the Democrats led to a 10-10-1 effective Democratic majority, with Roxanne Donnery as chair. The Republicans regained their majority in the 2009 elections.
Transportation.
The county is served by Stewart International Airport, located two miles west of Newburgh, New York. The airport serves Delta Air Lines, JetBlue Airways, Northwest Airlines, and US Airways. AirTran Airways stopped providing service to the airport in late 2008.
Ground transportation within Orange County is provided primarily by New Jersey Transit, Short Line Bus, and Metro-North Railroad's Port Jervis Line, as well as amenities such as senior citizen bussing and car services, which usually restrict themselves to their respective town or city.
The Port Jervis Line experienced major damage from Tropical Storm Irene in August 2011. As a result, train service was suspended for north of Suffern. now operational as of 12/02/2011. The Metropolitan Transportation Authority (MTA) has contracted an engineering firm to assess the damage and determine a plan to repair damaged track beds. In the meantime, the MTA is running 55 buses from 8 stations 24 hours a day 7 days a week to supplement the lost train service. The MTA has pledged to continue the buses until the trackbeds are repaired and the trains run again.
Politics.
George W. Bush won 54% of the Orange County vote in 2004 reflecting a solid Republican edge in county politics. However, Barack Obama carried the county by a 51% margin four years later. It was the first time a Democrat had carried Orange County when first elected to the presidency. That year the number of registered Democrats in the county exceeded Republicans for the first time.
The two presidential election results give the county a Cook PVI of R+2, consistent with county voters' willingness to sometimes elect Democrats, such as U.S. Rep. John Hall. From 2007 on, when Hall represented the 19th district, which covers most of the county, Orange's representation in Congress was exclusively Democratic, as Maurice Hinchey has represented the towns of Crawford, Montgomery and Newburgh and the city of Newburgh, all of which are in the 22nd district, since 1988.
In the 2010 midterms, Hall was defeated by Nan Hayworth. In [2012, Hayworth was defeated by Democrat Sean Patrick Maloney, a former adviser to President Bill Clinton. Maloney won a rematch against Hayworth in 2014.
At the state level, Republicans have continued to hold onto Senate seats while Democrats have made inroads in recent years on the Assembly side. Two State Senate districts—the 39th, held by Bill Larkin and 42nd, held by John Bonacic—cover the county.
Democrats have made significant gains in the county's State Assembly seats. The 98th district, which includes the far western part of the county as well as the Town of Warwick, is represented by Annie Rabbitt, and the 101st district, which includes the Towns of Crawford and Montgomery, is held by Claudia Tenney, both Republicans. The remainder of the county's Assembly districts are represented by Democrats: James Skoufis in the 99th district, Aileen Gunther in the 100th district, and Frank Skartados in the 104th district. Skoufis is the youngest New York State legislator currently serving.
Sports.
Delano-Hitch Stadium in Newburgh has played host to various professional and amateur teams from various leagues since opening in 1926. The most recent professional team to play their home games at Delano-Hitch Stadium was the Newburgh Black Diamonds.
High school sports.
High schools in Orange County compete in Section 9 of the New York State Public High School Athletic Association along with schools from Dutchess, Ulster, and Sullivan counties.
College sports.
The Army Black Knights of the United States Military Academy in West Point field NCAA Division I teams in 24 different sports. The Orange County Community College Colts compete in the National Junior College Athletic Association. Mount Saint Mary College in Newburgh fields 15 teams in the Eastern College Athletic Conference and the Skyline Conference of NCAA Division III.
Orange County Youth Football League (OCYFL).
The Orange County Youth Football League (O.C.Y.F.L.) is a non-profit organization that allows youth age 6 through 14 to play competitive American football. The League encompasses 15 towns with over 100 teams in Orange County and surrounding areas including Chester, Cornwall, Goshen, Highland Falls, Marlboro, Middletown, Minisink Valley, Monticello, Newburgh, New Windsor, Pine Bush, Port Jervis, Valley Central, Wallkill, Warwick and Washingtonville. It is composed of 4 Divisions, divided by weight restrictions, and a "Mighty Mite" Flag Football division for 6 & 7 year olds. In each division, there is additionally a complete cheerleading program for each team. There is a comprehensive annual schedule of play within each division for all teams, culminating in a divisional Championship game, often played in Michie Stadium or Shea Stadium at the historic United States Military Academy at West Point, New York.
Points of interest.
Points of interest in Orange County include the United States Military Academy at West Point; Brotherhood Winery, America's oldest winery, in Washingtonville; the birthplace of William H. Seward in Florida; the home and birthplace of Velveeta and Liederkranz Cheese in Monroe; the Harness Racing Museum & Hall of Fame in Goshen; the "Times Herald-Record" newspaper, the first cold press offset daily in the country, in Middletown; the Galleria at Crystal Run, in Wallkill; the Woodbury Common Premium Outlets in Monroe; and the Orange County Fair in Wallkill. The only state parks include Goosepond Mountain State Park, Harriman State Park and Sterling Forest State Park. It is also the location of Orange County Choppers, the custom motorcycle shop featured on The Discovery Channel television series "American Chopper".

</doc>
<doc id="56169" url="http://en.wikipedia.org/wiki?curid=56169" title="Oswego County, New York">
Oswego County, New York

Oswego County is a county located in the U.S. state of New York. As of the 2010 census, the population was 122,109. The county seat is Oswego. The county name is from a Mohawk language word meaning "the outpouring", referring to the mouth of the Oswego River.
Oswego County is part of the Syracuse, NY Metropolitan Statistical Area.
History.
When counties were established in the British colony of New York in 1683, the present Oswego County was part of Albany County. This was an enormous county, including the northern part of what is now New York state as well as all of the present state of Vermont and, in theory, extending westward to the Pacific Ocean. This county was reduced in size on July 3, 1766 by the creation of Cumberland County in the British colony, and further on March 16, 1770 by the creation of Gloucester County, both containing territory now in Vermont.
On March 12, 1772, what was left of Albany County was split into three parts, one remaining under the name Albany County. One of the other pieces, Tryon County, contained the western portion (and thus, since no western boundary was specified, theoretically still extended west to the Pacific). The eastern boundary of Tryon County was approximately five miles west of the present city of Schenectady, and the county included the western part of the Adirondack Mountains and the area west of the West Branch of the Delaware River. The area then designated as Tryon County now includes 37 counties of New York State. The county was named for William Tryon, colonial governor of New York.
In the years prior to 1776, most of the Loyalists in Tryon County fled to Canada. In 1784, following the peace treaty that ended the American Revolutionary War, the name of Tryon County was changed to Montgomery County to honor the general, Richard Montgomery, who had captured several places in Canada and died attempting to capture the city of Quebec, replacing the name of the hated British governor.
In 1789, the size of Montgomery County was reduced by the splitting off of Ontario County from Montgomery. The actual area split off from Montgomery County was much larger than the present county, also including the present Allegany, Cattaraugus, Chautauqua, Erie, Genesee, Livingston, Monroe, Niagara, Orleans, Steuben, Wyoming, Yates, and part of Schuyler and Wayne counties.
Oswego County was partly in Macomb's Purchase of 1791.
In 1791, Herkimer County was one of three counties split off from Montgomery (the other two being Otsego, and Tioga County). This was much larger than the present county, however, and was reduced by a number of subsequent splits.
In 1794, Onondaga County was created from a part of Herkimer County. This county was larger than the current Onondaga County, including the present Cayuga, Cortland, and part of Oswego counties.
In 1798, Oneida County was created from a part of Herkimer County. This county was larger than the current Oneida County, including the present Jefferson, Lewis, and part of Oswego counties.
In 1805, Oneida County was reduced in size by the splitting off of Jefferson and Lewis counties.
In 1816, Oswego County was created as New York State's 48th county from parts of Oneida and Onondaga counties.
In 1841, businessmen in Oswego attempted to divide Oswego County into two counties. They failed to persuade the State to do so, however. Occasionally, the topic still comes up today by dividing the county into an east part and a west part, with the east portion being renamed "Salmon County".
At various times, beginning in 1847 and as late as 1975, attempts were made to move the county seat to the Village of Mexico. However, none of these attempts succeeded.
During 1–12 February 2007, a major lake effect snowfall dumped over ten feet of snow in many places in Oswego County, resulting in several roof collapses, some communities being cut off, and some people being snowed-in in their homes. A state of emergency was declared for the county, and the National Guard was sent in to help clear the snow.
On April 20, 2002, around 6:50 am, many residents of Oswego County were shaken awake by a magnitude 5.2 earthquake centered near Plattsburgh, New York. Minor damage to a Fire Hall in Altmar was the only report of damage. No injuries were sustained.
Government and politics.
The Oswego County legislature has 25 members, elected from equal population districts, reduced from 36 in 1993.
Geography.
According to the U.S. Census Bureau, the county has a total area of 1312 sqmi, of which 952 sqmi is land and 360 sqmi (27%) is water.
Oswego County is in northwestern New York State, just north of Syracuse and northwest of Utica, on the eastern shore of Lake Ontario. Part of the Tug Hill Plateau is in the eastern part of the county and, at 1550 ft, is the highest point. The Salmon River Falls, a 110 ft waterfall, is a popular sightseeing destination in the northeastern portion of the county.
There are two harbors in the county, Oswego Harbor at the mouth of the Oswego River and Port Ontario on the Salmon River. The first major port of call on the Great Lakes is the Port of Oswego Authority dock.
The town of Orwell is officially designated as "dry".
Demographics.
As of the census of 2000, there were 122,377 people, 45,522 households, and 31,228 families residing in the county. The population density was 128 people per square mile (50/km²). There were 52,831 housing units at an average density of 55 per square mile (21/km²). The racial makeup of the county was 97.17% White, 0.59% Black or African American, 0.41% Native American, 0.42% Asian, 0.01% Pacific Islander, 0.48% from other races, and 0.93% from two or more races. Hispanic or Latino of any race were 1.30% of the population. 15.5% were of Irish, 14.0% German, 13.7% Italian, 13.3% English, 9.6% American, 7.9% French and 5.3% Polish ancestry according to Census 2000. 96.2% spoke English and 1.7% Spanish as their first language.
There were 45,522 households out of which 35.00% had children under the age of 18 living with them, 52.80% were married couples living together, 10.80% had a female householder with no husband present, and 31.40% were non-families. 24.30% of all households were made up of individuals and 9.70% had someone living alone who was 65 years of age or older. The average household size was 2.60 and the average family size was 3.08.
In the county the population was spread out with 26.80% under the age of 18, 10.90% from 18 to 24, 28.90% from 25 to 44, 22.10% from 45 to 64, and 11.30% who were 65 years of age or older. The median age was 35 years. For every 100 females there were 97.50 males. For every 100 females age 18 and over, there were 94.40 males.
The median income for a household in the county was $36,598, and the median income for a family was $43,821. Males had a median income of $34,976 versus $23,938 for females. The per capita income for the county was $16,853. About 9.70% of families and 14.00% of the population were below the poverty line, including 17.10% of those under age 18 and 9.50% of those age 65 or over.
Oswego County is also home to two colleges: State University of New York at Oswego located in the Town of Oswego and the Fulton Branch Campus of Cayuga County Community College located in the City of Fulton.
Communities.
Oswego County has 22 towns, 2 cities, and 10 villages.

</doc>
<doc id="56170" url="http://en.wikipedia.org/wiki?curid=56170" title="Otsego County, New York">
Otsego County, New York

Otsego County is a county located in the U.S. state of New York. At the 2010 census, the population was 62,259. The county seat is Cooperstown. The name "Otsego" is from a Mohawk word meaning "place of the rock."
Otsego County comprises the Oneonta, NY Micropolitan Statistical Area.
History.
In 1789, Ontario County was split off from Montgomery. The area split off from Montgomery County was much larger than the present county, as it included the present Allegany, Cattaraugus, Chautauqua, Erie, Genesee, Livingston, Monroe, Niagara, Orleans, Steuben, Wyoming, Yates, and part of Schuyler and Wayne counties.
Formation.
Otsego County was one of three counties split off from Montgomery (the other two being Herkimer, and Tioga). Otsego County was officially established on February 16, 1791, with Cooperstown as its county seat, although at the time the village of Cherry Valley was much larger. The original county consisted of three large townships:
Otsego and Cherry Valley together roughly covered the area of modern Otsego County, while Harpersfield covered the area south of the current county as far as the Delaware River.
The original appointments to Otsego County government positions, made by Governor George Clinton included:
New towns.
By 1793, four towns had been added to the county by division of the existing towns:
In 1795, a piece of Otsego County was joined with a portion taken from Albany County to create Schoharie County.
In 1797, a piece of Otsego County was joined with a portion taken from Ulster County to create Delaware County.
Geography.
According to the U.S. Census Bureau, the county has a total area of 1016 sqmi, of which 1002 sqmi is land and 14 sqmi (1.4%) is water.
Otsego County is in central New York State, to the west of Albany, southeast of Utica, and northeast of Binghamton. The county is part of the Mohawk Valley region of New York State. The county is considered by some to belong to the Southern Tier region of New York State.
Demographics.
As of the census of 2000, there were 61,676 people, 23,291 households, and 15,115 families residing in the county. The population density was 62 people per square mile (24/km²). There were 28,481 housing units at an average density of 28 per square mile (11/km²). The racial makeup of the county was 95.80% White, 1.75% African American, 0.23% Native American, 0.63% Asian, 0.05% Pacific Islander, 0.50% from other races, and 1.05% from two or more races. Hispanic or Latino of any race were 1.90% of the population. 15.0% were of Irish, 14.9% English, 14.9% German, 11.3% Italian and 9.1% American ancestry according to Census 2000. 95.4% spoke English and 2.1% Spanish as their first language.
There were 23,291 households out of which 29.60% had children under the age of 18 living with them, 51.10% were married couples living together, 9.50% had a female householder with no husband present, and 35.10% were non-families. 27.00% of all households were made up of individuals and 11.60% had someone living alone who was 65 years of age or older. The average household size was 2.43 and the average family size was 2.94.
In the county the population was spread out with 22.70% under the age of 18, 14.40% from 18 to 24, 24.30% from 25 to 44, 23.60% from 45 to 64, and 15.00% who were 65 years of age or older. The median age was 37 years. For every 100 females there were 93.10 males. For every 100 females age 18 and over, there were 90.00 males.
The median income for a household in the county was $33,444, and the median income for a family was $41,110. Males had a median income of $29,988 versus $22,609 for females. The per capita income for the county was $16,806. About 8.80% of families and 14.90% of the population were below the poverty line, including 15.80% of those under age 18 and 8.20% of those age 65 or over.
Government and politics.
Otsego County is a true swing county and bellwether; it has chosen the winner of the presidency for the last three decades. In 2004, Otsego County voted 51-48 percent in favor of George W. Bush. In 2008 and 2012, Otsego County voted in favor of Barack Obama. Democrats are prevalent in the City of Oneonta and Village of Cooperstown, and the majority of voters in many of the surrounding towns are registered Republicans.
Otsego County is the only county in New York that names its legislative body the Board of Representatives, which consists of members from 14 single-member districts. The Board Chair is Kathleen Clark (R). The county also has a County Attorney, County Auditor, County Clerk, and County Sheriff.
Economy.
The Village of Cooperstown (home of James Fenimore Cooper), located at the south end of Otsego Lake, attracts many tourists to the Baseball Hall of Fame and the New York State Historical Association museums. Its primary industry is healthcare as it is home to Bassett Medical Center, the headquarters of Bassett Healthcare Network and its more than 3,000 employees. The City of Oneonta is the home of Hartwick College, the State University of New York at Oneonta, A.O. Fox Memorial Hospital an affiliate of the Bassett Network, major retail activity, and numerous small businesses. The county as a whole remains relatively rural, with dairy farming a contributing industry that has consolidated employment in recent years, although production has remained steady.

</doc>
<doc id="56176" url="http://en.wikipedia.org/wiki?curid=56176" title="Fatimid Caliphate">
Fatimid Caliphate

The Fatimid Caliphate (Arabic: الفاطميون‎, "al-Fāṭimīyūn") was a Shia Islamic caliphate, which spanned a large area of North Africa, from the Red Sea in the east to the Atlantic Ocean in the west. The dynasty ruled across the Mediterranean coast of Africa and ultimately made Egypt the centre of the caliphate. At its height, the caliphate included in addition to Egypt varying areas of the Maghreb, Sudan, Sicily, the Levant, and Hijaz.
The Fatimids claimed to be descended from Fatima bint Muhammad (فاطمة بنت محمد), the daughter of Islamic prophet Muhammad. The Fatimids conquered North Africa and their Fatimid state took shape among the Kutama, in the Western North of Africa, particularly Algeria. In 909 Fatimid established the Tunisian city of Mahdia as their capital. In 948 they shifted their capital to Al-Mansuriya, near Kairouan, Tunisia. In 969 they conquered Egypt and established Cairo as the capital of their caliphate, and Egypt became the political, cultural, and religious centre of their empire.
The ruling class belonged to the Ismaili branch of Shi'ism, as did the leaders of the dynasty. The existence of the caliphate marked the only time the descendants of Ali through Fatimah, the daughter of the prophet were united to any degree, except for the final period of the Rashidun Caliphate under Ali himself, and the name Fatimid refers to Fatimah. The different term "Fatimite" is sometimes used to refer to the caliphate's subjects.
After the initial conquests, the caliphate often allowed a degree of religious tolerance towards non-Ismaili sects of Islam, as well as to Jews, Maltese Christians, and Egyptian Coptic Christians. The Fatimid caliphate was also distinguished by the central role of Berbers in its initial establishment and in helping its development, especially on the military and political levels.
During the late eleventh and twelfth century, however, the Fatimid caliphate declined rapidly, and in 1171 the country was invaded by Ṣalāḥ ad-Dīn. He founded the Ayyubid dynasty and incorporated the Fatimid state into the Abbasid Caliphate.
Rise of the Fatimids.
Origins.
The Fatimid Caliphate's religious ideology originated in an Ismaili Shia movement launched in Syria by the eighth Imam, Abd Allah al-Akbar. He claimed descent through Ismail, the seventh Shia imam, from Fatimah and her husband ʻAlī ibn-Abī-Tālib, the first Shīʻa Imām. The his name "al-Fātimiyyūn" "Fatimid". The eighth to tenth Imams, (Abadullah, Ahmed and Husain), remained hidden and worked for the movement against the period's time's rulers.
The 11th Imam Ubayd Allah al-Mahdi Billah, under the guise of being a merchant, and his son had made their way to Sijilmasa, fleeing persecution by the Abbasids, who found their Isma'ili Shi'ite beliefs not only unorthodox, but also threatening to the status quo of their caliphate. According to legend, 'Ubayd Allah and his son were fulfilling a prophecy that the "mahdi" would come from Mesopotamia to Sijilmasa. They hid among the population of Sijilmasa, then an independent emirate, for four years under the countenance of the Midrar rulers, specifically one Prince Yasa' ibn Midrar (r. 884-909).
Al-Mahdi was supported by dedicated Shi'ite Abu 'Abdullah al-Shi'i, and al-Shi'i started his preaching after he encountered a group of Muslim North African during his hajj. These men bragged about the country of the Kutama in western Ifriqiya (today part of Algeria), and the hostility of the Kutama towards, and their complete independence from, the Aghlabid rulers. This triggered al-Shi'i to travel to the region, where he started to preach the Ismaili doctrine. The Berber peasants, who had been oppressed for decades by the corrupt Aghlabid rule, would prove themselves to be a perfect basis for sedition. Instantly, al-Shi'i began conquering cities in the region: first Mila, then Sétif, Kairouan, and eventually Raqqada, the Aghlabid capital. In 909 Al-Shi'i sent a large expedition force to rescue the Mahdi, conquering the Khariji state of Tahert on its way there. After gaining his freedom, Abdullah al-Mahdi Billah became the leader of the growing state and assumed the position of imam and caliph.
The Fatimids existed during the Islamic Golden Age. The dynasty was founded in 909 by the eleventh Imam ʻAbdullāh al-Mahdī Billah. For the first half of its existence the empire's power rested primarily on its strength, as its army conquered northern Africa, Palestine, Syria, and for a short time, Baghdad.
A new capital was established at al-Mahdiyya. The Muslim Mahdia was founded by the Fatimids under the Caliph Abdallah al-Mahdi in 921 and made Ifriqiya their capital city. It was chosen as the capital because of its proximity to the sea and the promontory on which an important military settlement had been since the time of the Phoenicians.
Expansion.
The Fatimid Caliphate grew to include Sicily and to stretch across North Africa from the Atlantic Ocean to Libya. Abdullāh al-Mahdi's control soon extended over all of central Maghreb, an area consisting of the modern countries of Morocco, Algeria, Tunisia, and Libya, which he ruled from Mahdia, his newly built capital in Tunisia. Al-Mansuriya, or Mansuriyya (Arabic: المنصوريه‎), near Kairouan, Tunisia, was the capital of the Fatimid Caliphate during the rules of the Imams Al-Mansur Billah (r. 946–953) and Al-Mu'izz li-Din Allah (r. 953–975).
The Fatimid general Jawhar conquered Egypt in 969, and he built a new palace city there, near Fusṭāt, which he also called al-Manṣūriyya. Under Al-Muizz Lideenillah, the Fatimids conquered the Ikhshidid Wilayah (see Fatimid Egypt), founding a new capital at "al-Qāhira" (Cairo) in 969. The name was a reference to the planet Mars, "The Subduer", which was prominent in the sky at the moment that city construction started. Cairo was intended as a royal enclosure for the Fatimid caliph and his army, though the actual administrative and economic capital of Egypt was in cities such as Fustat until 1169. After Egypt, the Fatimids continued to conquer the surrounding areas until they ruled from Tunisia to Syria, as well as Sicily.
Under the Fatimids, Egypt became the center of an empire that included at its peak North Africa, Sicily, Palestine, Jordan, Lebanon, Syria, the Red Sea coast of Africa, Tihamah, Hejaz, and Yemen. Egypt flourished, and the Fatimids developed an extensive trade network in both the Mediterranean and the Indian Ocean. Their trade and diplomatic ties extended all the way to China and its Song Dynasty, which eventually determined the economic course of Egypt during the High Middle Ages. The Fatimid focus on long-distance trade was accompanied by a lack of interest in agriculture and a neglect of the Nile irrigation system.
Administration and culture.
Unlike western European governments in the era, advancement in Fatimid state offices was based more on merit than on heredity. Members of other branches of Islam, like the Sunnis, were just as likely to be appointed to government posts as Shiites. Tolerance was extended to non-Muslims such as Christians and Jews, who occupied high levels in government based on ability, and tolerance was set into place to ensure the flow of money from all those who were non-Muslims in order to finance the Caliphs' large army of Mamluks brought in from Circassia by Genoese merchants. There were exceptions to this general attitude of tolerance, however, most notably by Al-Hakim bi-Amr Allah, though this has been highly debated, with Al-Hakim's reputation among medieval Muslim historians conflated with his role in the Druze faith.
The Fatimids were also known for their exquisite arts. A type of ceramic, lustreware, was prevalent during the Fatimid period. Glassware and metalworking was also popular. Many traces of Fatimid architecture exist in Cairo today; the most defining examples include the Al Azhar University and the Al Hakim mosque. The Al Azhar University was the first university in the East and perhaps the oldest in history. The madrasa is one of the relics of the Fatimid dynasty era of Egypt, descended from Fatimah, daughter of Muhammad. Fatimah was called "Az-Zahra" (the brilliant), and the madrasa was named in her honor. It was founded as a mosque by the Fatimid commander Jawhar at the orders of the Caliph Al-Muizz when he founded the city of Cairo. It was (probably on Saturday) in Jamadi al-Awwal in the year 359 A.H. Its building was completed on the 9th of Ramadan in the year 361 A.H. Both Al-'Aziz Billah and Al-Hakim bi-Amr Allah added to its premises. It was further repaired, renovated, and extended by Al-Mustansir Billah and Al-Hafiz Li-Din-illah. Fatimid Caliphs always encouraged scholars and jurists to have their study-circles and gatherings in this mosque, and thus it was turned into a university that has the claim to be considered as the oldest still-functioning University.
Intellectual life in Egypt during the Fatimid period achieved great progress and activity, due to many scholars who lived in or came to Egypt, as well as the number of books available. Fatimid Caliphs gave prominent positions to scholars in their courts, encouraged students, and established libraries in their palaces, so that scholars might expand their knowledge and reap benefits from the work of their predecessors.
Perhaps the most significant feature of Fatimid rule, was the freedom of thought and reason extended to the people, who could believe in whatever they liked, provided they did not infringe on the rights of others. Fatimids reserved separate pulpits for different Islamic sects, where the scholars expressed their ideas in whatever manner they liked. Fatimids gave patronage to scholars and invited them from every place, spending money on them even when their beliefs conflicted with those of the Fatimids. The history of the Fatimids, from this point of view, is in fact the history of knowledge, literature, and philosophy. It is the history of sacred freedom of expression.
The Fatimid palace in Cairo had two parts. It stood in the Khan el-Khalili area at Bayn El-Qasryn street.
Military system.
The Fatimid military was based largely on the Kutama Berber tribesmen brought along on the march to Egypt, and they remained an important part of the military even after Tunisia began to break away. After their successful establishment in Egypt, local Egyptian forces were also incorporated into the army, so the Fatimids Army were reinforced by North African soldiers from Algeria to Egypt in the Eastern North. (and of succeeding dynasties as well).
A fundamental change occurred when the Fatimid Caliph attempted to push into Syria in the later half of the 10th century. The Fatimids were faced with the now Turkish-dominated forces of the Abbasid Caliph and began to realize the limits of their current military. Thus during the reign of Abu Mansur Nizar al-Aziz Billah and Al-Hakim bi-Amr Allah, the Caliph began incorporating armies of Turks and later Black Africans (even later, other groups such as Armenians were also used). The army units were generally separated along ethnic lines, thus the Berbers were usually the light cavalry and foot skirmishers, while the Turks would be the horse archers or heavy cavalry (known as Mamluks). The black Africans, Syrians, and Arabs generally acted as the heavy infantry and foot archers. This ethnic-based army system, along with the partial slave status of many of the imported ethnic fighters, would remain fundamentally unchanged in Egypt many centuries after the fall of the Fatimid caliph.
The Fatimids put all their military power toward the defense of the empire whenever it was menaced by dangers and threats, which they were able to repel, especially during the rule of Al-Muizz Lideenillah. During his reign, the Byzantine Empire was ruled by Nikephoros II Phokas, who had destroyed the muslim Emirate of Chandax in 961 and conquered Tartus, Al-Masaisah, 'Ain Zarbah, and other places, gaining complete control of Iraq and the Syrian borders as well as earning the sobriquet, "The Pale Death of the Saracens". With the Fatimids however, he proved less successful. After renouncing his payments of tribute to the Fatimid caliphs, he sent an expedition to Sicily, but was forced by defeats on land and sea to evacuate the island completely. In 967, he made peace with the Fatimids of Kairawan and turned to defend himself against their common enemy, Otto I, who had proclaimed himself Western emperor and had attacked Byzantine possessions in Italy.
Civil war and decline.
While the ethnic-based army was generally successful on the battlefield, it began to have negative effects on Fatimid internal politics. Traditionally the Berber element of the army had the strongest sway over political affairs, but as the Turkish element grew more powerful, it began to challenge this, and by 1020 serious riots had begun to break out among the Black African troops who were fighting back against a Berber-Turk Alliance.
By the 1060s, the tentative balance between the different ethnic groups within the Fatimid army collapsed as Egypt was suffering through a serious span of drought and famine. The declining resources accelerated the problems among the different ethnic factions, and outright civil war began, primarily between the Turks and Black African troops, while the Berbers shifted alliance between the two sides. The Turkish forces of the Fatimid army seized most of Cairo and held the city and Caliph at ransom, while the Berber troops and remaining Sudanese forces roamed the other parts of Egypt.
By 1072 the Fatimid Caliph Abū Tamīm Ma'ad al-Mustansir Billah in a desperate attempt to save Egypt recalled the general Badr al-Jamali, who was at the time the governor of Acre, Palestine. Badr al-Jamali led his troops into Egypt and was able to successfully suppress the different groups of the rebelling armies, largely purging the Turks in the process. Although the Caliphate was saved from immediate destruction, the decade long rebellion devastated Egypt and it was never able to regain much power. As a result, Badr al-Jamali was also made the vizier of the Fatimid caliph, becoming one of the first military viziers ("Amir al Juyush", Arabic: امير الجيوش‎, Commander of Forces of the Fatimids) that would dominate late Fatimid politics. Al-Jam`e Al-Juyushi (Arabic: الجامع الجيوشي‎, The Mosque of the Armies), or Juyushi Mosque, was built by Badr al-Jamali. The mosque was completed in 478 H/1085 AD under the patronage of then Caliph and Imam Ma'ad al-Mustansir Billah. It was built on an end of the Mokattam Hills, ensuring a view of the Cairo city. This Mosque/mashhad was also known as a victory monument commemorating vizier Badr's restoration of order for the Imam Mustansir. As the military viziers effectively became heads of state, the Caliph himself was reduced to the role of a figurehead. Badr al-Jamali's son, Al-Afdal Shahanshah, succeeded him in power as vizier.
After the eighteenth Imam, al-Mustansir Billah, the Nizari sect believed that his son Nizar was his successor, while another Ismāʿīlī branch known as the Mustaali (from whom the Dawoodi Bohra would eventually descend), supported his other son, al-Musta'li. The Fatimid dynasty continued with al-Musta'li as both Imam and Caliph, and that joint position held until the 20th Imam, al-Amir bi-Ahkami l-Lah (1132 CE). At the death of Imam Amir, one branch of the Mustaali faith claimed that he had transferred the imamate to his son at-Tayyib Abi l-Qasim, who was then two years old. Another faction claimed Amir died without producing an heir, and supported Amir's cousin al-Hafiz as both the rightful Caliph and Imam. The al-Hafiz faction became the Hafizi Ismailis, who later converted during the rule of Sultan Ṣalāḥ ad-Dīn Yūsuf ibn Ayyūbi. The supporters of Tayyeb became the Tayyibi Ismāʿīlī. Tayyeb's claim to the imamate was endorsed by the "Hurratu l-Malika" ("the Noble Queen") Arwa al-Sulayhi, the Queen of Yemen. Arwa was designated a "hujjah" (a holy, pious lady), the highest rank in the Yemeni Dawat, by al-Mustansir in 1084 CE. Under Queen Arwa, the "Dai al-Balagh" (intermediary between the Imam in Cairo and local headquarters) Lamak ibn Malik and then Yahya ibn Lamak worked for the cause of the Fatimids. After seclusion of Imam Taiyab Dai given independent charge by Queen Arwa, and were called Dai al Mutlaq. First Dai Mutlaq was Syedna Zoib, common Dai of all Taiybians.
Burial place of Fatimid.
There is the place known as "Al Mashhad al Husain" (Masjid Imam Husain, Cairo), wherein lie buried underground Twelve Fatimid Imams from 9th Taqi Muhammad to 20th Mansur al-Āmir. This place is also known as "B’ab Makhallif’at al Rasul" (door of remaining part of Rasul), where Sacred Hair
 of Muhammad is preserved.
Decay and fall.
In the 1040s, the Berber Zirids (governors of North Africa under the Fatimids) declared their independence from the Fatimids and their recognition of the Sunni Abbasid caliphs of Baghdad, which led the Fatimids to launch devastating Banū Hilal invasions. After about 1070, the Fatimid hold on the Levant coast and parts of Syria was challenged first by Turkic invasions, then the Crusades, so that Fatimid territory shrank until it consisted only of Egypt. The Fatimids gradually lost the Emirate of Sicily over thirty years to the Italo-Norman Roger I who was in total control of the entire island by 1091.
The reliance on the Iqta system also ate into Fatimid central authority, as more and more the military officers at the further ends of the empire became semi-independent and were often a source of problems.
After the decay of the Fatimid political system in the 1160s, the Zengid ruler Nūr ad-Dīn had his general, Shirkuh, seized Egypt from the vizier Shawar in 1169. Shirkuh died two months after taking power, and the rule went to his nephew, Saladin. This began the Ayyubid Sultanate of Egypt and Syria.
Fatimid heritage.
After caliph al-'Āḍid, the Fatimids were deposed from rule over Egypt by the Ayyubids.
Many groups( Alavi, Hebtiahs, Atbai Malak, Dawoodi) lay claim to the Fatimid legacy. The Taiyabi (the Dawoodi Bohra being a majority constituent) claim that their "Da`i"s (see List of Dai of Dawoodi Bohra) are successors in authority to 21st Imam Taiyab abi al-Qasim, the son of 20th Imam Mansur al-Āmir bi-Aḥkām Allāh "(10th Fatimid calipha)" (the office of Da`i being instituted by Sulayhid queen of Yemen Arwa al-Sulayhi). The current Dai of Dawoodi Bohra is also disputed due to 53rd Syedna succession controversy (Dawoodi Bohra) between two claimants Khuzaima Qutubuddin who was the Mazoon and Mufaddal Saifuddin , both claim to have taken office.
Imam Abdul Salam, Imam Ghareeb Mirza and continued Imamat as a next Imams after Imam Mustansir Billah and started imamat series of Nizari ismailies now as an Imam is Shah karim Al Hussaini Aga Khan IV as 49th Hazir Imam. The current claimant to be genealogical heir of the Nizari line is the Aga Khan.

</doc>
<doc id="56178" url="http://en.wikipedia.org/wiki?curid=56178" title="Bandana (disambiguation)">
Bandana (disambiguation)

Bandana can refer to:

</doc>
<doc id="56183" url="http://en.wikipedia.org/wiki?curid=56183" title="Battle of Abukir">
Battle of Abukir

The Battle of Abukir or Aboukir refers to three battles fought near Abukir, Egypt, on the Mediterranean coast west of the Nile delta:

</doc>
<doc id="56189" url="http://en.wikipedia.org/wiki?curid=56189" title="Interlaced video">
Interlaced video

Interlaced video is a technique for doubling the perceived frame rate of a video display without consuming extra bandwidth. The interlaced signal contains two fields of a video frame captured at two different times. This enhances motion perception to the viewer, and reduces flicker by taking advantage of the phi phenomenon effect. 
This effectively doubles the time resolution (also called "temporal resolution") as compared to non-interlaced footage (for frame rates equal to field rates). Interlaced signals require a display that is natively capable of showing the individual fields in a sequential order. Only CRT displays and ALiS plasma displays are capable of displaying interlaced signals, due to the electronic scanning and lack of apparent fixed-resolution.
Interlaced scan refers to one of two common methods for "painting" a video image on an electronic display screen (the other being progressive scan) by scanning or displaying each line or row of pixels. This technique uses two fields to create a frame. One field contains all odd lines in the image, the other contains all even lines.
A PAL-based television set display, for example, scans 50 fields every second (25 odd and 25 even). The two sets of 25 fields work together to create a full frame every 1/25 of a second (or 25 frames per second), but with interlacing create a new half frame every 1/50 of a second (or 50 fields per second). To display interlaced video on progressive scan displays, playback applies deinterlacing to the video signal (which adds input lag).
The European Broadcasting Union has argued against interlaced video in production and broadcasting. They recommend 720p 50 fps (frames per second) for the current production format—and are working with the industry to introduce 1080p50 as a future-proof production standard. 1080p 50 offers higher vertical resolution, better quality at lower bitrates, and easier conversion to other formats, such as 720p50 and 1080i50. The main argument is that no matter how complex the deinterlacing algorithm may be, the artifacts in the interlaced signal cannot be completely eliminated because some information is lost between frames.
Despite arguments against it, television standards organizations continue to support interlacing. It is still included in digital video transmission formats such as DV, DVB, and ATSC. New video compression standards in development, like High Efficiency Video Coding, do not support interlaced coding tools and target high-definition progressive video such as ultra high definition television.
Description.
Progressive scan captures, transmits, and displays an image in a path similar to text on a page—line by line, top to bottom.
The interlaced scan pattern in a CRT display also completes such a scan, but in two passes (two fields). The first displays the first and all odd numbered lines, from the top left corner to the bottom right corner. The second pass displays the second and all even numbered lines, filling in the gaps in the first scan.
This scan of every second line is called "interlacing". A "field" is an image that contains only half of the lines needed to make a complete picture. Persistence of vision makes the eye perceive the two fields as a continuous image. In the days of CRT displays, the afterglow of the display's phosphor aided this effect.
Interlacing provides full horizontal detail with the same bandwidth that would be required for a full progressive scan of twice the perceived frame rate and refresh rate. To prevent flicker, all analog broadcast television systems used interlacing.
Format identifiers like 576i50 and 720p50 specify the frame rate for progressive scan formats—but for interlaced formats, they typically specify the field rate (which is twice the frame rate). This can lead to confusion, because industry-standard SMPTE timecode formats always deal with frame rate, not field rate. To avoid confusion, SMPTE and EBU always use frame rate to specify interlaced formats, e.g., 480i60 is 480i/30, 576i50 is 576i/25, and 1080i50 is 1080i/25. This convention assumes that each frame in an interlaced signal contains two sub-fields in sequence.
Benefits of interlacing.
One of the most important factors in analog television is signal bandwidth, measured in megahertz. The greater the bandwidth, the more expensive and complex the entire production and broadcasting chain. This includes cameras, storage systems, broadcast systems—and reception systems: terrestrial, cable, satellite, Internet, and end-user displays (TVs computer monitors).
For a fixed bandwidth, interlace provides a video signal with twice the display refresh rate for a given line count (versus progressive scan video at a similar frame rate—for instance 1080i at 60 half-frames per second, vs. 1080p at 30 full frames per second). The higher refresh rate improves the appearance of objects motion, because it updates their position on the display more often, and when objects are stationary, human vision combines information from multiple similar half-frames to produce the same perceived resolution as progressive full frames. This technique is only useful though, if source material is available in higher refresh rates. Cinema movies are typically recorded at 24fps, and don't benefit from interlacing.
Given a fixed bandwidth and high refresh rate, interlaced video can also provide a higher spatial resolution than progressive scan. For instance, 1920×1080 pixel resolution interlaced HDTV with a 60 Hz field rate (known as 1080i60 or 1080i/30) has a similar bandwidth to 1280×720 pixel progressive scan HDTV with a 60 Hz frame rate (720p60 or 720p/60), but achieves approximately twice the spatial resolution for low-motion scenes.
However, bandwidth benefits only apply to analog or "uncompressed" digital video signal. With digital video compression, as used in all current digital TV standards, interlacing introduces additional inefficiencies. EBU has performed tests that show that the bandwidth savings of interlaced video over progressive video is minimal, even with twice the frame rate. I.e., 1080p50 signal produces roughly the same bit rate as 1080i50 (aka 1080i/25) signal, and 1080p50 actually requires less bandwidth to be perceived as subjectively better than its 1080i/25 (1080i50) equivalent when encoding a "sports-type" scene.
The VHS, and most other analogue video recording methods that use a rotary drum to record video on tape, benefit from interlacing. On the VHS, the drum turns a full revolution per frame, and carries two picture heads, each of which sweep the tape surface once for every revolution. If the device was made to record progressive scanned video, the switchover of the heads would fall in the middle of the picture and appear as a horizontal band. Interlacing allows the switchovers to occur at the top and bottom of the picture, areas which in a standard TV set are invisible to the viewer. The device can also be made more compact than if each sweep recorded a full frame, as this would require a double diameter drum rotating at half the angular velocity and making longer, shallower sweeps on the tape to compensate for the doubled line count per sweep. However, when a still image is produced from an interlaced video tape recording, on most older consumer grade units the tape would be stopped and both heads would just repeatedly read the "same" field of the picture, essentially halving the vertical resolution until playback proceeds. The other option is to capture a full frame (both fields) upon pressing the pause button right before actually stopping the tape, and then repetitively reproduce it from a frame buffer. The latter method can produce a sharper image but some degree of deinterlacing would mostly be required to gain notable visual benefit. While the former method will produce horizontal artifacts towards the top and bottom of the picture due to the heads being unable to traverse exactly the same path along the tape surface as when recording on a moving tape, this misalignment would actually be worse with progressive recording.
Interlacing can be exploited to produce 3D TV programming, especially with a CRT display and especially for color filtered glasses by transmitting the color keyed picture for each eye in the alternating fields. This does not require significant alterations to existing equipment. Shutter glasses can be adopted as well, obviously with the requirement of achieving synchronisation. If a progressive scan display is used to view such programming, any attempt to deinterlace the picture will render the effect useless. For color filtered glasses the picture has to be either buffered and shown as if it was progressive with alternating color keyed lines, or each field has to be line-doubled and displayed as discrete frames. The latter procedure is the only way to suit shutter glasses on a progressive display.
Interlacing problems.
Interlaced video is designed to be captured, stored, transmitted, and displayed in the same interlaced format. Because each interlaced video frame is two fields captured at different moments in time, interlaced video frames can exhibit motion artifacts known as "interlacing effects", or "combing", if recorded objects move fast enough to be in different positions when each individual field is captured. These artifacts may be more visible when interlaced video is displayed at a slower speed than it was captured, or in still frames. 
While there are simple methods to produce somewhat satisfactory progressive frames from the interlaced image, for example by doubling the lines of one field and omitting the other (halving vertical resolution), or anti-aliasing the image in the vertical axis to hide some of the combing, there are sometimes methods of producing results far superior to these. If there is only sideways (X axis) motion between the two fields and this motion is even throughout the full frame, it is possible to align the scanlines and crop the left and right ends that exceed the frame area to produce a visually satisfactory image. Minor Y axis motion can be corrected similarly by aligning the scanlines in a different sequence and cropping the excess at the top and bottom. Often the middle of the picture is the most necessary area to put into check, and whether there is only X or Y axis alignment correction, or both are applied, most artifacts will occur towards the edges of the picture. However, even these simple procedures require motion tracking between the fields, and a rotating or tilting object, or one that moves in the Z axis (away from or towards the camera) will still produce combing, possibly even looking worse than if the fields were joined in a simpler method. There is no perfect way around the conversion, but the best deinterlacing processes would have to analyze each frame individually and decide the best method.
Interline twitter.
Interlace introduces a potential problem called interline twitter, a form of moiré. This aliasing effect only shows up under certain circumstances—when the subject contains vertical detail that approaches the horizontal resolution of the video format. For instance, a finely striped jacket on a news anchor may produce a shimmering effect. This is "twittering". Television professionals avoid wearing clothing with fine striped patterns for this reason. Professional video cameras or Computer Generated Imagery systems apply a low-pass filter to the vertical resolution of the signal to prevent interline twitter.
Interline twitter is the primary reason that interlacing is less suited for computer displays. Each scanline on a high-resolution computer monitor typically displays discrete pixels that do not span the scanlines above or below. When the overall interlaced framerate is 30 frames per second, a pixel that spans only one scanline is visible for 1/30 of a second followed by 1/30 of a second of darkness, reducing the per-line/per-pixel framerate to 15 frames per second.
To avoid this, standard interlaced television sets typically don't display sharp detail. When computer graphics appear on a standard television set, the screen is treated as if it were half the resolution of what it actually is or even lower. If text is displayed, it is large enough so that horizontal lines are never one scanline wide. Most fonts for television programming have wide, fat strokes, and do not include fine-detail serifs that would make the twittering more visible.
Deinterlacing.
ALiS plasma panels and the old CRTs can display interlaced video directly, but modern computer video displays and TV sets are mostly based on LCD technology, which mostly use progressive scanning.
To display interlaced video on a progressive scan display requires a process called deinterlacing. This is an imperfect technique, and generally lowers resolution and causes various artifacts—particularly in areas with objects in motion. Providing the best picture quality for interlaced video signals requires expensive and complex devices and algorithms. For television displays, deinterlacing systems are integrated into progressive scan TV sets that accept interlaced signal, such as broadcast SDTV signal.
Most modern computer monitors do not support interlaced video, besides some legacy text-only display modes. Playing back interlaced video on a computer display requires some form of deinterlacing in the software player, which often uses very simple methods to deinterlace. This means that interlaced video often has visible artifacts on computer systems. Computer systems may be used to edit interlaced video, but the disparity between computer video display systems and interlaced television signal formats means that the video content being edited cannot be viewed properly without separate video display hardware.
Current manufacture TV sets employ a system of intelligently extrapolating the extra information that would be present in a progressive signal entirely from an interlaced original. In theory: this should simply be a problem of applying the appropriate algorithms to the interlaced signal, as all information should be present in that signal. In practice, results are currently variable, and depend on the quality of the input signal and amount of processing power applied to the conversion. The biggest impediment, at present, is artifacts in the lower quality interlaced signals (generally broadcast video), as these are not consistent from field to field. On the other hand, high bit rate interlaced signals such as from HD camcorders operating in their highest bit rate mode work well.
Deinterlacing algorithms temporarily store a few frames of interlaced images and then extrapolate extra frame data to make a smooth flicker-free image. This frame storage and processing results in a slight display lag that is visible in business showrooms with a large number of different models on display. Unlike the old unprocessed NTSC signal, the screens do not all follow motion in perfect synchrony. Some models appear to update slightly faster or slower than others. Similarly, the audio can have an echo effect due to different processing delays.
History.
When motion picture film was developed, the movie screen had to be illuminated at a high rate to prevent visible flicker. The exact rate necessary varies by brightness—40 Hz is acceptable in dimly lit rooms, while up to 80 Hz may be necessary for bright displays that extend into peripheral vision. The film solution was to project each frame of film three times using a three-bladed shutter: a movie shot at 16 frames per second illuminated the screen 48 times per second. Later, when sound film became available, the higher projection speed of 24 frames per second enabled a two bladed shutter to produce 48 times per second illumination—but only in projectors incapable of projecting at the lower speed.
This solution could not be used for television. To store a full video frame and display it twice requires a frame buffer—electronic memory (RAM—sufficient to store a video frame). This method did not become feasible until the late 1980s. In addition, avoiding on-screen interference patterns caused by studio lighting and the limits of vacuum tube technology required that CRTs for TV be scanned at AC line frequency. (This was 60 Hz in the US, 50 Hz Europe.)
In the domain of mechanical television, Léon Theremin demonstrated the concept of interlacing. He had been developing a mirror drum-based television, starting with 16 lines resolution in 1925, then 32 lines and eventually 64 using interlacing in 1926. As part of his thesis, on May 7, 1926, he electrically transmitted and projected near-simultaneous moving images on a five foot square screen.
In 1930, German Telefunken engineer Fritz Schröter first formulated and patented the concept of breaking a single video frame into interlaced lines. In the USA, RCA engineer Randall C. Ballard patented the same idea in 1932. Commercial implementation began in 1934 as cathode ray tube screens became brighter, increasing the level of flicker caused by progressive (sequential) scanning.
In 1936, when the UK was setting analog standards, CRTs could only scan at around 200 lines in 1/50 of a second. Using interlace, a pair of 202.5-line fields could be superimposed to become a sharper 405 line frame. The vertical scan frequency remained 50 Hz, but visible detail was noticeably improved. As a result, this system supplanted John Logie Baird's 240 line mechanical progressive scan system that was also used at the time.
From the 1940s onward, improvements in technology allowed the US and the rest of Europe to adopt systems using progressively more bandwidth to scan higher line counts, and achieve better pictures. However the fundamentals of interlaced scanning were at the heart of all of these systems. The US adopted the 525 line system known as NTSC, Europe adopted the 625 line system, and the UK switched from its 405 line system to 625 to avoid having to develop a unique method of color TV. France switched from its unique 819 line system to the more European standard of 625. Although the term PAL is often used to describe the line and frame standard of the TV system, this is in fact incorrect and refers only to the method of superimposing the colour information on the standard 625 line broadcast. The French adopted their own SECAM system, which was also adopted by some other countries, notably Russia and its satellites. PAL has been used on some otherwise NTSC broadcasts notably in Brazil.
Interlacing was ubiquitous in displays until the 1970s, when the needs of computer monitors resulted in the reintroduction of progressive scan. Interlace is still used for most standard definition TVs, and the 1080i HDTV broadcast standard, but not for LCD, micromirror (DLP), or plasma displays; these displays do not use a raster scan to create an image, and so cannot benefit from interlacing: in practice, they have to be driven with a progressive scan signal. The deinterlacing circuitry to get progressive scan from a normal interlaced broadcast television signal can add to the cost of a television set using such displays. Currently, progressive displays dominate the HDTV market.
Interlace and computers.
In the 1970s, computers and home video game systems began using TV sets as display devices. At that point, a 480-line NTSC signal was well beyond the graphics abilities of low cost computers, so these systems used a simplified video signal that made each video field scan directly on top of the previous one, rather than each line between two lines of the previous field. This marked the return of progressive scanning not seen since the 1920s. Since each field became a complete frame on its own, modern terminology would call this 240p on NTSC sets, and 288p on PAL. While consumer devices were permitted to create such signals, broadcast regulations prohibited TV stations from transmitting video like this. Computer monitor standards such as CGA were further simplifications to NTSC, which improved picture quality by omitting modulation of color, and allowing a more direct connection between the computer's graphics system and the CRT.
By the mid-1980s, computers had outgrown these video systems and needed better displays. The Apple IIgs suffered from the use of the old scanning method, with the highest display resolution being 640x200, resulting in a severely distorted tall narrow pixel shape, making the display of realistic proportioned images difficult. Solutions from various companies varied widely. Because PC monitor signals did not need to be broadcast, they could consume far more than the 6, 7 and 8 MHz of bandwidth that NTSC and PAL signals were confined to. IBM's Monochrome Display Adapter and Enhanced Graphics Adapter as well as the Hercules Graphics Card and the original Macintosh computer generated a video signal close to 350p. The Commodore Amiga created a true interlaced NTSC signal (as well as RGB variations). This ability resulted in the Amiga dominating the video production field until the mid-1990s, but the interlaced display mode caused flicker problems for more traditional PC applications where single-pixel detail is required. 1987 saw the introduction of VGA, on which PCs soon standardized, Apple only followed suit some years later with the Mac when the VGA standard was improved to match Apple's proprietary 24 bit color video standard also introduced in 1987.
In the late 1980s and early 1990s, monitor and graphics card manufacturers introduced newer high resolution standards that once again included interlace. These monitors ran at very high refresh rates, intending that this would alleviate flicker problems. Such monitors proved very unpopular. While flicker was not obvious on them at first, eyestrain and lack of focus nevertheless became a serious problem. The industry quickly abandoned this practice, and for the rest of the decade all monitors included the assurance that their stated resolutions were "non-interlaced". This experience is why the PC industry today remains against interlace in HDTV, and lobbied for the 720p standard. Also the industry is lobbying beyond 720p, actually 1080/60p for NTSC legacy countries, and 1080/50p for PAL legacy countries.

</doc>
<doc id="56190" url="http://en.wikipedia.org/wiki?curid=56190" title="Fifth Estate (periodical)">
Fifth Estate (periodical)

Fifth Estate (FE) is a US periodical, based in Detroit, Michigan begun in 1965, but with staff members across North America who connect via the Internet. Its editorial collective sometimes has divergent views on the topics the magazine addresses but generally shares anarchist, anti-authoritarian outlook and a non-dogmatic, action-oriented approach to change. The title implies that the periodical is an alternative to the fourth estate (traditional print journalism).
"Fifth Estate" is frequently cited as the longest running English language anarchist publication in North America, although this is sometimes disputed since it became only explicitly anti-authoritarian in 1975 after ten years of publishing as part of the 1960s Underground Press movement. The archives for the "Fifth Estate" are held at the Labadie Collection in Ann Arbor, Michigan.
History.
Origin.
"Fifth Estate" was started by Harvey Ovshinsky, a seventeen-year-old youth from Detroit. He was inspired by a 1965 summer trip to California where he worked on the "Los Angeles Free Press", the first underground paper in the US; Harvey's father, inventor Stan Ovshinsky, knew the editor of the "Free Press", Art Kunkin, from their years as comrades in the Socialist Party. The name "Fifth Estate" was inspired by The Fifth Estate coffee house on the Sunset Strip, where the "Free Press" had its office in the basement.
The first issue was published on November 19, 1965—"That's what we really are—the voice of the liberal element in Detroit," it said. It was produced on a typewriter and then reproduced by offset lithograph, in an 8-page tabloid newspaper format with two pages left blank. It featured a critical review of a Bob Dylan concert, a borrowed Jules Feiffer cartoon, alternative events listing and an announcement of a forthcoming anti-Vietnam War march. None of these things would have been included in contemporary newspapers.
In 1966 Ovshinsky moved the office from his parents' basement to a Cass Corridor storefront near Wayne State University. Here the paper was saved from extinction by the Detroit Committee to End the War in Vietnam, John Sinclair's Artist Workshop, and other radicals, with Sinclair signing on as the paper's first music editor. Later in 1966 the paper moved to Plum Street where they also established a bookshop. "Fifth Estate" thrived in the late sixties, a period when over 500 underground papers emerged in the US. Thousands of copies were distributed locally with hundreds more being sent to GIs in Vietnam. "Fifth Estate" openly called on soldiers to mutiny. In 1967 the "Fifth Estate" offices were tear-gassed by the National Guard during the 12th Street riot. In this period the print run reached 15,000 – 20,000 copies, publishing biweekly in a tabloid newspaper format of 20 to 32 pages, with local ads and listings.
The spirit of the paper during the first ten years of its existence was summed up in a Feb. 1, 1969 staff editorial:
"We believe that people who are serious in their criticism of this society and their desire to change it must involve themselves in serious revolutionary struggle. We do not believe that music is revolution. We do not believe that dope is revolution. We do not believe that poetry is revolution. We see these as part of a burgeoning revolutionary culture. They cannot replace political struggle as the main means by which the capitalist system will be destroyed. The Man will not allow his social and economic order to be taken from him by Marshall amps and clashing cymbals. Ask the Cubans, the Vietnamese or urban American blacks what lengths the system is willing to go to, to preserve itself."
1970s.
By 1972 the optimism of the sixties had worn off and the tone of the paper became more concerned with struggle than fun. Ovshinsky had left in 1969, leaving a group of young people (teenagers or in their early twenties) to run the paper. Peter Werbe, a 29-year-old Michigan State dropout who had been with the paper since March 1966, took over as editor. The staff sent delegations to Vietnam, Cambodia and Cuba. The massive defeat of George McGovern and the election of Richard Nixon for a second term with an increased vote damaged the movement—many underground papers stopped coming out and the alternative news services such as the Liberation News Service, and the Underground Press Syndicate collapsed. The "Fifth Estate" was mentioned in the national press when one of its reporters, Pat Halley, threw a shaving cream pie at Guru Maharaj Ji in 1973. Though the guru forgave him publicly, two of his followers attacked Halley a week later and fractured his skull.
By 1975, "Fifth Estate" was lingering on—many staff had burnt out through too much activism and they had their share of internal disputes. The debts were mounting. In August 1975, Vol. 11, No.1 declared "The issue you are now holding is the last issue of the "Fifth Estate" - the last issue of a failing capitalist enterprise…This is also the first issue of a new "Fifth Estate"." This was the first explicitly anti-authoritarian issue of "Fifth Estate". The paper had been taken over by the Eat the Rich Gang, a group that had successfully published several pamphlets and were particularly influenced by Fredy Perlman, Jacques Camatte, Jean Baudrillard, Council communism, and Left Communism, as well as the Situationists. They did not originally identify themselves as explicitly anarchist and had no contacts with the anarchist currents of the 1930s. However, they were contacted by veterans of that period who they saw as powerful role models. Those included Marcus Graham (publisher of the 1930s anarchist periodical "Man!") and Spanish and Italian anarchist veterans. They also developed a close relationship with Black and Red Press, a radical printers/publishers group with which Lorraine and Fredy Perlman were involved.
1980s and 1990s.
By 1980, the paper had become more anti-technological and anti-civilisation, something for which it was well known throughout the '80's. It was the focal point for the development of the political trend of anarcho-primitivism. Long-time contributor John Zerzan published his seminal essays on time, language, art, number and agriculture in the magazine. His articles were frequently accompanied by long critiques by George Bradford (née David Watson) or Bob Brubaker, who developed different versions of primitivism. After Zerzan's 1988 article on agriculture, he started publishing his new essays in . Dismayed by what he saw as the excesses of Zerzan and others, Watson eventually repudiated primitivism in his 1997 essay "Swamp Fever". However, as of 2012, Zerzan began publishing articles in the Fifth Estate again on subjects as varied as the Black Bloc, the sea, and the Luddies.
2001 to present.
In 2002, the center of the magazine shifted from Detroit, Michigan to Liberty, Tennessee when long-time contributor Andrew Smith (who wrote under the name Andy Sunfrog) took over the main editorial duties of the magazine, although long-time Detroit staffers like Peter Werbe remained involved.
In 2006, "Fifth Estate" decentralized their editorial group, and since then issues have been published that were primarily produced in Michigan, Tennessee, New York and Wisconsin. The current editorial collective has moved away from primitivism, does not endorse a specific political line and welcomes voices from disparate strains of anti-authoritarian thought. The group also continues to endorse anarchism as a specific ideology, but embraces a more inclusive, yet still radical, anti-capitalist perspective. Continuing to cover environmental and anti-capitalist resistance, articles have also appeared which address immigration, race, feminism, queer sexuality and transgender issues.
Smith left the paper in 2009 to pursue an academic career at a Tennessee university, but still contributes an occasional article. The magazine shifted back to Detroit for final editing and production with Peter and Marilyn Werbe having responsibility for much of that plus the magazine's business functions. The collective now consists of the Werbes and several others throughout North America
In 2008, long-time contributor Marie Mason was arrested as part of what some call the Green Scare. In February 2009, she was sentenced to almost 22 years for two acts of environmentally motivated property destruction. The "Fifth Estate" has run articles protesting both the labeling of her actions as "terrorism" as well as the long sentence she received.

</doc>
<doc id="56191" url="http://en.wikipedia.org/wiki?curid=56191" title="Memorial Day">
Memorial Day

Memorial Day is a federal holiday in the United States for remembering the people who died while serving in the country's armed forces. The holiday, which is observed every year on the last Monday of May, was formerly known as Decoration Day and originated after the American Civil War to commemorate the Union and Confederate soldiers who died in the war. By the 20th century, Memorial Day had been extended to honor all Americans who died while in the military service. It typically marks the start of the summer vacation season, while Labor Day marks its end.
Many people visit cemeteries and memorials, particularly to honor those who have died in military service. Many volunteers place an American flag on each grave in national cemeteries.
Annual Decoration Days for particular cemeteries are held on a Sunday in late spring or early summer in some rural areas of the American South, notably in the mountain areas. In cases involving a family graveyard where remote ancestors as well as those who were deceased more recently are buried, this may take on the character of an extended family reunion to which some people travel hundreds of miles. People gather on the designated day and put flowers on graves and renew contacts with relatives and others. There often is a religious service and a picnic-like "dinner on the ground," the traditional term for a potluck meal in which people used to spread the dishes out on sheets or tablecloths on the grass. It is believed that this practice began before the American Civil War and thus may reflect the real origin of the "memorial day" idea.
Memorial Day is not to be confused with Veterans Day; Memorial Day is a day of remembering the men and women who "died" while serving, while Veterans Day celebrates the service of all U.S. military veterans.
History of the holiday.
The practice of decorating soldiers' graves with flowers is an ancient custom. Soldiers' graves were decorated in the U.S. before and during the American Civil War. A claim was made in 1906 that the first Civil War soldier's grave ever decorated was in Warrenton, Virginia, on June 3, 1861, implying the first Memorial Day occurred there. Though not for Union soldiers, there is authentic documentation that women in Savannah, Georgia, decorated Confederate soldiers' graves in 1862. In 1863, the cemetery dedication at Gettysburg, Pennsylvania, was a ceremony of commemoration at the graves of dead soldiers. Local historians in Boalsburg, Pennsylvania, claim that ladies there decorated soldiers' graves on July 4, 1864. As a result, Boalsburg promotes itself as the birthplace of Memorial Day.
Following President Abraham Lincoln's assassination in April 1865, there were a variety of events of commemoration. The sheer number of soldiers of both sides who died in the Civil War, more than 600,000, meant that burial and memorialization took on new cultural significance. Under the leadership of women during the war, an increasingly formal practice of decorating graves had taken shape. In 1865, the federal government began creating national military cemeteries for the Union war dead.
The first widely-publicized observance of a Memorial Day-type observance after the Civil War was in Charleston, South Carolina, on May 1, 1865. During the war, Union soldiers who were prisoners of war had been held at the Hampton Park Race Course in Charleston; at least 257 Union prisoners died there and were hastily buried in unmarked graves. Together with teachers and missionaries, black residents of Charleston organized a May Day ceremony in 1865, which was covered by the "New York Tribune" and other national papers. The freedmen cleaned up and landscaped the burial ground, building an enclosure and an arch labeled "Martyrs of the Race Course." Nearly 10,000 people, mostly freedmen, gathered on May 1 to commemorate the war dead. Involved were about 3,000 school children, newly enrolled in freedmen's schools, as well as mutual aid societies, Union troops, black ministers and white northern missionaries. Most brought flowers to lay on the burial field. Today the site is remembrance celebration would come to be called the "First Decoration Day" in the North.
David W. Blight described the day:
This was the first Memorial Day. African Americans invented Memorial Day in Charleston, South Carolina. What you have there is black Americans recently freed from slavery announcing to the world with their flowers, their feet, and their songs what the war had been about. What they basically were creating was the Independence Day of a Second American Revolution.
However, Blight stated he "has no evidence" that this event in Charleston inspired the establishment of Memorial Day across the country.
On May 26, 1966, President Johnson signed a presidential proclamation naming Waterloo, New York, as the birthplace of Memorial Day. Earlier, the 89th Congress had adopted House Concurrent Resolution 587, which officially recognized that the patriotic tradition of observing Memorial Day began one hundred years prior in Waterloo, New York. Other communities claiming to be the birthplace of Memorial Day include Boalsburg, Pennsylvania, Carbondale, Illinois, Columbus, Georgia, and Columbus, Mississippi. A recent study investigating the Waterloo claim as well as dozens of other origination theories concludes that nearly all of them are apocryphal legends.
In the North.
Copying an earlier holiday that had been established in the Southern states, on May 5, 1868, in his capacity as commander-in-chief of the Grand Army of the Republic, the veterans' organization for Union Civil War veterans, General John A. Logan issued a proclamation calling for "Decoration Day" to be observed annually and nationwide. It was observed for the first time that year on Saturday May 30; the date was chosen because it was not the anniversary of any particular battle. According to the White House, the May 30 date was chosen as the optimal date for flowers to be in bloom.
Memorial events were held in 183 cemeteries in 27 states in 1868, and 336 in 1869. The northern states quickly adopted the holiday. Michigan made "Decoration Day" an official state holiday in 1871 and by 1890, every northern state had followed suit. The ceremonies were sponsored by the Women's Relief Corps, the women's auxiliary of the Grand Army of the Republic (GAR), which had 100,000 members. By 1870, the remains of nearly 300,000 Union dead had been reinterred in 73 national cemeteries, located near major battlefields and thus mainly in the South. The most famous are Gettysburg National Cemetery in Pennsylvania and Arlington National Cemetery, near Washington, D.C.
Memorial Day speeches became an occasion for veterans, politicians, and ministers to commemorate the War and, at first, to rehash the "atrocities" of the enemy. They mixed religion and celebratory nationalism and provided a means for the people to make sense of their history in terms of sacrifice for a better nation. People of all religious beliefs joined together and the point was often made that the German and Irish soldiers had become true Americans in the "baptism of blood" on the battlefield.
Ironton, Ohio, lays claim to the nation's oldest continuously running Memorial Day parade. Its first parade was held May 5, 1868, and the town has held it every year since; however, the Memorial Day parade in Doylestown, Pennsylvania, predates Ironton's by one year.
In the South.
Evidence exists that shows General Logan had adopted and adapted for the North the annual Confederate Memorial Day custom that had been in practice in the South since 1866. The U.S. National Park Service attributes the beginning to the ladies of Columbus, Georgia. The separate tradition of Memorial Day observance which had emerged earlier in the South was linked to the Lost Cause and served as the prototype for the national day of memory. Historians acknowledge the Ladies Memorial Association played a key role in its development. Various dates ranging from April 25 to mid-June were adopted in different Southern states. Across the South, associations were founded, many by women, to establish and care for permanent cemeteries for the Confederate dead, organize commemorative ceremonies, and sponsor appropriate monuments as a permanent way of remembering the Confederate cause and sacrifice. The most important was the United Daughters of the Confederacy, which grew from 17,000 members in 1900 to nearly 100,000 women by World War I. They were "strikingly successful at raising money to build Confederate monuments, lobbying legislatures and Congress for the reburial of Confederate dead, and working to shape the content of history textbooks."
On April 25, 1866, women in Columbus, Mississippi laid flowers on the graves of both the Union and Confederate dead in the city's cemetery. The early Confederate Memorial Day celebrations were simple, somber occasions for veterans and their families to honor the dead and tend to local cemeteries. By 1890, there was a shift from the emphasis on honoring specific soldiers to a public commemoration of the lost Confederate cause. Changes in the ceremony's hymns and speeches reflect an evolution of the ritual into a symbol of cultural renewal and conservatism in the South. By 1913, Blight argues, the theme of American nationalism shared equal time with the Lost Cause.
At Gettysburg.
The ceremonies and Memorial Day address at Gettysburg National Park became nationally well known, starting in 1868. In July 1913, veterans of the United States and Confederate armies gathered in Gettysburg to commemorate the fifty-year anniversary of the Civil War's bloodiest and most famous battle.
The four-day "Blue-Gray Reunion" featured parades, re-enactments, and speeches from a host of dignitaries, including President Woodrow Wilson, the first Southerner elected to the White House after the War. James Heflin of Alabama gave the main address. Heflin was a noted orator; two of his best-known speeches were an endorsement of the Lincoln Memorial and his call to make Mother's Day a holiday. His choice as Memorial Day speaker was criticized, as he was opposed for his support of segregation; however, his speech was moderate in tone and stressed national unity and goodwill, gaining him praise from newspapers.
Since the cemetery dedication at Gettysburg occurred on November 19, that day (or the closest weekend) has been designated as their own local memorial day that is referred to as Remembrance Day.
Name and date.
The preferred name for the holiday gradually changed from "Decoration Day" to "Memorial Day", which was first used in 1882. It did not become more common until after World War II, and was not declared the official name by Federal law until 1967. On June 28, 1968, the Congress passed the Uniform Monday Holiday Act, which moved four holidays, including Memorial Day, from their traditional dates to a specified Monday in order to create a convenient three-day weekend. The change moved Memorial Day from its traditional May 30 date to the last Monday in May. The law took effect at the federal level in 1971. After some initial confusion and unwillingness to comply, all 50 states adopted Congress' change of date within a few years.
Memorial Day endures as a holiday which most businesses observe because it marks the unofficial beginning of summer. The Veterans of Foreign Wars (VFW) and Sons of Union Veterans of the Civil War (SUVCW) advocate returning to the original date, although the significance of the date is tenuous. The VFW stated in a 2002 Memorial Day Address:
Changing the date merely to create three-day weekends has undermined the very meaning of the day. No doubt, this has contributed a lot to the general public's nonchalant observance of Memorial Day.
Starting in 1987 Hawaii's Senator Daniel Inouye, a World War II veteran, introduced a measure to return Memorial Day to its traditional date. Inouye continued introducing the resolution until his death in 2012.
Traditional observance.
On Memorial Day, the flag of the United States is raised briskly to the top of the staff and then solemnly lowered to the half-staff position, where it remains only until noon. It is then raised to full-staff for the remainder of the day.
The half-staff position remembers the more than one million men and women who gave their lives in service of their country. At noon, their memory is raised by the living, who resolve not to let their sacrifice be in vain, but to rise up in their stead and continue the fight for liberty and justice for all.
The National Memorial Day Concert takes place on the west lawn of the United States Capitol. The concert is broadcast on PBS and NPR. Music is performed, and respect is paid to the men and women who gave their lives for their country.
For many Americans, the central event is attending one of the thousands of parades held on Memorial Day in large and small cities all over the country. Most of these feature marching bands and an overall military theme with the National Guard and other servicemen participating along with veterans and military vehicles from various wars.
One of the longest-standing traditions is the running of the Indianapolis 500, an auto race which has been held in conjunction with Memorial Day since 1911. It runs on the Sunday preceding the Memorial Day holiday. The Coca-Cola 600 stock car race has been held later the same day since 1961. The Memorial Tournament golf event has been held on or close to the Memorial Day weekend since 1976.
Poppies.
In 1915, following the Second Battle of Ypres, Lieutenant Colonel John McCrae, a physician with the Canadian Expeditionary Force, wrote the poem, "In Flanders Fields". Its opening lines refer to the fields of poppies that grew among the soldiers' graves in Flanders.
In 1918, inspired by the poem, YWCA worker Moina Michael attended a YWCA Overseas War Secretaries' conference wearing a silk poppy pinned to her coat and distributed over two dozen more to others present. In 1920, the National American Legion adopted it as their official symbol of remembrance.
Interpretations.
Scholars, following the lead of sociologist Robert Bellah, often make the argument that the United States has a secular "civil religion" - one with no association with any religious denomination or viewpoint - that has incorporated Memorial Day as a sacred event. With the Civil War, a new theme of death, sacrifice and rebirth enters the civil religion. Memorial Day gave ritual expression to these themes, integrating the local community into a sense of nationalism. The American civil religion, in contrast to that of France, was never anticlerical or militantly secular; in contrast to Britain, it was not tied to a specific denomination, such as the Church of England. The Americans borrowed from different religious traditions so that the average American saw no conflict between the two, and deep levels of personal motivation were aligned with attaining national goals.
In literature and music.
Charles Ives's symphonic poem "Decoration Day" depicted the holiday as he experienced it in his childhood, with his father's band leading the way to the town cemetery, the playing of "Taps" on a trumpet, and a livelier march tune on the way back to the town. It is frequently played with three other Ives works based on holidays as the second movement of "".
There is also a 2012 film, "Memorial Day", starring James Cromwell, Jonathan Bennett, and John Cromwell.
Further reading.
</dl>

</doc>
<doc id="56192" url="http://en.wikipedia.org/wiki?curid=56192" title="Parnall">
Parnall

Parnall was a British aircraft manufacturer, that evolved from a wood-working company before the First World War to a significant designer of military and civil aircraft into the 1940s. It was based in the west of England.
History.
Parnall and Sons of Mivart Street, Eastville, Bristol was a wood-working firm in the period before the First World War. The demands of wartime aircraft production meant that many woodworking companies were contracted to build aircraft. Parnall received large orders from the Admiralty to build aircraft designed by other manufacturers, principally the Avro 504 and Short Admiralty Type 827 (of which 20 were built).
The company split in two in 1921, when George Geach Parnall left the company and formed George Parnall & Co. Ltd., the original company Parnall & Sons moving to Fishponds, Bristol, in 1923 to continue production of shop-fittings and aircraft components. In 1936 Parnall joined with Hendy Aircraft and Nash and Thompson to form Parnall Aircraft Ltd. In 1939 they stopped aircraft production to concentrate on aircraft components, particularly gun turrets.
Aircraft.
Scout.
The quality of workmanship and their enthusiasm for aircraft production was noted and so in 1916 they were approached for a design of their own to meet a requirement for a coastal defence aircraft. At that time the principal threat to Britain was seen as Zeppelin attacks and a specially designed fighter was sought to counter this threat.
Parnall's first aircraft, designed by A. Camden Pratt was called the Scout, a large single-seater, two-bay biplane powered by a 230 hp Sunbeam Maori, with an upward-firing gun mounted on the upper wing. It acquired nicknames including "Zeppelin Chaser" and "Zepp Straffer" but the design was not a success as it was too heavy; it is believed that only two flights were made.
Panther.
A batch of the Fairey Hamble Baby were built and then another enquiry came in for a shipboard reconnaissance plane. For this work the Admiralty released the services of Harold Bolas, an engineer who had been instrumental in designing flying-boat hulls. Bolas's first design for Parnall was the Panther; among its notable features was a birch plywood monocoque fuselage with the pilot and observer placed high, offering them an all-round view. The fuselage was hinged for shipboard stowage; for ditching the plane had a hydrovane ahead of the undercarriage as well as air-bag floatation gear. Tests in 1918 were disappointing as performance was only marginally better than the Sopwith 1½ Strutter which it was designed to replace, and only 312 aircraft were ordered from the firm.
Around this time Parnall and Sons was acquired by W. & T. Avery Ltd. who considered prospects in the aircraft industry poor with the ending of hostilities. An attempt by the Admiralty to reduce the order led to a disagreement with the result that Parnall ceased aircraft manufacture and production passed to Filton where 150 were built during 1919 and 1920. In service the aircraft performed well being described as delightful to fly with none of the vices associated with large rotary engines, however deck landings on ships were hazardous in this period and the accident rate was high. Late production Panthers were fitted with oleo undercarriage and remained in service until 1926. Two aircraft were acquired by the US Navy and 12 were supplied to Japan.
Puffin.
Despite this setback, the name Parnall was to reappear when George Geach Parnall formed a new company, George Parnall & Co. Ltd. with a handful of previous employees and opened the Coliseum Works in Park Row, Bristol in 1921. The first design, also by Bolas, was another naval aircraft: the Puffin. This was a large two-seat, two-bay amphibious biplane powered by a 450 hp Napier Lion engine. The fuselage was mounted above a large central float which contained wheels that could be lowered through a vertical slot, large out-rigger floats were fitted on the lower planes. The observer was equipped with a Scarff ring and had an uninterrupted field of fire as the fin and rudder were mounted beneath the fuselage. Three prototypes were built but production was not proceeded with.
Plover.
More successful was Bolas's next design, the Plover naval fighter, a single bay wooden biplane powered by one 436 hp Bristol Jupiter IV engine. With the pilot placed high for a good view over the short nose and close-cowled radial engine, it could almost have come from the Filton drawing boards. Amphibious wheeled floats were tested and one was fitted with an Armstrong Siddeley Jaguar radial. The Plover had a good performance but only six were built for service in 1923; the Royal Navy preferring the Fairey Flycatcher despite its lower speed. One Plover was entered in the 1926 King's Cup Air Race but failed to finish.
Possum.
Centrally mounted engines powering wing-mounted airscrews was a concept that was explored with the large four-engined Bristol Tramp, the twin-engined Boulton Paul Bodmin and the next design to emerge from Parnall, the single-engined "Possum". Both the Tramp and the Possum were triplanes with twin tractor airscrews driven by shafts from the fuselage. The Possum was officially described as a "postal aircraft" – a curious designation for an aircraft having gun positions in the nose and amidships. The centrally located 450 hp Napier Lion engine had side-mounted radiators which could be retracted in flight to achieve additional streamlining. The Possum fared rather better than the Tramp, which never flew, and performed well, making a public appearance at the 1923 Hendon Pageant. Despite having proved the practicality of its layout it was regarded as something of a curiosity by the pilots that flew it from Martlesham Heath in Suffolk. Experience revealed insufficient advantages to support any further development of this concept.
Pixie.
In 1923 the "Daily Mail" and the Duke of Sutherland sponsored competitions designed to stimulate light aircraft development; Parnall entered a single-seat low-wing monoplane, the "Pixie", built in two forms with 13 hp and 26 hp Douglas engines. The Pixie won the £500 Abdulla Company prize for speed. The aircraft produced for the 1923 Lympne Trials were unrealistic machines, being too lightly powered to be flown in even modest winds, and in 1924 the Air Council announced another competition for higher powered two-seaters. Bolas revised the Pixie to produce both a monoplane and a biplane with an upper wing called the Pixie III and Pixie IIIA, respectively. Both were powered by 32 hp Bristol Cherub III engines. Neither Pixie was successful in competition, as both suffered forced landings with engine trouble. The Pixie III was entered again in the 1926 Lympne Trials and finished fourth.
Parnall Pixie IIIa G-EBJG is still in existence with the Midland Air Museum, Coventry, England. The remains are in deep store and are not generally on view to the public without prior arrangement.
Perch.
Naval interest continued with Bolas' next design, the "Perch" fleet aircraft trainer. This was a dual role machine that could be used for training pilots in deck-landing techniques or, when fitted with floats, as a seaplane trainer. The aircraft was an equal-span biplane that featured side-by-side seating and a 220 hp Rolls-Royce Falcon engine set low in the nose to give the pilot an excellent view for landing. The Perch performed well but no production order was made.
Gyroplane.
In 1928 the Cierva Autogiro Company contracted Parnall to design and build two machines to be designated C10 and C11 in the Cierva series, the C11 was later called the Parnall Gyroplane. The airframes were designed by Harold Bolas, the C10 was powered by an Armstrong Siddeley Genet while the C11 used a 120 hp Airdisco. The C10 turned over on take-off at the airfield in Yate and was taken to Hamble for repair at which time it was modified to incorporate an engine-driven rotor-starting device. During these projects he worked with the inventor of autogyro, Juan de la Cierva.
Peto.
The "Peto" submarine-launched floatplane was amongst the most technically difficult tasks that Parnall took on. It was a two-seat reconnaissance float-biplane of very small overall dimensions designed to be folded and carried in the confines of a submarine. Of mixed wood, fabric, aluminium and steel construction, it had unequal span, warren-braced rectangular wings and the first aircraft was powered by a 128 hp Bristol Lucifer engine and had mahogany plywood "Consuta" type floats. Performance on test was generally satisfactory but modifications were put in hand and the machine was rebuilt with new wings, metal floats and a 169 hp Armstrong Siddeley Mongoose engine. Tests both on the sea and in the air showed that Bolas had fully met the requirements and it was officially judged to be exceptionally good; it was successfully launched by catapult from the ill-fated submarine "M2" but the concept of submarine-carried aircraft died in the Royal Navy after the loss of the "M2" which took one of the Petos with it.
By the mid-1920s it was clear that an aircraft factory in the middle of a town was less than satisfactory where test flying was concerned, some of the aircraft having made their first flights from Filton. Accordingly a move was made to Yate, then in Gloucestershire, where hangars were built beside a grass aerodrome. Rumour has it that money was so tight that George would only allow a central strip for the runway to be mowed as he needed the profit from the hay crop! New aircraft continued to emerge at a steady rate and Harold Bolas designed two further naval types, the Pike and the Pipit.
Pike.
The "Pike" was a large three-seater reconnaissance float-biplane powered by a 471 hp Napier Lion. The deep and narrow fuselage filled the gap between the back-staggered wings; the pilot's cockpit was located near the nose affording an excellent view. Defence was provided by a Scarf-mounted Lewis gun in the observer's cockpit at the upper wing trailing edge while the pilot had a forward-firing Vickers machine gun. Trials of the single prototype were carried out at Felixstowe during 1927 but the report was unfavourable; the handling in flight was considered poor, the pilot's cockpit was criticised for being cold and draughty and performance generally below specification. Neither the Pike nor its rival, the Short Sturgeon, were developed further.
Pipit.
The "Pipit" was an aircraft of a very different character, a fleet fighter biplane of very clean appearance, designed to specification 21/26 and powered by a 495 hp Rolls-Royce F.XI. Of metal construction with fabric covering, the Pipit had a number of innovative features including detachable panels giving easy access to the fuselage, a wide-track undercarriage and a retractable radiator.
Two prototypes were ordered, the first flying from Yate in mid-1928. Despite its promising appearance and engineering novelty the Pipit did not fly as well as expected, the elevator being criticised as heavy while the rudder was weak and the type was longitudinally unstable. Before much development flying could be done, however, the tailplane failed due to flutter in a diving test on 20 September. The pilot landed the damaged aircraft but it was destroyed in the resulting somersault. The pilot sustained serious injuries.
A second modified machine was built, powered by a Rolls-Royce F.XIIS, featuring a strut-braced tailplane, rigidly linked ailerons and a large elliptical horn balanced fin and rudder assembly designed to improve its effectiveness. It first flew in January 1929 and was an improvement over the earlier machine but the rudder was still unsatisfactory. On 24 February a test was made by a service test pilot; whilst investigating the rudder's properties in a series of dives, violent flutter developed and both the fin and rudder broke away. The machine became uncontrollable but luckily the pilot escaped by parachute below 1000 ft. It was the end for the Pipit and the incident left a stigma in official circles from which Parnall never really recovered.
Imp.
Harold Bolas always maintained an interest in light aircraft and in 1927 produced a small, two-seater biplane: the "Imp". Powered initially by an uncowled Armstrong Siddeley Genet II of 80 hp, it was of striking appearance as it had a straight lower wing joined by wide chord struts without bracing wires to sharply swept upper wings. With the engine installation cleaned up, front cockpit faired over and a headrest fitted it flew into 8th place in the 1928 King's Cup race. At that time Parnall had thoughts of going into engine manufacture and collaborated with D.R.Pobjoy in the development of the 65 hp Pobjoy P air-cooled radial engine. This was test flown in the Imp, but Pobjoy decided to form his own company and his collaboration with Parnall ceased.
Elf.
The attractive "Elf" was Bolas's last design for Parnall. It was a two-seater, touring biplane in the de Havilland Moth class using fabric-covered wooden construction and powered by an ADC Hermes I. It flew for the first time in 1929. A naval influence showed as it used Warren girder bracing with folding wings. A competent if somewhat uninspired performer, the prototype was sold to Lord Apsley in 1932 but it was destroyed in a crash in 1934. Two more were built as Elf IIs with ADC Hermes II engines. One, sold in 1933, crashed due to fuel-pump failure two months later but the second went to Lord Apsley as a replacement for the crashed Elf I.
In 1929 Harold Bolas, after some twelve years of trying to produce a winner for Parnalls, finally decided to leave for the USA. An enthusiastic and respected designer, he was highly regarded for the originality of his designs and was a skilful theoretician. He was not above test flying his own creations, suitably fortified after a visit to the nearby Railway Inn pub. One of his Elf biplanes survives to this day with the Shuttleworth Trust, occasionally flying at Old Warden in Bedfordshire.
Prawn & Parasol.
Bolas was succeeded by H.V. Clark who produced two interesting research aircraft, each built to test a specific aspect. The first was the "Prawn", a small single-engined, single-seater parasol flying-boat powered by a 65 hp Ricardo-Burt engine. It was designed to assess the feasibility of mounting a flying boat's engine in the extreme bow thereby producing a low drag installation. To make this feasible a very small four-bladed propeller was needed, and the engine could be tilted up to 22 degrees upwards to avoid the spray over the nose. It never was a very practical idea and it seems that little was done with it.
More useful was the "Parasol" of which two were built. This machine was a flying full scale aerodynamic test vehicle, it could test the effects seen in wind-tunnel tests but without the effects of scale inherent in a tunnel. It was a two-seater, the observer occupying the front cockpit which was equipped with a dynamometer for measuring flight-loads on the variable incidence wings which featured slots, flaps and separated ailerons. To eliminate the effects of the propeller, the Armstrong Siddeley Lynx engine could be stopped in flight for gliding then restarted with a gas starter. A camera could be mounted on struts above the tailplane and this was used for photographing tufts of wool that showed the airflow patterns over the wings.
G.4/31.
The inter-war years produced a series of requirements for "general purpose" aircraft; in those miserly times this was a cheap way of providing the Air Force with aircraft that, it was hoped, would be of some general use if hostilities arose. Air Ministry Specification G.4/31 was no exception. Conceived as a replacement for the Westland Wapiti and Fairey Gordon it initially called for day and night bombing capabilities, reconnaissance, torpedo and dive-bombing roles. Designs came from Handley Page, Vickers, Fairey, Armstrong Whitworth and Parnall with what was to be the final expressly military type, the unnamed type G.4/31. This was a large angular biplane with gull-type upper wings, wheel spats, a good collection of interplane and fuselage struts and very generous tail surfaces. Power came from a 690 hp Bristol Pegasus I M3 in a Townend ring, there was a forward-firing gun for the pilot and the observer had a Scarf-mounted Lewis gun. Flight tests were carried out during 1935 from Yate. It is believed the aircraft had handling problems for it was not delivered to Martlesham Heath until early in 1936, long after the competition had been decided in favour of the Vickers Type 253. The machine was used for armament trials until March 1937 when it was damaged in a crash and subsequently scrapped.
Heck.
In 1929 Parnall built a cabin monoplane called the "Hendy 302" to the design of Basil B. Henderson who followed it with another monoplane, the advanced "Heck". 1935 marked a major change for Parnall when the firm acquired the assets of both Hendy Aircraft Ltd. and the armaments firm of Nash and Thompson. A new company called Parnall Aircraft was formed and the Heck passed to Parnall ownership, renamed the Parnall Heck it set a new record for the run from Cape Town to England of 6 days, 8 hours and 27 minutes in November 1936. A three-seat derivative with a fixed, spatted undercarriage, powered by a Gypsy Six was produced as the Parnall Heck 2C, six were built in expectation of sales but none were sold and the first four were therefore used as communications aircraft by Parnall in connection with their armaments activities. The fifth and sixth aircraft were used for test-flying the Wolseley Aries radial engine and gun sight development work.
382/Heck III.
The final Parnall aircraft was an open two-seater trainer derivative of the Heck to Air Ministry Specification T.1/37 named the Parnall 382, later the Heck III. It featured the Heck's advanced wing and had a speed range of 139 mph to 43 mph; it first flew in 1939. At Martlesham Heath it was assessed as pleasant to fly and generally good as a trainer. Notwithstanding a few modifications, no order was forthcoming. It was to be the last Parnall machine to fly, after which Parnall turned his attention to producing aircraft gun turrets to Archie Frazer Nash's design in the Yate factory until the war ended.

</doc>
